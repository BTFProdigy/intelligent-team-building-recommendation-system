Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 688?697, Prague, June 2007. c?2007 Association for Computational Linguistics
The Infinite PCFG using Hierarchical Dirichlet Processes
Percy Liang Slav Petrov Michael I. Jordan Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, petrov, jordan, klein}@cs.berkeley.edu
Abstract
We present a nonparametric Bayesian model
of tree structures based on the hierarchical
Dirichlet process (HDP). Our HDP-PCFG
model allows the complexity of the grammar
to grow as more training data is available.
In addition to presenting a fully Bayesian
model for the PCFG, we also develop an ef-
ficient variational inference procedure. On
synthetic data, we recover the correct gram-
mar without having to specify its complex-
ity in advance. We also show that our tech-
niques can be applied to full-scale parsing
applications by demonstrating its effective-
ness in learning state-split grammars.
1 Introduction
Probabilistic context-free grammars (PCFGs) have
been a core modeling technique for many as-
pects of linguistic structure, particularly syntac-
tic phrase structure in treebank parsing (Charniak,
1996; Collins, 1999). An important question when
learning PCFGs is how many grammar symbols
to allocate to the learning algorithm based on the
amount of available data.
The question of ?how many clusters (symbols)??
has been tackled in the Bayesian nonparametrics
literature via Dirichlet process (DP) mixture mod-
els (Antoniak, 1974). DP mixture models have since
been extended to hierarchical Dirichlet processes
(HDPs) and HDP-HMMs (Teh et al, 2006; Beal et
al., 2002) and applied to many different types of
clustering/induction problems in NLP (Johnson et
al., 2006; Goldwater et al, 2006).
In this paper, we present the hierarchical Dirich-
let process PCFG (HDP-PCFG). a nonparametric
Bayesian model of syntactic tree structures based
on Dirichlet processes. Specifically, an HDP-PCFG
is defined to have an infinite number of symbols;
the Dirichlet process (DP) prior penalizes the use
of more symbols than are supported by the training
data. Note that ?nonparametric? does not mean ?no
parameters?; rather, it means that the effective num-
ber of parameters can grow adaptively as the amount
of data increases, which is a desirable property of a
learning algorithm.
As models increase in complexity, so does the un-
certainty over parameter estimates. In this regime,
point estimates are unreliable since they do not take
into account the fact that there are different amounts
of uncertainty in the various components of the pa-
rameters. The HDP-PCFG is a Bayesian model
which naturally handles this uncertainty. We present
an efficient variational inference algorithm for the
HDP-PCFG based on a structured mean-field ap-
proximation of the true posterior over parameters.
The algorithm is similar in form to EM and thus in-
herits its simplicity, modularity, and efficiency. Un-
like EM, however, the algorithm is able to take the
uncertainty of parameters into account and thus in-
corporate the DP prior.
Finally, we develop an extension of the HDP-
PCFG for grammar refinement (HDP-PCFG-GR).
Since treebanks generally consist of coarsely-
labeled context-free tree structures, the maximum-
likelihood treebank grammar is typically a poor
model as it makes overly strong independence as-
sumptions. As a result, many generative approaches
to parsing construct refinements of the treebank
grammar which are more suitable for the model-
ing task. Lexical methods split each pre-terminal
symbol into many subsymbols, one for each word,
and then focus on smoothing sparse lexical statis-
688
tics (Collins, 1999; Charniak, 2000). Unlexicalized
methods refine the grammar in a more conservative
fashion, splitting each non-terminal or pre-terminal
symbol into a much smaller number of subsymbols
(Klein and Manning, 2003; Matsuzaki et al, 2005;
Petrov et al, 2006). We apply our HDP-PCFG-GR
model to automatically learn the number of subsym-
bols for each symbol.
2 Models based on Dirichlet processes
At the heart of the HDP-PCFG is the Dirichlet pro-
cess (DP) mixture model (Antoniak, 1974), which is
the nonparametric Bayesian counterpart to the clas-
sical finite mixture model. In order to build up an
understanding of the HDP-PCFG, we first review
the Bayesian treatment of the finite mixture model
(Section 2.1). We then consider the DP mixture
model (Section 2.2) and use it as a building block
for developing nonparametric structured versions of
the HMM (Section 2.3) and PCFG (Section 2.4).
Our presentation highlights the similarities between
these models so that each step along this progression
reflects only the key differences.
2.1 Bayesian finite mixture model
We begin by describing the Bayesian finite mixture
model to establish basic notation that will carry over
the more complex models we consider later.
Bayesian finite mixture model
? ? Dirichlet(?, . . . , ?) [draw component probabilities]
For each component z ? {1, . . . ,K}:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point]
The model has K components whose prior dis-
tribution is specified by ? = (?1, . . . , ?K). The
Dirichlet hyperparameter ? controls how uniform
this distribution is: as ? increases, it becomes in-
creasingly likely that the components have equal
probability. For each mixture component z ?
{1, . . . ,K}, the parameters of the component ?z are
drawn from some prior G0. Given the model param-
eters (?,?), the data points are generated i.i.d. by
first choosing a component and then generating from
a data model F parameterized by that component.
In document clustering, for example, each data
point xi is a document represented by its term-
frequency vector. Each component (cluster) z
has multinomial parameters ?z which specifies a
distribution F (?;?z) over words. It is custom-
ary to use a conjugate Dirichlet prior G0 =
Dirichlet(??, . . . , ??) over the multinomial parame-
ters, which can be interpreted as adding ???1 pseu-
docounts for each word.
2.2 DP mixture model
We now consider the extension of the Bayesian finite
mixture model to a nonparametric Bayesian mixture
model based on the Dirichlet process. We focus
on the stick-breaking representation (Sethuraman,
1994) of the Dirichlet process instead of the stochas-
tic process definition (Ferguson, 1973) or the Chi-
nese restaurant process (Pitman, 2002). The stick-
breaking representation captures the DP prior most
explicitly and allows us to extend the finite mixture
model with minimal changes. Later, it will enable us
to readily define structured models in a form similar
to their classical versions. Furthermore, an efficient
variational inference algorithm can be developed in
this representation (Section 2.6).
The key difference between the Bayesian finite
mixture model and the DP mixture model is that
the latter has a countably infinite number of mixture
components while the former has a predefined K.
Note that if we have an infinite number of mixture
components, it no longer makes sense to consider
a symmetric prior over the component probabilities;
the prior over component probabilities must decay in
some way. The stick-breaking distribution achieves
this as follows. We write ? ? GEM(?) to mean
that ? = (?1, ?2, . . . ) is distributed according to the
stick-breaking distribution. Here, the concentration
parameter ? controls the number of effective com-
ponents. To draw ? ? GEM(?), we first generate
a countably infinite collection of stick-breaking pro-
portions u1, u2, . . . , where each uz ? Beta(1, ?).
The stick-breaking weights ? are then defined in
terms of the stick proportions:
?z = uz
?
z?<z
(1 ? uz?). (1)
The procedure for generating ? can be viewed as
iteratively breaking off remaining portions of a unit-
689
0 1?1 ?2 ?3 ...
Figure 1: A sample ? ? GEM(1).
length stick (Figure 1). The component probabilities
{?z} will decay exponentially in expectation, but
there is always some probability of getting a smaller
component before a larger one. The parameter ? de-
termines the decay of these probabilities: a larger ?
implies a slower decay and thus more components.
Given the component probabilities, the rest of the
DP mixture model is identical to the finite mixture
model:
DP mixture model
? ? GEM(?) [draw component probabilities]
For each component z ? {1, 2, . . . }:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point xn]
2.3 HDP-HMM
The next stop on the way to the HDP-PCFG is the
HDP hidden Markov model (HDP-HMM) (Beal et
al., 2002; Teh et al, 2006). An HMM consists of a
set of hidden states, where each state can be thought
of as a mixture component. The parameters of the
mixture component are the emission and transition
parameters. The main aspect that distinguishes it
from a flat finite mixture model is that the transi-
tion parameters themselves must specify a distribu-
tion over next states. Hence, we have not just one
top-level mixture model over states, but also a col-
lection of mixture models, one for each state.
In developing a nonparametric version of the
HMM in which the number of states is infinite, we
need to ensure that the transition mixture models
of each state share a common inventory of possible
next states. We can achieve this by tying these mix-
ture models together using the hierarchical Dirichlet
process (HDP) (Teh et al, 2006). The stick-breaking
representation of an HDP is defined as follows: first,
the top-level stick-breaking weights ? are drawn ac-
cording to the stick-breaking prior as before. Then,
a new set of stick-breaking weights ?? are generated
according based on ?:
?? ? DP(??,?), (2)
where the distribution of DP can be characterized
in terms of the following finite partition property:
for all partitions of the positive integers into sets
A1, . . . , Am,
(??(A1), . . . ,??(Am)) (3)
? Dirichlet
(
???(A1), . . . , ???(Am)
)
,
where ?(A) =
?
k?A ?k.
1 The resulting ?? is an-
other distribution over the positive integers whose
similarity to ? is controlled by a concentration pa-
rameter ??.
HDP-HMM
? ? GEM(?) [draw top-level state weights]
For each state z ? {1, 2, . . . }:
??Ez ? Dirichlet(?) [draw emission parameters]
??Tz ? DP(?
?, ?) [draw transition parameters]
For each time step i ? {1, . . . , n}:
?xi ? F (?;?Ezi) [emit current observation]
?zi+1 ? Multinomial(?Tzi) [choose next state]
Each state z is associated with emission param-
eters ?Ez . In addition, each z is also associated
with transition parameters ?Tz , which specify a dis-
tribution over next states. These transition parame-
ters are drawn from a DP centered on the top-level
stick-breaking weights ? according to Equations (2)
and (3). Assume that z1 is always fixed to a special
START state, so we do not need to generate it.
2.4 HDP-PCFG
We now present the HDP-PCFG, which is the focus
of this paper. For simplicity, we consider Chomsky
normal form (CNF) grammars, which has two types
of rules: emissions and binary productions. We con-
sider each grammar symbol as a mixture component
whose parameters are the rule probabilities for that
symbol. In general, we do not know the appropriate
number of grammar symbols, so our strategy is to
let the number of grammar symbols be infinite and
place a DP prior over grammar symbols.
1Note that this property is a specific instance of the general
stochastic process definition of Dirichlet processes.
690
HDP-PCFG
? ? GEM(?) [draw top-level symbol weights]
For each grammar symbol z ? {1, 2, . . . }:
??Tz ? Dirichlet(?
T ) [draw rule type parameters]
??Ez ? Dirichlet(?
E) [draw emission parameters]
??Bz ? DP(?
B ,??T ) [draw binary production parameters]
For each node i in the parse tree:
?ti ? Multinomial(?Tzi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Ezi) [emit terminal symbol]
?If ti = BINARY-PRODUCTION:
??(zL(i), zR(i)) ? Multinomial(?
B
zi) [generate children symbols]
?
?Bz
?Tz
?Ez
z ?
z1
z2
x2
z3
x3
T
Parameters Trees
Figure 2: The definition and graphical model of the HDP-PCFG. Since parse trees have unknown structure,
there is no convenient way of representing them in the visual language of traditional graphical models.
Instead, we show a simple fixed example tree. Node 1 has two children, 2 and 3, each of which has one
observed terminal child. We use L(i) and R(i) to denote the left and right children of node i.
In the HMM, the transition parameters of a state
specify a distribution over single next states; simi-
larly, the binary production parameters of a gram-
mar symbol must specify a distribution over pairs
of grammar symbols for its children. We adapt the
HDP machinery to tie these binary production distri-
butions together. The key difference is that now we
must tie distributions over pairs of grammar sym-
bols together via distributions over single grammar
symbols.
Another difference is that in the HMM, at each
time step, both a transition and a emission are made,
whereas in the PCFG either a binary production or
an emission is chosen. Therefore, each grammar
symbol must also have a distribution over the type
of rule to apply. In a CNF PCFG, there are only
two types of rules, but this can be easily generalized
to include unary productions, which we use for our
parsing experiments.
To summarize, the parameters of each grammar
symbol z consists of (1) a distribution over a finite
number of rule types ?Tz , (2) an emission distribu-
tion ?Ez over terminal symbols, and (3) a binary pro-
duction distribution ?Bz over pairs of children gram-
mar symbols. Figure 2 describes the model in detail.
Figure 3 shows the generation of the binary pro-
duction distributions ?Bz . We draw ?
B
z from a DP
centered on ??T , which is the product distribution
over pairs of symbols. The result is a doubly-infinite
matrix where most of the probability mass is con-
state
right child state
left child state
right child state
left child state
? ? GEM(?)
??T
?Bz ? DP(??
T )
Figure 3: The generation of binary production prob-
abilities given the top-level symbol probabilities ?.
First, ? is drawn from the stick-breaking prior, as
in any DP-based model (a). Next, the outer-product
??T is formed, resulting in a doubly-infinite matrix
matrix (b). We use this as the base distribution for
generating the binary production distribution from a
DP centered on ??T (c).
centrated in the upper left, just like the top-level dis-
tribution ??T .
Note that we have replaced the general
691
G0 and F (?Ezi) pair with Dirichlet(?
E) and
Multinomial(?Ezi) to specialize to natural language,
but there is no difficulty in working with parse
trees with arbitrary non-multinomial observations
or more sophisticated word models.
In many natural language applications, there is
a hard distinction between pre-terminal symbols
(those that only emit a word) and non-terminal sym-
bols (those that only rewrite as two non-terminal or
pre-terminal symbols). This can be accomplished
by letting ?T = (0, 0), which forces a draw ?Tz to
assign probability 1 to one rule type.
An alternative definition of an HDP-PCFG would
be as follows: for each symbol z, draw a distribution
over left child symbols lz ? DP(?) and an inde-
pendent distribution over right child symbols rz ?
DP(?). Then define the binary production distribu-
tion as their cross-product ?Bz = lzr
T
z . This also
yields a distribution over symbol pairs and hence de-
fines a different type of nonparametric PCFG. This
model is simpler and does not require any additional
machinery beyond the HDP-HMM. However, the
modeling assumptions imposed by this alternative
are unappealing as they assume the left child and
right child are independent given the parent, which
is certainly not the case in natural language.
2.5 HDP-PCFG for grammar refinement
An important motivation for the HDP-PCFG is that
of refining an existing treebank grammar to alle-
viate unrealistic independence assumptions and to
improve parsing accuracy. In this scenario, the set
of symbols is known, but we do not know how
many subsymbols to allocate per symbol. We in-
troduce the HDP-PCFG for grammar refinement
(HDP-PCFG-GR), an extension of the HDP-PCFG,
for this task.
The essential difference is that now we have a
collection of HDP-PCFG models for each symbol
s ? S, each one operating at the subsymbol level.
While these HDP-PCFGs are independent in the
prior, they are coupled through their interactions in
the parse trees. For completeness, we have also in-
cluded unary productions, which are essentially the
PCFG counterpart of transitions in HMMs. Finally,
since each node i in the parse tree involves a symbol-
subsymbol pair (si, zi), each subsymbol needs to
specify a distribution over both child symbols and
subsymbols. The former can be handled through
a finite Dirichlet distribution since all symbols are
known and observed, but the latter must be handled
with the Dirichlet process machinery, since the num-
ber of subsymbols is unknown.
HDP-PCFG for grammar refinement (HDP-PCFG-GR)
For each symbol s ? S:
??s ? GEM(?) [draw subsymbol weights]
?For each subsymbol z ? {1, 2, . . . }:
???Tsz ? Dirichlet(?
T ) [draw rule type parameters]
???Esz ? Dirichlet(?
E(s)) [draw emission parameters]
???usz ? Dirichlet(?
u) [unary symbol productions]
???bsz ? Dirichlet(?
b) [binary symbol productions]
??For each child symbol s? ? S:
????Uszs? ? DP(?
U ,?s?) [unary subsymbol prod.]
??For each pair of children symbols (s?, s??) ? S ? S:
????Bszs?s?? ? DP(?
B ,?s??
T
s??) [binary subsymbol]
For each node i in the parse tree:
?ti ? Multinomial(?Tsizi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Esizi) [emit terminal symbol]
?If ti = UNARY-PRODUCTION:
??sL(i) ? Multinomial(?
u
sizi) [generate child symbol]
??zL(i) ? Multinomial(?
U
sizisL(i)) [child subsymbol]
?If ti = BINARY-PRODUCTION:
??(sL(i), sR(i)) ? Mult(?sizi) [children symbols]
??(zL(i), zR(i)) ? Mult(?
B
sizisL(i)sR(i)) [subsymbols]
2.6 Variational inference
We present an inference algorithm for the HDP-
PCFG model described in Section 2.4, which can
also be adapted to the HDP-PCFG-GR model with
a bit more bookkeeping. Most previous inference
algorithms for DP-based models involve sampling
(Escobar and West, 1995; Teh et al, 2006). How-
ever, we chose to use variational inference (Blei
and Jordan, 2005), which provides a fast determin-
istic alternative to sampling, hence avoiding issues
of diagnosing convergence and aggregating samples.
Furthermore, our variational inference algorithm es-
tablishes a strong link with past work on PCFG re-
finement and induction, which has traditionally em-
ployed the EM algorithm.
In EM, the E-step involves a dynamic program
that exploits the Markov structure of the parse tree,
and the M-step involves computing ratios based on
expected counts extracted from the E-step. Our vari-
ational algorithm resembles the EM algorithm in
form, but the ratios in the M-step are replaced with
weights that reflect the uncertainty in parameter es-
692
??Bz
?Tz
?Ez
z ?
z1
z2 z3
T
Parameters Trees
Figure 4: We approximate the true posterior p over
parameters ? and latent parse trees z using a struc-
tured mean-field distribution q, in which the distri-
bution over parameters are completely factorized but
the distribution over parse trees is unconstrained.
timates. Because of this procedural similarity, our
method is able to exploit the desirable properties of
EM such as simplicity, modularity, and efficiency.
2.7 Structured mean-field approximation
We denote parameters of the HDP-PCFG as ? =
(?,?), where ? denotes the top-level symbol prob-
abilities and ? denotes the rule probabilities. The
hidden variables of the model are the training parse
trees z. We denote the observed sentences as x.
The goal of Bayesian inference is to compute the
posterior distribution p(?, z | x). The central idea
behind variational inference is to approximate this
intractable posterior with a tractable approximation.
In particular, we want to find the best distribution q?
as defined by
q?
def
= argmin
q?Q
KL(q(?, z)||p(?, z | x)), (4)
where Q is a tractable subset of distributions. We
use a structured mean-field approximation, meaning
that we only consider distributions that factorize as
follows (Figure 4):
Q
def
=
{
q(z)q(?)
K?
z=1
q(?Tz )q(?
E
z )q(?
B
z )
}
. (5)
We further restrict q(?Tz ), q(?
E
z ), q(?
B
z ) to be
Dirichlet distributions, but allow q(z) to be any
multinomial distribution. We constrain q(?) to be a
degenerate distribution truncated at K; i.e., ?z = 0
for z > K. While the posterior grammar does have
an infinite number of symbols, the exponential de-
cay of the DP prior ensures that most of the proba-
bility mass is contained in the first few symbols (Ish-
waran and James, 2001).2 While our variational ap-
proximation q is truncated, the actual PCFG model
is not. AsK increases, our approximation improves.
2.8 Coordinate-wise ascent
The optimization problem defined by Equation (4)
is intractable and nonconvex, but we can use a sim-
ple coordinate-ascent algorithm that iteratively op-
timizes each factor of q in turn while holding the
others fixed. The algorithm turns out to be similar in
form to EM for an ordinary PCFG: optimizing q(z)
is the analogue of the E-step, and optimizing q(?)
is the analogue of the M-step; however, optimizing
q(?) has no analogue in EM. We summarize each
of these updates below (see (Liang et al, 2007) for
complete derivations).
Parse trees q(z): The distribution over parse trees
q(z) can be summarized by the expected suffi-
cient statistics (rule counts), which we denote as
C(z ? zl zr) for binary productions and C(z ?
x) for emissions. We can compute these expected
counts using dynamic programming as in the E-step
of EM.
While the classical E-step uses the current rule
probabilities ?, our mean-field approximation in-
volves an entire distribution q(?). Fortunately, we
can still handle this case by replacing each rule prob-
ability with a weight that summarizes the uncer-
tainty over the rule probability as represented by q.
We define this weight in the sequel.
It is a common perception that Bayesian inference
is slow because one needs to compute integrals. Our
mean-field inference algorithm is a counterexample:
because we can represent uncertainty over rule prob-
abilities with single numbers, much of the existing
PCFG machinery based on EM can be modularly
imported into the Bayesian framework.
Rule probabilities q(?): For an ordinary PCFG,
the M-step simply involves taking ratios of expected
2In particular, the variational distance between the stick-
breaking distribution and the truncated version decreases expo-
nentially as the truncation level K increases.
693
counts:
?Bz (zl, zr) =
C(z ? zl zr)
C(z ? ??)
. (6)
For the variational HDP-PCFG, the optimal q(?) is
given by the standard posterior update for Dirichlet
distributions:3
q(?Bz ) = Dirichlet(?
B
z ;?
B??T + ~C(z)), (7)
where ~C(z) is the matrix of counts of rules with left-
hand side z. These distributions can then be summa-
rized with multinomial weights which are the only
necessary quantities for updating q(z) in the next it-
eration:
WBz (zl, zr)
def
= expEq[log?Bz (zl, zr)] (8)
=
e?(C(z?zl zr)+?
B?zl?zr )
e?(C(z???)+?B)
, (9)
where ?(?) is the digamma function. The emission
parameters can be defined similarly. Inspection of
Equations (6) and (9) reveals that the only difference
between the maximum likelihood and the mean-field
update is that the latter applies the exp(?(?)) func-
tion to the counts (Figure 5).
When the truncation K is large, ?B?zl?zr is near
0 for most right-hand sides (zl, zr), so exp(?(?)) has
the effect of downweighting counts. Since this sub-
traction affects large counts more than small counts,
there is a rich-get-richer effect: rules that have al-
ready have large counts will be preferred.
Specifically, consider a set of rules with the same
left-hand side. The weights for all these rules only
differ in the numerator (Equation (9)), so applying
exp(?(?)) creates a local preference for right-hand
sides with larger counts. Also note that the rule
weights are not normalized; they always sum to at
most one and are equal to one exactly when q(?) is
degenerate. This lack of normalization gives an ex-
tra degree of freedom not present in maximum like-
lihood estimation: it creates a global preference for
left-hand sides that have larger total counts.
Top-level symbol probabilities q(?): Recall that
we restrict q(?) = ???(?), so optimizing ? is
equivalent to finding a single best ??. Unlike q(?)
3Because we have truncated the top-level symbol weights,
the DP prior on ?Bz reduces to a finite Dirichlet distribution.
 
0
 
0.5 1
 
1.5 2  0
 
0.5
 
1
 
1.5
 
2
x
exp(?(x
)) x
Figure 5: The exp(?(?)) function, which is used in
computing the multinomial weights for mean-field
inference. It has the effect of reducing a larger frac-
tion of small counts than large counts.
and q(z), there is no closed form expression for
the optimal ??, and the objective function (Equa-
tion (4)) is not convex in ??. Nonetheless, we can
apply a standard gradient projection method (Bert-
sekas, 1999) to improve ?? to a local maxima.
The part of the objective function in Equation (4)
that depends on ?? is as follows:
L(??) = logGEM(??;?)+ (10)
K?
z=1
Eq[logDirichlet(?Bz ;?
B????T )]
See Liang et al (2007) for the derivation of the gra-
dient. In practice, this optimization has very little ef-
fect on performance. We suspect that this is because
the objective function is dominated by p(x | z) and
p(z | ?), while the contribution of p(? | ?) is mi-
nor.
3 Experiments
We now present an empirical evaluation of the HDP-
PCFG(-GR) model and variational inference tech-
niques. We first give an illustrative example of the
ability of the HDP-PCFG to recover a known gram-
mar and then present the results of experiments on
large-scale treebank parsing.
3.1 Recovering a synthetic grammar
In this section, we show that the HDP-PCFG-GR
can recover a simple grammar while a standard
694
S ? X1X1 | X2X2 | X3X3 | X4X4
X1 ? a1 | b1 | c1 | d1
X2 ? a2 | b2 | c2 | d2
X3 ? a3 | b3 | c3 | d3
X4 ? a4 | b4 | c4 | d4
S
Xi Xi
{ai, bi, ci, di} {ai, bi, ci, di}
(a) (b)
Figure 6: (a) A synthetic grammar with a uniform
distribution over rules. (b) The grammar generates
trees of the form shown on the right.
PCFG fails to do so because it has no built-in con-
trol over grammar complexity. From the grammar in
Figure 6, we generated 2000 trees. The two terminal
symbols always have the same subscript, but we col-
lapsed Xi to X in the training data. We trained the
HDP-PCFG-GR, with truncation K = 20, for both
S and X for 100 iterations. We set al hyperparame-
ters to 1.
Figure 7 shows that the HDP-PCFG-GR recovers
the original grammar, which contains only 4 sub-
symbols, leaving the other 16 subsymbols unused.
The standard PCFG allocates all the subsymbols to
fit the exact co-occurrence statistics of left and right
terminals.
Recall that a rule weight, as defined in Equa-
tion (9), is analogous to a rule probability for stan-
dard PCFGs. We say a rule is effective if its weight
is at least 10?6 and its left hand-side has posterior
is also at least 10?6. In general, rules with weight
smaller than 10?6 can be safely pruned without af-
fect parsing accuracy. The standard PCFG uses all
20 subsymbols of both S and X to explain the data,
resulting in 8320 effective rules; in contrast, the
HDP-PCFG uses only 4 subsymbols for X and 1 for
S, resulting in only 68 effective rules. If the thresh-
old is relaxed from 10?6 to 10?3, then only 20 rules
are effective, which corresponds exactly to the true
grammar.
3.2 Parsing the Penn Treebank
In this section, we show that our variational HDP-
PCFG can scale up to real-world data sets. We ran
experiments on the Wall Street Journal (WSJ) por-
tion of the Penn Treebank. We trained on sections
2?21, used section 24 for tuning hyperparameters,
and tested on section 22.
We binarize the trees in the treebank as follows:
for each non-terminal node with symbol X , we in-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
standard PCFG HDP-PCFG
Figure 7: The posteriors over the subsymbols of the
standard PCFG is roughly uniform, whereas the pos-
teriors of the HDP-PCFG is concentrated on four
subsymbols, which is the true number of symbols
in the grammar.
troduce a right-branching cascade of new nodes with
symbol X . The end result is that each node has at
most two children. To cope with unknown words,
we replace any word appearing fewer than 5 times
in the training set with one of 50 unknown word to-
kens derived from 10 word-form features.
Our goal is to learn a refined grammar, where each
symbol in the training set is split into K subsym-
bols. We compare an ordinary PCFG estimated with
maximum likelihood (Matsuzaki et al, 2005) and
the HDP-PCFG estimated using the variational in-
ference algorithm described in Section 2.6.
To parse new sentences with a grammar, we com-
pute the posterior distribution over rules at each span
and extract the tree with the maximum expected cor-
rect number of rules (Petrov and Klein, 2007).
3.2.1 Hyperparameters
There are six hyperparameters in the HDP-PCFG-
GR model, which we set in the following manner:
? = 1, ?T = 1 (uniform distribution over unar-
ies versus binaries), ?E = 1 (uniform distribution
over terminal words), ?u(s) = ?b(s) = 1N(s) , where
N(s) is the number of different unary (binary) right-
hand sides of rules with left-hand side s in the tree-
bank grammar. The two most important hyperpa-
rameters are ?U and ?B , which govern the sparsity
of the right-hand side for unary and binary rules.
We set ?U = ?B although more performance could
probably be gained by tuning these individually. It
turns out that there is not a single ?B that works for
all truncation levels, as shown in Table 1.
If the top-level distribution ? is uniform, the value
of ?B corresponding to a uniform prior over pairs of
children subsymbols is K2. Interestingly, the opti-
mal ?B appears to be superlinear but subquadratic
695
truncation K 2 4 8 12 16 20
best ?B 16 12 20 28 48 80
uniform ?B 4 16 64 144 256 400
Table 1: For each truncation level, we report the ?B
that yielded the highest F1 score on the development
set.
K PCFG PCFG (smoothed) HDP-PCFG
F1 Size F1 Size F1 Size
1 60.47 2558 60.36 2597 60.5 2557
2 69.53 3788 69.38 4614 71.08 4264
4 75.98 3141 77.11 12436 77.17 9710
8 74.32 4262 79.26 120598 79.15 50629
12 70.99 7297 78.8 160403 78.94 86386
16 66.99 19616 79.2 261444 78.24 131377
20 64.44 27593 79.27 369699 77.81 202767
Table 2: Shows development F1 and grammar sizes
(the number of effective rules) as we increase the
truncation K.
in K. We used these values of ?B in the following
experiments.
3.2.2 Results
The regime in which Bayesian inference is most
important is when training data is scarce relative to
the complexity of the model. We train on just sec-
tion 2 of the Penn Treebank. Table 2 shows how
the HDP-PCFG-GR can produce compact grammars
that guard against overfitting. Without smoothing,
ordinary PCFGs trained using EM improve as K in-
creases but start to overfit around K = 4. Simple
add-1.01 smoothing prevents overfitting but at the
cost of a sharp increase in grammar sizes. The HDP-
PCFG obtains comparable performance with a much
smaller number of rules.
We also trained on sections 2?21 to demon-
strate that our methods can scale up and achieve
broadly comparable results to existing state-of-the-
art parsers. When using a truncation level of K =
16, the standard PCFG with smoothing obtains an
F1 score of 88.36 using 706157 effective rules while
the HDP-PCFG-GR obtains an F1 score of 87.08 us-
ing 428375 effective rules. We expect to see greater
benefits from the HDP-PCFG with a larger trunca-
tion level.
4 Related work
The question of how to select the appropriate gram-
mar complexity has been studied in earlier work.
It is well known that more complex models nec-
essarily have higher likelihood and thus a penalty
must be imposed for more complex grammars. Ex-
amples of such penalized likelihood procedures in-
clude Stolcke and Omohundro (1994), which used
an asymptotic Bayesian model selection criterion
and Petrov et al (2006), which used a split-merge
algorithm which procedurally determines when to
switch between grammars of various complexities.
These techniques are model selection techniques
that use heuristics to choose among competing sta-
tistical models; in contrast, the HDP-PCFG relies on
the Bayesian formalism to provide implicit control
over model complexity within the framework of a
single probabilistic model.
Johnson et al (2006) also explored nonparamet-
ric grammars, but they do not give an inference al-
gorithm for recursive grammars, e.g., grammars in-
cluding rules of the form A ? BC and B ? DA.
Recursion is a crucial aspect of PCFGs and our
inference algorithm does handle it. Finkel et al
(2007) independently developed another nonpara-
metric model of grammars. Though their model is
also based on hierarchical Dirichlet processes and is
similar to ours, they present a different inference al-
gorithm which is based on sampling. Kurihara and
Sato (2004) and Kurihara and Sato (2006) applied
variational inference to PCFGs. Their algorithm is
similar to ours, but they did not consider nonpara-
metric models.
5 Conclusion
We have presented the HDP-PCFG, a nonparametric
Bayesian model for PCFGs, along with an efficient
variational inference algorithm. While our primary
contribution is the elucidation of the model and algo-
rithm, we have also explored some important empir-
ical properties of the HDP-PCFG and also demon-
strated the potential of variational HDP-PCFGs on a
full-scale parsing task.
696
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics, 2:1152?1174.
M. Beal, Z. Ghahramani, and C. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural
Information Processing Systems (NIPS), pages 577?
584.
D. Bertsekas. 1999. Nonlinear programming.
D. Blei and M. I. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Bayesian Analysis, 1:121?
144.
E. Charniak. 1996. Tree-bank grammars. In Association
for the Advancement of Artificial Intelligence (AAAI).
E. Charniak. 2000. A maximum-entropy-inspired parser.
In North American Association for Computational
Linguistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. D. Escobar and M. West. 1995. Bayesian density
estimation and inference using mixtures. Journal of
the American Statistical Association, 90:577?588.
T. S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Association for Computational Lin-
guistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
H. Ishwaran and L. F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96:161?173.
M. Johnson, T. Griffiths, and S. Goldwater. 2006. Adap-
tor grammars: A framework for specifying composi-
tional nonparametric Bayesian models. In Advances
in Neural Information Processing Systems (NIPS).
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Association for Computational Linguistics
(ACL), pages 423?430.
K. Kurihara and T. Sato. 2004. An application of the
variational Bayesian approach to probabilistic context-
free grammars. In International Joint Conference on
Natural Language Processing Workshop Beyond Shal-
low Analyses.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007. Nonparametric PCFGs using Dirichlet pro-
cesses. Technical report, Department of Statistics,
University of California at Berkeley.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Association for
Computational Linguistics (ACL).
S. Petrov and D. Klein. 2007. Learning and inference
for hierarchically split PCFGs. In Human Language
Technology and North American Association for Com-
putational Linguistics (HLT/NAACL).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL).
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications.
Y. W. Teh, M. I. Jordan, M. Beal, and D. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
697
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 887?896, Prague, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Diachronic Phonology
Alexandre Bouchard-Co?te?? Percy Liang? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present a probabilistic model of di-
achronic phonology in which individual
word forms undergo stochastic edits along
the branches of a phylogenetic tree. Our ap-
proach allows us to achieve three goals with
a single unified model: (1) reconstruction
of both ancient and modern word forms, (2)
discovery of general phonological changes,
and (3) selection among different phyloge-
nies. We learn our model using a Monte
Carlo EM algorithm and present quantitative
results validating the model.
1 Introduction
Modeling how languages change phonologically
over time (diachronic phonology) is a central topic
in historical linguistics (Campbell, 1998). The ques-
tions involved range from reconstruction of ancient
word forms, to the elucidation of phonological drift
processes, to the determination of phylogenetic re-
lationships between languages. However, this prob-
lem has received relatively little attention from the
computational community. What work there is has
focused on the reconstruction of phylogenies on the
basis of a Boolean matrix indicating the properties
of words in different languages (Gray and Atkinson,
2003; Evans et al, 2004; Ringe et al, 2002; Nakhleh
et al, 2005).
In this paper, we present a novel framework, along
with a concrete model and experiments, for the prob-
abilistic modeling of diachronic phonology. We fo-
cus on the case where the words are etymological
cognates across languages, e.g. French faire and
Spanish hacer from Latin facere (to do). Given
this information as input, we learn a model acting
at the level of individual phoneme sequences, which
can be used for reconstruction and prediction, Our
model is fully generative, and can be used to reason
about a variety of types of information. For exam-
ple, we can observe a word in one or more modern
languages, say French and Spanish, and query the
corresponding word form in another language, say
Italian. This kind of lexicon-filling has applications
in machine translation. Alternatively, we can also
reconstruct ancestral word forms or inspect the rules
learned along each branch of a phylogeny to identify
salient patterns. Finally, the model can be used as a
building block in a system for inferring the topology
of phylogenetic trees. We discuss all of these cases
further in Section 4.
The contributions of this paper are threefold.
First, the approach to modeling language change at
the phoneme sequence level is new, as is the spe-
cific model we present. Second, we compiled a new
corpus1 and developed a methodology for quantita-
tively evaluating such approaches. Finally, we de-
scribe an efficient inference algorithm for our model
and empirically study its performance.
1.1 Previous work
While our word-level model of phonological change
is new, there have been several computational inves-
tigations into diachronic linguistics which are rele-
vant to the present work.
The task of reconstructing phylogenetic trees
1nlp.cs.berkeley.edu/pages/historical.html
887
for languages has been studied by several authors.
These approaches descend from glottochronology
(Swadesh, 1955), which views a language as a col-
lection of shared cognates but ignores the structure
of those cognates. This information is obtained from
manually curated cognate lists such as the data of
Dyen et al (1997).
As an example of a cognate set encoding, consider
the meaning ?eat?. There would be one column for
the cognate set which appears in French as manger
and Italian as mangiare since both descend from the
Latin mandere (to chew). There would be another
column for the cognate set which appears in both
Spanish and Portuguese as comer, descending from
the Latin comedere (to consume). If this were the
only data, algorithms based on this data would tend
to conclude that French and Italian were closely re-
lated and that Spanish and Portuguese were equally
related. However, the cognate set representation has
several disadvantages: it does not capture the fact
that the cognate is closer between Spanish and Por-
tuguese than between French and Spanish, nor do
the resulting models let us conclude anything about
the regular processes which caused these languages
to diverge. Also, the existing cognate data has been
curated at a relatively high cost. In our work, we
track each word using an automatically obtained
cognate list. While our cognates may be noisier,
we compensate by modeling phonological changes
rather than boolean mutations in cognate sets.
There has been other computational work in this
broad domain. Venkataraman et al (1997) describe
an information theoretic measure of the distance be-
tween two dialects of Chinese. Like our approach,
they use a probabilistic edit model as a formaliza-
tion of the phonological process. However, they do
not consider the question of reconstruction or infer-
ence in multi-node phylogenies, nor do they present
a learning algorithm for such models.
Finally, for the specific application of cog-
nate prediction in machine translation, essentially
transliteration, there have been several approaches,
including Kondrak (2002). However, the phenom-
ena of interest, and therefore the models, are ex-
tremely different. Kondrak (2002) presents a model
for learning ?sound laws,? general phonological
changes governing two completely observed aligned
cognate lists. His model can be viewed as a special
la
es it
la
vl
ib
es pt
it
la
it pt
es
la
it es
pt
Topology 1 Topology 2 *Topology 3 *Topology 4
Figure 1: Tree topologies used in our experiments. *Topology
3 and *Topology 4 are incorrect evolutionary tree used for our
experiments on the selection of phylogenies (Section 4.4).
case of ours using a simple two-node topology.
There is also a rich literature (Huelsenbeck et al,
2001) on the related problems of evolutionary biol-
ogy. A good reference on the subject is Felsenstein
(2003). In particular, Yang and Rannala (1997), Mau
and Newton (1997) and Li et al (2000) each inde-
pendently presented a Bayesian model for comput-
ing posteriors over evolutionary trees. A key dif-
ference with our model is that independence across
evolutionary sites is assumed in their work, while
the evolution of the phonemes in our model depends
on the environment in which the change occurs.
2 A model of phonological change
Assume we have a fixed set of word types (cog-
nate sets) in our vocabulary V and a set of languages
L. Each word type i has a word form wil in each lan-
guage l ? L, which is represented as a sequence of
phonemes and might or might not be observed. The
languages are arranged according to some tree topol-
ogy T (see Figure 1 for examples). One might con-
sider models that simultaneously induce the topol-
ogy and cognate set assignments, but let us fix both
for now. We discuss one way to relax this assump-
tion and present experimental results in Section 4.4.
Our generative model (Figure 3) specifies a dis-
tribution over the word forms {wil} for each word
type i ? V and each language l ? L. The genera-
tive process starts at the root language and generates
all the word forms in each language in a top-down
manner. One appealing aspect about our model is
that, at a high-level, it reflects the actual phonolog-
ical process that languages undergo. However, im-
portant phenomena like lexical drift, borrowing, and
other non-phonological changes are not modeled.
888
Our generative model can be summarized as fol-
lows:
For each word i ? V :
?wiROOT ? LanguageModel
For each branch (k ? l) ? T :
??k?l ? Dirichlet(?) [choose edit params.]
?For each word i ? V :
??wil ? Edit(wik, ?k?l) [sample word form]
In the remainder of this section, we describe each
of the steps in the model.
2.1 Language model
For the distributionw ? LanguageModel, we used a
simple bigram phoneme model. The phonemes were
partitioned into natural classes (see Section 4 for de-
tails). A root word form consisting of n phonemes
x1 ? ? ?xn is generated with probability
plm(x1)
n?
j=2
plm(xj | NaturalClass(xj?1)),
where plm is the distribution of the language model.
2.2 Edit model
The stochastic edit model y ? Edit(x, ?) describes
how a single old word form x = x1 ? ? ?xn changes
along one branch of the phylogeny with parameters
? to produce a new word form y. This process is
parameterized by rule probabilities ?k?l, which are
specific to branch (k ? l).
The generative process is as follows: for each
phoneme xi in the old word form, walking from
left to right, choose a rule to apply. There are
three types of rules: (1) deletion of the phoneme,
(2) substitution with another phoneme (possibly the
same one), or (3) insertion of another phoneme, ei-
ther before or after the existing one. The prob-
ability of applying a rule depends on a context
(NaturalClass(xi?1),NaturalClass(xi+1)). Figure 2
illustrates the edits on an example. The context-
dependence allows us to represent phenomena such
as the fact that s is likely to be deleted only in word-
final contexts.
The edit model we have presented approximately
encodes a limited form of classic rewrite-driven seg-
mental phonology (Chomsky and Halle, 1968). One
# C V C V C #
# f o k u s #
# f w O k o #
# C V V C V #
f ? f / # Vo ? w O / C Ck ? k / V Vu ? o / C Cs ? / V #
Edits applied Rules used
Figure 2: An example of edits that were used to transform
the Latin word FOCUS (/fokus/) into the Italian word fuoco
(/fwOko/) (fire) along with the context-specific rules that were
applied.
could imagine basing our model on more modern
phonological theory, but the computational proper-
ties of the edit model are compelling, and it is ade-
quate for many kinds of phonological change.
In addition to simple edits, we can model some
classical changes that appear to be too complex to be
captured by a single left-to-right edit model of this
kind. For instance, bleeding and feeding arrange-
ments occur when one phonological change intro-
duces a new context, which triggers another phono-
logical change, but the two cannot occur simultane-
ously. For example, vowel raising e ? i / c might
be needed before palatalization t ? c / i. Instead
of capturing such an interaction directly, we can
break up a branch into two segments joined at an in-
termediate language node, conflating the concept of
historically intermediate languages with the concept
of intermediate stages in the application of sequen-
tial rules.
However, many complex processes are not well-
represented by our basic model. One problem-
atic case is chained shifts such as Grimm?s law in
Proto-Germanic or the Great Vowel Shift in English.
To model such dependent rules, we would need
to use a more complex prior distributions over the
edit parameters. Another difficult case is prosodic
changes, such as unstressed vowel neutralizations,
which would require a representation of supraseg-
mental features. While our basic model does not
account for these phenomena, extensions within the
generative framework could capture such richness.
3 Learning and inference
We use a Monte Carlo EM algorithm to fit the pa-
rameters of our model. The algorithm iterates be-
tween a stochastic E-step, which computes recon-
889
...
wiA
wiB
wiC wiD
... ...word type i = 1 . . . |V |
eiA?B?A?B
eiB?C?B?C eiB?D ?B?D
Figure 3: The graphical model representation of our model: ?
are the parameters specifying the stochastic edits e, which gov-
ern how the words w evolve. The plate notation indicates the
replication of the nodes corresponding to the evolving words.
structions based on the current edit parameters, and
an M-step, which updates the edit parameters based
on the reconstructions.
3.1 Monte Carlo E-step: sampling the edits
The E-step needs to produce expected counts of how
many times each edit (such as o ? O) was used in
each context. An exact E-step would require sum-
ming over all possible edits involving all languages
in the phylogeny (all unobserved {e}, {w} variables
in Figure 3). Unfortunately, unlike in the case of
HMMs and PCFGs, our model permits no tractable
dynamic program to compute these counts exactly.
Therefore, we resort to a Monte Carlo E-step,
where many samples of the edit variables are col-
lected, and counts are computed based on these sam-
ples. Samples are drawn using Gibbs sampling (Ge-
man and Geman, 1984): for each word form of a
particular language wil, we fix all other variables in
the model and sample wil along with its correspond-
ing edits.
In the E-step, we fix the parameters, which ren-
ders the word types conditionally independent, just
as in an HMM. Therefore, we can process each word
type in turn without approximation.
First consider the simple 4-language topology in
Figure 3. Suppose that the words in languages A,
C and D are fixed, and we wish to infer the word
at language B along with the three corresponding
sets of edits (remember the edits fully determine the
words). There are an exponential number of possi-
ble words/edits, but it turns out that we can exploit
theMarkov structure in the edit model to consider all
such words/edits using dynamic programming, in a
way broadly similar to the forward-backward algo-
rithm for HMMs.
Figure 4 shows the lattice for the dynamic pro-
gram. Each path connecting the two shaded end-
point states represents a particular word form for
language B and a corresponding set of edits. Each
node in the lattice is a state of the dynamic pro-
gram, which is a 5-tuple (iA, iC , iD, c1, c2), where
iA, iC and iD are the cursor positions (represented
by dots in Figure 4) in each of the word forms of
A,C and D, respectively; c1 is the natural class of
the phoneme in the word form for B that was last
generated; and c2 corresponds to the phoneme that
will be generated next.
Each state transition involves applying a rule
to A?s current phoneme (which produces 0?2
phonemes in B) and applying rules to B?s new 0?2
phonemes. There are three types of rules (deletion,
substitution, insertion), resulting in 30+32+34 = 91
types of state transitions. For illustration, Figure 4
shows the simpler case where B only has one child
C. Given these rules, the new state is computed by
advancing the appropriate cursors and updating the
natural classes c1 and c2. The weight of each tran-
sition w(s ? t) is a product of the language model
probability and the rule probabilities that were cho-
sen.
For each state s, the dynamic program computes
W (s), the sum of the weights of all paths leaving s,
W (s) =
?
s?t
w(s ? t)W (t).
To sample a path, we start at the leftmost state,
choose the transition with probability proportional
to its contribution in the sum for computing W (s),
and repeat until we reach the rightmost state.
We applied a few approximations to speed up the
sampling of words, which reduced the running time
by several orders of magnitude. For example, we
pruned rules with low probability and restricted the
890
An example of a dynamic programming lattice
...
...
... ... ... ... ... ... ...
...
patr ? ia
# C V C C# p a t r ? V #a #
patr ? ja
x [T1] p1ed(i ? /C V) x
x [T3] plm(j | C) p1ed(i ? j/C V) p2ed(j ? j/C V) x
x [T11] plm(j | C) plm(i | C) p1ed(i ? j i/C V) p2ed(j ? j/C V) p2ed(i ? /C V) x
. . .
patri ? a
# C V C C# p a t r ? V #a #
patr ? ja
patri ? a
# C V C C C# p a t r j ? V #a #
patrj ? a
patri ? a
# C V C C C V# p a t r j i ? V #a #
patrj ? a
. . .
Types of state transitions (x: ancient phoneme, y: intermediate, z: modern)
x
y
x
y
z
x
y
z
x
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z z
y
z
x
y
z z
y
z
x
y
z z
y
z z[T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] [T11] [T12] [T13]
Figure 4: The dynamic program involved in sampling an intermediate word form given one ancient and one modern word form.
One lattice node is expanded to show the dynamic program state (represented by the part not grayed out) and three of the many
possible transitions leaving the state. Each transition is labeled with the weight of the transition, which is the product of the relevant
model probabilities. At the bottom, the 13 types of state transitions are shown.
state space of the dynamic program by limiting the
deviation in cursor positions.
3.2 M-step: updating the parameters
The M-step is standard once we have computed
the expected counts of edits in the E-step. For
each branch (k ? l) ? T in the phylogeny,
we compute the maximum likelihood estimate
of the edit parameters {?k?l(x ? ? / c1 c2)}.
For example, the parameter corresponding to
x = /e/, ? = /e s/, c1 = ALVEOLAR, c2 = # is
the probability of inserting a final /s/ after an /e/
which is itself preceded by an alveolar phoneme.
The probability of each rule is estimated as follows:
?k?l(x ? ? / c1 c2) =
#(x ? ? / c1 c2) + ?(x ? ? / c1 c2)? 1?
?? #(x ? ?? / c1 c2) + ?(x ? ?? / c1 c2)? 1
,
where ? is the concentration hyperparameter of the
Dirichlet prior. The value ? ? 1 can be interpreted
as the number of pseudocounts for a rule.
4 Experiments
In this section we show the results of our experi-
ments with our model. The experimental conditions
are summarized in Table 1, with additional informa-
Experiment Topology Heldout
Latin reconstruction (4.2) 1 la:293
Italian reconstruction (4.2) 1 it:117
Sound changes (4.3) 2 None
Phylogeny selection (4.4) 2, 3, 4 None
Table 1: Conditions under which each of the experiments pre-
sented in this section were performed. The topology indices
correspond to those displayed in Figure 1. Note that by condi-
tional independence, the topology used for Spanish reconstruc-
tion reduces to a chain. The heldout column indicates howmany
words, if any, were heldout for edit distance evaluation, and
from which language.
tion on the specifics of the experiments presented in
Section 4.5. We start with a description of the corpus
we created for these experiments.
4.1 Corpus
In order to train and evaluate our system, we
compiled a corpus of Romance cognate words.
The raw data was taken from three sources: the
wiktionary.org website, a Bible parallel cor-
pus (Resnik et al, 1999) and the Europarl corpus
(Koehn, 2002). From an XML dump of the Wik-
tionary data, we extracted multilingual translations,
which provide a list of word tuples in a large num-
ber of languages, including a few ancient languages.
891
The Europarl and the biblical data were processed
and aligned in the standard way, using combined
GIZA++ alignments (Och and Ney, 2003).
We performed our experiments with four lan-
guages from the Romance family (Latin, Italian,
Spanish, and Portuguese). For each of these lan-
guages, we used a simple in-house rule-based sys-
tem to convert the words into their IPA represen-
tations.2 After augmenting our alignments with
the transitive closure3 of the Europarl, Bible and
Wiktionary data, we filtered out non-cognate words
by thresholding the ratio of edit distance to word
length.4 The preprocessing is constraining in that we
require that all the elements of a tuple to be cognates,
which leaves out a significant portion of the data be-
hind (see the row Full entries in Table 2). However,
our approach relies on this assumption, as there is no
explicit model of non-cognate words. An interest-
ing direction for future work is the joint modeling of
phonology with the determination of the cognates,
but our simpler setting lets us focus on the proper-
ties of the edit model. Moreover, the restriction to
full entries has the side advantage that the Latin bot-
tleneck prevents the introduction of too many neol-
ogisms, which are numerous in the Europarl data, to
the final corpus.
Since we used automatic tools for preparing our
corpus rather than careful linguistic analysis, our
cognate list is much noiser in terms of the pres-
ence of borrowed words and phonemeic transcrip-
tion errors compared to the ones used by previous
approaches (Swadesh, 1955; Dyen et al, 1997). The
benefit of our mechanical preprocessing is that more
cognate data can easily be made available, allowing
us to effectively train richer models. We show in the
rest of this section that our phonological model can
indeed overcome this noise and recover meaningful
patterns from the data.
2The tool and the rules we used are available at
nlp.cs.berkeley.edu/pages/historical.html.
3For example, we would infer from an la-es bible align-
ment confessionem-confesio?n (confession) and an es-it Eu-
roparl alignment confesio?n-confessione that the Latin word con-
fessionem and the Italian word confessione are related.
4To be more precise we keep a tuple (w1, w2, . . . , wp) iff
d(wi,wj)
l?(wi,wj)
? 0.7 for all i, j ? {1, 2, . . . , p}, where l? is the mean
length
|wi|+|wj |
2 and d is the Levenshtein distance.
Name Languages Tuples Word forms
Raw sources of data used to create the corpus
Wiktionary es,pt,la,it 5840 11724
Bible la,es 2391 4782
Europarl es,pt 36905 73773
it,es 39506 78982
Main stages of preprocessing of the corpus
Closure es,pt,la,it 40944 106090
Cognates es,pt,la,it 27996 69637
Full entries es,pt,la,it 586 2344
Table 2: Statistics of the dataset we compiled for the evaluation
of our model. We show the languages represented, the number
of tuples and the number of word forms found in each of the
source of data and pre-processing steps involved in the creation
of the dataset we used to test our model. By full entry, we mean
the number of tuples that are jointly considered cognate by our
preprocessing system and that have a word form known for each
of the languages of interest. These last row forms the dataset
used for our experiments.
Language Baseline Model Improvement
Latin 2.84 2.34 9%
Spanish 3.59 3.21 11%
Table 3: Results of the edit distance experiment. The language
column corresponds to the language held-out for evaluation. We
show the mean edit distance across the evaluation examples.
4.2 Reconstruction of word forms
We ran the system using Topology 1 in Figure 1 to
demonstrate the the system can propose reasonable
reconstructions of Latin word forms on the basis of
modern observations. Half of the Latin words at the
root of the tree were held out, and the (uniform cost)
Levenshtein edit distance from the predicted recon-
struction to the truth was computed. Our baseline is
to pick randomly, for each heldout node in the tree,
an observed neighboring word (i.e. copy one of the
modern forms). We stopped EM after 15 iterations,
and reported the result on a Viterbi derivation using
the parameters obtained. Our model outperformed
this baseline by a 9% relative reduction in average
edit distance. Similarly, reconstruction of modern
forms was also demonstrated, with an improvement
of 11% (see Table 3).
To give a qualitative feel for the operation of the
system (good and bad), consider the example in Fig-
ure 5, taken from this experiment. The Latin dentis
/dEntis/ (teeth) is nearly correctly reconstructed as
/dEntes/, reconciling the appearance of the /j/ in the
892
/dEntis/
/djEntes/ /dEnti/
i ? E
E? j E s ?
Figure 5: An example of a Latin reconstruction given the Span-
ish and Italian word forms.
Spanish and the disappearance of the final /s/ in the
Italian. Note that the /is/ vs. /es/ ending is difficult
to predict in this context (indeed, it was one of the
early distinctions to be eroded in vulgar Latin).
While the uniform-cost edit distance misses im-
portant aspects of phonology (all phoneme substitu-
tions are not equal, for instance), it is parameter-free
and still seems to correlate to a large extent with lin-
guistic quality of reconstruction. It is also superior
to held-out log-likelihood, which fails to penalize er-
rors in the modeling assumptions, and to measuring
the percentage of perfect reconstructions, which ig-
nores the degree of correctness of each reconstructed
word.
4.3 Inference of phonological changes
Another use of our model is to automatically recover
the phonological drift processes between known or
partially known languages. To facilitate evaluation,
we continued in the well-studied Romance evolu-
tionary tree. Again, the root is Latin, but we now add
an additional modern language, Portuguese, and two
additional hidden nodes. One of the nodes charac-
terizes the least common ancestor of modern Span-
ish and Portuguese; the other, the least common an-
cestor of all three modern languages. In Figure 1,
Topology 2, these two nodes are labelled vl (Vulgar
Latin) and ib (Proto-Ibero Romance) respectively.
Since we are omitting many other branches, these
names should not be understood as referring to ac-
tual historical proto-languages, but, at best, to col-
lapsed points representing several centuries of evo-
lution. Nonetheless, the major reconstructed rules
still correspond to well known phenomena and the
learned model generally places them on reasonable
branches.
Figure 6 shows the top four general rules for
each of the evolutionary branches in this experiment,
ranked by the number of times they were used in the
derivations during the last iteration of EM. The la,
es, pt, and it forms are fully observed while the
vl and ib forms are automatically reconstructed.
Figure 6 also shows a specific example of the evolu-
tion of the Latin VERBUM (word/verb), along with
the specific edits employed by the model.
While quantitative evaluation such as measuring
edit distance is helpful for comparing results, it is
also illuminating to consider the plausibility of the
learned parameters in a historical light, which we
do here briefly. In particular, we consider rules on
the branch between la and vl, for which we have
historical evidence. For example, documents such
as the Appendix Probi (Baehrens, 1922) provide in-
dications of orthographic confusions which resulted
from the growing gap between Classical Latin and
Vulgar Latin phonology around the 3rd and 4th cen-
turies AD. The Appendix lists common misspellings
of Latin words, from which phonological changes
can be inferred.
On the la to vl branch, rules for word-final dele-
tion of classical case markers dominate the list (rules
ranks 1 and 3 for deletion of final /s/, ranks 2 and
4 for deletion of final /m/). It is indeed likely that
these were generally eliminated in Vulgar Latin. For
the deletion of the /m/, the Appendix Probi contains
pairs such as PASSIM NON PASSI and OLIM NON
OLI. For the deletion of final /s/, this was observed
in early inscriptions, e.g. CORNELIO for CORNE-
LIOS (Allen, 1989). The frequent leveling of the
distinction between /o/ and /u/ (rules ranked 5 and 6)
can be also be found in the Appendix Probi: COLU-
BER NON COLOBER. Note that in the specific ex-
ample shown, the model lowers the orignal /u/ and
then re-raises it in the pt branch due to a latter pro-
cess along that branch.
Similarily, major canonical rules were discovered
in other branches as well, for example, /v/ to /b/
fortition in Spanish, /s/ to /z/ voicing in Italian,
palatalization along several branches, and so on. Of
course, the recovered words and rules are not per-
fect. For example, reconstructed Ibero /tRinta/ to
Spanish /tReinta/ (thirty) is generated in an odd fash-
ion using rules /e/ to /i/ and /n/ to /in/. Moreover,
even when otherwise reasonable systematic sound
changes are captured, the crudeness of our fixed-
granularity contexts can prevent the true context
893
r ? R / many environmentse ? / #i ? / #t ? d / UNROUNDED UNROUNDED
u ? o / many environmentsv ? b / initial or intervocalict ? t e / ALVEOLAR #z ? s / ROUNDED UNROUNDED
/werbum/ (la)
/verbo/ (vl)
/veRbo/ (ib)
/beRbo/ (es) /veRbu/ (pt)
/vErbo/ (it)
s ? / #m ? /u ? o / many environmentsw ? v / # UNROUNDED
u ? o / ALVEOLAR #e ? E / many environmentsi ? / many environmentsi ? e / ALVEOLAR #
a ? 5 / ALVEOLAR #n ? m / UNROUNDED ALVEOLARo ? u / ALVEOLAR #e ? 1 / BILABIAL ALVEOLAR
m ?u ? ow ? v
r ? R
v ? b o ? u
e ? E
Figure 6: The tree shows the system?s hypothesised derivation of a selected Latin word form, VERBUM (word/verb) into the modern
Spanish, Italian and Portuguese pronunciations. The Latin root and modern leaves were observed while the hidden nodes as well as
all the derivations were obtained using the parameters computed by our model after 15 iterations of EM. Nontrivial rules (i.e. rules
that are not identities) used at each stage are shown along the corresponding edge. The boxes display the top four nontrivial rules
corresponding to each of these evolutionary branches, ordered by the number of time they were applied during the last E round of
sampling. Note that since our natural classes are of fixed granularity, some rules must be redundantly discovered, which tends to
flood the top of the rule lists with duplicates of the top few rules. We summarized such redundancies in the above tables.
from being captured, resulting in either rules apply-
ing with low probability in overly coarse environ-
ments or rules being learned redundantly in overly
fine environments.
4.4 Selection of phylogenies
In this experiment, we show that our model can be
used to select between various topologies of phylo-
genies. We first presented to the algorithm the uni-
versally accepted evolutionary tree corresponding to
the evolution of Latin into Spanish, Portuguese and
Italian (Topology 2 in Figure 1). We estimated the
log-likelihood L? of the data under this topology.
Next, we estimated the log-likelihood L? under two
defective topologies (*Topology 3 and *Topology
4). We recorded the log-likelihood ratio L? ? L?
after the last iteration of EM. Note that the two like-
lihoods are comparable since the complexity of the
two models is the same.5
We obtained a ratio of L? ? L? = ?4458 ?
(?4766) = 307 for Topology 2 versus *Topology
3, and ?4877? (?5125) = 248 for Topology 2 ver-
sus *Topology 4 (the experimental setup is described
in Table 1). As one would hope, this log-likelihood
ratio is positive in both cases, indicating that the sys-
tem prefers the true topology over the wrong ones.
While it may seem, at the first glance, that this re-
sult is limited in scope, knowing the relative arrange-
5If a word was not reachable in one of the topology, it was
ignored in both models for the computation of the likelihoods.
ment of all groups of four nodes is actually sufficient
for constructing a full-fledged phylogenetic tree. In-
deed, quartet-based methods, which have been very
popular in the computational biology community,
are precisely based on this fact (Erdos et al, 1996).
There is a rich literature on this subject and approxi-
mate algorithms exist which are robust to misclassi-
fication of a subset of quartets (Wu et al, 2007).
4.5 More experimental details
This section summarizes the values of the parame-
ters we used in these experiments, their interpreta-
tion, and the effect of setting them to other values.
The Dirichlet prior on the parameters can be in-
terpreted as adding pseudocounts to the correspond-
ing edits. It is an important way of infusing par-
simony into the model by setting the prior of the
self-substitution parameters much higher than that
of the other parameters. We used 6.0 as the prior on
the self-substitution parameters, and for all environ-
ments, 1.1 was divided uniformly across the other
edits. As long as the prior on self-substitution is
kept within this rough order of magnitude, varying
them has a limited effect on our results. We also ini-
tialized the parameters with values that encourage
self-substitutions. Again, the results were robust to
perturbation of initialization as long as the value for
self-substitution dominates the other parameters.
The experiments used two natural classes for
vowels (rounded and unrounded), and six natural
894
classes for consonants, based on the place of ar-
ticulation (alveolar, bilabial, labiodental, palatal,
postalveolar, and velar). We conducted experi-
ments to evaluate the effect of using different natural
classes and found that finer ones can help if enough
data is used for training. We defer the meticulous
study of the optimal granularity to future work, as it
would be a more interesting experiment under a log-
linear model. In such a model, contexts of different
granularities can coexist, whereas such coexistence
is not recognized by the current model, giving rise
to many duplicate rules.
We estimated the bigram phoneme model on the
words in the root languages that were not heldout.
Just as in machine translation, the language model
was found to contribute significantly to reconstruc-
tion performance. We tried to increase the weight of
the language model by exponentiating it to a power,
as is often done in NLP applications, but we did
not find that it had any significant impact on per-
formance.
In the reconstruction experiments, when the data
was not reachable by the model, the word used in
the initialization was used as the prediction, and
the evolution of these words were ignored when re-
estimating the parameters. Words were initialized
by picking at random, for each unobserved node, an
observed node?s corresponding word.
5 Conclusion
We have presented a novel probabilistic model of
diachronic phonology and an associated inference
procedure. Our experiments indicate that our model
is able to both produce accurate reconstructions as
measured by edit distance and identify linguisti-
cally plausible rules that account for the phonologi-
cal changes. We believe that the probabilistic frame-
work we have introduced for diachronic phonology
is promising, and scaling it up to richer phylogenetic
may indeed reveal something insightful about lan-
guage change.
6 Acknowledgement
We would like to thank Bonnie Chantarotwong for
her help with the IPA converter and our reviewers
for their comments. This work was supported by
a FQRNT fellowship to the first author, a NDSEG
fellowship to the second author, NSF grant number
BCS-0631518 to the third author, and a Microsoft
Research New Faculty Fellowship to the fourth au-
thor.
References
W. Sidney Allen. 1989. Vox Latina: The Pronunciation
of Classical Latin. Cambridge University Press.
W.A. Baehrens. 1922. Sprachlicher Kommentar zur
vulga?rlateinischen Appendix Probi. Halle (Saale) M.
Niemeyer.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
N. Chomsky and M. Halle. 1968. The Sound Pattern of
English. Harper & Row.
I. Dyen, J.B. Kruskal, and P. Black.
1997. FILE IE-DATA1. Available at
http://www.ntu.edu.au/education/langs/ielex/IE-
DATA1.
P. L. Erdos, M. A. Steel, L. A. Szekely, and T. J. Warnow.
1996. Local quartet splits of a binary tree infer all
quartet splits via one dyadic inference rule. Technical
report, DIMACS.
S. N. Evans, D. Ringe, and T. Warnow. 2004. Inference
of divergence times as a statistical inverse problem. In
P. Forster and C. Renfrew, editors, Phylogenetic Meth-
ods and the Prehistory of Languages. McDonald Insti-
tute Monographs.
Joseph Felsenstein. 2003. Inferring Phylogenies. Sin-
auer Associates.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
R. D. Gray and Q. Atkinson. 2003. Language-tree di-
vergence times support the Anatolian theory of Indo-
European origins. Nature.
John P. Huelsenbeck, Fredrik Ronquist, Rasmus Nielsen,
and Jonathan P. Bollback. 2001. Bayesian inference
of phylogeny and its impact on evolutionary biology.
Science.
P. Koehn. 2002. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
895
S. Li, D. K. Pearl, and H. Doss. 2000. Phylogenetic tree
construction using Markov chain Monte Carlo. Jour-
nal of the American Statistical Association.
Bob Mau and M.A. Newton. 1997. Phylogenetic in-
ference for binary data on dendrograms using markov
chain monte carlo. Journal of Computational and
Graphical Statistics.
L. Nakhleh, D. Ringe, and T. Warnow. 2005. Perfect
phylogenetic networks: A new methodology for re-
constructing the evolutionary history of natural lan-
guages. Language, 81:382?420.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29:19?51.
P. Resnik, Mari Broman Olsen, and Mona Diab. 1999.
The bible as a parallel corpus: Annotating the ?book of
2000 tongues?. Computers and the Humanities, 33(1-
2):129?153.
D. Ringe, T. Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100:59?129.
M. Swadesh. 1955. Towards greater accuracy in lex-
icostatistic dating. Journal of American Linguistics,
21:121?137.
A. Venkataraman, J. Newman, and J.D. Patrick. 1997.
A complexity measure for diachronic chinese phonol-
ogy. In J. Coleman, editor, Computational Phonology.
Association for Computational Linguistics.
G. Wu, J. A. You, and G. Lin. 2007. Quartet-based
phylogeny reconstruction with answer set program-
ming. IEEE/ACM Transactions on computational bi-
ology, 4:139?152.
Ziheng Yang and Bruce Rannala. 1997. Bayesian phy-
logenetic inference using dna sequences: A markov
chain monte carlo method. Molecular Biology and
Evolution 14.
896
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 897?905, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Structured Models for Phone Recognition
Slav Petrov Adam Pauls Dan Klein
Computer Science Department, EECS Divison
University of California at Berkeley
Berkeley, CA, 94720, USA
{petrov,adpauls,klein}@cs.berkeley.edu
Abstract
We present a maximally streamlined approach to
learning HMM-based acoustic models for automatic
speech recognition. In our approach, an initial mono-
phone HMM is iteratively refined using a split-merge
EM procedure which makes no assumptions about
subphone structure or context-dependent structure,
and which uses only a single Gaussian per HMM
state. Despite the much simplified training process,
our acoustic model achieves state-of-the-art results
on phone classification (where it outperforms almost
all other methods) and competitive performance on
phone recognition (where it outperforms standard CD
triphone / subphone / GMM approaches). We also
present an analysis of what is and is not learned by
our system.
1 Introduction
Continuous density hiddenMarkov models (HMMs)
underlie most automatic speech recognition (ASR)
systems in some form. While the basic algorithms
for HMM learning and inference are quite general,
acoustic models of speech standardly employ rich
speech-specific structures to improve performance.
For example, it is well known that a monophone
HMM with one state per phone is too coarse an
approximation to the true articulatory and acoustic
process. The HMM state space is therefore refined
in several ways. To model phone-internal dynam-
ics, phones are split into beginning, middle, and end
subphones (Jelinek, 1976). To model cross-phone
coarticulation, the states of the HMM are refined
by splitting the phones into context-dependent tri-
phones. These states are then re-clustered (Odell,
1995) and the parameters of their observation dis-
tributions are tied back together (Young and Wood-
land, 1994). Finally, to model complex emission
densities, states emit mixtures of multivariate Gaus-
sians. This standard structure is shown schemati-
cally in Figure 1. While this rich structure is pho-
netically well-motivated and empirically success-
ful, so much structural bias may be unnecessary, or
even harmful. For example in the domain of syn-
tactic parsing with probabilistic context-free gram-
mars (PCFGs), a surprising recent result is that au-
tomatically induced grammar refinements can out-
perform sophisticated methods which exploit sub-
stantial manually articulated structure (Petrov et al,
2006).
In this paper, we consider a much more automatic,
data-driven approach to learning HMM structure for
acoustic modeling, analagous to the approach taken
by Petrov et al (2006) for learning PCFGs. We start
with a minimal monophone HMM in which there is
a single state for each (context-independent) phone.
Moreover, the emission model for each state is a sin-
gle multivariate Gaussian (over the standard MFCC
acoustic features). We then iteratively refine this
minimal HMM through state splitting, adding com-
plexity as needed. States in the refined HMMs are
always substates of the original HMM and are there-
fore each identified with a unique base phone. States
are split, estimated, and (perhaps) merged, based on
a likelihood criterion. Our model never allows ex-
plicit Gaussian mixtures, though substates may de-
velop similar distributions and thereby emulate such
mixtures.
In principle, discarding the traditional structure
can either help or hurt the model. Incorrect prior
splits can needlessly fragment training data and in-
correct prior tying can limit the model?s expressiv-
ity. On the other hand, correct assumptions can
increase the efficiency of the learner. Empirically,
897
Start
begin end
End
mid begin endmid
d
7 
= c(#-d-ae)
begin endmid
ae
3 
= c(d-ae-d) d
13 
= c(ae-d-#)
Start
a d
End
a d a d
d ae d
b c b c b c
Figure 1: Comparison of the standard model to our model (here
shown with k = 4 subphones per phone) for the word dad.
The dependence of subphones across phones in our model is
not shown, while the context clustering in the standard model is
shown only schematically.
we show that our automatic approach outperforms
classic systems on the task of phone recognition on
the TIMIT data set. In particular, it outperforms
standard state-tied triphone models like Young and
Woodland (1994), achieving a phone error rate of
26.4% versus 27.7%. In addition, our approach
gives state-of-the-art performance on the task of
phone classification on the TIMIT data set, suggest-
ing that our learned structure is particularly effec-
tive at modeling phone-internal structure. Indeed,
our error rate of 21.4% is outperformed only by the
recent structured margin approach of Sha and Saul
(2006). It remains to be seen whether these posi-
tive results on acoustic modeling will facilitate better
word recognition rates in a large vocabulary speech
recognition system.
We also consider the structures learned by the
model. Subphone structure is learned, similar to,
but richer than, standard begin-middle-end struc-
tures. Cross-phone coarticulation is also learned,
with classic phonological classes often emerging
naturally.
Many aspects of this work are intended to sim-
plify rather than further articulate the acoustic pro-
cess. It should therefore be clear that the basic tech-
niques of splitting, merging, and learning using EM
are not in themselves new for ASR. Nor is the basic
latent induction method new (Matsuzaki et al, 2005;
Petrov et al, 2006). What is novel in this paper is (1)
the construction of an automatic system for acous-
tic modeling, with substantially streamlined struc-
ture, (2) the investigation of variational inference for
such a task, (3) the analysis of the kinds of struc-
tures learned by such a system, and (4) the empirical
demonstration that such a system is not only com-
petitive with the traditional approach, but can indeed
outperform even very recent work on some prelimi-
nary measures.
2 Learning
In the following, we propose a greatly simplified
model that does not impose any manually specified
structural constraints. Instead of specifying struc-
ture a priori, we use the Expectation-Maximization
(EM) algorithm for HMMs (Baum-Welch) to auto-
matically induce the structure in a way that maxi-
mizes data likelihood.
In general, our training data consists of sets
of acoustic observation sequences and phone level
transcriptions r which specify a sequence of phones
from a set of phones Y , but does not label each
time frame with a phone. We refer to an observa-
tion sequence as x = x1, . . . , xT where xi ? R39
are standard MFCC features (Davis and Mermel-
stein, 1980). We wish to induce an HMM over a
set of states S for which we also have a function
pi : S ? Y that maps every state in S to a phone
in Y . Note that in the usual formulation of the EM
algorithm for HMMs, one is interested in learning
HMM parameters ? that maximize the likelihood of
the observations P(x|?); in contrast, we aim to max-
imize the joint probability of our observations and
phone transcriptions P(x, r|?) or observations and
phone sequences P(x,y|?) (see below). We now de-
scribe this relatively straightforward modification of
the EM algorithm.
2.1 The Hand-Aligned Case
For clarity of exposition we first consider a simpli-
fied scenario in which we are given hand-aligned
phone labels y = y1, . . . , yT for each time t, as is
the case for the TIMIT dataset. Our procedure does
not require such extensive annotation of the training
data and in fact gives better performance when the
exact transition point between phones are not pre-
specified but learned.
We define forward and backward probabilities
(Rabiner, 1989) in the following way: the forward
probability is the probability of observing the se-
quence x1, . . . , xt with transcription y1, . . . , yt and
898
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
(a)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0 1
(b)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
3
2
1
(c)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
1
6
0
3
4
7
25
(d)
Figure 2: Iterative refinement of the /ih/ phone with 1, 2, 4, 8 substates.
ending in state s at time t:
?t(s) = P(x1, . . . , xt, y1, . . . yt, st = s|?),
and the backward probability is the probability of
observing the sequence xt+1, . . . , xT with transcrip-
tion yt+1, . . . , yT , given that we start in state s at
time t:
?t(s) = P(xt+1, . . . , xT , yt+1, . . . , yT |st = s, ?),
where ? are the model parameters. As usual, we
parameterize our HMMs with ass? , the probability
of transitioning from state s to s?, and bs(x) ?
N (?s,?s), the probability emitting the observation
x when in state s.
These probabilities can be computed using the
standard forward and backward recursions (Rabiner,
1989), except that at each time t, we only con-
sider states st for which pi(st) = yt, because we
have hand-aligned labels for the observations. These
quantities also allow us to compute the posterior
counts necessary for the E-step of the EM algorithm.
2.2 Splitting
One way of inducing arbitrary structural annota-
tions would be to split each HMM state in into
m substates, and re-estimate the parameters for the
split HMM using EM. This approach has two ma-
jor drawbacks: for larger m it is likely to converge
to poor local optima, and it allocates substates uni-
formly across all states, regardless of how much an-
notation is required for good performance.
To avoid these problems, we apply a hierarchical
parameter estimation strategy similar in spirit to the
work of Sankar (1998) and Ueda et al (2000), but
here applied to HMMs rather than to GMMs. Be-
ginning with the baseline model, where each state
corresponds to one phone, we repeatedly split and
re-train the HMM. This strategy ensures that each
split HMM is initialized ?close? to some reasonable
maximum.
Concretely, each state s in the HMM is split in
two new states s1, s2 with pi(s1) = pi(s2) = pi(s).
We initialize EM with the parameters of the previ-
ous HMM, splitting every previous state s in two
and adding a small amount of randomness  ? 1%
to its transition and emission probabilities to break
symmetry:
as1s? ? ass? + ,
bs1(o) ? N (?s + ,?s),
and similarly for s2. The incoming transitions are
split evenly.
We then apply the EM algorithm described above
to re-estimate these parameters before performing
subsequent split operations.
2.3 Merging
Since adding substates divides HMM statistics into
many bins, the HMM parameters are effectively es-
timated from less data, which can lead to overfitting.
Therefore, it would be to our advantage to split sub-
899
states only where needed, rather than splitting them
all.
We realize this goal by merging back those splits
s ? s1s2 for which, if the split were reversed, the
loss in data likelihood would be smallest. We ap-
proximate the loss in data likelihood for a merge
s1s2 ? swith the following likelihood ratio (Petrov
et al, 2006):
?(s1 s2 ? s) =
?
sequences
?
t
Pt(x,y)
P(x,y)
.
Here P(x,y) is the joint likelihood of an emission
sequence x and associated state sequence y. This
quantity can be recovered from the forward and
backward probabilities using
P(x,y) =
?
s:pi(s)=yt
?t(s) ? ?t(s).
Pt(x,y) is an approximation to the same joint like-
lihood where states s1 and s2 are merged. We ap-
proximate the true loss by only considering merging
states s1 and s2 at time t, a value which can be ef-
ficiently computed from the forward and backward
probabilities. The forward score for the merged state
s at time t is just the sum of the two split scores:
??t(s) = ?t(s1) + ?t(s2),
while the backward score is a weighted sum of the
split scores:
??t(s) = p1?t(s1) + p2?t(s2),
where p1 and p2 are the relative (posterior) frequen-
cies of the states s1 and s2.
Thus, the likelihood after merging s1 and s2 at
time t can be computed from these merged forward
and backward scores as:
P t(x,y) = ??t(s) ? ??t(s) +
?
s?
?t(s
?) ? ?t(s
?)
where the second sum is over the other substates of
xt, i.e. {s? : pi(s?) = xt, s? /? {s1, s2}}. This
expression is an approximation because it neglects
interactions between instances of the same states at
multiple places in the same sequence. In particular,
since phones frequently occur with multiple consec-
utive repetitions, this criterion may vastly overesti-
mate the actual likelihood loss. As such, we also im-
plemented the exact criterion, that is, for each split,
we formed a new HMM with s1 and s2 merged and
calculated the total data likelihood. This method
is much more computationally expensive, requiring
a full forward-backward pass through the data for
each potential merge, and was not found to produce
noticeably better performance. Therefore, all exper-
iments use the approximate criterion.
2.4 The Automatically-Aligned Case
It is straightforward to generalize the hand-aligned
case to the case where the phone transcription is
known, but no frame level labeling is available. The
main difference is that the phone boundaries are not
known in advance, which means that there is now
additional uncertainty over the phone states. The
forward and backward recursions must thus be ex-
panded to consider all state sequences that yield the
given phone transcription. We can accomplish this
with standard Baum-Welch training.
3 Inference
An HMM over refined subphone states s ? S nat-
urally gives posterior distributions P(s|x) over se-
quences of states s. We would ideally like to ex-
tract the transcription r of underlying phones which
is most probable according to this posterior1. The
transcription is two stages removed from s. First,
it collapses the distinctions between states s which
correspond to the same phone y = pi(s). Second,
it collapses the distinctions between where phone
transitions exactly occur. Viterbi state sequences can
easily be extracted using the basic Viterbi algorithm.
On the other hand, finding the best phone sequence
or transcription is intractable.
As a compromise, we extract the phone sequence
(not transcription) which has highest probability in
a variational approximation to the true distribution
(Jordan et al, 1999). Let the true posterior distri-
bution over phone sequences be P(y|x). We form
an approximation Q(y) ? P(y|x), where Q is an
approximation specific to the sequence x and factor-
1Remember that by ?transcription? we mean a sequence of
phones with duplicates removed.
900
izes as:
Q(y) =
?
t
q(t, xt, yt+1).
We would like to fit the values q, one for each time
step and state-state pair, so as to make Q as close to
P as possible:
min
q
KL(P(y|x)||Q(y)).
The solution can be found analytically using La-
grange multipliers:
q(t, y, y?) =
P(Yt = y, Yt+1 = y?|x)
P(Yt = y|x)
.
where we have made the position-specific random
variables Yt explicit for clarity. This approximation
depends only on our ability to calculate posteriors
over phones or phone-phone pairs at individual po-
sitions t, which is easy to obtain from the state pos-
teriors, for example:
P(Yt = y,Yt+1 = y
?|x) =
?
s:pi(s)=y
?
s?:pi(s?)=y?
?t(s)ass?bs?(xt)?t+1(s
?)
P(x)
Finding the Viterbi phone sequence in the approxi-
mate distribution Q, can be done with the Forward-
Backward algorithm over the lattice of q values.
4 Experiments
We tested our model on the TIMIT database, using
the standard setups for phone recognition and phone
classification. We partitioned the TIMIT data into
training, development, and (core) test sets according
to standard practice (Lee and Hon, 1989; Gunawar-
dana et al, 2005; Sha and Saul, 2006). In particu-
lar, we excluded all sa sentences and mapped the 61
phonetic labels in TIMIT down to 48 classes before
training our HMMs. At evaluation, these 48 classes
were further mapped down to 39 classes, again in
the standard way.
MFCC coefficients were extracted from the
TIMIT source as in Sha and Saul (2006), includ-
ing delta and delta-delta components. For all experi-
ments, our system and all baselines we implemented
used full covariance when parameterizing emission
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38 0.4
 
0.42  0
 
200 
400 
600 
800 1
000 1
200 1
400 1
600 1
800 2
000
Phone Recognition Error
Numb
er of S
tates
split o
nly
split a
nd me
rge
split a
nd me
rge, au
tomati
c align
ment
Figure 3: Phone recognition error for models of increasing size
models.2 All Gaussians were endowed with weak
inverse Wishart priors with zero mean and identity
covariance.3
4.1 Phone Recognition
In the task of phone recognition, we fit an HMM
whose output, with subsequent states collapsed, cor-
responds to the training transcriptions. In the TIMIT
data set, each frame is manually phone-annotated, so
the only uncertainty in the basic setup is the identity
of the (sub)states at each frame.
We therefore began with a single state for each
phone, in a fully connected HMM (except for spe-
cial treatment of dedicated start and end states). We
incrementally trained our model as described in Sec-
tion 2, with up to 6 split-merge rounds. We found
that reversing 25% of the splits yielded good overall
performance while maintaining compactness of the
model.
We decoded using the variational decoder de-
scribed in Section 3. The output was then scored
against the reference phone transcription using the
standard string edit distance.
During both training and decoding, we used ?flat-
tened? emission probabilities by exponentiating to
some 0 < ? < 1. We found the best setting for ?
to be 0.2, as determined by tuning on the develop-
ment set. This flattening compensates for the non-
2Most of our findings also hold for diagonal covariance
Gaussians, albeit the final error rates are 2-3% higher.
3Following previous work with PCFGs (Petrov et al, 2006),
we experimented with smoothing the substates towards each
other to prevent overfitting, but we were unable to achieve any
performance gains.
901
Method Error Rate
State-Tied Triphone HMM
27.7%1
(Young and Woodland, 1994)
Gender Dependent Triphone HMM
27.1%1
(Lamel and Gauvain, 1993)
This Paper 26.4%
Bayesian Triphone HMM
25.6%
(Ming and Smith, 1998)
Heterogeneous classifiers
24.4%
(Halberstadt and Glass, 1998)
Table 1: Phone recognition error rates on the TIMIT core test
from Glass (2003).
1These results are on a slightly easier test set.
independence of the frames, partially due to over-
lapping source samples and partially due to other
unmodeled correlations.
Figure 3 shows the recognition error as the model
grows in size. In addition to the basic setup de-
scribed so far (split and merge), we also show a
model in which merging was not performed (split
only). As can be seen, the merging phase not only
decreases the number of HMM states at each round,
but also improves phone recognition error at each
round.
We also compared our hierarchical split only
model with a model where we directly split all states
into 2k substates, so that these models had the same
number of states as a a hierarchical model after k
split and merge cycles. While for small k, the dif-
ference was negligible, we found that the error in-
creased by 1% absolute for k = 5. This trend is to
be expected, as the possible interactions between the
substates grows with the number of substates.
Also shown in Figure 3, and perhaps unsurprising,
is that the error rate can be further reduced by allow-
ing the phone boundaries to drift from the manual
alignments provided in the TIMIT training data. The
split and merge, automatic alignment line shows the
result of allowing the EM fitting phase to reposition
each phone boundary, giving absolute improvements
of up to 0.6%.
We investigated how much improvement in accu-
racy one can gain by computing the variational ap-
proximation introduced in Section 3 versus extract-
ing the Viterbi state sequence and projecting that se-
quence to its phone transcription. The gap varies,
Method Error Rate
GMM Baseline (Sha and Saul, 2006) 26.0%
HMM Baseline (Gunawardana et al, 2005) 25.1%
SVM (Clarkson and Moreno, 1999) 22.4%
Hidden CRF (Gunawardana et al, 2005) 21.7%
This Paper 21.4%
Large Margin GMM (Sha and Saul, 2006) 21.1%
Table 2: Phone classification error rates on the TIMIT core test.
but on a model with roughly 1000 states (5 split-
merge rounds), the variational decoder decreases er-
ror from 26.5% to 25.6%. The gain in accuracy
comes at a cost in time: we must run a (possibly
pruned) Forward-Backward pass over the full state
space S, then another over the smaller phone space
Y . In our experiments, the cost of variational decod-
ing was a factor of about 3, which may or may not
justify a relative error reduction of around 4%.
The performance of our best model (split and
merge, automatic alignment, and variational decod-
ing) on the test set is 26.4%. A comparison of our
performance with other methods in the literature is
shown in Table 1. Despite our structural simplic-
ity, we outperform state-tied triphone systems like
Young andWoodland (1994), a standard baseline for
this task, by nearly 2% absolute. However, we fall
short of the best current systems.
4.2 Phone Classification
Phone classification is the fairly constrained task of
classifying in isolation a sequence of frames which
is known to span exactly one phone. In order to
quantify how much of our gains over the triphone
baseline stem from modeling context-dependencies
and how much from modeling the inner structure of
the phones, we fit separate HMM models for each
phone, using the same split and merge procedure as
above (though in this case only manual alignments
are reasonable because we test on manual segmen-
tations). For each test frame sequence, we com-
pute the likelihood of the sequence from the forward
probabilities of each individual phone HMM. The
phone giving highest likelihood to the input was se-
lected. The error rate is a simple fraction of test
phones classified correctly.
Table 2 shows a comparison of our performance
with that of some other methods in the literature.
A minimal comparison is to a GMM with the same
number of mixtures per phone as our model?s maxi-
902
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
Hypothesis
Ref
ere
nce
vowels/semivowels
nasals/flaps
strong fricatives
weak fricatives
stops
Figure 4: Phone confusion matrix. 76% of the substitutions fall
within the shown classes.
mum substates per phone. While these models have
the same number of total Gaussians, in our model
the Gaussians are correlated temporally, while in
the GMM they are independent. Enforcing begin-
middle-end HMM structure (see HMM Baseline) in-
creases accuracy somewhat, but our more general
model clearly makes better use of the available pa-
rameters than those baselines.
Indeed, our best model achieves a surpris-
ing performance of 21.4%, greatly outperform-
ing other generative methods and achieving perfor-
mance competitive with state-of-the-art discrimina-
tive methods. Only the recent structured margin ap-
proach of Sha and Saul (2006) gives a better perfor-
mance than our model. The strength of our system
on the classification task suggests that perhaps it is
modeling phone-internal structure more effectively
than cross-phone context.
5 Analysis
While the overall phone recognition and classifi-
cation numbers suggest that our system is broadly
comparable to and perhaps in certain ways superior
to classical approaches, it is illuminating to investi-
gate what is and is not learned by the model.
Figure 4 gives a confusion matrix over the substi-
tution errors made by our model. The majority of the
next
previous
eh
ow
ao
aa
ey
iy
ix
v
f
k
m
ow
ao
aa
ey
iy
ih
ae
ix
z
f
s
1
4
3
5
62
0
p
Figure 5: Phone contexts and subphone structure. The /l/ phone
after 3 split-merge iterations is shown.
confusions are within natural classes. Some partic-
ularly frequent and reasonable confusions arise be-
tween the consonantal /r/ and the vocalic /er/ (the
same confusion arises between /l/ and /el/, but the
standard evaluation already collapses this distinc-
tion), the reduced vowels /ax/ and /ix/, the voiced
and voiceless alveolar sibilants /z/ and /s/, and the
voiced and voiceless stop pairs. Other vocalic con-
fusions are generally between vowels and their cor-
responding reduced forms. Overall, 76% of the sub-
stitutions are within the broad classes shown in the
figure.
We can also examine the substructure learned for
the various phones. Figure 2 shows the evolution
of the phone /ih/ from a single state to 8 substates
during split/merge (no merges were chosen for this
phone), using hand-alignment of phones to frames.
These figures were simplified from the complete
state transition matrices as follows: (1) adjacent
phones? substates are collapsed, (2) adjacent phones
are selected based on frequency and inbound prob-
ability (and forced to be the same across figures),
(3) infrequent arcs are suppressed. In the first split,
(b), a sonorant / non-sonorant distinction is learned
over adjacent phones, along with a state chain which
captures basic duration (a self-looping state gives
an exponential model of duration; the sum of two
such states is more expressive). Note that the nat-
903
ural classes interact with the chain in a way which
allows duration to depend on context. In further re-
finements, more structure is added, including a two-
track path in (d) where one track captures the distinct
effects on higher formants of r-coloring and nasal-
ization. Figure 5 shows the corresponding diagram
for /l/, where some merging has also occurred. Dif-
ferent natural classes emerge in this case, with, for
example, preceding states partitioned into front/high
vowels vs. rounded vowels vs. other vowels vs. con-
sonants. Following states show a front/back dis-
tinction and a consonant distinction, and the phone
/m/ is treated specially, largely because the /lm/ se-
quence tends to shorten the /l/ substantially. Note
again how context, internal structure, and duration
are simultaneously modeled. Of course, it should
be emphasized that post hoc analysis of such struc-
ture is a simplification and prone to seeing what one
expects; we present these examples to illustrate the
broad kinds of patterns which are detected.
As a final illustration of the nature of the learned
models, Table 3 shows the number of substates allo-
cated to each phone by the split/merge process (the
maximum is 32 for this stage) for the case of hand-
aligned (left) as well as automatically-aligned (right)
phone boundaries. Interestingly, in the hand-aligned
case, the vowels absorb most of the complexity since
many consonantal cues are heavily evidenced on
adjacent vowels. However, in the automatically-
aligned case, many vowel frames with substantial
consontant coloring are re-allocated to those adja-
cent consonants, giving more complex consonants,
but comparatively less complex vowels.
6 Conclusions
We have presented a minimalist, automatic approach
for building an accurate acoustic model for phonetic
classification and recognition. Our model does not
require any a priori phonetic bias or manual spec-
ification of structure, but rather induces the struc-
ture in an automatic and streamlined fashion. Start-
ing from a minimal monophone HMM, we auto-
matically learn models that achieve highly compet-
itive performance. On the TIMIT phone recogni-
tion task our model clearly outperforms standard
state-tied triphone models like Young and Wood-
land (1994). For phone classification, our model
Vowels
aa 31 32
ae 32 17
ah 31 8
ao 32 23
aw 18 6
ax 18 3
ay 32 28
eh 32 16
el 6 4
en 4 3
er 32 31
ey 32 30
ih 32 11
ix 31 16
iy 31 32
ow 26 10
oy 4 4
uh 5 2
uw 21 8
Consonants
b 2 32
ch 13 30
d 2 14
dh 6 31
dx 2 3
f 32 32
g 2 15
hh 3 5
jh 3 16
k 30 32
l 25 32
m 25 25
n 29 32
ng 3 4
p 5 24
r 32 32
s 32 32
sh 30 32
t 24 32
th 8 11
v 23 11
w 10 21
y 3 7
z 31 32
zh 2 2
Other
epi 2 4
sil 32 32
vcl 29 30
cl 31 32
Table 3: Number of substates allocated per phone. The left
column gives the number of substates allocated when training
on manually aligned training sequences, while the right column
gives the number allocated when we automatically determine
phone boundaries.
achieves performance competitive with the state-of-
the-art discriminative methods (Sha and Saul, 2006),
despite being generative in nature. This result to-
gether with our analysis of the context-dependencies
and substructures that are being learned, suggests
that our model is particularly well suited for mod-
eling phone-internal structure. It does, of course
remain to be seen if and how these benefits can be
scaled to larger systems.
References
P. Clarkson and P. Moreno. 1999. On the use of Sup-
port Vector Machines for phonetic classification. In
ICASSP ?99.
S. B. Davis and P. Mermelstein. 1980. Comparison
of parametric representation for monosyllabic word
recognition in continuously spoken sentences. IEEE
Transactions on Acoustics, Speech, and Signal Pro-
cessing, 28(4).
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17(2).
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden Conditional Random Fields for phone
recognition. In Eurospeech ?05.
A. K. Halberstadt and J. R. Glass. 1998. Hetero-
geneous measurements and multiple classifiers for
speech recognition. In ICSLP ?98.
F. Jelinek. 1976. Continuous speech recognition by sta-
tistical methods. Proceedings of the IEEE.
904
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Learning in Graphical Models.
L. Lamel and J. Gauvain. 1993. Cross-lingual experi-
ments with phone recognition. In ICASSP ?93.
K. F. Lee and H. W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models.
IEEE Transactions on Acoustics, Speech, and Signal
Processing, 37(11).
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
J. Ming and F.J. Smith. 1998. Improved phone recogni-
tion using Bayesian triphone models. In ICASSP ?98.
J. J. Odell. 1995. The Use of Context in Large Vocab-
ulary Speech Recognition. Ph.D. thesis, University of
Cambridge.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In COLING-ACL ?06.
L. Rabiner. 1989. A Tutorial on hidden Markov mod-
els and selected applications in speech recognition. In
IEEE.
A. Sankar. 1998. Experiments with a Gaussian merging-
splitting algorithm for HMM training for speech
recognition. In DARPA Speech Recognition Workshop
?98.
F. Sha and L. K. Saul. 2006. Large margin Gaussian mix-
ture modeling for phonetic classification and recogni-
tion. In ICASSP ?06.
N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton.
2000. Split andMerge EM algorithm for mixture mod-
els. Neural Computation, 12(9).
S. J. Young and P. C. Woodland. 1994. State clustering
in HMM-based continuous speech recognition. Com-
puter Speech and Language, 8(4).
905
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108?116,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Coarse-to-Fine Syntactic Machine Translation
using Language Projections
Slav Petrov Aria Haghighi Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, aria42, klein}@eecs.berkeley.edu
Abstract
The intersection of tree transducer-based
translation models with n-gram language
models results in huge dynamic programs for
machine translation decoding. We propose a
multipass, coarse-to-fine approach in which
the language model complexity is incremen-
tally introduced. In contrast to previous order-
based bigram-to-trigram approaches, we fo-
cus on encoding-based methods, which use
a clustered encoding of the target language.
Across various encoding schemes, and for
multiple language pairs, we show speed-ups of
up to 50 times over single-pass decoding while
improving BLEU score. Moreover, our entire
decoding cascade for trigram language models
is faster than the corresponding bigram pass
alone of a bigram-to-trigram decoder.
1 Introduction
In the absence of an n-gram language model, decod-
ing a synchronous CFG translation model is very
efficient, requiring only a variant of the CKY al-
gorithm. As in monolingual parsing, dynamic pro-
gramming items are simply indexed by a source lan-
guage span and a syntactic label. Complexity arises
when n-gram language model scoring is added, be-
cause items must now be distinguished by their ini-
tial and final few target language words for purposes
of later combination. This lexically exploded search
space is a root cause of inefficiency in decoding, and
several methods have been suggested to combat it.
The approach most relevant to the current work is
Zhang and Gildea (2008), which begins with an ini-
tial bigram pass and uses the resulting chart to guide
a final trigram pass. Substantial speed-ups are ob-
tained, but computation is still dominated by the ini-
tial bigram pass. The key challenge is that unigram
models are too poor to prune well, but bigram mod-
els are already huge. In short, the problem is that
there are too many words in the target language. In
this paper, we propose a new, coarse-to-fine, mul-
tipass approach which allows much greater speed-
ups by translating into abstracted languages. That
is, rather than beginning with a low-order model of
a still-large language, we exploit language projec-
tions, hierarchical clusterings of the target language,
to effectively reduce the size of the target language.
In this way, initial passes can be very quick, with
complexity phased in gradually.
Central to coarse-to-fine language projection is
the construction of sequences of word clusterings
(see Figure 1). The clusterings are deterministic
mappings from words to clusters, with the property
that each clustering refines the previous one. There
are many choice points in this process, including
how these clusterings are obtained and how much
refinement is optimal for each pass. We demon-
strate that likelihood-based hierarchical EM train-
ing (Petrov et al, 2006) and cluster-based language
modeling methods (Goodman, 2001) are superior
to both rank-based and random-projection methods.
In addition, we demonstrate that more than two
passes are beneficial and show that our computa-
tion is equally distributed over all passes. In our
experiments, passes with less than 16-cluster lan-
guage models are most advantageous, and even a
single pass with just two word clusters can reduce
decoding time greatly.
108
To follow related work and to focus on the effects
of the language model, we present translation re-
sults under an inversion transduction grammar (ITG)
translation model (Wu, 1997) trained on the Eu-
roparl corpus (Koehn, 2005), described in detail in
Section 3, and using a trigram language model. We
show that, on a range of languages, our coarse-to-
fine decoding approach greatly outperforms base-
line beam pruning and bigram-to-trigram pruning on
time-to-BLEU plots, reducing decoding times by up
to a factor of 50 compared to single pass decoding.
In addition, coarse-to-fine decoding increases BLEU
scores by up to 0.4 points. This increase is a mixture
of improved search and subtly advantageous coarse-
to-fine effects which are further discussed below.
2 Coarse-to-Fine Decoding
In coarse-to-fine decoding, we create a series of ini-
tially simple but increasingly complex search prob-
lems. We then use the solutions of the simpler prob-
lems to prune the search spaces for more complex
models, reducing the total computational cost.
2.1 Related Work
Taken broadly, the coarse-to-fine approach is not
new to machine translation (MT) or even syntactic
MT. Many common decoder precomputations can
be seen as coarse-to-fine methods, including the A*-
like forward estimates used in the Moses decoder
(Koehn et al, 2007). In an ITG framework like
ours, Zhang and Gildea (2008) consider an approach
in which the results of a bigram pass are used as
an A* heuristic to guide a trigram pass. In their
two-pass approach, the coarse bigram pass becomes
computationally dominant. Our work differs in two
ways. First, we use posterior pruning rather than
A* search. Unlike A* search, posterior pruning
allows multipass methods. Not only are posterior
pruning methods simpler (for example, there is no
need to have complex multipart bounds), but they
can be much more effective. For example, in mono-
lingual parsing, posterior pruning methods (Good-
man, 1997; Charniak et al, 2006; Petrov and Klein,
2007) have led to greater speedups than their more
cautious A* analogues (Klein and Manning, 2003;
Haghighi et al, 2007), though at the cost of guaran-
teed optimality.
L
M
 
O
r
d
e
r
Bits in language model
the,report-NP-these,states
1
pi
2
3
2 3
the-NP-states0-NP-1 01-NP-10 010-NP-100
0,1-NP-0,1 01,10-NP-00,10 010,100-NP-000,100
...
...
?
Figure 2: Possible state projections pi for the target noun
phrase ?the report for these states? using the clusters
from Figure 1. The number of bits used to encode the tar-
get language vocabulary is varied along the x-axis. The
language model order is varied along the y-axis.
Second, we focus on an orthogonal axis of ab-
straction: the size of the target language. The in-
troduction of abstract languages gives better control
over the granularity of the search space and provides
a richer set of intermediate problems, allowing us
to adapt the level of refinement of the intermediate,
coarse passes to minimize total computation.
Beyond coarse-to-fine approaches, other related
approaches have also been demonstrated for syntac-
tic MT. For example, Venugopal et al (2007) con-
siders a greedy first pass with a full model followed
by a second pass which bounds search to a region
near the greedy results. Huang and Chiang (2007)
searches with the full model, but makes assumptions
about the the amount of reordering the language
model can trigger in order to limit exploration.
2.2 Language Model Projections
When decoding in a syntactic translation model with
an n-gram language model, search states are spec-
ified by a grammar nonterminal X as well as the
the n-1 left-most target side words ln?1, . . . , l1 and
right-most target side words r1, . . . , rn?1 of the gen-
erated hypothesis. We denote the resulting lexical-
ized state as ln?1, . . . , l1-X-r1, . . . , rn?1. Assum-
ing a vocabulary V and grammar symbol set G, the
state space size is up to |V |2(n?1)|G|, which is im-
mense for a large vocabulary when n > 1. We
consider two ways to reduce the size of this search
space. First, we can reduce the order of the lan-
guage model. Second, we can reduce the number
of words in the vocabulary. Both can be thought
of as projections of the search space to smaller ab-
109
these
one
we
they
the
a
that
for
states
report
of
to
also
been
will
must
0
1
00
01
000 001
010 011 100 101 110 111
10
11
Figure 1: An example of hierarchical clustering of target language vocabulary (see Section 4). Even with a small
number of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes.
stracted spaces. Figure 2 illustrates those two or-
thogonal axes of abstraction.
Order-based projections are simple. As shown
in Figure 2, they simply strip off the appropriate
words from each state, collapsing dynamic program-
ming items which are identical from the standpoint
of their left-to-right combination in the lower or-
der language model. However, having only order-
based projections is very limiting. Zhang and Gildea
(2008) found that their computation was dominated
by their bigram pass. The only lower-order pass
possible uses a unigram model, which provides no
information about the interaction of the language
model and translation model reorderings. We there-
fore propose encoding-based projections. These
projections reduce the size of the target language vo-
cabulary by deterministically projecting each target
language word to a word cluster. This projection ex-
tends to the whole search state in the obvious way:
assuming a bigram language model, the state l-X-r
projects to c(l)-X-c(r), where c(?) is the determin-
istic word-to-cluster mapping.
In our multipass approach, we will want a se-
quence c1 . . . cn of such projections. This requires a
hierarchical clustering of the target words, as shown
in Figure 1. Each word?s cluster membership can be
represented by an n-bit binary string. Each prefix of
length k declares that word?s cluster assignment at
the k-bit level. As we vary k, we obtain a sequence
of projections ck(?), each one mapping words to a
more refined clustering. When performing inference
in a k-bit projection, we replace the detailed original
language model over words with a coarse language
model LMk over the k-bit word clusters. In addition,
we replace the phrase table with a projected phrase
table, which further increases the speed of projected
passes. In Section 4, we describe the various clus-
tering schemes explored, as well as how the coarse
LMk are estimated.
2.3 Multipass Decoding
Unlike previous work, where the state space exists
only at two levels of abstraction (i.e. bigram and tri-
gram), we have multiple levels to choose from (Fig-
ure 2). Because we use both encoding-based and
order-based projections, our options form a lattice
of coarser state spaces, varying from extremely sim-
ple (a bigram model with just two word clusters) to
nearly the full space (a trigram model with 10 bits or
1024 word clusters).
We use this lattice to perform a series of coarse
passes with increasing complexity. More formally,
we decode a source sentence multiple times, in a
sequence of state spaces S0, S1, . . . , Sn=S, where
each Si is a refinement of Si?1 in either language
model order, language encoding size, or both. The
state spaces Si and Sj (i < j) are related to each
other via a projection operator pij?i(?) which maps
refined states deterministically to coarser states.
We start by decoding an input x in the simplest
state space S0. In particular, we compute the chart
of the posterior distributions p0(s) = P (s|x) for all
states s ? S0. These posteriors will be used to prune
the search space S1 of the following pass. States s
whose posterior falls below a threshold t trigger the
removal of all more refined states s? in the subse-
quent pass (see Figure 3). This technique is poste-
rior pruning, and is different from A* methods in
two main ways. First, it can be iterated in a multi-
pass setting, and, second, it is generally more effi-
110
0-X-0
11-X-10 10-X-11 11-X-1100-X-11 10-X-1011-X-01 01-X-1010-X-00 11-X-00 10-X-0100-X-00 01-X-00 00-X-01
1-X-0 0-X-1 1-X-1
2-Bit Pass
1-Bit Pass
 < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?
< t ?
< t ? < t ? < t ?
01-X-1100-X-1001-X-01
Figure 3: Example of state pruning in coarse-to-fine decoding using the language encoding projection (see Section 2.2).
During the coarse one-bit word cluster pass, two of the four possible states are pruned. Every extension of the pruned
one-bit states (indicated by the grey shading) are not explored during the two-bit word cluster pass.
cient with a potential cost of increased search errors
(see Section 2.1 for more discussion).
Looking at Figure 2, multipass coarse-to-fine de-
coding can be visualized as a walk from a coarse
point somewhere in the lower left to the most re-
fined point in the upper right of the grid. Many
coarse-to-fine schedules are possible. In practice,
we might start decoding with a 1-bit word bigram
pass, followed by an 3-bit word bigram pass, fol-
lowed by a 5-bit word trigram pass and so on (see
Section 5.3 for an empirical investigation). In terms
if time, we show that coarse-to-fine gives substantial
speed-ups. There is of course an additional mem-
ory requirement, but it is negligible. As we will see
in our experiments (Section 5) the largest gains can
be obtained with extremely coarse language mod-
els. In particular, the largest coarse model we use in
our best multipass decoder uses a 4-bit encoding and
hence has only 16 distinct words (or at most 4096
trigrams).
3 Inversion Transduction Grammars
While our approach applies in principle to a vari-
ety of machine translation systems (phrase-based or
syntactic), we will use the inversion transduction
grammar (ITG) approach of Wu (1997) to facili-
tate comparison with previous work (Zens and Ney,
2003; Zhang and Gildea, 2008) as well as to focus on
language model complexity. ITGs are a subclass of
synchronous context-free grammars (SCFGs) where
there are only three kinds of rules. Preterminal unary
productions produce terminal strings on both sides
(words or phrases): X ? e/f . Binary in-order pro-
ductions combine two phrases monotonically (X ?
[Y Z]). Finally, binary inverted productions invert
the order of their children (X ? ?Y Z?). These pro-
ductions are associated with rewrite weights in the
standard way.
Without a language model, SCFG decoding is just
like (monolingual) CFG parsing. The dynamic pro-
gramming states are specified by iXj , where ?i, j? is
a source sentence span and X is a nonterminal. The
only difference is that whenever we apply a CFG
production on the source side, we need to remem-
ber the corresponding synchronous production on
the target side and store the best obtainable transla-
tion via a backpointer. See Wu (1996) or Melamed
(2004) for a detailed exposition.
Once we integrate an n-gram language model, the
state space becomes lexicalized and combining dy-
namic programming items becomes more difficult.
Each state is now parametrized by the initial and
final n?1 words in the target language hypothesis:
ln?1, ..., l1-iXj-r1, ..., rn?1. Whenever we combine
two dynamic programming items, we need to score
the fluency of their concatentation by incorporat-
ing the score of any language model features which
cross the target side boundaries of the two concate-
nated items (Chiang, 2005). Decoding with an in-
tegrated language model is computationally expen-
sive for two reasons: (1) the need to keep track of
a large number of lexicalized hypotheses for each
source span, and (2) the need to frequently query the
large language model for each hypothesis combina-
tion.
Multipass coarse-to-fine decoding can alleviate
both computational issues. We start by decoding
in an extremely coarse bigram search space, where
there are very few possible translations. We com-
pute standard inside/outside probabilities (iS/oS),
as follows. Consider the application of non-inverted
binary rule: we combine two items lb-iBk-rb and
lc-kCj-rc spanning ?i, k? and ?k, j? respectively to
form a larger item lb-iAj-rc, spanning ?i, j?. The
111
lb-iAj -rc lb-iBk-rb lc-kCj-rc
rclb
+
lb rc
+=
iS(lb-iAj -rc) += iS(lb-iBk-rb) ? iS(lc-kCj-rc)LM(rb, lc) ?p(X?[Y Z]) ?
lcrb
Figure 4: Monotonic combination of two hypotheses dur-
ing the inside pass involves scoring the fluency of the con-
catenation with the language model.
inside score of the new item is incremented by:
iS(lb-iAj-rc) += p(X ? [Y Z]) ? iS(lb-iBk-rb) ?
iS(lc-kCj-rc) ? LM(rb, lc)
This process is also illustrated in Figure 4. Of
course, we also loop over the split point k and ap-
ply the other two rule types (inverted concatenation,
terminal generation). We omit those cases from this
exposition, as well as the update for the outside pass;
they are standard and similar. Once we have com-
puted the inside and outside scores, we compute pos-
terior probabilities for all items:
p(la-iAj-ra) =
iS(la-iAj-ra)oS(la-iAj-ra)
iS(root)
where iS(root) is sum of all translations? scores.
States with low posteriors are then pruned away.
We proceed to compute inside/outside score in the
next, more refined search space, using the projec-
tions pii?i?1 to map between states in Si and Si?1.
In each pass, we skip all items whose projection into
the previous stage had a probability below a stage-
specific threshold. This process is illustrated in Fig-
ure 3. When we reach the most refined search space
S?, we do not prune, but rather extract the Viterbi
derivation instead.1
4 Learning Coarse Languages
Central to our encoding-based projections (see Sec-
tion 2.2) are hierarchical clusterings of the tar-
get language vocabulary. In the present work,
these clusterings are each k-bit encodings and yield
sequences of coarse language models LMk and
phrasetables PTk.
1Other final decoding strategies are possible, of course, in-
cluding variational methods and minimum-risk methods (Zhang
and Gildea, 2008).
Given a hierarchical clustering, we estimate the
corresponding LMk from a corpus obtained by re-
placing each token in a target language corpus with
the appropriate word cluster. As with our original
refined language model, we estimate each coarse
language model using the SRILM toolkit (Stolcke,
2002). The phrasetables PTk are similarly estimated
by replacing the words on the target side of each
phrase pair with the corresponding cluster. This pro-
cedure can potentially map two distinct phrase pairs
to the same coarse translation. In such cases we keep
only one coarse phrase pair and sum the scores of the
colliding originals.
There are many possible schemes for creating hi-
erarchical clusterings. Here, we consider several di-
visive clustering methods, where coarse word clus-
ters are recursively split into smaller subclusters.
4.1 Random projections
The simplest approach to splitting a cluster is to ran-
domly assign each word type to one of two new sub-
clusters. Random projections have been shown to be
a good and computationally inexpensive dimension-
ality reduction technique, especially for high dimen-
sional data (Bingham andMannila, 2001). Although
our best performance does not come from random
projections, we still obtain substantial speed-ups
over a single pass fine decoder when using random
projections in coarse passes.
4.2 Frequency clustering
In frequency clustering, we allocate words to clus-
ters by frequency. At each level, the most frequent
words go into one cluster and the rarest words go
into another one. Concretely, we sort the words in
a given cluster by frequency and split the cluster so
that the two halves have equal token mass. This ap-
proach can be seen as a radically simplified version
of Brown et al (1992). It can, and does, result in
highly imbalanced cluster hierarchies.
4.3 HMM clustering
An approach found to be effective by Petrov and
Klein (2007) for coarse-to-fine parsing is to use
likelihood-based hierarchical EM training. We
adopt this approach here by identifying each clus-
ter with a latent state in an HMM and determiniz-
ing the emissions so that each word type is emitted
112
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 0  1  2  3  4  5  6  7  8  9  10
Per
ple
xity
Number of bits in coarse language model
HMMJClusterFrequencyRandom
Figure 5: Results of coarse language model perplexity
experiment (see Section 4.5). HMM and JClustering have
lower perplexity than frequency and random clustering
for all number of bits in the language encoding.
by only one state. When splitting a cluster s into
s1 and s2, we initially clone and mildly perturb its
corresponding state. We then use EM to learn pa-
rameters, which splits the state, and determinize the
result. Specifically, each word w is assigned to s1 if
P (w|s1) > P (w|s2) and s2 otherwise. Because of
this determinization after each round of EM, a word
in one cluster will be allocated to exactly one of that
cluster?s children. This process not only guarantees
that the clusters are hierarchical, it also avoids the
state drift discussed by Petrov and Klein (2007). Be-
cause the emissions are sparse, learning is very effi-
cient. An example of some of the words associated
with early splits can be seen in Figure 1.
4.4 JCluster
Goodman (2001) presents a clustering scheme
which aims to minimize the entropy of a word given
a cluster. This is accomplished by incrementally
swapping words between clusters to locally mini-
mize entropy.2 This clustering algorithm was devel-
oped with a slightly different application in mind,
but fits very well into our framework, because the
hierarchical clusters it produces are trained to maxi-
mize predictive likelihood.
4.5 Clustering Results
We applied the above clustering algorithms to our
monolingual language model data to obtain hierar-
2The software for this clustering technique is available at
http://research.microsoft.com/?joshuago/.
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000  100000
BL
EU
Total time in seconds
HMMJClusterFrequenceRandomSingle pass (no clustering)
Figure 6: Coarse-to-fine decoding with HMM or JClus-
tering coarse language models reduce decoding times
while increasing accuracy.
chical clusters. We then trained coarse language
models of varying granularity and evaluated them on
a held-out set. To measure the quality of the coarse
language models we use perplexity (exponentiated
cross-entropy).3 Figure 5 shows that HMM clus-
tering and JClustering have lower perplexity than
frequency and random based clustering for all com-
plexities. In the next section we will present a set of
machine translation experiments using these coarse
language models; the clusterings with better per-
plexities generally produce better decoders.
5 Experiments
We ran our experiments on the Europarl corpus
(Koehn, 2005) and show results on Spanish, French
and German to English translation. We used the
setup and preprocessing steps detailed in the 2008
Workshop on Statistical Machine Translation.4 Our
baseline decoder uses an ITG with an integrated tri-
gram language model. Phrase translation parame-
ters are learned from parallel corpora with approx-
imately 8.5 million words for each of the language
pairs. The English language model is trained on the
entire corpus of English parliamentary proceedings
provided with the Europarl distribution. We report
results on the 2000 development test set sentences
of length up to 126 words (average length was 30
words).
3We assumed that each cluster had a uniform distribution
over all the words in that cluster.
4See http://www.statmt.org/wmt08 for details.
113
 0
 50
 100
 150
 200
 250
 300
1-2-3-f1-3-f2-3-f1-f2-f3-f4-ff
Tot
al t
ime
 in 
min
ute
s
Language model bits for coarse passes
fine4 bits3 bits2 bits1 bit
Figure 7: Many passes with extremely simple language
models produce the highest speed-ups.
Our ITG translation model is broadly competitive
with state-of-the-art phrase-based-models trained on
the same data. For example, on the Europarl devel-
opment test set, we fall short of Moses (Koehn et al,
2007) by less than one BLEU point. On Spanish-
English we get 29.47 BLEU (compared to Moses?s
30.40), on French-English 29.34 (vs. 29.95), and
23.80 (vs. 24.64) on German-English. These differ-
ences can be attributed primarily to the substantially
richer distortion model used by Moses.
The multipass coarse-to-fine architecture that we
have introduced presents many choice points. In
the following, we investigate various axes individu-
ally. We present our findings as BLEU-to-time plots,
where the tradeoffs were generated by varying the
complexity and the number of coarse passes, as well
as the pruning thresholds and beam sizes. Unless
otherwise noted, the experiments are on Spanish-
English using trigram language models. When
different decoder settings are applied to the same
model, MERT weights (Och, 2003) from the unpro-
jected single pass setup are used and are kept con-
stant across runs. In particular, the same MERT
weights are used for all coarse passes; note that this
slightly disadvantages the multipass runs, which use
MERT weights optimized for the single pass de-
coder.
5.1 Clustering
In section Section 4, HMM clustering and JCluster-
ing gave lower perplexities than frequency and ran-
dom clustering when using the same number of bits
for encoding the language model. To test how these
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 29.6
 100  1000  10000  100000
BL
EU
Total time in seconds
Encoding+OrderOrderEncodingSingle pass
Figure 8: A combination of order-based and encoding-
based coarse-to-fine decoding yields the best results.
models perform at pruning, we ran our decoder sev-
eral times, varying only the clustering source. In
each case, we used a 2-bit trigram model as a sin-
gle coarse pass, followed by a fine output pass. Fig-
ure 6 shows that we can obtain significant improve-
ments over the single-pass baseline regardless of the
clustering. To no great surprise, HMM clustering
and JClustering yield better results, giving a 30-fold
speed-up at the same accuracy, or improvements of
about 0.3 BLEU when given the same time as the
single pass decoder. We discuss this increase in ac-
curacy over the baseline in Section 5.5. Since the
performance differences between those two cluster-
ing algorithms are negligible, we will use the sim-
pler HMM clustering in all subsequent experiments.
5.2 Spacing
Given a hierarchy of coarse language models, all
trigam for the moment, we need to decide on the
number of passes and the granularity of the coarse
language models used in each pass. Figure 7 shows
how decoding time varies for different multipass
schemes to achieve the same translation quality.
A single coarse pass with a 4-bit language model
cuts decoding time almost in half. However, one
can further cut decoding time by starting with even
coarser language models. In fact, the best results
are achieved by decoding in sequence with 1-, 2-
and 3-bit language models before running the final
fine trigram pass. Interestingly, in this setting, each
pass takes about the same amount of time. A simi-
lar observation was reported in the parsing literature,
where coarse-to-fine inference with multiple passes
114
 28 28.2
 28.4 28.6
 28.8 29
 29.2 29.4
 29.6
 100  1000  10000
BLE
U
Total time in seconds
Spanish
Coarse-To-FineFine Baseline  28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000
BLE
U
Total time in seconds
French
Coarse-To-FineFine Baseline  22
 22.5
 23
 23.5
 24
 100  1000  10000
BLE
U
Total time in seconds
German
Coarse-To-FineFine Baseline
Figure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to better
BLEU scores on all language pairs and for all parameter settings.
of roughly equal complexity produces tremendous
speed-ups (Petrov and Klein, 2007).
5.3 Encoding vs. Order
As described in Section 2, the language model com-
plexity can be reduced either by decreasing the vo-
cabulary size (encoding-based projection) or by low-
ering the language model order from trigram to bi-
gram (order-based projection). Figure 7 shows that
both approaches alone yield comparable improve-
ments over the single pass baseline. Fortunately,
the two approaches are complimentary, allowing us
to obtain further improvements by combining both.
We found it best to first do a series of coarse bigram
passes, followed by a fine bigram pass, followed by
a fine trigram pass.
5.4 Final Results
Figure 9 compares our multipass coarse-to-fine de-
coder using language refinement to single pass de-
coding on three different languages. On each lan-
guage we get significant improvements in terms of
efficiency as well as accuracy. Overall, we can
achieve up to 50-fold speed-ups at the same accu-
racy, or alternatively, improvements of 0.4 BLEU
points over the best single pass run.
In absolute terms, our decoder translates on aver-
age about two Spanish sentences per second at the
highest accuracy setting.5 This compares favorably
to the Moses decoder (Koehn et al, 2007), which
takes almost three seconds per sentence.
5Of course, the time for an average sentence is much lower,
since long sentences dominate the overall translation time.
5.5 Search Error Analysis
In multipass coarse-to-fine decoding, we noticed
that in addition to computational savings, BLEU
scores tend to improve. A first hypothesis is
that coarse-to-fine decoding simply improves search
quality, where fewer good items fall off the beam
compared to a simple fine pass. However, this hy-
pothesis turns out to be incorrect. Table 1 shows
the percentage of test sentences for which the BLEU
score or log-likelihood changes when we switch
from single pass decoding to coarse-to-fine multi-
pass decoding. Only about 30% of the sentences
get translated in the same way (if much faster) with
coarse-to-fine decoding. For the rest, coarse-to-fine
decoding mostly finds translations with lower likeli-
hood, but higher BLEU score, than single pass de-
coding.6 An increase of the underlying objectives of
interest when pruning despite an increase in model-
score search errors has also been observed in mono-
lingual coarse-to-fine syntactic parsing (Charniak et
al., 1998; Petrov and Klein, 2007). This effect may
be because coarse-to-fine approximates certain min-
imum Bayes risk objective. It may also be an effect
of model intersection between the various passes?
models. In any case, both possibilities are often per-
fectly desirable. It is also worth noting that the num-
ber of search errors incurred in the coarse-to-fine
approach can be dramatically reduced (at the cost
of decoding time) by increasing the pruning thresh-
olds. However, the fortuitous nature of coarse-to-
fine search errors seems to be a substantial and de-
sirable effect.
6We compared the influence of multipass decoding on the
TM score and the LM score; both decrease.
115
LL
> = <
B
L
E
U > 3.6% - 26.3%
= 1.5% 29.6 % 12.9 %
< 2.2% - 24.1%
Table 1: Percentage of sentences for which the BLEU
score/log-likelihood improves/drops during coarse-to-
fine decoding (compared to single pass decoding).
6 Conclusions
We have presented a coarse-to-fine syntactic de-
coder which utilizes a novel encoding-based lan-
guage projection in conjunction with order-based
projections to achieve substantial speed-ups. Un-
like A* methods, a posterior pruning approach al-
lows multiple passes, which we found to be very
beneficial for total decoding time. When aggres-
sively pruned, coarse-to-fine decoding can incur ad-
ditional search errors, but we found those errors to
be fortuitous more often than harmful. Our frame-
work applies equally well to other translation sys-
tems, though of course interesting new challenges
arise when, for example, the underlying SCFGs be-
come more complex.
References
E. Bingham and H.i Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In KDD ?01.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ?06.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ?05.
J. Goodman. 1997. Global thresholding and multiple-
pass parsing. In EMNLP ?97.
J. Goodman. 2001. A bit of progress in language model-
ing. Technical report, Microsoft Research.
A. Haghighi, J. DeNero, and D. Klein. 2007. A* search
via approximate factoring. In NAACL ?07.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL
?07.
D. Klein and C. Manning. 2003. A* parsing: fast exact
viterbi parse selection. In NAACL ?03.
P. Koehn, H. Hoang, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL ?07.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In ACL ?04.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In ICSLP ?02.
A. Venugopal, A. Zollmann, and S. Vogel. 2007. An ef-
ficient two-pass approach to synchronous-CFG driven
statistical MT. In HLT-NAACL ?07.
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In ACL ?96.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. In
Computational Linguistics.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL ?03.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
ACL ?08.
116
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314?323,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sampling Alignment Structure under a Bayesian Translation Model
John DeNero, Alexandre Bouchard-Co?te? and Dan Klein
Computer Science Department
University of California, Berkeley
{denero, bouchard, klein}@cs.berkeley.edu
Abstract
We describe the first tractable Gibbs sam-
pling procedure for estimating phrase pair
frequencies under a probabilistic model of
phrase alignment. We propose and evalu-
ate two nonparametric priors that successfully
avoid the degenerate behavior noted in previ-
ous work, where overly large phrases mem-
orize the training data. Phrase table weights
learned under our model yield an increase in
BLEU score over the word-alignment based
heuristic estimates used regularly in phrase-
based translation systems.
1 Introduction
In phrase-based translation, statistical knowledge
of translation equivalence is primarily captured by
counts of how frequently various phrase pairs occur
in training bitexts. Since bitexts do not come seg-
mented and aligned into phrase pairs, these counts
are typically gathered by fixing a word alignment
and applying phrase extraction heuristics to this
word-aligned training corpus. Alternatively, phrase
pair frequencies can be learned via a probabilistic
model of phrase alignment, but this approach has
presented several practical challenges.
In this paper, we address the two most signifi-
cant challenges in phrase alignment modeling. The
first challenge is with inference: computing align-
ment expectations under general phrase models is
#P-hard (DeNero and Klein, 2008). Previous phrase
alignment work has sacrificed consistency for effi-
ciency, employing greedy hill-climbing algorithms
and constraining inference with word alignments
(Marcu and Wong, 2002; DeNero et al, 2006; Birch
et al, 2006). We describe a Gibbs sampler that con-
sistently and efficiently approximates expectations,
using only polynomial-time computable operators.
Despite the combinatorial complexity of the phrase
alignment space, our sampled phrase pair expecta-
tions are guaranteed to converge to the true poste-
rior distributions under the model (in theory) and do
converge to effective values (in practice).
The second challenge in learning phrase align-
ments is avoiding a degenerate behavior of the gen-
eral model class: as with many models which can
choose between large and small structures, the larger
structures win out in maximum likelihood estima-
tion. Indeed, the maximum likelihood estimate of
a joint phrase alignment model analyzes each sen-
tence pair as one large phrase with no internal struc-
ture (Marcu andWong, 2002). We describe two non-
parametric priors that empirically avoid this degen-
erate solution.
Fixed word alignments are used in virtually ev-
ery statistical machine translation system, if not to
extract phrase pairs or rules directly, then at least
to constrain the inference procedure for higher-level
models. We estimate phrase translation features
consistently using an inference procedure that is not
constrained by word alignments, or any other heuris-
tic. Despite this substantial change in approach, we
report translation improvements over the standard
word-alignment-based heuristic estimates of phrase
table weights. We view this result as an important
step toward building fully model-based translation
systems that rely on fewer procedural heuristics.
2 Phrase Alignment Model
While state-of-the-art phrase-based translation sys-
tems include an increasing number of features,
translation behavior is largely driven by the phrase
pair count ratios ?(e|f) and ?(f |e). These features
are typically estimated heuristically using the counts
c(?e, f?) of all phrase pairs in a training corpus that
are licensed by word alignments:
?(e|f) =
c(?e, f?)
?
e? c(?e
?, f?)
.
314
Gracias
,
lo
har?
de
muy
buen
grado
.
you
do so
Thank , I shall
gladly
.
you
do so
Thank , I shall
gladly
.
Gracias
,
lo
har?
de
muy
buen
grado
.
(a) example word alignment (b) example phrase alignment
Figure 1: In this corpus example, the phrase
alignment model found the non-literal translation
pair ?gladly, de muy buen grado? while heuristically-
combined word alignment models did not. (a) is a grow-
diag-final-and combined IBM Model 4 word alignment;
(b) is a phrase alignment under our model.
In contrast, a generative model that explicitly
aligns pairs of phrases ?e, f? gives us well-founded
alternatives for estimating phrase pair scores. For
instance, we could use the model?s parameters as
translation features. In this paper, we compute the
expected counts of phrase pairs in the training data
according to our model, and derive features from
these expected counts. This approach endows phrase
pair scores with well-defined semantics relative to a
probabilistic model. Practically, phrase models can
discover high-quality phrase pairs that often elude
heuristics, as in Figure 1. In addition, the model-
based approach fits neatly into the framework of sta-
tistical learning theory for unsupervised problems.
2.1 Generative Model Description
We first describe the symmetric joint model of
Marcu and Wong (2002), which we will extend. A
two-step generative process constructs an ordered
set of English phrases e1:m, an ordered set of for-
eign phrases f1:n, and a phrase-to-phrase alignment
between them, a = {(j, k)} indicating that ?ej , fk?
is an aligned pair.
1. Choose a number of components ` and generate
each of ` phrase pairs independently.
2. Choose an ordering for the phrases in the for-
eign language; the ordering for English is fixed
by the generation order.1
1We choose the foreign to reorder without loss of generality.
In this process, m = n = |a|; all phrases in both
sentences are aligned one-to-one.
We parameterize the choice of ` using a geometric
distribution, denoted PG, with stop parameter p$:
P (`) = PG(`; p$) = p$ ? (1 ? p$)
`?1 .
Each aligned phrase pair ?e, f? is drawn from a
multinomial distribution ?J which is unknown. We
fix a simple distortion model, setting the probability
of a permutation of the foreign phrases proportional
to the product of position-based distortion penalties
for each phrase:
P (a|{?e, f?}) ?
?
a?a
?(a)
?(a = (j, k)) = b|pos(ej)?pos(fk)?s| ,
where pos(?) denotes the word position of the start
of a phrase, and s the ratio of the length of the En-
glish to the length of the foreign sentence. This po-
sitional distortion model was deemed to work best
by Marcu and Wong (2002).
We can now state the joint probability for a
phrase-aligned sentence consisting of ` phrase pairs:
P ({?e, f?}, a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
?J(?e, f?) .
While this model has several free parameters in ad-
dition to ?J, we fix them to reasonable values to fo-
cus learning on the phrase pair distribution.2
2.2 Unaligned Phrases
Sentence pairs do not always contain equal informa-
tion on both sides, and so we revise the generative
story to include unaligned phrases in both sentences.
When generating each component of a sentence pair,
we first decide whether to generate an aligned phrase
pair or, with probability p?, an unaligned phrase.3
Then, we either generate an aligned phrase pair from
?J or an unaligned phrase from ?N, where ?N is a
multinomial over phrases. Now, when generating
e1:m, f1:n and alignment a, the number of phrases
m+ n can be greater than 2 ? |a|.
2Parameters were chosen by hand during development on a
small training corpus. p$ = 0.1, b = 0.85 in experiments.
3We strongly discouraged unaligned phrases in order to
align as much of the corpus as possible: p? = 10?10 in ex-
periments.
315
To unify notation, we denote unaligned phrases as
phrase pairs with one side equal to null: ?e, null? or
?null, f?. Then, the revised model takes the form:
P ({?e, f?},a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
PM(?e, f?)
PM(?e, f?) = p??N(?e, f?) + (1 ? p?)?J(?e, f?) .
In this definition, the distribution ?N gives non-
zero weight only to unaligned phrases of the form
?e, null? or ?null, f?, while ?J gives non-zero
weight only to aligned phrase pairs.
3 Model Training and Expectations
Our model involves observed sentence pairs, which
in aggregate we can call x, latent phrase segmenta-
tions and alignments, which we can call z, and pa-
rameters ?J and ?N, which together we can call ?.
A model such as ours could be used either for the
learning of the key phrase pair parameters in ?, or
to compute expected counts of phrase pairs in our
data. These two uses are very closely related, but
we focus on the computation of phrase pair expecta-
tions. For exposition purposes, we describe a Gibbs
sampling algorithm for computing expected counts
of phrases under P (z|x, ?) for fixed ?. Such ex-
pectations would be used, for example, to compute
maximum likelihood estimates in the E-step of EM.
In Section 4, we instead compute expectations under
P (z|x), with ? marginalized out entirely.
In a Gibbs sampler, we start with a complete
phrase segmentation and alignment, state z0, which
sets all latent variables to some initial configuration.
We then produce a sequence of sample states zi,
each of which differs from the last by some small
local change. The samples zi are guaranteed (in the
limit) to consistently approximate the conditional
distribution P (z|x, ?) (or P (z|x) later). Therefore,
the average counts of phrase pairs in the samples
converge to expected counts under the model. Nor-
malizing these expected counts yields estimates for
the features ?(e|f) and ?(f |e).
Gibbs sampling is not new to the natural language
processing community (Teh, 2006; Johnson et al,
2007). However, it is usually used as a search pro-
cedure akin to simulated annealing, rather than for
approximating expectations (Goldwater et al, 2006;
Finkel et al, 2007). Our application is also atypical
for an NLP application in that we use an approxi-
mate sampler not only to include Bayesian prior in-
formation (section 4), but also because computing
phrase alignment expectations exactly is a #P-hard
problem (DeNero and Klein, 2008). That is, we
could not run EM exactly, even if we wanted maxi-
mum likelihood estimates.
3.1 Related Work
Expected phrase pair counts under P (z|x, ?) have
been approximated before in order to run EM.
Marcu and Wong (2002) employed local search
from a heuristic initialization and collected align-
ment counts during a hill climb through the align-
ment space. DeNero et al (2006) instead proposed
an exponential-time dynamic program pruned using
word alignments. Subsequent work has relied heav-
ily on word alignments to constrain inference, even
under reordering models that admit polynomial-time
E-steps (Cherry and Lin, 2007; Zhang et al, 2008).
None of these approximations are consistent, and
they offer no method of measuring their biases.
Gibbs sampling is not only consistent in the limit,
but also allows us to add Bayesian priors conve-
niently (section 4). Of course, sampling has liabili-
ties as well: we do not know in advance how long we
need to run the sampler to approximate the desired
expectations ?closely enough.?
Snyder and Barzilay (2008) describe a Gibbs sam-
pler for a bilingual morphology model very similar
in structure to ours. However, the basic sampling
step they propose ? resampling all segmentations
and alignments for a sequence at once ? requires a
#P-hard computation. While this asymptotic com-
plexity was apparently not prohibitive in the case of
morphological alignment, where the sequences are
short, it is prohibitive in phrase alignment, where the
sentences are often very long.
3.2 Sampling with the SWAP Operator
Our Gibbs sampler repeatedly applies each of five
operators to each position in each training sentence
pair. Each operator freezes all of the current state zi
except a small local region, determines all the ways
that region can be reconfigured, and then chooses a
(possibly) slightly different zi+1 from among those
outcomes according to the conditional probability of
each, given the frozen remainder of the state. This
316
frozen region of the state is called a Markov blanket
(denoted m), and plays a critical role in proving the
correctness of the sampler.
The first operator we consider is SWAP, which
changes alignments but not segmentations. It freezes
the set of phrases, then picks two English phrases e1
and e2 (or two foreign phrases, but we focus on the
English case). All alignments are frozen except the
phrase pairs ?e1, f1? and ?e2, f2?. SWAP chooses be-
tween keeping ?e1, f1? and ?e2, f2? aligned as they
are (outcome o0), or swapping their alignments to
create ?e1, f2? and ?e2, f1? (outcome o1).
SWAP chooses stochastically in proportion to
each outcome?s posterior probability: P (o0|m,x, ?)
and P (o1|m,x, ?). Each phrase pair in each out-
come contributes to these posteriors the probability
of adding a new pair, deciding whether it is null, and
generating the phrase pair along with its contribu-
tion to the distortion probability. This is all captured
in a succinct potential function ?(?e, f?) =
{
(1?p$) (1?p?) ?J(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise
.
Thus, outcome o0 is chosen with probability
P (o0|m,x, ?) =
?(?e1, f1?)?(?e2, f2?)
?(?e1, f1?)?(?e2, f2?) + ?(?e1, f2?)?(?e2, f1?)
.
Operators in a Gibbs sampler require certain con-
ditions to guarantee the correctness of the sampler.
First, they must choose among all possible configu-
rations of the unfrozen local state. Second, imme-
diately re-applying the operator from any outcome
must yield the same set of outcome options as be-
fore.4 If these conditions are not met, the sampler
may no longer be guaranteed to yield consistent ap-
proximations of the posterior distribution.
A subtle issue arises with SWAP as defined:
should it also consider an outcome o2 of ?e1, null?
and ?e2, null? that removes alignments? No part
of the frozen state is changed by removing these
alignments, so the first Gibbs condition dictates that
we must include o2. However, after choosing o2,
when we reapply the operator to positions e1 and
4These are two sufficient conditions to guarantee that the
Metropolis-Hastings acceptance ratio of the sampling step is 1.
(b) FLIP(a) SWAP
(c) TOGGLE
(d) FLIP TWO
(e) MOVE
Figure 2: Each local operator manipulates a small portion
of a single alignment. Relevant phrases are exaggerated
for clarity. The outcome sets (depicted by arrows) of each
possible configuration are fully connected. Certain con-
figurations cannot be altered by certain operators, such as
the final configuration in SWAP. Unalterable configura-
tions for TOGGLE have been omitted for space.
e2, we freeze all alignments except ?e1, null? and
?e2, null?, which prevents us from returning to o0.
Thus, we fail to satisfy the second condition. This
point is worth emphasizing because some prior work
has treated Gibbs sampling as randomized search
and, intentionally or otherwise, proposed inconsis-
tent operators.
Luckily, the problem is not with SWAP, but with
our justification of it: we can salvage SWAP by aug-
menting its Markov blanket. Given that we have se-
lected ?e1, f1? and ?e2, f2?, we not only freeze all
other alignments and phrase boundaries, but also the
number of aligned phrase pairs. With this count held
invariant, o2 is not among the possible outcomes of
SWAP given m. Moreover, regardless of the out-
come chosen, SWAP can immediately be reapplied
at the same location with the same set of outcomes.
All the possible starting configurations and out-
come sets for SWAP appear in Figure 2(a).
317
The boys are
Ellos
comen
Current State
Includes segmentations
and alignments for all
sentence pairs
Markov Blanket
Freezes most of the
segmentations and 
alignments, along with 
the alignment count
Outcomes
An exhaustive set of 
possibilities given 
the Markov blanket
eating
? ?
Apply the FLIP operator 
to English position 1
1
Compute the conditional 
probability of each outcome
2
Finally, select a new state proportional 
to its conditional probability
3
?
Figure 3: The three steps involved in applying the FLIP
operator. The Markov blanket freezes all segmentations
except English position 1 and all alignments except those
for Ellos and The boys. The blanket alo freezes the num-
ber of alignments, which disallows the lower right out-
come.
3.3 The FLIP operator
SWAP can arbitrarily shuffle alignments, but we
need a second operator to change the actual phrase
boundaries. The FLIP operator changes the status of
a single segmentation position5 to be either a phrase
boundary or not. In this sense FLIP is a bilingual
analog of the segmentation boundary flipping oper-
ator of Goldwater et al (2006).
Figure 3 diagrams the operator and its Markov
blanket. First, FLIP chooses any between-word po-
sition in either sentence. The outcome sets for FLIP
vary based on the current segmentation and adjacent
alignments, and are depicted in Figure 2.
Again, for FLIP to satisfy the Gibbs conditions,
we must augment its Markov blanket to freeze not
only all other segmentation points and alignments,
but also the number of aligned phrase pairs. Oth-
erwise, we end up allowing outcomes from which
5A segmentation position is a position between two words
that is also potentially a boundary between two phrases in an
aligned sentence pair.
we cannot return to the original state by reapply-
ing FLIP. Consequently, when a position is already
segmented and both adjacent phrases are currently
aligned, FLIP cannot unsegment the point because
it can?t create two aligned phrase pairs with the one
larger phrase that results (see bottom of Figure 2(b)).
3.4 The TOGGLE operator
Both SWAP and FLIP freeze the number of align-
ments in a sentence. The TOGGLE operator, on the
other hand, can add or remove individual alignment
links. In TOGGLE, we first choose an e1 and f1. If
?e1, f1? ? a or both e1 and f1 are null, we freeze
all segmentations and the rest of the alignments, and
choose between including ?e1, f1? in the alignment
or leaving both e1 and f1 unaligned. If only one of
e1 and f1 are aligned, or they are not aligned to each
other, then TOGGLE does nothing.
3.5 A Complete Sampler
Together, FLIP, SWAP and TOGGLE constitute a
complete Gibbs sampler that consistently samples
from the posterior P (z|x, ?). Not only are these
operators valid Gibbs steps, but they also can form
a path of positive probability from any source state
to any target state in the space of phrase alignments
(formally, the induced Markov chain is irreducible).
Such a path can at worst be constructed by unalign-
ing all phrases in the source state with TOGGLE,
composing applications of FLIP to match the target
phrase boundaries, then applying TOGGLE to match
the target algnments.
We include two more local operators to speed up
the rate at which the sampler explores the hypothesis
space. In short, FLIP TWO simultaneously flips an
English and a foreign segmentation point (to make a
large phrase out of two smaller ones or vice versa),
while MOVE shifts an aligned phrase boundary to
the left or right. We omit details for lack of space.
3.6 Phrase Pair Count Estimation
With our sampling procedure in place, we can now
estimate the expected number of times a given
phrase pair occurs in our data, for fixed ?, using a
Monte-Carlo average,
1
N
N?
i=1
count?e,f?(x, zi)
a.s.
?? E
[
count?e,f?(x, ?)
]
.
318
The left hand side is simple to compute; we count
aligned phrase pairs in each sample we generate.
In practice, we only count phrase pairs after apply-
ing every operator to every position in every sen-
tence (one iteration).6 Appropriate normalizations
of these expected counts can be used either in an M-
step as maximum likelihood estimates, or to com-
pute values for features ?(f |e) and ?(e|f).
4 Nonparametric Bayesian Priors
The Gibbs sampler we presented addresses the infer-
ence challenges of learning phrase alignment mod-
els. With slight modifications, it also enables us to
include prior information into the model. In this sec-
tion, we treat ? as a random variable and shape its
prior distribution in order to correct the well-known
degenerate behavior of the model.
4.1 Model Degeneracy
The structure of our joint model penalizes explana-
tions that use many small phrase pairs. Each phrase
pair token incurs the additional expense of genera-
tion and distortion. In fact, the maximum likelihood
estimate of the model puts mass on ?e, f? pairs that
span entire sentences, explaining the training corpus
with one phrase pair per sentence.
Previous phrase alignment work has primarily
mitigated this tendency by constraining the in-
ference procedure, for example with word align-
ments and linguistic features (Birch et al, 2006),
or by disallowing large phrase pairs using a non-
compositional constraint (Cherry and Lin, 2007;
Zhang et al, 2008). However, the problem lies with
the model, and therefore should be corrected in the
model, rather than the inference procedure.
Model-based solutions appear in the literature as
well, though typically combined with word align-
ment constraints on inference. A sparse Dirichlet
prior coupled with variational EM was explored by
Zhang et al (2008), but it did not avoid the degen-
erate solution. Moore and Quirk (2007) proposed a
new conditional model structure that does not cause
large and small phrases to compete for probabil-
ity mass. May and Knight (2007) added additional
model terms to balance the cost of long and short
derivations in a syntactic alignment model.
6For experiments, we ran the sampler for 100 iterations.
4.2 A Dirichlet Process Prior
We control this degenerate behavior by placing a
Dirichlet process (DP) prior over ?J, the distribution
over aligned phrase pairs (Ferguson, 1973).
If we were to assume a maximum number K of
phrase pair types, a (finite) Dirichlet distribution
would be an appropriate prior. A draw from a K-
dimensional Dirichlet distribution is a list of K real
numbers in [0, 1] that sum to one, which can be in-
terpreted as a distribution overK phrase pair types.
However, since the event space of possible phrase
pairs is in principle unbounded, we instead use a
Dirichlet process. A draw from a DP is a countably
infinite list of real numbers in [0, 1] that sum to one,
which we interpret as a distribution over a countably
infinite list of phrase pair types.7
The Dirichlet distribution and the DP distribution
have similar parameterizations. A K-dimensional
Dirichlet can be parameterized with a concentration
parameter ? > 0 and a base distribution M0 =
(?1, . . . , ?K?1), with ?i ? (0, 1).8 This parameteri-
zation has an intuitive interpretation: under these pa-
rameters, the average of independent samples from
the Dirichlet will converge toM0. That is, the aver-
age of the ith element of the samples will converge
to ?i. Hence, the base distributionM0 characterizes
the sample mean. The concentration parameter ?
only affects the variance of the draws.
Similarly, we can parameterize the Dirichlet pro-
cess with a concentration parameter ? (that affects
only the variance) and a base distribution M0 that
determines the mean of the samples. Just as in the
finite Dirichlet case,M0 is simply a probability dis-
tribution, but now with countably infinite support:
all possible phrase pairs in our case. In practice, we
can use an unnormalized M0 (a base measure) by
appropriately rescaling ?.
In our model, we select a base measure that
strongly prefers shorter phrases, encouraging the
model to use large phrases only when it has suffi-
cient evidence for them. We continue the model:
7Technical note: to simplify exposition, we restrict the dis-
cussion to settings such as ours where the base measure of the
DP has countable support.
8This parametrization is equivalent to the standard pseudo-
counts parametrization of K positive real numbers. The bi-
jection is given by ? =
PK
i=1 ??i and ?i = ??i/?, where
(??1, . . . , ??K) are the pseudo-counts.
319
?J ? DP (M0, ?)
M0(?e, f?) = [Pf (f)PWA(e|f) ? Pe(e)PWA(f |e)]
1
2
Pf (f) = PG(|f |; ps) ?
(
1
nf
)|f |
Pe(e) = PG(|e|; ps) ?
(
1
ne
)|e|
.
.
PWA is the IBM model 1 likelihood of one phrase
conditioned on the other (Brown et al, 1994). Pf
and Pe are uniform over types for each phrase
length: the constants nf and ne denote the vocab-
ulary size of the foreign and English languages, re-
spectively, and PG is a geometric distribution.
Above, ?J is drawn from a DP centered on the ge-
ometric mean of two joint distributions over phrase
pairs, each of which is composed of a monolingual
unigram model and a lexical translation component.
This prior has two advantages. First, we pressure
the model to use smaller phrases by increasing ps
(ps = 0.8 in experiments). Second, we encour-
age good phrase pairs by incorporating IBM Model
1 distributions. This use of word alignment distri-
butions is notably different from lexical weighting
or word alignment constraints: we are supplying
prior knowledge that phrases will generally follow
word alignments, though with enough corpus evi-
dence they need not (and often do not) do so in the
posterior samples. The model proved largely insen-
sitive to changes in the sparsity parameter ?, which
we set to 100 for experiments.
4.3 Unaligned phrases and the DP Prior
Introducing unaligned phrases invites further degen-
erate megaphrase behavior: a sentence pair can be
generated cheaply as two unaligned phrases that
each span an entire sentence. We attempted to place
a similar DP prior over ?N, but surprisingly, this
modeling choice invoked yet another degenerate be-
havior. The DP prior imposes a rich-get-richer prop-
erty over the phrase pair distribution, strongly en-
couraging the model to reuse existing pairs rather
than generate new ones. As a result, common
words consistently aligned to null, even while suit-
able translations were present, simply because each
null alignment reinforced the next. For instance, the
was always unaligned.
Instead, we fix ?N to a simple unigram model that
is uniform over word types. This way, we discour-
age unaligned phrases while focusing learning on ?J.
For simplicity, we reuse Pf (f) and Pe(e) from the
prior over ?J.
?N(?e, f?) =
{
1
2 ? Pe(e) if f = null
1
2 ? Pf (f) if e = null .
The 12 represents a choice of whether the aligned
phrase is in the foreign or English sentence.
4.4 Collapsed Sampling with a DP Prior
Our entire model now has the general form
P (x, z, ?J); all other model parameters have been
fixed. Instead of searching for a suitable ?J,9 we
sample from the posterior distribution P (z|x) with
?J marginalized out.
To this end, we convert our Gibbs sampler into
a collapsed Gibbs sampler10 using the Chinese
Restaurant Process (CRP) representation of the DP
(Aldous, 1985). With the CRP, we avoid the prob-
lem of explicitely representing samples from the
DP. CRP-based samplers have served the commu-
nity well in related language tasks, such as word seg-
mentation and coreference resolution (Goldwater et
al., 2006; Haghighi and Klein, 2007).
Under this representation, the probability of each
sampling outcome is a simple expression in terms
of the state of the rest of the training corpus (the
Markov blanket), rather than explicitly using ?J.
Let zm be the set of aligned phrase pair tokens ob-
served in the rest of the corpus. Then, when ?e, f? is
aligned (that is, neither e nor f are null), the condi-
tional probability for a pair ?e, f? takes the form:
?(?e, f?|zm) =
count?e,f?(zm) + ? ?M0(?e, f?)
|zm| + ?
,
where count?e,f?(zm) is the number of times that
?e, f? appears in zm. We can write this expression
thanks to the exchangeability of the model. For fur-
ther exposition of this collapsed sampler posterior,
9For instance, using approximate MAP EM.
10A collapsed sampler is simply one in which the model pa-
rameters have been marginalized out.
320
025
50
75
100
2007 2008
1 x 1
1 x 2, 2 x 1
2 x 2
2 x 3, 3 x 2
3+ x 3+ 
0
25
50
75
100
1x1 1x2 & 2x1 1x3 & 3x1 2x2 2x3 & 3x2 3x3 and up
Minimal extracted phrases
Sampled phrases
All extracted phrases
Figure 4: The distribution of phrase pair sizes (denoted
English length x foreign length) favors small phrases un-
der the model.
see Goldwater et al (2006).11
The sampler remains exactly the same as de-
scribed in Section 3, except that the posterior con-
ditional probability of each outcome uses a revised
potential function ?DP(?e, f?) =
{
(1?p$) (1?p?) ?(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise .
?DP is like ?, but the fixed ?J is replaced with the
constantly-updated ? function.
4.5 Degeneracy Analysis
Figure 4 shows a histogram of phrase pair sizes in
the distribution of expected counts under the model.
As reference, we show the size distribution of both
minimal and all phrase pairs extracted from word
alignments using the standard heuristic. Our model
tends to select minimal phrases, only using larger
phrases when well motivated.12
This result alone is important: a model-based
solution with no inference constraint has yielded
a non-degenerate distribution over phrase lengths.
Note that our sampler does find the degenerate solu-
tion quickly under a uniform prior, confirming that
the model, and not the inference procedure, is select-
ing these small phrases.
11Note that the expression for ? changes slightly under con-
ditions where two phrase pairs being changed simultaneously
coincidentally share the same lexical content. Details of these
fringe conditions have been omitted for space, but were in-
cluded in our implementation.
12The largest phrase pair found was 13 English words by 7
Spanish words.
4.6 A Hierarchical Dirichlet Process Prior
We also evaluate a hierarchical Dirichlet process
(HDP) prior over ?J, which draws monolingual dis-
tributions ?E and ?F from a DP and ?J from their
cross-product:
?J ? DP (M
?
0, ?)
M ?0(?e, f?) = [?F(f)PWA(e|f) ? ?E(e)PWA(f |e)]
1
2
?F ? DP (Pf , ?
?)
?E ? DP (Pe, ?
?) .
This prior encourages novel phrase pairs to be com-
posed of phrases that have been used before. In the
sampler, we approximate table counts for ?E and
?F with their expectations, which can be computed
from phrase pair counts (see the appendix of Gold-
water et al (2006) for details). The HDP prior gives
a similar distribution over phrase sizes.
5 Translation Results
We evaluate our new estimates using the baseline
translation pipeline from the 2007 Statistical Ma-
chine Translation Workshop shared task.
5.1 Baseline System
We trained Moses on all Spanish-English Europarl
sentences up to length 20 (177k sentences) using
GIZA++ Model 4 word alignments and the grow-
diag-final-and combination heuristic (Koehn et al,
2007; Och and Ney, 2003; Koehn, 2002), which
performed better than any alternative combination
heuristic.13 The baseline estimates (Heuristic) come
from extracting phrases up to length 7 from the word
alignment. We used a bidirectional lexicalized dis-
tortion model that conditions on both foreign and
English phrases, along with their orientations. Our
5-gram language model was trained on 38.3 million
words of Europarl using Kneser-Ney smoothing. We
report results with and without lexical weighting,
denoted lex.
We tuned and tested on development corpora for
the 2006 translation workshop. The parameters for
each phrase table were tuned separately using min-
imum error rate training (Och, 2003). Results are
13Sampling iteration time scales quadratically with sentence
length. Short sentences were chosen to speed up our experiment
cycle.
321
Phrase Exact
Pair NIST Match
Estimate Count BLEU METEOR
Heuristic 4.4M 29.8 52.4
DP 0.6M 28.8 51.7
HDP 0.3M 29.1 52.0
DP-composed 3.7M 30.1 52.7
HDP-composed 3.1M 30.1 52.6
DP-smooth 4.8M 30.1 52.5
HDP-smooth 4.6M 30.2 52.7
Heuristic + lex 4.4M 30.5 52.9
DP-smooth + lex 4.8M 30.4 53.0
HDP-smooth + lex 4.6M 30.7 53.2
Table 1: BLEU results for learned distributions improve
over a heuristic baseline. Estimate labels are described
fully in section 5.3. The label lex indicates the addition
of a lexical weighting feature.
scored with lowercased, tokenized NIST BLEU, and
exact match METEOR (Papineni et al, 2002; Lavie
and Agarwal, 2007).
The baseline system gives a BLEU score of 29.8,
which increases to 30.5 with lex, as shown in Table
1. For reference, training on all sentences of length
less than 40 (the shared task baseline default) gives
32.4 BLEU with lex.
5.2 Learned Distribution Performance
We initialized the sampler with a configuration de-
rived from the word alignments generated by the
baseline. We greedily constructed a phrase align-
ment from the word alignment by identifying min-
imal phrase pairs consistent with the word align-
ment in each region of the sentence. We then ran
the sampler for 100 iterations through the training
data. Each iteration required 12 minutes under the
DP prior, and 30 minutes under the HDP prior. Total
running time for the HDP model neared two days on
an eight-processor machine with 16 Gb of RAM.
Estimating phrase counts under the DP prior de-
creases BLEU to 28.8, or 29.1 under the HDP prior.
This gap is not surprising: heuristic extraction dis-
covers many more phrase pairs than sampling. Note
that sacrificing only 0.7 BLEU while shrinking the
phrase table by 92% is an appealing trade-off in
resource-constrained settings.
5.3 Increasing Phrase Pair Coverage
The estimates DP-composed and HDP-composed in
Table 1 take expectations of a more liberal count
function. While sampling, we count not only aligned
phrase pairs, but also larger ones composed of two or
more contiguous aligned pairs. This count function
is similar to the phrase pair extraction heuristic, but
never includes unaligned phrases in any way. Expec-
tations of these composite phrases still have a proba-
bilistic interpretation, but they are not the structures
we are directly modeling. Notably, these estimates
outperform the baseline by 0.3 BLEU without ever
extracting phrases from word alignments, and per-
formance increases despite a reduction in table size.
We can instead increase coverage by smooth-
ing the learned estimates with the heuristic counts.
The estimates DP-smooth and HDP-smooth add
counts extracted from word alignments to the sam-
pler?s running totals, which improves performance
by 0.4 BLEU over the baseline. This smoothing bal-
ances the lower-bias sampler counts with the lower-
variance heuristics ones.
6 Conclusion
Our novel Gibbs sampler and nonparametric pri-
ors together address two open problems in learn-
ing phrase alignment models, approximating infer-
ence consistently and efficiently while avoiding de-
generate solutions. While improvements are mod-
est relative to the highly developed word-alignment-
centered baseline, we show for the first time com-
petitive results from a system that uses word align-
ments only for model initialization and smoothing,
rather than inference and estimation. We view this
milestone as critical to eventually developing a clean
probabilistic approach to machine translation that
unifies model structure across both estimation and
decoding, and decreases the use of heuristics.
References
David Aldous. 1985. Exchangeability and related topics.
In E?cole d?e?te? de probabilitie?s de Saint-Flour, Berlin.
Springer.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In The Con-
322
ference for the Association for Machine Translation in
the Americas.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Workshop on Syntax and Structure in Statistical Trans-
lation.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In The Annual Confer-
ence of the Association for Computational Linguistics:
Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics Workshop on Statistical Ma-
chine Translation.
Thomas S Ferguson. 1973. A bayesian analysis of some
nonparametric problems. In Annals of Statistics.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In The Annual Conference of the
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In The Annual Conference of the Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In The Annual Conference of the
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In The An-
nual Conference of the Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In The Annual
Conference of the Association for Computational Lin-
guistics Workshop on Statistical Machine Translation.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In The Conference on Empirical Methods in
Natural Language Processing.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics
Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In The Annual Conference of the Association
for Computational Linguistics.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In The Annual
Conference of the Association for Computational Lin-
guistics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
The Annual Conference of the Association for Compu-
tational Linguistics.
323
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867?876,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sparse Multi-Scale Grammars
for Discriminative Latent Variable Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, klein}@eecs.berkeley.edu
Abstract
We present a discriminative, latent variable
approach to syntactic parsing in which rules
exist at multiple scales of refinement. The
model is formally a latent variable CRF gram-
mar over trees, learned by iteratively splitting
grammar productions (not categories). Dif-
ferent regions of the grammar are refined to
different degrees, yielding grammars which
are three orders of magnitude smaller than
the single-scale baseline and 20 times smaller
than the split-and-merge grammars of Petrov
et al (2006). In addition, our discriminative
approach integrally admits features beyond lo-
cal tree configurations. We present a multi-
scale training method along with an efficient
CKY-style dynamic program. On a variety of
domains and languages, this method produces
the best published parsing accuracies with the
smallest reported grammars.
1 Introduction
In latent variable approaches to parsing (Matsuzaki
et al, 2005; Petrov et al, 2006), one models an ob-
served treebank of coarse parse trees using a gram-
mar over more refined, but unobserved, derivation
trees. The parse trees represent the desired output
of the system, while the derivation trees represent
the typically much more complex underlying syntac-
tic processes. In recent years, latent variable meth-
ods have been shown to produce grammars which
are as good as, or even better than, earlier parsing
work (Collins, 1999; Charniak, 2000). In particular,
in Petrov et al (2006) we exhibited a very accurate
category-splitting approach, in which a coarse ini-
tial grammar is refined by iteratively splitting each
grammar category into two subcategories using the
EM algorithm. Of course, each time the number of
grammar categories is doubled, the number of bi-
nary productions is increased by a factor of eight.
As a result, while our final grammars used few cat-
egories, the number of total active (non-zero) pro-
ductions was still substantial (see Section 7). In ad-
dition, it is reasonable to assume that some genera-
tively learned splits have little discriminative utility.
In this paper, we present a discriminative approach
which addresses both of these limitations.
We introduce multi-scale grammars, in which
some productions reference fine categories, while
others reference coarse categories (see Figure 2).
We use the general framework of hidden variable
CRFs (Lafferty et al, 2001; Koo and Collins, 2005),
where gradient-based optimization maximizes the
likelihood of the observed variables, here parse
trees, summing over log-linearly scored derivations.
With multi-scale grammars, it is natural to refine
productions rather than categories. As a result, a
category such as NP can be complex in some re-
gions of the grammar while remaining simpler in
other regions. Additionally, we exploit the flexibility
of the discriminative framework both to improve the
treatment of unknown words as well as to include
span features (Taskar et al, 2004), giving the bene-
fit of some input features integrally in our dynamic
program. Our multi-scale grammars are 3 orders
of magnitude smaller than the fully-split baseline
grammar and 20 times smaller than the generative
split-and-merge grammars of Petrov et al (2006).
867
In addition, we exhibit the best parsing numbers on
several metrics, for several domains and languages.
Discriminative parsing has been investigated be-
fore, such as in Johnson (2001), Clark and Curran
(2004), Henderson (2004), Koo and Collins (2005),
Turian et al (2007), Finkel et al (2008), and, most
similarly, in Petrov and Klein (2008). However, in
all of these cases, the final parsing performance fell
short of the best generative models by several per-
centage points or only short sentences were used.
Only in combination with a generative model was
a discriminative component able to produce high
parsing accuracies (Charniak and Johnson, 2005;
Huang, 2008). Multi-scale grammars, in contrast,
give higher accuracies using smaller grammars than
previous work in this direction, outperforming top
generative models in grammar size and in parsing
accuracy.
2 Latent Variable Parsing
Treebanks are typically not annotated with fully de-
tailed syntactic structure. Rather, they present only
a coarse trace of the true underlying processes. As
a result, learning a grammar for parsing requires
the estimation of a more highly articulated model
than the naive CFG embodied by such treebanks.
A manual approach might take the category NP and
subdivide it into one subcategory NP?S for subjects
and another subcategory NP?VP for objects (John-
son, 1998; Klein and Manning, 2003). However,
rather than devising linguistically motivated features
or splits, latent variable parsing takes a fully auto-
mated approach, in which each symbol is split into
unconstrained subcategories.
2.1 Latent Variable Grammars
Latent variable grammars augment the treebank
trees with latent variables at each node. This cre-
ates a set of (exponentially many) derivations over
split categories for each of the original parse trees
over unsplit categories. For each observed category
A we now have a set of latent subcategories Ax. For
example, NP might be split into NP1 through NP8.
The parameters of the refined productions
Ax ? By Cz, where Ax is a subcategory of A, By
of B, and Cz of C , can then be estimated in var-
ious ways; past work has included both generative
(Matsuzaki et al, 2005; Liang et al, 2007) and dis-
criminative approaches (Petrov and Klein, 2008).
We take the discriminative log-linear approach here.
Note that the comparison is only between estimation
methods, as Smith and Johnson (2007) show that the
model classes are the same.
2.2 Log-Linear Latent Variable Grammars
In a log-linear latent variable grammar, each pro-
duction r = Ax ? By Cz is associated with a
multiplicative weight ?r (Johnson, 2001; Petrov and
Klein, 2008) (sometimes we will use the log-weight
?r when convenient). The probability of a derivation
t of a sentence w is proportional to the product of the
weights of its productions r:
P (t|w) ?
?
r?t
?r
The score of a parse T is then the sum of the scores
of its derivations:
P (T |w) =
?
t?T
P (t|w)
3 Hierarchical Refinement
Grammar refinement becomes challenging when the
number of subcategories is large. If each category
is split into k subcategories, each (binary) produc-
tion will be split into k3. The resulting memory lim-
itations alone can prevent the practical learning of
highly split grammars (Matsuzaki et al, 2005). This
issue was partially addressed in Petrov et al (2006),
where categories were repeatedly split and some
splits were re-merged if the gains were too small.
However, while the grammars are indeed compact
at the (sub-)category level, they are still dense at the
production level, which we address here.
As in Petrov et al (2006), we arrange our subcat-
egories into a hierarchy, as shown in Figure 1. In
practice, the construction of the hierarchy is tightly
coupled to a split-based learning process (see Sec-
tion 5). We use the naming convention that an origi-
nal category A becomes A0 and A1 in the first round;
A0 then becoming A00 and A01 in the second round,
and so on. We will use x? ? x to indicate that the
subscript or subcategory x is a refinement of x?.1 We
1Conversely, x? is a coarser version of x, or, in the language
of Petrov and Klein (2007), x? is a projection of x.
868
+ 7 . 3
+ 5 . 0
+ 7 . 3
+ 1 2
+ 2 . 1
S i n g l e  s c a l e p r o d u c t i o n s
+ 5 . 0
+ 5 . 0
+ 7 . 3
+ 2 . 1
+ 2 . 1
+ 2 . 1
+ 2 . 1
M u l t i  s c a l e p r o d u c t i o n s
+ 2 . 1
+ 1 2
+ 5 . 0
?
0 0
1
0 1 0 0 1 1
+ 5 . 0 + 5 . 0 + 7 . 3 + 1 2
?
0 0 0 1 0 0 1 0 1 1 1 0 1 1 10 0 1 0 1 0 0 1 1
+ 2 . 1 + 2 . 1+ 2 . 1+ 2 . 1
?r? ?r?
0 0
0
*
1
1 00 1 1 1
0
*
0 1
D T 0 0 0 ? t h e
D T 0 0 1 ? t h e
D T 0 1 0 ? t h e
D T 0 1 1 ? t h e
D T 1 0 0 ? t h e
D T 1 0 1 ? t h e
D T 1 1 0 ? t h e
D T 1 1 1 ? t h e
D T 0 0 ? t h e
D T 0 1 0 ? t h e
D T 0 1 1 ? t h e
D T 1 ? t h e}
}
+ 1 2
Figure 1: Multi-scale refinement of the DT ? the production. The multi-scale grammar can be encoded much more
compactly than the equally expressive single scale grammar by using only the shaded features along the fringe.
will also say that x? dominates x, and x will refer to
fully refined subcategories. The same terminology
can be applied to (binary) productions, which split
into eight refinements each time the subcategories
are split in two.
The core observation leading to multi-scale gram-
mars is that when we look at the refinements of a
production, many are very similar in weight. It is
therefore advantageous to record productions only at
the level where they are distinct from their children
in the hierarchy.
4 Multi-Scale Grammars
A multi-scale grammar is a grammar in which some
productions reference fine categories, while others
reference coarse categories. As an example, con-
sider the multi-scale grammar in Figure 2, where the
NP category has been split into two subcategories
(NP0, NP1) to capture subject and object distinc-
tions. Since it can occur in subject and object po-
sition, the production NP ? it has remained unsplit.
In contrast, in a single-scale grammar, two produc-
tions NP0 ? it and NP1 ? it would have been nec-
essary. We use * as a wildcard, indicating that NP?
can combine with any other NP, while NP1 can only
combine with other NP1. Whenever subcategories
of different granularity are combined, the resulting
constituent takes the more specific label.
In terms of its structure, a multi-scale grammar is
a set of productions over varyingly refined symbols,
where each production is associated with a weight.
Consider the refinement of the production shown in
Figure 1. The original unsplit production (at top)
would naively be split into a tree of many subpro-
ductions (downward in the diagram) as the grammar
categories are incrementally split. However, it may
be that many of the fully refined productions share
the same weights. This will be especially common
in the present work, where we go out of our way to
achieve it (see Section 5). For example, in Figure 1,
the productions DTx ? the have the same weight
for all categories DTx which refine DT1.2 A multi-
scale grammar can capture this behavior with just 4
productions, while the single-scale grammar has 8
productions. For binary productions the savings will
of course be much higher.
In terms of its semantics, a multi-scale grammar is
simply a compact encoding of a fully refined latent
variable grammar, in which identically weighted re-
finements of productions have been collapsed to the
coarsest possible scale. Therefore, rather than at-
tempting to control the degree to which categories
are split, multi-scale grammars simply encode pro-
ductions at varying scales. It is hence natural to
speak of refining productions, while considering
the categories to exist at all degrees of refinement.
Multi-scale grammars enable the use of coarse (even
unsplit) categories in some regions of the grammar,
while requiring very specific subcategories in others,
as needed. As we will see in the following, this flex-
ibility results in a tremendous reduction of grammar
parameters, as well as improved parsing time, be-
cause the vast majority of productions end up only
partially split.
Since a multi-scale grammar has productions
which can refer to different levels of the category
hierarchy, there must be constraints on their coher-
ence. Specifically, for each fully refined produc-
tion, exactly one of its dominating coarse produc-
tions must be in the grammar. More formally, the
multi-scale grammar partitions the space of fully re-
fined base rules such that each r maps to a unique
2We define dominating productions and refining productions
analogously as for subcategories.
869
i t
s a w
V P 0
N P 1
V *
S *
N P 0
V 0 N P *
h e r
s a w
V P 0
N P 1V *
S *
N P 0
V 0 N P 1
V P 0
N P 1V *
S *
V P *N P 0
h e r
N P 1
s
h e
N P 0
i t
N P *
s a w
V 0
L
e x i c o n :
G r
a m m a
r :
V P * V P *
s
h e
N P 0
i t
N P *
Figure 2: In multi-scale grammars, the categories exist
at varying degrees of refinement. The grammar in this
example enforces the correct usage of she and her, while
allowing the use of it in both subject and object position.
dominating rule r?, and for all base rules r? such that
r? ? r?, r? maps to r? as well. This constraint is al-
ways satisfied if the multi-scale grammar consists of
fringes of the production refinement hierarchies, in-
dicated by the shading in Figure 1.
A multi-scale grammar straightforwardly assigns
scores to derivations in the corresponding fully re-
fined single scale grammar: simply map each refined
derivation rule to its dominating abstraction in the
multi-scale grammar and give it the corresponding
weight. The fully refined grammar is therefore triv-
ially (though not compactly) reconstructable from
its multi-scale encoding.
It is possible to directly define a derivational se-
mantics for multi-scale grammars which does not
appeal to the underlying single scale grammar.
However, in the present work, we use our multi-
scale grammars only to compute expectations of the
underlying grammars in an efficient, implicit way.
5 Learning Sparse Multi-Scale Grammars
We now consider how to discriminatively learn
multi-scale grammars by iterative splitting produc-
tions. There are two main concerns. First, be-
cause multi-scale grammars are most effective when
many productions share the same weight, sparsity
is very desirable. In the present work, we exploit
L1-regularization, though other techniques such as
structural zeros (Mohri and Roark, 2006) could
also potentially be used. Second, training requires
repeated parsing, so we use coarse-to-fine chart
caching to greatly accelerate each iteration.
5.1 Hierarchical Training
We learn discriminative multi-scale grammars in an
iterative fashion (see Figure 1). As in Petrov et al
(2006), we start with a simple X-bar grammar from
an input treebank. The parameters ? of the grammar
(production log-weights for now) are estimated in a
log-linear framework by maximizing the penalized
log conditional likelihood Lcond ?R(?), where:
Lcond(?) = log
?
i
P(Ti|wi)
R(?) =
?
r
|?r|
We directly optimize this non-convex objective
function using a numerical gradient based method
(LBFGS (Nocedal and Wright, 1999) in our imple-
mentation). To handle the non-diferentiability of the
L1-regularization term R(?) we use the orthant-wise
method of Andrew and Gao (2007). Fitting the log-
linear model involves the following derivatives:
?Lcond(?)
??r
=
?
i
(
E? [fr(t)|Ti] ? E?[fr(t)|wi]
)
where the first term is the expected count fr of a pro-
duction r in derivations corresponding to the correct
parse tree Ti and the second term is the expected
count of the production in all derivations of the sen-
tence wi. Note that r may be of any scale. As we
will show below, these expectations can be com-
puted exactly using marginals from the chart of the
inside/outside algorithm (Lari and Young, 1990).
Once the base grammar has been estimated, all
categories are split in two, meaning that all binary
productions are split in eight. When splitting an al-
ready refined grammar, we only split productions
whose log-weight in the previous grammar deviates
from zero.3 This creates a refinement hierarchy over
productions. Each newly split production r is given
a unique feature, as well as inheriting the features of
its parent productions r? ? r:
?r = exp
(
?
r??r
?r?
)
The parent productions r? are then removed from the
grammar and the new features are fit as described
3L1-regularization drives more than 95% of the feature
weights to zero in each round.
870
V P
N P
S
N P
D T N N V B D D T N N
V P
i k j
S 0 ? N P 1 V P 0 1
I(S0, i, j) I(S11, i, j)
Figure 3: A multi-scale chart can be used to efficiently
compute inside/outside scores using productions of vary-
ing specificity.
above. We detect that we have split a production too
far when all child production features are driven to
zero under L1 regularization. In such cases, the chil-
dren are collapsed to their parent production, which
forms an entry in the multi-scale grammar.
5.2 Efficient Multi-Scale Inference
In order to compute the expected counts needed for
training, we need to parse the training set, score
all derivations and compute posteriors for all sub-
categories in the refinement hierarchy. The in-
side/outside algorithm (Lari and Young, 1990) is an
efficient dynamic program for summing over deriva-
tions under a context-free grammar. It is fairly
straightforward to adapt this algorithm to multi-
scale grammars, allowing us to sum over an expo-
nential number of derivations without explicitly re-
constructing the underlying fully split grammar.
For single-scale latent variable grammars, the in-
side score I(Ax, i, j) of a fully refined category Ax
spanning ?i, j? is computed by summing over all
possible productions r = Ax ? By Cz with weight
?r, spanning ?i, k? and ?k, j? respectively:4
I(Ax, i, j) =
?
r
?r
?
k
I(By, i, k)I(Cz , k, j)
Note that this involves summing over all relevant
fully refined grammar productions.
The key quantities we will need are marginals of
the form I(Ax, i, j), the sum of the scores of all fully
refined derivations rooted at any Ax dominated by
Ax and spanning ?i, j?. We define these marginals
4These scores lack any probabilistic interpretation, but can
be normalized to compute the necessary expectations for train-
ing (Petrov and Klein, 2008).
in terms of the standard inside scores of the most
refined subcategories Ax:
I(Ax, i, j) =
?
x?x
I(Ax, i, j)
When working with multi-scale grammars, we
expand the standard three-dimensional chart over
spans and grammar categories to store the scores of
all subcategories of the refinement hierarchy, as il-
lustrated in Figure 3. This allows us to compute the
scores more efficiently by summing only over rules
r? = Ax? ? By? Cz? ? r:
I(Ax, i, j) =
?
r?
?
r?r?
?r
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
r?r?
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
y?y?
?
z?z?
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
k
?
y?y?
I(By, i, k)
?
z?z?
I(Cz, k, j)
=
?
r?
?r?
?
k
I(By?, i, k)I(Cz? , k, j)
Of course, some of the same quantities are computed
repeatedly in the above equation and can be cached
in order to obtain further efficiency gains. Due to
space constraints we omit these details, and also the
computation of the outside score, as well as the han-
dling of unary productions.
5.3 Feature Count Approximations
Estimating discriminative grammars is challenging,
as it requires repeatedly taking expectations over all
parses of all sentences in the training set. To make
this computation practical on large data sets, we
use the same approach as Petrov and Klein (2008).
Therein, the idea of coarse-to-fine parsing (Charniak
et al, 1998) is extended to handle the repeated pars-
ing of the same sentences. Rather than computing
the entire coarse-to-fine history in every round of
training, the pruning history is cached between train-
ing iterations, effectively avoiding the repeated cal-
culation of similar quantities and allowing the effi-
cient approximation of feature count expectations.
871
6 Additional Features
The discriminative framework gives us a convenient
way of incorporating additional, overlapping fea-
tures. We investigate two types of features: un-
known word features (for predicting the part-of-
speech tags of unknown or rare words) and span fea-
tures (for determining constituent boundaries based
on individual words and the overall sentence shape).
6.1 Unknown Word Features
Building a parser that can process arbitrary sen-
tences requires the handling of previously unseen
words. Typically, a classification of rare words into
word classes is used (Collins, 1999). In such an ap-
proach, the word classes need to be manually de-
fined a priori, for example based on discriminating
word shape features (suffixes, prefixes, digits, etc.).
While this component of the parsing system is
rarely talked about, its importance should not be un-
derestimated: when using only one unknown word
class, final parsing performance drops several per-
centage points. Some unknown word features are
universal (e.g. digits, dashes), but most of them
will be highly language dependent (prefixes, suf-
fixes), making additional human expertise necessary
for training a parser on a new language. It is there-
fore beneficial to automatically learn what the dis-
criminating word shape features for a language are.
The discriminative framework allows us to do that
with ease. In our experiments we extract prefixes
and suffixes of length ? 3 and add those features to
words that occur 25 times or less in the training set.
These unknown word features make the latent vari-
able grammar learning process more language inde-
pendent than in previous work.
6.2 Span Features
There are many features beyond local tree config-
urations which can enhance parsing discrimination;
Charniak and Johnson (2005) presents a varied list.
In reranking, one can incorporate any such features,
of course, but even in our dynamic programming ap-
proach it is possible to include features that decom-
pose along the dynamic program structure, as shown
by Taskar et al (2004). We use non-local span fea-
tures, which condition on properties of input spans
(Taskar et al, 2004). We illustrate our span features
with the following example and the span ?1, 4?:
0 ? 1 [ Yes 2 ? 3 , ] 4 he 5 said 6 . 7
We first added the following lexical features:
? the first (Yes), last (comma), preceding (?) and
following (he) words,
? the word pairs at the left edge ??,Yes?, right
edge ?comma,he?, inside border ?Yes,comma?
and outside border ??,he?.
Lexical features were added for each span of length
three or more. We used two groups of span features,
one for natural constituents and one for synthetic
ones.5 We found this approach to work slightly
better than anchoring the span features to particular
constituent labels or having only one group.
We also added shape features, projecting the
sentence to abstract shapes to capture global sen-
tence structures. Punctuation shape replaces ev-
ery non-punctuation word with x and then further
collapses strings of x to x+. Our example be-
comes #??x??,x+.#, and the punctuation feature
for our span is ??[x??,]x. Capitalization shape
projects the example sentence to #.X..xx.#, and
.[X..]x for our span. Span features are a rich
source of information and our experiments should
be seen merely as an initial investigation of their ef-
fect in our system.
7 Experiments
We ran experiments on a variety of languages and
corpora using the standard training and test splits,
as described in Table 1. In each case, we start
with a completely unannotated X-bar grammar, ob-
tained from the raw treebank by a simple right-
branching binarization scheme. We then train multi-
scale grammars of increasing latent complexity as
described in Section 5, directly incorporating the
additional features from Section 6 into the training
procedure. Hierarchical training starting from a raw
treebank grammar and proceeding to our most re-
fined grammars took three days in a parallel im-
plementation using 8 CPUs. At testing time we
marginalize out the hidden structure and extract the
tree with the highest number of expected correct pro-
ductions, as in Petrov and Klein (2007).
5Synthetic constituents are nodes that are introduced during
binarization.
872
Training Set Dev. Set Test Set
ENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21
ENGLISH-BROWN see 10% of 10% of the
(Francis et al 2002) ENGLISH-WSJ the data6 the data6
FRENCH7 Sentences Sentences Sentences
(Abeille et al, 2000) 1-18,609 18,610-19,609 19,609-20,610
GERMAN Sentences Sentences Sentences
(Skut et al, 1997) 1-18,602 18,603-19,602 19,603-20,602
Table 1: Corpora and standard experimental setups.
We compare to a baseline of discriminatively
trained latent variable grammars (Petrov and Klein,
2008). We also compare our discriminative multi-
scale grammars to their generative split-and-merge
cousins, which have been shown to produce the
state-of-the-art figures in terms of accuracy and effi-
ciency on many corpora. For those comparisons we
use the grammars from Petrov and Klein (2007).
7.1 Sparsity
One of the main motivations behind multi-scale
grammars was to create compact grammars. Fig-
ure 4 shows parsing accuracies vs. grammar sizes.
Focusing on the grammar size for now, we see that
multi-scale grammars are extremely compact - even
our most refined grammars have less than 50,000 ac-
tive productions. This is 20 times smaller than the
generative split-and-merge grammars, which use ex-
plicit category merging. The graph also shows that
this compactness is due to controlling production
sparsity, as the single-scale discriminative grammars
are two orders of magnitude larger.
7.2 Accuracy
Figure 4 shows development set results for En-
glish. In terms of parsing accuracy, multi-scale
grammars significantly outperform discriminatively
trained single-scale latent variable grammars and
perform on par with the generative split-and-merge
grammars. The graph also shows that the unknown
word and span features each add about 0.5% in final
parsing accuracy. Note that the span features im-
prove the performance of the unsplit baseline gram-
mar by 8%, but not surprisingly their contribution
6See Gildea (2001) for the exact setup.
7This setup contains only sentences without annotation er-
rors, as in (Arun and Keller, 2005).
90
85
80
75
100000010000010000
Pa
rs
in
g 
ac
cu
ra
cy
 (F
1)
Number of grammar productions
Discriminative Multi-Scale Grammars
+ Lexical Features
+ Span Features
Generative Split-Merge Grammars
Flat Discriminative Grammars
Figure 4: Discriminative multi-scale grammars give sim-
ilar parsing accuracies as generative split-merge gram-
mars, while using an order of magnitude fewer rules.
gets smaller when the grammars get more refined.
Section 8 contains an analysis of some of the learned
features, as well as a comparison between discrimi-
natively and generatively trained grammars.
7.3 Efficiency
Petrov and Klein (2007) demonstrates how the idea
of coarse-to-fine parsing (Charniak et al, 1998;
Charniak et al, 2006) can be used in the context of
latent variable models. In coarse-to-fine parsing the
sentence is rapidly pre-parsed with increasingly re-
fined grammars, pruning away unlikely chart items
in each pass. In their work the grammar is pro-
jected onto coarser versions, which are then used
for pruning. Multi-scale grammars, in contrast, do
not require projections. The refinement hierarchy is
built in and can be used directly for coarse-to-fine
pruning. Each production in the grammar is associ-
ated with a set of hierarchical features. To obtain a
coarser version of a multi-scale grammar, one there-
fore simply limits which features in the refinement
hierarchy can be accessed. In our experiments, we
start by parsing with our coarsest grammar and al-
low an additional level of refinement at each stage of
the pre-parsing. Compared to the generative parser
of Petrov and Klein (2007), parsing with multi-scale
grammars requires the evaluation of 29% fewer pro-
ductions, decreasing the average parsing time per
sentence by 36% to 0.36 sec/sentence.
873
? 40 words all
Parser F1 EX F1 EX
ENGLISH-WSJ
Petrov and Klein (2008) 88.8 35.7 88.3 33.1
Charniak et al (2005) 90.3 39.6 89.7 37.2
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
This work w/o span features 89.7 39.6 89.2 37.2
This work w/ span features 90.0 40.1 89.4 37.7
ENGLISH-WSJ (reranked)
Huang (2008) 92.3 46.2 91.7 43.5
ENGLISH-BROWN
Charniak et al (2005) 84.5 34.8 82.9 31.7
Petrov and Klein (2007) 84.9 34.5 83.7 31.2
This work w/o span features 85.3 35.6 84.3 32.1
This work w/ span features 85.6 35.8 84.5 32.3
ENGLISH-BROWN (reranked)
Charniak et al (2005) 86.8 39.9 85.2 37.8
FRENCH
Arun and Keller (2005) 79.2 21.2 75.6 16.4
This Paper 80.1 24.2 77.2 19.2
GERMAN
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper 81.5 45.2 80.7 43.9
Table 2: Our final test set parsing accuracies compared to
the best previous work on English, French and German.
7.4 Final Results
For each corpus we selected the grammar that gave
the best performance on the development set to parse
the final test set. Table 2 summarizes our final test
set performance, showing that multi-scale grammars
achieve state-of-the-art performance on most tasks.
On WSJ-English, the discriminative grammars per-
form on par with the generative grammars of Petrov
et al (2006), falling slightly short in terms of F1, but
having a higher exact match score. When trained
on WSJ-English but tested on the Brown corpus,
the discriminative grammars clearly outperform the
generative grammars, suggesting that the highly reg-
ularized and extremely compact multi-scale gram-
mars are less prone to overfitting. All those meth-
ods fall short of reranking parsers like Charniak and
Johnson (2005) and Huang (2008), which, however,
have access to many additional features, that cannot
be used in our dynamic program.
When trained on the French and German tree-
banks, our multi-scale grammars achieve the best
figures we are aware of, without any language spe-
cific modifications. This confirms that latent vari-
able models are well suited for capturing the syn-
tactic properties of a range of languages, and also
shows that discriminative grammars are still effec-
tive when trained on smaller corpora.
8 Analysis
It can be illuminating to see the subcategories that
are being learned by our discriminative multi-scale
grammars and to compare them to generatively es-
timated latent variable grammars. Compared to the
generative case, the lexical categories in the discrim-
inative grammars are substantially less refined. For
example, in the generative case, the nominal cate-
gories were fully refined, while in the discrimina-
tive case, fewer nominal clusters were heavily used.
One reason for this can be seen by inspecting the
first two-way split in the NNP tag. The genera-
tive model split into initial NNPs (San, Wall) and
final NNPs (Francisco, Street). In contrast, the dis-
criminative split was between organizational entities
(Stock, Exchange) and other entity types (September,
New, York). This constrast is unsurprising. Genera-
tive likelihood is advantaged by explaining lexical
choice ? New and York occur in very different slots.
However, they convey the same information about
the syntactic context above their base NP and are
therefore treated the same, discriminatively, while
the systematic attachment distinctions between tem-
porals and named entities are more predictive.
Analyzing the syntactic and semantic patterns
learned by the grammars shows similar trends. In
Table 3 we compare the number of subcategories
in the generative split-and-merge grammars to the
average number of features per unsplit production
with that phrasal category as head in our multi-scale
grammars after 5 split (and merge) rounds. These
quantities are inherently different: the number of
features should be roughly cubic in the number of
subcategories. However, we observe that the num-
bers are very close, indicating that, due to the spar-
sity of our productions, and the efficient multi-scale
encoding, the number of grammar parameters grows
linearly in the number of subcategories. Further-
more, while most categories have similar complex-
ity in those two cases, the complexity of the two
most refined phrasal categories are flipped. Gener-
ative grammars split NPs most highly, discrimina-
874
N
P
V
P
PP S SB
A
R
A
D
JP
A
D
V
P
QP PR
N
Generative 32 24 20 12 12 12 8 7 5
subcategories
Discriminative 19 32 20 14 14 8 7 9 6production parameters
Table 3: Complexity of highly split phrasal categories in
generative and discriminative grammars. Note that sub-
categories are compared to production parameters, indi-
cating that the number of parameters grows cubicly in the
number of subcategories for generative grammars, while
growing linearly for multi-scale grammars.
tive grammars split the VP. This distinction seems
to be because the complexity of VPs is more syntac-
tic (e.g. complex subcategorization), while that of
NPs is more lexical (noun choice is generally higher
entropy than verb choice).
It is also interesting to examine the automatically
learned word class features. Table 4 shows the suf-
fixes with the highest weight for a few different cat-
egories across the three languages that we experi-
mented with. The learning algorithm has selected
discriminative suffixes that are typical derviational
or inflectional morphemes in their respective lan-
guages. Note that the highest weighted suffixes will
typically not correspond to the most common suffix
in the word class, but to the most discriminative.
Finally, the span features also exhibit clear pat-
terns. The highest scoring span features encourage
the words between the last two punctuation marks
to form a constituent (excluding the punctuation
marks), for example ,[x+]. and :[x+]. Words
between quotation marks are also encouraged to
form constituents: ??[x+]?? and x[??x+??]x.
Span features can also discourage grouping words
into constituents. The features with the highest neg-
ative weight involve single commas: x[x,x+],
and x[x+,x+]x and so on (indeed, such spans
were structurally disallowed by the Collins (1999)
parser).
9 Conclusions
Discriminatively trained multi-scale grammars give
state-of-the-art parsing performance on a variety of
languages and corpora. Grammar size is dramati-
cally reduced compared to the baseline, as well as to
ENGLISH GERMAN FRENCH
Adjectives
-ous -los -ien
-ble -bar -ble
-nth -ig -ive
Nouns
-ion -ta?t -te?
-en -ung -eur
-cle -rei -ges
Verbs -ed -st -e?es
-s -eht -e?
Adverbs -ly -mal -ent
Numbers -ty -zig ?
Table 4: Automatically learned suffixes with the highest
weights for different languages and part-of-speech tags.
methods like split-and-merge (Petrov et al, 2006).
Because fewer parameters are estimated, multi-scale
grammars may also be less prone to overfitting, as
suggested by a cross-corpus evaluation experiment.
Furthermore, the discriminative framework enables
the seamless integration of additional, overlapping
features, such as span features and unknown word
features. Such features further improve parsing per-
formance and make the latent variable grammars
very language independent.
Our parser, along with trained grammars
for a variety of languages, is available at
http://nlp.cs.berkeley.edu.
References
A. Abeille, L. Clement, and A. Kinyon. 2000. Building a
treebank for French. In 2nd International Conference
on Language Resources and Evaluation.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In ICML ?07.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: the case of french. In
ACL ?05.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ?06.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In ACL ?04.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
875
J. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In ACL ?08.
W. N. Francis and H. Kucera. 2002. Manual of infor-
mation to accompany a standard corpus of present-day
edited american english. In TR, Brown University.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. EMNLP ?01.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In ACL ?04.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL ?08.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In ACL ?01.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ?03, pages 423?430.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In EMNLP ?05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML ?01.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In EMNLP ?07.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
M. Mohri and B. Roark. 2006. Probabilistic context-free
grammar induction based on structural zeros. In HLT-
NAACL ?06.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In NIPS ?08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Conf. on Applied Natural Language Processing.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Lingusitics.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In EMNLP ?04.
J. Turian, B. Wellington, and I. D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In NIPS ?07.
876
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 877?886,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Two Languages are Better than One (for Syntactic Parsing)
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
We show that jointly parsing a bitext can sub-
stantially improve parse quality on both sides.
In a maximum entropy bitext parsing model,
we define a distribution over source trees, tar-
get trees, and node-to-node alignments be-
tween them. Features include monolingual
parse scores and various measures of syntac-
tic divergence. Using the translated portion
of the Chinese treebank, our model is trained
iteratively to maximize the marginal likeli-
hood of training tree pairs, with alignments
treated as latent variables. The resulting bi-
text parser outperforms state-of-the-art mono-
lingual parser baselines by 2.5 F1 at predicting
English side trees and 1.8 F1 at predicting Chi-
nese side trees (the highest published numbers
on these corpora). Moreover, these improved
trees yield a 2.4 BLEU increase when used in
a downstream MT evaluation.
1 Introduction
Methods for machine translation (MT) have increas-
ingly leveraged not only the formal machinery of
syntax (Wu, 1997; Chiang, 2007; Zhang et al,
2008), but also linguistic tree structures of either the
source side (Huang et al, 2006; Marton and Resnik,
2008; Quirk et al, 2005), the target side (Yamada
and Knight, 2001; Galley et al, 2004; Zollmann et
al., 2006; Shen et al, 2008), or both (Och et al,
2003; Aue et al, 2004; Ding and Palmer, 2005).
These methods all rely on automatic parsing of one
or both sides of input bitexts and are therefore im-
pacted by parser quality. Unfortunately, parsing gen-
eral bitexts well can be a challenge for newswire-
trained treebank parsers for many reasons, including
out-of-domain input and tokenization issues.
On the other hand, the presence of translation
pairs offers a new source of information: bilin-
gual constraints. For example, Figure 1 shows a
case where a state-of-the-art English parser (Petrov
and Klein, 2007) has chosen an incorrect structure
which is incompatible with the (correctly chosen)
output of a comparable Chinese parser. Smith and
Smith (2004) previously showed that such bilin-
gual constraints can be leveraged to transfer parse
quality from a resource-rich language to a resource-
impoverished one. In this paper, we show that bilin-
gual constraints and reinforcement can be leveraged
to substantially improve parses on both sides of a
bitext, even for two resource-rich languages.
Formally, we present a log-linear model over
triples of source trees, target trees, and node-to-
node tree alignments between them. We consider
a set of core features which capture the scores of
monolingual parsers as well as measures of syntactic
alignment. Our model conditions on the input sen-
tence pair and so features can and do reference input
characteristics such as posterior distributions from a
word-level aligner (Liang et al, 2006; DeNero and
Klein, 2007).
Our training data is the translated section of the
Chinese treebank (Xue et al, 2002; Bies et al,
2007), so at training time correct trees are observed
on both the source and target side. Gold tree align-
ments are not present and so are induced as latent
variables using an iterative training procedure. To
make the process efficient and modular to existing
monolingual parsers, we introduce several approxi-
mations: use of k-best lists in candidate generation,
an adaptive bound to avoid considering all k2 com-
binations, and Viterbi approximations to alignment
posteriors.
877
Figure 1: Two possible parse pairs for a Chinese-English sentence pair. The parses in a) are chosen by independent
monolingual statistical parsers, but only the Chinese side is correct. The gold English parse shown in b) is further down
in the 100-best list, despite being more consistent with the gold Chinese parse. The circles show where the two parses
differ. Note that in b), the ADVP and PP nodes correspond nicely to Chinese tree nodes, whereas the correspondence
for nodes in a), particularly the SBAR node, is less clear.
We evaluate our system primarily as a parser and
secondarily as a component in a machine translation
pipeline. For both English and Chinese, we begin
with the state-of-the-art parsers presented in Petrov
and Klein (2007) as a baseline. Joint parse selection
improves the English trees by 2.5 F1 and the Chi-
nese trees by 1.8 F1. While other Chinese treebank
parsers do not have access to English side transla-
tions, this Chinese figure does outperform all pub-
lished monolingual Chinese treebank results on an
equivalent split of the data.
As MT motivates this work, another valuable
evaluation is the effect of joint selection on down-
stream MT quality. In an experiment using a
syntactic MT system, we find that rules extracted
from joint parses results in an increase of 2.4
BLEU points over rules extracted from independent
parses.1 In sum, jointly parsing bitexts improves
parses substantially, and does so in a way that that
carries all the way through the MT pipeline.
2 Model
In our model, we consider pairs of sentences (s, s?),
where we use the convention that unprimed vari-
ables are source domain and primed variables are
target domain. These sentences have parse trees t
(respectively t?) taken from candidate sets T (T ?).
1It is anticipated that in some applications, such as tree trans-
ducer extraction, the alignments themselves may be of value,
but in the present work they are not evaluated.
Non-terminal nodes in trees will be denoted by n
(n?) and we abuse notation by equating trees with
their node sets. Alignments a are simply at-most-
one-to-one matchings between a pair of trees t and
t? (see Figure 2a for an example). Note that we will
also mention word alignments in feature definitions;
a and the unqualified term alignment will always re-
fer to node alignments. Words in a sentence are de-
noted by v (v?).
Our model is a general log-linear (maximum en-
tropy) distribution over triples (t, a, t?) for sentence
pairs (s, s?):
P(t, a, t|s, s?) ? exp(w>?(t, a, t?))
Features are thus defined over (t, a, t?) triples; we
discuss specific features below.
3 Features
To use our model, we need features of a triple
(t, a, t?) which encode both the monolingual quality
of the trees as well as the quality of the alignment
between them. We introduce a variety of features in
the next sections.
3.1 Monolingual Features
To capture basic monolingual parse quality, we be-
gin with a single source and a single target feature
whose values are the log likelihood of the source
tree t and the target tree t?, respectively, as given
878
by our baseline monolingual parsers. These two fea-
tures are called SOURCELL and TARGETLL respec-
tively. It is certainly possible to augment these sim-
ple features with what would amount to monolin-
gual reranking features, but we do not explore that
option here. Note that with only these two features,
little can be learned: all positive weightsw cause the
jointly optimal parse pair (t, t?) to comprise the two
top-1 monolingual outputs (the baseline).
3.2 Word Alignment Features
All other features in our model reference the entire
triple (t, a, t?). In this work, such features are de-
fined over aligned node pairs for efficiency, but gen-
eralizations are certainly possible.
Bias: The first feature is simply a bias feature
which has value 1 on each aligned node pair (n, n?).
This bias allows the model to learn a general prefer-
ence for denser alignments.
Alignment features: Of course, some alignments
are better than others. One indicator of a good node-
to-node alignment between n and n? is that a good
word alignment model thinks that there are many
word-to-word alignments in their bispan. Similarly,
there should be few alignments that violate that bis-
pan. To compute such features, we define a(v, v?)
to be the posterior probability assigned to the word
alignment between v and v? by an independent word
aligner.2
Before defining alignment features, we need to
define some additional variables. For any node n ? t
(n? ? t?), the inside span i(n) (i(n?)) comprises
the input tokens of s (s?) dominated by that node.
Similarly, the complement, the outside span, will be
denoted o(n) (o(n?)), and comprises the tokens not
dominated by that node. See Figure 2b,c for exam-
ples of the resulting regions.
INSIDEBOTH =
?
v?i(n)
?
v??i(n?)
a(v, v?)
INSRCOUTTRG =
?
v?i(n)
?
v??o(n?)
a(v, v?)
INTRGOUTSRC =
?
v?o(n)
?
v??i(n?)
a(v, v?)
2It is of course possible to learn good alignments using lexi-
cal indicator functions or other direct techniques, but given our
very limited training data, it is advantageous to leverage counts
from an unsupervised alignment system.
Hard alignment features: We also define the
hard versions of these features, which take counts
from the word aligner?s hard top-1 alignment output
?:
HARDINSIDEBOTH =
?
v?i(n)
?
v??i(n?)
?(v, v?)
HARDINSRCOUTTRG =
?
v?i(n)
?
v??o(n?)
?(v, v?)
HARDINTRGOUTSRC =
?
v?o(n)
?
v??i(n?)
?(v, v?)
Scaled alignment features: Finally, undesirable
larger bispans can be relatively sparse at the word
alignment level, yet still contain many good word
alignments simply by virtue of being large. We
therefore define a scaled count which measures den-
sity rather than totals. The geometric mean of span
lengths was a superior measure of bispan ?area? than
the true area because word-level alignments tend to
be broadly one-to-one in our word alignment model.
SCALEDINSIDEBOTH =
INSIDEBOTH
?
|i(n)| ? |i(n?)|
SCALEDINSRCOUTTRG =
INSRCOUTTRG
?
|i(n)| ? |o(n?)|
SCALEDINTRGOUTSRC =
INTRGOUTSRC
?
|o(n)| ? |i(n?)|
Head word alignment features: When consider-
ing a node pair (n, n?), especially one which dom-
inates a large area, the above measures treat all
spanned words as equally important. However, lex-
ical heads are generally more representative than
other spanned words. Let h select the headword of
a node according to standard head percolation rules
(Collins, 2003; Bikel and Chiang, 2000).
ALIGNHEADWORD = a(h(n), h(n?))
HARDALIGNHEADWORD = ?(h(n), h(n?))
3.3 Tree Structure Features
We also consider features that measure correspon-
dences between the tree structures themselves.
Span difference: We expect that, in general,
aligned nodes should dominate spans of roughly the
same length, and so we allow the model to learn to
879
Figure 2: a) An example of a legal alignment on a Chinese-English sentence fragment with one good and one bad node
pair, along with sample word alignment posteriors. Hard word alignments are bolded. b) The word alignment regions
for the good NP-NP alignment. InsideBoth regions are shaded in black, InSrcOutTrg in light grey, and InTrgOutSrc in
grey. c) The word alignment regions for the bad PP-NP alignment.
penalize node pairs whose inside span lengths differ
greatly.
SPANDIFF = ||i(n)| ? |i(n?)||
Number of children: We also expect that there
will be correspondences between the rules of the
CFGs that generate the trees in each language. To
encode some of this information, we compute in-
dicators of the number of children c that the nodes
have in t and t?.
NUMCHILDREN?|c(n)|, |c(n?)|? = 1
Child labels: In addition, we also encode whether
certain label pairs occur as children of matched
nodes. Let c(n, `) select the children of n with la-
bel `.
CHILDLABEL?`, `?? = |c(n, `)| ? |c(n?, `?)|
Note that the corresponding ?self labels? feature
is not listed because it arises in the next section as a
typed variant of the bias feature.
3.4 Typed vs untyped features
For each feature above (except monolingual fea-
tures), we create label-specific versions by conjoin-
ing the label pair (`(n), `(n?)). We use both the
typed and untyped variants of all features.
4 Training
Recall that our data condition supplies sentence
pairs (s, s?) along with gold parse pairs (g, g?). We
do not observe the alignments a which link these
parses. In principle, we want to find weights which
maximize the marginal log likelihood of what we do
observe given our sentence pairs:3
w? = arg max
w
?
a
P(g, a, g?|s, s?, w) (1)
= arg max
w
?
a exp(w
>?(g, a, g?))
?
(t,t?)
?
a exp(w
>?(t, a, t?))
(2)
There are several challenges. First, the space of
symmetric at-most-one-to-one matchings is #P-hard
3In this presentation, we only consider a single sentence pair
for the sake of clarity, but our true objective was multiplied over
all sentence pairs in the training data.
880
to sum over exactly (Valiant, 1979). Second, even
without matchings to worry about, standard meth-
ods for maximizing the above formulation would re-
quire summation over pairs of trees, and we want
to assume a fairly generic interface to independent
monolingual parsers (though deeper joint modeling
and/or training is of course a potential extension).
As we have chosen to operate in a reranking mode
over monolingual k-best lists, we have another is-
sue: our k-best outputs on the data which trains
our model may not include the gold tree pair. We
therefore make several approximations and modifi-
cations, which we discuss in turn.
4.1 Viterbi Alignments
Because summing over alignments a is intractable,
we cannot evaluate (2) or its derivatives. However,
if we restrict the space of possible alignments, then
we can make this optimization more feasible. One
way to do this is to stipulate in advance that for each
tree pair, there is a canonical alignment a0(t, t?). Of
course, we want a0 to reflect actual correspondences
between t and t?, so we want a reasonable definition
that ensures the alignments are of reasonable qual-
ity. Fortunately, it turns out that we can efficiently
optimize a given a fixed tree pair and weight vector:
a? = arg max
a
P(a|t, t?, s, s?, w)
= arg max
a
P(t, a, t?|s, s?, w)
= arg max
a
exp(w>?(t, a, t?))
This optimization requires only that we search for
an optimal alignment. Because all our features can
be factored to individual node pairs, this can be done
with the Hungarian algorithm in cubic time.4 Note
that we do not enforce any kind of domination con-
sistency in the matching: for example, the optimal
alignment might in principle have the source root
aligning to a target non-root and vice versa.
We then define a0(t, t?) as the alignment that
maximizes w>0 ?(t, a, t
?), where w0 is a fixed initial
weight vector with a weight of 1 for INSIDEBOTH,
-1 for INSRCOUTTRG and INTRGOUTSRC, and 0
4There is a minor modification to allow nodes not to match.
Any alignment link which has negative score is replaced by a
zero-score link, and any zero-score link in the solution is con-
sidered a pair of unmatched nodes.
for all other features. Then, we simplify (2) by fix-
ing the alignments a0:
w? = arg max
w
exp(w>?(g, a0(g, g?), g?))
?
(t,t?) exp(w
>?(t, a0(t, t?), t?))
(3)
This optimization has no latent variables and is
therefore convex and straightforward. However,
while we did use this as a rapid training procedure
during development, fixing the alignments a priori is
both unsatisfying and also less effective than a pro-
cedure which allows the alignments a to adapt dur-
ing training.
Again, for fixed alignments a, optimizing w is
easy. Similarly, with a fixed w, finding the optimal
a for any particular tree pair is also easy. Another
option is therefore to use an iterative procedure that
alternates between choosing optimal alignments for
a fixed w, and then reoptimizing w for those fixed
alignments according to (3). By iterating, we per-
form the following optimization:
w? = arg max
w
maxa exp(w>?(g, a, g?))
?
(t,t?) maxa exp(w
>?(t, a, t?))
(4)
Note that (4) is just (2) with summation replaced
by maximization. Though we do not know of any
guarantees for this EM-like algorithm, in practice
it converges after a few iterations given sufficient
training data. We initialize the procedure by setting
w0 as defined above.
4.2 Pseudo-gold Trees
When training our model, we approximate the sets
of all trees with k-best lists, T and T ?, produced
by monolingual parsers. Since these sets are not
guaranteed to contain the gold trees g and g?, our
next approximation is to define a set of pseudo-gold
trees, following previous work in monolingual parse
reranking (Charniak and Johnson, 2005). We define
T? (T? ?) as the F1-optimal subset of T (T ?). We then
modify (4) to reflect the fact that we are seeking to
maximize the likelihood of trees in this subset:
w? = arg max
w
?
(t,t?)?(T? ,T? ?)
P(t, t?|s, s?, w) (5)
where P(t, t?|s, s?, w) =
maxa exp(w>?(t, a, t?))
?
(t?,t??)?(T,T ?) maxa exp(w
>?(t?, a, t??))
(6)
881
4.3 Training Set Pruning
To reduce the time and space requirements for train-
ing, we do not always use the full k-best lists. To
prune the set T , we rank all the trees in T from 1 to
k, according to their log likelihood under the base-
line parsing model, and find the rank of the least
likely pseudo-gold tree:
r? = min
t?T?
rank(t)
Finally, we restrict T based on rank:
Tpruned = {t ? T |rank(t) ? r
? + }
where  is a free parameter of the pruning procedure.
The restricted set T ?pruned is constructed in the same
way. When training, we replace the sum over all tree
pairs in (T, T ?) in the denominator of (6) with a sum
over all tree pairs in (Tpruned, T ?pruned).
The parameter  can be set to any value from 0
to k, with lower values resulting in more efficient
training, and higher values resulting in better perfor-
mance. We set  by empirically determining a good
speed/performance tradeoff (see ?6.2).
5 Joint Selection
At test time, we have a weight vector w and so
selecting optimal trees for the sentence pair (s, s?)
from a pair of k best lists, (T, T ?) is straightforward.
We just find:
(t?, t??) = arg max
(t,t?)?(T,T ?)
max
a
P(t, a, t?|s, s?, w)
= arg max
(t,t?)?(T,T ?)
max
a
w>?(t, a, t?)
Note that with no additional cost, we can also find
the optimal alignment between t? and t??:
a? = arg max
a
w>?(t?, a, t??)
5.1 Test Set Pruning
Because the size of (T, T ?) grows asO(k2), the time
spent iterating through all these tree pairs can grow
unreasonably long, particularly when reranking a set
of sentence pairs the size of a typical MT corpus. To
combat this, we use a simple pruning technique to
limit the number of tree pairs under consideration.
Training Dev Test
Articles 1-270 301-325 271-300
Ch Sentences 3480 352 348
Eng Sentences 3472 358 353
Bilingual Pairs 2298 270 288
Table 1: Sentence counts from bilingual Chinese tree-
bank corpus.
To prune the list of tree pairs, first we rank them
according to the metric:
wSOURCELL ? SOURCELL +wTARGETLL ? TARGETLL
Then, we simply remove all tree pairs whose rank-
ing falls below some empirically determined cutoff.
As we show in ?6.3, by using this technique we are
able to speed up reranking by a factor of almost 20
without an appreciable loss of performance.
6 Statistical Parsing Experiments
All the data used to train the joint parsing model and
to evaluate parsing performance were taken from ar-
ticles 1-325 of the Chinese treebank, which all have
English translations with gold-standard parse trees.
The articles were split into training, development,
and test sets according to the standard breakdown for
Chinese parsing evaluations. Not all sentence pairs
could be included for various reasons, including
one-to-many Chinese-English sentence alignments,
sentences omitted from the English translations, and
low-fidelity translations. Additional sentence pairs
were dropped from the training data because they
had unambiguous parses in at least one of the two
languages. Table 1 shows how many sentences were
included in each dataset.
We had two training setups: rapid and full. In the
rapid training setup, only 1000 sentence pairs from
the training set were used, and we used fixed align-
ments for each tree pair rather than iterating (see
?4.1). The full training setup used the iterative train-
ing procedure on all 2298 training sentence pairs.
We used the English and Chinese parsers in
Petrov and Klein (2007)5 to generate all k-best lists
and as our evaluation baseline. Because our bilin-
gual data is from the Chinese treebank, and the data
5Available at http://nlp.cs.berkeley.edu.
882
typically used to train a Chinese parser contains the
Chinese side of our bilingual training data, we had
to train a new Chinese grammar using only articles
400-1151 (omitting articles 1-270). This modified
grammar was used to generate the k-best lists that
we trained our model on. However, as we tested on
the same set of articles used for monolingual Chi-
nese parser evaluation, there was no need to use
a modified grammar to generate k-best lists at test
time, and so we used a regularly trained Chinese
parser for this purpose.
We also note that since all parsing evaluations
were performed on Chinese treebank data, the Chi-
nese test sentences were in-domain, whereas the
English sentences were very far out-of-domain for
the Penn Treebank-trained baseline English parser.
Hence, in these evaluations, Chinese scores tend to
be higher than English ones.
Posterior word alignment probabilities were ob-
tained from the word aligner of Liang et al (2006)
and DeNero and Klein (2007)6, trained on approxi-
mately 1.7 million sentence pairs. For our alignment
model we used an HMM in each direction, trained to
agree (Liang et al, 2006), and we combined the pos-
teriors using DeNero and Klein?s (2007) soft union
method.
Unless otherwise specified, the maximum value
of k was set to 100 for both training and testing, and
all experiments used a value of 25 as the  parameter
for training set pruning and a cutoff rank of 500 for
test set pruning.
6.1 Feature Ablation
To verify that all our features were contributing to
the model?s performance, we did an ablation study,
removing one group of features at a time. Table 2
shows the F1 scores on the bilingual development
data resulting from training with each group of fea-
tures removed.7 Note that though head word fea-
tures seemed to be detrimental in our rapid train-
ing setup, earlier testing had shown a positive effect,
so we reran the comparison using our full training
setup, where we again saw an improvement when
including these features.
6Available at http://nlp.cs.berkeley.edu.
7We do not have a test with the basic alignment features
removed because they are necessary to compute a0(t, t?).
Baseline Parsers
Features Ch F1 Eng F1 Tot F1
Monolingual 84.95 76.75 81.15
Rapid Training
Features Ch F1 Eng F1 Tot F1
All 86.37 78.92 82.91
?Hard align 85.83 77.92 82.16
?Scaled align 86.21 78.62 82.69
?Head word 86.47 79.00 83.00
?Span diff 86.00 77.49 82.07
?Num children 86.26 78.56 82.69
?Child labels 86.35 78.45 82.68
Full Training
Features Ch F1 Eng F1 Tot F1
All 86.76 79.41 83.34
?Head word 86.42 79.53 83.22
Table 2: Feature ablation study. F1 on dev set after train-
ing with individual feature groups removed. Performance
with individual baseline parsers included for reference.
 Ch F1 Eng F1 Tot F1 Tree Pairs
15 85.78 77.75 82.05 1,463,283
20 85.88 77.27 81.90 1,819,261
25 86.37 78.92 82.91 2,204,988
30 85.97 79.18 82.83 2,618,686
40 86.10 78.12 82.40 3,521,423
50 85.95 78.50 82.50 4,503,554
100 86.28 79.02 82.91 8,997,708
Table 3: Training set pruning study. F1 on dev set after
training with different values of the  parameter for train-
ing set pruning.
6.2 Training Set Pruning
To find a good value of the  parameter for train-
ing set pruning we tried several different values, us-
ing our rapid training setup and testing on the dev
set. The results are shown in Table 3. We selected
25 as it showed the best performance/speed trade-
off, on average performing as well as if we had done
no pruning at all, while requiring only a quarter the
memory and CPU time.
6.3 Test Set Pruning
We also tried several different values of the rank cut-
off for test set pruning, using the full training setup
883
Cutoff Ch F1 Eng F1 Tot F1 Time (s)
50 86.34 79.26 83.04 174
100 86.61 79.31 83.22 307
200 86.67 79.39 83.28 509
500 86.76 79.41 83.34 1182
1000 86.80 79.39 83.35 2247
2000 86.78 79.35 83.33 4476
10,000 86.71 79.37 83.30 20,549
Table 4: Test set pruning study. F1 on dev set obtained
using different cutoffs for test set pruning.
and testing on the dev set. The results are in Table 4.
For F1 evaluation, which is on a very small set of
sentences, we selected 500 as the value with the best
speed/performance tradeoff. However, when rerank-
ing our entire MT corpus, we used a value of 200,
sacrificing a tiny bit of performance for an extra fac-
tor of 2 in speed.8
6.4 Sensitivity to k
Since our bitext parser currently operates as a
reranker, the quality of the trees is limited by the
quality of the k-best lists produced by the baseline
parsers. To test this limitation, we evaluated perfor-
mance on the dev set using baseline k-best lists of
varying length. Training parameters were fixed (full
training setup with k = 100) and test set pruning was
disabled for these experiments. The results are in Ta-
ble 5. The relatively modest gains with increasing k,
even as the oracle scores continue to improve, indi-
cate that performance is limited more by the model?s
reliance on the baseline parsers than by search errors
that result from the reranking approach.
6.5 Final Results
Our final evaluation was done using the full training
setup. Here, we report F1 scores on two sets of data.
First, as before, we only include the sentence pairs
from our bilingual corpus to fully demonstrate the
gains made by joint parsing. We also report scores
on the full test set to allow easier comparison with
8Using a rank cutoff of 200, the reranking step takes slightly
longer than serially running both baseline parsers, and generat-
ing k-best lists takes slightly longer than getting 1-best parses,
so in total, joint parsing takes about 2.3 times as long as mono-
lingual parsing. With a rank cutoff of 500, total parsing time is
scaled by a factor of around 3.8.
Joint Parsing Oracle
k Ch F1 Eng F1 Ch F1 Eng F1
1 84.95 76.75 84.95 76.75
10 86.23 78.43 90.05 81.99
25 86.64 79.27 90.99 83.37
50 86.61 79.10 91.82 84.14
100 86.71 79.37 92.23 84.73
150 86.67 79.47 92.49 85.17
Table 5: Sensitivity to k study. Joint parsing and oracle
F1 obtained on dev set using different maximum values
of k when generating baseline k-best lists.
F1 on bilingual data only
Parser Ch F1 Eng F1 Tot F1
Baseline 83.50 79.25 81.44
Joint 85.25 81.72 83.52
F1 on full test set
Parser Ch F1 Eng F1 Tot F1
Baseline 82.91 78.93 81.00
Joint 84.24 80.87 82.62
Table 6: Final evaluation. Comparison of F1 on test set
between baseline parsers and joint parser.
past work on Chinese parsing. For the latter evalu-
ation, sentences that were not in the bilingual cor-
pus were simply parsed with the baseline parsers.
The results are in Table 6. Joint parsing improves
F1 by 2.5 points on out-of-domain English sentences
and by 1.8 points on in-domain Chinese sentences;
this represents the best published Chinese treebank
parsing performance, even after sentences that lack
a translation are taken into account.
7 Machine Translation
To test the impact of joint parsing on syntactic MT
systems, we compared the results of training an MT
system with two different sets of trees: those pro-
duced by the baseline parsers, and those produced by
our joint parser. For this evaluation, we used a syn-
tactic system based on Galley et al (2004) and Gal-
ley et al (2006), which extracts tree-to-string trans-
ducer rules based on target-side trees. We trained the
system on 150,000 Chinese-English sentence pairs
from the training corpus of Wang et al (2007), and
used a large (close to 5 billion tokens) 4-gram lan-
884
Baseline Joint Moses
BLEU 18.7 21.1 18.8
Table 7: MT comparison on a syntactic system trained
with trees output from either baseline monolingual
parsers or our joint parser. To facilitate relative compari-
son, the Moses (Koehn et al, 2007) number listed reflects
the default Moses configuration, including its full distor-
tion model, and standard training pipeline.
guage model for decoding. We tuned and evaluated
BLEU (Papineni et al, 2001) on separate held-out
sets of sentences of up to length 40 from the same
corpus. The results are in Table 7, showing that joint
parsing yields a BLEU increase of 2.4.9
8 Conclusions
By jointly parsing (and aligning) sentences in a
translation pair, it is possible to exploit mutual con-
straints that improve the quality of syntactic analy-
ses over independent monolingual parsing. We pre-
sented a joint log-linear model over source trees,
target trees, and node-to-node alignments between
them, which is used to select an optimal tree pair
from a k-best list. On Chinese treebank data, this
procedure improves F1 by 1.8 on Chinese sentences
and by 2.5 on out-of-domain English sentences. Fur-
thermore, by using this joint parsing technique to
preprocess the input to a syntactic MT system, we
obtain a 2.4 BLEU improvement.
Acknowledgements
We would like to thank the anonymous reviewers for
helpful comments on an earlier draft of this paper
and Adam Pauls and Jing Zheng for help in running
our MT experiments.
References
Anthony Aue, Arul Menezes, Bob Moore, Chris Quirk,
and Eric Ringger. 2004. Statistical machine trans-
lation using labeled semantic dependency graphs. In
TMI.
9Note that all numbers are single-reference BLEU scores
and are not comparable to multiple reference scores or scores
on other corpora.
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English chinese translation treebank v 1.0. Web
download. LDC2007T02.
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the chinese treebank. In
Second Chinese Language Processing Workshop.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In ACL.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, CLSP, Johns Hopkins University.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. Research report, IBM.
RC22176.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
885
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In ACL.
Libin Shen, Jinxi Xu, and Ralph Weishedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
ACL.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using english
to parse korean. In EMNLP.
Leslie G. Valiant. 1979. The complexity of computing
the permanent. In Theoretical Computer Science 8.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with struc-
tured and web-based language models. In IEEE ASRU
Workshop.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The cmu-aka syntax aug-
mented machine translation system for iwslt-06. In
IWSLT.
886
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152?1161,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Simple Coreference Resolution with Rich Syntactic and Semantic Features
Aria Haghighi and Dan Klein
Computer Science Division
UC Berkeley
{aria42, klein}@cs.berkeley.edu
Abstract
Coreference systems are driven by syntactic, se-
mantic, and discourse constraints. We present
a simple approach which completely modularizes
these three aspects. In contrast to much current
work, which focuses on learning and on the dis-
course component, our system is deterministic and
is driven entirely by syntactic and semantic com-
patibility as learned from a large, unlabeled corpus.
Despite its simplicity and discourse naivete, our
system substantially outperforms all unsupervised
systems and most supervised ones. Primary con-
tributions include (1) the presentation of a simple-
to-reproduce, high-performing baseline and (2) the
demonstration that most remaining errors can be at-
tributed to syntactic and semantic factors external
to the coreference phenomenon (and perhaps best
addressed by non-coreference systems).
1 Introduction
The resolution of entity reference is influenced by
a variety of constraints. Syntactic constraints like
the binding theory, the i-within-i filter, and appos-
itive constructions restrict reference by configura-
tion. Semantic constraints like selectional compat-
ibility (e.g. a spokesperson can announce things)
and subsumption (e.g. Microsoft is a company)
rule out many possible referents. Finally, dis-
course phenomena such as salience and centering
theory are assumed to heavily influence reference
preferences. As these varied factors have given
rise to a multitude of weak features, recent work
has focused on how best to learn to combine them
using models over reference structures (Culotta et
al., 2007; Denis and Baldridge, 2007; Klenner and
Ailloud, 2007).
In this work, we break from the standard view.
Instead, we consider a vastly more modular system
in which coreference is predicted from a determin-
istic function of a few rich features. In particu-
lar, we assume a three-step process. First, a self-
contained syntactic module carefully represents
syntactic structures using an augmented parser and
extracts syntactic paths from mentions to potential
antecedents. Some of these paths can be ruled in
or out by deterministic but conservative syntactic
constraints. Importantly, the bulk of the work in
the syntactic module is in making sure the parses
are correctly constructed and used, and this mod-
ule?s most important training data is a treebank.
Second, a self-contained semantic module evalu-
ates the semantic compatibility of headwords and
individual names. These decisions are made from
compatibility lists extracted from unlabeled data
sources such as newswire and web data. Finally,
of the antecedents which remain after rich syntac-
tic and semantic filtering, reference is chosen to
minimize tree distance.
This procedure is trivial where most systems are
rich, and so does not need any supervised corefer-
ence data. However, it is rich in important ways
which we argue are marginalized in recent coref-
erence work. Interestingly, error analysis from our
final system shows that its failures are far more
often due to syntactic failures (e.g. parsing mis-
takes) and semantic failures (e.g. missing knowl-
edge) than failure to model discourse phenomena
or appropriately weigh conflicting evidence.
One contribution of this paper is the exploration
of strong modularity, including the result that our
system beats all unsupervised systems and ap-
proaches the state of the art in supervised ones.
Another contribution is the error analysis result
that, even with substantial syntactic and semantic
richness, the path to greatest improvement appears
to be to further improve the syntactic and semantic
modules. Finally, we offer our approach as a very
strong, yet easy to implement, baseline. We make
no claim that learning to reconcile disparate fea-
tures in a joint model offers no benefit, only that it
must not be pursued to the exclusion of rich, non-
reference analysis.
2 Coreference Resolution
In coreference resolution, we are given a docu-
ment which consists of a set of mentions; each
1152
mention is a phrase in the document (typically
an NP) and we are asked to cluster mentions ac-
cording to the underlying referent entity. There
are three basic mention types: proper (Barack
Obama), nominal (president), and pronominal
(he).
1
For comparison to previous work, we eval-
uate in the setting where mention boundaries are
given at test time; however our system can easily
annotate reference on all noun phrase nodes in a
parse tree (see Section 3.1.1).
2.1 Data Sets
In this work we use the following data sets:
Development: (see Section 3)
? ACE2004-ROTH-DEV: Dev set split of the ACE
2004 training set utilized in Bengston and
Roth (2008). The ACE data also annotates
pre-nominal mentions which we map onto
nominals. 68 documents and 4,536 mentions.
Testing: (see Section 4)
? ACE2004-CULOTTA-TEST: Test set split of the
ACE 2004 training set utilized in Culotta et
al. (2007) and Bengston and Roth (2008).
Consists of 107 documents.
2
? ACE2004-NWIRE: ACE 2004 Newswire set to
compare against Poon and Domingos (2008).
Consists of 128 documents and 11,413 men-
tions; intersects with the other ACE data sets.
? MUC-6-TEST: MUC6 formal evaluation set
consisting of 30 documents and 2,068 men-
tions.
Unlabeled: (see Section 3.2)
? BLIPP: 1.8 million sentences of newswire
parsed with the Charniak (2000) parser. No
labeled coreference data; used for mining se-
mantic information.
? WIKI: 25k articles of English Wikipedia ab-
stracts parsed by the Klein and Manning
(2003) parser.
3
No labeled coreference data;
used for mining semantic information.
1
Other mention types exist and are annotated (such as pre-
nominal), which are treated as nominals in this work.
2
The evaluation set was not made available to non-
participants.
3
Wikipedia abstracts consist of roughly the first paragraph
of the corresponding article
2.2 Evaluation
We will present evaluations on multiple corefer-
ence resolution metrics, as no single one is clearly
superior:
? Pairwise F1: precision, recall, and F1 over
all pairs of mentions in the same entity clus-
ter. Note that this over-penalizes the merger
or separation of clusters quadratically in the
size of the cluster.
? b
3
(Amit and Baldwin, 1998): For each men-
tion, form the intersection between the pre-
dicted cluster and the true cluster for that
mention. The precision is the ratio of the in-
tersection and the true cluster sizes and recall
the ratio of the intersection to the predicted
sizes; F1 is given by the harmonic mean over
precision and recall from all mentions.
? MUC (Vilain et al, 1995): For each true clus-
ter, compute the number of predicted clusters
which need to be merged to cover the true
cluster. Divide this quantity by true cluster
size minus one. Recall is given by the same
procedure with predicated and true clusters
reversed.
4
? CEAF (Luo, 2005): For a similarity function
between predicted and true clusters, CEAF
scores the best match between true and pre-
dicted clusters using this function. We use
the ?
3
similarity function from Luo (2005).
3 System Description
In this section we develop our system and re-
port developmental results on ACE2004-ROTH-
DEV (see Section 2.1); we report pairwise F1 fig-
ures here, but report on many more evaluation
metrics in Section 4. At a high level, our system
resembles a pairwise coreference model (Soon et
al., 1999; Ng and Cardie, 2002; Bengston and
Roth, 2008); for each mention m
i
, we select ei-
ther a single-best antecedent amongst the previ-
ous mentions m
1
, . . . ,m
i?1
, or the NULL men-
tion to indicate the underlying entity has not yet
been evoked. Mentions are linearly ordered ac-
cording to the position of the mention head with
ties being broken by the larger node coming first.
4
The MUC measure is problematic when the system pre-
dicts many more clusters than actually exist (Luo, 2005;
Finkel and Manning, 2008); also, singleton clusters do not
contribute to evaluation.
1153
While much research (Ng and Cardie, 2002; Cu-
lotta et al, 2007; Haghighi and Klein, 2007; Poon
and Domingos, 2008; Finkel and Manning, 2008)
has explored how to reconcile pairwise decisions
to form coherent clusters, we simply take the tran-
sitive closure of our pairwise decision (as in Ng
and Cardie (2002) and Bengston and Roth (2008))
which can and does cause system errors.
In contrast to most recent research, our pair-
wise decisions are not made with a learned model
which outputs a probability or confidence, but in-
stead for each mentionm
i
, we select an antecedent
amongst m
1
, . . . ,m
i?1
or the NULL mention as
follows:
? Syntactic Constraint: Based on syntac-
tic configurations, either force or disallow
coreference between the mention and an an-
tecedent. Propagate this constraint (see Fig-
ure 4).
? Semantic/Syntactic Filter: Filter the re-
maining possible antecedents based upon
compatibility with the mention (see Fig-
ure 2).
? Selection: Select the ?closest? mention from
the set of remaining possible antecedents (see
Figure 1) or the NULL antecedent if empty.
Initially, there is no syntactic constraint (im-
proved in Section 3.1.3), the antecedent com-
patibility filter allows proper and nominal men-
tions to corefer only with mentions that have the
same head (improved in Section 3.2), and pro-
nouns have no compatibility constraints (improved
in Section 3.1.2). Mention heads are determined
by parsing the given mention span with the Stan-
ford parser (Klein and Manning, 2003) and us-
ing the Collins head rules (Collins, 1999); Poon
and Domingos (2008) showed that using syntactic
heads strongly outperformed a simple rightmost
headword rule. The mention type is determined
by the head POS tag: proper if the head tag is NNP
or NNPS, pronoun if the head tag is PRP, PRP$, WP,
or WP$, and nominal otherwise.
For the selection phase, we order mentions
m
1
, . . . ,m
i?1
according to the position of the
head word and select the closest mention that re-
mains after constraint and filtering are applied.
This choice reflects the intuition of Grosz et al
(1995) that speakers only use pronominal men-
tions when there are not intervening compatible
S!!!!!!!"""""""
NP#1###$$$
NP
NNP
Nintendo
PP%%&&IN
of
NP#2
NNP
America
VP''''((((VBD
announced
NP#3)))***NP#1
PRP$
its
NP%%&&JJ
new
NN
console
Figure 1: Example sentence where closest tree dis-
tance between mentions outperforms raw distance.
For clarity, each mention NP is labeled with the
underlying entity id.
mentions. This system yields a rather low 48.9
pairwise F1 (see BASE-FLAT in Table 2). There
are many, primarily recall, errors made choos-
ing antecedents for all mention types which we
will address by adding syntactic and semantic con-
straints.
3.1 Adding Syntactic Information
In this section, we enrich the syntactic represen-
tation and information in our system to improve
results.
3.1.1 Syntactic Salience
We first focus on fixing the pronoun antecedent
choices. A common error arose from the use of
mention head distance as a poor proxy for dis-
course salience. For instance consider the exam-
ple in Figure 1, the mention America is closest
to its in flat mention distance, but syntactically
Nintendo of America holds a more prominent syn-
tactic position relative to the pronoun which, as
Hobbs (1977) argues, is key to discourse salience.
MappingMentions to Parse Nodes: In order to
use the syntactic position of mentions to determine
anaphoricity, we must associate each mention in
the document with a parse tree node. We parse
all document sentences with the Stanford parser,
and then for each evaluation mention, we find the
largest-span NP which has the previously deter-
mined mention head as its head.
5
Often, this re-
sults in a different, typically larger, mention span
than annotated in the data.
Now that each mention is situated in a parse
tree, we utilize the length of the shortest tree path
between mentions as our notion of distance. In
5
If there is no NP headed by a given mention head, we
add an NP over just that word.
1154
S!!!!!!!!""""""""NP-ORG#1###$$$The Israelis
VP!!!!!!!!!!!%%%% """""""""""VBP
regard
NP#2###$$$NP
the site
PP&&''IN
as
NP#2&&''a shrine
SBAR(((((()******IN
because
PP++,,TO
to
NP#1
PRP
them
S###$$$it is sacred
Figure 2: Example of a coreference decision fixed
by agreement constraints (see Section 3.1.2). The
pronoun them is closest to the sitemention, but has
an incompatible number feature with it. The clos-
est (in tree distance, see Section 3.1.1) compatible
mention is The Israelis, which is correct
particular, this fixes examples such as those in
Figure 1 where the true antecedent has many em-
bedded mentions between itself and the pronoun.
This change by itself yields 51.7 pairwise F1 (see
BASE-TREE in Table 2), which is small overall, but
reduces pairwise pronoun antecedent selection er-
ror from 51.3% to 42.5%.
3.1.2 Agreement Constraints
We now refine our compatibility filtering to in-
corporate simple agreement constraints between
coreferent mentions. Since we currently allow
proper and nominal mentions to corefer only with
matching head mentions, agreement is only a con-
cern for pronouns. Traditional linguistic theory
stipulates that coreferent mentions must agree in
number, person, gender, and entity type (e.g. an-
imacy). Here, we implement person, number and
entity type agreement.
6
A number feature is assigned to each mention
deterministically based on the head and its POS
tag. For entity type, we use NER labels. Ideally,
we would like to have information about the en-
tity type of each referential NP, however this in-
formation is not easily obtainable. Instead, we opt
to utilize the Stanford NER tagger (Finkel et al,
2005) over the sentences in a document and anno-
tate each NP with the NER label assigned to that
mention head. For each mention, when its NP is
assigned an NER label we allow it to only be com-
patible with that NER label.
7
For pronouns, we
deterministically assign a set of compatible NER
values (e.g. personal pronouns can only be a PER-
6
Gender agreement, while important for general corefer-
ence resolution, did not contribute to the errors in our largely
newswire data sets.
7
Or allow it to be compatible with all NER labels if the
NER tagger doesn?t predict a label.
gore president florida state
bush governor lebanese territory
nation people arafat leader
inc. company aol company
nation country assad president
Table 1: Most common recall (missed-link) errors
amongst non-pronoun mention heads on our de-
velopment set. Detecting compatibility requires
semantic knowledge which we obtain from a large
corpus (see Section 3.2).
S
`
`
`
 
 
 
NP#1
NNP
Wal-Mart
VP
h
h
h
h
(
(
(
(
VBZ
says
S
h
h
h
h
(
(
(
(
NP#2
X
X


NP
NNP
Gitano
,
,
NP-APPOS#2
P
P


NP#1
PRP
its
JJ
top
NNS
brand
VP
P
P


is underselling
Figure 4: Example of interaction between the ap-
positive and i-within-i constraint. The i-within-
i constraint disallows coreference between parent
and child NPs unless the child is an appositive.
Hashed numbers indicate ground truth but are not
in the actual trees.
SON, but its can be an ORGANIZATION or LOCA-
TION). Since the NER tagger typically does not
label non-proper NP heads, we have no NER com-
patibility information for nominals.
We incorporate agreement constraints by filter-
ing the set of possible antecedents to those which
have compatible number and NER types with the
target mention. This yields 53.4 pairwise F1, and
reduces pronoun antecedent errors to 42.5% from
34.4%. An example of the type of error fixed by
these agreement constraints is given by Figure 2.
3.1.3 Syntactic Configuration Constraints
Our system has so far focused only on improving
pronoun anaphora resolution. However, a plurality
of the errors made by our system are amongst non-
pronominal mentions.
8
We take the approach that
in order to align a non-pronominal mention to an
antecedent without an identical head, we require
evidence that the mentions are compatible.
Judging compatibility of mentions generally re-
quires semantic knowledge, to which we return
later. However, some syntactic configurations
8
There are over twice as many nominal mentions in our
development data as pronouns.
1155
NP#1!!!!!!!"""""""
NP####$$$$NN#1
painter
NNP
Pablo
NNP
Picasso
,
,
NP#1%%%%%%&&&&&&subject of the [exhibition]2
NP-PERS#1!!!!!!!!""########NP$$$$%%%%NP-APPOS#1
NN
painter
NP-PERS&&''NNP
Pablo
NNP
Picasso
,
,
NP-APPOS#1(((((())))))subject of the [exhibition]2
(a) (b)
Figure 3: NP structure annotation: In (a) we have the raw parse from the Klein and Manning (2003)
parser with the mentions annotated by entity. In (b), we demonstrate the annotation we have added. NER
labels are added to all NP according to the NER label given to the head (see Section 3.1.1). Appositive
NPs are also annotated. Hashes indicate forced coreferent nodes
guarantee coreference. The one exploited most
in coreference work (Soon et al, 1999; Ng and
Cardie, 2002; Luo et al, 2004; Culotta et al, 2007;
Poon and Domingos, 2008; Bengston and Roth,
2008) is the appositive construction. Here, we rep-
resent apposition as a syntactic feature of an NP
indicating that it is coreferent with its parent NP
(e.g. it is an exception to the i-within-i constraint
that parent and child NPs cannot be coreferent).
We deterministically mark a node as NP-APPOS
(see Figure 3) when it is the third child in of a par-
ent NP whose expansion begins with (NP , NP),
and there is not a conjunction in the expansion (to
avoid marking elements in a list as appositive).
Role Appositives: During development, we dis-
covered many errors which involved a variant of
appositives which we call ?role appositives? (see
painter in Figure 3), where an NP modifying the
head NP describes the role of that entity (typi-
cally a person entity). There are several challenges
to correctly labeling these role NPs as being ap-
positives. First, the NPs produced by Treebank
parsers are flat and do not have the required inter-
nal structure (see Figure 3(a)). While fully solving
this problem is difficult, we can heuristically fix
many instances of the problem by placing an NP
around maximum length sequences of NNP tags
or NN (and JJ) tags within an NP; note that this
will fail for many constructions such as U.S. Pres-
ident Barack Obama, which is analyzed as a flat
sequence of proper nouns. Once this internal NP
structure has been added, whether the NP immedi-
ately to the left of the head NP is an appositive de-
pends on the entity type. For instance, Rabbi Ashi
is an apposition but Iranian army is not. Again, a
full solution would require its own model, here we
mark as appositions any NPs immediately to the
left of a head child NP where the head child NP is
identified as a person by the NER tagger.
9
We incorporate NP appositive annotation as a
constraint during filtering. Any mention which
corresponds to an appositive node has its set of
possible antecedents limited to its parent. Along
with the appositive constraint, we implement the
i-within-i constraint that any non-appositive NP
cannot be be coreferent with its parent; this con-
straint is then propagated to any node its parent
is forced to agree with. The order in which these
constraints are applied is important, as illustrated
by the example in Figure 4: First the list of pos-
sible antecedents for the appositive NP is con-
strained to only its parent. Now that all apposi-
tives have been constrained, we apply the i-within-
i constraint, which prevents its from having the NP
headed by brand in the set of possible antecedents,
and by propagation, also removes the NP headed
by Gitano. This leaves the NP Wal-Mart as the
closest compatible mention.
Adding these syntactic constraints to our system
yields 55.4 F1, a fairly substantial improvement,
but many recall errors remain between mentions
with differing heads. Resolving such cases will
require external semantic information, which we
will automatically acquire (see Section 3.2).
Predicate Nominatives: Another syntactic con-
straint exploited in Poon and Domingos (2008) is
the predicate nominative construction, where the
object of a copular verb (forms of the verb be) is
constrained to corefer with its subject (e.g. Mi-
crosoft is a company in Redmond). While much
less frequent than appositive configurations (there
are only 17 predicate nominatives in our devel-
9
Arguably, we could also consider right modifying NPs
(e.g., [Microsoft [Company]
1
]
1
) to be role appositive, but we
do not do so here.
1156
Path Example
NP!!!"""
NP-NNP PRN-NNP
NP#####$$
%%%%%
NP-president CC NP-NNP
America Online Inc. (AOL)
NP
NP-NNP PRN-NNP
NP
$$
NP-president CC NP-NNP
[President and C.E.O] Bill Gates
Figure 5: Example paths extracted via semantic compatibility mining (see Section 3.2) along with exam-
ple instantiations. In both examples the left child NP is coreferent with the rightmost NP. Each category
in the interior of the tree path is annotated with the head word as well as its subcategorization. The
examples given here collapse multiple instances of extracted paths.
opment set), predicate nominatives are another
highly reliable coreference pattern which we will
leverage in Section 3.2 to mine semantic knowl-
edge. As with appositives, we annotate object
predicate-nominative NPs and constrain corefer-
ence as before. This yields a minor improvement
to 55.5 F1.
3.2 Semantic Knowledge
While appositives and related syntactic construc-
tions can resolve some cases of non-pronominal
reference, most cases require semantic knowledge
about the various entities as well as the verbs used
in conjunction with those entities to disambiguate
references (Kehler et al, 2008).
However, given a semantically compatible men-
tion head pair, say AOL and company, one
might expect to observe a reliable appositive
or predicative-nominative construction involving
these mentions somewhere in a large corpus.
In fact, the Wikipedia page for AOL
10
has a
predicate-nominative construction which supports
the compatibility of this head pair: AOL LLC (for-
merly America Online) is an American global In-
ternet services and media company operated by
Time Warner.
In order to harvest compatible head pairs, we
utilize our BLIPP and WIKI data sets (see Sec-
tion 2), and for each noun (proper or common) and
pronoun, we assign a maximal NP mention node
for each nominal head as in Section 3.1.1; we then
annotate appositive and predicate-nominative NPs
as in Section 3.1.3. For any NP which is annotated
as an appositive or predicate-nominative, we ex-
tract the head pair of that node and its constrained
antecedent.
10
http://en.wikipedia.org/wiki/AOL
The resulting set of compatible head words,
while large, covers a little more than half of the
examples given in Table 1. The problem is that
these highly-reliable syntactic configurations are
too sparse and cannot capture all the entity infor-
mation present. For instance, the first sentence of
Wikipedia abstract for Al Gore is:
Albert Arnold ?Al? Gore, Jr. is an
American environmental activist who
served as the 45th Vice President of the
United States from 1993 to 2001 under
President Bill Clinton.
The required lexical pattern X who served as Y is
a general appositive-like pattern that almost surely
indicates coreference. Rather than opt to manu-
ally create a set of these coreference patterns as in
Hearst (1992), we instead opt to automatically ex-
tract these patterns from large corpora as in Snow
et al (2004) and Phillips and Riloff (2007). We
take a simple bootstrapping technique: given a
set of mention pairs extracted from appositives
and predicate-nominative configurations, we ex-
tract counts over tree fragments between nodes
which have occurred in this set of head pairs (see
Figure 5); the tree fragments are formed by an-
notating the internal nodes in the tree path with
the head word and POS along with the subcatego-
rization. We limit the paths extracted in this way
in several ways: paths are only allowed to go be-
tween adjacent sentences and have a length of at
most 10. We then filter the set of paths to those
which occur more than a hundred times and with
at least 10 distinct seed head word pairs.
The vast majority of the extracted fragments are
variants of traditional appositives and predicate-
nominatives with some of the structure of the NPs
1157
MUC b
3
Pairwise CEAF
System P R F1 P R F1 P R F1 P R F1
ACE2004-ROTH-DEV
BASIC-FLAT 73.5 66.8 70.0 80.6 68.6 74.1 63.6 39.7 48.9 68.4 68.4 68.4
BASIC-TREE 75.8 68.9 72.2 81.9 69.9 75.4 65.6 42.7 51.7 69.8 69.8 69.8
+SYN-COMPAT 77.8 68.5 72.9 84.1 69.7 76.2 71.0 43.1 53.4 69.8 69.8 69.8
+SYN-CONSTR 78.3 70.5 74.2 84.0 71.0 76.9 71.3 45.4 55.5 70.8 70.8 70.8
+SEM-COMPAT 77.9 74.1 75.9 81.8 74.3 77.9 68.2 51.2 58.5 72.5 72.5 72.5
ACE2004-CULOTTA-TEST
BASIC-FLAT 68.6 60.9 64.5 80.3 68.0 73.6 57.1 30.5 39.8 66.5 66.5 66.5
BASIC-TREE 71.2 63.2 67.0 81.6 69.3 75.0 60.1 34.5 43.9 67.9 67.9 67.9
+SYN-COMPAT 74.6 65.2 69.6 84.2 70.3 76.6 66.7 37.2 47.8 69.2 69.2 69.2
+SYN-CONSTR 74.3 66.4 70.2 83.6 71.0 76.8 66.4 38.0 48.3 69.6 69.6 69.6
+SEM-COMPAT 74.8 77.7 79.6 79.6 78.5 79.0 57.5 57.6 57.5 73.3 73.3 73.3
Supervised Results
Culotta et al (2007) - - - 86.7 73.2 79.3 - - - - - -
Bengston and Roth (2008) 82.7 69.9 75.8 88.3 74.5 80.8 55.4 63.7 59.2 - - -
MUC6-TEST
+SEM-COMPAT 87.2 77.3 81.9 84.7 67.3 75.0 80.5 57.8 67.3 72.0 72.0 72.0
Unsupervised Results
Poon and Domingos (2008) 83.0 75.8 79.2 - - - 63.0 57.0 60.0 - - -
Supervised Results
Finkel and Manning (2008) 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 - - -
ACE2004-NWIRE
+SEM-COMPAT 77.0 75.9 76.5 79.4 74.5 76.9 66.9 49.2 56.7 71.5 71.5 71.5
Unsupervised Results
Poon and Domingos (2008) 71.3 70.5 70.9 - - - 62.6 38.9 48.0 - - -
Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the
largest result is bolded. The CEAF measure has equal values for precision, recall, and F1.
specified. However there are some tree fragments
which correspond to the novel coreference pat-
terns (see Figure 5) of parenthetical alias as well
as conjunctions of roles in NPs.
We apply our extracted tree fragments to our
BLIPP and WIKI data sets and extract a set of com-
patible word pairs which match these fragments;
these words pairs will be used to relax the seman-
tic compatibility filter (see the start of the section);
mentions are compatible with prior mentions with
the same head or with a semantically compatible
head word. This yields 58.5 pairwise F1 (see SEM-
COMPAT in Table 2) as well as similar improve-
ments across other metrics.
By and large the word pairs extracted in this
way are correct (in particular we now have cov-
erage for over two-thirds of the head pair recall
errors from Table 1.) There are however word-
pairs which introduce errors. In particular city-
state constructions (e.g. Los Angeles, California)
appears to be an appositive and incorrectly allows
our system to have angeles as an antecedent for
california. Another common error is that the %
symbol is made compatible with a wide variety of
common nouns in the financial domain.
4 Experimental Results
We present formal experimental results here
(see Table 2). We first evaluate our model
on the ACE2004-CULOTTA-TEST dataset used in
the state-of-the-art systems from Culotta et al
(2007) and Bengston and Roth (2008). Both of
these systems were supervised systems discrimi-
natively trained to maximize b
3
and used features
from many different structured resources includ-
ing WordNet, as well as domain-specific features
(Culotta et al, 2007). Our best b
3
result of 79.0
is broadly in the range of these results. We should
note that in our work we use neither the gold men-
tion types (we do not model pre-nominals sepa-
rately) nor do we use the gold NER tags which
Bengston and Roth (2008) does. Across metrics,
the syntactic constraints and semantic compatibil-
ity components contribute most to the overall final
result.
On the MUC6-TEST dataset, our system outper-
1158
PR
O
P
E
R
N
O
M
I
N
A
L
P
R
O
N
O
U
N
N
U
L
L
T
O
T
A
L
PROPER 21/451 8/20 - 72/288 101/759
NOMINAL 16/150 99/432 - 158/351 323/933
PRONOUN 29/149 60/128 15/97 1/2 105/376
Table 3: Errors for each type of antecedent deci-
sion made by the system. Each row is a mention
type and the column the predicted mention type
antecedent. The majority of errors are made in the
NOMINAL category.
forms both Poon and Domingos (2008) (an un-
supervised Markov Logic Network system which
uses explicit constraints) and Finkel and Manning
(2008) (a supervised system which uses ILP in-
ference to reconcile the predictions of a pairwise
classifier) on all comparable measures.
11
Simi-
larly, on the ACE2004-NWIRE dataset, we also out-
perform the state-of-the-art unsupervised system
of Poon and Domingos (2008).
Overall, we conclude that our system outper-
forms state-of-the-art unsupervised systems
12
and
is in the range of the state-of-the art systems of Cu-
lotta et al (2007) and Bengston and Roth (2008).
5 Error Analysis
There are several general trends to the errors made
by our system. Table 3 shows the number of
pairwise errors made on MUC6-TEST dataset by
mention type; note these errors are not equally
weighted in the final evaluations because of the
transitive closure taken at the end. The most er-
rors are made on nominal mentions with pronouns
coming in a distant second. In particular, we most
frequently say a nominal is NULL when it has an
antecedent; this is typically due to not having the
necessary semantic knowledge to link a nominal
to a prior expression.
In order to get a more thorough view of the
cause of pairwise errors, we examined 20 random
errors made in aligning each mention type to an
antecedent. We categorized the errors as follows:
? SEM. COMPAT: Missing information about
the compatibility of two words e.g. pay and
wage. For pronouns, this is used to mean that
11
Klenner and Ailloud (2007) took essentially the same ap-
proach but did so on non-comparable data.
12
Poon and Domingos (2008) outperformed Haghighi and
Klein (2007). Unfortunately, we cannot compare against Ng
(2008) since we do not have access to the version of the ACE
data used in their evaluation.
we incorrectly aligned a pronoun to a men-
tion with which it is not semantically com-
patible (e.g. he aligned to board).
? SYN. COMPAT: Error in assigning linguistic
features of nouns for compatibility with pro-
nouns (e.g. disallowing they to refer to team).
? HEAD: Errors involving the assumption that
mentions with the same head are always com-
patible. Includes modifier and specificity er-
rors such as allowing Lebanon and Southern
Lebanon to corefer. This also includes errors
of definiteness in nominals (e.g. the people
in the room and Chinese people). Typically,
these errors involve a combination of missing
syntactic and semantic information.
? INTERNAL NP: Errors involving lack of inter-
nal NP structure to mark role appositives (see
Section 3.1.3).
? PRAG. / DISC.: Errors where discourse salience
or pragmatics are needed to disambiguate
mention antecedents.
? PROCESS ERROR: Errors which involved a tok-
enization, parse, or NER error.
The result of this error analysis is given in Ta-
ble 4; note that a single error may be attributed to
more than one cause. Despite our efforts in Sec-
tion 3 to add syntactic and semantic information
to our system, the largest source of error is still
a combination of missing semantic information or
annotated syntactic structure rather than the lack
of discourse or salience modeling.
Our error analysis suggests that in order to im-
prove the state-of-the-art in coreference resolu-
tion, future research should consider richer syntac-
tic and semantic information than typically used in
current systems.
6 Conclusion
Our approach is not intended as an argument
against the more complex, discourse-focused ap-
proaches that typify recent work. Instead, we note
that rich syntactic and semantic processing vastly
reduces the need to rely on discourse effects or ev-
idence reconciliation for reference resolution. In-
deed, we suspect that further improving the syn-
tactic and semantic modules in our system may
produce greater error reductions than any other
1159
Mention Type SEM. COMPAT SYN. COMPAT HEAD INTENAL NP PRAG / DISC. PROCESS ERROR OTHER Comment
NOMINAL 7 - 5 6 2 2 1 2 general appos. patterns
PRONOUN 6 3 - 6 3 3 3 2 cataphora
PROPER 6 - 3 4 4 4 1
Table 4: Error analysis on ACE2004-CULOTTA-TEST data by mention type. The dominant errors are in
either semantic or syntactic compatibility of mentions rather than discourse phenomena. See Section 5.
route forward. Of course, a system which is rich
in all axes will find some advantage over any sim-
plified approach.
Nonetheless, our coreference system, despite
being relatively simple and having no tunable pa-
rameters or complexity beyond the non-reference
complexity of its component modules, manages
to outperform state-of-the-art unsupervised coref-
erence resolution and be broadly comparable to
state-of-the-art supervised systems.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
Eric Bengston and Dan Roth. 2008. Understanding the
value of features for corefernce resolution. In Em-
pirical Methods in Natural Language Processing.
E. Charniak. 2000. Maximum entropy inspired parser.
In North American Chapter of the Association of
Computational Linguistics (NAACL).
Mike Collins. 1999. Head-driven statistical models for
natural language parsing.
A Culotta, M Wick, R Hall, and A McCallum. 2007.
First-order probabilistic models for coreference res-
olution. In NAACL-HLT.
Pascal Denis and Jason Baldridge. 2007. Global,
Joint Determination of Anaphoricity and Corefer-
ence Resolution using Integer Programming. In
HLT-NAACL.
Jenny Finkel and Christopher Manning. 2008. Enforc-
ing transitivity in coreference resolution. In Associ-
ation of Computational Linguists (ACL).
Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by gibbs sam-
pling. In ACL.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for modelling
the local coherence of discourse.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. As-
sociation for Computational Linguistics.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Conference on
Natural Language Learning (COLING).
J. R. Hobbs. 1977. Resolving pronoun references.
Lingua.
Andrew Kehler, Laura Kertz, Hannah Rohde, and Jef-
frey Elman. 2008. Coherence and coreference re-
visited.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Association of Computational Lin-
guists (ACL).
Manfred Klenner and Etienne Ailloud. 2007. Op-
timization in coreference resolution is not needed:
A nearly-optimal algorithm with intensional con-
straints. In Recent Advances in Natural Language
Processing.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Association of
Computational Linguists.
X Luo. 2005. On coreference resolution performance
metrics. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approaches to Coreference Resolu-
tion. In Association of Computational Linguists.
Vincent Ng. 2008. Unsupervised models of corefer-
ence resolution. In EMNLP.
W. Phillips and E. Riloff. 2007. Exploiting role-
identifying nouns and expressions for information
extraction. In Recent Advances in Natural Language
Processing (RANLP).
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
R. Snow, D. Jurafsky, and A. Ng. 2004. Learning syn-
tactic patterns for automatic hypernym discovery. In
Neural Information Processing Systems (NIPS).
W.H. Soon, H. T. Ng, and D. C. Y. Lim. 1999. A
machine learning approach to coreference resolution
of noun phrases.
1160
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC-6.
1161
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418?1427,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Consensus Training for Consensus Decoding in Machine Translation
Adam Pauls, John DeNero and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,denero,klein}@cs.berkeley.edu
Abstract
We propose a novel objective function for dis-
criminatively tuning log-linear machine trans-
lation models. Our objective explicitly op-
timizes the BLEU score of expected n-gram
counts, the same quantities that arise in forest-
based consensus and minimum Bayes risk de-
coding methods. Our continuous objective
can be optimized using simple gradient as-
cent. However, computing critical quantities
in the gradient necessitates a novel dynamic
program, which we also present here. As-
suming BLEU as an evaluation measure, our
objective function has two principle advan-
tages over standard max BLEU tuning. First,
it specifically optimizes model weights for
downstream consensus decoding procedures.
An unexpected second benefit is that it reduces
overfitting, which can improve test set BLEU
scores when using standard Viterbi decoding.
1 Introduction
Increasing evidence suggests that machine trans-
lation decoders should not search for a single
top scoring Viterbi derivation, but should instead
choose a translation that is sensitive to the model?s
entire predictive distribution. Several recent con-
sensus decoding methods leverage compact repre-
sentations of this distribution by choosing transla-
tions according to n-gram posteriors and expected
counts (Tromble et al, 2008; DeNero et al, 2009;
Li et al, 2009; Kumar et al, 2009). This change
in decoding objective suggests a complementary
change in tuning objective, to one that optimizes
expected n-gram counts directly. The ubiquitous
minimum error rate training (MERT) approach op-
timizes Viterbi predictions, but does not explicitly
boost the aggregated posterior probability of de-
sirable n-grams (Och, 2003).
We therefore propose an alternative objective
function for parameter tuning, which we call con-
sensus BLEU or CoBLEU, that is designed to
maximize the expected counts of the n-grams that
appear in reference translations. To maintain con-
sistency across the translation pipeline, we for-
mulate CoBLEU to share the functional form of
BLEU used for evaluation. As a result, CoBLEU
optimizes exactly the quantities that drive efficient
consensus decoding techniques and precisely mir-
rors the objective used for fast consensus decoding
in DeNero et al (2009).
CoBLEU is a continuous and (mostly) differ-
entiable function that we optimize using gradient
ascent. We show that this function and its gradient
are efficiently computable over packed forests of
translations generated by machine translation sys-
tems. The gradient includes expectations of prod-
ucts of features and n-gram counts, a quantity that
has not appeared in previous work. We present a
new dynamic program which allows the efficient
computation of these quantities over translation
forests. The resulting gradient ascent procedure
does not require any k-best approximations. Op-
timizing over translation forests gives similar sta-
bility benefits to recent work on lattice-based min-
imum error rate training (Macherey et al, 2008)
and large-margin training (Chiang et al, 2008).
We developed CoBLEU primarily to comple-
ment consensus decoding, which it does; it pro-
duces higher BLEU scores than coupling MERT
with consensus decoding. However, we found
an additional empirical benefit: CoBLEU is less
prone to overfitting than MERT, even when using
Viterbi decoding. In experiments, models trained
to maximize tuning set BLEU using MERT con-
sistently degraded in performance from tuning to
test set, while CoBLEU-trained models general-
ized more robustly. As a result, we found that op-
timizing CoBLEU improved test set performance
reliably using consensus decoding and occasion-
ally using Viterbi decoding.
1418
Once upon a rhyme
H
1
) Once on a rhyme
H
3
) Once upon a time
H
2
) Once upon a rhyme
Il ?tait une rime
(a) Tuning set sentence and translation
(a) Hypotheses ranked by ?
TM 
= ?
LM 
= 1
(a)  Model score as a function of ?
LM
 
Reference r:
Sentence f:
TM LM
-3 -7 0.67
-5 -6 0.24
-9 -3 0.09
Pr
(b)  Objectives as functions of ?
LM
(b) Computing Consensus Bigram Precision
-18
-12
-6
0
0 2
H
3
H
1
H
2
Parameter: ?
LM
M
o
d
e
l
:
 
T
M
 
+
 
?
L
M
 
?
 
L
M
 
V
i
t
e
r
b
i
 
&
 
C
o
n
s
e
n
s
u
s
 
O
b
j
e
c
t
i
v
e
s
Parameter: ?
LM
E
?
[c(?Once upon?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?upon a?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?a rhyme?, d)|f ] = 0.67 + 0.24 = 0.91
?
g
E
?
[c(g, d)|f ] = 3[0.67 + 0.24 + 0.09]
?
g
min{E
?
[c(g, d)|f ], c(g, r)}
?
g
E
?
[c(g, d)|f ]
=
0.33 + 0.33 + 0.91
3
Figure 1: (a) A simple hypothesis space of translations
for a single sentence containing three alternatives, each
with two features. The hypotheses are scored under a
log-linear model with parameters ? equal to the identity
vector. (b) The expected counts of all bigrams that ap-
pear in the computation of consensus bigram precision.
2 Consensus Objective Functions
Our proposed objective function maximizes n-
gram precision by adapting the BLEU evaluation
metric as a tuning objective (Papineni et al, 2002).
To simplify exposition, we begin by adapting a
simpler metric: bigram precision.
2.1 Bigram Precision Tuning
Let the tuning corpus consist of source sentences
F = f
1
. . . f
m
and human-generated references
R = r
1
. . . r
m
, one reference for each source
sentence. Let e
i
be a translation of f
i
, and let
E = e
1
. . . e
m
be a corpus of translations, one for
each source sentence. A simple evaluation score
for E is its bigram precision BP(R,E):
BP(R,E) =
?
m
i=1
?
g
2
min{c(g
2
, e
i
), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, e
i
)
where g
2
iterates over the set of bigrams in the tar-
get language, and c(g
2
, e) is the count of bigram
g
2
in translation e. As in BLEU, we ?clip? the bi-
gram counts of e in the numerator using counts of
bigrams in the reference sentence.
Modern machine translation systems are typi-
cally tuned to maximize the evaluation score of
Viterbi derivations
1
under a log-linear model with
parameters ?. Let d
?
?
(f
i
) = arg max
d
P
?
(d|f
i
) be
the highest scoring derivation d of f
i
. For a system
employing Viterbi decoding and evaluated by bi-
gram precision, we would want to select ? to max-
imize MaxBP(R,F, ?):
?
m
i=1
?
g
2
min{c(g
2
, d
?
?
(f
i
)), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, d
?
?
(f
i
))
On the other hand, for a system that uses ex-
pected bigram counts for decoding, we would pre-
fer to choose ? such that expected bigram counts
match bigrams in the reference sentence. To this
end, we can evaluate an entire posterior distri-
bution over derivations by computing the same
clipped precision for expected bigram counts us-
ing CoBP(R,F, ?):
?
m
i=1
?
g
2
min{E
?
[c(g
2
, d)|f
i
], c(g
2
, r
i
)}
?
m
i=1
?
g
2
E
?
[c(g
2
, d)|f
i
]
(1)
where
E
?
[c(g
2
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(g
2
, d)
is the expected count of bigram g
2
in all deriva-
tions d of f
i
. We define the precise parametric
form of P
?
(d|f
i
) in Section 3. Figure 1 shows pro-
posed translations for a single sentence along with
the bigram expectations needed to compute CoBP.
Equation 1 constitutes an objective function for
tuning the parameters of a machine translation
model. Figure 2 contrasts the properties of CoBP
and MaxBP as tuning objectives, using the simple
example from Figure 1.
Consensus bigram precision is an instance of a
general recipe for converting n-gram based eval-
uation metrics into consensus objective functions
for model tuning. For the remainder of this pa-
per, we focus on consensus BLEU. However, the
techniques herein, including the optimization ap-
proach of Section 3, are applicable to many differ-
entiable functions of expected n-gram counts.
1
By derivation, we mean a translation of a foreign sen-
tence along with any latent structure assumed by the model.
Each derivation corresponds to a particular English transla-
tion, but many derivations may yield the same translation.
1419
1.0 1.5 2.0 2.5 3.0
-16
-14
-12
-10
?
LM
Log M
odel 
Score
H
1
H
2
H
3
(a)
0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
?
LM
Valu
e of O
bject
ive
CoBP
MaxBP
H
1
H
3
H
1
H
2
H
3
(b)
Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensus
bigram precision (CoBP) on the simple example from Figure 1. (a) MaxBP is only sensitive to the convex hull (the
solid line) of model scores. When varying the single parameter ?
LM
, it entirely disregards the correct translation
H
2
becauseH
2
never attains a maximal model score. (b) A plot of both objectives shows their differing characteris-
tics. The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would
select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP
is only sensitive to the single point of discontinuity between H
1
and H
3
, and disregards H
2
entirely. CoBP peaks
when the distribution most heavily favorsH
2
while suppressingH
1
. ThoughH
2
never has a maximal model score,
if ?
LM
is in the indicated range, consensus decoding would select H
2
, the desired translation.
2.2 CoBLEU
The logarithm of the single-reference
2
BLEU met-
ric (Papineni et al, 2002) has the following form:
ln BLEU(R,E) =
(
1?
|R|
?
m
i=1
?
g
1
c(g
1
, e
i
)
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{c(g
n
, e
i
), c(g
n
, r
i
)}
?
m
i=1
?
g
n
c(g
n
, e
i
)
Above, |R| denotes the number of words in the
reference corpus. The notation (?)
?
is shorthand
for min(?, 0). In the inner sums, g
n
iterates over
all n-grams of order n. In order to adapt BLEU
to be a consensus tuning objective, we follow the
recipe of Section 2.1: we replace n-gram counts
from a candidate translation with expected n-gram
counts under the model.
CoBLEU(R,F, ?)=
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
The brevity penalty term in BLEU is calculated
using the expected length of the corpus, which
2
Throughout this paper, we use only a single reference,
but our objective readily extends to multiple references.
equals the sum of all expected unigram counts.
We call this objective function consensus BLEU,
or CoBLEU for short.
3 Optimizing CoBLEU
Unlike the more common MaxBLEU tuning ob-
jective optimized by MERT, CoBLEU is con-
tinuous. For distributions P
?
(d|f
i
) that factor
over synchronous grammar rules and n-grams, we
show below that it is also analytically differen-
tiable, permitting a straightforward gradient ascent
optimization procedure.
3
In order to perform gra-
dient ascent, we require methods for efficiently
computing the gradient of the objective function
for a given parameter setting ?. Once we have the
gradient, we can perform an update at iteration t
of the form
?
(t+1)
? ?
(t)
+ ?
t
?
?
CoBLEU(R,F, ?
(t)
)
where ?
t
is an adaptive step size.
4
3
Technically, CoBLEU is non-differentiable at some
points because of clipping. At these points, we must com-
pute a sub-gradient, and so our optimization is formally sub-
gradient ascent. See the Appendix for details.
4
After each successful step, we grow the step size by a
constant factor. Whenever the objective does not decrease
after a step, we shrink the step size by a constant factor and
try again until a decrease is attained.
1420
head(h)
tail(h)
u=
Once
S
rhyme
v
1
=
Once
RB
Once
v
2
=
upon
IN
upon
v
3
=
a
NP
rhyme
c(?Once upon?, h)
c(?upon a?, h)
= 1
= 1
!
2
(h) = 2
Figure 3: A hyperedge h represents a ?rule? used in
syntactic machine translation. tail(h) refers to the ?chil-
dren? of the rule, while head(h) refers to the ?head? or
?parent?. A forest of translations is built by combining
the nodes v
i
using h to form a new node u = head(h).
Each forest node consists of a grammar symbol and tar-
get language boundary words used to track n-grams. In
the above, we keep one boundary word for each node,
which allows us to track bigrams.
In this section, we develop an analytical expres-
sion for the gradient of CoBLEU, then discuss
how to efficiently compute the value of the objec-
tive function and gradient.
3.1 Translation Model Form
We first assume the general hypergraph setting of
Huang and Chiang (2007), namely, that deriva-
tions under our translation model form a hyper-
graph. This framework allows us to speak about
both phrase-based and syntax-based translation in
a unified framework.
We define a probability distribution over deriva-
tions d via ? as:
P
?
(d|f
i
) =
w(d)
Z(f
i
)
with
Z(f
i
) =
?
d
?
w(d
?
)
where w(d) = exp(?
>
?(d, f
i
)) is the weight of a
derivation and ?(d, f
i
) is a featurized representa-
tion of the derivation d of f
i
. We further assume
that these features decompose over hyperedges in
the hypergraph, like the one in Figure 3. That is,
?(d, f
i
) =
?
h?d
?(h, f
i
).
In this setting, we can analytically compute the
gradient of CoBLEU. We provide a sketch of the
derivation of this gradient in the Appendix. In
computing this gradient, we must calculate the fol-
lowing expectations:
E
?
[c(?
k
, d)|f
i
] (2)
E
?
[`
n
(d)|f
i
] (3)
E
?
[c(?
k
, d) ? `
n
(d)|f
i
] (4)
where `
n
(d) =
?
g
n
c(g
n
, d) is the sum of all n-
grams on derivation d (its ?length?). The first ex-
pectation is an expected count of the kth feature
?
k
over all derivations of f
i
. The second is an ex-
pected length, the total expected count of all n-
grams in derivations of f
i
. We call the final ex-
pectation an expected product of counts. We now
present the computation of each of these expecta-
tions in turn.
3.2 Computing Feature Expectations
The expected feature counts E
?
[c(?
k
, d)|f
i
] can be
written as
E
?
[c(?
k
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(?
k
, d)
=
?
h
P
?
(h|f
i
)c(?
k
, h)
We can justify the second step since fea-
ture counts are local to hyperedges, i.e.
c(?
k
, d) =
?
h?d
c(?
k
, h). The posterior
probability P
?
(h|f
i
) can be efficiently computed
with inside-outside scores. Let I(u) and O(u) be
the standard inside and outside scores for a node
u in the forest.
5
P
?
(h|f
i
) =
1
Z(f)
w(h) O(head(h))
?
v?tail(h)
I(v)
where w(h) is the weight of hyperedge h, given
by exp(?
>
?(h)), and Z(f) = I(root) is the in-
side score of the root of the forest. Computing
these inside-outside quantities takes time linear in
the number of hyperedges in the forest.
3.3 Computing n-gram Expectations
We can compute the expectations of any specific
n-grams, or of total n-gram counts `, in the same
way as feature expectations, provided that target-
side n-grams are also localized to hyperedges (e.g.
consider ` to be a feature of a hyperedge whose
value is the number of n-grams on h). If the
nodes in our forests are annotated with target-side
5
Appendix Figure 7 gives recursions for I(u) and O(u).
1421
boundary words as in Figure 3, then this will be the
case. Note that this is the same approach used by
decoders which integrate a target language model
(e.g. Chiang (2007)). Other work has computed
n-gram expectations in the same way (DeNero et
al., 2009; Li et al, 2009).
3.4 Computing Expectations of Products of
Counts
While the previous two expectations can be com-
puted using techniques known in the literature, the
expected product of counts E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
is a novel quantity. Fortunately, an efficient dy-
namic program exists for computing this expec-
tation as well. We present this dynamic program
here as one of the contributions of this paper,
though we omit a full derivation due to space re-
strictions.
To see why this expectation cannot be computed
in the same way as the expected feature or n-gram
counts, we expand the definition of the expectation
above to get
?
d
P
?
(d|f
i
) [c(?
k
, d)`
n
(d)]
Unlike feature and n-gram counts, the product of
counts in brackets above does not decompose over
hyperedges, at least not in an obvious way. We
can, however, still decompose the feature counts
c(?
k
, d) over hyperedges. After this decomposi-
tion and a little re-arranging, we get
=
?
h
c(?
k
, h)
?
d:h?d
P
?
(d|f
i
)`
n
(d)
=
1
Z(f
i
)
?
h
c(?
k
, h)
[
?
d:h?d
w(d)`
n
(d)
]
=
1
Z(f
i
)
?
h
c(?
k
, h)
?
D
n
?
(h|f
i
)
The quantity
?
D
n
?
(h|f
i
) =
?
d:h?d
w(d)`
n
(d) is the
sum of the weight-length products of all deriva-
tions d containing hyperedge h. In the same
way that P
?
(h|f
i
) can be efficiently computed
from inside and outside probabilities, this quan-
tity
?
D
n
?
(h|f
i
) can be efficiently computed with two
new inside and outside quantities, which we call
?
I
n
(u) and
?
O
n
(u). We provide recursions for these
quantities in Figure 4. Like the standard inside and
outside computations, these recursions run in time
linear in the number of hyperedges in the forest.
While a full exposition of the algorithm is not
possible in the available space, we give some brief
intuition behind this dynamic program. We first
define
?
I
n
(u):
?
I
n
(u) =
?
d
u
w(d
u
)`
n
(d)
where d
u
is a derivation rooted at node u. This is
a sum of weight-length products similar to
?
D. To
give a recurrence for
?
I, we rewrite it:
?
I
n
(u) =
?
d
u
?
h?d
u
[w(d
u
)`
n
(h)]
Here, we have broken up the total value of `
n
(d)
across hyperedges in d. The bracketed quantity
is a score of a marked derivation pair (d, h) where
the edge h is some specific element of d. The score
of a marked derivation includes the weight of the
derivation and the factor `
n
(h) for the marked hy-
peredge.
This sum over marked derivations gives the in-
side recurrence in Figure 4 by the following de-
composition. For
?
I
n
(u) to sum over all marked
derivation pairs rooted at u, we must consider two
cases. First, the marked hyperedge could be at the
root, in which case we must choose child deriva-
tions from regular inside scores and multiply in the
local `
n
, giving the first summand of
?
I
n
(u). Alter-
natively, the marked hyperedge is in exactly one
of the children; for each possibility we recursively
choose a marked derivation for one child, while
the other children choose regular derivations. The
second summand of
?
I
n
(u) compactly expresses
a sum over instances of this case.
?
O
n
(u) de-
composes similarly: the marked hyperedge could
be local (first summand), under a sibling (second
summand), or higher in the tree (third summand).
Once we have these new inside-outside quanti-
ties, we can compute
?
D as in Figure 5. This com-
bination states that marked derivations containing
h are either marked at h, below h, or above h.
As a final detail, computing the gradient
?C
clip
n
(?) (see the Appendix) involves a clipped
version of the expected product of counts, for
which a clipped
?
D is required. This quantity can
be computed with the same dynamic program with
a slight modification. In Figure 4, we show the dif-
ference as a choice point when computing `
n
(h).
3.5 Implementation Details
As stated, the runtime of computing the required
expectations for the objective and gradient is lin-
ear in the number of hyperedges in the forest. The
1422
?I
n
(u) =
?
h?IN(u)
w(h)
?
?
`
n
(h)
?
v?tail(h)
I(v) +
?
v?tail(h)
?
I
n
(v)
?
w 6=v
I(w)
?
?
?
O
n
(u) =
?
h?OUT(u)
w(h)
?
?
?
?
?
?
`
n
(h) O(head(h))
?
v?tail(h)
v 6=u
I(v) + O(head(h))
?
v?tail(h)
v 6=u
?
I
n
(v)
?
w?tail(h)
w 6=v
w 6=u
I(w) +
?
O
n
(head(h))
?
w?tail(h)
w 6=u
I(w)
?
?
?
?
?
?
`
n
(h) =
{
?
g
n
c(g
n
, h) computing unclipped counts
?
g
n
c(g
n
, h)1 [E
?
[c(g
n
, d)] ? c(g
n
, r
i
)] computing clipped counts
Figure 4: Inside and Outside recursions for
?
I
n
(u) and
?
O
n
(u). IN(u) and OUT(u) refer to the incoming and
outgoing hyperedges of u, respectively. I(?) and O(?) refer to standard inside and outside quantities, defined in
Appendix Figure 7. We initialize with
?
I
n
(u) = 0 for all terminal forest nodes u and
?
O
n
(root) = 0 for the root
node. `
n
(h) computes the sum of all n-grams of order n on a hyperedge h.
?
D
n
?
(h|f
i
) =
w(h)
?
?
?
?
`
n
(h)O(head(h))
?
v?tail(h)
I(v) + O(head(h))
?
v?tail(h)
?
I
n
(v)
?
v?tail(h)
w 6=v
I(w) +
?
O
n
(head(h))
?
w?tail(h)
I(w)
?
?
?
?
Figure 5: Calculation of
?
D
n
?
(h|f
i
) after
?
I
n
(u) and
?
O
n
(u) have been computed.
number of hyperedges is very large, however, be-
cause we must track n-gram contexts in the nodes,
just as we would in an integrated language model
decoder. These contexts are required both to cor-
rectly compute the model score of derivations and
to compute clipped n-gram counts. To speed our
computations, we use the cube pruning method of
Huang and Chiang (2007) with a fixed beam size.
For regularization, we added an L
2
penalty on
the size of ? to the CoBLEU objective, a simple
addition for gradient ascent. We did not find that
our performance varied very much for moderate
levels of regularization.
3.6 Related Work
The calculation of expected counts can be for-
mulated using the expectation semiring frame-
work of Eisner (2002), though that work does
not show how to compute expected products of
counts which are needed for our gradient calcu-
lations. Concurrently with this work, Li and Eis-
ner (2009) have generalized Eisner (2002) to com-
pute expected products of counts on translation
forests. The training algorithm of Kakade et al
(2002) makes use of a dynamic program similar to
ours, though specialized to the case of sequence
models.
4 Consensus Decoding
Once model parameters ? are learned, we must
select an appropriate decoding objective. Sev-
eral new decoding approaches have been proposed
recently that leverage some notion of consensus
over the many weighted derivations in a transla-
tion forest. In this paper, we adopt the fast consen-
sus decoding procedure of DeNero et al (2009),
which directly complements CoBLEU tuning. For
a source sentence f , we first build a translation
forest, then compute the expected count of each
n-gram in the translation of f under the model.
We extract a k-best list from the forest, then select
the translation that yields the highest BLEU score
relative to the forest?s expected n-gram counts.
Specifically, let BLEU(e; r) compute the simi-
larity of a sentence e to a reference r based on
the n-gram counts of each. When training with
CoBLEU, we replace e with expected counts and
maximize ?. In consensus decoding, we replace r
with expected counts and maximize e.
Several other efficient consensus decoding pro-
1423
cedures would similarly benefit from a tuning pro-
cedure that aggregates over derivations. For in-
stance, Blunsom and Osborne (2008) select the
translation sentence with highest posterior proba-
bility under the model, summing over derivations.
Li et al (2009) propose a variational approxima-
tion maximizing sentence probability that decom-
poses over n-grams. Tromble et al (2008) min-
imize risk under a loss function based on the lin-
ear Taylor approximation to BLEU, which decom-
poses over n-gram posterior probabilities.
5 Experiments
We compared CoBLEU training with an imple-
mentation of minimum error rate training on two
language pairs.
5.1 Model
Our optimization procedure is in principle
tractable for any syntactic translation system. For
simplicity, we evaluate the objective using an In-
version Transduction Grammar (ITG) (Wu, 1997)
that emits phrases as terminal productions, as in
(Cherry and Lin, 2007). Phrasal ITG models have
been shown to perform comparably to the state-of-
the art phrase-based system Moses (Koehn et al,
2007) when using the same phrase table (Petrov et
al., 2008).
We extract a phrase table using the Moses
pipeline, based on Model 4 word alignments gen-
erated from GIZA++ (Och and Ney, 2003). Our fi-
nal ITG grammar includes the five standard Moses
features, an n-gram language model, a length fea-
ture that counts the number of target words, a fea-
ture that counts the number of monotonic ITG
rewrites, and a feature that counts the number of
inverted ITG rewrites.
5.2 Data
We extracted phrase tables from the Spanish-
English and French-English sections of the Eu-
roparl corpus, which include approximately 8.5
million words of bitext for each of the language
pairs (Koehn, 2002). We used a trigram lan-
guage model trained on the entire corpus of En-
glish parliamentary proceedings provided with the
Europarl distribution and generated according to
the ACL 2008 SMT shared task specifications.
6
For tuning, we used all sentences from the 2007
SMT shared task up to length 25 (880 sentences
6
See http://www.statmt.org/wmt08 for details.
2 4 6 8 10
0.00
.20.
40.6
0.81
.0
Iterations
Fractio
n of Va
lue at C
onverge
nce
CoBLEU
MERT
Figure 6: Trajectories of MERT and CoBLEU dur-
ing optimization show that MERT is initially unstable,
while CoBLEU training follows a smooth path to con-
vergence. Because these two training procedures op-
timize different functions, we have normalized each
trajectory by the final objective value at convergence.
Therefore, the absolute values of this plot do not re-
flect the performance of either objective, but rather
the smoothness with which the final objective is ap-
proached. The rates of convergence shown in this plot
are not directly comparable. Each iteration for MERT
above includes 10 iterations of coordinate ascent, fol-
lowed by a decoding pass through the training set. Each
iteration of CoBLEU training involves only one gradi-
ent step.
for Spanish and 923 for French), and we tested on
the subset of the first 1000 development set sen-
tences which had length at most 25 words (447
sentences for Spanish and 512 for French).
5.3 Tuning Optimization
We compared two techniques for tuning the nine
log-linear model parameters of our ITG grammar.
We maximized CoBLEU using gradient ascent, as
described above. As a baseline, we maximized
BLEU of the Viterbi translation derivations using
minimum error rate training. To improve opti-
mization stability, MERT used a cumulative k-best
list that included all translations generated during
the tuning process.
One of the benefits of CoBLEU training is that
we compute expectations efficiently over an entire
forest of translations. This has substantial stabil-
ity benefits over methods based on k-best lists. In
Figure 6, we show the progress of CoBLEU as
compared to MERT. Both models are initialized
from 0 and use the same features. This plot ex-
hibits a known issue with MERT training: because
new k-best lists are generated at each iteration,
the objective function can change drastically be-
tween iterations. In contrast, CoBLEU converges
1424
Consensus Decoding
Spanish
Tune Test ? Br.
MERT 32.5 30.2 -2.3 0.992
CoBLEU 31.4 30.4 -1.0 0.992
MERT?CoBLEU 31.7 30.8 -0.9 0.992
French
Tune Test ? Br.
MERT 32.5 31.1* -1.4 0.972
CoBLEU 31.9 30.9 -1.0 0.954
MERT?CoBLEU 32.4 31.2* -0.8 0.953
Table 1: Performance measured by BLEU using a con-
sensus decoding method over translation forests shows
an improvement over MERT when using CoBLEU
training. The first two conditions were initialized by
0 vectors. The third condition was initialized by the
final parameters of MERT training. Br. indicates the
brevity penalty on the test set. The * indicates differ-
ences which are not statistically significant.
smoothly to its final objective because the forests
do not change substantially between iterations, de-
spite the pruning needed to track n-grams. Similar
stability benefits have been observed for lattice-
based MERT (Macherey et al, 2008).
5.4 Results
We performed experiments from both French and
Spanish into English under three conditions. In the
first two, we initialized both MERT and CoBLEU
training uniformly with zero weights and trained
until convergence. In the third condition, we ini-
tialized CoBLEU with the final parameters from
MERT training, denoted MERT?CoBLEU in the
results tables. We evaluated each of these condi-
tions on both the tuning and test sets using the con-
sensus decoding method of DeNero et al (2009).
The results appear in Table 1.
In Spanish-English, CoBLEU slightly outper-
formed MERT under the same initialization, while
the opposite pattern appears for French-English.
The best test set performance in both language
pairs was the third condition, in which CoBLEU
training was initialized with MERT. This con-
dition also gave the highest CoBLEU objective
value. This pattern indicates that CoBLEU is a
useful objective for translation with consensus de-
coding, but that the gradient ascent optimization is
getting stuck in local maxima during tuning. This
issue can likely be addressed with annealing, as
described in (Smith and Eisner, 2006).
Interestingly, the brevity penatly results in
French indicate that, even though CoBLEU did
Viterbi Decoding
Spanish
Tune Test ?
MERT 32.5 30.2 -2.3
MERT?CoBLEU 30.5 30.9 +0.4
French
Tune Test ?
MERT 32.0 31.0 -1.0
MERT?CoBLEU 31.7 30.9 -0.8
Table 2: Performance measured by BLEU using Viterbi
decoding indicates that CoBLEU is less prone to over-
fitting than MERT.
not outperform MERT in a statistically significant
way, CoBLEU tends to find shorter sentences with
higher n-gram precision than MERT.
Table 1 displays a second benefit of CoBLEU
training: compared to MERT training, CoBLEU
performance degrades less from tuning to test
set. In Spanish, initializing with MERT-trained
weights and then training with CoBLEU actually
decreases BLEU on the tuning set by 0.8 points.
However, this drop in tuning performance comes
with a corresponding increase of 0.6 on the test
set, relative to MERT training. We see the same
pattern in French, albeit to a smaller degree.
While CoBLEU ought to outperform MERT us-
ing consensus decoding, we expected that MERT
would give better performance under Viterbi de-
coding. Surprisingly, we found that CoBLEU
training actually outperformed MERT in Spanish-
English and performed equally well in French-
English. Table 2 shows the results. In these ex-
periments, we again see that CoBLEU overfit the
training set to a lesser degree than MERT, as evi-
denced by a smaller drop in performance from tun-
ing to test set. In fact, test set performance actually
improved for Spanish-English CoBLEU training
while dropping by 2.3 BLEU for MERT.
6 Conclusion
CoBLEU takes a fundamental quantity used in
consensus decoding, expected n-grams, and trains
to optimize a function of those expectations.
While CoBLEU can therefore be expected to in-
crease test set BLEU under consensus decoding, it
is more surprising that it seems to better regularize
learning even for the Viterbi decoding condition.
It is also worth emphasizing that the CoBLEU ap-
proach is applicable to functions of expected n-
gram counts other than BLEU.
1425
Appendix: The Gradient of CoBLEU
We would like to compute the gradient of
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
To simplify notation, we introduce the functions
C
n
(?) =
m
?
i=1
?
g
n
E
?
[c(g
n
, e)|f
i
]
C
clip
n
(?) =
m
?
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(r, g
n
)}
C
n
(?) represents the sum of the expected counts
of all n-grams or order n in all translations of
the source corpus F , while C
clip
n
(?) represents the
sum of the same expected counts, but clipped with
reference counts c(g
n
, r
i
).
With this notation, we can write our objective
function CoBLEU(R,F, ?) in three terms:
(
1?
|R|
C
1
(?)
)
?
+
1
4
4
?
n=1
lnC
clip
n
(?)?
1
4
4
?
n=1
lnC
n
(?)
We first state an identity:
?
g
n
?
??
k
E
?
[c(g
n
, d)|f
i
] =
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
?E
?
[`
n
(d)|f
i
] ? E
?
[c(?
k
, d)|f
i
]
which can be derived by expanding the expectation on
the left-hand side
?
g
n
?
d
?
??
k
P
?
(d|f
i
)c(g
n
, d)
and substituting
?
??
k
P
?
(d|f
i
) =
P
?
(d|f
i
)c(?
k
, d)? P
?
(d|f
i
)
?
d
?
P
?
(d
?
|f
i
)c(?
k
, d
?
)
Using this identity and some basic calculus, the
gradient?C
n
(?) is
m
?
i=1
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]? C
n
(?)E
?
[c(?
k
, d)|f
i
]
I(u) =
?
h?IN(u)
w(h)
?
?
?
v?tail(h)
I(v)
?
?
O(u) =
?
h?OUT (u)
w(h)
?
?
?
?
O(head(h))
?
v?tail(h)
v 6=u
I(v)
?
?
?
?
Figure 7: Standard Inside-Outside recursions which
compute I(u) and O(u). IN(u) and OUT(u) refer to the
incoming and outgoing hyperedges of u, respectively.
We initialize with I(u) = 1 for all terminal forest nodes
u and O(root) = 1 for the root node. These quantities
are referenced in Figure 4.
and the gradient?C
clip
n
(?) is given by
m
?
i=1
?
g
n
[
E
?
[c(g
n
, d) ? c(?
k
, d)|f
i
]
?1
[
E
?
[c(g
n
, d)|f
i
] ? c(g
n
, r
i
)
]
]
?C
clip
n
(?)E
?
[c(?
k
, d) + f
i
]
where 1 denotes an indicator function. At the top
level, the gradient of the first term (the brevity
penalty) is
|R|?C
1
(?)
C
1
(?)
2
1
[
C
1
(?) ? |R|
]
The gradient of the second term is
1
4
4
?
n=1
?C
clip
n
(?)
C
clip
n
(?)
and the gradient of the third term is
?
1
4
4
?
n=1
?C
n
(?)
C
n
(?)
Note that, because of the indicator func-
tions, CoBLEU is non-differentiable when
E
?
[c(g
n
, d)|f
i
] = c(g
n
, r
i
) or C
n
(?) = |R|.
Formally, we must compute a sub-gradient at
these points. In practice, we can choose between
the gradients calculated assuming the indicator
function is 0 or 1; we always choose the latter.
1426
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings
of the Conference on Emprical Methods for Natural
Language Processing.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In The Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics Workshop on Syntax and Structure in
Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In The Conference on Em-
pirical Methods in Natural Language Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
The Annual Conference of the Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In The Annual Conference of the Association for
Computational Linguistics.
Sham Kakade, Yee Whye Teh, and Sam T. Roweis.
2002. An alternate objective function for markovian
fields. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
The Annual Conference of the Association for Com-
putational Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In The Annual
Conference of the Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In The Annual Conference of the Association
for Computational Linguistics.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In The Annual
Conference of the Association for Computational
Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 108?116, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
David Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In In Pro-
ceedings of the Association for Computational Lin-
guistics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
1427
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 73?80, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Discriminative Matching Approach to Word Alignment
Ben Taskar Simon Lacoste-Julien Dan Klein
Computer Science Division, EECS Department
University of California, Berkeley
Berkeley, CA 94720
Abstract
We present a discriminative, large-
margin approach to feature-based
matching for word alignment. In this
framework, pairs of word tokens re-
ceive a matching score, which is based
on features of that pair, including mea-
sures of association between the words,
distortion between their positions, sim-
ilarity of the orthographic form, and so
on. Even with only 100 labeled train-
ing examples and simple features which
incorporate counts from a large unla-
beled corpus, we achieve AER perfor-
mance close to IBM Model 4, in much
less time. Including Model 4 predic-
tions as features, we achieve a relative
AER reduction of 22% in over inter-
sected Model 4 alignments.
1 Introduction
The standard approach to word alignment from
sentence-aligned bitexts has been to construct
models which generate sentences of one lan-
guage from the other, then fitting those genera-
tive models with EM (Brown et al, 1990; Och
and Ney, 2003). This approach has two primary
advantages and two primary drawbacks. In its
favor, generative models of alignment are well-
suited for use in a noisy-channel translation sys-
tem. In addition, they can be trained in an un-
supervised fashion, though in practice they do
require labeled validation alignments for tuning
model hyper-parameters, such as null counts or
smoothing amounts, which are crucial to pro-
ducing alignments of good quality. A primary
drawback of the generative approach to align-
ment is that, as in all generative models, explic-
itly incorporating arbitrary features of the in-
put is difficult. For example, when considering
whether to align two words in the IBM models
(Brown et al, 1990), one cannot easily include
information about such features as orthographic
similarity (for detecting cognates), presence of
the pair in various dictionaries, similarity of the
frequency of the two words, choices made by
other alignment systems on this sentence pair,
and so on. While clever models can implicitly
capture some of these information sources, it
takes considerable work, and can make the re-
sulting models quite complex. A second draw-
back of generative translation models is that,
since they are learned with EM, they require
extensive processing of large amounts of data
to achieve good performance. While tools like
GIZA++ (Och and Ney, 2003) do make it eas-
ier to build on the long history of the generative
IBM approach, they also underscore how com-
plex high-performance generative models can,
and have, become.
In this paper, we present a discriminative ap-
proach to word alignment. Word alignment is
cast as a maximum weighted matching problem
(Cormen et al, 1990) in which each pair of words
(e
j
, f
k
) in a sentence pair (e, f) is associated
with a score s
jk
(e, f) reflecting the desirability
of the alignment of that pair. The alignment
73
for the sentence pair is then the highest scoring
matching under some constraints, for example
the requirement that matchings be one-to-one.
This view of alignment as graph matching is
not, in itself, new: Melamed (2000) uses com-
petitive linking to greedily construct matchings
where the pair score is a measure of word-
to-word association, and Matusov et al (2004)
find exact maximum matchings where the pair
scores come from the alignment posteriors of
generative models. Tiedemann (2003) proposes
incorporating a variety of word association
?clues? into a greedy linking algorithm.
What we contribute here is a principled ap-
proach for tractable and efficient learning of the
alignment score s
jk
(e, f) as a function of ar-
bitrary features of that token pair. This con-
tribution opens up the possibility of doing the
kind of feature engineering for alignment that
has been so successful for other NLP tasks. We
first present the algorithm for large margin es-
timation of the scoring function. We then show
that our method can achieve AER rates com-
parable to unsymmetrized IBM Model 4, using
extremely little labeled data (as few as 100 sen-
tences) and a simple feature set. Remarkably,
by including bi-directional IBM Model 4 predic-
tions as features, we achieve an absolute AER
of 5.4 on the English-French Hansards alignment
task, a relative reduction of 22% in AER over in-
tersected Model 4 alignments and, to our knowl-
edge, the best AER result published on this task.
2 Algorithm
We model the alignment prediction task as a
maximum weight bipartite matching problem,
where nodes correspond to the words in the
two sentences. For simplicity, we assume here
that each word aligns to one or zero words in
the other sentence. The edge weight s
jk
repre-
sents the degree to which word j in one sentence
can translate into the word k in the other sen-
tence. Our goal is to find an alignment that
maximizes the sum of edge scores. We represent
a matching using a set of binary variables y
jk
that are set to 1 if word j is assigned to word
k in the other sentence, and 0 otherwise. The
score of an assignment is the sum of edge scores:
s(y) =
?
jk
s
jk
y
jk
. The maximum weight bi-
partite matching problem, arg maxy?Y s(y), can
be solved using well known combinatorial algo-
rithms or the following linear program:
max
z
?
jk
s
jk
z
jk
(1)
s.t.
?
j
z
jk
? 1,
?
k
z
jk
? 1, 0 ? z
jk
? 1,
where the continuous variables z
jk
correspond to
the binary variables y
jk
. This LP is guaranteed
to have integral (and hence optimal) solutions
for any scoring function s(y) (Schrijver, 2003).
Note that although the above LP can be used to
compute alignments, combinatorial algorithms
are generally more efficient. However, we use
the LP to develop the learning algorithm below.
For a sentence pair x, we denote position
pairs by x
jk
and their scores as s
jk
. We let
s
jk
= wf(x
jk
) for some user provided fea-
ture mapping f and abbreviate wf(x,y) =
?
jk
y
jk
wf(x
jk
). We can include in the fea-
ture vector the identity of the two words, their
relative positions in their respective sentences,
their part-of-speech tags, their string similarity
(for detecting cognates), and so on.
At this point, one can imagine estimating a
linear matching model in multiple ways, includ-
ing using conditional likelihood estimation, an
averaged perceptron update (see which match-
ings are proposed and adjust the weights ac-
cording to the difference between the guessed
and target structures (Collins, 2002)), or in
large-margin fashion. Conditional likelihood es-
timation using a log-linear model P (y | x) =
1
Z
w
(x)
exp{wf(x,y)} requires summing over all
matchings to compute the normalization Zw(x),
which is #P-complete (Valiant, 1979). In our
experiments, we therefore investigated the aver-
aged perceptron in addition to the large-margin
method outlined below.
2.1 Large-margin estimation
We follow the large-margin formulation of
Taskar et al (2005a). Our input is a set of
training instances {(x
i
,y
i
)}m
i=1
, where each in-
stance consists of a sentence pair x
i
and a target
74
alignment y
i
. We would like to find parameters
w that predict correct alignments on the train-
ing data:
y
i
= arg max
?y
i
?Y
i
wf(x
i
, y?
i
), ?i,
where Y
i
is the space of matchings appropriate
for the sentence pair i.
In standard classification problems, we typi-
cally measure the error of prediction, (y
i
, y?
i
),
using the simple 0-1 loss. In structured prob-
lems, where we are jointly predicting multiple
variables, the loss is often more complex. While
the F-measure is a natural loss function for this
task, we instead chose a sensible surrogate that
fits better in our framework: Hamming distance
between y
i
and y?
i
, which simply counts the
number of edges predicted incorrectly.
We use an SVM-like hinge upper bound on
the loss (y
i
, y?
i
), given by max
?y
i
?Y
i
[wf
i
(y?
i
) +

i
(y?
i
) ? wf
i
(y
i
)], where 
i
(y?
i
) = (y
i
, y?
i
), and
f
i
(y?
i
) = f(x
i
, y?
i
). Minimizing this upper bound
encourages the true alignment y
i
to be optimal
with respect to w for each instance i:
min
||w||??
?
i
max
?y
i
?Y
i
[wf
i
(y?
i
) + 
i
(y?
i
)] ? wf
i
(y
i
),
where ? is a regularization parameter.
In this form, the estimation problem is a mix-
ture of continuous optimization over w and com-
binatorial optimization over y
i
. In order to
transform it into a more standard optimization
problem, we need a way to efficiently handle the
loss-augmented inference, max
?y
i
?Y
i
[wf
i
(y?
i
) +

i
(y?
i
)]. This optimization problem has pre-
cisely the same form as the prediction prob-
lem whose parameters we are trying to learn
? max
?y
i
?Y
i
wf
i
(y?
i
) ? but with an additional
term corresponding to the loss function. Our as-
sumption that the loss function decomposes over
the edges is crucial to solving this problem. In
particular, we use weighted Hamming distance,
which counts the number of variables in which
a candidate solution y?
i
differs from the target
output y
i
, with different cost for false positives
(c+) and false negatives (c-):

i
(y?
i
) =
?
jk
[
c-y
i,jk
(1 ? y?
i,jk
) + c+y?
i,jk
(1 ? y
i,jk
)
]
=
?
jk
c-y
i,jk
+
?
jk
[c+ ? (c- + c+)y
i,jk
]y?
i,jk
.
The loss-augmented matching problem can then
be written as an LP similar to Equation 1 (with-
out the constant term
?
jk
c-y
i,jk
):
max
z
?
jk
z
i,jk
[wf(x
i,jk
) + c+ ? (c- + c+)y
i,jk
]
s.t.
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ? z
i,jk
? 1.
Hence, without any approximations, we have a
continuous optimization problem instead of a
combinatorial one:
max
?y
i
?Y
i
wf
i
(y?
i
)+
i
(y?
i
) = d
i
+max
z
i
?Z
i
(wF
i
+c
i
)z
i
,
where d
i
=
?
jk
c-y
i,jk
is the constant term, F
i
is the appropriate matrix that has a column of
features f(x
i,jk
) for each edge jk, c
i
is the vector
of the loss terms c+ ? (c- + c+)y
i,jk
and finally
Z
i
= {z
i
:
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ?
z
i,jk
? 1}.
Plugging this LP back into our estimation
problem, we have
min
||w||??
max
z?Z
?
i
wF
i
z
i
+ c
i
z
i
? wF
i
y
i
, (2)
where z = {z
1
, . . . , z
m
}, Z = Z
1
? . . .?Z
m
. In-
stead of the derivation in Taskar et al (2005a),
which produces a joint convex optimization
problem using Lagrangian duality, here we
tackle the problem in its natural saddle-point
form.
2.2 The extragradient method
For saddle-point problems, a well-known solu-
tion strategy is the extragradient method (Ko-
rpelevich, 1976), which is closely related to
projected-gradient methods.
The gradient of the objective in Equation 2
is given by:
?
i
F
i
(z
i
? y
i
) (with respect to w)
and F
i
w + c
i
(with respect to each z
i
). We de-
note the Euclidean projection of a vector onto
Z
i
as P
Z
i
(v) = arg minu?Z
i
||v ? u|| and pro-
jection onto the ball ||w|| ? ? as P
?
(w) =
?w/max(?, ||w||).
75
An iteration of the extragradient method con-
sists of two very simple steps, prediction:
w?t+1 = P
?
(wt + ?
k
?
i
F
i
(y
i
? zt
i
));
z?t+1
i
= P
Z
i
(zt
i
+ ?
k
(F
i
wt + c
i
));
and correction:
wt+1 = P
?
(wt + ?
k
?
i
F
i
(y
i
? z?t+1
i
));
zt+1
i
= P
Z
i
(zt
i
+ ?
k
(F
i
w?t+1 + c
i
)),
where ?
k
are appropriately chosen step sizes.
The method is guaranteed to converge linearly
to a solution w?, z? (Korpelevich, 1976; He and
Liao, 2002; Taskar et al, 2005b). Please see
www.cs.berkeley.edu/~taskar/extragradient.pdf
for more details.
The key subroutine of the algorithm is Eu-
clidean projection onto the feasible sets Z
i
. In
case of word alignment, Z
i
is the convex hull of
bipartite matchings and the problem reduces to
the much-studied minimum cost quadratic flow
problem (Bertsekas et al, 1997). The projection
problem P
Z
i
(z?
i
) is given by
min
z
?
jk
1
2
(z?
i,jk
? z
i,jk
)2
s.t.
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ? z
i,jk
? 1.
We can now use a standard reduction of bipar-
tite matching to min cost flow by introducing a
source node connected to all the words in one
sentence and a sink node connected to all the
words in the other sentence, using edges of ca-
pacity 1 and cost 0. The original edges jk have
a quadratic cost 1
2
(z?
i,jk
? z
i,jk
)2 and capacity 1.
Now the minimum cost flow from the source to
the sink computes projection of z?
i
onto Z
i
We
use standard, publicly-available code for solving
this problem (Guerriero and Tseng, 2002).
3 Experiments
We applied this matching algorithm to word-
level alignment using the English-French
Hansards data from the 2003 NAACL shared
task (Mihalcea and Pedersen, 2003). This
corpus consists of 1.1M automatically aligned
sentences, and comes with a validation set of 39
sentence pairs and a test set of 447 sentences.
The validation and test sentences have been
hand-aligned (see Och and Ney (2003)) and are
marked with both sure and possible alignments.
Using these alignments, alignment error rate
(AER) is calculated as:
AER(A,S, P ) = 1 ? |A ? S| + |A ? P |
|A| + |S|
Here, A is a set of proposed index pairs, S is
the sure gold pairs, and P is the possible gold
pairs. For example, in Figure 1, proposed align-
ments are shown against gold alignments, with
open squares for sure alignments, rounded open
squares for possible alignments, and filled black
squares for proposed alignments.
Since our method is a supervised algorithm,
we need labeled examples. For the training data,
we split the original test set into 100 training
examples and 347 test examples. In all our ex-
periments, we used a structured loss function
(y
i
, y?
i
) that penalized false negatives 3 times
more than false positives, where 3 was picked by
testing several values on the validation set. In-
stead of selecting a regularization parameter ?
and running to convergence, we used early stop-
ping as a cheap regularization method, by set-
ting ? to a very large value (10000) and running
the algorithm for 500 iterations. We selected a
stopping point using the validation set by simply
picking the best iteration on the validation set in
terms of AER (ignoring the initial ten iterations,
which were very noisy in our experiments). All
selected iterations turned out to be in the first
50 iterations, as the algorithm converged fairly
rapidly.
3.1 Features and Results
Very broadly speaking, the classic IBM mod-
els of word-level translation exploit four primary
sources of knowledge and constraint: association
of words (all IBM models), competition between
alignments (all models), zero- or first-order pref-
erences of alignment positions (2,4+), and fer-
tility (3+). We model all of these in some way,
76
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
(a) Dice only (b) Dice and Distance
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
(c) Dice, Distance, Orthographic, and BothShort (d) All features
Figure 1: Example alignments for each successive feature set.
except fertility.1
First, and, most importantly, we want to in-
clude information about word association; trans-
lation pairs are likely to co-occur together in
a bitext. This information can be captured,
among many other ways, using a feature whose
1In principle, we can model also model fertility, by
allowing 0-k matches for each word rather than 0-1, and
having bias features on each word. However, we did not
explore this possibility.
value is the Dice coefficient (Dice, 1945):
Dice(e, f) = 2CEF (e, f)C
E
(e)C
F
(f)
Here, C
E
and C
F
are counts of word occurrences
in each language, while C
EF
is the number of
co-occurrences of the two words. With just this
feature on a pair of word tokens (which depends
only on their types), we can already make a stab
77
at word alignment, aligning, say, each English
word with the French word (or null) with the
highest Dice value (see (Melamed, 2000)), sim-
ply as a matching-free heuristic model. With
Dice counts taken from the 1.1M sentences, this
gives and AER of 38.7 with English as the tar-
get, and 36.0 with French as the target (in line
with the numbers from Och and Ney (2003)).
As observed in Melamed (2000), this use of
Dice misses the crucial constraint of competi-
tion: a candidate source word with high asso-
ciation to a target word may be unavailable for
alignment because some other target has an even
better affinity for that source word. Melamed
uses competitive linking to incorporate this con-
straint explicitly, while the IBM-style models
get this effect via explaining-away effects in EM
training. We can get something much like the
combination of Dice and competitive linking by
running with just one feature on each pair: the
Dice value of that pair?s words.2 With just a
Dice feature ? meaning no learning is needed
yet ? we achieve an AER of 29.8, between the
Dice with competitive linking result of 34.0 and
Model 1 of 25.9 given in Och and Ney (2003).
An example of the alignment at this stage is
shown in Figure 1(a). Note that most errors lie
off the diagonal, for example the often-correct
to-a` match.
IBM Model 2, as usually implemented, adds
the preference of alignments to lie near the di-
agonal. Model 2 is driven by the product of a
word-to-word measure and a (usually) Gaussian
distribution which penalizes distortion from the
diagonal. We can capture the same effect us-
ing features which reference the relative posi-
tions j and k of a pair (e
j
, f
k
). In addition to a
Model 2-style quadratic feature referencing rela-
tive position, we threw in the following proxim-
ity features: absolute difference in relative posi-
tion abs(j/|e|?k/|f |), and the square and square
root of this value. In addition, we used a con-
junction feature of the dice coefficient times the
proximity. Finally, we added a bias feature on
each edge, which acts as a threshold that allows
2This isn?t quite competitive linking, because we use
a non-greedy matching.
in
19
78
Am
er
ic
an
s
di
vo
rc
ed
1,
12
2,
00
0
ti
me
s .
en
1978
,
on
a
enregistre?
1,122,000
divorces
sur
le
continent
.
in
19
78
Am
er
ic
an
s
di
vo
rc
ed
1,
12
2,
00
0
ti
me
s .
en
1978
,
on
a
enregistre?
1,122,000
divorces
sur
le
continent
.
(a) (b)
Figure 2: Example alignments showing the ef-
fects of orthographic cognate features. (a) Dice
and Distance, (b) With Orthographic Features.
sparser, higher precision alignments. With these
features, we got an AER of 15.5 (compare to 19.5
for Model 2 in (Och and Ney, 2003)). Note that
we already have a capacity that Model 2 does
not: we can learn a non-quadratic penalty with
linear mixtures of our various components ? this
gives a similar effect to learning the variance of
the Gaussian for Model 2, but is, at least in
principle, more flexible.3 These features fix the
to-a` error in Figure 1(a), giving the alignment
in Figure 1(b).
On top of these features, we included other
kinds of information, such as word-similarity
features designed to capture cognate (and ex-
act match) information. We added a feature for
exact match of words, exact match ignoring ac-
cents, exact matching ignoring vowels, and frac-
tion overlap of the longest common subsequence.
Since these measures were only useful for long
words, we also added a feature which indicates
that both words in a pair are short. These or-
thographic and other features improved AER to
14.4. The running example now has the align-
ment in Figure 1(c), where one improvement
may be attributable to the short pair feature ? it
has stopped proposing the-de, partially because
the short pair feature downweights the score of
that pair. A clearer example of these features
making a difference is shown in Figure 2, where
both the exact-match and character overlap fea-
3The learned response was in fact close to a Gaussian,
but harsher near zero displacement.
78
tures are used.
One source of constraint which our model still
does not explicitly capture is the first-order de-
pendency between alignment positions, as in the
HMM model (Vogel et al, 1996) and IBM mod-
els 4+. The the-le error in Figure 1(c) is symp-
tomatic of this lack. In particular, it is a slightly
better pair according to the Dice value than the
correct the-les. However, the latter alignment
has the advantage that major-grands follows it.
To use this information source, we included a
feature which gives the Dice value of the words
following the pair.4 We also added a word-
frequency feature whose value is the absolute
difference in log rank of the words, discourag-
ing very common words from translating to very
rare ones. Finally, we threw in bilexical features
of the pairs of top 5 non-punctuation words in
each language.5 This helped by removing spe-
cific common errors like the residual tendency
for French de to mistakenly align to English the
(the two most common words). The resulting
model produces the alignment in Figure 1(d).
It has sorted out the the-le / the-les confusion,
and is also able to guess to-de, which is not the
most common translation for either word, but
which is supported by the good Dice value on
the following pair (make-faire).
With all these features, we got a final AER
of 10.7, broadly similar to the 8.9 or 9.7 AERs
of unsymmetrized IBM Model 4 trained on the
same data that the Dice counts were taken
from.6 Of course, symmetrizing Model 4 by in-
tersecting alignments from both directions does
yield an improved AER of 6.9, so, while our
model does do surprisingly well with cheaply ob-
tained count-based features, Model 4 does still
outperform it so far. However, our model can
4It is important to note that while our matching algo-
rithm has no first-order effects, the features can encode
such effects in this way, or in better ways ? e.g. using as
features posteriors from the HMM model in the style of
Matusov et al (2004).
5The number of such features which can be learned
depends on the number of training examples, and since
some of our experiments used only a few dozen training
examples we did not make heavy use of this feature.
6Note that the common word pair features affected
common errors and therefore had a particularly large im-
pact on AER.
Model AER
Dice (without matching) 38.7 / 36.0
Model 4 (E-F, F-E, intersected) 8.9 / 9.7/ 6.9
Discriminative Matching
Dice Feature Only 29.8
+ Distance Features 15.5
+ Word Shape and Frequency 14.4
+ Common Words and Next-Dice 10.7
+ Model 4 Predictions 5.4
Figure 3: AER on the Hansards task.
also easily incorporate the predictions of Model
4 as additional features. We therefore added
three new features for each edge: the prediction
of Model 4 in the English-French direction, the
prediction in the French-English direction, and
the intersection of the two predictions. With
these powerful new features, our AER dropped
dramatically to 5.4, a 22% improvement over the
intersected Model 4 performance.
Another way of doing the parameter estima-
tion for this matching task would have been
to use an averaged perceptron method, as in
Collins (2002). In this method, we merely run
our matching algorithm and update weights
based on the difference between the predicted
and target matchings. However, the perfor-
mance of the average perceptron learner on the
same feature set is much lower, only 8.1, not
even breaking the AER of its best single feature
(the intersected Model 4 predictions).
3.2 Scaling Experiments
We explored the scaling of our method by learn-
ing on a larger training set, which we created by
using GIZA++ intersected bi-directional Model
4 alignments for the unlabeled sentence pairs.
We then took the first 5K sentence pairs from
these 1.1M Model 4 alignments. This gave us
more training data, albeit with noisier labels.
On a 3.4GHz Intel Xeon CPU, GIZA++ took
18 hours to align the 1.1M words, while our
method learned its weights in between 6 min-
utes (100 training sentences) and three hours
(5K sentences).
79
4 Conclusions
We have presented a novel discriminative, large-
margin method for learning word-alignment
models on the basis of arbitrary features of word
pairs. We have shown that our method is suit-
able for the common situation where a moder-
ate number of good, fairly general features must
be balanced on the basis of a small amount of
labeled data. It is also likely that the method
will be useful in conjunction with a large labeled
alignment corpus (should such a set be created).
We presented features capturing a few separate
sources of information, producing alignments on
the order of those given by unsymmetrized IBM
Model 4 (using labeled training data of about
the size others have used to tune generative
models). In addition, when given bi-directional
Model 4 predictions as features, our method
provides a 22% AER reduction over intersected
Model 4 predictions alone. The resulting 5.4
AER on the English-French Hansarks task is,
to our knowledge, the best published AER fig-
ure for this training scenario (though since we
use a subset of the test set, evaluations are not
problem-free). Finally, our method scales to
large numbers of training sentences and trains
in minutes rather than hours or days for the
higher-numbered IBM models, a particular ad-
vantage when not using features derived from
those slower models.
References
D. P. Bertsekas, L. C. Polymenakos, and P. Tseng. 1997.
An e-relaxation method for separable convex cost net-
work flow problems. SIAM J. Optim., 7(3):853?870.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990.
Introduction to Algorithms. MIT Press, Cambridge,
MA.
L. R. Dice. 1945. Measures of the amount of ecologic as-
sociation between species. Journal of Ecology, 26:297?
302.
F. Guerriero and P. Tseng. 2002. Implementation
and test of auction methods for solving generalized
network flow problems with separable convex cost.
Journal of Optimization Theory and Applications,
115(1):113?144, October.
B.S. He and L. Z. Liao. 2002. Improvements of some
projection methods for monotone nonlinear variational
inequalities. JOTA, 112:111:128.
G. M. Korpelevich. 1976. The extragradient method for
finding saddle points and other problems. Ekonomika
i Matematicheskie Metody, 12:747:756.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In Proc.
of COLING 2004.
I. D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop, Building and Using parallel
Texts: Data Driven Machine Translation and Beyond,
pages 1?6, Edmonton, Alberta, Canada.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
A. Schrijver. 2003. Combinatorial Optimization: Poly-
hedra and Efficiency. Springer.
B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.
2005a. Learning structured prediction models: a large
margin approach. In Proceedings of the International
Conference on Machine Learning.
B. Taskar, S. Lacoste-Julien, and M. Jordan. 2005b.
Structured prediction via the extragradient method.
In Proceedings of Neural Information Processing Sys-
tems.
J. Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of EACL.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?201.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In COLING
16, pages 836?841.
80
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 104?111,
New York, June 2006. c?2006 Association for Computational Linguistics
Alignment by Agreement
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Ben Taskar
UC Berkeley
Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present an unsupervised approach to
symmetric word alignment in which two
simple asymmetric models are trained
jointly to maximize a combination of
data likelihood and agreement between
the models. Compared to the stan-
dard practice of intersecting predictions of
independently-trained models, joint train-
ing provides a 32% reduction in AER.
Moreover, a simple and efficient pair of
HMM aligners provides a 29% reduction
in AER over symmetrized IBM model 4
predictions.
1 Introduction
Word alignment is an important component of a
complete statistical machine translation pipeline
(Koehn et al, 2003). The classic approaches to un-
supervised word alignment are based on IBM mod-
els 1?5 (Brown et al, 1994) and the HMM model
(Ney and Vogel, 1996) (see Och and Ney (2003) for
a systematic comparison). One can classify these
six models into two groups: sequence-based models
(models 1, 2, and HMM) and fertility-based models
(models 3, 4, and 5).1 Whereas the sequence-based
models are tractable and easily implemented, the
more accurate fertility-based models are intractable
and thus require approximation methods which are
1IBM models 1 and 2 are considered sequence-based models
because they are special cases of HMMs with transitions that do
not depend on previous states.
difficult to implement. As a result, many practition-
ers use the complex GIZA++ software package (Och
and Ney, 2003) as a black box, selecting model 4 as
a good compromise between alignment quality and
efficiency.
Even though the fertility-based models are more
accurate, there are several reasons to consider av-
enues for improvement based on the simpler and
faster sequence-based models. First, even with
the highly optimized implementations in GIZA++,
models 3 and above are still very slow to train. Sec-
ond, we seem to have hit a point of diminishing re-
turns with extensions to the fertility-based models.
For example, gains from the new model 6 of Och
and Ney (2003) are modest. When models are too
complex to reimplement, the barrier to improvement
is raised even higher. Finally, the fertility-based
models are asymmetric, and symmetrization is com-
monly employed to improve alignment quality by
intersecting alignments induced in each translation
direction. It is therefore natural to explore models
which are designed from the start with symmetry in
mind.
In this paper, we introduce a new method for word
alignment that addresses the three issues above. Our
development is motivated by the observation that in-
tersecting the predictions of two directional models
outperforms each model alone. Viewing intersec-
tion as a way of finding predictions that both models
agree on, we take the agreement idea one step fur-
ther. The central idea of our approach is to not only
make the predictions of the models agree at test time,
but also encourage agreement during training. We
define an intuitive objective function which incor-
104
porates both data likelihood and a measure of agree-
ment between models. Then we derive an EM-like
algorithm to maximize this objective function. Be-
cause the E-step is intractable in our case, we use
a heuristic approximation which nonetheless works
well in practice.
By jointly training two simple HMM models, we
obtain 4.9% AER on the standard English-French
Hansards task. To our knowledge, this is the lowest
published unsupervised AER result, and it is com-
petitive with supervised approaches. Furthermore,
our approach is very practical: it is no harder to
implement than a standard HMM model, and joint
training is no slower than the standard training of
two HMM models. Finally, we show that word
alignments from our system can be used in a phrase-
based translation system to modestly improve BLEU
score.
2 Alignment models: IBM 1, 2 and HMM
We briefly review the sequence-based word align-
ment models (Brown et al, 1994; Och and Ney,
2003) and describe some of the choices in our
implementation. All three models are generative
models of the form p(f | e) = ?a p(a, f | e),
where e = (e1, . . . , eI) is the English sentence,
f = (f1, . . . , fJ) is the French sentence, and a =
(a1, . . . , aJ ) is the (asymmetric) alignment which
specifies the position of an English word aligned to
each French word. All three models factor in the
following way:
p(a, f | e) =
J
?
j=1
pd(aj | aj? , j)pt(fj | eaj ), (1)
where j? is the position of the last non-null-aligned
French word before position j.2
The translation parameters pt(fj | eaj ) are pa-
rameterized by an (unsmoothed) lookup table that
stores the appropriate local conditional probability
distributions. The distortion parameters pd(aj = i? |
aj? = i) depend on the particular model (we write
aj = 0 to denote the event that the j-th French word
2The dependence on aj? can in fact be implemented as a
first-order HMM (see Och and Ney (2003)).
is null-aligned):
pd(aj =0 | aj?= i) = p0
pd(aj = i? 6= 0 | aj?= i) ?
(1? p0) ?
?
?
?
?
?
1 (IBM 1)
c(i??b jIJ c) (IBM 2)
c(i??i) (HMM),
where p0 is the null-word probability and c(?) con-
tains the distortion parameters for each offset argu-
ment. We set the null-word probability p0 = 1I+1
depending on the length of the English sentence,
which we found to be more effective than using a
constant p0.
In model 1, the distortion pd(? | ?) specifies a uni-
form distribution over English positions. In model
2, pd(? | ?) is still independent of aj? , but it can now
depend on j and i? through c(?). In the HMM model,
there is a dependence on aj? = i, but only through
c(i? i?).
We parameterize the distortion c(?) using a multi-
nomial distribution over 11 offset buckets c(?
?5), c(?4), . . . , c(4), c(? 5).3 We use three sets of
distortion parameters, one for transitioning into the
first state, one for transitioning out of the last state,
and one for all other transitions. This works better
than using a single set of parameters or ignoring the
transitions at the two ends.
3 Training by agreement
To motivate our joint training approach, we first
consider the standard practice of intersecting align-
ments. While the English and French sentences
play a symmetric role in the word alignment task,
sequence-based models are asymmetric: they are
generative models of the form p(f | e) (E?F), or
p(e | f) (F?E) by reversing the roles of source and
target. In general, intersecting the alignment predic-
tions of two independently-trained directional mod-
els reduces AER, e.g., from 11% to 7% for HMM
models (Table 2). This suggests that two models
make different types of errors that can be eliminated
upon intersection. Figure 1 (top) shows a common
type of error that intersection can partly remedy. In
3For each sentence, the probability mass of each of the two
end buckets c(??5) or c(? 5) is uniformly divided among
those valid offsets.
105
In
de
pe
n
de
n
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 84.2/92.0/13.0 F?E: 86.9/91.1/11.5 Intersection: 97.0/86.9/7.6
Jo
in
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 89.9/93.6/8.7 F?E: 92.2/93.5/7.3 Intersection: 96.5/91.4/5.7
Figure 1: An example of the Viterbi output of a pair of independently trained HMMs (top) and a pair of
jointly trained HMMs (bottom), both trained on 1.1 million sentences. Rounded boxes denote possible
alignments, square boxes are sure alignments, and solid boxes are model predictions. For each model, the
overall Precision/Recall/AER on the development set is given. See Section 4 for details.
this example, COJO is a rare word that becomes a
garbage collector (Moore, 2004) for the models in
both directions. Intersection eliminates the spurious
alignments, but at the expense of recall.
Intersection after training produces alignments
that both models agree on. The joint training pro-
cedure we describe below builds on this idea by en-
couraging the models to agree during training. Con-
sider the output of the jointly trained HMMs in Fig-
ure 1 (bottom). The garbage-collecting rare word is
no longer a problem. Not only are the individual
E?F and F?E jointly-trained models better than
their independently-trained counterparts, the jointly-
trained intersected model also provides a signifi-
cant overall gain over the independently-trained in-
tersected model. We maintain both high precision
and recall.
Before we introduce the objective function for
joint training, we will write the two directional mod-
els in a symmetric way so that they share the same
106
alignment spaces. We first replace the asymmetric
alignments a with a set of indicator variables for
each potential alignment edge (i, j): z = {zij ?
{0, 1} : 1 ? i ? I, 1 ? j ? J}. Each z can be
thought of as an element in the set of generalized
alignments, where any subset of word pairs may be
aligned (Och and Ney, 2003). Sequence-based mod-
els p(a | e, f) induce a distribution over p(z | e, f)
by letting p(z | e, f) = 0 for any z that does not
correspond to any a (i.e., if z contains many-to-one
alignments).
We also introduce the more compact notation
x = (e, f) to denote an input sentence pair. We
put arbitrary distributions p(e) and p(f) to remove
the conditioning, noting that this has no effect on
the optimization problem in the next section. We
can now think of the two directional sequence-based
models as each inducing a distribution over the
same space of sentence pairs and alignments (x, z):
p1(x, z; ?1) = p(e)p(a, f | e; ?1)
p2(x, z; ?2) = p(f)p(a, e | f ; ?2).
3.1 A joint objective
In the next two sections, we describe how to jointly
train the two models using an EM-like algorithm.
We emphasize that this technique is quite general
and can be applied in many different situations
where we want to couple two tractable models over
input x and output z.
To train two models p1(x, z; ?1) and p2(x, z; ?2)
independently, we maximize the data likelihood
?
x pk(x; ?k) =
?
x
?
z pk(x, z; ?k) of each model
separately, k ? {1, 2}:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2)] . (2)
Above, the summation over x enumerates the sen-
tence pairs in the training data.
There are many possible ways to quantify agree-
ment between two models. We chose a particularly
simple and mathematically convenient measure ?
the probability that the alignments produced by the
two models agree on an example x:
?
z
p1(z | x; ?1)p2(z | x; ?2).
We add the (log) probability of agreement to the
standard log-likelihood objective to couple the two
models:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2) +
log
?
z
p1(z | x; ?1)p2(z | x; ?2)]. (3)
3.2 Optimization via EM
We first review the EM algorithm for optimizing a
single model, which consists of iterating the follow-
ing two steps:
E : q(z;x) := p(z | x; ?),
M : ?? := argmax
?
?
x,z
q(z;x) log p(x, z; ?).
In the E-step, we compute the posterior distribution
of the alignments q(z;x) given the sentence pair x
and current parameters ?. In the M-step, we use ex-
pected counts with respect to q(z;x) in the maxi-
mum likelihood update ? := ??.
To optimize the objective in Equation 3, we can
derive a similar and simple procedure. See the ap-
pendix for the derivation.
E: q(z;x) := 1Zxp1(z | x; ?1)p2(z | x; ?2),
M: ?? = argmax
?
?
x,z
q(z;x) log p1(x, z; ?1)
+
?
x,z
q(z;x) log p2(x, z; ?2),
where Zx is a normalization constant. The M-step
decouples neatly into two independent optimization
problems, which lead to single model updates using
the expected counts from q(z;x). To compute Zx in
the E-step, we must sum the product of two model
posteriors over the set of possible zs with nonzero
probability under both models. In general, if both
posterior distributions over the latent variables z
decompose in the same tractable manner, as in
the context-free grammar induction work of Klein
and Manning (2004), the summation could be
carried out efficiently, for example using dynamic
programming. In our case, we would have to sum
over the set of alignments where each word in
English is aligned to at most one word in French
and each word in French is aligned to at most one
107
word in English. Unfortunately, for even very
simple models such as IBM 1 or 2, computing the
normalization constant over this set of alignments
is a #P -complete problem, by a reduction from
counting matchings in a bipartite graph (Valiant,
1979). We could perhaps attempt to compute q us-
ing a variety of approximate probabilistic inference
techniques, for example, sampling or variational
methods. With efficiency as our main concern, we
opted instead for a simple heuristic procedure by
letting q be a product of marginals:
q(z;x) :=
?
i,j
p1(zij | x; ?1)p2(zij | x; ?2),
where each pk(zij | x; ?k) is the posterior marginal
probability of the (i, j) edge being present (or ab-
sent) in the alignment according to each model,
which can be computed separately and efficiently.
Now the new E-step only requires simple
marginal computations under each of the mod-
els. This procedure is very intuitive: edges on
which the models disagree are discounted in the E-
step because the product of the marginals p1(zij |
x; ?1)p2(zij | x; ?2) is small. Note that in general,
this new procedure is not guaranteed to increase our
joint objective. Nonetheless, our experimental re-
sults show that it provides an effective method of
achieving model agreement and leads to significant
accuracy gains over independent training.
3.3 Prediction
Once we have trained two models, either jointly
or independently, we must decide how to combine
those two models to predict alignments for new sen-
tences.
First, let us step back to the case of one model.
Typically, the Viterbi alignment argmaxz p(z | x)
is used. An alternative is to use posterior decoding,
where we keep an edge (i, j) if the marginal edge
posterior p(zij | x) exceeds some threshold 0 < ? <
1. In symbols, z = {zij = 1 : p(zij = 1 | x) ? ?}.4
Posterior decoding has several attractive advan-
tages over Viterbi decoding. Varying the threshold
? gives a natural way to tradeoff precision and re-
call. In fact, these posteriors could be used more di-
4See Matusov et al (2004) for an alternative use of these
marginals.
rectly in extracting phrases for phrase-based trans-
lation. Also, when we want to combine two mod-
els for prediction, finding the Viterbi alignment
argmaxz p1(z | x)p2(z | x) is intractable for
HMM models (by a reduction from quadratic as-
signment), and a hard intersection argmaxz1 p1(z1 |
x) ? argmaxz2 p2(z2 | x) might be too sparse.
On the other hand, we can threshold the product of
two edge posteriors quite easily: z = {zij = 1 :
p1(zij = 1 | x)p2(zij = 1 | x) ? ?}.
We noticed a 5.8% relative reduction in AER (for
our best model) by using posterior decoding with a
validation-set optimized threshold ? instead of using
hard intersection of Viterbi alignments.
4 Experiments
We tested our approach on the English-French
Hansards data from the NAACL 2003 Shared Task,
which includes a training set of 1.1 million sen-
tences, a validation set of 37 sentences, and a test set
of 447 sentences. The validation and test sentences
have been hand-aligned (see Och and Ney (2003))
and are marked with both sure and possible align-
ments. Using these alignments, alignment error rate
(AER) is calculated as:
(
1? |A ? S|+ |A ? P ||A|+ |S|
)
? 100%,
where A is a set of proposed edges, S is the sure
gold edges, and P is the possible gold edges.
As a preprocessing step, we lowercased all words.
Then we used the validation set and the first 100 sen-
tences of the test set as our development set to tune
our models. Lastly, we ran our models on the last
347 sentences of the test set to get final AER results.
4.1 Basic results
We trained models 1, 2, and HMM on the Hansards
data. Following past work, we initialized the trans-
lation probabilities of model 1 uniformly over word
pairs that occur together in some sentence pair.
Models 2 and HMM were initialized with uni-
form distortion probabilities and model 1 translation
probabilities. Each model was trained for 5 itera-
tions, using the same training regimen as in Och and
Ney (2003).
108
Model Indep. Joint Reduction
10K sentences
Model 1 27.4 23.6 13.8
Model 2 18.2 14.9 18.5
HMM 12.1 8.4 30.6
100K sentences
Model 1 21.5 19.2 10.9
Model 2 13.1 10.2 21.7
HMM 8.0 5.3 33.1
1.1M sentences
Model 1 20.0 16.5 17.5
Model 2 11.4 9.2 18.8
HMM 6.6 5.2 21.5
Table 1: Comparison of AER between independent
and joint training across different size training sets
and different models, evaluated on the development
set. The last column shows the relative reduction in
AER.
Table 1 shows a summary of the performance of
independently and jointly trained models under var-
ious training conditions. Quite remarkably, for all
training data sizes and all of the models, we see
an appreciable reduction in AER, especially on the
HMM models. We speculate that since the HMM
model provides a richer family of distributions over
alignments than either models 1 or 2, we can learn
to synchronize the predictions of the two models,
whereas models 1 and 2 have a much more limited
capacity to synchronize.
Table 2 shows the HMM models compared to
model 4 alignments produced by GIZA++ on the test
set. Our jointly trained model clearly outperforms
not only the standard HMM but also the more com-
plex IBM 4 model. For these results, the threshold
used for posterior decoding was tuned on the devel-
opment set. ?GIZA HMM? and ?HMM, indep? are
the same algorithm but differ in implementation de-
tails. The E?F and F?E models benefit a great
deal by moving from independent to joint training,
and the combined models show a smaller improve-
ment.
Our best performing model differs from standard
IBM word alignment models in two ways. First and
most importantly, we use joint training instead of
Model E?F F?E Combined
GIZA HMM 11.5 11.5 7.0
GIZA Model 4 8.9 9.7 6.9
HMM, indep 11.2 11.5 7.2
HMM, joint 6.1 6.6 4.9
Table 2: Comparison of test set AER between vari-
ous models trained on the full 1.1 million sentences.
Model I+V I+P J+V J+P
10K sentences
Model 1 29.4 27.4 22.7 23.6
Model 2 20.1 18.2 16.5 14.9
HMM 15.2 12.1 8.9 8.4
100K sentences
Model 1 22.9 21.5 18.6 19.2
Model 2 15.1 13.1 12.9 10.2
HMM 9.2 8.0 6.0 5.3
1.1M sentences
Model 1 20.0 19.4 16.5 17.3
Model 2 12.7 11.4 11.6 9.2
HMM 7.6 6.6 5.7 5.2
Table 3: Contributions of using joint training versus
independent training and posterior decoding (with
the optimal threshold) instead of Viterbi decoding,
evaluated on the development set.
independent training, which gives us a huge boost.
The second change, which is more minor and or-
thogonal, is using posterior decoding instead of
Viterbi decoding, which also helps performance for
model 2 and HMM, but not model 1. Table 3 quan-
tifies the contribution of each of these two dimen-
sions.
Posterior decoding In our results, we have tuned
our threshold to minimize AER. It turns out that
AER is relatively insensitive to the threshold as Fig-
ure 2 shows. There is a large range from 0.2 to 0.5
where posterior decoding outperforms Viterbi de-
coding.
Initialization and convergence In addition to im-
proving performance, joint training also enjoys cer-
tain robustness properties. Specialized initialization
is absolutely crucial for an independently-trained
109
 0
 2
 4
 6
 8
 10
 12
 14
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pe
rfo
rm
an
ce
Posterior threshold
100-Precision
100-Recall
AER
Viterbi AER
Figure 2: The precision, recall, and AER as the
threshold is varied for posterior decoding in a jointly
trained pair of HMMs.
HMM model. If we initialize the HMM model with
uniform translation parameters, the HMM converges
to a completely senseless local optimum with AER
above 50%. Initializing the HMM with model 1 pa-
rameters alleviates this problem.
On the other hand, if we jointly train two HMMs
starting from a uniform initialization, the HMMs
converge to a surprisingly good solution. On the full
training set, training two HMMs jointly from uni-
form initialization yields 5.7% AER, only slightly
higher than 5.2% AER using model 1 initialization.
We suspect that the agreement term of the objective
forces the two HMMs to avoid many local optima
that each one would have on its own, since these lo-
cal optima correspond to posteriors over alignments
that would be very unlikely to agree. We also ob-
served that jointly trained HMMs converged very
quickly?in 5 iterations?and did not exhibit over-
fitting with increased iterations.
Common errors The major source of remaining
errors are recall errors that come from the shortcom-
ings of the HMM model. The E?F model gives 0
probability to any many-to-one alignments and the
F?E model gives 0 probability to any one-to-many
alignments. By enforcing agreement, the two mod-
els are effectively restricted to one-to-one (or zero)
alignments. Posterior decoding is in principle ca-
pable of proposing many-to-many alignments, but
these alignments occur infrequently since the poste-
riors are generally sharply peaked around the Viterbi
alignment. In some cases, however, we do get one-
to-many alignments in both directions.
Another common type of errors are precision er-
rors due to the models overly-aggressively prefer-
ring alignments that preserve monotonicity. Our
HMM model only uses 11 distortion parameters,
which means distortions are not sensitive to the lex-
ical context of the sentences. For example, in one
sentence, le is incorrectly aligned to the as a mono-
tonic alignment following another pair of correctly
aligned words, and then the monotonicity is broken
immediately following le?the. Here, the model is
insensitive to the fact that alignments following arti-
cles tend to be monotonic, but alignments preceding
articles are less so.
Another phenomenon is the insertion of ?stepping
stone? alignments. Suppose two edges (i, j) and
(i+4, j+4) have a very high probability of being in-
cluded in an alignment, but the words between them
are not good translations of each other. If the inter-
vening English words were null-aligned, we would
have to pay a big distortion penalty for jumping 4
positions. On the other hand, if the edge (i+2, j+2)
were included, that penalty would be mitigated. The
translation cost for forcing that edge is smaller than
the distortion cost.
4.2 BLEU evaluation
To see whether our improvement in AER also im-
proves BLEU score, we aligned 100K English-
French sentences from the Europarl corpus and
tested on 3000 sentences of length 5?15. Using
GIZA++ model 4 alignments and Pharaoh (Koehn
et al, 2003), we achieved a BLEU score of 0.3035.
By using alignments from our jointly trained HMMs
instead, we get a BLEU score of 0.3051. While this
improvement is very modest, we are currently inves-
tigating alternative ways of interfacing with phrase
table construction to make a larger impact on trans-
lation quality.
5 Related Work
Our approach is similar in spirit to co-training,
where two classifiers, complementary by the virtue
of having different views of the data, are trained
jointly to encourage agreement (Blum and Mitchell,
1998; Collins and Singer, 1999). One key difference
110
in our work is that we rely exclusively on data like-
lihood to guide the two models in an unsupervised
manner, rather than relying on an initial handful of
labeled examples.
The idea of exploiting agreement between two la-
tent variable models is not new; there has been sub-
stantial previous work on leveraging the strengths
of two complementary models. Klein and Man-
ning (2004) combine two complementary mod-
els for grammar induction, one that models con-
stituency and one that models dependency, in a man-
ner broadly similar to the current work. Aside from
investigating a different domain, one novel aspect of
this paper is that we present a formal objective and a
training algorithm for combining two generic mod-
els.
6 Conclusion
We have described an efficient and fully unsuper-
vised method of producing state-of-the-art word
alignments. By training two simple sequence-based
models to agree, we achieve substantial error re-
ductions over standard models. Our jointly trained
HMM models reduce AER by 29% over test-time
intersected GIZA++ model 4 alignments and also
increase our robustness to varying initialization reg-
imens. While AER is only a weak indicator of final
translation quality in many current translation sys-
tems, we hope that more accurate alignments can
eventually lead to improvements in the end-to-end
translation process.
Acknowledgments We thank the anonymous re-
viewers for their comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-training. In Proceedings of the
COLT 1998.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Computational
Linguistics, 19:263?311.
Michael Collins and Yoram Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings of
EMNLP 1999.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine translation.
In Proceedings of HLT-EMNLP.
Dan Klein and Christopher D. Manning. 2004. Corpus-Based
Induction of Syntactic Structure: Models of Dependency and
Constituency. In Proceedings of ACL 2004.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of HLT-
NAACL 2003.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In Proceed-
ings of the 20th International Conference on Computational
Linguistics, August.
Robert C. Moore. 2004. Improving IBM Word Alignment
Model 1. In Proceedings of ACL 2004.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of EMNLP.
Hermann Ney and Stephan Vogel. 1996. HMM-Based Word
Alignment in Statistical Translation. In COLING.
Franz Josef Och and Hermann Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models. Computa-
tional Linguistics, 29:19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
Discriminative Matching Approach to Word Alignment. In
Proceedings of EMNLP 2005.
L. G. Valiant. 1979. The complexity of computing the perma-
nent. Theoretical Computer Science, 8:189?201.
Appendix: Derivation of agreement EM
To simplify notation, we drop the explicit reference
to the parameters ?. Lower bound the objective in
Equation 3 by introducing a distribution q(z;x) and
using the concavity of log:
X
x
log p1(x)p2(x)
X
z
p1(z | x)p2(z | x) (4)
?
X
x,z
q(z;x) log p1(x)p2(x)p1(z | x)p2(z | x)q(z;x) (5)
=
X
x,z
q(z;x) log p1(z | x)p2(z | x)q(z;x) + C (6)
=
X
x,z
q(z;x) log p1(x, z)p2(x, z) + D, (7)
where C depends only on ? but not q and D de-
pends only q but not ?. The E-step chooses q given
a fixed ? to maximize the lower bound. Equation 6
is exactly
?
x?KL(q||p1p2) + C , which is maxi-
mized by setting q proportional to p1p2. The M-step
chooses ? given a fixed q. Equation 7 decomposes
into two separate optimization problems.
111
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 112?119,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Alignment via Quadratic Assignment
Simon Lacoste-Julien
UC Berkeley, Berkeley, CA 94720
slacoste@cs.berkeley.edu
Ben Taskar
UC Berkeley, Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley, Berkeley, CA 94720
klein@cs.berkeley.edu
Michael I. Jordan
UC Berkeley, Berkeley, CA 94720
jordan@cs.berkeley.edu
Abstract
Recently, discriminative word alignment methods
have achieved state-of-the-art accuracies by extend-
ing the range of information sources that can be
easily incorporated into aligners. The chief advan-
tage of a discriminative framework is the ability
to score alignments based on arbitrary features of
the matching word tokens, including orthographic
form, predictions of other models, lexical context
and so on. However, the proposed bipartite match-
ing model of Taskar et al (2005), despite being
tractable and effective, has two important limita-
tions. First, it is limited by the restriction that
words have fertility of at most one. More impor-
tantly, first order correlations between consecutive
words cannot be directly captured by the model. In
this work, we address these limitations by enrich-
ing the model form. We give estimation and infer-
ence algorithms for these enhancements. Our best
model achieves a relative AER reduction of 25%
over the basic matching formulation, outperform-
ing intersected IBM Model 4 without using any
overly compute-intensive features. By including
predictions of other models as features, we achieve
AER of 3.8 on the standard Hansards dataset.
1 Introduction
Word alignment is a key component of most end-
to-end statistical machine translation systems. The
standard approach to word alignment is to construct
directional generative models (Brown et al, 1990;
Och and Ney, 2003), which produce a sentence in
one language given the sentence in another lan-
guage. While these models require sentence-aligned
bitexts, they can be trained with no further super-
vision, using EM. Generative alignment models do,
however, have serious drawbacks. First, they require
extensive tuning and processing of large amounts
of data which, for the better-performing models, is
a non-trivial resource requirement. Second, condi-
tioning on arbitrary features of the input is difficult;
for example, we would like to condition on the or-
thographic similarity of a word pair (for detecting
cognates), the presence of that pair in various dic-
tionaries, the similarity of the frequency of its two
words, choices made by other alignment systems,
and so on.
Recently, Moore (2005) proposed a discrimina-
tive model in which pairs of sentences (e, f) and
proposed alignments a are scored using a linear
combination of arbitrary features computed from the
tuples (a, e, f). While there are no restrictions on
the form of the model features, the problem of find-
ing the highest scoring alignment is very difficult
and involves heuristic search. Moreover, the param-
eters of the model must be estimated using averaged
perceptron training (Collins, 2002), which can be
unstable. In contrast, Taskar et al (2005) cast word
alignment as a maximum weighted matching prob-
lem, in which each pair of words (ej , fk) in a sen-
tence pair (e, f) is associated with a score sjk(e, f)
reflecting the desirability of the alignment of that
pair. Importantly, this problem is computationally
tractable. The alignment for the sentence pair is the
highest scoring matching under constraints (such as
the constraint that matchings be one-to-one). The
scoring model sjk(e, f) can be based on a rich fea-
ture set defined on word pairs (ej , fk) and their con-
text, including measures of association, orthogra-
phy, relative position, predictions of generative mod-
els, etc. The parameters of the model are estimated
within the framework of large-margin estimation; in
particular, the problem turns out to reduce to the
112
solution of a (relatively) small quadratic program
(QP). The authors show that large-margin estimation
is both more stable and more accurate than percep-
tron training.
While the bipartite matching approach is a use-
ful first step in the direction of discriminative word
alignment, for discriminative approaches to com-
pete with and eventually surpass the most sophisti-
cated generative models, it is necessary to consider
more realistic underlying statistical models. Note in
particular two substantial limitations of the bipartite
matching model of Taskar et al (2005): words have
fertility of at most one, and there is no way to incor-
porate pairwise interactions among alignment deci-
sions. Moving beyond these limitations?while re-
taining computational tractability?is the next major
challenge for discriminative word alignment.
In this paper, we show how to overcome both lim-
itations. First, we introduce a parameterized model
that penalizes different levels of fertility. While this
extension adds very useful expressive power to the
model, it turns out not to increase the computa-
tional complexity of the aligner, for either the pre-
diction or the parameter estimation problem. Sec-
ond, we introduce a more thoroughgoing extension
which incorporates first-order interactions between
alignments of consecutive words into the model. We
do this by formulating the alignment problem as a
quadratic assignment problem (QAP), where in ad-
dition to scoring individual edges, we also define
scores of pairs of edges that connect consecutive
words in an alignment. The predicted alignment is
the highest scoring quadratic assignment.
QAP is an NP-hard problem, but in the range of
problem sizes that we need to tackle the problem can
be solved efficiently. In particular, using standard
off-the-shelf integer program solvers, we are able to
solve the QAP problems in our experiments in under
a second. Moreover, the parameter estimation prob-
lem can also be solved efficiently by making use of
a linear relaxation of QAP for the min-max formu-
lation of large-margin estimation (Taskar, 2004).
We show that these two extensions yield signif-
icant improvements in error rates when compared
to the bipartite matching model. The addition of a
fertility model improves the AER by 0.4. Model-
ing first-order interactions improves the AER by 1.8.
Combining the two extensions results in an improve-
ment in AER of 2.3, yielding alignments of better
quality than intersected IBM Model 4. Moreover,
including predictions of bi-directional IBM Model
4 and model of Liang et al (2006) as features, we
achieve an absolute AER of 3.8 on the English-
French Hansards alignment task?the best AER re-
sult published on this task to date.
2 Models
We begin with a quick summary of the maximum
weight bipartite matching model in (Taskar et al,
2005). More precisely, nodes V = Vs ? V t cor-
respond to words in the ?source? (Vs) and ?tar-
get? (V t) sentences, and edges E = {jk : j ?
Vs, k ? V t} correspond to alignments between word
pairs.1 The edge weights sjk represent the degree
to which word j in one sentence can be translated
using the word k in the other sentence. The pre-
dicted alignment is chosen by maximizing the sum
of edge scores. A matching is represented using a
set of binary variables yjk that are set to 1 if word
j is assigned to word k in the other sentence, and 0
otherwise. The score of an assignment is the sum of
edge scores: s(y) = ?jk sjkyjk. For simplicity, let
us begin by assuming that each word aligns to one or
zero words in the other sentence; we revisit the issue
of fertility in the next section. The maximum weight
bipartite matching problem, arg maxy?Y s(y), can
be solved using combinatorial algorithms for min-
cost max-flow, expressed in a linear programming
(LP) formulation as follows:
max
0?z?1
?
jk?E
sjkzjk (1)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs,
where the continuous variables zjk are a relax-
ation of the corresponding binary-valued variables
yjk. This LP is guaranteed to have integral (and
hence optimal) solutions for any scoring function
s(y) (Schrijver, 2003). Note that although the above
LP can be used to compute alignments, combina-
torial algorithms are generally more efficient. For
1The source/target designation is arbitrary, as the models
considered below are all symmetric.
113
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
(a) (b)
Figure 2: An example fragment that requires fertility
greater than one to correctly label. (a) The guess of
the baseline M model. (b) The guess of the M+F
fertility-augmented model.
example, in Figure 1(a), we show a standard con-
struction for an equivalent min-cost flow problem.
However, we build on this LP to develop our exten-
sions to this model below. Representing the predic-
tion problem as an LP or an integer LP provides a
precise (and concise) way of specifying the model
and allows us to use the large-margin framework
of Taskar (2004) for parameter estimation described
in Section 3.
For a sentence pair x, we denote position pairs by
xjk and their scores as sjk. We let sjk = w>f(xjk)
for some user provided feature mapping f and ab-
breviate w>f(x,y) = ?jk yjkw>f(xjk). We can
include in the feature vector the identity of the two
words, their relative positions in their respective sen-
tences, their part-of-speech tags, their string similar-
ity (for detecting cognates), and so on.
2.1 Fertility
An important limitation of the model in Eq. (1) is
that in each sentence, a word can align to at most
one word in the translation. Although it is common
that words have gold fertility zero or one, it is cer-
tainly not always true. Consider, for example, the
bitext fragment shown in Figure 2(a), where back-
bone is aligned to the phrase e?pine dorsal. In this
figure, outlines are gold alignments, square for sure
alignments, round for possibles, and filled squares
are target algnments (for details on gold alignments,
see Section 4). When considering only the sure
alignments on the standard Hansards dataset, 7 per-
cent of the word occurrences have fertility 2, and 1
percent have fertility 3 and above; when considering
the possible alignments high fertility is much more
common?31 percent of the words have fertility 3
and above.
One simple fix to the original matching model is
to increase the right hand sides for the constraints
in Eq. (1) from 1 to D, where D is the maximum
allowed fertility. However, this change results in
an undesirable bimodal behavior, where maximum
weight solutions either have all words with fertil-
ity 0 or D, depending on whether most scores sjk
are positive or negative. For example, if scores tend
to be positive, most words will want to collect as
many alignments as they are permitted. What the
model is missing is a means for encouraging the
common case of low fertility (0 or 1), while allowing
higher fertility when it is licensed. This end can be
achieved by introducing a penalty for having higher
fertility, with the goal of allowing that penalty to
vary based on features of the word in question (such
as its frequency or identity).
In order to model such a penalty, we introduce
indicator variables zdj? (and zd?k) with the intended
meaning: node j has fertility of at least d (and node
k has fertility of at least d). In the following LP, we
introduce a penalty of
?
2?d?D sdj?zdj? for fertility
of node j, where each term sdj? ? 0 is the penalty
increment for increasing the fertility from d ? 1 to
d:
max
0?z?1
?
jk?E
sjkzjk (2)
?
?
j?Vs,2?d?D
sdj?zdj? ?
?
k?Vt,2?d?D
sd?kzd?k
s.t.
?
j?Vs
zjk ? 1 +
?
2?d?D
zd?k, ?k ? V t;
?
k?Vt
zjk ? 1 +
?
2?d?D
zdj?, ?j ? Vs.
We can show that this LP always has integral so-
lutions by a reduction to a min-cost flow problem.
The construction is shown in Figure 1(b). To ensure
that the new variables have the intended semantics,
we need to make sure that sdj? ? sd?j? if d ? d?,
so that the lower cost zdj? is used before the higher
cost zd?j? to increase fertility. This restriction im-
114
(a) (b) (c)
Figure 1: (a) Maximum weight bipartite matching as min-cost flow. Diamond-shaped nodes represent flow
source and sink. All edge capacities are 1, with edges between round nodes (j, k) have cost ?sjk, edges
from source and to sink have cost 0. (b) Expanded min-cost flow graph with new edges from source and to
sink that allow fertility of up to 3. The capacities of the new edges are 1 and the costs are 0 for solid edges
from source and to sink, s2j?, s2?k for dashed edges, and s3j?, s3?k for dotted edges. (c) Three types of pairs
of edges included in the QAP model, where the nodes on both sides correspond to consecutive words.
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
(a) (b)
Figure 3: An example fragment with a monotonic
gold alignment. (a) The guess of the baseline M
model. (b) The guess of the M+Q quadratic model.
plies that the penalty must be monotonic and convex
as a function of the fertility.
To anticipate the results that we report in Sec-
tion 4, adding fertility to the basic matching model
makes the target algnment of the backbone example
feasible and, in this case, the model correctly labels
this fragment as shown in Figure 2(b).
2.2 First-order interactions
An even more significant limitation of the model
in Eq. (1) is that the edges interact only indi-
rectly through the competition induced by the con-
straints. Generative alignment models like the
HMM model (Vogel et al, 1996) and IBM models 4
and above (Brown et al, 1990; Och and Ney, 2003)
directly model correlations between alignments of
consecutive words (at least on one side). For exam-
ple, Figure 3 shows a bitext fragment whose gold
alignment is strictly monotonic. This monotonicity
is quite common ? 46% of the words in the hand-
aligned data diagonally follow a previous alignment
in this way. We can model the common local align-
ment configurations by adding bonuses for pairs of
edges. For example, strictly monotonic alignments
can be encouraged by boosting the scores of edges
of the form ?(j, k), (j + 1, k + 1)?. Another trend,
common in English-French translation (7% on the
hand-aligned data), is the local inversion of nouns
and adjectives, which typically involves a pair of
edges ?(j, k + 1), (j + 1, k)?. Finally, a word in one
language is often translated as a phrase (consecutive
sequence of words) in the other language. This pat-
tern involves pairs of edges with the same origin on
one side: ?(j, k), (j, k+1)? or ?(j, k), (j+1, k)?. All
three of these edge pair patterns are shown in Fig-
ure 1(c). Note that the set of such edge pairs Q =
{jklm : |j ? l| ? 1, |k ? m| ? 1} is of linear size
in the number of edges.
Formally, we add to the model variables zjklm
which indicate whether both edge jk and lm are in
the alignment. We also add a corresponding score
sjklm, which we assume to be non-negative, since
the correlations we described are positive. (Nega-
tive scores can also be used, but the resulting for-
mulation we present below would be slightly differ-
ent.) To enforce the semantics zjklm = zjkzlm, we
use a pair of constraints zjklm ? zjk; zjklm ? zlm.
Since sjklm is positive, at the optimum, zjklm =
115
min(zjk, zlm). If in addition zjk, zlm are integral (0
or 1), then zjklm = zjkzlm. Hence, solving the fol-
lowing LP as an integer linear program will find the
optimal quadratic assignment for our model:
max
0?z?1
?
jk?E
sjkzjk +
?
jklm?Q
sjklmzjklm (3)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs;
zjklm ? zjk, zjklm ? zlm, ?jklm ? Q.
Note that we can also combine this extension with
the fertility extension described above.
To once again anticipate the results presented in
Section 4, the baseline model of Taskar et al (2005)
makes the prediction given in Figure 3(a) because
the two missing alignments are atypical translations
of common words. With the addition of edge pair
features, the overall monotonicity pushes the align-
ment to that of Figure 3(b).
3 Parameter estimation
To estimate the parameters of our model, we fol-
low the large-margin formulation of Taskar (2004).
Our input is a set of training instances {(xi,yi)}mi=1,
where each instance consists of a sentence pair xi
and a target algnment yi. We would like to find
parameters w that predict correct alignments on the
training data: yi = arg max
y?i?Yi
w>f(xi, y?i) for each i,
where Yi is the space of matchings for the sentence
pair xi.
In standard classification problems, we typically
measure the error of prediction, `(yi, y?i), using the
simple 0-1 loss. In structured problems, where we
are jointly predicting multiple variables, the loss is
often more complex. While the F-measure is a nat-
ural loss function for this task, we instead chose a
sensible surrogate that fits better in our framework:
weighted Hamming distance, which counts the num-
ber of variables in which a candidate solution y? dif-
fers from the target output y, with different penalty
for false positives (c+) and false negatives (c?):
`(y, y?) =
?
jk
[
c+(1 ? yjk)y?jk + c?(1 ? y?jk)yjk
]
.
We use an SVM-like hinge upper bound on
the loss `(yi, y?i), given by maxy?i?Yi [w>fi(y?i) +
`i(y?i) ? w>fi(yi)], where `i(y?i) = `(yi, y?i), and
fi(y?i) = f(xi, y?i). Minimizing this upper bound
encourages the true alignment yi to be optimal with
respect to w for each instance i:
min
||w||??
?
i
max
y?i?Yi
[w>fi(y?i) + `i(y?i)] ? w>fi(yi),
where ? is a regularization parameter.
In this form, the estimation problem is a mixture
of continuous optimization over w and combinato-
rial optimization over yi. In order to transform it
into a more standard optimization problem, we need
a way to efficiently handle the loss-augmented in-
ference, maxy?i?Yi [w>fi(y?i) + `i(y?i)]. This opti-
mization problem has precisely the same form as the
prediction problem whose parameters we are trying
to learn ? maxy?i?Yi w>fi(y?i) ? but with an addi-
tional term corresponding to the loss function. Our
assumption that the loss function decomposes over
the edges is crucial to solving this problem. We omit
the details here, but note that we can incorporate the
loss function into the LPs for various models we de-
scribed above and ?plug? them into the large-margin
formulation by converting the estimation problem
into a quadratic problem (QP) (Taskar, 2004). This
QP can be solved using any off-the-shelf solvers,
such as MOSEK or CPLEX.2 An important differ-
ence that comes into play for the estimation of the
quadratic assignment models in Equation (3) is that
inference involves solving an integer linear program,
not just an LP. In fact the LP is a relaxation of the in-
teger LP and provides an upper bound on the value
of the highest scoring assignment. Using the LP re-
laxation for the large-margin QP formulation is an
approximation, but as our experiments indicate, this
approximation is very effective. At testing time, we
use the integer LP to predict alignments. We have
also experimented with using just the LP relaxation
at testing time and then independently rounding each
fractional edge value, which actually incurs no loss
in alignment accuracy, as we discuss below.
2When training on 200 sentences, the QP we obtain contains
roughly 700K variables and 300K constraints and is solved in
roughly 10 minutes on a 2.8 GHz Pentium 4 machine. Aligning
the whole training set with the flow formulation takes a few
seconds, whereas using the integer programming (for the QAP
formulation) takes 1-2 minutes.
116
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
(a) (b) (c)
Figure 4: An example fragment with several multiple fertility sure alignments. (a) The guess of the M+Q
model with maximum fertility of one. (b) The guess of the M+Q+F quadratic model with fertility two
permitted. (c) The guess of the M+Q+F model with lexical fertility features.
4 Experiments
We applied our algorithms to word-level alignment
using the English-French Hansards data from the
2003 NAACL shared task (Mihalcea and Pedersen,
2003). This corpus consists of 1.1M automatically
aligned sentences, and comes with a validation set of
37 sentence pairs and a test set of 447 sentences. The
validation and test sentences have been hand-aligned
(see Och and Ney (2003)) and are marked with both
sure and possible alignments. Using these align-
ments, alignment error rate (AER) is calculated as:
(
1 ? |A ? S| + |A ? P ||A| + |S|
)
? 100%.
Here, A is a set of proposed index pairs, S is the
sure gold pairs, and P is the possible gold pairs.
For example, in Figure 4, proposed alignments are
shown against gold alignments, with open squares
for sure alignments, rounded open squares for possi-
ble alignments, and filled black squares for proposed
alignments.
The input to our algorithm is a small number of
labeled examples. In order to make our results more
comparable with Moore (2005), we split the origi-
nal set into 200 training examples and 247 test ex-
amples. We also trained on only the first 100 to
make our results more comparable with the exper-
iments of Och and Ney (2003), in which IBM model
4 was tuned using 100 sentences. In all our experi-
ments, we used a structured loss function that penal-
ized false negatives 10 times more than false posi-
tives, where the value of 10 was picked by using a
validation set. The regularization parameter ? was
also chosen using the validation set.
4.1 Features and results
We parameterized all scoring functions sjk, sdj?,
sd?k and sjklm as weighted linear combinations of
feature sets. The features were computed from
the large unlabeled corpus of 1.1M automatically
aligned sentences.
In the remainder of this section we describe the
improvements to the model performance as various
features are added. One of the most useful features
for the basic matching model is, of course, the set of
predictions of IBM model 4. However, computing
these features is very expensive and we would like to
build a competitive model that doesn?t require them.
Instead, we made significant use of IBM model 2 as
a source of features. This model, although not very
accurate as a predictive model, is simple and cheap
to construct and it is a useful source of features.
The Basic Matching Model: Edge Features In
the basic matching model of Taskar et al (2005),
called M here, one can only specify features on pairs
of word tokens, i.e. alignment edges. These features
117
include word association, orthography, proximity,
etc., and are documented in Taskar et al (2005). We
also augment those features with the predictions of
IBM Model 2 run on the training and test sentences.
We provided features for model 2 trained in each
direction, as well as the intersected predictions, on
each edge. By including the IBM Model 2 features,
the performance of the model described in Taskar et
al. (2005) on our test set (trained on 200 sentences)
improves from 10.0 AER to 8.2 AER, outperforming
unsymmetrized IBM Model 4 (but not intersected
model 4).
As an example of the kinds of errors the baseline
M system makes, see Figure 2 (where multiple fer-
tility cannot be predicted), Figure 3 (where a prefer-
ence for monotonicity cannot be modeled), and Fig-
ure 4 (which shows several multi-fertile cases).
The Fertility Model: Node Features To address
errors like those shown in Figure 2, we increased
the maximum fertility to two using the parameter-
ized fertility model of Section 2.1. The model learns
costs on the second flow arc for each word via fea-
tures not of edges but of single words. The score of
taking a second match for a word w was based on
the following features: a bias feature, the proportion
of times w?s type was aligned to two or more words
by IBM model 2, and the bucketed frequency of the
word type. This model was called M+F. We also in-
cluded a lexicalized feature for words which were
common in our training set: whether w was ever
seen in a multiple fertility alignment (more on this
feature later). This enabled the system to learn that
certain words, such as the English not and French
verbs like aurait commonly participate in multiple
fertility configurations.
Figure 5 show the results using the fertility exten-
sion. Adding fertility lowered AER from 8.5 to 8.1,
though fertility was even more effective in conjunc-
tion with the quadratic features below. The M+F set-
ting was even able to correctly learn some multiple
fertility instances which were not seen in the training
data, such as those shown in Figure 2.
The First-Order Model: Quadratic Features
With or without the fertility model, the model makes
mistakes such as those shown in Figure 3, where
atypical translations of common words are not cho-
sen despite their local support from adjacent edges.
In the quadratic model, we can associate features
with pairs of edges. We began with features which
identify each specific pattern, enabling trends of
monotonicity (or inversion) to be captured. We also
added to each edge pair the fraction of times that
pair?s pattern (monotonic, inverted, one to two) oc-
curred according each version of IBM model 2 (for-
ward, backward, intersected).
Figure 5 shows the results of adding the quadratic
model. M+Q reduces error over M from 8.5 to 6.7
(and fixes the errors shown in Figure 3). When both
the fertility and quadratic extensions were added,
AER dropped further, to 6.2. This final model is
even able to capture the diamond pattern in Figure 4;
the adjacent cycle of alignments is reinforced by the
quadratic features which boost adjacency. The ex-
ample in Figure 4 shows another interesting phe-
nomenon: the multi-fertile alignments for not and
de?pute? are learned even without lexical fertility fea-
tures (Figure 4b), because the Dice coefficients of
those words with their two alignees are both high.
However the surface association of aurait with have
is much higher than with would. If, however, lexi-
cal features are added, would is correctly aligned as
well (Figure 4c), since it is observed in similar pe-
riphrastic constructions in the training set.
We have avoided using expensive-to-compute fea-
tures like IBM model 4 predictions up to this point.
However, if these are available, our model can im-
prove further. By adding model 4 predictions to the
edge features, we get a relative AER reduction of
27%, from 6.5 to 4.5. By also including as features
the posteriors of the model of Liang et al (2006), we
achieve AER of 3.8, and 96.7/95.5 precision/recall.
It is comforting to note that in practice, the burden
of running an integer linear program at test time can
be avoided. We experimented with using just the LP
relaxation and found that on the test set, only about
20% of sentences have fractional solutions and only
0.2% of all edges are fractional. Simple rounding3
of each edge value in the LP solution achieves the
same AER as the integer LP solution, while using
about a third of the computation time on average.
3We slightly bias the system on the recall side by rounding
0.5 up, but this doesn?t yield a noticeable difference in the re-
sults.
118
Model Prec Rec AER
Generative
IBM 2 (E?F) 73.6 87.7 21.7
IBM 2 (F?E) 75.4 87.0 20.6
IBM 2 (intersected) 90.1 80.4 14.3
IBM 4 (E?F) 90.3 92.1 9.0
IBM 4 (F?E) 90.8 91.3 9.0
IBM 4 (intersected) 98.0 88.1 6.5
Discriminative (100 sentences)
Matching (M) 94.1 88.5 8.5
M + Fertility (F) 93.9 89.4 8.1
M + Quadratic (Q) 94.4 91.9 6.7
M + F + Q 94.8 92.5 6.2
M + F + Q + IBM4 96.4 94.4 4.5
Discriminative (200 sentences)
Matching (M) 93.4 89.7 8.2
M + Fertility (F) 93.6 90.1 8.0
M + Quadratic (Q) 95.0 91.1 6.8
M + F + Q 95.2 92.4 6.1
M + F + Q + IBM4 96.0 95.0 4.4
Figure 5: AER on the Hansards task.
5 Conclusion
We have shown that the discriminative approach to
word alignment can be extended to allow flexible
fertility modeling and to capture first-order inter-
actions between alignments of consecutive words.
These extensions significantly enhance the expres-
sive power of the discriminative approach; in partic-
ular, they make it possible to capture phenomena of
monotonicity, local inversion and contiguous fertil-
ity trends?phenomena that are highly informative
for alignment. They do so while remaining compu-
tationally efficient in practice both for prediction and
for parameter estimation.
Our best model achieves a relative AER reduc-
tion of 25% over the basic matching formulation,
beating intersected IBM Model 4 without the use
of any compute-intensive features. Including Model
4 predictions as features, we achieve a further rela-
tive AER reduction of 32% over intersected Model
4 alignments. By also including predictions of an-
other model, we drive AER down to 3.8. We are
currently investigating whether the improvement in
AER results in better translation BLEU score. Al-
lowing higher fertility and optimizing a recall bi-
ased cost function provide a significant increase in
recall relative to the intersected IBM model 4 (from
88.1% to 94.4%), with only a small degradation in
precision. We view this as a particularly promising
aspect of our work, given that phrase-based systems
such as Pharaoh (Koehn et al, 2003) perform better
with higher recall alignments.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In HLT-NAACL.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop, Building and Using parallel
Texts: Data Driven Machine Translation and Beyond,
pages 1?6, Edmonton, Alberta, Canada.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. HLT/EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
A. Schrijver. 2003. Combinatorial Optimization: Poly-
hedra and Efficiency. Springer.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
EMNLP.
B. Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In COLING
16, pages 836?841.
119
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 320?327,
New York, June 2006. c?2006 Association for Computational Linguistics
Prototype-Driven Learning for Sequence Models
Aria Haghighi
Computer Science Division
University of California Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California Berkeley
klein@cs.berkeley.edu
Abstract
We investigate prototype-driven learning for pri-
marily unsupervised sequence modeling. Prior
knowledge is specified declaratively, by provid-
ing a few canonical examples of each target an-
notation label. This sparse prototype information
is then propagated across a corpus using distri-
butional similarity features in a log-linear gener-
ative model. On part-of-speech induction in En-
glish and Chinese, as well as an information extrac-
tion task, prototype features provide substantial er-
ror rate reductions over competitive baselines and
outperform previous work. For example, we can
achieve an English part-of-speech tagging accuracy
of 80.5% using only three examples of each tag
and no dictionary constraints. We also compare to
semi-supervised learning and discuss the system?s
error trends.
1 Introduction
Learning, broadly taken, involves choosing a good
model from a large space of possible models. In su-
pervised learning, model behavior is primarily de-
termined by labeled examples, whose production
requires a certain kind of expertise and, typically,
a substantial commitment of resources. In unsu-
pervised learning, model behavior is largely deter-
mined by the structure of the model. Designing
models to exhibit a certain target behavior requires
another, rare kind of expertise and effort. Unsuper-
vised learning, while minimizing the usage of la-
beled data, does not necessarily minimize total ef-
fort. We therefore consider here how to learn mod-
els with the least effort. In particular, we argue for a
certain kind of semi-supervised learning, which we
call prototype-driven learning.
In prototype-driven learning, we specify prototyp-
ical examples for each target label or label configu-
ration, but do not necessarily label any documents or
sentences. For example, when learning a model for
Penn treebank-style part-of-speech tagging in En-
glish, we may list the 45 target tags and a few exam-
ples of each tag (see figure 4 for a concrete prototype
list for this task). This manner of specifying prior
knowledge about the task has several advantages.
First, is it certainly compact (though it remains to
be proven that it is effective). Second, it is more or
less the minimum one would have to provide to a
human annotator in order to specify a new annota-
tion task and policy (compare, for example, with the
list in figure 2, which suggests an entirely different
task). Indeed, prototype lists have been used ped-
agogically to summarize tagsets to students (Man-
ning and Schu?tze, 1999). Finally, natural language
does exhibit proform and prototype effects (Radford,
1988), which suggests that learning by analogy to
prototypes may be effective for language tasks.
In this paper, we consider three sequence mod-
eling tasks: part-of-speech tagging in English and
Chinese and a classified ads information extraction
task. Our general approach is to use distributional
similarity to link any given word to similar pro-
totypes. For example, the word reported may be
linked to said, which is in turn a prototype for the
part-of-speech VBD. We then encode these pro-
totype links as features in a log-linear generative
model, which is trained to fit unlabeled data (see
section 4.1). Distributional prototype features pro-
vide substantial error rate reductions on all three
tasks. For example, on English part-of-speech tag-
ging with three prototypes per tag, adding prototype
features to the baseline raises per-position accuracy
from 41.3% to 80.5%.
2 Tasks and Related Work: Tagging
For our part-of-speech tagging experiments, we used
data from the English and Chinese Penn treebanks
(Marcus et al, 1994; Ircs, 2002). Example sentences
320
(a) DT VBN NNS RB MD VB NNS TO VB NNS IN NNS RBR CC RBR RB .
The proposed changes also would allow executives to report exercises of options later and less often .
(b) NR AD VV AS PU NN VV DER VV PU PN AD VV DER VV PU DEC NN VV PU
! " # $ % & ? ( ) * + , - . / 0 * + , 1 2 3 4 5 6 7
(c) FEAT FEAT FEAT FEAT NBRHD NBRHD NBRHD NBRHD NBRHD SIZE SIZE SIZE SIZE
Vine covered cottage , near Contra Costa Hills . 2 bedroom house ,
FEAT FEAT FEAT FEAT FEAT RESTR RESTR RESTR RESTR RENT RENT RENT RENT
modern kitchen and dishwasher . No pets allowed . 1050 / month$
Figure 1: Sequence tasks: (a) English POS, (b) Chinese POS, and (c) Classified ad segmentation
are shown in figure 1(a) and (b). A great deal of re-
search has investigated the unsupervised and semi-
supervised induction of part-of-speech models, es-
pecially in English, and there is unfortunately only
space to mention some highly related work here.
One approach to unsupervised learning of part-
of-speech models is to induce HMMs from un-
labeled data in a maximum-likelihood framework.
For example, Merialdo (1991) presents experiments
learning HMMs using EM. Merialdo?s results most
famously show that re-estimation degrades accu-
racy unless almost no examples are labeled. Less
famously, his results also demonstrate that re-
estimation can improve tagging accuracies to some
degree in the fully unsupervised case.
One recent and much more successful approach
to part-of-speech learning is contrastive estimation,
presented in Smith and Eisner (2005). They utilize
task-specific comparison neighborhoods for part-of-
speech tagging to alter their objective function.
Both of these works require specification of the
legal tags for each word. Such dictionaries are large
and embody a great deal of lexical knowledge. A
prototype list, in contrast, is extremely compact.
3 Tasks and Related Work: Extraction
Grenager et al (2005) presents an unsupervised
approach to an information extraction task, called
CLASSIFIEDS here, which involves segmenting clas-
sified advertisements into topical sections (see fig-
ure 1(c)). Labels in this domain tend to be ?sticky?
in that the correct annotation tends to consist of
multi-element fields of the same label. The over-
all approach of Grenager et al (2005) typifies the
process involved in fully unsupervised learning on
new domain: they first alter the structure of their
HMM so that diagonal transitions are preferred, then
modify the transition structure to explicitly model
boundary tokens, and so on. Given enough refine-
Label Prototypes
ROOMATES roommate respectful drama
RESTRICTIONS pets smoking dog
UTILITIES utilities pays electricity
AVAILABLE immediately begin cheaper
SIZE 2 br sq
PHOTOS pictures image link
RENT $ month *number*15*1
CONTACT *phone* call *time*
FEATURES kitchen laundry parking
NEIGHBORHOOD close near shopping
ADDRESS address carlmont *ordinal*5
BOUNDARY ; . !
Figure 2: Prototype list derived from the develop-
ment set of the CLASSIFIEDS data. The BOUND-
ARY field is not present in the original annotation,
but added to model boundaries (see Section 5.3).
The starred tokens are the results of collapsing of
basic entities during pre-processing as is done in
(Grenager et al, 2005)
ments the model learns to segment with a reasonable
match to the target structure.
In section 5.3, we discuss an approach to this
task which does not require customization of model
structure, but rather centers on feature engineering.
4 Approach
In the present work, we consider the problem of
learning sequence models over text. For each doc-
ument x = [xi], we would like to predict a sequence
of labels y = [yi], where xi ? X and yi ? Y . We
construct a generative model, p(x, y|?), where ? are
the model?s parameters, and choose parameters to
maximize the log-likelihood of our observed dataD:
L(?;D) =
?
x?D
log p(x|?)
=
?
x?D
log
?
y
p(x, y|?)
321
yi?1
?DT,NN?
yi
?NN,VBD?
xi
reported
xi?1
witness
f(xi, yi) =
?
?
?
?
?
?
?
?
?
word = reported
suffix-2 = ed
proto = said
proto = had
?
?
?
?
?
?
?
?
?
?VBD
f(yi?1, yi) = DT ?NN ?VBD
Figure 3: Graphical model representation of trigram
tagger for English POS domain.
4.1 Markov Random Fields
We take our model family to be chain-structured
Markov random fields (MRFs), the undirected
equivalent of HMMs. Our joint probability model
over (x, y) is given by
p(x, y|?) = 1Z(?)
n
?
i=1
?(xi, yi)?(yi?1, yi)
where ?(c) is a potential over a clique c, taking the
form exp
{
?T f(c)
}
, and f(c) is the vector of fea-
tures active over c. In our sequence models, the
cliques are over the edges/transitions (yi?1, yi) and
nodes/emissions (xi, yi). See figure 3 for an exam-
ple from the English POS tagging domain.
Note that the only way an MRF differs from
a conditional random field (CRF) (Lafferty et al,
2001) is that the partition function is no longer ob-
servation dependent; we are modeling the joint prob-
ability of x and y instead of y given x. As a result,
learning an MRF is slightly harder than learning a
CRF; we discuss this issue in section 4.4.
4.2 Prototype-Driven Learning
We assume prior knowledge about the target struc-
ture via a prototype list, which specifies the set of
target labels Y and, for each label y ? Y , a set of
prototypes words, py ? Py. See figures 2 and 4 for
examples of prototype lists.1
1Note that this setting differs from the standard semi-
supervised learning setup, where a small number of fully la-
beled examples are given and used in conjunction with a larger
amount of unlabeled data. In our prototype-driven approach, we
never provide a single fully labeled example sequence. See sec-
tion 5.3 for further comparison of this setting to semi-supervised
learning.
Broadly, we would like to learn sequence models
which both explain the observed data and meet our
prior expectations about target structure. A straight-
forward way to implement this is to constrain each
prototype word to take only its given label(s) at
training time. As we show in section 5, this does
not work well in practice because this constraint on
the model is very sparse.
In providing a prototype, however, we generally
mean something stronger than a constraint on that
word. In particular, we may intend that words which
are in some sense similar to a prototype generally be
given the same label(s) as that prototype.
4.3 Distributional Similarity
In syntactic distributional clustering, words are
grouped on the basis of the vectors of their pre-
ceeding and following words (Schu?tze, 1995; Clark,
2001). The underlying linguistic idea is that replac-
ing a word with another word of the same syntactic
category should preserve syntactic well-formedness
(Radford, 1988). We present more details in sec-
tion 5, but for now assume that a similarity function
over word types is given.
Suppose further that for each non-prototype word
type w, we have a subset of prototypes, Sw, which
are known to be distributionally similar to w (above
some threshold). We would like our model to relate
the tags of w to those of Sw.
One approach to enforcing the distributional as-
sumption in a sequence model is by supplementing
the training objective (here, data likelihood) with a
penalty term that encourages parameters for which
each w?s posterior distribution over tags is compati-
ble with it?s prototypes Sw. For example, we might
maximize,
?
x?D
log p(x|?) ?
?
w
?
z?Sw
KL( t|z || t|w)
where t|w is the model?s distribution of tags for
word w. The disadvantage of a penalty-based ap-
proach is that it is difficult to construct the penalty
term in a way which produces exactly the desired
behavior.
Instead, we introduce distributional prototypes
into the learning process as features in our log-linear
model. Concretely, for each prototype z, we intro-
duce a predicate PROTO = z which becomes active
322
at each w for which z ? Sw (see figure 3). One ad-
vantage of this approach is that it allows the strength
of the distributional constraint to be calibrated along
with any other features; it was also more successful
in our experiments.
4.4 Parameter Estimation
So far we have ignored the issue of how we learn
model parameters ? which maximizeL(?;D). If our
model family were HMMs, we could use the EM al-
gorithm to perform a local search. Since we have
a log-linear formulation, we instead use a gradient-
based search. In particular, we use L-BFGS (Liu
and Nocedal, 1989), a standard numerical optimiza-
tion technique, which requires the ability to evaluate
L(?;D) and its gradient at a given ?.
The density p(x|?) is easily calculated up to the
global constant Z(?) using the forward-backward
algorithm (Rabiner, 1989). The partition function
is given by
Z(?) =
?
x
?
y
n
?
i=1
?(xi, yi)?(yi?1, yi)
=
?
x
?
y
score(x, y)
Z(?) can be computed exactly under certain as-
sumptions about the clique potentials, but can in all
cases be bounded by
Z?(?) =
K
?
`=1
Z?`(?) =
K
?
`=1
?
x:|x|=`
score(x, y)
WhereK is a suitably chosen large constant. We can
efficiently compute Z?`(?) for fixed ` using a gener-
alization of the forward-backward algorithm to the
lattice of all observations x of length ` (see Smith
and Eisner (2005) for an exposition).
Similar to supervised maximum entropy prob-
lems, the partial derivative of L(?;D) with respect
to each parameter ?j (associated with feature fj) is
given by a difference in feature expectations:
?L(?;D)
??j
=
?
x?D
(
Ey|x,?fj ? Ex,y|?fj
)
The first expectation is the expected count of the fea-
ture under the model?s p(y|x, ?) and is again eas-
ily computed with the forward-backward algorithm,
Num Tokens
Setting 48K 193K
BASE 42.2 41.3
PROTO 61.9 68.8
PROTO+SIM 79.1 80.5
Table 1: English POS results measured by per-
position accuracy
just as for CRFs or HMMs. The second expectation
is the expectation of the feature under the model?s
joint distribution over all x, y pairs, and is harder to
calculate. Again assuming that sentences beyond a
certain length have negligible mass, we calculate the
expectation of the feature for each fixed length ` and
take a (truncated) weighted sum:
Ex,y|?fj =
K
?
`=1
p(|x| = `)Ex,y|`,?fj
For fixed `, we can calculate Ex,y|`,?fj using the lat-
tice of all inputs of length `. The quantity p(|x| = `)
is simply Z?`(?)/Z?(?).
As regularization, we use a diagonal Gaussian
prior with variance ?2 = 0.5, which gave relatively
good performance on all tasks.
5 Experiments
We experimented with prototype-driven learning in
three domains: English and Chinese part-of-speech
tagging and classified advertisement field segmenta-
tion. At inference time, we used maximum poste-
rior decoding,2 which we found to be uniformly but
slightly superior to Viterbi decoding.
5.1 English POS Tagging
For our English part-of-speech tagging experiments,
we used the WSJ portion of the English Penn tree-
bank (Marcus et al, 1994). We took our data to be
either the first 48K tokens (2000 sentences) or 193K
tokens (8000 sentences) starting from section 2. We
used a trigram tagger of the model form outlined in
section 4.1 with the same set of spelling features re-
ported in Smith and Eisner (2005): exact word type,
2At each position choosing the label which has the highest
posterior probability, obtained from the forward-backward al-
gorithm.
323
Label Prototype Label Prototype
NN % company year NNS years shares companies
JJ new other last VBG including being according
MD will would could -LRB- -LRB- -LCB-
VBP are ?re ?ve DT the a The
RB n?t also not WP$ whose
-RRB- -RRB- -RCB- FW bono del kanji
WRB when how where RP Up ON
IN of in for VBD said was had
SYM c b f $ $ US$ C$
CD million billion two # #
TO to To na : ? : ;
VBN been based compared NNPS Philippines Angels Rights
RBR Earlier duller ? ? ? non-?
VBZ is has says VB be take provide
JJS least largest biggest RBS Worst
NNP Mr. U.S. Corp. , ,
POS ?S CC and or But
PRP$ its their his JJR smaller greater larger
PDT Quite WP who what What
WDT which Whatever whatever . . ? !
EX There PRP it he they
? ? UH Oh Well Yeah
Figure 4: English POS prototype list
Correct Tag Predicted Tag % of Errors
CD DT 6.2
NN JJ 5.3
JJ NN 5.2
VBD VBN 3.3
NNS NN 3.2
Figure 5: Most common English POS confusions for
PROTO+SIM on 193K tokens
character suffixes of length up to 3, initial-capital,
contains-hyphen, and contains-digit. Our only edge
features were tag trigrams.
With just these features (our baseline BASE) the
problem is symmetric in the 45 model labels. In
order to break initial symmetry we initialized our
potentials to be near one, with some random noise.
To evaluate in this setting, model labels must be
mapped to target labels. We followed the common
approach in the literature, greedily mapping each
model label to a target label in order to maximize
per-position accuracy on the dataset. The results of
BASE, reported in table 1, depend upon random ini-
tialization; averaging over 10 runs gave an average
per-position accuracy of 41.3% on the larger training
set.
We automatically extracted the prototype list by
taking our data and selecting for each annotated la-
bel the top three occurring word types which were
not given another label more often. This resulted
in 116 prototypes for the 193K token setting.3 For
comparison, there are 18,423 word types occurring
in this data.
Incorporating the prototype list in the simplest
possible way, we fixed prototype occurrences in the
data to their respective annotation labels. In this
case, the model is no longer symmetric, and we
no longer require random initialization or post-hoc
mapping of labels. Adding prototypes in this way
gave an accuracy of 68.8% on all tokens, but only
47.7% on non-prototype occurrences, which is only
a marginal improvement over BASE. It appears as
though the prototype information is not spreading to
non-prototype words.
In order to remedy this, we incorporated distri-
butional similarity features. Similar to (Schu?tze,
1995), we collect for each word type a context vector
of the counts of the most frequent 500 words, con-
joined with a direction and distance (e.g +1,-2). We
then performed an SVD on the matrix to obtain a re-
duced rank approximation. We used the dot product
between left singular vectors as a measure of distri-
butional similarity. For each word w, we find the set
of prototype words with similarity exceeding a fixed
threshold of 0.35. For each of these prototypes z,
we add a predicate PROTO = z to each occurrence of
w. For example, we might add PROTO = said to each
token of reported (as in figure 3).4
Each prototype word is also its own prototype
(since a word has maximum similarity to itself), so
when we lock the prototype to a label, we are also
pushing all the words distributionally similar to that
prototype towards that label.5
3To be clear: this method of constructing a prototype list
required statistics from the labeled data. However, we believe
it to be a fair and necessary approach for several reasons. First,
we wanted our results to be repeatable. Second, we did not want
to overly tune this list, though experiments below suggest that
tuning could greatly reduce the error rate. Finally, it allowed us
to run on Chinese, where the authors have no expertise.
4Details of distributional similarity features: To extract con-
text vectors, we used a window of size 2 in either direction and
use the first 250 singular vectors. We collected counts from
all the WSJ portion of the Penn Treebank as well as the entire
BLIPP corpus. We limited each word to have similarity features
for its top 5 most similar prototypes.
5Note that the presence of a prototype feature does not en-
sure every instance of that word type will be given its proto-
type?s label; pressure from ?edge? features or other prototype
features can cause occurrences of a word type to be given differ-
ent labels. However, rare words with a single prototype feature
are almost always given that prototype?s label.
324
This setting, PROTO+SIM, brings the all-tokens
accuracy up to 80.5%, which is a 37.5% error re-
duction over PROTO. For non-prototypes, the accu-
racy increases to 67.8%, an error reduction of 38.4%
over PROTO. The overall error reduction from BASE
to PROTO+SIM on all-token accuracy is 66.7%.
Table 5 lists the most common confusions for
PROTO+SIM. The second, third, and fourth most
common confusions are characteristic of fully super-
vised taggers (though greater in number here) and
are difficult. For instance, both JJs and NNs tend to
occur after determiners and before nouns. The CD
and DT confusion is a result of our prototype list not
containing a contains-digit prototype for CD, so the
predicate fails to be linked to CDs. Of course in a
realistic, iterative design setting, we could have al-
tered the prototype list to include a contains-digit
prototype for CD and corrected this confusion.
Figure 6 shows the marginal posterior distribu-
tion over label pairs (roughly, the bigram transi-
tion matrix) according to the treebank labels and the
PROTO+SIM model run over the training set (using
a collapsed tag set for space). Note that the broad
structure is recovered to a reasonable degree.
It is difficult to compare our results to other sys-
tems which utilize a full or partial tagging dictio-
nary, since the amount of provided knowledge is
substantially different. The best comparison is to
Smith and Eisner (2005) who use a partial tagging
dictionary. In order to compare with their results,
we projected the tagset to the coarser set of 17 that
they used in their experiments. On 24K tokens, our
PROTO+SIM model scored 82.2%. When Smith and
Eisner (2005) limit their tagging dictionary to words
which occur at least twice, their best performing
neighborhood model achieves 79.5%. While these
numbers seem close, for comparison, their tagging
dictionary contained information about the allow-
able tags for 2,125 word types (out of 5,406 types)
and the their system must only choose, on average,
between 4.4 tags for a word. Our prototype list,
however, contains information about only 116 word
types and our tagger must on average choose be-
tween 16.9 tags, a much harder task. When Smith
and Eisner (2005) include tagging dictionary entries
for all words in the first half of their 24K tokens, giv-
ing tagging knowledge for 3,362 word types, they do
achieve a higher accuracy of 88.1%.
Setting Accuracy
BASE 46.4
PROTO 53.7
PROTO+SIM 71.5
PROTO+SIM+BOUND 74.1
Figure 7: Results on test set for ads data in
(Grenager et al, 2005).
5.2 Chinese POS Tagging
We also tested our POS induction system on the Chi-
nese POS data in the Chinese Treebank (Ircs, 2002).
The model is wholly unmodified from the English
version except that the suffix features are removed
since, in Chinese, suffixes are not a reliable indi-
cator of part-of-speech as in English (Tseng et al,
2005). Since we did not have access to a large aux-
iliary unlabeled corpus that was segmented, our dis-
tributional model was built only from the treebank
text, and the distributional similarities are presum-
ably degraded relative to the English. On 60K word
tokens, BASE gave an accuracy of 34.4, PROTO gave
39.0, and PROTO+SIM gave 57.4, similar in order if
not magnitude to the English case.
We believe the performance for Chinese POS tag-
ging is not as high as English for two reasons: the
general difficulty of Chinese POS tagging (Tseng et
al., 2005) and the lack of a larger segmented corpus
from which to build distributional models. Nonethe-
less, the addition of distributional similarity features
does reduce the error rate by 35% from BASE.
5.3 Information Field Segmentation
We tested our framework on the CLASSIFIEDS data
described in Grenager et al (2005) under conditions
similar to POS tagging. An important characteristic
of this domain (see figure 1(a)) is that the hidden la-
bels tend to be ?sticky,? in that fields tend to consist
of runs of the same label, as in figure 1(c), in con-
trast with part-of-speech tagging, where we rarely
see adjacent tokens given the same label. Grenager
et al (2005) report that in order to learn this ?sticky?
structure, they had to alter the structure of their
HMM so that a fixed mass is placed on each diag-
onal transition. In this work, we learned this struc-
ture automatically though prototype similarity fea-
tures without manually constraining the model (see
325
INPUNC
PRT
TO
VBN
LPUNC
W
DET
ADV
V
POS
ENDPUNC
VBG
PREP
ADJ
RPUNC
N
CONJ
IN
PU
N
C
PR
T
TO V
BN
LP
U
N
C
W D
ET
A
DV
V PO
S
EN
D
PU
N
C
V
BG
PR
EP
A
D
J
RP
U
N
C
N CO
N
J
INPUNC
PRT
TO
VBN
LPUNC
W
DET
ADV
V
POS
ENDPUNC
VBG
PREP
ADJ
RPUNC
N
CONJ
IN
PU
N
C
PR
T
TO V
BN
LP
U
N
C
W D
ET
A
DV
V PO
S
EN
D
PU
N
C
V
BG
PR
EP
A
D
J
RP
U
N
C
N CO
N
J
(a) (b)
Figure 6: English coarse POS tag structure: a) corresponds to ?correct? transition structure from labeled
data, b) corresponds to PROTO+SIM on 24K tokens
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
ROOMATES
UTILITIES
RESTRICTIONS
AVAILABLE
SIZE
PHOTOS
RENT
FEATURES
CONTACT
NEIGHBORHOOD
ADDRESS
(a) (b) (c)
Figure 8: Field segmentation observed transition structure: (a) labeled data, (b) BASE(c)
BASE+PROTO+SIM+BOUND (after post-processing)
figure 8), though we did change the similarity func-
tion (see below).
On the test set of (Grenager et al, 2005),
BASE scored an accuracy of 46.4%, comparable to
Grenager et al (2005)?s unsupervised HMM base-
line. Adding the prototype list (see figure 2) without
distributional features yielded a slightly improved
accuracy of 53.7%. For this domain, we utilized
a slightly different notion of distributional similar-
ity: we are not interested in the syntactic behavior
of a word type, but its topical content. Therefore,
when we collect context vectors for word types in
this domain, we make no distinction by direction
or distance and collect counts from a wider win-
dow. This notion of distributional similarity is more
similar to latent semantic indexing (Deerwester et
al., 1990). A natural consequence of this definition
of distributional similarity is that many neighboring
words will share the same prototypes. Therefore
distributional prototype features will encourage la-
bels to persist, naturally giving the ?sticky? effect
of the domain. Adding distributional similarity fea-
tures to our model (PROTO+SIM) improves accuracy
substantially, yielding 71.5%, a 38.4% error reduc-
tion over BASE.6
Another feature of this domain that Grenager et
al. (2005) take advantage of is that end of sen-
tence punctuation tends to indicate the end of a
field and the beginning of a new one. Grenager et
al. (2005) experiment with manually adding bound-
ary states and biasing transitions from these states
to not self-loop. We capture this ?boundary? ef-
fect by simply adding a line to our protoype-list,
adding a new BOUNDARY state (see figure 2) with
a few (hand-chosen) prototypes. Since we uti-
lize a trigram tagger, we are able to naturally cap-
ture the effect that the BOUNDARY tokens typically
indicate transitions between the fields before and
after the boundary token. As a post-processing
step, when a token is tagged as a BOUNDARY
6Distributional similarity details: We collect for each word
a context vector consisting of the counts for words occurring
within three token occurrences of a word. We perform a SVD
onto the first 50 singular vectors.
326
Correct Tag Predicted Tag % of Errors
FEATURES SIZE 11.2
FEATURES NBRHD 9.0
SIZE FEATURES 7.7
NBRHD FEATURES 6.4
ADDRESS NBRHD 5.3
UTILITIES FEATURES 5.3
Figure 9: Most common classified ads confusions
token it is given the same label as the previous
non-BOUNDARY token, which reflects the annota-
tional convention that boundary tokens are given the
same label as the field they terminate. Adding the
BOUNDARY label yields significant improvements,
as indicated by the PROTO+SIM+BOUND setting in
Table 5.3, surpassing the best unsupervised result
of Grenager et al (2005) which is 72.4%. Further-
more, our PROTO+SIM+BOUND model comes close
to the supervised HMM accuracy of 74.4% reported
in Grenager et al (2005).
We also compared our method to the most ba-
sic semi-supervised setting, where fully labeled doc-
uments are provided along with unlabeled ones.
Roughly 25% of the data had to be labeled
in order to achieve an accuracy equal to our
PROTO+SIM+BOUND model, suggesting that the use
of prior knowledge in the prototype system is partic-
ularly efficient.
In table 5.3, we provide the top confusions made
by our PROTO+SIM+BOUND model. As can be seen,
many of our confusions involve the FEATURE field,
which serves as a general purpose background state,
which often differs subtly from other fields such as
SIZE. For instance, the parenthical comment: ( mas-
ter has walk - in closet with vanity ) is labeled as
a SIZE field in the data, but our model proposed
it as a FEATURE field. NEIGHBORHOOD and AD-
DRESS is another natural confusion resulting from
the fact that the two fields share much of the same
vocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs.
[NBRHD near Telegraph]).
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by aMicrosoft / CITRIS grant and
by an equipment donation from Intel.
6 Conclusions
We have shown that distributional prototype features
can allow one to specify a target labeling scheme
in a compact and declarative way. These features
give substantial error reduction on several induction
tasks by allowing one to link words to prototypes ac-
cording to distributional similarity. Another positive
property of this approach is that it tries to reconcile
the success of sequence-free distributional methods
in unsupervised word clustering with the success of
sequence models in supervised settings: the similar-
ity guides the learning of the sequence model.
References
Alexander Clark. 2001. The unsupervised induction of stochas-
tic context-free grammars using distributional clustering. In
CoNLL.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,
George W. Furnas, and Richard A. Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the American
Society of Information Science, 41(6):391?407.
Trond Grenager, Dan Klein, and Christopher Manning. 2005.
Unsupervised learning of field segmentation models for in-
formation extraction. In Proceedings of the 43rd Meeting of
the ACL.
Nianwen Xue Ircs. 2002. Building a large-scale annotated chi-
nese corpus.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International Con-
ference on Machine Learning (ICML).
Dong C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Mathematical
Programming.
Christopher D. Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. The MIT
Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313?330.
Bernard Merialdo. 1991. Tagging english text with a proba-
bilistic model. In ICASSP, pages 809?812.
L.R Rabiner. 1989. A tutorial on hidden markov models and
selected applications in speech recognition. In IEEE.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Hinrich Schu?tze. 1995. Distributional part-of-speech tagging.
In EACL.
Noah Smith and Jason Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proceed-
ings of the 43rd Meeting of the ACL.
Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.
2005. Morphological features help pos tagging of unknown
words across language varieties. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Process-
ing.
327
Proceedings of NAACL HLT 2007, pages 404?411,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Improved Inference for Unlexicalized Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov,klein}@eecs.berkeley.edu
Abstract
We present several improvements to unlexicalized
parsing with hierarchically state-split PCFGs. First,
we present a novel coarse-to-fine method in which
a grammar?s own hierarchical projections are used
for incremental pruning, including a method for ef-
ficiently computing projections of a grammar with-
out a treebank. In our experiments, hierarchical
pruning greatly accelerates parsing with no loss in
empirical accuracy. Second, we compare various
inference procedures for state-split PCFGs from the
standpoint of risk minimization, paying particular
attention to their practical tradeoffs. Finally, we
present multilingual experiments which show that
parsing with hierarchical state-splitting is fast and
accurate in multiple languages and domains, even
without any language-specific tuning.
1 Introduction
Treebank parsing comprises two problems: learn-
ing, in which we must select a model given a tree-
bank, and inference, in which we must select a
parse for a sentence given the learned model. Pre-
vious work has shown that high-quality unlexical-
ized PCFGs can be learned from a treebank, either
by manual annotation (Klein and Manning, 2003)
or automatic state splitting (Matsuzaki et al, 2005;
Petrov et al, 2006). In particular, we demon-
strated in Petrov et al (2006) that a hierarchically
split PCFG could exceed the accuracy of lexical-
ized PCFGs (Collins, 1999; Charniak and Johnson,
2005). However, many questions about inference
with such split PCFGs remain open. In this work,
we present
1. an effective method for pruning in split PCFGs
2. a comparison of objective functions for infer-
ence in split PCFGs,
3. experiments on automatic splitting for lan-
guages other than English.
In Sec. 3, we present a novel coarse-to-fine pro-
cessing scheme for hierarchically split PCFGs. Our
method considers the splitting history of the final
grammar, projecting it onto its increasingly refined
prior stages. For any projection of a grammar, we
give a new method for efficiently estimating the pro-
jection?s parameters from the source PCFG itself
(rather than a treebank), using techniques for infi-
nite tree distributions (Corazza and Satta, 2006) and
iterated fixpoint equations. We then parse with each
refinement, in sequence, much along the lines of
Charniak et al (2006), except with much more com-
plex and automatically derived intermediate gram-
mars. Thresholds are automatically tuned on held-
out data, and the final system parses up to 100 times
faster than the baseline PCFG parser, with no loss in
test set accuracy.
In Sec. 4, we consider the well-known issue of
inference objectives in split PCFGs. As in many
model families (Steedman, 2000; Vijay-Shanker and
Joshi, 1985), split PCFGs have a derivation / parse
distinction. The split PCFG directly describes a gen-
erative model over derivations, but evaluation is sen-
sitive only to the coarser treebank symbols. While
the most probable parse problem is NP-complete
(Sima?an, 1992), several approximate methods exist,
including n-best reranking by parse likelihood, the
labeled bracket alorithm of Goodman (1996), and
a variational approximation introduced in Matsuzaki
et al (2005). We present experiments which explic-
itly minimize various evaluation risks over a can-
didate set using samples from the split PCFG, and
relate those conditions to the existing non-sampling
algorithms. We demonstrate that n-best reranking
according to likelihood is superior for exact match,
and that the non-reranking methods are superior for
maximizing F1. A specific contribution is to discuss
the role of unary productions, which previous work
has glossed over, but which is important in under-
standing why the various methods work as they do.
404
Finally, in Sec. 5, we learn state-split PCFGs for
German and Chinese and examine out-of-domain
performance for English. The learned grammars are
compact and parsing is very quick in our multi-stage
scheme. These grammars produce the highest test
set parsing figures that we are aware of in each lan-
guage, except for English for which non-local meth-
ods such as feature-based discriminative reranking
are available (Charniak and Johnson, 2005).
2 Hierarchically Split PCFGs
We consider PCFG grammars which are derived
from a raw treebank as in Petrov et al (2006): A
simple X-bar grammar is created by binarizing the
treebank trees. We refer to this grammar as G0.
From this starting point, we iteratively refine the
grammar in stages, as illustrated in Fig. 1. In each
stage, all symbols are split in two, for example DT
might become DT-1 and DT-2. The refined grammar
is estimated using a variant of the forward-backward
algorithm (Matsuzaki et al, 2005). After a split-
ting stage, many splits are rolled back based on (an
approximation to) their likelihood gain. This pro-
cedure gives an ontogeny of grammars Gi, where
G = Gn is the final grammar. Empirically, the
gains on the English Penn treebank level off after 6
rounds. In Petrov et al (2006), some simple smooth-
ing is also shown to be effective. It is interesting to
note that these grammars capture many of the ?struc-
tural zeros? described by Mohri and Roark (2006)
and pruning rules with probability below e?10 re-
duces the grammar size drastically without influenc-
ing parsing performance. Some of our methods and
conclusions are relevant to all state-split grammars,
such as Klein and Manning (2003) or Dreyer and
Eisner (2006), while others apply most directly to
the hierarchical case.
3 Search
When working with large grammars, it is standard to
prune the search space in some way. In the case of
lexicalized grammars, the unpruned chart often will
not even fit in memory for long sentences. Several
proven techniques exist. Collins (1999) combines a
punctuation rule which eliminates many spans en-
tirely, and then uses span-synchronous beams to
prune in a bottom-up fashion. Charniak et al (1998)
G0
G1
G2
G3
G4
G5
G6
X-bar =
G =
pi i
DT:
DT-1: DT-2:
the
that
this
this
0 1 2 3 4
That
5 6 7
some
some
8 9 10 11
these
12 13
the
the
the
14 15
The
16
a
a
17
Figure 1: Hierarchical refinement proceeds top-down while pro-
jection recovers coarser grammars. The top word for the first
refinements of the determiner tag (DT) is shown on the right.
introduces best-first parsing, in which a figure-of-
merit prioritizes agenda processing. Most relevant
to our work is Charniak and Johnson (2005) which
uses a pre-parse phase to rapidly parse with a very
coarse, unlexicalized treebank grammar. Any item
X:[i, j] with sufficiently low posterior probability in
the pre-parse triggers the pruning of its lexical vari-
ants in a subsequent full parse.
3.1 Coarse-to-Fine Approaches
Charniak et al (2006) introduces multi-level coarse-
to-fine parsing, which extends the basic pre-parsing
idea by adding more rounds of pruning. In their
work, the extra pruning was with grammars even
coarser than the raw treebank grammar, such as
a grammar in which all nonterminals are col-
lapsed. We propose a novel multi-stage coarse-to-
fine method which is particularly natural for our hi-
erarchically split grammar, but which is, in princi-
ple, applicable to any grammar. As in Charniak et
al. (2006), we construct a sequence of increasingly
refined grammars, reparsing with each refinement.
The contributions of our method are that we derive
sequences of refinements in a new way (Sec. 3.2),
we consider refinements which are themselves com-
plex, and, because our full grammar is not impossi-
ble to parse with, we automatically tune the pruning
thresholds on held-out data.
3.2 Projection
In our method, which we call hierarchical coarse-
to-fine parsing, we consider a sequence of PCFGs
G0, G1, . . . Gn = G, where each Gi is a refinement
of the preceding grammar Gi?1 and G is the full
grammar of interest. Each grammar Gi is related to
G = Gn by a projection pin?i or pii for brevity. A
405
projection is a map from the non-terminal (including
pre-terminal) symbols of G onto a reduced domain.
A projection of grammar symbols induces a pro-
jection of rules and therefore entire non-weighted
grammars (see Fig. 1).
In our case, we also require the projections to be
sequentially compatible, so that pii?j =pik?j?pii?k.
That is, each projection is itself a coarsening of the
previous projections. In particular, we take the pro-
jection pii?j to be the map that collapses split sym-
bols in round i to their earlier identities in round j.
It is straightforward to take a projection pi and
map a CFG G to its induced projection pi(G). What
is less obvious is how the probabilities associated
with the rules of G should be mapped. In the case
where pi(G) is more coarse than the treebank orig-
inally used to train G, and when that treebank is
available, it is easy to project the treebank and di-
rectly estimate, say, the maximum-likelihood pa-
rameters for pi(G). This is the approach taken by
Charniak et al (2006), where they estimate what in
our terms are projections of the raw treebank gram-
mar from the treebank itself.
However, treebank estimation has several limita-
tions. First, the treebank used to train G may not
be available. Second, if the grammar G is heavily
smoothed or otherwise regularized, its own distri-
bution over trees may be far from that of the tree-
bank. Third, the meanings of the split states can and
do drift between splitting stages. Fourth, and most
importantly, we may wish to project grammars for
which treebank estimation is problematic, for exam-
ple, grammars which are more refined than the ob-
served treebank grammars. Our method effectively
avoids all of these problems by rebuilding and refit-
ting the pruning grammars on the fly from the final
grammar.
3.2.1 Estimating Projected Grammars
Fortunately, there is a well worked-out notion of
estimating a grammar from an infinite distribution
over trees (Corazza and Satta, 2006). In particular,
we can estimate parameters for a projected grammar
pi(G) from the tree distribution induced by G (which
can itself be estimated in any manner). The earli-
est work that we are aware of on estimating models
from models in this way is that of Nederhof (2005),
who considers the case of learning language mod-
els from other language models. Corazza and Satta
(2006) extend these methods to the case of PCFGs
and tree distributions.
The generalization of maximum likelihood esti-
mation is to find the estimates for pi(G) with min-
imum KL divergence from the tree distribution in-
duced by G. Since pi(G) is a grammar over coarser
symbols, we fit pi(G) to the distribution G induces
over pi-projected trees: P (pi(T )|G). The proofs
of the general case are given in Corazza and Satta
(2006), but the resulting procedure is quite intuitive.
Given a (fully observed) treebank, the maximum-
likelihood estimate for the probability of a rule X ?
Y Z would simply be the ratio of the count of X to
the count of the configuration X ? Y Z . If we wish
to find the estimate which has minimum divergence
to an infinite distribution P (T ), we use the same for-
mula, but the counts become expected counts:
P (X ? Y Z) =
EP (T )[X ? Y Z]
EP (T )[X]
with unaries estimated similarly. In our specific
case, X,Y, and Z are symbols in pi(G), and the
expectations are taken over G?s distribution of pi-
projected trees, P (pi(T )|G). We give two practical
methods for obtaining these expectations below.
3.2.2 Calculating Projected Expectations
Concretely, we can now estimate the minimum
divergence parameters of pi(G) for any projection
pi and PCFG G if we can calculate the expecta-
tions of the projected symbols and rules according to
P (pi(T )|G). The simplest option is to sample trees
T from G, project the samples, and take average
counts off of these samples. In the limit, the counts
will converge to the desired expectations, provided
the grammar is proper. However, we can exploit the
structure of our projections to obtain the desired ex-
pectations much more simply and efficiently.
First, consider the problem of calculating the ex-
pected counts of a symbol X in a tree distribution
given by a grammar G, ignoring the issue of projec-
tion. These expected counts obey the following one-
step equations (assuming a unique root symbol):
c(root) = 1
c(X) =
?
Y??X?
P (?X?|Y )c(Y )
406
Here, ?, ?, or both can be empty, and a rule X ? ?
appears in the sum once for each X it contains. In
principle, this linear system can be solved in any
way.1 In our experiments, we solve this system it-
eratively, with the following recurrences:
c0(X)?
{
1 if X = root
0 otherwise
ci+1(X)?
?
Y??X?
P (?X?|Y )ci(Y )
Note that, as in other iterative fixpoint methods, such
as policy evaluation for Markov decision processes
(Sutton and Barto, 1998), the quantities ck(X) have
a useful interpretation as the expected counts ignor-
ing nodes deeper than depth k (i.e. the roots are all
the root symbol, so c0(root) = 1). In our experi-
ments this method converged within around 25 iter-
ations; this is unsurprising, since the treebank con-
tains few nodes deeper than 25 and our base gram-
mar G seems to have captured this property.
Once we have the expected counts of symbols
in G, the expected counts of their projections
X ? = pi(X) according to P (pi(T )|G) are given by
c(X ?) = ?X:pi(X)=X? c(X). Rules can be esti-
mated directly using similar recurrences, or given by
one-step equations:
c(X ? ?) = c(X)P (?|X)
This process very rapidly computes the estimates
for a projection of a grammar (i.e. in a few seconds
for our largest grammars), and is done once during
initialization of the parser.
3.2.3 Hierarchical Projections
Recall that our final state-split grammars G come,
by their construction process, with an ontogeny of
grammars Gi where each grammar is a (partial)
splitting of the preceding one. This gives us a nat-
ural chain of projections pii?j which projects back-
wards along this ontogeny of grammars (see Fig. 1).
Of course, training also gives us parameters for
the grammars, but only the chain of projections is
needed. Note that the projected estimates need not
1Whether or not the system has solutions depends on the
parameters of the grammar. In particular, G may be improper,
though the results of Chi (1999) imply that G will be proper if
it is the maximum-likelihood estimate of a finite treebank.
(and in general will not) recover the original param-
eters exactly, nor would we want them to. Instead
they take into account any smoothing, substate drift,
and so on which occurred by the final grammar.
Starting from the base grammar, we run the pro-
jection process for each stage in the sequence, cal-
culating pii (chained incremental projections would
also be possible). For the remainder of the paper,
except where noted otherwise, all coarser grammars?
estimates are these reconstructions, rather than those
originally learned.
3.3 Experiments
As demonstrated by Charniak et al (2006) parsing
times can be greatly reduced by pruning chart items
that have low posterior probability under a simpler
grammar. Charniak et al (2006) pre-parse with a se-
quence of grammars which are coarser than (parent-
annotated) treebank grammars. However, we also
work with grammars which are already heavily split,
up to half as split as the final grammar, because we
found the computational cost for parsing with the
simple X-bar grammar to be insignificant compared
to the costs for parsing with more refined grammars.
For a final grammar G = Gn, we compute esti-
mates for the n projections Gn?1, . . . , G0 =X-Bar,
where Gi = pii(G) as described in the previous sec-
tion. Additionally we project to a grammar G?1 in
which all nonterminals, except for the preterminals,
have been collapsed. During parsing, we start of
by exhaustively computing the inside/outside scores
with G?1. At each stage, chart items with low poste-
rior probability are removed from the chart, and we
proceed to compute inside/outside scores with the
next, more refined grammar, using the projections
pii?i?1 to map between symbols in Gi and Gi?1. In
each pass, we skip chart items whose projection into
the previous stage had a probability below a stage-
specific threshold, until we reach G = Gn (after
seven passes in our case). For G, we do not prune
but instead return the minimum risk tree, as will be
described in Sec. 4.
Fig. 2 shows the (unlabeled) bracket posteriors af-
ter each pass and demonstrates that most construc-
tions can be ruled out by the simpler grammars,
greatly reducing the amount of computation for the
following passes. The pruning thresholds were em-
pirically determined on a held out set by computing
407
In
flu
en
tia
l
m
e
m
be
rs of th
e
H
ou
se
W
ay
s
a
n
d
M
ea
ns
Co
m
m
itt
ee
in
tro
du
ce
d
le
gi
sla
tio
n
th
at
w
o
u
ld
re
st
ric
t
ho
w
th
e
n
e
w
s&
l
ba
ilo
ut
a
ge
nc
y
ca
n
ra
is
e
ca
pi
ta
l ;
cr
e
a
tin
g
a
n
o
th
er
po
te
nt
ia
l
o
bs
ta
cle to th
e
go
ve
rn
m
en
t
?s
sa
le of
si
ck
th
rif
ts .
G?1 G0=X-bar G1
G2 G3 G4
G5
(G6=G)
Output
Figure 2: Bracket posterior probabilities (black = high) for the
first sentence of our development set during coarse-to-fine
pruning. Note that we compute the bracket posteriors at a much
finer level but are showing the unlabeled posteriors for illustra-
tion purposes. No pruning is done at the finest level (G6 = G)
but the minimum risk tree is returned instead.
the most likely tree under G directly (without prun-
ing) and then setting the highest pruning threshold
for each stage that would not prune the optimal tree.
This setting also caused no search errors on the test
set. We found our projected grammar estimates to be
at least equally well suited for pruning as the orig-
inal grammar estimates which were learned during
the hierarchical training. Tab. 1 shows the tremen-
dous reduction in parsing time (all times are cumu-
lative) and gives an overview over grammar sizes
and parsing accuracies. In particular, in our Java im-
plementation on a 3GHz processor, it is possible to
parse the 1578 development set sentences (of length
40 or less) in less than 1200 seconds with an F1 of
91.2% (no search errors), or, by pruning more, in
680 seconds at 91.1%. For comparison, the Feb.
2006 release of the Charniak and Johnson (2005)
parser runs in 1150 seconds on the same machine
with an F1 of 90.7%.
4 Objective Functions for Parsing
A split PCFG is a grammar G over symbols of the
form X-k where X is an evaluation symbol (such
as NP) and k is some indicator of a subcategory,
such as a parent annotation. G induces a deriva-
tion distribution P (T |G) over trees T labeled with
split symbols. This distribution in turn induces
a parse distribution P (T ?|G) = P (pi(T )|G) over
(projected) trees with unsplit evaluation symbols,
where P (T ?|G) = ?T :T ?=pi(T ) P (T |G). We now
have several choices of how to select a tree given
these posterior distributions over trees. In this sec-
tion, we present experiments with the various op-
tions and explicitly relate them to parse risk mini-
mization (Titov and Henderson, 2006).
G0 G2 G4 G6
Nonterminals 98 219 498 1140
Rules 3,700 19,600 126,100 531,200
No pruning 52 min 99 min 288 min 1612 min
X-bar pruning 8 min 14 min 30 min 111 min
C-to-F (no loss) 6 min 12 min 16 min 20 min
F1 for above 64.8 85.2 89.7 91.2
C-to-F (lossy) 6 min 8 min 9 min 11 min
F1 for above 64.3 84.7 89.4 91.1
Table 1: Grammar sizes, parsing times and accuracies for hier-
archically split PCFGs with and without hierarchical coarse-to-
fine parsing on our development set (1578 sentences with 40 or
less words from section 22 of the Penn Treebank). For compar-
ison the parser of Charniak and Johnson (2005) has an accuracy
of F1=90.7 and runs in 19 min on this set.
The decision-theoretic approach to parsing would
be to select the parse tree which minimizes our ex-
pected loss according to our beliefs:
T ?P = argmin
TP
?
TT
P (TT |w,G)L(TP , TT )
where TT and TP are ?true? and predicted parse
trees. Here, our loss is described by the function L
whose first argument is the predicted parse tree and
the second is the gold parse tree. Reasonable can-
didates for L include zero-one loss (exact match),
precision, recall, F1 (specifically EVALB here), and
so on. Of course, the naive version of this process is
intractable: we have to loop over all (pairs of) pos-
sible parses. Additionally, it requires parse likeli-
hoods P (TP |w,G), which are tractable, but not triv-
ial, to compute for split models. There are two op-
tions: limit the predictions to a small candidate set or
choose methods for which dynamic programs exist.
For arbitrary loss functions, we can approximate
the minimum-risk procedure by taking the min over
only a set of candidate parses TP . In some cases,
each parse?s expected risk can be evaluated in closed
408
Rule score: r(A? B C, i, k, j) =
?
x
?
y
?
z
POUT(Ax, i, j)P(Ax ? By Cz)PIN(By, i, k)PIN(Cy, k, j)
VARIATIONAL: q(A? B C, i, k, j) = r(A? B C, i, k, j)P
x POUT(Ax,i,j)PIN(Ax,i,j)
TG = argmaxT
?
e?T q(e)
MAX-RULE-SUM: q(A? B C, i, k, j) = r(A? B C, i, k, j)PIN(root,0,n) TG = argmaxT
?
e?T q(e)
MAX-RULE-PRODUCT: q(A? B C, i, k, j) = r(A? B C, i, k, j)PIN(root,0,n) TG = argmaxT
?
e?T q(e)
Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z
are latent annotations and i, j, k are between-word indices. Hence (Ax, i, j) denotes a constituent labeled with Ax spanning from
i to j. Furthermore, we write e = (A? B C, i, j, k) for brevity.
form. Exact match (likelihood) has this property. In
general, however, we can approximate the expecta-
tion with samples from P (T |w,G). The method for
sampling derivations of a PCFG is given in Finkel
et al (2006) and Johnson et al (2007). It requires a
single inside-outside computation per sentence and
is then efficient per sample. Note that for split gram-
mars, a posterior parse sample can be drawn by sam-
pling a derivation and projecting away the substates.
Fig. 2 shows the results of the following exper-
iment. We constructed 10-best lists from the full
grammar G in Sec. 2 using the parser of Petrov et
al. (2006). We then took the same grammar and ex-
tracted 500-sample lists using the method of Finkel
et al (2006). The minimum risk parse candidate was
selected for various loss functions. As can be seen,
in most cases, risk minimization reduces test-set loss
of the relevant quantity. Exact match is problematic,
however, because 500 samples is often too few to
draw a match when a sentence has a very flat poste-
rior, and so there are many all-way ties.2 Since ex-
act match permits a non-sampled calculation of the
expected risk, we show this option as well, which
is substantially superior. This experiment highlights
that the correct procedure for exact match is to find
the most probable parse.
An alternative approach to reranking candidate
parses is to work with inference criteria which ad-
mit dynamic programming solutions. Fig. 3 shows
three possible objective functions which use the eas-
ily obtained posterior marginals of the parse tree dis-
tribution. Interestingly, while they have fairly differ-
ent decision theoretic motivations, their closed-form
solutions are similar.
25,000 samples do not improve the numbers appreciably.
One option is to maximize likelihood in an ap-
proximate distribution. Matsuzaki et al (2005)
present a VARIATIONAL approach, which approxi-
mates the true posterior over parses by a cruder, but
tractable sentence-specific one. In this approximate
distribution there is no derivation / parse distinction
and one can therefore optimize exact match by se-
lecting the most likely derivation.
Instead of approximating the tree distribution we
can use an objective function that decomposes along
parse posteriors. The labeled brackets algorithm of
Goodman (1996) has such an objective function. In
its original formulation this algorithm maximizes
the number of expected correct nodes, but instead
we can use it to maximize the number of correct
rules (the MAX-RULE-SUM algorithm). A worry-
ing issue with this method is that it is ill-defined for
grammars which allow infinite unary chains: there
will be no finite minimum risk tree under recall loss
(you can always reduce the risk by adding one more
cycle). We implement MAX-RULE-SUM in a CNF-
like grammar family where above each binary split
is exactly one unary (possibly a self-loop). With
this limitation, unary chains are not a problem. As
might be expected, this criterion improves bracket
measures at the expense of exact match.
We found it optimal to use a third approach,
in which rule posteriors are multiplied instead of
added. This corresponds to choosing the tree with
greatest chance of having all rules correct, under
the (incorrect) assumption that the rules correct-
ness are independent. This MAX-RULE-PRODUCT
algorithm does not need special treatment of infi-
nite unary chains because it is optimizing a product
rather than a sum. While these three methods yield
409
Objective P R F1 EX
BEST DERIVATION
Viterbi Derivation 89.6 89.4 89.5 37.4
RERANKING
Random 87.6 87.7 87.7 16.4
Precision (sampled) 91.1 88.1 89.6 21.4
Recall (sampled) 88.2 91.3 89.7 21.5
F1 (sampled) 90.2 89.3 89.8 27.2
Exact (sampled) 89.5 89.5 89.5 25.8
Exact (non-sampled) 90.8 90.8 90.8 41.7
Exact/F1 (oracle) 95.3 94.4 95.0 63.9
DYNAMIC PROGRAMMING
VARIATIONAL 90.7 90.9 90.8 41.4
MAX-RULE-SUM 90.5 91.3 90.9 40.4
MAX-RULE-PRODUCT 91.2 91.1 91.2 41.4
Table 2: A 10-best list from our best G can be reordered as to
maximize a given objective either using samples or, under some
restricting assumptions, in closed form.
very similar results (see Fig. 2), the MAX-RULE-
PRODUCT algorithm consistently outperformed the
other two.
Overall, the closed-form options were superior to
the reranking ones, except on exact match, where the
gains from correctly calculating the risk outweigh
the losses from the truncation of the candidate set.
5 Multilingual Parsing
Most research on parsing has focused on English
and parsing performance on other languages is gen-
erally significantly lower.3 Recently, there have
been some attempts to adapt parsers developed for
English to other languages (Levy and Manning,
2003; Cowan and Collins, 2005). Adapting lexi-
calized parsers to other languages in not a trivial
task as it requires at least the specification of head
rules, and has had limited success. Adapting unlexi-
calized parsers appears to be equally difficult: Levy
and Manning (2003) adapt the unlexicalized parser
of Klein and Manning (2003) to Chinese, but even
after significant efforts on choosing category splits,
only modest performance gains are reported.
In contrast, automatically learned grammars like
the one of Matsuzaki et al (2005) and Petrov et al
(2006) require a treebank for training but no addi-
tional human input. One has therefore reason to
3Of course, cross-linguistic comparison of results is com-
plicated by differences in corpus annotation schemes and sizes,
and differences in linguistic characteristics.
ENGLISH GERMAN CHINESE
(Marcus et al, 1993) (Skut et al, 1997) (Xue et al, 2002)
TrainSet Section 2-21 Sentences 1-18,602 Articles 26-270
DevSet Section 22 18,603-19,602 Articles 1-25
TestSet Section 23 19,603-20,602 Articles 271-300
Table 3: Experimental setup.
believe that their performance will generalize bet-
ter across languages than the performance of parsers
that have been hand tailored to English.
5.1 Experiments
We trained models for English, Chinese and Ger-
man using the standard corpora and splits as shown
in Tab. 3. We applied our model directly to each
of the treebanks, without any language dependent
modifications. Specifically, the same model hyper-
parameters (merging percentage and smoothing fac-
tor) were used in all experiments.
Tab. 4 shows that automatically inducing latent
structure is a technique that generalizes well across
language boundaries and results in state of the art
performance for Chinese and German. On English,
the parser is outperformed only by the reranking
parser of Charniak and Johnson (2005), which has
access to a variety of features which cannot be cap-
tured by a generative model.
Space does not permit a thorough exposition of
our analysis, but as in the case of English (Petrov
et al, 2006), the learned subcategories exhibit inter-
esting linguistic interpretations. In German, for ex-
ample, the model learns subcategories for different
cases and genders.
5.2 Corpus Variation
Related to cross language generalization is the gen-
eralization across domains for the same language.
It is well known that a model trained on the Wall
Street Journal loses significantly in performance
when evaluated on the Brown Corpus (see Gildea
(2001) for more details and the exact setup of their
experiment, which we duplicated here). Recently
McClosky et al (2006) came to the conclusion that
this performance drop is not due to overfitting the
WSJ data. Fig. 4 shows the performance on the
Brown corpus during hierarchical training. While
the F1 score on the WSJ is rising we observe a drop
in performance after the 5th iteration, suggesting
that some overfitting is occurring.
410
? 40 words all
Parser LP LR LP LR
ENGLISH
Charniak et al (2005) 90.1 90.1 89.5 89.6
Petrov et al (2006) 90.3 90.0 89.8 89.6
This Paper 90.7 90.5 90.2 89.9
ENGLISH (reranked)
Charniak et al (2005)4 92.4 91.6 91.8 91.0
GERMAN
Dubey (2005) F1 76.3 -
This Paper 80.8 80.7 80.1 80.1
CHINESE5
Chiang et al (2002) 81.1 78.8 78.0 75.2
This Paper 80.8 80.7 78.8 78.5
Table 4: Our final test set parsing performance compared to the
best previous work on English, German and Chinese.
78
80
82
84
86
Grammar Size
F 1
Hierarchically Split PCFGs
Charniak and Johnson (2005) generative parser
Charniak and Johnson (2005) reranking parser
G3
G5 G6G4
Figure 4: Parsing accuracy starts dropping after 5 training iter-
ations on the Brown corpus, while it is improving on the WSJ,
indicating overfitting.
6 Conclusions
The coarse-to-fine scheme presented here, in con-
junction with the risk-appropriate parse selection
methodology, allows fast, accurate parsing, in multi-
ple languages and domains. For training, one needs
only a raw context-free treebank and for decoding
one needs only a final grammar, along with coars-
ening maps. The final parser is publicly available at
http://www.nlp.cs.berkeley.edu.
Acknowledgments We would like to thank Eu-
gene Charniak, Mark Johnson and Noah Smith for
helpful discussions and comments.
References
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-Best
Parsing and MaxEnt Discriminative Reranking. In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based
best-first chart parsing. 6th Wkshop on Very Large Corpora.
4This is the performance of the updated reranking parser
available at http://www.cog.brown.edu/mj/software.htm
5Sun and Jurafsky (2004) report even better performance on
this dataset but since they assume gold POS tags their work is
not directly comparable (p.c.).
E. Charniak, M. Johnson, et al 2006. Multi-level coarse-to-fine
PCFG Parsing. In HLT-NAACL ?06.
Z. Chi. 1999. Statistical properties of probabilistic context-free
grammars. In Computational Linguistics.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, U. of Pennsylvania.
A. Corazza and G. Satta. 2006. Cross-entropy and estimation
of probabilistic context-free grammars. In HLT-NAACL ?06.
B. Cowan and M. Collins. 2005. Morphology and reranking
for the statistical parsing of Spanish. In HLT-EMNLP ?05.
M. Dreyer and J. Eisner. 2006. Better informed training of
latent syntactic features. In EMNLP ?06, pages 317?326.
A. Dubey. 2005. What to do when lexicalization fails: parsing
German with suffix analysis and smoothing. In ACL ?05.
J. Finkel, C. Manning, and A. Ng. 2006. Solving the prob-
lem of cascading errors: approximate Bayesian inference for
lingusitic annotation pipelines. In EMNLP ?06.
D. Gildea. 2001. Corpus variation and parser performance.
EMNLP ?01, pages 167?202.
J. Goodman. 1996. Parsing algorithms and metrics. ACL ?96.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov Chain Monte Carlo. In
HLT-NAACL ?07.
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL ?03, pages 423?430.
R. Levy and C. Manning. 2003. Is it harder to parse Chinese,
or the Chinese treebank? In ACL ?03, pages 439?446.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG
with latent annotations. In ACL ?05, pages 75?82.
D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking
and self-training for parser adaptation. In COLING-ACL?06.
M. Mohri and B. Roark. 2006. Probabilistic context-free gram-
mar induction based on structural zeros. In HLT-NAACL ?06.
M.-J. Nederhof. 2005. A general technique to train language
models on language models. In Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL ?06, pages 443?440.
K. Sima?an. 1992. Computatoinal complexity of probabilistic
disambiguation. Grammars, 5:125?151.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997. An anno-
tation scheme for free word order languages. In Conference
on Applied Natural Language Processing.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, Massachusetts.
H. Sun and D. Jurafsky. 2004. Shallow semantic parsing of
Chinese. In HLT-NAACL ?04, pages 249?256.
R. Sutton and A. Barto. 1998. Reinforcement Learning: An
Introduction. MIT Press.
I. Titov and J. Henderson. 2006. Loss minimization in parse
reranking. In EMNLP ?06, pages 560?567.
K. Vijay-Shanker and A. Joshi. 1985. Some computational
properties of Tree Adjoining Grammars. In ACL ?85.
N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building a large
scale annotated Chinese corpus. In COLING ?02.
411
Proceedings of NAACL HLT 2007, pages 412?419,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Approximate Factoring for A? Search
Aria Haghighi, John DeNero, Dan Klein
Computer Science Division
University of California Berkeley
{aria42, denero, klein}@cs.berkeley.edu
Abstract
We present a novel method for creating A? esti-
mates for structured search problems. In our ap-
proach, we project a complex model onto multiple
simpler models for which exact inference is effi-
cient. We use an optimization framework to es-
timate parameters for these projections in a way
which bounds the true costs. Similar to Klein and
Manning (2003), we then combine completion es-
timates from the simpler models to guide search
in the original complex model. We apply our ap-
proach to bitext parsing and lexicalized parsing,
demonstrating its effectiveness in these domains.
1 Introduction
Inference tasks in NLP often involve searching for
an optimal output from a large set of structured out-
puts. For many complex models, selecting the high-
est scoring output for a given observation is slow or
even intractable. One general technique to increase
efficiency while preserving optimality is A? search
(Hart et al, 1968); however, successfully using A?
search is challenging in practice. The design of ad-
missible (or nearly admissible) heuristics which are
both effective (close to actual completion costs) and
also efficient to compute is a difficult, open prob-
lem in most domains. As a result, most work on
search has focused on non-optimal methods, such
as beam search or pruning based on approximate
models (Collins, 1999), though in certain cases ad-
missible heuristics are known (Och and Ney, 2000;
Zhang and Gildea, 2006). For example, Klein and
Manning (2003) show a class of projection-based A?
estimates, but their application is limited to models
which have a very restrictive kind of score decom-
position. In this work, we broaden their projection-
based technique to give A? estimates for models
which do not factor in this restricted way.
Like Klein and Manning (2003), we focus on
search problems where there are multiple projec-
tions or ?views? of the structure, for example lexical
parsing, in which trees can be projected onto either
their CFG backbone or their lexical attachments. We
use general optimization techniques (Boyd and Van-
denberghe, 2005) to approximately factor a model
over these projections. Solutions to the projected
problems yield heuristics for the original model.
This approach is flexible, providing either admissi-
ble or nearly admissible heuristics, depending on the
details of the optimization problem solved. Further-
more, our approach allows a modeler explicit control
over the trade-off between the tightness of a heuris-
tic and its degree of inadmissibility (if any). We de-
scribe our technique in general and then apply it to
two concrete NLP search tasks: bitext parsing and
lexicalized monolingual parsing.
2 General Approach
Many inference problems in NLP can be solved
with agenda-based methods, in which we incremen-
tally build hypotheses for larger items by combining
smaller ones with some local configurational struc-
ture. We can formalize such tasks as graph search
problems, where states encapsulate partial hypothe-
ses and edges combine or extend them locally.1 For
example, in HMM decoding, the states are anchored
labels, e.g. VBD[5], and edges correspond to hidden
transitions, e.g. VBD[5] ? DT[6].
The search problem is to find a minimal cost path
from the start state to a goal state, where the path
cost is the sum of the costs of the edges in the path.
1In most complex tasks, we will in fact have a hypergraph,
but the extension is trivial and not worth the added notation.
412
( a
a?
)
?
( b
b?
)
( b
a?
)
?
( c
b?
)
1
( a
a?
)
?
( b
b?
)
( b
b?
)
?
( c
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
Local Configurations
a' ? b' b'  c'
a ? b
b ? c
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 4.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 5.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
(a) (b) (c)
Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row and
column of the cell). In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration. The
bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the
actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along
two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost.
Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.
For probabilistic inference problems, the cost of an
edge is typically a negative log probability which de-
pends only on some local configuration type. For
instance, in PCFG parsing, the (hyper)edges refer-
ence anchored spans X[i, j], but the edge costs de-
pend only on the local rule type X ? Y Z. We will
use a to refer to a local configuration and use c(a)
to refer to its cost. Because edge costs are sensi-
tive only to local configurations, the cost of a path
is
?
a c(a). A? search requires a heuristic function,
which is an estimate h(s) of the completion cost, the
cost of a best path from state s to a goal.
In this work, following Klein and Manning
(2003), we consider problems with projections or
?views,? which define mappings to simpler state and
configuration spaces. For instance, suppose that we
are using an HMM to jointly model part-of-speech
(POS) and named-entity-recognition (NER) tagging.
There might be one projection onto the NER com-
ponent and another onto the POS component. For-
mally, a projection pi is a mapping from states to
some coarser domain. A state projection induces
projections of edges and of the entire graph pi(G).
We are particularly interested in search problems
with multiple projections {pi1, . . . , pi`} where each
projection, pii, has the following properties: its state
projections induce well-defined projections of the
local configurations pii(a) used for scoring, and the
projected search problem admits a simpler infer-
ence. For instance, the POS projection in our NER-
POS HMM is a simpler HMM, though the gains
from this method are greater when inference in the
projections have lower asymptotic complexity than
the original problem (see sections 3 and 4).
In defining projections, we have not yet dealt with
the projected scoring function. Suppose that the
cost of local configurations decomposes along pro-
jections as well. In this case,
c (a) =
?`
i=1
ci(a) , ?a ? A (1)
where A is the set of local configurations and ci(a)
represents the cost of configuration a under projec-
tion pii. A toy example of such a cost decomposi-
tion in the context of a Markov process over two-part
states is shown in figure 1(b), where the costs of the
joint transitions equal the sum of costs of their pro-
jections. Under the strong assumption of equation
(1), Klein and Manning (2003) give an admissible
A? bound. They note that the cost of a path decom-
poses as a sum of projected path costs. Hence, the
following is an admissible additive heuristic (Felner
et al, 2004),
h(s) =
?`
i=1
h?i (s) (2)
where h?i (s) denote the optimal completion costs in
the projected search graph pii(G). That is, the com-
pletion cost of a state bounds the sum of the comple-
tion costs in each projection.
In virtually all cases, however, configuration costs
will not decompose over projections, nor would we
expect them to. For instance, in our joint POS-NER
task, this assumption requires that the POS and NER
413
transitions and observations be generated indepen-
dently. This independence assumption undermines
the motivation for assuming a joint model. In the
central contribution of this work, we exploit the pro-
jection structure of our search problem without mak-
ing any assumption about cost decomposition.
Rather than assuming decomposition, we propose
to find scores ? for the projected configurations
which are pointwise admissible:
?`
i=1
?i(a) ? c(a), ?a ? A (3)
Here, ?i(a) represents a factored projection cost of
pii(a), the pii projection of configuration a. Given
pointwise admissible ?i?s we can again apply the
heuristic recipe of equation (2). An example of
factored projection costs are shown in figure 1(c),
where no exact decomposition exists, but a point-
wise admissible lower bound is easy to find.
Claim. If a set of factored projection costs
{?1, . . . , ?`} satisfy pointwise admissibility, then
the heuristic from (2) is an admissible A? heuristic.
Proof. Assume a1, . . . , ak are configurations used
to optimally reach the goal from state s. Then,
h?(s) =
kX
j=1
c(aj) ?
kX
j=1
X`
i=1
?i(aj)
=
X`
i=1
 
kX
j=1
?i(aj)
!
?
X`
i=1
h?i (s) = h(s)
The first inequality follows from pointwise admis-
sibility. The second inequality follows because each
inner sum is a completion cost for projected problem
pii and therefore h?i (s) lower bounds it. Intuitively,
we can see two sources of slack in such projection
heuristics. First, there may be slack in the pointwise
admissible scores. Second, the best paths in the pro-
jections will be overly optimistic because they have
been decoupled (see figure 5 for an example of de-
coupled best paths in projections).
2.1 Finding Factored Projections for
Non-Factored Costs
We can find factored costs ?i(a) which are point-
wise admissible by solving an optimization problem.
We think of our unknown factored costs as a block
vector ? = [?1, .., ?`], where vector ?i is composed
of the factored costs, ?i(a), for each configuration
a ? A. We can then find admissible factored costs
by solving the following optimization problem,
minimize
?
??? (4)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
?a ? 0, ?a ? A
We can think of each ?a as the amount by which
the cost of configuration a exceeds the factored pro-
jection estimates (the pointwise A? gap). Requiring
?a ? 0 insures pointwise admissibility. Minimiz-
ing the norm of the ?a variables encourages tighter
bounds; indeed if ??? = 0, the solution corresponds
to an exact factoring of the search problem. In the
case where we minimize the 1-norm or ?-norm, the
problem above reduces to a linear program, which
can be solved efficiently for a large number of vari-
ables and constraints.2
Viewing our procedure decision-theoretically, by
minimizing the norm of the pointwise gaps we are
effectively choosing a loss function which decom-
poses along configuration types and takes the form
of the norm (i.e. linear or squared losses). A com-
plete investigation of the alternatives is beyond the
scope of this work, but it is worth pointing out that
in the end we will care only about the gap on entire
structures, not configurations, and individual config-
uration factored costs need not even be pointwise ad-
missible for the overall heuristic to be admissible.
Notice that the number of constraints is |A|, the
number of possible local configurations. For many
search problems, enumerating the possible configu-
rations is not feasible, and therefore neither is solv-
ing an optimization problem with all of these con-
straints. We deal with this situation in applying our
technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search
optimality for efficiency. In our approach, we can
explicitly make this trade-off by designing an alter-
native optimization problem which allows for slack
2We used the MOSEK package (Andersen and Andersen,
2000).
414
in the admissibility constraints. We solve the follow-
ing soft version of problem (4):
minimize
?
??+?+ C???? (5)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
where ?+ = max{0, ?} and ?? = max{0,??}
represent the componentwise positive and negative
elements of ? respectively. Each ??a > 0 represents
a configuration where our factored projection esti-
mate is not pointwise admissible. Since this situa-
tion may result in our heuristic becoming inadmis-
sible if used in the projected completion costs, we
more heavily penalize overestimating the cost by the
constant C.
2.2 Bounding Search Error
In the case where we allow pointwise inadmissibil-
ity, i.e. variables ??a , we can bound our search er-
ror. Suppose ??max = maxa?A ??a and that L? is
the length of the longest optimal solution for the
original problem. Then, h(s) ? h?(s) + L???max,
?s ? S. This ?-admissible heuristic (Ghallab and
Allard, 1982) bounds our search error by L???max.3
3 Bitext Parsing
In bitext parsing, one jointly infers a synchronous
phrase structure tree over a sentence ws and its
translation wt (Melamed et al, 2004; Wu, 1997).
Bitext parsing is a natural candidate task for our
approximate factoring technique. A synchronous
tree projects monolingual phrase structure trees onto
each sentence. However, the costs assigned by
a weighted synchronous grammar (WSG) G do
not typically factor into independent monolingual
WCFGs. We can, however, produce a useful surro-
gate: a pair of monolingual WCFGs with structures
projected by G and weights that, when combined,
underestimate the costs of G.
Parsing optimally relative to a synchronous gram-
mar using a dynamic program requires time O(n6)
in the length of the sentence (Wu, 1997). This high
degree of complexity makes exhaustive bitext pars-
ing infeasible for all but the shortest sentences. In
3This bound may be very loose if L is large.
contrast, monolingual CFG parsing requires time
O(n3) in the length of the sentence.
3.1 A? Parsing
Alternatively, we can search for an optimal parse
guided by a heuristic. The states in A? bitext pars-
ing are rooted bispans, denoted X [i, j] :: Y [k, l].
States represent a joint parse over subspans [i, j] of
ws and [k, l] of wt rooted by the nonterminals X and
Y respectively.
Given a WSG G, the algorithm prioritizes a state
(or edge) e by the sum of its inside cost ?G(e) (the
negative log of its inside probability) and its outside
estimate h(e), or completion cost.4 We are guaran-
teed the optimal parse if our heuristic h(e) is never
greater than ?G(e), the true outside cost of e.
We now consider a heuristic combining the com-
pletion costs of the monolingual projections of G,
and guarantee admissibility by enforcing point-wise
admissibility. Each state e = X [i, j] :: Y [k, l]
projects a pair of monolingual rooted spans. The
heuristic we propose sums independent outside costs
of these spans in each monolingual projection.
h(e) = ?s(X [i, j]) + ?t(Y [k, l])
These monolingual outside scores are computed rel-
ative to a pair of monolingual WCFG grammars Gs
and Gt given by splitting each synchronous rule
r =
(
X(s)
Y(t)
)
?
(
? ?
? ?
)
into its components pis(r) = X? ?? and pit(r) =
Y??? and weighting them via optimized ?s(r) and
?t(r), respectively.5
To learn pointwise admissible costs for the mono-
lingual grammars, we formulate the following opti-
mization problem:6
minimize
?,?s,?t
???1
such that, ?r = c(r)? [?s(r) + ?t(r)]
for all synchronous rules r ? G
?s ? 0, ?t ? 0, ? ? 0
4All inside and outside costs are Viterbi, not summed.
5Note that we need only parse each sentence (monolin-
gually) once to compute the outside probabilities for every span.
6The stated objective is merely one reasonable choice
among many possibilities which require pointwise admissibil-
ity and encourage tight estimates.
415
ij
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
? ?
Cost under Gt Cost under G
Synchronized completion 
scored by original model
Synchronized completion 
scored by factored model
Monolingual completions 
scored by factored model
Cost under Gs
Figure 2: The gap between the heuristic (left) and true comple-
tion cost (right) comes from relaxing the synchronized problem
to independent subproblems and slack in the factored models.
Figure 2 diagrams the two bounds that enforce the
admissibility of h(e). For any outside cost ?G(e),
there is a corresponding optimal completion struc-
ture o under G, which is an outer shell of a syn-
chronous tree. o projects monolingual completions
os and ot which have well-defined costs cs(os) and
ct(ot) under Gs and Gt respectively. Their sum
cs(os) + ct(ot) will underestimate ?G(e) by point-
wise admissibility.
Furthermore, the heuristic we compute underesti-
mates this sum. Recall that the monolingual outside
score ?s(X [i, j]) is the minimal costs for any com-
pletion of the edge. Hence, ?s(X [i, j]) ? cs(os)
and ?t(X [k, l]) ? ct(ot). Admissibility follows.
3.2 Experiments
We demonstrate our technique using the syn-
chronous grammar formalism of tree-to-tree trans-
ducers (Knight and Graehl, 2004). In each weighted
rule, an aligned pair of nonterminals generates two
ordered lists of children. The non-terminals in each
list must align one-to-one to the non-terminals in the
other, while the terminals are placed freely on either
side. Figure 3(a) shows an example rule.
Following Galley et al (2004), we learn a gram-
mar by projecting English syntax onto a foreign lan-
guage via word-level alignments, as in figure 3(b).7
We parsed 1200 English-Spanish sentences using
a grammar learned from 40,000 sentence pairs of
the English-Spanish Europarl corpus.8 Figure 4(a)
shows that A? expands substantially fewer states
while searching for the optimal parse with our op-
7The bilingual corpus consists of translation pairs with fixed
English parses and word alignments. Rules were scored by their
relative frequencies.
8Rare words were replaced with their parts of speech to limit
the memory consumption of the parser.
(a)
?
NP(s)
NP(t)
?
?
 
NN(s)1 NNS
(s)
2
NNS(t)2 de NN
(t)
1
!
(b)
T
r
a
n
s
l
a
t
i
o
n
s
y
s
t
e
m
s
s
o
m
e
t
i
m
e
s
w
o
r
k
sistemas
traduccion
funcionan
a
veces
de
NNS
NN
NP
NNSNN
NP
RB VB
S
Figure 3: (a) A tree-to-tree transducer rule. (b) An example
training sentence pair that yields rule (a).
timization heuristic. The exhaustive curve shows
edge expansions using the null heuristic. The in-
termediate result, labeled English only, used only
the English monolingual outside score as a heuris-
tic. Similar results using only Spanish demonstrate
that both projections contribute to parsing efficiency.
All three curves in figure 4 represent running times
for finding the optimal parse.
Zhang and Gildea (2006) offer a different heuris-
tic for A? parsing of ITG grammars that provides a
forward estimate of the cost of aligning the unparsed
words in both sentences. We cannot directly apply
this technique to our grammar because tree-to-tree
transducers only align non-terminals. Instead, we
can augment our synchronous grammar model to in-
clude a lexical alignment component, then employ
both heuristics. We learned the following two-stage
generative model: a tree-to-tree transducer generates
trees whose leaves are parts of speech. Then, the
words of each sentence are generated, either jointly
from aligned parts of speech or independently given
a null alignment. The cost of a complete parse un-
der this new model decomposes into the cost of the
synchronous tree over parts of speech and the cost
of generating the lexical items.
Given such a model, both our optimization heuris-
tic and the lexical heuristic of Zhang and Gildea
(2006) can be computed independently. Crucially,
the sum of these heuristics is still admissible. Re-
sults appear in figure 4(b). Both heuristics (lexi-
cal and optimization) alone improve parsing perfor-
mance, but their sum opt+lex substantially improves
upon either one.
416
(a) 050
100150
200
5 7 9 11 13 15Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 11 13 15Sentence lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
(b) 050
100150
200
5 7 9 1 13 15Sent ce lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 1 13 15Sent ce lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efficiency.
4 Lexicalized Parsing
We next apply our technique to lexicalized pars-
ing (Charniak, 1997; Collins, 1999). In lexical-
ized parsing, the local configurations are lexicalized
rules of the form X[h, t] ? Y [h?, t?] Z[h, t], where
h, t, h?, and t? are the head word, head tag, ar-
gument word, and argument tag, respectively. We
will use r = X ? Y Z to refer to the CFG back-
bone of a lexicalized rule. As in Klein and Man-
ning (2003), we view each lexicalized rule, `, as
having a CFG projection, pic(`) = r, and a de-
pendency projection, pid(`) = (h, t, h?, t?)(see fig-
ure 5).9 Broadly, the CFG projection encodes con-
stituency structure, while the dependency projection
encodes lexical selection, and both projections are
asymptotically more efficient than the original prob-
lem. Klein and Manning (2003) present a factored
model where the CFG and dependency projections
are generated independently (though with compati-
ble bracketing):
P (Y [h, t]Z[h?, t?] | X[h, t]) = (6)
P (Y Z|X)P (h?, t?|t, h)
In this work, we explore the following non-factored
model, which allows correlations between the CFG
and dependency projections:
P (Y [h, t]Z[h?, t?] | X[h, t]) = P (Y Z|X, t, h) (7)
P (t?|t, Z, h?, h) P (h?|t?, t, Z, h?, h)
This model is broadly representative of the suc-
cessful lexicalized models of Charniak (1997) and
9We assume information about the distance and direction of
the dependency is encoded in the dependency tuple, but we omit
it from the notation for compactness.
Collins (1999), though simpler.10
4.1 Choosing Constraints and Handling
Unseen Dependencies
Ideally we would like to be able to solve the op-
timization problem in (4) for this task. Unfortu-
nately, exhaustively listing all possible configura-
tions (lexical rules) yields an impractical number of
constraints. We therefore solve a relaxed problem in
which we enforce the constraints for only a subset
of the possible configurations, A? ? A. Once we
start dropping constraints, we can no longer guaran-
tee pointwise admissibility, and therefore there is no
reason not to also allow penalized violations of the
constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we
first include all configurations observed in the gold
training trees. We then sample novel configurations
by choosing (X,h, t) from the training distribution
and then using the model to generate the rest of the
configuration. In our experiments, we ended up with
434,329 observed configurations, and sampled the
same number of novel configurations. Our penalty
multiplier C was 10.
Even if we supplement our training set with many
sample configurations, we will still see new pro-
jected dependency configurations at test time. It is
therefore necessary to generalize scores from train-
ing configurations to unseen ones. We enrich our
procedure by expressing the projected configuration
costs as linear functions of features. Specifically, we
define feature vectors fc(r) and fd(h, t, h?t?) over
the CFG and dependency projections, and intro-
10All probability distributions for the non-factored model are
estimated by Witten-Bell smoothing (Witten and Bell, 1991)
where conditioning lexical items are backed off first.
417
SXXXXXXNPSaaa!!!NPNP
DT
These
PPNPHHHNNS
stocks
NPPP
RB
eventually
VPS
VBD
reopened
reopened-VBDhhhhhhhh""((((((((These-DT
These
stocks-NNS
stocks
reopened-VBDPPPP
eventually-RB
eventually
reopened-VBD
reopened
S, reopened-VBDhhhhhhhhhh
((((((((((NPS , stocks-NNSbb""DT
These
NNS
stocks
ADVPS , eventually-RB
RB
eventually
VPS , reopened-VBD
VBD
reopened
Actual Cost: 18.7
Best Projected CFG Cost: 4.1 Best Projected Dep. Cost: 9.5 CFG Projection Cost : 6.9
Dep. Projection Cost: 11.1(a) (b) (c)
Figure 5: Lexicalized parsing projections. The figure in (a) is the optimal CFG projection solution and the figure in (b) is the
optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the
CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.
duce corresponding weight vectors wc and wd. The
weight vectors are learned by solving the following
optimization problem:
minimize
?,wc,wd
??+?2 + C????2 (8)
such that, wc ? 0, wd ? 0
?` = c(`)? [w
T
c fc(r) + w
T
d fd(h, t, h
?, t?)]
for ` = (r, h, t, h?, t?) ? A?
Our CFG feature vector has only indicator features
for the specific rule. However, our dependency fea-
ture vector consists of an indicator feature of the tu-
ple (h, t, h?, t?) (including direction), an indicator of
the part-of-speech type (t, t?) (also including direc-
tion), as well as a bias feature.
4.2 Experimental Results
We tested our approximate projection heuristic on
two lexicalized parsing models. The first is the fac-
tored model of Klein and Manning (2003), given
by equation (6), and the second is the non-factored
model described in equation (7). Both models
use the same parent-annotated head-binarized CFG
backbone and a basic dependency projection which
models direction, but not distance or valence.11
In each case, we compared A? using our approxi-
mate projection heuristics to exhaustive search. We
measure efficiency in terms of the number of ex-
panded hypotheses (edges popped); see figure 6.12
In both settings, the factored A? approach substan-
tially outperforms exhaustive search. For the fac-
11The CFG and dependency projections correspond to the
PCFG-PA and DEP-BASIC settings in Klein and Manning
(2003).
12All models are trained on section 2 through 21 of the En-
glish Penn treebank, and tested on section 23.
tored model of Klein and Manning (2003), we can
also compare our reconstructed bound to the known
tight bound which would result from solving the
pointwise admissible problem in (4) with all con-
straints. As figure 6 shows, the exact factored
heuristic does outperform our approximate factored
heuristic, primarily because of many looser, backed-
off cost estimates for unseen dependency tuples. For
the non-factored model, we compared our approxi-
mate factored heuristic to one which only bounds the
CFG projection as suggested by Klein and Manning
(2003). They suggest,
?c(r) = min
`?A:pic(`)=r
c(`)
where we obtain factored CFG costs by minimizing
over dependency projections. As figure 6 illustrates,
this CFG only heuristic is substantially less efficient
than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be
admissible, we evaluated its effect on search in sev-
eral ways. The first is to check for search errors,
where the model-optimal parse is not found. In the
case of the factored model, we can find the optimal
parse using the exact factored heuristic and compare
it to the parse found by our learned heuristic. In our
test set, the approximate projection heuristic failed
to return the model optimal parse in less than 1% of
sentences. Of these search errors, none of the costs
were more than 0.1% greater than the model optimal
cost in negative log-likelihood. For the non-factored
model, the model optimal parse is known only for
shorter sentences which can be parsed exhaustively.
For these sentences up to length 15, there were no
search errors. We can also check for violations of
pointwise admissibility for configurations encoun-
418
(a)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
(b)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
Figure 6: Edges popped by exhaustive versus factored A? search. The chart in (a) is using the factored lexicalized model from
Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4.
tered during search. For both the factored and non-
factored model, less than 2% of the configurations
scored by the approximate projection heuristic dur-
ing search violated pointwise admissibility.
While this is a paper about inference, we also
measured the accuracy in the standard way, on sen-
tences of length up to 40, using EVALB. The fac-
tored model with the approximate projection heuris-
tic achieves an F1 of 82.2, matching the performance
with the exact factored heuristic, though slower. The
non-factored model, using the approximate projec-
tion heuristic, achieves an F1 of 83.8 on the test set,
which is slightly better than the factored model.13
We note that the CFG and dependency projections
are as similar as possible across models, so the in-
crease in accuracy is likely due in part to the non-
factored model?s coupling of CFG and dependency
projections.
5 Conclusion
We have presented a technique for creating A? es-
timates for inference in complex models. Our tech-
nique can be used to generate provably admissible
estimates when all search transitions can be enumer-
ated, and an effective heuristic even for problems
where all transitions cannot be efficiently enumer-
ated. In the future, we plan to investigate alterna-
tive objective functions and error-driven methods for
learning heuristic bounds.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a DHS fellowship to the first
13Since we cannot exhaustively parse with this model, we
cannot compare our F1 to an exact search method.
author and a Microsoft new faculty fellowship to the
third author.
References
E. D. Andersen and K. D. Andersen. 2000. The MOSEK in-
terior point optimizer for linear programming. In H. Frenk
et al, editor, High Performance Optimization. Kluwer Aca-
demic Publishers.
Stephen Boyd and Lieven Vandenberghe. 2005. Convex Opti-
mization. Cambridge University Press.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In National Conference on Ar-
tificial Intelligence.
Michael Collins. 1999. Head-driven statistical models for nat-
ural language parsing.
Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive
pattern database heuristics. JAIR.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
Malik Ghallab and Dennis G. Allard. 1982. A?? - an efficient
near admissible heuristic search algorithm. In IJCAI.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for
the heuristic determination of minimum cost paths. In IEEE
Transactions on Systems Science and Cybernetics. IEEE.
Dan Klein and Christopher D. Manning. 2003. Factored A*
search for models over sequences and trees. In IJCAI.
Kevin Knight and Jonathan Graehl. 2004. Training tree trans-
ducers. In HLT-NAACL.
I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004.
Generalized multitext grammars. In ACL.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In ACL.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comput. Linguist.
Hao Zhang and Daniel Gildea. 2006. Efficient search for inver-
sion transduction grammar. In EMNLP.
419
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65?73,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Reconstruction of Protolanguage Word Forms
Alexandre Bouchard-Co?te?? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present an unsupervised approach to re-
constructing ancient word forms. The present
work addresses three limitations of previous
work. First, previous work focused on faith-
fulness features, which model changes be-
tween successive languages. We add marked-
ness features, which model well-formedness
within each language. Second, we introduce
universal features, which support generaliza-
tions across languages. Finally, we increase
the number of languages to which these meth-
ods can be applied by an order of magni-
tude by using improved inference methods.
Experiments on the reconstruction of Proto-
Oceanic, Proto-Malayo-Javanic, and Classical
Latin show substantial reductions in error rate,
giving the best results to date.
1 Introduction
A central problem in diachronic linguistics is the re-
construction of ancient languages from their modern
descendants (Campbell, 1998). Here, we consider
the problem of reconstructing phonological forms,
given a known linguistic phylogeny and known cog-
nate groups. For example, Figure 1 (a) shows a col-
lection of word forms in several Oceanic languages,
all meaning to cry. The ancestral form in this case
has been presumed to be /taNis/ in Blust (1993). We
are interested in models which take as input many
such word tuples, each representing a cognate group,
along with a language tree, and induce word forms
for hidden ancestral languages.
The traditional approach to this problem has been
the comparative method, in which reconstructions
are done manually using assumptions about the rel-
ative probability of different kinds of sound change
(Hock, 1986). There has been work attempting to
automate part (Durham and Rogers, 1969; Eastlack,
1977; Lowe and Mazaudon, 1994; Covington, 1998;
Kondrak, 2002) or all of the process (Oakes, 2000;
Bouchard-Co?te? et al, 2008). However, previous au-
tomated methods have been unable to leverage three
important ideas a linguist would employ. We ad-
dress these omissions here, resulting in a more pow-
erful method for automatically reconstructing an-
cient protolanguages.
First, linguists triangulate reconstructions from
many languages, while past work has been lim-
ited to small numbers of languages. For example,
Oakes (2000) used four languages to reconstruct
Proto-Malayo-Javanic (PMJ) and Bouchard-Co?te? et
al. (2008) used two languages to reconstruct Clas-
sical Latin (La). We revisit these small datasets
and show that our method significantly outperforms
these previous systems. However, we also show that
our method can be applied to a much larger data
set (Greenhill et al, 2008), reconstructing Proto-
Oceanic (POc) from 64 modern languages. In ad-
dition, performance improves with more languages,
which was not the case for previous methods.
Second, linguists exploit knowledge of phonolog-
ical universals. For example, small changes in vowel
height or consonant place are more likely than large
changes, and much more likely than change to ar-
bitrarily different phonemes. In a statistical system,
one could imagine either manually encoding or auto-
matically inferring such preferences. We show that
both strategies are effective.
Finally, linguists consider not only how languages
change, but also how they are internally consistent.
Past models described how sounds do (or, more of-
ten, do not) change between nodes in the tree. To
borrow broad terminology from the Optimality The-
ory literature (Prince and Smolensky, 1993), such
models incorporated faithfulness features, captur-
ing the ways in which successive forms remained
similar to one another. However, each language
has certain regular phonotactic patterns which con-
65
strain these changes. We encode such patterns us-
ing markedness features, characterizing the internal
phonotactic structure of each language. Faithfulness
and markedness play roles analogous to the channel
and language models of a noisy-channel system. We
show that markedness features improve reconstruc-
tion, and can be used efficiently.
2 Related work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given pro-
tolanguage are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is not
tractable for larger trees, since the time complexity
of their multi-alignment algorithm grows exponen-
tially in the number of languages. Second, deter-
ministic rules, while elegant in theory, are not robust
to noise: even in experiments with only four daugh-
ter languages, a large fraction of the words could not
be reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. The model does not support gener-
alizations across languages, and has no way to cap-
ture phonotactic regularities within languages. As a
consequence, the resulting method does not scale to
large phylogenies. The work we present here ad-
dresses both of these issues, with a richer model
and faster inference allowing improved reconstruc-
tion and increased scale.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples
in Figure 3 (c-e). In such a tree, the modern lan-
guages, whose word forms will be observed, are the
leaves of ? . All internal nodes, particularly the root,
are languages whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
We assume that word forms evolve along the
branches of the tree ? . However, it is not the case
that each cognate set exists in each modern lan-
guage. Formally, we assume there to be a known
list of C cognate sets. For each c ? {1, . . . , C}
let L(c) denote the subset of modern languages that
have a word form in the c-th cognate set. For each
set c ? {1, . . . , C} and each language ` ? L(c), we
denote the modern word form by wc`. For cognate
set c, only the minimal subtree ?(c) containing L(c)
and the root is relevant to the reconstruction infer-
ence problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn incre-
mentally as follows: for each edge ` ? `? in ?(c) use
a branch-specific distribution over changes in strings
to generate the word at node `?.
In the remainder of this section, we clarify the ex-
act form of the conditional distributions over string
changes, the distribution over strings at the root, and
the parameterization of this process.
3.1 Markedness and Faithfulness
In Optimality Theory (OT) (Prince and Smolensky,
1993), two types of constraints influence the selec-
tion of a realized output given an input form: faith-
fulness andmarkedness constraints. Faithfulness en-
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarded them in this work.
66
t a
a
?
n g
i
i
s
#
#
#
#
a
?
n
g
/angi/
/a?i/
/ta?i/
/angi/
/a?i/
/ta?i/
?
S
?
I
x
1
x
2
x
3
x
7
y
1
y
2
y
3
y
7
x
4
y
4
y
5
y
6
x
5
x
6
?
n
g
1[Insert]
1[Subst]
1[(n g)@Kw]
1[??g@Kw]
1[??g]
1[(n)@Kw]
1[(g)@Kw]
Language Word form
Proto Oceanic /taNis/
Lau /aNi/
Kwara?ae /angi/
Taiof /taNis/
Table 1: A cognate set from the Austronesian dataset. All
word forms mean to cry.
constrain these changes. We encode such patterns
using markedness features, characterizing the inter-
nal phonotactic structure of each language. Faith-
fulness and marked ess play roles analogous to the
channel and language models of a noisy-channel
system. We show that markedness features greatly
improve reconstruction quality, and we show how to
work with them efficiently.
2 Related Work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given proto-
language are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is
not tractable for larger trees since the complexity of
the multi-alignment algorithm grows exponentially
in the number of languages. Second, determinis-
tic rules, while elegant in theory, are not robust to
noise: even in experiments with only four daughter
languages, a large fraction of the words could not be
reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. Use of approximate inference and
stochastic rules addresses some of the limitations of
(Oakes, 2000), but the resulting method is computa-
tionally demanding and consequently does not scale
to large phylogenies. The high computational cost
of probabilistic inference also limits the features that
can be included in the model (omitting global fea-
tures supporting generalizations across languages,
and markedness features within languages). The
work we present here addresses both of these issues,
with faster inference and a richer model allowing in-
creased scale and improved reconstruction.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples in
Figure 4 (c-e). In such a tree, the modern languages,
whose word forms will be observed, are the leaves
"1 . . . "m. All internal nodes, particularly the root,
are languages " whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
As a first approximation, we assume that word
forms evolve along the branches of the tree ? . How-
ever, it is not the case that each cognate set exists
in each modern langugage. Formally, we assume
there to be a known list of C cognate sets. For each
c ? {1, . . . , C} let L(c) denote the subset of mod-
ern languages that have a word form in the c-th cog-
nate set. For each set c ? {1, . . . , C} and each lan-
guage " ? L(c), we denote the modern word form
by wc!. For cognate set c, only the minimal subtree
?(c) containing L(c) and the root is relevant to the
reconstruction inference problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn in-
crementally as follows: for each edge " ? "? in ?(c)
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarted them in this work.
(a) (b)
(f)
(c)
(d)(e)
..
?
Figure 1: (a) A cognate set from the Austronesian dataset.
All word orms mean to cry. (b-d) The mutation model
used in this paper. (b) The mutation of POc /taNis/ to
Kw. /angi/. (c) Graphical model depicting the dependen-
cie among variables in one step of the mutation Markov
chain. (d) Active features for one step in this process.
(e-f) Comparison of two inference procedures on trees:
Single sequence resampling (e) draws one sequence at a
time, conditio ed on its parent and children, while ances-
try resampling (f) draws an aligned slice from all words
simultaneously. In large trees, the latter is ore efficien
than the former.
courages similarity between the input and output
while markedness favors well-formed output.
Viewed from this perspective, previous comput -
tional approaches to reconstruction are based almost
xclusively n faithf lnes , ex r ssed thr ug a mu-
tation model. Only the words in the language at the
root of the tree, if any, are explicitly encouraged to
be w ll-formed. In ontrast, we incorporate con-
straints on markedness for each language with both
general and branc -specific constraints on faithful-
ness. This is done using a lexicalized stochastic
string transducer (Varadarajan et al, 2008).
We now make precise the conditional distribu-
tions over pairs of evolving strings, referring to Fig-
ure 1 (b-d). Consider a language `? evolving to `
for cognate set c. Assume we have a word form
x = wcl? . The generative process for producing
y = wcl works as follows. First, we consider
x to be composed of characters x1x2 . . . xn, with
the first and last being a special boundary symbol
x1 = # ? ? which is never deleted, mutated, or
created. The process generates y = y1y2 . . . yn in
n chunks yi ? ??, i ? {1, . . . , n}, one for each xi.
The yi?s may be a single character, multiple charac-
ters, or even empty. In the example shown, all three
of these cases occur.
T generat yi, we define a mutation Markov
chain that incrementally adds zero or more charac-
ters to an initially empty yi. First, we decide whether
the current phoneme in the top word t = xi will be
deleted, in which case yi =  as in the example of
/s/ being deleted. If t is not deleted, we chose a sin-
gle substitution character in the bottom word. This
is the case both when /a/ is unchanged and when /N/
substitutes to /n/. We writeS = ??{?} for this set
of outcomes, where ? is the special outcome indi-
cating deletion. Importantly, the probabilities of this
multinomial can depend on both the previous char-
acter gen rated so far (i.e. the rightmost character
p of yi?1) and the current character in the previous
generation string (t). As we will see shortly, this al-
lows modelling markedness and faithfulness at every
branch, jointly. This multinomial decision acts as
the initial distribution of the mutation Markov chain.
We consider insertions only if a deletion was not
selected in the first step. Here, we draw from a
multinomial overS , where this time the special out-
come ? corresponds to stopping insertions, and the
other elements ofS correspond to symbols that are
appende to yi. In this case, the conditioning envi-
ronment is t = xi and the current rightmost symbol
p in yi. Insertions continue until ? is selected. In
the example, w follow the substitution of /N/ to /n/
with an insertion of /g/, followed by a decision to
stop that yi. We will use ?S,t,p,` and ?I,t,p,` to denote
the probabilities ver the substitution and insertion
decisions in the current branch `? ? `.
A similar process generates the word at the root
` of a tree, treating this word as a single string
y1 generated from a dummy ancestor t = x1. In
this case, only the insertion probabilities matter, and
we separately parameterize these probabilities with
?R,t,p,`. There is no actual dependence on t at the
root, but this formulation allows us to unify the pa-
rameterization, with each ??,t,p,` ? R|?|+1 where
? ? {R,S, I}.
3.2 Parameterization
Instead of directly estimating the transition proba-
bilities of the mutation Markov chain (as the param-
eters of a collection of multinomial distributions) we
67
express them as the output of a log-linear model. We
used the following feature templates:
OPERATION identifies whether an operation in the
mutation Markov chain is an insertion, a deletion,
a substitution, a self-substitution (i.e. of the form
x ? y, x = y), or the end of an insertion event.
Examples in Figure 1 (d): 1[Subst] and 1[Insert].
MARKEDNESS consists of language-specific n-
gram indicator functions for all symbols in ?. Only
unigram and bigram features are used for computa-
tional reasons, but we show in Section 5 that this
already captures important constraints. Examples in
Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw
stands for Kwara?ae, a language of the Solomon
Islands), the unigram indicators 1[(n)@Kw] and
1[(g)@Kw].
FAITHFULNESS consists of indicators for muta-
tion events of the form 1[x ? y], where x ? ?,
y ? S . Examples: 1[N ? n], 1[N ? n@Kw].
Feature templates similar to these can be found
for instance in Dreyer et al (2008) and Chen (2003),
in the context of string-to-string transduction. Note
also the connection with stochastic OT (Goldwater
and Johnson, 2003; Wilson, 2006), where a log-
linear model mediates markedness and faithfulness
of the production of an output form from an under-
lying input form.
3.3 Parameter sharing
Data sparsity is a significant challenge in protolan-
guage reconstruction. While the experiments we
present here use an order of magnitude more lan-
guages than previous computational approaches, the
increase in observed data also brings with it addi-
tional unknowns in the form of intermediate pro-
tolanguages. Since there is one set of parameters
for each language, adding more data is not sufficient
for increasing the quality of the reconstruction: we
show in Section 5.2 that adding extra languages can
actually hurt reconstruction using previous methods.
It is therefore important to share parameters across
different branches in the tree in order to benefit from
having observations from more languages.
As an example of useful parameter sharing, con-
sider the faithfulness features 1[/p/ ? /b/] and
1[/p/ ? /r/], which are indicator functions for the
appearance of two substitutions for /p/. We would
like the model to learn that the former event (a sim-
ple voicing change) should be preferred over the lat-
ter. In Bouchard-Co?te? et al (2008), this has to be
learned for each branch in the tree. The difficulty is
that not all branches will have enough information
to learn this preference, meaning that we need to de-
fine the model in such a way that it can generalize
across languages.
We used the following technique to address this
problem: we augment the sufficient statistics of
Bouchard-Co?te? et al (2008) to include the current
language (or language at the bottom of the current
branch) and use a single, global weight vector in-
stead of a set of branch-specific weights. Gener-
alization across branches is then achieved by using
features that ignore `, while branch-specific features
depend on `.
For instance, in Figure 1 (d), 1[N ? n] is
an example of a universal (global) feature shared
across all branches while 1[N ? n@Kw] is branch-
specific. Similarly, all of the features in OPERA-
TION, MARKEDNESS and FAITHFULNESS have uni-
versal and branch-specific versions.
3.4 Objective function
Concretely, the transition probabilities of the muta-
tion and root generation are given by:
??,t,p,`(?) = exp{??, f(?, t, p, `, ?)?}Z(?, t, p, `, ?) ? ?(?, t, ?),
where ? ? S , f : {S, I,R}?????L?S ? Rk
is the sufficient statistics or feature function, ??, ??
denotes inner product and ? ? Rk is a weight vector.
Here, k is the dimensionality of the feature space of
the log-linear model. In the terminology of exponen-
tial families, Z and ? are the normalization function
and reference measure respectively:
Z(?, t, p, `, ?) = ?
???S
exp{??, f(?, t, p, `, ??)?}
?(?, t, ?) =
?
???
???
0 if ? = S, t = #, ? 6= #
0 if ? = R, ? = ?
0 if ? 6= R, ? = #
1 o.w.
Here, ? is used to handle boundary conditions.
We will also need the following notation: let
P?(?),P?(?|?) denote the root and branch probabil-ity models described in Section 3.1 (with transition
probabilities given by the above log-linear model),
I(c), the set of internal (non-leaf) nodes in ?(c),
pa(`), the parent of language `, r(c), the root of ?(c)
68
and W (c) = (??)|I(c)|. We can summarize our ob-
jective function as follows:
CX
c=1
log
X
~w?W (c)
P?(wc,r(c))
Y
`?I(c)
P?(wc,`|wc,pa(`)) ? ||?||
2
2
2?2
The second term is a standard L2 regularization
penalty (we used ?2 = 1).
4 Learning algorithm
Learning is done using a Monte Carlo variant of the
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977). The M step is convex and com-
puted using L-BFGS (Liu et al, 1989); but the E
step is intractable (Lunter et al, 2003), so we used
a Markov chain Monte Carlo (MCMC) approxima-
tion (Tierney, 1994). At E step t = 1, 2, . . . , we
simulated the chain for O(t) iterations; this regime
is necessary for convergence (Jank, 2005).
In the E step, the inference problem is to com-
pute an expectation under the posterior over strings
in a protolanguage given observed word forms at the
leaves of the tree. The typical approach in biology
or historical linguistics (Holmes and Bruno, 2001;
Bouchard-Co?te? et al, 2008) is to use Gibbs sam-
pling, where the entire string at a single node in the
tree is sampled, conditioned on its parent and chil-
dren. This sampling domain is shown in Figure 1 (e),
where the middle word is completely resampled but
adjacent words are fixed. We will call this method
Single Sequence Resampling (SSR). While concep-
tually simple, this approach suffers from problems
in large trees (Holmes and Bruno, 2001). Con-
sequently, we use a different MCMC procedure,
called Ancestry Resampling (AR) that alleviates
the mixing problems (Figure 1 (f)). This method
was originally introduced for biological applications
(Bouchard-Co?te? et al, 2009), but commonalities be-
tween the biological and linguistic cases make it
possible to use it in our model.
Concretely, the problem with SSR arises when the
tree under consideration is large or unbalanced. In
this case, it can take a long time for information
from the observed languages to propagate to the root
of the tree. Indeed, samples at the root will ini-
tially be independent of the observations. AR ad-
dresses this problem by resampling one thin vertical
slice of all sequences at a time, called an ancestry.
For the precise definition, see Bouchard-Co?te? et al
(2009). Slices condition on observed data, avoiding
the problems mentioned above, and can propagate
information rapidly across the tree.
5 Experiments
We performed a comprehensive set of experiments
to test the new method for reconstruction outlined
above. In Section 5.1, we analyze in isolation the
effects of varying the set of features, the number of
observed languages, the topology, and the number
of iterations of EM. In Section 5.2 we compare per-
formance to an oracle and to three other systems.
Evaluation of all methods was done by computing
the Levenshtein distance (Levenshtein, 1966) be-
tween the reconstruction produced by each method
and the reconstruction produced by linguists. We
averaged this distance across reconstructed words to
report a single number for each method. We show
in Table 2 the average word length in each corpus;
note that the Latin average is much larger, giving
an explanation to the higher errors in the Romance
dataset. The statistical significance of all perfor-
mance differences are assessed using a paired t-test
with significance level of 0.05.
5.1 Evaluating system performance
We used the Austronesian Basic Vocabulary
Database (Greenhill et al, 2008) as the basis for
a series of experiments used to evaluate the per-
formance of our system and the factors relevant to
its success. The database includes partial cognacy
judgments and IPA transcriptions, as well as a few
reconstructed protolanguages. A reconstruction of
Proto-Oceanic (POc) originally developed by Blust
(1993) using the comparative method was the basis
for evaluation.
We used the cognate information provided in
the database, automatically constructing a global
tree2 and set of subtrees from the cognate set in-
dicator matrix M(`, c) = 1[` ? L(c)], c ?
{1, . . . , C}, ` ? L. For constructing the global tree,
we used the implementation of neighbor joining in
the Phylip package (Felsenstein, 1989). We used
a distance based on cognates overlap, dc(`1, `2) =?C
c=1 M(`1, c)M(`2, c). We bootstrapped 1000
2The dataset included a tree, but it was out of date as of
November 2008 (Greenhill et al, 2008).
69
NggelaBugotuTapeAvavaNeveeiNamanNeseSantaAnaNahavaqNatiKwaraaeSol
LauKwameraToloMarshalles
PuloAnnaChuukeseAK
SaipanCaro
Puluwatese
WoleaianPuloAnnan
Carolinian
WoleaiChuukeseNaunaPaameseSou
AnutaVaeakauTau
TakuuTokelauTonganSamoanIfiraMeleM
TikopiaTuvaluNiueFutunaEast
UveaEastRennellese
EmaeKapingamar
SikaianaNukuoroLuangiuaHawaiianMarquesan
TahitianthRurutuanMaoriTuamotuMangareva
Rarotongan
PenrhynRapanuiEas
PukapukaMwotlapMotaFijianBauNamakirNgunaArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
Figure 3: Phylogenetic trees for three language families.Clockwise, from the top left: Romance, Austronesian andProto-Malayo-Javanic.
formance of our system and the factors relevant toits success. The database contained, as of Novem-ber 2008, 124,468 lexical items from 587 languagesmostly from the Austronesian language family. Thedatabase includes partial cognacy judgments andIPA transcriptions, as well as a few reconstructedproto-languages. A reconstruction of Proto Oceanic(POc) originally developed by (Blust, 1993) usingthe comparative method was the basis for evaluation.We used the cognate information provided in thedatabase, automatically constructing a global tree2and set of subtrees from the cognate set indicatormatrix M(!, c) = 1[! ? L(c)], c ? {1, . . . , C}, ! ?
L. For constructing the global tree, we used theimplementation of neighbor joining in the Phylippackage (Felsenstein, 1989). The distance ma-trix used the Hamming distance of cognate indi-cators, dc(!1, !2) = ?Cc=1 M(!1, c)M(!2, c). Webootstrapped 1000 samples and formed an accurate(90%) consensus tree. The tree obtained is not bi-nary, but the AR inference algorithm scales linearlyin the branching factor of the tree (in contrast, SSRscales exponentially (Lunter et al, 2003)).The first claim we verified experimentally is thathaving more observed languages aids reconstructionof proto-languages. To test this hypothesis we addedobserved modern languages in increasing order ofdistance dc to the target reconstruction of POc sothat the languages that are most useful for POc re-construction are added first. This prevents the ef-fects of adding a close language after several distant
2The dataset included a tree, but as of November 2008, itwas generated automatically and ?has [not] been updated in awhile.?
0 10 20 30 40 50 60 701.4
1.6
1.8
2
2.2
2.4
2.6
Number of modern languages
Error
Figure 4: Mean distance to the target reconstruction ofproto Oceanic as a function of the number of modern lan-guages used by the inference procedure.
ones being confused with an improvement producedby increasing the number of languages.The results are reported in Figure 4. They con-firm that large-scale inference is desirable for auto-matic proto-language reconstruction: going from 2-to-4, 4-to-8, 8-to-16, 16-to-32 languages all signifi-cantly helped reconstruction. There was still an av-erage edit distance improvement of 0.05 from 32 to64 languages, altough this was not statistically sig-nificant.We then conducted a number of experiments in-tended to assess the robustness of the system, and toidentify the contribution made by different factors itincorporates. First, we ran the system with 20 dif-ferent random seeds and assessed the stability of thesolution found. In each cases, learning was stableand helded performances. See Figure 5.Next, we found that all of the following ablationssignificantly hurts reconstruction: using a flat treein which all languages are equidistant from the re-constructed root and from each other instead of theconsensus tree, dropping the markedness features,disabling sharing across branches and dropping thefaithfulness features. The results of these experi-ments are shown in Table 2.For comparison, we also included in the sametable the performance of a semi-supervised systemtrained by K-fold validation. The system was ran
K time, with disjoint 1 ? K?1 of the POc. wordsgiven to the system (as observations in the graph-
Condition Edit dist.Unsupervised full system 1.87-FAITHFULNESS 2.02-MARKEDNESS 2.18-Sharing 1.99-Topology 2.06Semi-supervised system 1.75
Table 2: Effects of ablation of various aspects of ourunsupervised system on mean edit distance to protoOceanic. -Sharing corresponds to the subset of the fea-tures in OPERATION, FAITHFULNESS and MARKEDNESSthat condition on the current language, -Topolo y corre-sponds to using a flat topology where the only edges inthe tree connect modern languages to proto Oc anic. Thesemi-supervised system i described in text. All dif-ferences (compared to the unsupervised full system) arestatistically significant.
ical model) fo each run. It is semi-supervised inthe sense that gold reconstruction for many internalnodes are not avail bl (such as th common ances-tor of Kw. nd Lau in Fi re 6).3
Figure 6 shows the results of a concrete run over32 languages, zooming in to a pair of the Solomoniclanguages and the cognate set from Table 1. In theexample shown, the reconstruction is as good as theoracle, though off by one character (the final /s/ isnot resent in any of the 32 inputs and thereforeis not reconstructed). The diagrams show, for boththe global and the local features, the expectationsof each substitution superimposed on an IPA soundch rt, as well as a list of the top changes. Darkerlines indicate higher counts. T is run did not usen tural class constraints, but it can be seen that lin-guistically plausibl substitutions are learned. Theglobal features prefer a range of voic ng changes,manner changes, adjace t vowel motion, and so on,including mu ations like /s/ to /h/ which are commonbut poorly repre ented in a naive attribute-based nat-ural class scheme. On the other hand, the features l -cal to the lang ag Kwara?a (Kw.) pick out the sub-set of these change which are active in that branch,such as /s/?/t/ fortition.
3We also tried a fully supervised system where a flat topol-ogy is used so that all of these latent internal nodes are avoided;but it did not perform as well.
0 2 4 6 8 10 12 14 16 18 201.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4
3.6
EM Iteration
Error
Figure 5: Mean distance to the target reconstruction ofPOc as a function of the EM iteration.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) andBouchard-Co?te? et al (2008) respectively and sum-marized them in Section 1. Neither approach scaleswell to large datasets. In the first case, the bottleneckis the complexity of computing multi-alignmentswithout guide trees and the vanishing probabilitythat independent reconstructions agree. In the sec-ond case, the problem comes from slow mixing ofthe inference algorithm and the unregularized pro-liferation of parameters. For this reason, we built athird baseline that scales well in large datasets.This third baseline, CENTROID, computes thecentroid of the observed word forms in Leven-shtein distance. Let L(x, y) denote the Lev-enshtein distance between word forms x and
y. Ideally, we would like the baseline toreturn argminx????y?O L(x, y), where O ={y1, . . . , y|O|} is the set of observed word forms.Note that the optimum is not changed if we restrictthe minimization to be taken on x ? ?(O)? suchthat m ? |x| ? M where m = mini |yi|,M =maxi |yi| and?(O) is the set of characters occurringin O. Even with this restriction, this optimizationis intractable. As an approximation, we consideredonly strings built by at most k contiguous substringstaken from the word forms in O. If k = 1, then itis equivalent to taking the min over x ? O. At theother end of the spectrum, if k = M , it is exact.This scheme is exponential in k, but since words arerelatively short, we found that k = 2 often finds the
E
r
r
o
r
N. of m d rn lang. EM iteration
100 20300 60
1.4
1.8
2.2
2.6
1.8
2.4
3
3.6
Figure 2: Left: Mean distance to the target reconstruction
of POc as a function of the number of modern languages
used by the inference procedure. Right: Mean distance
and confidenc intervals as a function of th EM it ration,
averag d over 20 random seeds an ran on 4 languages.
samples nd forme a accurate (90%) consen us
tre . The tree obtained is o binary, but the AR
infer ce algorithm scales lin arly in the branching
factor of the tree (in contrast, SSR scale exp nen-
tially (Lunter et al, 2003)).
T e first laim we ver fied experimentally is that
having more observed languages aids reconstruction
of protolanguages. To t t this hypothesis we added
observed mod rn l nguage in increasing order of
distance dc to the target reconstruction of POc so
that the languages that are most useful for POc re-
construction are added first. This prevents the ef-
fects of adding a close language after several distant
ones being confused with an improvement produced
by increasing the number of languages.
The results are reported in Figure 2 (a). They con-
firm that large-scale inference is desirable for au-
tomatic protolanguage reconstruction: reconstruc-
tion improved statistically significantly with each in-
crease except from 32 to 64 languages, where the
average edit distance improvement was 0.05.
We then conducted a number of experiments in-
tended to assess the robustness of the system, and to
identify the contribution made by different factors it
incorporates. First, we ran the system with 20 dif-
ferent random seeds to assess the stability of the so-
lutions found. In each case, learning was stable and
accuracy improved during training. See Figure 2 (b).
Next, we found that all of the following ablations
significantly hurt reconstruction: using a flat tree (in
which all languages are equidistant from the recon-
structed root and from each other) instead of the con-
sensus tree, dropping the markedness features, drop-
Condition Edit dist.
Unsupervised full system 1.87
-FAITHFULNESS 2.02
-MARKEDNESS 2.18
-Sharing 1.99
-Topology 2.06
Semi-supervised system 1.75
Table 1: Effects of ablation of various aspects of our
unsupervised system on mean edit distance to POc.
-Sharing corresponds to the restriction to the subset of the
features in OPERATION, FAITHFULNESS and MARKED-
NESS that are branch-specific, -Topology corresponds to
using a flat topology where the only edges in the tree con-
nect modern languages to POc. The semi-supervised sys-
tem is described in the text. All differences (compared to
the unsupervised full system) are statistically significant.
ping the faithfulness features, and disabling sharing
across branches. The results of these experiments
are shown in Table 1.
For comparison, we also included in the same
table the performance of a semi-supervised system
trained by K-fold validation. The system was ran
K = 5 times, with 1?K?1 of the POc words given
to the system as observations in the graphical model
for each run. It is semi-supervised in the sense that
gold reconstruction for many internal nodes are not
available in the dataset (for example the common an-
cestor of Kwara?ae (Kw.) and Lau in Figure 3 (b)),
so they are still not filled.3
Figure 3 (b) shows the results of a concrete run
over 32 languages, zooming in to a pair of the
Solomonic languages and the cognate set from Fig-
ure 1 (a). In the example shown, the reconstruc-
tion is as good as the ORACLE (described in Sec-
tion 5.2), though off by one character (the final /s/
is not present in any of the 32 inputs and therefore
is not reconstructed). In (a), diagrams show, for
both the global and the local (Kwara?ae) features,
the expectations of each substitution superimposed
on an IPA sound chart, as well as a list of the top
changes. Darker lines indicate higher counts. This
run did not use natural class constraints, but it can
3We also tried a fully supervised system where a flat topol-
ogy is used so that all of these latent internal nodes are avoided;
but it did not perform as well?this is consistent with the
-Topology experiment of Table 1.
70
be seen that linguistically plausible substitutions are
learned. The global features prefer a range of voic-
ing changes, manner changes, adjacent vowel mo-
tion, and so on, including mutations like /s/ to /h/
which are common but poorly represented in a naive
attribute-based natural class scheme. On the other
hand, the features local to the language Kwara?ae
pick out the subset of these changes which are ac-
tive in that branch, such as /s/?/t/ fortition.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) and
Bouchard-Co?te? et al (2008) respectively and sum-
marized in Section 1. Neither approach scales well
to large datasets. In the first case, the bottleneck is
the complexity of computing multi-alignments with-
out guide trees and the vanishing probability that in-
dependent reconstructions agree. In the second case,
the problem comes from the unregularized prolifera-
tion of parameters and slow mixing of the inference
algorithm. For this reason, we built a third baseline
that scales well in large datasets.
This third baseline, CENTROID, computes the
centroid of the observed word forms in Leven-
shtein distance. Let L(x, y) denote the Lev-
enshtein distance between word forms x and
y. Ideally, we would like the baseline to
return argminx???
?
y?O L(x, y), where O =
{y1, . . . , y|O|} is the set of observed word forms.
Note that the optimum is not changed if we restrict
the minimization to be taken on x ? ?(O)? such
that m ? |x| ? M where m = mini |yi|,M =
maxi |yi| and ?(O) is the set of characters occurring
in O. Even with this restriction, this optimization
is intractable. As an approximation, we considered
only strings built by at most k contiguous substrings
taken from the word forms in O. If k = 1, then it
is equivalent to taking the min over x ? O. At the
other end of the spectrum, if k = M , it is exact.
This scheme is exponential in k, but since words are
relatively short, we found that k = 2 often finds the
same solution as higher values of k. The difference
was in all the cases not statistically significant, so we
report the approximation k = 2 in what follows.
We also compared against an oracle, denoted OR-
ACLE, which returns argminy?OL(y, x?), where x?
is the target reconstruction. We will denote it by OR-
Comparison CENTROID PRAGUE BCLKG
Protolanguage POc PMJ La
Heldout (prop.) 243 (1.0) 79 (1.0) 293 (0.5)
Modern languages 70 4 2
Cognate sets 1321 179 583
Observed words 10783 470 1463
Mean word length 4.5 5.0 7.4
Table 2: Experimental setup: number of held-out proto-
word from (absolute and relative), of modern languages,
cognate sets and total observed words. The split for
BCLKG is the same as in Bouchard-Co?te? et al (2008).
ACLE. This is superior to picking a single closest
language to be used for all word forms, but it is pos-
sible for systems to perform better than the oracle
since it has to return one of the observed word forms.
We performed the comparison against Oakes
(2000) and Bouchard-Co?te? et al (2008) on the same
dataset and experimental conditions as those used in
the respective papers (see Table 2). Note that the
setup of Bouchard-Co?te? et al (2008) provides super-
vision (half of the Latin word forms are provided);
all of the other comparisons are performed in a com-
pletely unsupervised manner.
The PMJ dataset was compiled by Nothofer
(1975), who also reconstructed the corresponding
protolanguage. Since PRAGUE is not guaranteed to
return a reconstruction for each cognate set, only 55
word forms could be directly compared to our sys-
tem. We restricted comparison to this subset of the
data. This favors PRAGUE since the system only pro-
poses a reconstruction when it is certain. Still, our
system outperformed PRAGUE, with an average dis-
tance of 1.60 compared to 2.02 for PRAGUE. The
difference is marginally significant, p = 0.06, partly
due to the small number of word forms involved.
We also exceeded the performance of BCLKG on
the Romance dataset. Our system?s reconstruction
had an edit distance of 3.02 to the truth against 3.10
for BCLKG. However, this difference was not signifi-
cant (p = 0.15). We think this is because of the high
level of noise in the data (the Romance dataset is the
only dataset we consider that was automatically con-
structed rather than curated by linguists). A second
factor contributing to this small difference may be
that the the experimental setup of BCLKG used very
few languages, while the performance of our system
improves markedly with more languages.
71
Nggela
Bugotu
TapeAvava
Neveei
Naman
NeseSantaAna
Nahavaq
NatiKwaraaeSol
LauKwamera
ToloMarshalles
PuloAnna
ChuukeseAK
SaipanCaro
Puluwatese
Woleaian
PuloAnnan
Carolinian
Woleai
Chuukese
Nauna
PaameseSou
AnutaVaeakauTau
Takuu
Tokelau
Tongan
Samoan
IfiraMeleM
Tikopia
Tuvalu
NiueFutunaEast
UveaEast
Rennellese
EmaeKapingamar
Sikaiana
Nukuoro
Luangiua
Hawaiian
Marquesan
Tahitianth
Rurutuan
Maori
Tuamotu
Mangareva
Rarotongan
Penrhyn
RapanuiEas
Pukapuka
Mwotlap
MotaFijianBau
Namakir
Nguna
ArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
/a?i/ (Lau)/angi/ (Kw.)
/a?i/
/ta?i/ (POc)
....
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te ?? io ?? a
1
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te io a
1
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
(a) (b) (c) (d) (e)
Figure 3: (a) A visualization of two learned faithfulness parameters: on the top, from the universal features, on
the bottom, for one particular branch. Each pair of phonemes have a link with grayscale value proportional to the
expectation of a transition between them. The five strongest links are also included at the right. (b) A sample taken
from our POc experiments (see text). (c-e) Phylogenetic trees for three language families: Proto-Malayo-Javanic,
Austronesian and Romance.
We conducted another experiment to verify this
by running both systems in larger trees. Because the
Romance dataset had only three modern languages
transcribed in IPA, we used the Austronesian dataset
to perform the test. The results were all significant in
this setup: while our method went from an edit dis-
tance of 2.01 to 1.79 in the 4-to-8 languages exper-
iment described in Section 5.1, BCLKG went from
3.30 to 3.38. This suggests that more languages can
actually hurt systems that do not support parameter
sharing.
Since we have shown evidence that PRAGUE and
BCLKG do not scale well to large datasets, we
also compared against ORACLE and CENTROID in a
large-scale setting. Specifically, we compare to the
experimental setup on 64 modern languages used to
reconstruct POc described before. Encouragingly,
while the system?s average distance (1.49) does not
attain that of the ORACLE (1.13), we significantly
outperform the CENTROID baseline (1.79).
5.3 Incorporating prior linguistic knowledge
The model also supports the addition of prior lin-
guistic knowledge. This takes the form of feature
templates with more internal structure. We per-
formed experiments with an additional feature tem-
plate:
STRUCT-FAITHFULNESS is a structured version of
FAITHFULNESS, replacing x and y with their natu-
ral classes N?(x) and N?(y) where ? indexes types
of classes, ranging over {manner, place, phonation,
isOral, isCentral, height, backness, roundedness}.
This feature set is reminiscent of the featurized rep-
resentation of Kondrak (2000).
We compared the performance of the system with
and without STRUCT-FAITHFULNESS to check if the
algorithm can recover the structure of natural classes
in an unsupervised fashion. We found that with
2 or 4 observed languages, FAITHFULNESS under-
performed STRUCT-FAITHFULNESS, but for larger
trees, the difference was not significant. FAITH-
FULNESS even slightly outperformed its structured
cousin with 16 observed languages.
6 Conclusion
By enriching our model to include important fea-
tures like markedness, and by scaling up to much
larger data sets than were previously possible, we
obtained substantial improvements in reconstruc-
tion quality, giving the best results on past data
sets. While many more complex phenomena are
still unmodeled, from reduplication to borrowing to
chained sound shifts, the current approach signifi-
cantly increases the power, accuracy, and efficiency
of automatic reconstruction.
Acknowledgments
We would like to thank Anna Rafferty and our re-
viewers for their comments. This work was sup-
ported by a NSERC fellowship to the first author and
NSF grant number BCS-0631518 to the second au-
thor.
72
References
R. Blust. 1993. Central and central-Eastern Malayo-
Polynesian. Oceanic Linguistics, 32:241?293.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change. In
Advances in Neural Information Processing Systems
20.
A. Bouchard-Co?te?, M. I. Jordan, and D. Klein. 2009.
Efficient inference in phylogenetic InDel trees. In Ad-
vances in Neural Information Processing Systems 21.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
M. A. Covington. 1998. Alignment of multiple lan-
guages for historical comparison. In Proceedings of
ACL 1998.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
M. Dreyer, J. R. Smith, and J. Eisner. 2008. Latent-
variable modeling of string transductions with finite-
state methods. In Proceedings of EMNLP 2008.
S. P. Durham and D. E. Rogers. 1969. An application
of computer programming to the reconstruction of a
proto-language. In Proceedings of the 1969 confer-
ence on Computational linguistics.
C. L. Eastlack. 1977. Iberochange: A program to
simulate systematic sound change in Ibero-Romance.
Computers and the Humanities.
J. Felsenstein. 1989. PHYLIP - PHYLogeny Inference
Package (Version 3.2). Cladistics, 5:164?166.
S. Goldwater and M. Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
Proceedings of the Workshop on Variation within Op-
timality Theory.
S. J. Greenhill, R. Blust, and R. D. Gray. 2008. The
Austronesian basic vocabulary database: From bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271?283.
H. H. Hock. 1986. Principles of Historical Linguistics.
Walter de Gruyter.
I. Holmes and W. J. Bruno. 2001. Evolutionary HMM:
a Bayesian approach to multiple alignment. Bioinfor-
matics, 17:803?820.
W. Jank. 2005. Stochastic variants of EM: Monte Carlo,
quasi-Monte Carlo and more. In Proceedings of the
American Statistical Association.
G. Kondrak. 2000. A new algorithm for the alignment of
phonetic sequences. In Proceedings of NAACL 2000.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
V. I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions and reversals. Soviet Physics
Doklady, 10, February.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
J. B. Lowe and M. Mazaudon. 1994. The reconstruction
engine: a computer implementation of the comparative
method. Comput. Linguist., 20(3):381?417.
G. A. Lunter, I. Miklo?s, Y. S. Song, and J. Hein. 2003.
An efficient algorithm for statistical multiple align-
ment on arbitrary phylogenetic trees. Journal of Com-
putational Biology, 10:869?889.
B. Nothofer. 1975. The reconstruction of Proto-Malayo-
Javanic. M. Nijhoff.
M. P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daugh-
ter languages. Journal of Quantitative Linguistics,
7(3):233?244.
A. Prince and P. Smolensky. 1993. Optimality theory:
Constraint interaction in generative grammar. Techni-
cal Report 2, Rutgers University Center for Cognitive
Science.
L. Tierney. 1994. Markov chains for exploring posterior
distributions. The Annals of Statistics, 22(4):1701?
1728.
A. Varadarajan, R. K. Bradley, and I. H. Holmes. 2008.
Tools for simulating evolution of aligned genomic re-
gions with integrated parameter estimation. Genome
Biology, 9:R147.
C. Wilson. 2006. Learning phonology with substantive
bias: An experimental and computational study of ve-
lar palatalization. Cognitive Science, 30.5:945?982.
73
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227?235,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing for Transducer Grammars
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, mbansal, adpauls, klein}@cs.berkeley.edu
Abstract
The tree-transducer grammars that arise in
current syntactic machine translation systems
are large, flat, and highly lexicalized. We ad-
dress the problem of parsing efficiently with
such grammars in three ways. First, we
present a pair of grammar transformations
that admit an efficient cubic-time CKY-style
parsing algorithm despite leaving most of the
grammar in n-ary form. Second, we show
how the number of intermediate symbols gen-
erated by this transformation can be substan-
tially reduced through binarization choices.
Finally, we describe a two-pass coarse-to-fine
parsing approach that prunes the search space
using predictions from a subset of the origi-
nal grammar. In all, parsing time reduces by
81%. We also describe a coarse-to-fine prun-
ing scheme for forest-based language model
reranking that allows a 100-fold increase in
beam size while reducing decoding time. The
resulting translations improve by 1.3 BLEU.
1 Introduction
Current approaches to syntactic machine translation
typically include two statistical models: a syntac-
tic transfer model and an n-gram language model.
Recent innovations have greatly improved the effi-
ciency of language model integration through multi-
pass techniques, such as forest reranking (Huang
and Chiang, 2007), local search (Venugopal et al,
2007), and coarse-to-fine pruning (Petrov et al,
2008; Zhang and Gildea, 2008). Meanwhile, trans-
lation grammars have grown in complexity from
simple inversion transduction grammars (Wu, 1997)
to general tree-to-string transducers (Galley et al,
2004) and have increased in size by including more
synchronous tree fragments (Galley et al, 2006;
Marcu et al, 2006; DeNeefe et al, 2007). As a result
of these trends, the syntactic component of machine
translation decoding can now account for a substan-
tial portion of total decoding time. In this paper,
we focus on efficient methods for parsing with very
large tree-to-string grammars, which have flat n-ary
rules with many adjacent non-terminals, as in Fig-
ure 1. These grammars are sufficiently complex that
the purely syntactic pass of our multi-pass decoder is
the compute-time bottleneck under some conditions.
Given that parsing is well-studied in the mono-
lingual case, it is worth asking why MT grammars
are not simply like those used for syntactic analy-
sis. There are several good reasons. The most im-
portant is that MT grammars must do both analysis
and generation. To generate, it is natural to mem-
orize larger lexical chunks, and so rules are highly
lexicalized. Second, syntax diverges between lan-
guages, and each divergence expands the minimal
domain of translation rules, so rules are large and
flat. Finally, we see most rules very few times, so
it is challenging to subcategorize non-terminals to
the degree done in analytic parsing. This paper de-
velops encodings, algorithms, and pruning strategies
for such grammars.
We first investigate the qualitative properties of
MT grammars, then present a sequence of parsing
methods adapted to their broad characteristics. We
give normal forms which are more appropriate than
Chomsky normal form, leaving the rules mostly flat.
We then describe a CKY-like algorithm which ap-
plies such rules efficiently, working directly over the
n-ary forms in cubic time. We show how thoughtful
227
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical normal form (LNF) transformation
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF transformation
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
NP ? DT+NN NNS NP ? DT NN+NNSor
Type-minimizing binarization
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT,NN
DT,NN,NNS 
Minimal binary rules for LNF
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
Figure 1: (a) A synchronous transducer rule has co-
indexed non-terminals on the source and target side. In-
ternal grammatical structure of the target side has been
omitted. (b) The source-side projection of the rule is a
monolingual source-language rule with target-side gram-
mar symbols. (c) A training sentence pair is annotated
with a target-side parse tree and a word alignment, which
license this rule to be extracted.
binarization can further increase parsing speed, and
we present a new coarse-to-fine scheme that uses
rule subsets rather than symbol clustering to build
a coarse grammar projection. These techniques re-
duce parsing time by 81% in aggregate. Finally,
we demonstrate that we can accelerate forest-based
reranking with a language model by pruning with
information from the parsing pass. This approach
enables a 100-fold increase in maximum beam size,
improving translation quality by 1.3 BLEU while
decreasing total decoding time.
2 Tree Transducer Grammars
Tree-to-string transducer grammars consist of
weighted rules like the one depicted in Figure 1.
Each n-ary rule consists of a root symbol, a se-
quence of lexical items and non-terminals on the
source-side, and a fragment of a syntax tree on
the target side. Each non-terminal on the source
side corresponds to a unique one on the target side.
Aligned non-terminals share a grammar symbol de-
rived from a target-side monolingual grammar.
These grammars are learned from word-aligned
sentence pairs annotated with target-side phrase
structure trees. Extraction proceeds by using word
alignments to find correspondences between target-
side constituents and source-side word spans, then
discovering transducer rules that match these con-
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
Figure 2: Transducer grammars are composed of very flat
rules. Above, the histogram shows rule counts for each
rule size among the 332,000 rules that apply to an indi-
vidual 30-word sentence. The size of a rule is the total
number of non-terminals and lexical items in its source-
side yield.
stituent alignments (Galley et al, 2004). Given this
correspondence, an array of extraction procedures
yields rules that are well-suited to machine trans-
lation (Galley et al, 2006; DeNeefe et al, 2007;
Marcu et al, 2006). Rule weights are estimated
by discriminatively combining relative frequency
counts and other rule features.
A transducer grammarG can be projected onto its
source language, inducing a monolingual grammar.
If we weight each rule by the maximumweight of its
projecting synchronous rules, then parsing with this
projected grammar maximizes the translation model
score for a source sentence. We need not even con-
sider the target side of transducer rules until integrat-
ing an n-gram language model or other non-local
features of the target language.
We conduct experiments with a grammar ex-
tracted from 220 million words of Arabic-English
bitext, extracting rules with up to 6 non-terminals. A
histogram of the size of rules applicable to a typical
30-word sentence appears in Figure 2. The grammar
includes 149 grammatical symbols, an augmentation
of the Penn Treebank symbol set. To evaluate, we
decoded 300 sentences of up to 40 words in length
from the NIST05 Arabic-English test set.
3 Efficient Grammar Encodings
Monolingual parsing with a source-projected trans-
ducer grammar is a natural first pass in multi-pass
decoding. These grammars are qualitatively dif-
ferent from syntactic analysis grammars, such as
the lexicalized grammars of Charniak (1997) or the
heavily state-split grammars of Petrov et al (2006).
228
In this section, we develop an appropriate grammar
encoding that enables efficient parsing.
It is problematic to convert these grammars into
Chomsky normal form, which CKY requires. Be-
cause transducer rules are very flat and contain spe-
cific lexical items, binarization introduces a large
number of intermediate grammar symbols. Rule size
and lexicalization affect parsing complexity whether
the grammar is binarized explicitly (Zhang et al,
2006) or implicitly binarized using Early-style inter-
mediate symbols (Zollmann et al, 2006). Moreover,
the resulting binary rules cannot be Markovized to
merge symbols, as in Klein andManning (2003), be-
cause each rule is associated with a target-side tree
that cannot be abstracted.
We also do not restrict the form of rules in the
grammar, a common technique in syntactic machine
translation. For instance, Zollmann et al (2006)
follow Chiang (2005) in disallowing adjacent non-
terminals. Watanabe et al (2006) limit grammars
to Griebach-Normal form. However, general tree
transducer grammars provide excellent translation
performance (Galley et al, 2006), and so we focus
on parsing with all available rules.
3.1 Lexical Normal Form
Sequences of consecutive non-terminals complicate
parsing because they require a search over non-
terminal boundaries when applied to a sentence
span. We transform the grammar to ensure that all
rules containing lexical items (lexical rules) do not
contain sequences of non-terminals. We allow both
unary and binary non-lexical rules.
Let L be the set of lexical items and V the set
of non-terminal symbols in the original grammar.
Then, lexical normal form (LNF) limits productions
to two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? (X1)?(X2)
? = w+(Xiw+)?
Above, all Xi ? V and w+ ? L+. Symbols in
parentheses are optional. The nucleus ? of lexical
rules is a mixed sequence that has lexical items on
each end and no adjacent non-terminals.
Converting a grammar into LNF requires two
steps. In the sequence elimination step, for every
NNP
1
 no d ba una bofetada  DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verd
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
LNF replaces non-terminal sequences in lexical rules
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Non-lexical rules before binarization:
Equivalent binary rules, minimizing symbol count:
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
DT+NN ? DT NN
NP ? DT NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
Figure 3: We transform the original grammar by first
eliminating non-terminal sequences in lexical rules.
Next, we binarize, adding a minimal number of inter-
mediate grammar symbols and binary non-lexical rules.
Finally, anchored LNF further transforms lexical rules
to begin and end with lexical items by introducing ad-
ditional symbols.
lexical rule we replace each sequence of consecutive
non-terminalsX1 . . . Xn with the intermediate sym-
bol X1+. . .+Xn (abbreviated X1:n) and introduce a
non-lexical rule X1+. . .+Xn ? X1 . . . Xn. In the
binarization step, we introduce further intermediate
symbols and rules to binarize all non-lexical rules
in the grammar, including those added by sequence
elimination.
3.2 Non-terminal Binarization
Exactly howwe binarize non-lexical rules affects the
total number of intermediate symbols introduced by
the LNF transformation.
Binarization involves selecting a set of symbols
that will allow us to assemble the right-hand side
X1 . . . Xn of every non-lexical rule using binary
productions. This symbol set must at least include
the left-hand side of every rule in the grammar
(lexical and non-lexical), including the intermediate
229
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+ Figure 4: The number of non-terminal symbols intro-
duced to the grammar through LNF binarization depends
upon the policy for binarizing type sequences. This ex-
periment shows results from transforming a grammar that
has already been filtered for a particular short sentence.
Both the greedy and optimal binarizations use far fewer
symbols than naive binarizations.
symbols X1:n introduced by sequence elimination.
To ensure that a symbol sequence X1 . . . Xn can
be constructed, we select a split point k and add in-
termediate types X1:k and Xk+1:n to the grammar.
We must also ensure that the sequences X1 . . . Xk
and Xk+1 . . . Xn can be constructed. As baselines,
we used left-branching (where k = 1 always) and
right-branching (where k = n? 1) binarizations.
We also tested a greedy binarization approach,
choosing k to minimize the number of grammar
symbols introduced. We first try to select k such that
both X1:k and Xk+1:n are already in the grammar.
If no such k exists, we select k such that one of the
intermediate types generated is already used. If no
such k exists again, we choose k = ?12n
?. This pol-
icy only creates new intermediate types when nec-
essary. Song et al (2008) propose a similar greedy
approach to binarization that uses corpus statistics to
select common types rather than explicitly reusing
types that have already been introduced.
Finally, we computed an optimal binarization that
explicitly minimizes the number of symbols in the
resulting grammar. We cast the minimization as an
integer linear program (ILP). Let V be the set of
all base non-terminal symbols in the grammar. We
introduce an indicator variable TY for each symbol
Y ? V + to indicate that Y is used in the grammar.
Y can be either a base non-terminal symbol Xi or
an intermediate symbol X1:n. We also introduce in-
dicators AY,Z for each pairs of symbols, indicating
that both Y and Z are used in the grammar. Let
L ? V + be the set of left-hand side symbols for
all lexical and non-lexical rules already in the gram-
mar. Let R be the set of symbol sequences on the
right-hand side of all non-lexical rules. Then, the
ILP takes the form:
min ?
Y ?V +
TY (1)
s.t. TY = 1 ? Y ? L (2)
1 ??
k
AX1:k,Xk+1:n ? X1 . . . Xn ? R (3)
TX1:n ?
?
k
AX1:k,Xk+1:n ? X1:n (4)
AY,Z ? TY , AY,Z ? TZ ? Y, Z (5)
The solution to this ILP indicates which symbols
appear in a minimal binarization. Equation 1 explic-
itly minimizes the number of symbols. Equation 2
ensures that all symbols already in the grammar re-
main in the grammar.
Equation 3 does not require that a symbol repre-
sent the entire right-hand side of each non-lexical
rule, but does ensure that each right-hand side se-
quence can be built from two subsequence symbols.
Equation 4 ensures that any included intermediate
type can also be built from two subsequence types.
Finally, Equation 5 ensures that if a pair is used, each
member of the pair is included. This program can be
optimized with an off-the-shelf ILP solver.1
Figure 4 shows the number of intermediate gram-
mar symbols needed for the four binarization poli-
cies described above for a short sentence. Our ILP
solver could only find optimal solutions for very
short sentences (which have small grammars after
relativization). Because greedy requires very little
time to compute and generates symbol counts that
are close to optimal when both can be computed, we
use it for our remaining experiments.
3.3 Anchored Lexical Normal Form
We also consider a further grammar transformation,
anchored lexical normal form (ALNF), in which the
yield of lexical rules must begin and end with a lex-
ical item. As shown in the following section, ALNF
improves parsing performance over LNF by shifting
work from lexical rule applications to non-lexical
1We used lp solve: http://sourceforge.net/projects/lpsolve.
230
rule applications. ALNF consists of rules with the
following two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? w+(Xiw+)?
To convert a grammar into ALNF, we first transform
it into LNF, then introduce additional binary rules
that split off non-terminal symbols from the ends of
lexical rules, as shown in Figure 3.
4 Efficient CKY Parsing
We now describe a CKY-style parsing algorithm for
grammars in LNF. The dynamic program is orga-
nized into spans Sij and computes the Viterbi score
w(i, j,X) for each edge Sij [X], the weight of the
maximum parse over words i+1 to j, rooted at sym-
bol X . For each Sij , computation proceeds in three
phases: binary, lexical, and unary.
4.1 Applying Non-lexical Binary Rules
For a span Sij , we first apply the binary non-lexical
rules just as in standard CKY, computing an interme-
diate Viterbi score wb(i, j,X). Let ?r be the weight
of rule r. Then, wb(i, j,X) =
max
r=X?X1X2
?r
j?1max
k=i+1
w(i, k,X1) ? w(k, j,X2).
The quantitiesw(i, k,X1) andw(k, j,X2) will have
already been computed by the dynamic program.
The work in this phase is cubic in sentence length.
4.2 Applying Lexical Rules
On the other hand, lexical rules in LNF can be ap-
plied without binarization, because they only apply
to particular spans that contain the appropriate lexi-
cal items. For a given Sij , we first compute all the le-
gal mappings of each rule onto the span. A mapping
consists of a correspondence between non-terminals
in the rule and subspans of Sij . In practice, there
is typically only one way that a lexical rule in LNF
can map onto a span, because most lexical items will
appear only once in the span.
Let m be a legal mapping and r its corresponding
rule. Let S(i)k` [X] be the edge mapped to the ith non-terminal of r underm, and ?r the weight of r. Then,
wl(i, j,X) = maxm ?r
?
S(i)k` [X]
w(k, `,X).
Again, w(k, `,X) will have been computed by the
dynamic program. Assuming only a constant num-
ber of mappings per rule per span, the work in this
phase is quadratic. We can then merge wl and wb:
w(i, j,X) = max(wl(i, j,X), wb(i, j,X)).
To efficiently compute mappings, we store lexi-
cal rules in a trie (or suffix array) ? a searchable
graph that indexes rules according to their sequence
of lexical items and non-terminals. This data struc-
ture has been used similarly to index whole training
sentences for efficient retrieval (Lopez, 2007). To
find all rules that map onto a span, we traverse the
trie using depth-first search.
4.3 Applying Unary Rules
Unary non-lexical rules are applied after lexical
rules and non-lexical binary rules.
w(i, j,X) = max
r:r=X?X1
?rw(i, j,X1).
While this definition is recursive, we allow only one
unary rule application per symbol X at each span
to prevent infinite derivations. This choice does not
limit the generality of our algorithm: chains of unar-
ies can always be collapsed via a unary closure.
4.4 Bounding Split Points for Binary Rules
Non-lexical binary rules can in principle apply to
any span Sij where j ? i ? 2, using any split point
k such that i < k < j. In practice, however, many
rules cannot apply to many (i, k, j) triples because
the symbols for their children have not been con-
structed successfully over the subspans Sik and Skj .
Therefore, the precise looping order over rules and
split points can influence computation time.
We found the following nested looping order for
the binary phase of processing an edge Sij [X] gave
the fastest parsing times for these grammars:
1. Loop over symbols X1 for the left child
2. Loop over all rules X ? X1X2 containing X1
3. Loop over split points k : i < k < j
4. Update wb(i, j,X) as necessary
This looping order allows for early stopping via
additional bookkeeping in the algorithm. We track
the following statistics as we parse:
231
Grammar Bound checks Parsing time
LNF no 264
LNF yes 181
ALNF yes 104
Table 1: Adding bound checks to CKY and transforming
the grammar from LNF to anchored LNF reduce parsing
time by 61% for 300 sentences of length 40 or less. No
approximations have been applied, so all three scenarios
produce no search errors. Parsing time is in minutes.
minEND(i,X), maxEND(i,X): The minimum and
maximum position k for which symbol X was
successfully built over Sik.
minSTART(j,X), maxSTART(j,X): The minimum
and maximum position k for which symbol X
was successfully built over Skj .
We then bound k by mink and maxk in the inner
loop using these statistics. If ever mink > maxk,
then the loop is terminated early.
1. set mink = i+ 1,maxk = j ? 1
2. loop over symbols X1 for the left child
mink = max(mink,minEND(i,X1))
maxk = min(maxk,maxEND(i,X1))
3. loop over rules X ? X1X2
mink = max(mink,minSTART(j,X2))
maxk = min(maxk,maxSTART(j,X2))
4. loop over split points k : mink ? k ? maxk
5. update wb(i, j,X) as necessary
In this way, we eliminate unnecessary work by
avoiding split points that we know beforehand can-
not contribute to wb(i, j,X).
4.5 Parsing Time Results
Table 1 shows the decrease in parsing time from in-
cluding these bound checks, as well as switching
from lexical normal form to anchored LNF.
Using ALNF rather than LNF increases the num-
ber of grammar symbols and non-lexical binary
rules, but makes parsing more efficient in three
ways. First, it decreases the number of spans for
which a lexical rule has a legal mapping. In this way,
ALNF effectively shifts work from the lexical phase
to the binary phase. Second, ALNF reduces the time
spent searching the trie for mappings, because the
first transition into the trie must use an edge with a
lexical item. Finally, ALNF improves the frequency
that, when a lexical rule matches a span, we have
successfully built every edge Sk`[X] in the mapping
for that rule. This frequency increases from 45% to
96% with ALNF.
5 Coarse-to-Fine Search
We now consider two coarse-to-fine approximate
search procedures for parsing with these grammars.
Our first approach clusters grammar symbols to-
gether during the coarse parsing pass, following
work in analytic parsing (Charniak and Caraballo,
1998; Petrov and Klein, 2007). We collapse all
intermediate non-terminal grammar symbols (e.g.,
NP) to a single coarse symbol X, while pre-terminal
symbols (e.g., NN) are hand-clustered into 7 classes
(nouns, verbals, adjectives, punctuation, etc.). We
then project the rules of the original grammar into
this simplified symbol set, weighting each rule of
the coarse grammar by the maximum weight of any
rule that mapped onto it.
In our second and more successful approach, we
select a subset of grammar symbols. We then in-
clude only and all rules that can be built using those
symbols. Because the grammar includes many rules
that are compositions of smaller rules, parsing with
a subset of the grammar still provides meaningful
scores that can be used to prune base grammar sym-
bols while parsing under the full grammar.
5.1 Symbol Selection
To compress the grammar, we select a small sub-
set of symbols that allow us to retain as much of
the original grammar as possible. We use a voting
scheme to select the symbol subset. After conver-
sion to LNF (or ALNF), each lexical rule in the orig-
inal grammar votes for the symbols that are required
to build it. A rule votes as many times as it was ob-
served in the training data to promote frequent rules.
We then select the top nl symbols by vote count and
include them in the coarse grammar C.
We would also like to retain as many non-lexical
rules from the original grammar as possible, but the
right-hand side of each rule can be binarized in many
ways. We again use voting, but this time each non-
232
Pruning Minutes Model score BLEU
No pruning 104 60,179 44.84
Clustering 79 60,179 44.84
Subsets 50 60,163 44.82
Table 2: Coarse-to-fine pruning speeds up parsing time
with minimal effect on either model score or translation
quality. The coarse grammar built using symbol subsets
outperforms clustering grammar symbols, reducing pars-
ing time by 52%. These experiments do not include a
language model.
lexical rule votes for its yield, a sequence of sym-
bols. We select the top nu symbol sequences as the
set R of right-hand sides.
Finally, we augment the symbol set of C with in-
termediate symbols that can construct all sequences
in R, using only binary rules. This step again re-
quires choosing a binarization for each sequence,
such that a minimal number of additional symbols is
introduced. We use the greedy approach from Sec-
tion 3.2. We then include in C all rules from the
original grammar that can be built from the symbols
we have chosen. Surprisingly, we are able to re-
tain 76% of the grammar rules while excluding 92%
of the grammar symbols2, which speeds up parsing
substantially.
5.2 Max Marginal Thresholding
We parse first with the coarse grammar to find the
Viterbi derivation score for each edge Sij [X]. We
then perform a Viterbi outside pass over the chart,
like a standard outside pass but replacing ? with
max (Goodman, 1999). The product of an edge?s
Viterbi score and its Viterbi outside score gives a
max marginal, the score of the maximal parse that
uses the edge.
We then prune away regions of the chart that de-
viate in their coarse max marginal from the global
Viterbi score by a fixed margin tuned on a develop-
ment set. Table 2 shows that both methods of con-
structing a coarse grammar are effective in pruning,
but selecting symbol subsets outperformed the more
typical clustering approach, reducing parsing time
by an additional factor of 2.
2We used nl of 500 and nu of 4000 for experiments. These
parameters were tuned on a development set.
6 Language Model Integration
Large n-gram language models (LMs) are critical
to the performance of machine translation systems.
Recent innovations have managed the complexity
of LM integration using multi-pass architectures.
Zhang and Gildea (2008) describes a coarse-to-fine
approach that iteratively increases the order of the
LM. Petrov et al (2008) describes an additional
coarse-to-fine hierarchy over language projections.
Both of these approaches integrate LMs via bottom-
up dynamic programs that employ beam search. As
an alternative, Huang and Chiang (2007) describes a
forest-based reranking algorithm called cube grow-
ing, which also employs beam search, but focuses
computation only where necessary in a top-down
pass through a parse forest.
In this section, we show that the coarse-to-fine
idea of constraining each pass using marginal pre-
dictions of the previous pass also applies effectively
to cube growing. Max marginal predictions from the
parse can substantially reduce LM integration time.
6.1 Language Model Forest Reranking
Parsing produces a forest of derivations, where each
edge in the forest holds its Viterbi (or one-best)
derivation under the transducer grammar. In forest
reranking via cube growing, edges in the forest pro-
duce k-best lists of derivations that are scored by
both the grammar and an n-gram language model.
Using ALNF, each edge must first generate a k-best
list of derivations that are not scored by the language
model. These derivations are then flattened to re-
move the binarization introduced by ALNF, so that
the resulting derivations are each rooted by an n-
ary rule r from the original grammar. The leaves of
r correspond to sub-edges in the chart, which are
recursively queried for their best language-model-
scored derivations. These sub-derivations are com-
bined by r, and new n-grams at the edges of these
derivations are scored by the language model.
The language-model-scored derivations for the
edge are placed on a priority queue. The top of
the priority queue is repeatedly removed, and its
successors added back on to the queue, until k
language-model-scored derivations have been dis-
covered. These k derivations are then sorted and
233
Pruning Max TM LM Total Inside Outside LM Total
strategy beam BLEU score score score time time time time
No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346
CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239
CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241
CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253
Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220
million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.
Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar. Coarse-to-fine reranking used
max marginals to constrain the reranking pass. Coarse-to-fine parse + rerank employed both of these approximations.
supplied to parent edges upon request.3
6.2 Coarse-to-Fine Parsing
Even with this efficient reranking algorithm, inte-
grating a language model substantially increased de-
coding time and memory use. As a baseline, we
reranked using a small fixed-size beam of 20 deriva-
tions at each edge. Larger beams exceeded the mem-
ory of our hardware. Results appear in Table 3.
Coarse-to-fine parsing before LM integration sub-
stantially improved language model reranking time.
By pruning the chart with max marginals from the
coarse symbol subset grammar from Section 5, we
were able to rerank with beams of length 200, lead-
ing to a 0.8 BLEU increase and a 31% reduction in
total decoding time.
6.3 Coarse-to-Fine Forest Reranking
We realized similar performance and speed bene-
fits by instead pruning with max marginals from the
full grammar. We found that LM reranking explored
many edges with low max marginals, but used few
of them in the final decoder output. Following the
coarse-to-fine paradigm, we restricted the reranker
to edges with a max marginal above a fixed thresh-
old. Furthermore, we varied the beam size of each
edge based on the parse. Let ?m be the ratio of
the max marginal for edge m to the global Viterbi
derivation for the sentence. We used a beam of size?
k ? 2ln?m? for each edge.
Computing max marginals under the full gram-
mar required an additional outside pass over the full
parse forest, adding substantially to parsing time.
3Huang and Chiang (2007) describes the cube growing al-
gorithm in further detail, including the precise form of the suc-
cessor function for derivations.
However, soft coarse-to-fine pruning based on these
max marginals also allowed for beams up to length
200, yielding a 1.0 BLEU increase over the baseline
and a 30% reduction in total decoding time.
We also combined the coarse-to-fine parsing ap-
proach with this soft coarse-to-fine reranker. Tiling
these approximate search methods allowed another
10-fold increase in beam size, further improving
BLEU while only slightly increasing decoding time.
7 Conclusion
As translation grammars increase in complexity
while innovations drive down the computational cost
of language model integration, the efficiency of the
parsing phase of machine translation decoding is be-
coming increasingly important. Our grammar nor-
mal form, CKY improvements, and symbol subset
coarse-to-fine procedure reduced parsing time for
large transducer grammars by 81%.
These techniques also improved forest-based lan-
guage model reranking. A full decoding pass with-
out any of our innovations required 511 minutes us-
ing only small beams. Coarse-to-fine pruning in
both the parsing and language model passes allowed
a 100-fold increase in beam size, giving a perfor-
mance improvement of 1.3 BLEU while decreasing
total decoding time by 50%.
Acknowledgements
This work was enabled by the Information Sci-
ences Institute Natural Language Group, primarily
through the invaluable assistance of Jens Voeckler,
and was supported by the National Science Founda-
tion (NSF) under grant IIS-0643742.
234
References
Eugene Charniak and Sharon Caraballo. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
In Computational Linguistics.
Eugene Charniak. 1997. Statistical techniques for natu-
ral language parsing. In National Conference on Arti-
ficial Intelligence.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
The Annual Conference of the Association for Compu-
tational Linguistics.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the Association for
Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In The Conference on Empiri-
cal Methods in Natural Language Processing.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In The Annual Conference of
the Association for Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In The Conference on Empirical
Methods in Natural Language Processing.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In The Con-
ference on Empirical Methods in Natural Language
Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In The Annual Conference
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In The Annual Conference of the Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In North American Chapter of the Associ-
ation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In The Statistical Machine Translation
Workshop at the North American Association for Com-
putational Linguistics Conference.
235
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 557?565,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Search for Parsing
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
Berkeley, CA 94720, USA
{adpauls,klein}@cs.berkeley.edu
Abstract
Both coarse-to-fine and A? parsing use simple
grammars to guide search in complex ones.
We compare the two approaches in a com-
mon, agenda-based framework, demonstrat-
ing the tradeoffs and relative strengths of each
method. Overall, coarse-to-fine is much faster
for moderate levels of search errors, but be-
low a certain threshold A? is superior. In addi-
tion, we present the first experiments on hier-
archical A? parsing, in which computation of
heuristics is itself guided by meta-heuristics.
Multi-level hierarchies are helpful in both ap-
proaches, but are more effective in the coarse-
to-fine case because of accumulated slack in
A? heuristics.
1 Introduction
The grammars used by modern parsers are ex-
tremely large, rendering exhaustive parsing imprac-
tical. For example, the lexicalized grammars of
Collins (1997) and Charniak (1997) and the state-
split grammars of Petrov et al (2006) are all
too large to construct unpruned charts in memory.
One effective approach is coarse-to-fine pruning, in
which a small, coarse grammar is used to prune
edges in a large, refined grammar (Charniak et al,
2006). Indeed, coarse-to-fine is even more effective
when a hierarchy of successive approximations is
used (Charniak et al, 2006; Petrov and Klein, 2007).
In particular, Petrov and Klein (2007) generate a se-
quence of approximations to a highly subcategorized
grammar, parsing with each in turn.
Despite its practical success, coarse-to-fine prun-
ing is approximate, with no theoretical guarantees
on optimality. Another line of work has explored
A? search methods, in which simpler problems are
used not for pruning, but for prioritizing work in
the full search space (Klein and Manning, 2003a;
Haghighi et al, 2007). In particular, Klein and Man-
ning (2003a) investigated A? for lexicalized parsing
in a factored model. In that case, A? vastly im-
proved the search in the lexicalized grammar, with
provable optimality. However, their bottleneck was
clearly shown to be the exhaustive parsing used to
compute the A? heuristic itself. It is not obvious,
however, how A? can be stacked in a hierarchical or
multi-pass way to speed up the computation of such
complex heuristics.
In this paper, we address three open questions
regarding efficient hierarchical search. First, can
a hierarchy of A? bounds be used, analogously to
hierarchical coarse-to-fine pruning? We show that
recent work in hierarchical A? (Felzenszwalb and
McAllester, 2007) can naturally be applied to both
the hierarchically refined grammars of Petrov and
Klein (2007) as well as the lexicalized grammars
of Klein and Manning (2003a). Second, what are
the tradeoffs between coarse-to-fine pruning and A?
methods? We show that coarse-to-fine is generally
much faster, but at the cost of search errors.1 Below
a certain search error rate, A? is faster and, of course,
optimal. Finally, when and how, qualitatively, do
these methods fail? A? search?s work grows quickly
as the slack increases between the heuristic bounds
and the true costs. On the other hand, coarse-to-fine
prunes unreliably when the approximating grammar
1In this paper, we consider only errors made by the search
procedure, not modeling errors.
557
Name Rule Priority
IN r : wr I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? I(At, i, j) : ?A = ?B + ?C + wr ?A + h(A, i, j)
Table 1: Deduction rule for A? parsing. The items on the left of the ? indicate what edges must be present on the
chart and what rule can be used to combine them, and the item on the right is the edge that may be added to the agenda.
The weight of each edge appears after the colon. The rule r is A ? B C.
is very different from the target grammar. We em-
pirically demonstrate both failure modes.
2 Parsing algorithms
Our primary goal in this paper is to compare hi-
erarchical A? (HA?) and hierarchical coarse-to-fine
(CTF) pruning methods. Unfortunately, these two
algorithms are generally deployed in different archi-
tectures: CTF is most naturally implemented using
a dynamic program like CKY, while best-first al-
gorithms like A? are necessarily implemented with
agenda-based parsers. To facilitate comparison, we
would like to implement them in a common architec-
ture. We therefore work entirely in an agenda-based
setting, noting that the crucial property of CTF is
not the CKY order of exploration, but the pruning
of unlikely edges, which can be equally well done
in an agenda-based parser. In fact, it is possible to
closely mimic dynamic programs like CKY using a
best-first algorithm with a particular choice of prior-
ities; we discuss this in Section 2.3.
While a general HA? framework is presented in
Felzenszwalb and McAllester (2007), we present
here a specialization to the parsing problem. We first
review the standard agenda-driven search frame-
work and basic A? parsing before generalizing to
HA?.
2.1 Agenda-Driven Parsing
A non-hierarchical, best-first parser takes as input a
PCFG G (with root symbol R), a priority function
p(?) and a sentence consisting of terminals (words)
T0 . . .Tn?1. The parser?s task is to find the best
scoring (Viterbi) tree structure which is rooted at R
and spans the input sentence. Without loss of gen-
erality, we consider grammars in Chomsky normal
form, so that each non-terminal rule in the grammar
has the form r = A ? B C with weight wr. We
assume that weights are non-negative (e.g. negative
log probabilities) and that we wish to minimize the
sum of the rule weights.
A
CB
B C
?
B
A
?
A
=
?
B
+?
C
+w
r
p=?
A
+h(A,i,j)
i k k j ji
w
r
?
C
Figure 1: Deduction rule for A? depicted graphically.
Items to the left of the arrow indicate edges and rules that
can be combined to produce the edge to the right of the ar-
row. Edges are depicted as complete triangles. The value
inside an edge represents the weight of that edge. Each
new edge is assigned the priority written above the arrow
when added to the agenda.
The objects in an agenda-based parser are edges
e = I(X, i, j), also called items, which represent
parses spanning i to j and rooted at symbol X. We
denote edges as triangles, as in Figure 1. At all
times, edges have scores ?e, which are estimates
of their Viterbi inside probabilities (also called path
costs). These estimates improve over time as new
derivations are considered, and may or may not be
correct at termination, depending on the properties
of p. The parser maintains an agenda (a priority
queue of edges), as well as a chart (or closed list
in search terminology) of edges already processed.
The fundamental operation of the algorithm is to pop
the best (lowest) priority edge e from the agenda,
put it into the chart, and enqueue any edges which
can be built by combining e with other edges in the
chart. The combination of two adjacent edges into
a larger edge is shown graphically in Figure 1 and
as a weighted deduction rule in Table 1 (Shieber et
al., 1995; Nederhof, 2003). When an edge a is built
from adjacent edges b and c and a rule r, its cur-
rent score ?a is compared to ?b + ?c + wr and up-
dated if necessary. To allow reconstruction of best
parses, backpointers are maintained in the standard
way. The agenda is initialized with I(Ti, i, i + 1)
558
for i = 0 . . . n ? 1. The algorithm terminates when
I(R, 0, n) is popped off the queue.
Priorities are in general different than weights.
Whenever an edge e?s score changes, its priority
p(e), which may or may not depend on its score,
may improve. Edges are promoted accordingly in
the agenda if their priorities improve. In the sim-
plest case, the priorities are simply the ?e estimates,
which gives a correct uniform cost search wherein
the root edge is guaranteed to have its correct inside
score estimate at termination (Caraballo and Char-
niak, 1996).
A? parsing (Klein and Manning, 2003b) is a spe-
cial case of such an agenda-driven parser in which
the priority function p takes the form p(e) = ?e +
h(e), where e = I(X, i, j) and h(?) is some approx-
imation of e?s Viterbi outside cost (its completion
cost). If h is consistent, then the A? algorithm guar-
antees that whenever an edge comes off the agenda,
its weight is its true Viterbi inside cost. In particular,
this guarantee implies that the first edge represent-
ing the root I(R, 0, n) will be scored with the true
Viterbi score for the sentence.
2.2 Hierarchical A?
In the standard A? case the heuristics are assumed
to come from a black box. For example, Klein and
Manning (2003b) precomputes most heuristics of-
fline, while Klein and Manning (2003a) solves sim-
pler parsing problems for each sentence. In such
cases, the time spent to compute heuristics is often
non-trivial. Indeed, it is typical that effective heuris-
tics are themselves expensive search problems. We
would therefore like to apply A? methods to the
computation of the heuristics themselves. Hierar-
chical A? allows us to do exactly that.
Formally, HA? takes as input a sentence and a se-
quence (or hierarchy) of m + 1 PCFGs G0 . . .Gm,
where Gm is the target grammar and G0 . . .Gm?1
are auxiliary grammars. Each grammar Gt has an in-
ventory of symbols ?t, hereafter denoted with capi-
tal letters. In particular, each grammar has a distin-
guished terminal symbol Tit for each word Ti in the
input and a root symbol Rt.
The grammars G0 . . .Gm must form a hierarchy in
which Gt is a relaxed projection of Gt+1. A grammar
Gt?1 is a projection of Gt if there exists some onto
function pit : ?t $? ?t?1 defined for all symbols in
Agenda
Chart
I(NP, 3, 5)
O(VP, 4, 8)
I(NN, 2, 3)
.
.
.
.
.
I
I
I
O
O
O
G1
G0
G2
Figure 3: Operation of hierarchical A? parsing. An edge
comes off the agenda and is added to the chart (solid line).
From this edge, multiple new edges can be constructed
and added to the agenda (dashed lines). The chart is com-
posed of two subcharts for each grammar in the hierar-
chy: an inside chart (I) and an outside chart (O).
Gt; hereafter, we will use A?t to represent pit(At). A
projection is a relaxation if, for every rule r = At ?
Bt Ct with weight wr the projection r? = pit(r) =
A?t ? B?tC?t has weight wr? ? wr in Gt?1. Given
a target grammar Gm and a projection function pim,
it is easy to construct a relaxed projection Gm?1 by
minimizing over rules collapsed by pim:
wr? = min
r?Gm:pim(r)=r?
wr
Given a series of projection functions pi1 . . .pim,
we can construct relaxed projections by projecting
Gm to Gm?1, then Gm?1 to Gm?2 and so on. Note
that by construction, parses in a relaxed projection
give lower bounds on parses in the target grammar
(Klein and Manning, 2003b).
HA? differs from standard A? in two ways.
First, it tracks not only standard inside edges
e = I(X, i, j) which represent derivations of
X ? Ti . . .Tj , but also outside edges o =
O(X, i, j) which represent derivations of R ?
T0 . . .Ti?1 X Tj+1 . . .Tn. For example, where
I(VP, 0, 3) denotes trees rooted at VP covering the
span [0, 3], O(VP, 0, 3) denotes the derivation of the
?rest? of the structure to the root. Where inside
edges e have scores ?e which represent (approxima-
tions of) their Viterbi inside scores, outside edges o
have scores ?o which are (approximations of) their
Viterbi outside scores. When we need to denote the
inside version of an outside edge, or the reverse, we
write o = e?, etc.
559
Name Rule Priority
IN-BASE O(T?it , i, i + 1) : ?T ? I(Tit, i, i + 1) : 0 ?TIN r : wr O(A?t, i, j) : ?A? I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? I(At, i, j) : ?A = ?B + ?C + wr ?A + ?A?
OUT-BASE I(Rt, 0, n) : ?R ? O(Rt, 0, n) : 0 ?R
OUT-L r : wr O(At, i, j) : ?A I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? O(Bt, i, k) : ?B = ?A + ?C + wr ?B + ?B
OUT-R r : wr O(At, i, j) : ?A I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? O(Ct, k, j) : ?C = ?A + ?B + wr ?C + ?C
Table 2: Deduction rules for HA?. The rule r is in all cases At ? Bt Ct.
A
CB
A'
B C
?
B
A
?
A
=
?
B
+?
C
+w
r
?
A'
p=?
A
+?
A'
i k k j ji
IN
i j
p
=
?
B
+
?
B
?
B
=
?
A
+?
C
+w
r
B
p
=
?
C
+
?
C
?
C
=
?
A
+?
B
+w
r
C
i k
k j
O
U
T
-
L
O
U
T
-
R
A
CB
A
B C
?
C
?
B
i k k
j
w
r
w
r
i j n0
0
n
?
A
?
C
n
0
0 n
(a)
(b)
Figure 2: Non-base case deduction rules for HA? depicted graphically. (a) shows the rule used to build inside edges
and (b) shows the rules to build outside edges. Inside edges are depicted as complete triangles, while outside edges
are depicted as chevrons. An edge from a previous level in the hierarchy is denoted with dashed lines.
The second difference is that HA? tracks items
from all levels of the hierarchy on a single, shared
agenda, so that all items compete (see Figure 3).
While there is only one agenda, it is useful to imag-
ine several charts, one for each type of edge and each
grammar level. In particular, outside edges from one
level of the hierarchy are the source of completion
costs (heuristics) for inside edges at the next level.
The deduction rules for HA? are given in Table 2
and represented graphically in Figure 2. The IN rule
(a) is the familiar deduction rule from standard A?:
we can combine two adjacent inside edges using a
binary rule to form a new inside edge. The new twist
is that because heuristics (scores of outside edges
from the previous level) are also computed on the
fly, they may not be ready yet. Therefore, we cannot
carry out this deduction until the required outside
edge is present in the previous level?s chart. That
is, fine inside deductions wait for the relevant coarse
outside edges to be popped. While coarse outside
edges contribute to priorities of refined inside scores
(as heuristic values), they do not actually affect the
inside scores of edges (again just like basic A?).
In standard A?, we begin with all terminal edges
on the agenda. However, in HA?, we cannot en-
queue refined terminal edges until their outside
scores are ready. The IN-BASE rule specifies the
base case for a grammar Gt: we cannot begin un-
til the outside score for the terminal symbol T is
ready in the coarser grammar Gt?1. The initial queue
contains only the most abstract level?s terminals,
I(Ti0, i, i + 1). The entire search terminates when
the inside edge I(Rm, 0, n), represting root deriva-
tions in the target grammar, is dequeued.
The deductions which assemble outside edges are
less familiar from the standard A? algorithm. These
deductions take larger outside edges and produce
smaller sub-edges by linking up with inside edges,
as shown in Figure 2(b). The OUT-BASE rule states
that an outside pass for Gt can be started if the in-
side score of the root symbol for that level Rt has
been computed. The OUT-L and OUT-R rules are
560
the deduction rules for building outside edges. OUT-
L states that, given an outside edge over the span
[i, j] and some inside edge over [i, k], we may con-
struct an outside edge over [k, j]. For outside edges,
the score reflects an estimate of the Viterbi outside
score.
As in standard A?, inside edges are placed on the
agenda with a priority equal to their path cost (inside
score) and some estimate of their completion cost
(outside score), now taken from the previous projec-
tion rather than a black box. Specifically, the priority
function takes the form p(e) = ?e + ?e?? , where e??
is the outside version of e one level previous in the
hierarchy.
Outside edges also have priorities which combine
path costs with a completion estimate, except that
the roles of inside and outside scores are reversed:
the path cost for an outside edge o is its (outside)
score ?o, while the completion cost is some estimate
of the inside score, which is the weight ?e of o?s
complementary edge e = o?. Therefore, p(o) = ?o+
?o?.
Note that inside edges combine their inside score
estimates with outside scores from a previous level
(a lower bound), while outside edges combine their
outside score estimates with inside scores from the
same level, which are already available. Felzen-
szwalb and McAllester (2007) show that these
choices of priorities have the same guarantee as stan-
dard A?: whenever an inside or outside edge comes
off the queue, its path cost is optimal.
2.3 Agenda-driven Coarse-to-Fine Parsing
We can always replace the HA? priority function
with an alternate priority function of our choosing.
In doing so, we may lose the optimality guarantees
of HA?, but we may also be able to achieve sig-
nificant increases in performance. We do exactly
this in order to put CTF pruning in an agenda-based
framework. An agenda-based implementation al-
lows us to put CTF on a level playing field with HA?,
highlighting the effectiveness of the various parsing
strategies and normalizing their implementations.
First, we define coarse-to-fine pruning. In stan-
dard CTF, we exhaustively parse in each projection
level, but skip edges whose projections in the previ-
ous level had sufficiently low scores. In particular,
an edge e in the grammar Gt will be skipped entirely
if its projection e? in Gt?1 had a low max marginal:
?e?? + ?e? , that is, the score of the best tree contain-
ing e? was low compared to the score best overall
root derivation ?R? . Formally, we prune all e where
?e?? + ?e? > ?R? + ? for some threshold ? .
The priority function we use to implement CTF in
our agenda-based framework is:
p(e) = ?e
p(o) =
8
><
>:
? ?o + ?o? >
?Rt + ?t
?o + ?o? otherwise
Here, ?t ? 0 is a user-defined threshold for level
t and ?Rt is the inside score of the root for gram-
mar Gt. These priorities lead to uniform-cost explo-
ration for inside edges and completely suppress out-
side edges which would have been pruned in stan-
dard CTF. Note that, by the construction of the IN
rule, pruning an outside edge also prunes all inside
edges in the next level that depend on it; we there-
fore prune slightly earlier than in standard CTF. In
any case, this priority function maintains the set of
states explored in CKY-based CTF, but does not nec-
essarily explore those states in the same order.
3 Experiments
3.1 Evaluation
Our focus is parsing speed. Thus, we would ideally
evaluate our algorithms in terms of CPU time. How-
ever, this measure is problematic: CPU time is influ-
enced by a variety of factors, including the architec-
ture of the hardware, low-level implementation de-
tails, and other running processes, all of which are
hard to normalize.
It is common to evaluate best-first parsers in terms
of edges popped off the agenda. This measure is
used by Charniak et al (1998) and Klein and Man-
ning (2003b). However, when edges from grammars
of varying size are processed on the same agenda,
the number of successor edges per edge popped
changes depending on what grammar the edge was
constructed from. In particular, edges in more re-
fined grammars are more expensive than edges in
coarser grammars. Thus, our basic unit of measure-
ment will be edges pushed onto the agenda. We
found in our experiments that this was well corre-
lated with CPU time.
561
UCS A*
3
HA*
3
HA*
3-5
HA*
0-5
CTF
3
CTF
3-5
CTF
0-5
Edges 
pushed
 (billio
ns)
0
100
200
300
400 424
86.6
78.2
58.8 60.1
8.83 7.12
1.98
Figure 4: Efficiency of several hierarchical parsing algo-
rithms, across the test set. UCS and all A? variants are
optimal and thus make no search errors. The CTF vari-
ants all make search errors on about 2% of sentences.
3.2 State-Split Grammars
We first experimented with the grammars described
in Petrov et al (2006). Starting with an X-Bar gram-
mar, they iteratively refine each symbol in the gram-
mar by adding latent substates via a split-merge pro-
cedure. This training procedure creates a natural hi-
erarchy of grammars, and is thus ideal for our pur-
poses. We used the Berkeley Parser2 to train such
grammars on sections 2-21 of the Penn Treebank
(Marcus et al, 1993). We ran 6 split-merge cycles,
producing a total of 7 grammars. These grammars
range in size from 98 symbols and 8773 rules in the
unsplit X-Bar grammar to 1139 symbols and 973696
rules in the 6-split grammar. We then parsed all sen-
tences of length ? 30 of section 23 of the Treebank
with these grammars. Our ?target grammar? was in
all cases the largest (most split) grammar. Our pars-
ing objective was to find the Viterbi derivation (i.e.
fully refined structure) in this grammar. Note that
this differs from the objective used by Petrov and
Klein (2007), who use a variational approximation
to the most probable parse.
3.2.1 A? versus HA?
We first compare HA? with standard A?. In A? as
presented by Klein and Manning (2003b), an aux-
iliary grammar can be used, but we are restricted
to only one and we must compute inside and out-
side estimates for that grammar exhaustively. For
our single auxiliary grammar, we chose the 3-split
grammar; we found that this grammar provided the
best overall speed.
For HA?, we can include as many or as few
auxiliary grammars from the hierarchy as desired.
Ideally, we would find that each auxiliary gram-
2http://berkeleyparser.googlecode.com
mar increases performance. To check this, we per-
formed experiments with all 6 auxiliary grammars
(0-5 split); the largest 3 grammars (3-5 split); and
only the 3-split grammar.
Figure 4 shows the results of these experiments.
As a baseline, we also compare with uniform cost
search (UCS) (A? with h = 0 ). A? provides a
speed-up of about a factor of 5 over this UCS base-
line. Interestingly, HA? using only the 3-split gram-
mar is faster than A? by about 10% despite using the
same grammars. This is because, unlike A?, HA?
need not exhaustively parse the 3-split grammar be-
fore beginning to search in the target grammar.
When we add the 4- and 5-split grammars to HA?,
it increases performance by another 25%. However,
we can also see an important failure case of HA?:
using all 6 auxiliary grammars actually decreases
performance compared to using only 3-5. This is be-
cause HA? requires that auxiliary grammars are all
relaxed projections of the target grammar. Since the
weights of the rules in the smaller grammars are the
minimum of a large set of rules in the target gram-
mar, these grammars have costs that are so cheap
that all edges in those grammars will be processed
long before much progress is made in the refined,
more expensive levels. The time spent parsing in
the smaller grammars is thus entirely wasted. This
is in sharp contrast to hierarchical CTF (see below)
where adding levels is always beneficial.
To quantify the effect of optimistically cheap
costs in the coarsest projections, we can look at the
degree to which the outside costs in auxiliary gram-
mars underestimate the true outside cost in the target
grammar (the ?slack?). In Figure 5, we plot the aver-
age slack as a function of outside context size (num-
ber of unincorporated words) for each of the auxil-
iary grammars. The slack for large outside contexts
gets very large for the smaller, coarser grammars. In
Figure 6, we plot the number of edges pushed when
bounding with each auxiliary grammar individually,
against the average slack in that grammar. This plot
shows that greater slack leads to more work, reflect-
ing the theoretical property of A? that the work done
can be exponential in the slack of the heuristic.
3.2.2 HA? versus CTF
In this section, we compare HA? to CTF, again
using the grammars of Petrov et al (2006). It is
562
5 10 15 20
020
4060
80100
Number of words in outside context
Average slack
0 split
1 split
2 split
3 split
4 split
5 split
Figure 5: Average slack (difference between estimated
outside cost and true outside cost) at each level of ab-
straction as a function of the size of the outside context.
The average is over edges in the Viterbi tree. The lower
and upper dashed lines represent the slack of the exact
and uniformly zero heuristics.
5 10 15 20 25 30 35
0500
1500
2500
3500
Slack for span length 10
Edges 
pushed
 (million
s)
Figure 6: Edges pushed as a function of the average slack
for spans of length 10 when parsing with each auxiliary
grammar individually.
important to note, however, that we do not use the
same grammars when parsing with these two al-
gorithms. While we use the same projections to
coarsen the target grammar, the scores in the CTF
case need not be lower bounds. Instead, we fol-
low Petrov and Klein (2007) in taking coarse gram-
mar weights which make the induced distribution
over trees as close as possible to the target in KL-
divergence. These grammars represent not a mini-
mum projection, but more of an average.3
The performance of CTF as compared to HA?
is shown in Figure 4. CTF represents a significant
speed up over HA?. The key advantage of CTF, as
shown here, is that, where the work saved by us-
3We tried using these average projections as heuristics in
HA?, but doing so violates consistency, causes many search er-
rors, and does not substantially speed up the search.
5 10 15 20 25 30
020
4060
80
120
Length of sentence
Edges pu
shed per
 sentenc
e (millio
ns)
HA* 3-5
CTF 0-5
Figure 7: Edges pushed as function of sentence length for
HA? 3-5 and CTF 0-5.
ing coarser projections falls off for HA?, the work
saved with CTF increases with the addition of highly
coarse grammars. Adding the 0- through 2-split
grammars to CTF was responsible for a factor of 8
speed-up with no additional search errors.
Another important property of CTF is that it
scales far better with sentence length than does HA?.
Figure 7 shows a plot of edges pushed against sen-
tence length. This is not surprising in light of the in-
crease in slack that comes with parsing longer sen-
tences. The more words in an outside context, the
more slack there will generally be in the outside es-
timate, which triggers the time explosion.
Since we prune based on thresholds ?t in CTF,
we can explore the relationship between the number
of search errors made and the speed of the parser.
While it is possible to tune thresholds for each gram-
mar individually, we use a single threshold for sim-
plicity. In Figure 8, we plot the performance of CTF
using all 6 auxiliary grammars for various values of
? . For a moderate number of search errors (< 5%),
CTF parses more than 10 times faster than HA? and
nearly 100 times faster than UCS. However, below a
certain tolerance for search errors (< 1%) on these
grammars, HA? is the faster option.4
3.3 Lexicalized parsing experiments
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003a).
This lexicalized parsing model is constructed as the
product of a dependency model and the unlexical-
4In Petrov and Klein (2007), fewer search errors are re-
ported; this difference is because their search objective is more
closely aligned to the CTF pruning criterion.
563
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0.5
2.05.0
20.0
100.0
500.0
Fraction of sentences without search errors
Edges p
ushed (b
illions) HA* 3-5
UCS
Figure 8: Performance of CTF as a function of search er-
rors for state split grammars. The dashed lines represent
the time taken by UCS and HA? which make no search
errors. As search accuracy increases, the time taken by
CTF increases until it eventually becomes slower than
HA?. The y-axis is a log scale.
ized PCFG model in Klein and Manning (2003c).
We constructed these grammars using the Stanford
Parser.5 The PCFG has 19054 symbols 36078 rules.
The combined (sentence-specific) grammar has n
times as many symbols and 2n2 times as many rules,
where n is the length of an input sentence. This
model was trained on sections 2-20 of the Penn Tree-
bank and tested on section 21.
For these lexicalized grammars, we did not per-
form experiments with UCS or more than one level
of HA?. We used only the single PCFG projection
used in Klein and Manning (2003a). This grammar
differs from the state split grammars in that it factors
into two separate projections, a dependency projec-
tion and a PCFG. Klein and Manning (2003a) show
that one can use the sum of outside scores computed
in these two projections as a heuristic in the com-
bined lexicalized grammar. The generalization of
HA? to the factored case is straightforward but not
effective. We therefore treated the dependency pro-
jection as a black box and used only the PCFG pro-
jection inside the HA? framework. When comput-
ing A? outside estimates in the combined space, we
use the sum of the two projections? outside scores as
our completion costs. This is the same procedure as
Klein and Manning (2003a). For CTF, we carry out
a uniform cost search in the combined space where
we have pruned items based on their max-marginals
5http://nlp.stanford.edu/software/
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
3
4
5
67
8
Fraction of sentences without search errors
Edges p
ushed (b
illions)
A*
Figure 9: Performance of CTF for lexicalized parsing as
a function of search errors. The dashed line represents
the time taken by A?, which makes no search errors. The
y-axis is a log scale.
in the PCFG model only.
In Figure 9, we examine the speed/accuracy trade
off for the lexicalized grammar. The trend here is
the reverse of the result for the state split grammars:
HA? is always faster than posterior pruning, even for
thresholds which produce many search errors. This
is because the heuristic used in this model is actu-
ally an extraordinarily tight bound ? on average, the
slack even for spans of length 1 was less than 1% of
the overall model cost.
4 Conclusions
We have a presented an empirical comparison of
hierarchical A? search and coarse-to-fine pruning.
While HA? does provide benefits over flat A?
search, the extra levels of the hierarchy are dramat-
ically more beneficial for CTF. This is because, in
CTF, pruning choices cascade and even very coarse
projections can prune many highly unlikely edges.
However, in HA?, overly coarse projections become
so loose as to not rule out anything of substance. In
addition, we experimentally characterized the fail-
ure cases of A? and CTF in a way which matches
the formal results on A?: A? does vastly more work
as heuristics loosen and only outperforms CTF when
either near-optimality is desired or heuristics are ex-
tremely tight.
Acknowledgements
This work was partially supported by an NSERC Post-Graduate
Scholarship awarded to the first author.
564
References
Sharon Caraballo and Eugene Charniak. 1996. Figures
of Merit for Best-First Probabalistic Parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eugene Charniak. 1997 Statistical Parsing with a
Context-Free Grammar and Word Statistics. In Pro-
ceedings of the Fourteenth National Conference on Ar-
tificial Intelligence.
Eugene Charniak, Sharon Goldwater and Mark Johnson.
1998. Edge-based Best First Parsing. In Proceedings
of the Sixth Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel Coarse-to-fine PCFG
Parsing. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
P. Felzenszwalb and D. McAllester. 2007. The General-
ized A? Architecture. In Journal of Artificial Intelli-
gence Research.
Aria Haghighi, John DeNero, and Dan Klein. 2007. Ap-
proximate Factoring for A? Search. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Chris Manning. 2002. Fast Exact In-
ference with a Factored Model for Natural Language
Processing. In Advances in Neural Information Pro-
cessing Systems.
Dan Klein and Chris Manning. 2003. Factored A?
Search for Models over Sequences and Trees. In Pro-
ceedings of the International Joint Conference on Ar-
tificial Intelligence.
Dan Klein and Chris Manning. 2003. A? Parsing: Fast
Exact Viterbi Parse Selection. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Mark-Jan Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. In Computational Linguistics,
29(1):135?143.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2003. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. In Journal of Logic Programming,
24:3?36.
565
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 611?619,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Online EM for Unsupervised Models
Percy Liang Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang,klein}@cs.berkeley.edu
Abstract
The (batch) EM algorithm plays an important
role in unsupervised induction, but it some-
times suffers from slow convergence. In this
paper, we show that online variants (1) provide
significant speedups and (2) can even find bet-
ter solutions than those found by batch EM.
We support these findings on four unsuper-
vised tasks: part-of-speech tagging, document
classification, word segmentation, and word
alignment.
1 Introduction
In unsupervised NLP tasks such as tagging, parsing,
and alignment, one wishes to induce latent linguistic
structures from raw text. Probabilistic modeling has
emerged as a dominant paradigm for these problems,
and the EM algorithm has been a driving force for
learning models in a simple and intuitive manner.
However, on some tasks, EM can converge
slowly. For instance, on unsupervised part-of-
speech tagging, EM requires over 100 iterations to
reach its peak performance on the Wall-Street Jour-
nal (Johnson, 2007). The slowness of EM is mainly
due to its batch nature: Parameters are updated only
once after each pass through the data. When param-
eter estimates are still rough or if there is high redun-
dancy in the data, computing statistics on the entire
dataset just to make one update can be wasteful.
In this paper, we investigate two flavors of on-
line EM?incremental EM (Neal and Hinton, 1998)
and stepwise EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2009), both of which involve updating pa-
rameters after each example or after a mini-batch
(subset) of examples. Online algorithms have the
potential to speed up learning by making updates
more frequently. However, these updates can be
seen as noisy approximations to the full batch up-
date, and this noise can in fact impede learning.
This tradeoff between speed and stability is famil-
iar to online algorithms for convex supervised learn-
ing problems?e.g., Perceptron, MIRA, stochastic
gradient, etc. Unsupervised learning raises two ad-
ditional issues: (1) Since the EM objective is non-
convex, we often get convergence to different local
optima of varying quality; and (2) we evaluate on
accuracy metrics which are at best loosely correlated
with the EM likelihood objective (Liang and Klein,
2008). We will see that these issues can lead to sur-
prising results.
In Section 4, we present a thorough investigation
of online EM, mostly focusing on stepwise EM since
it dominates incremental EM. For stepwise EM, we
find that choosing a good stepsize and mini-batch
size is important but can fortunately be done ade-
quately without supervision. With a proper choice,
stepwise EM reaches the same performance as batch
EM, but much more quickly. Moreover, it can even
surpass the performance of batch EM. Our results
are particularly striking on part-of-speech tagging:
Batch EM crawls to an accuracy of 57.3% after 100
iterations, whereas stepwise EM shoots up to 65.4%
after just two iterations.
2 Tasks, models, and datasets
In this paper, we focus on unsupervised induction
via probabilistic modeling. In particular, we define
a probabilistic model p(x, z; ?) of the input x (e.g.,
611
a sentence) and hidden output z (e.g., a parse tree)
with parameters ? (e.g., rule probabilities). Given a
set of unlabeled examples x(1), . . . ,x(n), the stan-
dard training objective is to maximize the marginal
log-likelihood of these examples:
`(?) =
n?
i=1
log p(x(i); ?). (1)
A trained model ?? is then evaluated on the accuracy
of its predictions: argmaxz p(z | x(i); ??) against the
true output z(i); the exact evaluation metric depends
on the task. What makes unsupervised induction
hard at best and ill-defined at worst is that the train-
ing objective (1) does not depend on the true outputs
at all.
We ran experiments on four tasks described be-
low. Two of these tasks?part-of-speech tagging and
document classification?are ?clustering? tasks. For
these, the output z consists of labels; for evalua-
tion, we map each predicted label to the true label
that maximizes accuracy. The other two tasks?
segmentation and alignment?only involve unla-
beled combinatorial structures, which can be eval-
uated directly.
Part-of-speech tagging For each sentence x =
(x1, . . . , x`), represented as a sequence of words, we
wish to predict the corresponding sequence of part-
of-speech (POS) tags z = (z1, . . . , z`). We used
a simple bigram HMM trained on the Wall Street
Journal (WSJ) portion of the Penn Treebank (49208
sentences, 45 tags). No tagging dictionary was used.
We evaluated using per-position accuracy.
Document classification For each document x =
(x1, . . . , x`) consisting of ` words,1 we wish to pre-
dict the document class z ? {1, . . . , 20}. Each doc-
ument x is modeled as a bag of words drawn inde-
pendently given the class z. We used the 20 News-
groups dataset (18828 documents, 20 classes). We
evaluated on class accuracy.
Word segmentation For each sentence x =
(x1, . . . , x`), represented as a sequence of English
phonemes or Chinese characters without spaces
separating the words, we would like to predict
1We removed the 50 most common words and words that
occurred fewer than 5 times.
a segmentation of the sequence into words z =
(z1, . . . , z|z|), where each segment (word) zi is a
contiguous subsequence of 1, . . . , `. Since the na??ve
unigram model has a degenerate maximum likeli-
hood solution that makes each sentence a separate
word, we incorporate a penalty for longer segments:
p(x, z; ?) ? ?|z|k=1 p(xzk ; ?)e?|zk|? , where ? > 1determines the strength of the penalty. For English,
we used ? = 1.6; Chinese, ? = 2.5. To speed up in-
ference, we restricted the maximum segment length
to 10 for English and 5 for Chinese.
We applied this model on the Bernstein-Ratner
corpus from the CHILDES database used in
Goldwater et al (2006) (9790 sentences) and
the Academia Sinica (AS) corpus from the first
SIGHAN Chinese word segmentation bakeoff (we
used the first 100K sentences). We evaluated using
F1 on word tokens.
To the best of our knowledge, our penalized uni-
gram model is new and actually beats the more com-
plicated model of Johnson (2008) 83.5% to 78%,
which had been the best published result on this task.
Word alignment For each pair of translated sen-
tences x = (e1, . . . , ene , f1, . . . , fnf ), we wish to
predict the word alignments z ? {0, 1}nenf . We
trained two IBM model 1s using agreement-based
learning (Liang et al, 2008). We used the first
30K sentence pairs of the English-French Hansards
data from the NAACL 2003 Shared Task, 447+37
of which were hand-aligned (Och and Ney, 2003).
We evaluated using the standard alignment error rate
(AER).
3 EM algorithms
Given a probabilistic model p(x, z; ?) and unla-
beled examples x(1), . . . ,x(n), recall we would like
to maximize the marginal likelihood of the data
(1). Let ?(x, z) denote a mapping from a fully-
labeled example (x, z) to a vector of sufficient statis-
tics (counts in the case of multinomials) for the
model. For example, one component of this vec-
tor for HMMs would be the number of times state
7 emits the word ?house? in sentence x with state
sequence z. Given a vector of sufficient statistics ?,
let ?(?) denote the maximum likelihood estimate. In
our case, ?(?) are simply probabilities obtained by
normalizing each block of counts. This closed-form
612
Batch EM
?? initialization
for each iteration t = 1, . . . , T :
??? ? 0
?for each example i = 1, . . . , n:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? ? ?? + s?i [accumulate new]
??? ?? [replace old with new]
solution is one of the features that makes EM (both
batch and online) attractive.
3.1 Batch EM
In the (batch) EM algorithm, we alternate between
the E-step and the M-step. In the E-step, we com-
pute the expected sufficient statistics ?? across all
the examples based on the posterior over z under the
current parameters ?(?). In all our models, this step
can be done via a dynamic program (for example,
forward-backward for POS tagging).
In the M-step, we use these sufficient statistics
?? to re-estimate the parameters. Since the M-step
is trivial, we represent it implicitly by ?(?) in order
to concentrate on the computation of the sufficient
statistics. This focus will be important for online
EM, so writing batch EM in this way accentuates
the parallel between batch and online.
3.2 Online EM
To obtain an online EM algorithm, we store a sin-
gle set of sufficient statistics ? and update it after
processing each example. For the i-th example, we
compute sufficient statistics s?i. There are two main
variants of online EM algorithms which differ in ex-
actly how the new s?i is incorporated into ?.
The first is incremental EM (iEM) (Neal and Hin-
ton, 1998), in which we not only keep track of ? but
also the sufficient statistics s1, . . . , sn for each ex-
ample (? =?ni=1 si). When we process example i,
we subtract out the old si and add the new s?i.
Sato and Ishii (2000) developed another variant,
later generalized by Cappe? and Moulines (2009),
which we call stepwise EM (sEM). In sEM, we in-
terpolate between ? and s?i based on a stepsize ?k (k
is the number of updates made to ? so far).
The two algorithms are motivated in different
ways. Recall that the log-likelihood can be lower
Incremental EM (iEM)
si ? initialization for i = 1, . . . , n
???ni=1 sifor each iteration t = 1, . . . , T :
?for each example i = 1, . . . , n in random order:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? ?+ s?i ? si; si ? s?i [replace old with new]
Stepwise EM (sEM)
?? initialization; k = 0
for each iteration t = 1, . . . , T :
?for each example i = 1, . . . , n in random order:
??s?i ?
?
z p(z | x(i); ?(?))?(x(i), z) [inference]
???? (1??k)?+ ?ks?i; k ? k+1 [towards new]
bounded as follows (Neal and Hinton, 1998):
`(?) ? L(q1, . . . , qn, ?) (2)
def=
n?
i=1
[?
z
qi(z | x(i)) log p(x(i), z; ?) +H(qi)
]
,
where H(qi) is the entropy of the distribution qi(z |
x(i)). Batch EM alternates between optimizing L
with respect to q1, . . . , qn in the E-step (represented
implicitly via sufficient statistics ??) and with re-
spect to ? in the M-step. Incremental EM alternates
between optimizing with respect to a single qi and ?.
Stepwise EM is motivated from the stochastic ap-
proximation literature, where we think of approxi-
mating the update ?? in batch EM with a single sam-
ple s?i. Since one sample is a bad approximation,
we interpolate between s?i and the current ?. Thus,
sEM can be seen as stochastic gradient in the space
of sufficient statistics.
Stepsize reduction power ? Stepwise EM leaves
open the choice of the stepsize ?k. Standard results
from the stochastic approximation literature state
that ??k=0 ?k = ? and
??
k=0 ?2k < ? are suffi-
cient to guarantee convergence to a local optimum.
In particular, if we take ?k = (k + 2)??, then any
0.5 < ? ? 1 is valid. The smaller the ?, the larger
the updates, and the more quickly we forget (decay)
our old sufficient statistics. This can lead to swift
progress but also generates instability.
Mini-batch size m We can add some stability
to sEM by updating on multiple examples at once
613
instead of just one. In particular, partition the
n examples into mini-batches of size m and run
sEM, treating each mini-batch as a single exam-
ple. Formally, for each i = 0,m, 2m, 3m, . . . , first
compute the sufficient statistics s?i+1, . . . , s?i+m on
x(i+1), . . . ,x(i+m) and then update ? using s?i+1 +
? ? ? + s?i+m. The larger the m, the less frequent
the updates, but the more stable they are. In this
way, mini-batches interpolate between a pure online
(m = 1) and a pure batch (m = n) algorithm.2
Fast implementation Due to sparsity in NLP, the
sufficient statistics of an example s?i are nonzero for
a small fraction of its components. For iEM, the
time required to update ? with s?i depends only on
the number of nonzero components of s?i. However,
the sEM update is ?? (1??k)?+?ks?i, and a na??ve
implementation would take time proportional to the
total number of components. The key to a more effi-
cient solution is to note that ?(?) is invariant to scal-
ing of ?. Therefore, we can store S = ?Q
j<k(1??j)instead of ? and make the following sparse update:
S ? S + ?kQ
j?k(1??j)
s?i, taking comfort in the fact
that ?(?) = ?(S).
For both iEM and sEM, we also need to efficiently
compute ?(?). We can do this by maintaining the
normalizer for each multinomial block (sum of the
components in the block). This extra maintenance
only doubles the number of updates we have to make
but allows us to fetch any component of ?(?) in con-
stant time by dividing out the normalizer.
3.3 Incremental versus stepwise EM
Incremental EM increases L monotonically after
each update by virtue of doing coordinate-wise as-
cent and thus is guaranteed to converge to a local
optimum of both L and ` (Neal and Hinton, 1998).
However, ` is not guaranteed to increase after each
update. Stepwise EM might not increase either L or
` after each update, but it is guaranteed to converge
to a local optimum of ` given suitable conditions on
the stepsize discussed earlier.
Incremental and stepwise EM actually coincide
under the following setting (Cappe? and Moulines,
2Note that running sEM with m = n is similar but not
equivalent to batch EM since old sufficient statistics are still
interpolated rather than replaced.
2009): If we set (?,m) = (1, 1) for sEM and ini-
tialize all si = 0 for iEM, then both algorithms make
the same updates on the first pass through the data.
They diverge thereafter as iEM subtracts out old sis,
while sEM does not even remember them.
One weakness of iEM is that its memory require-
ments grow linearly with the number of examples
due to storing s1, . . . , sn. For large datasets, these
sis might not even fit in memory, and resorting to
physical disk would be very slow. In contrast, the
memory usage of sEM does not depend on n.
The relationship between iEM and sEM (with
m = 1) is analogous to the one between exponen-
tiated gradient (Collins et al, 2008) and stochastic
gradient for supervised learning of log-linear mod-
els. The former maintains the sufficient statistics of
each example and subtracts out old ones whereas the
latter does not. In the supervised case, the added sta-
bility of exponentiated gradient tends to yield bet-
ter performance. For the unsupervised case, we will
see empirically that remembering the old sufficient
statistics offers no benefit, and much better perfor-
mance can be obtained by properly setting (?,m)
for sEM (Section 4).
4 Experiments
We now present our empirical results for batch EM
and online EM (iEM and sEM) on the four tasks de-
scribed in Section 2: part-of-speech tagging, docu-
ment classification, word segmentation (English and
Chinese), and word alignment.
We used the following protocol for all experi-
ments: We initialized the parameters to a neutral set-
ting plus noise to break symmetries.3 Training was
performed for 20 iterations.4 No parameter smooth-
ing was used. All runs used a fixed random seed for
initializing the parameters and permuting the exam-
ples at the beginning of each iteration. We report two
performance metrics: log-likelihood normalized by
the number of examples and the task-specific accu-
racy metric (see Section 2). All numbers are taken
from the final iteration.
3Specifically, for each block of multinomial probabilities
?1, . . . , ?K , we set ?k ? exp{10?3(1 + ak)}, where ak ?
U [0, 1]. Exception: for batch EM on POS tagging, we used 1
instead of 10?3; more noise worked better.
4Exception: for batch EM on POS tagging, 100 iterations
was needed to get satisfactory performance.
614
Stepwise EM (sEM) requires setting two
optimization parameters: the stepsize reduc-
tion power ? and the mini-batch size m (see
Section 3.2). As Section 4.3 will show, these
two parameters can have a large impact on
performance. As a default rule of thumb, we
chose (?,m) ? {0.5, 0.6, 0.7, 0.8, 0.9, 1.0} ?
{1, 3, 10, 30, 100, 300, 1K, 3K, 10K} to maximize
log-likelihood; let sEM` denote stepwise EM with
this setting. Note that this setting requires no labeled
data. We will also consider fixing (?,m) = (1, 1)
(sEMi) and choosing (?,m) to maximize accuracy
(sEMa).
In the results to follow, we first demonstrate that
online EM is faster (Section 4.1) and sometimes
leads to higher accuracies (Section 4.2). Next, we
explore the effect of the optimization parameters
(?,m) (Section 4.3), briefly revisiting the connec-
tion between incremental and stepwise EM. Finally,
we show the stability of our results under different
random seeds (Section 4.4).
4.1 Speed
One of the principal motivations for online EM
is speed, and indeed we found this motivation to
be empirically well-justified. Figure 1 shows that,
across all five datasets, sEM` converges to a solution
with at least comparable log-likelihood and accuracy
with respect to batch EM, but sEM` does it anywhere
from about 2 (word alignment) to 10 (POS tagging)
times faster. This supports our intuition that more
frequent updates lead to faster convergence. At the
same time, note that the other two online EM vari-
ants in Figure 1 (iEM and sEMi) are prone to catas-
trophic failure. See Section 4.3 for further discus-
sion on this issue.
4.2 Performance
It is fortunate but perhaps not surprising that step-
wise EM is faster than batch EM. But Figure 1 also
shows that, somewhat surprisingly, sEM` can actu-
ally converge to a solution with higher accuracy, in
particular on POS tagging and document classifica-
tion. To further explore the accuracy-increasing po-
tential of sEM, consider choosing (?,m) to maxi-
mize accuracy (sEMa). Unlike sEM`, sEMa does re-
quire labeled data. In practice, (?,m) can be tuned
EM sEM` sEMa ?` m` ?a ma
POS 57.3 59.6 66.7 0.7 3 0.5 3
DOC 39.1 47.8 49.9 0.8 1K 0.5 3K
SEG(en) 80.5 80.7 83.5 0.7 1K 1.0 100
SEG(ch) 78.2 77.2 78.1 0.6 10K 1.0 10K
ALIGN 78.8 78.9 78.9 0.7 10K 0.7 10K
Table 1: Accuracy of batch EM and stepwise EM, where
the optimization parameters (?,m) are tuned to either
maximize log-likelihood (sEM`) or accuracy (sEMa).
With an appropriate setting of (?,m), stepwise EM out-
performs batch EM significantly on POS tagging and
document classification.
on a small labeled set alng with any model hyper-
parameters.
Table 1 shows that sEMa improves the accuracy
compared to batch EM even more than sEM`. The
result for POS is most vivid: After one iteration of
batch EM, the accuracy is only at 24.0% whereas
sEMa is already at 54.5%, and after two iterations,
at 65.4%. Not only is this orders of magnitude faster
than batch EM, batch EM only reaches 57.3% after
100 iterations.
We get a similarly striking result for document
classification, but the results for word segmentation
and word alignment are more modest. A full un-
derstanding of this phenomenon is left as an open
problem, but we will comment on one difference be-
tween the tasks where sEM improves accuracy and
the tasks where it doesn?t. The former are ?clus-
tering? tasks (POS tagging and document classifi-
cation), while the latter are ?structural? tasks (word
segmentation and word alignment). Learning of
clustering models centers around probabilities over
words given a latent cluster label, whereas in struc-
tural models, there are no cluster labels, and it is
the combinatorial structure (the segmentations and
alignments) that drives the learning.
Likelihood versus accuracy From Figure 1, we
see that stepwise EM (sEM`) can outperform batch
EM in both likelihood and accuracy. This suggests
that stepwise EM is better at avoiding local minima,
perhaps leveraging its stochasticity to its advantage.
However, on POS tagging, tuning sEM to maxi-
mize accuracy (sEMa) results in a slower increase
in likelihood: compare sEMa in Figure 2 with sEM`
in Figure 1(a). This shouldn?t surprise us too much
given that likelihood and accuracy are only loosely
615
20 40 60 80
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
20 40 60 80
iterations
-9.8
-8.8
-7.8
-6.9
-5.9
log
-lik
elih
ood
EM
sEMi
sEM`
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
2 4 6 8 10
iterations
-9.8
-9.3
-8.8
-8.3
-7.8
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
(a) POS tagging (b) Document classification
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
F 1
2 4 6 8 10
iterations
-4.8
-4.6
-4.4
-4.2
-4.0
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
F 1
2 4 6 8 10
iterations
-9.5
-8.9
-8.4
-7.8
-7.2
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
(c) Word segmentation (English) (d) Word segmentation (Chinese)
2 4 6 8 10
iterations
0.2
0.4
0.6
0.8
1.0
1?
AE
R
2 4 6 8 10
iterations
-10.9
-9.4
-7.9
-6.5
-5.0
log
-lik
elih
ood
EM
iEM
sEMi
sEM`
accuracy log-likelihood
EM sEM` EM sEM`pos 57.3 59.6 -6.03 -6.08doc 39.1 47.8 -7.96 -7.88seg(en) 80.5 80.7 -4.11 -4.11seg(ch) 78.2 77.2 -7.27 -7.28align 78.8 78.9 -5.05 -5.12
(e) Word alignment (f) Results after convergence
Figure 1: Accuracy and log-likelihood plots for batch EM, incremental EM, and stepwise EM across all five datasets.
sEM` outperforms batch EM in terms of convergence speed and even accuracy and likelihood; iEM and sEMi fail in
some cases. We did not run iEM on POS tagging due to memory limitations; we expect the performance would be
similar to sEMi, which is not very encouraging (Section 4.3).
correlated (Liang and Klein, 2008). But it does sug-
gest that stepwise EM is injecting a bias that favors
accuracy over likelihood?a bias not at all reflected
in the training objective.
We can create a hybrid (sEMa+EM) that com-
bines the strengths of both sEMa and EM: First run
sEMa for 5 iterations, which quickly takes us to a
part of the parameter space yielding good accura-
cies; then run EM, which quickly improves the like-
lihood. Fortunately, accuracy does not degrade as
likelihood increases (Figure 2).
4.3 Varying the optimization parameters
Recall that stepwise EM requires setting two opti-
mization parameters: the stepsize reduction power ?
and the mini-batch size m. We now explore the ef-
fect of (?,m) on likelihood and accuracy.
As mentioned in Section 3.2, larger mini-batches
(increasing m) stabilize parameter updates, while
larger stepsizes (decreasing ?) provide swifter
616
doc accuracy?\m 1 3 10 30 100 300 1K 3K 10K0.5 5.4 5.4 5.5 5.6 6.0 25.7 48.8 49.9 44.60.6 5.4 5.4 5.6 5.6 22.3 36.1 48.7 49.3 44.20.7 5.5 5.5 5.6 11.1 39.9 43.3 48.1 49.0 43.50.8 5.6 5.6 6.0 21.7 47.3 45.0 47.8 49.5 42.80.9 5.8 6.0 13.4 32.4 48.7 48.4 46.4 49.4 42.41.0 6.2 11.8 19.6 35.2 47.6 49.5 47.5 49.3 41.7
pos doc align
doc log-likelihood?\m 1 3 10 30 100 300 1K 3K 10K0.5 -8.875 -8.71 -8.61 -8.555 -8.505 -8.172 -7.92 -7.906 -7.9160.6 -8.604 -8.575 -8.54 -8.524 -8.235 -8.041 -7.898 -7.901 -7.9160.7 -8.541 -8.533 -8.531 -8.354 -8.023 -7.943 -7.886 -7.896 -7.9180.8 -8.519 -8.506 -8.493 -8.228 -7.933 -7.896 -7.883 -7.89 -7.9220.9 -8.505 -8.486 -8.283 -8.106 -7.91 -7.889 -7.889 -7.891 -7.9271.0 -8.471 -8.319 -8.204 -8.052 -7.919 -7.889 -7.892 -7.896 -7.937
seg(en) seg(ch)
Figure 3: Effect of optimization parameters (stepsize reduction power ? and mini-batch size m) on accuracy and
likelihood. Numerical results are shown for document classification. In the interest of space, the results for each task
are compressed into two gray scale images, one for accuracy (top) and one for log-likelihood (bottom), where darker
shades represent larger values. Bold (red) numbers denote the best ? for a given m.
20 40 60 80
iterations
0.2
0.4
0.6
0.8
1.0
acc
ura
cy
20 40 60 80
iterations
-12.7
-11.0
-9.3
-7.6
-5.9
log
-lik
elih
ood
EM
sEMa
sEMa+EM
Figure 2: sEMa quickly obtains higher accuracy than
batch EM but suffers from a slower increase in likeli-
hood. The hybrid sEMa+EM (5 iterations of EMa fol-
lowed by batch EM) increases both accuracy and likeli-
hood sharply.
progress. Remember that since we are dealing with a
nonconvex objective, the choice of stepsize not only
influences how fast we converge, but also the quality
of the solution that we converge to.
Figure 3 shows the interaction between ? and m
in terms of likelihood and accuracy. In general, the
best (?,m) depends on the task and dataset. For ex-
ample, for document classification, larger m is criti-
cal for good performance; for POS tagging, it is bet-
ter to use smaller values of ? and m.
Fortunately, there is a range of permissible set-
tings (corresponding to the dark regions in Figure 3)
that lead to reasonable performance. Furthermore,
the settings that perform well on likelihood gener-
ally correspond to ones that perform well on accu-
racy, which justifies using sEM`.
A final observation is that as we use larger mini-
batches (larger m), decreasing the stepsize more
gradually (smaller ?) leads to better performance.
Intuitively, updates become more reliable with larger
m, so we can afford to trust them more and incorpo-
rate them more aggressively.
Stepwise versus incremental EM In Section 3.2,
we mentioned that incremental EM can be made
equivalent to stepwise EM with ? = 1 and m = 1
(sEMi). Figure 1 provides the empirical support:
iEM and sEMi have very similar training curves.
Therefore, keeping around the old sufficient statis-
tics does not provide any advantage and still requires
a substantial storage cost. As mentioned before, set-
ting (?,m) properly is crucial. While we could sim-
ulate mini-batches with iEM by updating multiple
coordinates simultaneously, iEM is not capable of
exploiting the behavior of ? < 1.
4.4 Varying the random seed
All our results thus far represent single runs with a
fixed random seed. We now investigate the impact
of randomness on our results. Recall that we use
randomness for two purposes: (1) initializing the
parameters (affects both batch EM and online EM),
617
accuracy log-likelihood
EM sEM` EM sEM`
POS 56.2 ?1.36 58.8 ?0.73, 1.41 ?6.01 ?6.09
DOC 41.2 ?1.97 51.4 ?0.97, 2.82 ?7.93 ?7.88
SEG(en) 80.5 ?0.0 81.0 ?0.0, 0.42 ?4.1 ?4.1
SEG(ch) 78.2 ?0.0 77.2 ?0.0, 0.04 ?7.26 ?7.27
ALIGN 79.0 ?0.14 78.8 ?0.14, 0.25 ?5.04 ?5.11
Table 2: Mean and standard deviation over different ran-
dom seeds. For EM and sEM, the first number after ?
is the standard deviation due to different initializations
of the parameters. For sEM, the second number is the
standard deviation due to different permutations of the
examples. Standard deviation for log-likelihoods are all
< 0.01 and therefore left out due to lack of space.
and (2) permuting the examples at the beginning of
each iteration (affects only online EM).
To separate these two purposes, we used two
different seeds, Si ? {1, 2, 3, 4, 5} and Sp ?
{1, 2, 3, 4, 5} for initializing and permuting, respec-
tively. Let X be a random variable denoting either
log-likelihood or accuracy. We define the variance
due to initialization as var(E(X | Si)) (E averages
over Sp for each fixed Si) and the variance due to
permutation as E(var(X | Si)) (E averages over Si).
These two variances provide an additive decompo-
sition of the total variance: var(X) = var(E(X |
Si)) + E(var(X | Si)).
Table 2 summarizes the results across the 5 tri-
als for EM and 25 for sEM`. Since we used a very
small amount of noise to initialize the parameters,
the variance due to initialization is systematically
smaller than the variance due to permutation. sEM`
is less sensitive to initialization than EM, but addi-
tional variance is created by randomly permuting the
examples. Overall, the accuracy of sEM` is more
variable than that of EM, but not by a large amount.
5 Discussion and related work
As datasets increase in size, the demand for online
algorithms has grown in recent years. One sees
this clear trend in the supervised NLP literature?
examples include the Perceptron algorithm for tag-
ging (Collins, 2002), MIRA for dependency parsing
(McDonald et al, 2005), exponentiated gradient al-
gorithms (Collins et al, 2008), stochastic gradient
for constituency parsing (Finkel et al, 2008), just
to name a few. Empirically, online methods are of-
ten faster by an order of magnitude (Collins et al,
2008), and it has been argued on theoretical grounds
that the fast, approximate nature of online meth-
ods is a good fit given that we are interested in test
performance, not the training objective (Bottou and
Bousquet, 2008; Shalev-Shwartz and Srebro, 2008).
However, in the unsupervised NLP literature, on-
line methods are rarely seen,5 and when they are,
incremental EM is the dominant variant (Gildea and
Hofmann, 1999; Kuo et al, 2008). Indeed, as we
have shown, applying online EM does require some
care, and some variants (including incremental EM)
can fail catastrophically in face of local optima.
Stepwise EM provides finer control via its optimiza-
tion parameters and has proven quite successful.
One family of methods that resembles incremen-
tal EM includes collapsed samplers for Bayesian
models?for example, Goldwater et al (2006) and
Goldwater and Griffiths (2007). These samplers
keep track of a sample of the latent variables for
each example, akin to the sufficient statistics that we
store in incremental EM. In contrast, stepwise EM
does not require this storage and operates more in
the spirit of a truly online algorithm.
Besides speed, online algorithms are of interest
for two additional reasons. First, in some applica-
tions, we receive examples sequentially and would
like to estimate a model in real-time, e.g., in the clus-
tering of news articles. Second, since humans learn
sequentially, studying online EM might suggest new
connections to cognitive mechanisms.
6 Conclusion
We have explored online EM on four tasks and
demonstrated how to use stepwise EM to overcome
the dangers of stochasticity and reap the benefits of
frequent updates and fast learning. We also discov-
ered that stepwise EM can actually improve accu-
racy, a phenomenon worthy of further investigation.
This paper makes some progress on elucidating the
properties of online EM. With this increased under-
standing, online EM, like its batch cousin, could be-
come a mainstay for unsupervised learning.
5Other types of learning methods have been employed
successfully, for example, Venkataraman (2001) and Seginer
(2007).
618
References
L. Bottou and O. Bousquet. 2008. The tradeoffs of large
scale learning. In Advances in Neural Information
Processing Systems (NIPS).
O. Cappe? and E. Moulines. 2009. Online expectation-
maximization algorithm for latent data models. Jour-
nal of the Royal Statistics Society: Series B (Statistical
Methodology), 71.
M. Collins, A. Globerson, T. Koo, X. Carreras, and
P. Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
Perceptron algorithms. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
J. R. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Human Language Technology and Association for
Computational Linguistics (HLT/ACL).
D. Gildea and T. Hofmann. 1999. Topic-based language
models using EM. In Eurospeech.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL).
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguistic
structure. In Human Language Technology and As-
sociation for Computational Linguistics (HLT/ACL),
pages 398?406.
J. Kuo, H. Li, and C. Lin. 2008. Mining transliterations
from web query results: An incremental approach. In
Sixth SIGHAN Workshop on Chinese Language Pro-
cessing.
P. Liang and D. Klein. 2008. Analyzing the errors
of unsupervised learning. In Human Language Tech-
nology and Association for Computational Linguistics
(HLT/ACL).
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In Advances in Neural Information
Processing Systems (NIPS).
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Association for Computational Linguistics (ACL).
R. Neal and G. Hinton. 1998. A view of the EM algo-
rithm that justifies incremental, sparse, and other vari-
ants. In Learning in Graphical Models.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29:19?51.
M. Sato and S. Ishii. 2000. On-line EM algorithm for the
normalized Gaussian network. Neural Computation,
12:407?432.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Association for Computational Linguistics (ACL).
S. Shalev-Shwartz and N. Srebro. 2008. SVM optimiza-
tion: Inverse dependence on training set size. In Inter-
national Conference on Machine Learning (ICML).
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27:351?372.
619
Proceedings of the 43rd Annual Meeting of the ACL, pages 371?378,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Unsupervised Learning of Field Segmentation Models
for Information Extraction
Trond Grenager
Computer Science Department
Stanford University
Stanford, CA 94305
grenager@cs.stanford.edu
Dan Klein
Computer Science Division
U.C. Berkeley
Berkeley, CA 94709
klein@cs.berkeley.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
The applicability of many current information ex-
traction techniques is severely limited by the need
for supervised training data. We demonstrate that
for certain field structured extraction tasks, such
as classified advertisements and bibliographic ci-
tations, small amounts of prior knowledge can be
used to learn effective models in a primarily unsu-
pervised fashion. Although hidden Markov models
(HMMs) provide a suitable generative model for
field structured text, general unsupervised HMM
learning fails to learn useful structure in either of
our domains. However, one can dramatically im-
prove the quality of the learned structure by ex-
ploiting simple prior knowledge of the desired so-
lutions. In both domains, we found that unsuper-
vised methods can attain accuracies with 400 un-
labeled examples comparable to those attained by
supervised methods on 50 labeled examples, and
that semi-supervised methods can make good use
of small amounts of labeled data.
1 Introduction
Information extraction is potentially one of the most
useful applications enabled by current natural lan-
guage processing technology. However, unlike gen-
eral tools like parsers or taggers, which generalize
reasonably beyond their training domains, extraction
systems must be entirely retrained for each appli-
cation. As an example, consider the task of turn-
ing a set of diverse classified advertisements into a
queryable database; each type of ad would require
tailored training data for a supervised system. Ap-
proaches which required little or no training data
would therefore provide substantial resource savings
and extend the practicality of extraction systems.
The term information extraction was introduced
in the MUC evaluations for the task of finding short
pieces of relevant information within a broader text
that is mainly irrelevant, and returning it in a struc-
tured form. For such ?nugget extraction? tasks, the
use of unsupervised learning methods is difficult and
unlikely to be fully successful, in part because the
nuggets of interest are determined only extrinsically
by the needs of the user or task. However, the term
information extraction was in time generalized to a
related task that we distinguish as field segmenta-
tion. In this task, a document is regarded as a se-
quence of pertinent fields, and the goal is to segment
the document into fields, and to label the fields. For
example, bibliographic citations, such as the one in
Figure 1(a), exhibit clear field structure, with fields
such as author, title, and date. Classified advertise-
ments, such as the one in Figure 1(b), also exhibit
field structure, if less rigidly: an ad consists of de-
scriptions of attributes of an item or offer, and a set
of ads for similar items share the same attributes. In
these cases, the fields present a salient, intrinsic form
of linguistic structure, and it is reasonable to hope
that field segmentation models could be learned in
an unsupervised fashion.
In this paper, we investigate unsupervised learn-
ing of field segmentation models in two domains:
bibliographic citations and classified advertisements
for apartment rentals. General, unconstrained induc-
tion of HMMs using the EM algorithm fails to detect
useful field structure in either domain. However, we
demonstrate that small amounts of prior knowledge
can be used to greatly improve the learned model. In
both domains, we found that unsupervised methods
can attain accuracies with 400 unlabeled examples
comparable to those attained by supervised methods
on 50 labeled examples, and that semi-supervised
methods can make good use of small amounts of la-
beled data.
371
(a) AUTH
Pearl
AUTH
,
AUTH
J.
DATE
(
DATE
1988
DATE
)
DATE
.
TTL
Probabilistic
TTL
Reasoning
TTL
in
TTL
Intelligent
TTL
Systems
TTL
:
TTL
Networks
TTL
of
TTL
Plausible
TTL
Inference
TTL
.
PUBL
Morgan
PUBL
Kaufmann
PUBL
.
(b) SIZE
Spacious
SIZE
1
SIZE
Bedroom
SIZE
apt
SIZE
.
FEAT
newly
FEAT
remodeled
FEAT
,
FEAT
gated
FEAT
,
FEAT
new
FEAT
appliance
FEAT
,
FEAT
new
FEAT
carpet
FEAT
,
NBRHD
near
NBRHD
public
NBRHD
transportion
NBRHD
,
NBRHD
close
NBRHD
to
NBRHD
580
NBRHD
freeway
NBRHD
,
RENT
$
RENT
500.00
RENT
Deposit
CONTACT
(510)655-0106
(c) RB
No
,
,
PRP
it
VBD
was
RB
n?t
NNP
Black
NNP
Monday
.
.
Figure 1: Examples of three domains for HMM learning: the bibliographic citation fields in (a) and classified advertisements for
apartment rentals shown in (b) exhibit field structure. Contrast these to part-of-speech tagging in (c) which does not.
2 Hidden Markov Models
Hidden Markov models (HMMs) are commonly
used to represent a wide range of linguistic phe-
nomena in text, including morphology, parts-of-
speech (POS), named entity mentions, and even
topic changes in discourse. An HMM consists of
a set of states S, a set of observations (in our case
words or tokens) W , a transition model specify-
ing P(st|st?1), the probability of transitioning from
state st?1 to state st, and an emission model specify-
ing P(w|s) the probability of emitting word w while
in state s. For a good tutorial on general HMM tech-
niques, see Rabiner (1989).
For all of the unsupervised learning experiments
we fit an HMM with the same number of hidden
states as gold labels to an unannotated training set
using EM.1 To compute hidden state expectations
efficiently, we use the Forward-Backward algorithm
in the standard way. Emission models are initialized
to almost-uniform probability distributions, where
a small amount of noise is added to break initial
symmetry. Transition model initialization varies by
experiment. We run the EM algorithm to conver-
gence. Finally, we use the Viterbi algorithm with
the learned parameters to label the test data.
All baselines and experiments use the same tok-
enization, normalization, and smoothing techniques,
which were not extensively investigated. Tokeniza-
tion was performed in the style of the Penn Tree-
bank, and tokens were normalized in various ways:
numbers, dates, phone numbers, URLs, and email
1EM is a greedy hill-climbing algorithm designed for this
purpose, but it is not the only option; one could also use coordi-
nate ascent methods or sampling methods.
addresses were collapsed to dedicated tokens, and
all remaining tokens were converted to lowercase.
Unless otherwise noted, the emission models use
simple add-? smoothing, where ? was 0.001 for su-
pervised techniques, and 0.2 for unsupervised tech-
niques.
3 Datasets and Evaluation
The bibliographic citations data is described in
McCallum et al (1999), and is distributed at
http://www.cs.umass.edu/~mccallum/. It consists of
500 hand-annotated citations, each taken from the
reference section of a different computer science re-
search paper. The citations are annotated with 13
fields, including author, title, date, journal, and so
on. The average citation has 35 tokens in 5.5 fields.
We split this data, using its natural order, into a 300-
document training set, a 100-document development
set, and a 100-document test set.
The classified advertisements data set is
novel, and consists of 8,767 classified ad-
vertisements for apartment rentals in the San
Francisco Bay Area downloaded in June 2004
from the Craigslist website. It is distributed at
http://www.stanford.edu/~grenager/. 302 of the
ads have been labeled with 12 fields, including
size, rent, neighborhood, features, and so on.
The average ad has 119 tokens in 8.7 fields. The
annotated data is divided into a 102-document
training set, a 100-document development set,
and a 100-document test set. The remaining 8465
documents form an unannotated training set.
In both cases, all system development and param-
eter tuning was performed on the development set,
372
size
rent
features
restrictions
neighborhood
utilities
available
contact
photos
roomates
other
address
author
title
editorjournal
booktitle
volume
pages
publisher
location
tech
institution
date
DT
JJ
NN
NNS
NNP
PRP
CC
MD
VBD
VB
TO
IN
(a) (b) (c)
Figure 2: Matrix representations of the target transition structure in two field structured domains: (a) classified advertisements
(b) bibliographic citations. Columns and rows are indexed by the same sequence of fields. Also shown is (c) a submatrix of the
transition structure for a part-of-speech tagging task. In all cases the column labels are the same as the row labels.
and the test set was only used once, for running fi-
nal experiments. Supervised learning experiments
train on documents selected randomly from the an-
notated training set and test on the complete test set.
Unsupervised learning experiments also test on the
complete test set, but create a training set by first
adding documents from the test set (without anno-
tation), then adding documents from the annotated
training set (without annotation), and finally adding
documents from the unannotated training set. Thus
if an unsupervised training set is larger than the test
set, it fully contains the test set.
To evaluate our models, we first learn a set of
model parameters, and then use the parameterized
model to label the sequence of tokens in the test data
with the model?s hidden states. We then compare
the similarity of the guessed sequence to the human-
annotated sequence of gold labels, and compute ac-
curacy on a per-token basis.2 In evaluation of su-
pervised methods, the model states and gold labels
are the same. For models learned in a fully unsuper-
vised fashion, we map each model state in a greedy
fashion to the gold label to which it most often cor-
responds in the gold data. There is a worry with
this kind of greedy mapping: it increasingly inflates
the results as the number of hidden states grows. To
keep the accuracies meaningful, all of our models
have exactly the same number of hidden states as
gold labels, and so the comparison is valid.
2This evaluation method is used by McCallum et al (1999)
but otherwise is not very standard. Compared to other evalu-
ation methods for information extraction systems, it leads to a
lower penalty for boundary errors, and allows long fields also
contribute more to accuracy than short ones.
4 Unsupervised Learning
Consider the general problem of learning an HMM
from an unlabeled data set. Even abstracting away
from concrete search methods and objective func-
tions, the diversity and simultaneity of linguistic
structure is already worrying; in Figure 1 compare
the field structure in (a) and (b) to the parts-of-
speech in (c). If strong sequential correlations exist
at multiple scales, any fixed search procedure will
detect and model at most one of these levels of struc-
ture, not necessarily the level desired at the moment.
Worse, as experience with part-of-speech and gram-
mar learning has shown, induction systems are quite
capable of producing some uninterpretable mix of
various levels and kinds of structure.
Therefore, if one is to preferentially learn one
kind of inherent structure over another, there must
be some way of constraining the process. We could
hope that field structure is the strongest effect in
classified ads, while parts-of-speech is the strongest
effect in newswire articles (or whatever we would
try to learn parts-of-speech from). However, it is
hard to imagine how one could bleach the local
grammatical correlations and long-distance topical
correlations from our classified ads; they are still
English text with part-of-speech patterns. One ap-
proach is to vary the objective function so that the
search prefers models which detect the structures
which we have in mind. This is the primary way
supervised methods work, with the loss function rel-
ativized to training label patterns. However, for un-
supervised learning, the primary candidate for an
objective function is the data likelihood, and we
don?t have another suggestion here. Another ap-
proach is to inject some prior knowledge into the
373
search procedure by carefully choosing the starting
point; indeed smart initialization has been critical
to success in many previous unsupervised learning
experiments. The central idea of this paper is that
we can instead restrict the entire search domain by
constraining the model class to reflect the desired
structure in the data, thereby directing the search to-
ward models of interest. We do this in several ways,
which are described in the following sections.
4.1 Baselines
To situate our results, we provide three different
baselines (see Table 1). First is the most-frequent-
field accuracy, achieved by labeling all tokens with
the same single label which is then mapped to the
most frequent field. This gives an accuracy of 46.4%
on the advertisements data and 27.9% on the cita-
tions data. The second baseline method is to pre-
segment the unlabeled data using a crude heuristic
based on punctuation, and then to cluster the result-
ing segments using a simple Na??ve Bayes mixture
model with the Expectation-Maximization (EM) al-
gorithm. This approach achieves an accuracy of
62.4% on the advertisements data, and 46.5% on the
citations data.
As a final baseline, we trained a supervised first-
order HMM from the annotated training data using
maximum likelihood estimation. With 100 training
examples, supervised models achieve an accuracy of
74.4% on the advertisements data, and 72.5% on the
citations data. With 300 examples, supervised meth-
ods achieve accuracies of 80.4 on the citations data.
The learning curves of the supervised training ex-
periments for different amounts of training data are
shown in Figure 4. Note that other authors have
achieved much higher accuracy on the the citation
dataset using HMMs trained with supervision: Mc-
Callum et al (1999) report accuracies as high as
92.9% by using more complex models and millions
of words of BibTeX training data.
4.2 Unconstrained HMM Learning
From the supervised baseline above we know that
there is some first-order HMM over |S| states which
captures the field structure we?re interested in, and
we would like to find such a model without super-
vision. As a first attempt, we try fitting an uncon-
strained HMM, where the transition function is ini-
1
2
3
4
5
6
7
8
9
10
11
12
(a) Classified Advertisements
1
2
3
4
5
6
7
8
9
10
11
12
(b) Citations
Figure 3: Matrix representations of typical transition models
learned by initializing the transition model uniformly.
tialized randomly, to the unannotated training data.
Not surprisingly, the unconstrained approach leads
to predictions which poorly align with the desired
field segmentation: with 400 unannotated training
documents, the accuracy is just 48.8% for the ad-
vertisements and 49.7% for the citations: better than
the single state baseline but far from the supervised
baseline. To illustrate what is (and isn?t) being
learned, compare typical transition models learned
by this method, shown in Figure 3, to the maximum-
likelihood transition models for the target annota-
tions, shown in Figure 2. Clearly, they aren?t any-
thing like the target models: the learned classified
advertisements matrix has some but not all of the
desired diagonal structure, and the learned citations
matrix has almost no mass on the diagonal, and ap-
pears to be modeling smaller scale structure.
4.3 Diagonal Transition Models
To adjust our procedure to learn larger-scale pat-
terns, we can constrain the parametric form of the
transition model to be
P(st|st?1) =
?
?
?
? + (1??)|S| if st = st?1
(1??)
|S| otherwise
where |S| is the number of states, and ? is a global
free parameter specifying the self-loop probability:
374
(a) Classified advertisements
(b) Bibliographic citations
Figure 4: Learning curves for supervised learning and unsuper-
vised learning with a diagonal transition matrix on (a) classified
advertisements, and (b) bibliographic citations. Results are av-
eraged over 50 runs.
the probability of a state transitioning to itself. (Note
that the expected mean field length for transition
functions of this form is 11?? .) This constraint pro-
vides a notable performance improvement: with 400
unannotated training documents the accuracy jumps
from 48.8% to 70.0% for advertisements and from
49.7% to 66.3% for citations. The complete learning
curves for models of this form are shown in Figure 4.
We have tested training on more unannotated data;
the slope of the learning curve is leveling out, but
by training on 8000 unannotated ads, accuracy im-
proves significantly to 72.4%. On the citations task,
an accuracy of approximately 66% can be achieved
either using supervised training on 50 annotated ci-
tations, or unsupervised training using 400 unanno-
tated citations. 3
Although ? can easily be reestimated with EM
(even on a per-field basis), doing so does not yield
3We also tested training on 5000 additional unannotated ci-
tations collected from papers found on the Internet. Unfortu-
nately the addition of this data didn?t help accuracy. This prob-
ably results from the fact that the datasets were collected from
different sources, at different times.
Figure 5: Unsupervised accuracy as a function of the expected
mean field length 11?? for the classified advertisements dataset.
Each model was trained with 500 documents and tested on the
development set. Results are averaged over 50 runs.
better models.4 On the other hand, model accuracy
is not very sensitive to the exact choice of ?, as
shown in Figure 5 for the classified advertisements
task (the result for the citations task has a similar
shape). For the remaining experiments on the adver-
tisements data, we use ? = 0.9, and for those on the
citations data, we use ? = 0.5.
4.4 Hierarchical Mixture Emission Models
Consider the highest-probability state emissions
learned by the diagonal model, shown in Figure 6(a).
In addition to its characteristic content words, each
state also emits punctuation and English function
words devoid of content. In fact, state 3 seems to
have specialized entirely in generating such tokens.
This can become a problem when labeling decisions
are made on the basis of the function words rather
than the content words. It seems possible, then,
that removing function words from the field-specific
emission models could yield an improvement in la-
beling accuracy.
One way to incorporate this knowledge into the
model is to delete stopwords, which, while perhaps
not elegant, has proven quite effective in the past.
A better founded way of making certain words un-
available to the model is to emit those words from
all states with equal probability. This can be accom-
plished with the following simple hierarchical mix-
ture emission model
Ph(w|s) = ?Pc(w) + (1 ? ?)P(w|s)
where Pc is the common word distribution, and ? is
4While it may be surprising that disallowing reestimation of
the transition function is helpful here, the same has been ob-
served in acoustic modeling (Rabiner and Juang, 1993).
375
State 10 Most Common Words
1 . $ no ! month deposit , pets rent avail-
able
2 , . room and with in large living kitchen
-
3 . a the is and for this to , in
4 [NUM1] [NUM0] , bedroom bath / - .
car garage
5 , . and a in - quiet with unit building
6 - . [TIME] [PHONE] [DAY] call
[NUM8] at
(a)
State 10 Most Common Words
1 [NUM2] bedroom [NUM1] bath bed-
rooms large sq car ft garage
2 $ no month deposit pets lease rent avail-
able year security
3 kitchen room new , with living large
floors hardwood fireplace
4 [PHONE] call please at or for [TIME] to
[DAY] contact
5 san street at ave st # [NUM:DDD] fran-
cisco ca [NUM:DDDD]
6 of the yard with unit private back a
building floor
Comm. *CR* . , and - the in a / is with : of for
to
(b)
Figure 6: Selected state emissions from a typical model learned
from unsupervised data using the constrained transition func-
tion: (a) with a flat emission model, and (b) with a hierarchical
emission model.
a new global free parameter. In such a model, before
a state emits a token it flips a coin, and with probabil-
ity ? it allows the common word distribution to gen-
erate the token, and with probability (1??) it gener-
ates the token from its state-specific emission model
(see Vaithyanathan and Dom (2000) and Toutanova
et al (2001) for more on such models). We tuned
? on the development set and found that a range of
values work equally well. We used a value of 0.5 in
the following experiments.
We ran two experiments on the advertisements
data, both using the fixed transition model described
in Section 4.3 and the hierarchical emission model.
First, we initialized the emission model of Pc to a
general-purpose list of stopwords, and did not rees-
timate it. This improved the average accuracy from
70.0% to 70.9%. Second, we learned the emission
model of Pc using EM reestimation. Although this
method did not yield a significant improvement in
accuracy, it learns sensible common words: Fig-
ure 6(b) shows a typical emission model learned
with this technique. Unfortunately, this technique
does not yield improvements on the citations data.
4.5 Boundary Models
Another source of error concerns field boundaries.
In many cases, fields are more or less correct, but the
boundaries are off by a few tokens, even when punc-
tuation or syntax make it clear to a human reader
where the exact boundary should be. One way to ad-
dress this is to model the fact that in this data fields
often end with one of a small set of boundary tokens,
such as punctuation and new lines, which are shared
across states.
To accomplish this, we enriched the Markov pro-
cess so that each field s is now modeled by two
states, a non-final s? ? S? and a final s+ ? S+.
The transition model for final states is the same as
before, but the transition model for non-final states
has two new global free parameters: ?, the probabil-
ity of staying within the field, and ?, the probability
of transitioning to the final state given that we are
staying in the field. The transition function for non-
final states is then
P(s?|s?) =
?
?
?
?
?
?
?
?
?
?
?
(1 ? ?)(? + (1??)|S?| ) if s? = s?
?(?+ (1??)|S?| ) if s? = s+
(1??)
|S?| if s
? ? S?\s?
0 otherwise.
Note that it can bypass the final state, and transi-
tion directly to other non-final states with probabil-
ity (1 ? ?), which models the fact that not all field
occurrences end with a boundary token. The transi-
tion function for non-final states is then
P(s?|s+) =
?
?
?
?
?
?
?
? + (1??)|S?| if s? = s?
(1??)
|S?| if s
? ? S?\s?
0 otherwise.
Note that this has the form of the standard diagonal
function. The reason for the self-loop from the fi-
nal state back to the non-final state is to allow for
field internal punctuation. We tuned the free param-
eters on the development set, and found that ? = 0.5
and ? = 0.995 work well for the advertisements do-
main, and ? = 0.3 and ? = 0.9 work well for the
citations domain. In all cases it works well to set
? = 1 ? ?. Emissions from non-final states are as
376
Ads Citations
Baseline 46.4 27.9
Segment and cluster 62.4 46.5
Supervised 74.4 72.5
Unsup. (learned trans) 48.8 49.7
Unsup. (diagonal trans) 70.0 66.3
+ Hierarchical (learned) 70.1 39.1
+ Hierarchical (given) 70.9 62.1
+ Boundary (learned) 70.4 64.3
+ Boundary (given) 71.9 68.2
+ Hier. + Bnd. (learned) 71.0 ?
+ Hier. + Bnd. (given) 72.7 ?
Table 1: Summary of results. For each experiment, we report
percentage accuracy on the test set. Supervised experiments
use 100 training documents, and unsupervised experiments use
400 training documents. Because unsupervised techniques are
stochastic, those results are averaged over 50 runs, and differ-
ences greater than 1.0% are significant at p=0.05% or better ac-
cording to the t-test. The last 6 rows are not cumulative.
before (hierarchical or not depending on the experi-
ment), while all final states share a boundary emis-
sion model. Note that the boundary emissions are
not smoothed like the field emissions.
We tested both supplying the boundary token dis-
tributions and learning them with reestimation dur-
ing EM. In experiments on the advertisements data
we found that learning the boundary emission model
gives an insignificant raise from 70.0% to 70.4%,
while specifying the list of allowed boundary tokens
gives a significant increase to 71.9%. When com-
bined with the given hierarchical emission model
from the previous section, accuracy rises to 72.7%,
our best unsupervised result on the advertisements
data with 400 training examples. In experiments on
the citations data we found that learning boundary
emission model hurts accuracy, but that given the set
of boundary tokens it boosts accuracy significantly:
increasing it from 66.3% to 68.2%.
5 Semi-supervised Learning
So far, we have largely focused on incorporating
prior knowledge in rather general and implicit ways.
As a final experiment we tested the effect of adding
a small amount of supervision: augmenting the large
amount of unannotated data we use for unsuper-
vised learning with a small amount of annotated
data. There are many possible techniques for semi-
supervised learning; we tested a particularly simple
one. We treat the annotated labels as observed vari-
ables, and when computing sufficient statistics in the
M-step of EM we add the observed counts from the
Figure 7: Learning curves for semi-supervised learning on the
citations task. A separate curve is drawn for each number of
annotated documents. All results are averaged over 50 runs.
annotated documents to the expected counts com-
puted in the E-step. We estimate the transition
function using maximum likelihood from the an-
notated documents only, and do not reestimate it.
Semi-supervised results for the citations domain are
shown in Figure 7. Adding 5 annotated citations
yields no improvement in performance, but adding
20 annotated citations to 300 unannotated citations
boosts performance greatly from 65.2% to 71.3%.
We also tested the utility of this approach in the clas-
sified advertisement domain, and found that it did
not improve accuracy. We believe that this is be-
cause the transition information provided by the su-
pervised data is very useful for the citations data,
which has regular transition structure, but is not as
useful for the advertisements data, which does not.
6 Previous Work
A good amount of prior research can be cast as
supervised learning of field segmentation models,
using various model families and applied to var-
ious domains. McCallum et al (1999) were the
first to compare a number of supervised methods
for learning HMMs for parsing bibliographic cita-
tions. The authors explicitly claim that the domain
would be suitable for unsupervised learning, but
they do not present experimental results. McCallum
et al (2000) applied supervised learning of Maxi-
mum Entropy Markov Models (MEMMs) to the do-
main of parsing Frequently Asked Question (FAQ)
lists into their component field structure. More re-
cently, Peng and McCallum (2004) applied super-
vised learning of Conditional Random Field (CRF)
sequence models to the problem of parsing the head-
377
ers of research papers.
There has also been some previous work on un-
supervised learning of field segmentation models in
particular domains. Pasula et al (2002) performs
limited unsupervised segmentation of bibliographic
citations as a small part of a larger probabilistic
model of identity uncertainty. However, their sys-
tem does not explicitly learn a field segmentation
model for the citations, and encodes a large amount
of hand-supplied information about name forms, ab-
breviation schemes, and so on. More recently, Barzi-
lay and Lee (2004) defined content models, which
can be viewed as field segmentation models occur-
ring at the level of discourse. They perform un-
supervised learning of these models from sets of
news articles which describe similar events. The
fields in that case are the topics discussed in those
articles. They consider a very different set of ap-
plications from the present work, and show that
the learned topic models improve performance on
two discourse-related tasks: information ordering
and extractive document summarization. Most im-
portantly, their learning method differs significantly
from ours; they use a complex and special purpose
algorithm, which is difficult to adapt, while we see
our contribution to be a demonstration of the inter-
play between model family and learned structure.
Because the structure of the HMMs they learn is
similar to ours it seems that their system could ben-
efit from the techniques of this paper. Finally, Blei
and Moreno (2001) use an HMM augmented by an
aspect model to automatically segment documents,
similar in goal to the system of Hearst (1997), but
using techniques more similar to the present work.
7 Conclusions
In this work, we have examined the task of learn-
ing field segmentation models using unsupervised
learning. In two different domains, classified ad-
vertisements and bibliographic citations, we showed
that by constraining the model class we were able
to restrict the search space of EM to models of in-
terest. We used unsupervised learning methods with
400 documents to yield field segmentation models
of a similar quality to those learned using supervised
learning with 50 documents. We demonstrated that
further refinements of the model structure, including
hierarchical mixture emission models and boundary
models, produce additional increases in accuracy.
Finally, we also showed that semi-supervised meth-
ods with a modest amount of labeled data can some-
times be effectively used to get similar good results,
depending on the nature of the problem.
While there are enough resources for the citation
task that much better numbers than ours can be and
have been obtained (with more knowledge and re-
source intensive methods), in domains like classi-
fied ads for lost pets or used bicycles unsupervised
learning may be the only practical option. In these
cases, we find it heartening that the present systems
do as well as they do, even without field-specific
prior knowledge.
8 Acknowledgements
We would like to thank the reviewers for their con-
sideration and insightful comments.
References
R. Barzilay and L. Lee. 2004. Catching the drift: Probabilistic
content models, with applications to generation and summa-
rization. In Proceedings of HLT-NAACL 2004, pages 113?
120.
D. Blei and P. Moreno. 2001. Topic segmentation with an aspect
hidden Markov model. In Proceedings of the 24th SIGIR,
pages 343?348.
M. A. Hearst. 1997. TextTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):33?64.
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 1999.
A machine learning approach to building domain-specific
search engines. In IJCAI-1999.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum
entropy Markov models for information extraction and seg-
mentation. In Proceedings of the 17th ICML, pages 591?598.
Morgan Kaufmann, San Francisco, CA.
H. Pasula, B. Marthi, B. Milch, S. Russell, and I. Shpitser. 2002.
Identity uncertainty and citation matching. In Proceedings of
NIPS 2002.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using Conditional Random Fields.
In Proceedings of HLT-NAACL 2004.
L. R. Rabiner and B.-H. Juang. 1993. Fundamentals of Speech
Recognition. Prentice Hall.
L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and
selected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?286.
K. Toutanova, F. Chen, K. Popat, and T. Hofmann. 2001. Text
classification in a hierarchical mixture model for small train-
ing sets. In CIKM ?01: Proceedings of the tenth interna-
tional conference on Information and knowledge manage-
ment, pages 105?113. ACM Press.
S. Vaithyanathan and B. Dom. 2000. Model-based hierarchical
clustering. In UAI-2000.
378
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433?440,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Accurate, Compact, and Interpretable Tree Annotation
Slav Petrov Leon Barrett Romain Thibaux Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, lbarrett, thibaux, klein}@eecs.berkeley.edu
Abstract
We present an automatic approach to tree annota-
tion in which basic nonterminal symbols are alter-
nately split and merged to maximize the likelihood
of a training treebank. Starting with a simple X-
bar grammar, we learn a new grammar whose non-
terminals are subsymbols of the original nontermi-
nals. In contrast with previous work, we are able
to split various terminals to different degrees, as ap-
propriate to the actual complexity in the data. Our
grammars automatically learn the kinds of linguistic
distinctions exhibited in previous work on manual
tree annotation. On the other hand, our grammars
are much more compact and substantially more ac-
curate than previous work on automatic annotation.
Despite its simplicity, our best grammar achieves
an F1 of 90.2% on the Penn Treebank, higher than
fully lexicalized systems.
1 Introduction
Probabilistic context-free grammars (PCFGs) underlie
most high-performance parsers in one way or another
(Collins, 1999; Charniak, 2000; Charniak and Johnson,
2005). However, as demonstrated in Charniak (1996)
and Klein and Manning (2003), a PCFG which sim-
ply takes the empirical rules and probabilities off of a
treebank does not perform well. This naive grammar
is a poor one because its context-freedom assumptions
are too strong in some places (e.g. it assumes that sub-
ject and object NPs share the same distribution) and too
weak in others (e.g. it assumes that long rewrites are
not decomposable into smaller steps). Therefore, a va-
riety of techniques have been developed to both enrich
and generalize the naive grammar, ranging from simple
tree annotation and symbol splitting (Johnson, 1998;
Klein and Manning, 2003) to full lexicalization and in-
tricate smoothing (Collins, 1999; Charniak, 2000).
In this paper, we investigate the learning of a gram-
mar consistent with a treebank at the level of evalua-
tion symbols (such as NP, VP, etc.) but split based on
the likelihood of the training trees. Klein and Manning
(2003) addressed this question from a linguistic per-
spective, starting with a Markov grammar and manu-
ally splitting symbols in response to observed linguistic
trends in the data. For example, the symbol NP might
be split into the subsymbol NP?S in subject position
and the subsymbol NP?VP in object position. Recently,
Matsuzaki et al (2005) and also Prescher (2005) ex-
hibited an automatic approach in which each symbol is
split into a fixed number of subsymbols. For example,
NP would be split into NP-1 through NP-8. Their ex-
citing result was that, while grammars quickly grew too
large to be managed, a 16-subsymbol induced grammar
reached the parsing performance of Klein and Manning
(2003)?s manual grammar. Other work has also investi-
gated aspects of automatic grammar refinement; for ex-
ample, Chiang and Bikel (2002) learn annotations such
as head rules in a constrained declarative language for
tree-adjoining grammars.
We present a method that combines the strengths of
both manual and automatic approaches while address-
ing some of their common shortcomings. Like Mat-
suzaki et al (2005) and Prescher (2005), we induce
splits in a fully automatic fashion. However, we use a
more sophisticated split-and-merge approach that allo-
cates subsymbols adaptively where they are most effec-
tive, like a linguist would. The grammars recover pat-
terns like those discussed in Klein and Manning (2003),
heavily articulating complex and frequent categories
like NP and VP while barely splitting rare or simple
ones (see Section 3 for an empirical analysis).
Empirically, hierarchical splitting increases the ac-
curacy and lowers the variance of the learned gram-
mars. Another contribution is that, unlike previous
work, we investigate smoothed models, allowing us to
split grammars more heavily before running into the
oversplitting effect discussed in Klein and Manning
(2003), where data fragmentation outweighs increased
expressivity.
Our method is capable of learning grammars of sub-
stantially smaller size and higher accuracy than previ-
ous grammar refinement work, starting from a simpler
initial grammar. For example, even beginning with an
X-bar grammar (see Section 1.1) with 98 symbols, our
best grammar, using 1043 symbols, achieves a test set
F1 of 90.2%. This is a 27% reduction in error and a sig-
nificant reduction in size1 over the most accurate gram-
1This is a 97.5% reduction in number of symbols. Mat-
suzaki et al (2005) do not report a number of rules, but our
small number of symbols and our hierarchical training (which
433
(a) FRAG
RB
Not
NP
DT
this
NN
year
.
.
(b) ROOT
FRAG
FRAG
RB
Not
NP
DT
this
NN
year
.
.
Figure 1: (a) The original tree. (b) The X-bar tree.
mar in Matsuzaki et al (2005). Our grammar?s accu-
racy was higher than fully lexicalized systems, includ-
ing the maximum-entropy inspired parser of Charniak
and Johnson (2005).
1.1 Experimental Setup
We ran our experiments on the Wall Street Journal
(WSJ) portion of the Penn Treebank using the stan-
dard setup: we trained on sections 2 to 21, and we
used section 1 as a validation set for tuning model hy-
perparameters. Section 22 was used as development
set for intermediate results. All of section 23 was re-
served for the final test. We used the EVALB parseval
reference implementation, available from Sekine and
Collins (1997), for scoring. All reported development
set results are averages over four runs. For the final test
we selected the grammar that performed best on the de-
velopment set.
Our experiments are based on a completely unanno-
tated X-bar style grammar, obtained directly from the
Penn Treebank by the binarization procedure shown in
Figure 1. For each local tree rooted at an evaluation
nonterminal X , we introduce a cascade of new nodes
labeled X so that each has two children. Rather than
experiment with head-outward binarization as in Klein
and Manning (2003), we simply used a left branching
binarization; Matsuzaki et al (2005) contains a com-
parison showing that the differences between binariza-
tions are small.
2 Learning
To obtain a grammar from the training trees, we want
to learn a set of rule probabilities ? on latent annota-
tions that maximize the likelihood of the training trees,
despite the fact that the original trees lack the latent
annotations. The Expectation-Maximization (EM) al-
gorithm allows us to do exactly that.2 Given a sen-
tence w and its unannotated tree T , consider a non-
terminal A spanning (r, t) and its children B and C
spanning (r, s) and (s, t). Let Ax be a subsymbol
of A, By of B, and Cz of C. Then the inside and
outside probabilities PIN(r, t, Ax) def= P (wr:t|Ax) and
POUT(r, t, Ax) def= P (w1:rAxwt:n) can be computed re-
encourages sparsity) suggest a large reduction.
2Other techniques are also possible; Henderson (2004)
uses neural networks to induce latent left-corner parser states.
cursively:
PIN(r, t, Ax) =
?
y,z
?(Ax ? ByCz)
?PIN(r, s, By)PIN(s, t, Cz)
POUT(r, s, By) =
?
x,z
?(Ax ? ByCz)
?POUT(r, t, Ax)PIN(s, t, Cz)
POUT(s, t, Cz) =
?
x,y
?(Ax ? ByCz)
?POUT(r, t, Ax)PIN(r, s, By)
Although we show only the binary component here, of
course there are both binary and unary productions that
are included. In the Expectation step, one computes
the posterior probability of each annotated rule and po-
sition in each training set tree T :
P ((r, s, t, Ax ? ByCz)|w, T ) ? POUT(r, t, Ax)
??(Ax ? ByCz)PIN(r, s, By)PIN(s, t, Cz) (1)
In the Maximization step, one uses the above probabil-
ities as weighted observations to update the rule proba-
bilities:
?(Ax ? ByCz) :=
#{Ax ? ByCz}
?
y?,z? #{Ax ? By?Cz?}
Note that, because there is no uncertainty about the lo-
cation of the brackets, this formulation of the inside-
outside algorithm is linear in the length of the sentence
rather than cubic (Pereira and Schabes, 1992).
For our lexicon, we used a simple yet robust method
for dealing with unknown and rare words by extract-
ing a small number of features from the word and then
computing appproximate tagging probabilities.3
2.1 Initialization
EM is only guaranteed to find a local maximum of the
likelihood, and, indeed, in practice it often gets stuck in
a suboptimal configuration. If the search space is very
large, even restarting may not be sufficient to alleviate
this problem. One workaround is to manually specify
some of the annotations. For instance, Matsuzaki et al
(2005) start by annotating their grammar with the iden-
tity of the parent and sibling, which are observed (i.e.
not latent), before adding latent annotations.4 If these
manual annotations are good, they reduce the search
space for EM by constraining it to a smaller region. On
the other hand, this pre-splitting defeats some of the
purpose of automatically learning latent annotations,
3A word is classified into one of 50 unknown word cate-
gories based on the presence of features such as capital let-
ters, digits, and certain suffixes and its tagging probability is
given by: P?(word|tag) = k P?(class|tag) where k is a con-
stant representing P (word|class) and can simply be dropped.
Rare words are modeled using a combination of their known
and unknown distributions.
4In other words, in the terminology of Klein and Man-
ning (2003), they begin with a (vertical order=2, horizontal
order=1) baseline grammar.
434
DT
the (0.50) a (0.24) The (0.08)
that (0.15) this (0.14) some (0.11)
this (0.39)
that (0.28)
That (0.11)
this (0.52)
that (0.36)
another (0.04)
That (0.38)
This (0.34)
each (0.07)
some (0.20)
all (0.19)
those (0.12)
some (0.37)
all (0.29)
those (0.14)
these (0.27)
both (0.21)
Some (0.15)
the (0.54) a (0.25) The (0.09)
the (0.80)
The (0.15)
a (0.01)
the (0.96)
a (0.01)
The (0.01)
The (0.93)
A(0.02)
No(0.01)
a (0.61)
the (0.19)
an (0.10)
a (0.75)
an (0.12)
the (0.03)
Figure 2: Evolution of the DT tag during hierarchical splitting and merging. Shown are the top three words for
each subcategory and their respective probability.
leaving to the user the task of guessing what a good
starting annotation might be.
We take a different, fully automated approach. We
start with a completely unannotated X-bar style gram-
mar as described in Section 1.1. Since we will evaluate
our grammar on its ability to recover the Penn Treebank
nonterminals, we must include them in our grammar.
Therefore, this initialization is the absolute minimum
starting grammar that includes the evaluation nontermi-
nals (and maintains separate grammar symbols for each
of them).5 It is a very compact grammar: 98 symbols,6
236 unary rules, and 3840 binary rules. However, it
also has a very low parsing performance: 65.8/59.8
LP/LR on the development set.
2.2 Splitting
Beginning with this baseline grammar, we repeatedly
split and re-train the grammar. In each iteration we
initialize EM with the results of the smaller gram-
mar, splitting every previous annotation symbol in two
and adding a small amount of randomness (1%) to
break the symmetry. The results are shown in Fig-
ure 3. Hierarchical splitting leads to better parame-
ter estimates over directly estimating a grammar with
2k subsymbols per symbol. While the two procedures
are identical for only two subsymbols (F1: 76.1%),
the hierarchical training performs better for four sub-
symbols (83.7% vs. 83.2%). This advantage grows
as the number of subsymbols increases (88.4% vs.
87.3% for 16 subsymbols). This trend is to be ex-
pected, as the possible interactions between the sub-
symbols grows as their number grows. As an exam-
ple of how staged training proceeds, Figure 2 shows
the evolution of the subsymbols of the determiner (DT)
tag, which first splits demonstratives from determiners,
then splits quantificational elements from demonstra-
tives along one branch and definites from indefinites
along the other.
5If our purpose was only to model language, as measured
for instance by perplexity on new text, it could make sense
to erase even the labels of the Penn Treebank to let EM find
better labels by itself, giving an experiment similar to that of
Pereira and Schabes (1992).
645 part of speech tags, 27 phrasal categories and the 26
intermediate symbols which were added during binarization
Because EM is a local search method, it is likely to
converge to different local maxima for different runs.
In our case, the variance is higher for models with few
subcategories; because not all dependencies can be ex-
pressed with the limited number of subcategories, the
results vary depending on which one EM selects first.
As the grammar size increases, the important depen-
dencies can be modeled, so the variance decreases.
2.3 Merging
It is clear from all previous work that creating more la-
tent annotations can increase accuracy. On the other
hand, oversplitting the grammar can be a serious prob-
lem, as detailed in Klein and Manning (2003). Adding
subsymbols divides grammar statistics into many bins,
resulting in a tighter fit to the training data. At the same
time, each bin gives a less robust estimate of the gram-
mar probabilities, leading to overfitting. Therefore, it
would be to our advantage to split the latent annota-
tions only where needed, rather than splitting them all
as in Matsuzaki et al (2005). In addition, if all sym-
bols are split equally often, one quickly (4 split cycles)
reaches the limits of what is computationally feasible
in terms of training time and memory usage.
Consider the comma POS tag. We would like to see
only one sort of this tag because, despite its frequency,
it always produces the terminal comma (barring a few
annotation errors in the treebank). On the other hand,
we would expect to find an advantage in distinguishing
between various verbal categories and NP types. Addi-
tionally, splitting symbols like the comma is not only
unnecessary, but potentially harmful, since it need-
lessly fragments observations of other symbols? behav-
ior.
It should be noted that simple frequency statistics are
not sufficient for determining how often to split each
symbol. Consider the closed part-of-speech classes
(e.g. DT, CC, IN) or the nonterminal ADJP. These
symbols are very common, and certainly do contain
subcategories, but there is little to be gained from
exhaustively splitting them before even beginning to
model the rarer symbols that describe the complex in-
ner correlations inside verb phrases. Our solution is
to use a split-and-merge approach broadly reminiscent
of ISODATA, a classic clustering procedure (Ball and
435
Hall, 1967).
To prevent oversplitting, we could measure the util-
ity of splitting each latent annotation individually and
then split the best ones first. However, not only is this
impractical, requiring an entire training phase for each
new split, but it assumes the contributions of multiple
splits are independent. In fact, extra subsymbols may
need to be added to several nonterminals before they
can cooperate to pass information along the parse tree.
Therefore, we go in the opposite direction; that is, we
split every symbol in two, train, and then measure for
each annotation the loss in likelihood incurred when
removing it. If this loss is small, the new annotation
does not carry enough useful information and can be
removed. What is more, contrary to the gain in like-
lihood for splitting, the loss in likelihood for merging
can be efficiently approximated.7
Let T be a training tree generating a sentence w.
Consider a node n of T spanning (r, t) with the label
A; that is, the subtree rooted at n generates wr:t and
has the label A. In the latent model, its label A is split
up into several latent labels, Ax. The likelihood of the
data can be recovered from the inside and outside prob-
abilities at n:
P(w, T ) =
?
x
PIN(r, t, Ax)POUT(r, t, Ax) (2)
Consider merging, at n only, two annotations A1 and
A2. Since A now combines the statistics of A1 and A2,
its production probabilities are the sum of those of A1
and A2, weighted by their relative frequency p1 and p2
in the training data. Therefore the inside score of A is:
PIN(r, t, A) = p1PIN(r, t, A1) + p2PIN(r, t, A2)
Since A can be produced as A1 or A2 by its parents, its
outside score is:
POUT(r, t, A) = POUT(r, t, A1) + POUT(r, t, A2)
Replacing these quantities in (2) gives us the likelihood
Pn(w, T ) where these two annotations and their corre-
sponding rules have been merged, around only node n.
We approximate the overall loss in data likelihood
due to merging A1 and A2 everywhere in all sentences
wi by the product of this loss for each local change:
?ANNOTATION (A1, A2) =
?
i
?
n?Ti
Pn(wi, Ti)
P(wi, Ti)
This expression is an approximation because it neglects
interactions between instances of a symbol at multiple
places in the same tree. These instances, however, are
7The idea of merging complex hypotheses to encourage
generalization is also examined in Stolcke and Omohundro
(1994), who used a chunking approach to propose new pro-
ductions in fully unsupervised grammar induction. They also
found it necessary to make local choices to guide their likeli-
hood search.
often far apart and are likely to interact only weakly,
and this simplification avoids the prohibitive cost of
running an inference algorithm for each tree and an-
notation. We refer to the operation of splitting anno-
tations and re-merging some them based on likelihood
loss as a split-merge (SM) cycle. SM cycles allow us to
progressively increase the complexity of our grammar,
giving priority to the most useful extensions.
In our experiments, merging was quite valuable. De-
pending on how many splits were reversed, we could
reduce the grammar size at the cost of little or no loss
of performance, or even a gain. We found that merging
50% of the newly split symbols dramatically reduced
the grammar size after each splitting round, so that af-
ter 6 SM cycles, the grammar was only 17% of the size
it would otherwise have been (1043 vs. 6273 subcat-
egories), while at the same time there was no loss in
accuracy (Figure 3). Actually, the accuracy even in-
creases, by 1.1% at 5 SM cycles. The numbers of splits
learned turned out to not be a direct function of symbol
frequency; the numbers of symbols for both lexical and
nonlexical tags after 4 SM cycles are given in Table 2.
Furthermore, merging makes large amounts of splitting
possible. It allows us to go from 4 splits, equivalent to
the 24 = 16 substates of Matsuzaki et al (2005), to 6
SM iterations, which take a few days to run on the Penn
Treebank.
2.4 Smoothing
Splitting nonterminals leads to a better fit to the data by
allowing each annotation to specialize in representing
only a fraction of the data. The smaller this fraction,
the higher the risk of overfitting. Merging, by allow-
ing only the most beneficial annotations, helps mitigate
this risk, but it is not the only way. We can further
minimize overfitting by forcing the production proba-
bilities from annotations of the same nonterminal to be
similar. For example, a noun phrase in subject position
certainly has a distinct distribution, but it may benefit
from being smoothed with counts from all other noun
phrases. Smoothing the productions of each subsym-
bol by shrinking them towards their common base sym-
bol gives us a more reliable estimate, allowing them to
share statistical strength.
We perform smoothing in a linear way. The es-
timated probability of a production px = P(Ax ?
By Cz) is interpolated with the average over all sub-
symbols of A.
p?x = (1 ? ?)px + ?p? where p? =
1
n
?
x
px
Here, ? is a small constant: we found 0.01 to be a good
value, but the actual quantity was surprisingly unimpor-
tant. Because smoothing is most necessary when pro-
duction statistics are least reliable, we expect smooth-
ing to help more with larger numbers of subsymbols.
This is exactly what we observe in Figure 3, where
smoothing initially hurts (subsymbols are quite distinct
436
and do not need their estimates pooled) but eventually
helps (as symbols have finer distinctions in behavior
and smaller data support).
2.5 Parsing
When parsing new sentences with an annotated gram-
mar, returning the most likely (unannotated) tree is in-
tractable: to obtain the probability of an unannotated
tree, one must sum over combinatorially many annota-
tion trees (derivations) for each tree (Sima?an, 1992).
Matsuzaki et al (2005) discuss two approximations.
The first is settling for the most probable derivation
rather than most probable parse, i.e. returning the single
most likely (Viterbi) annotated tree (derivation). This
approximation is justified if the sum is dominated by
one particular annotated tree. The second approxima-
tion that Matsuzaki et al (2005) present is the Viterbi
parse under a new sentence-specific PCFG, whose rule
probabilities are given as the solution of a variational
approximation of the original grammar. However, their
rule probabilities turn out to be the posterior probabil-
ity, given the sentence, of each rule being used at each
position in the tree. Their algorithm is therefore the la-
belled recall algorithm of Goodman (1996) but applied
to rules. That is, it returns the tree whose expected
number of correct rules is maximal. Thus, assuming
one is interested in a per-position score like F1 (which
is its own debate), this method of parsing is actually
more appropriate than finding the most likely parse,
not simply a cheap approximation of it, and it need not
be derived by a variational argument. We refer to this
method of parsing as the max-rule parser. Since this
method is not a contribution of this paper, we refer the
reader to the fuller presentations in Goodman (1996)
and Matsuzaki et al (2005). Note that contrary to the
original labelled recall algorithm, which maximizes the
number of correct symbols, this tree only contains rules
allowed by the grammar. As a result, the percentage of
complete matches with the max-rule parser is typically
higher than with the Viterbi parser. (37.5% vs. 35.8%
for our best grammar).
These posterior rule probabilities are still given by
(1), but, since the structure of the tree is no longer
known, we must sum over it when computing the in-
side and outside probabilities:
PIN(r, t, Ax)=
?
B,C,s
?
y,z
?(Ax ? ByCz)?
PIN(r, s, By)PIN(s, t, Cz)
POUT(r, s, By)=
?
A,C,t
?
x,z
?(Ax ? ByCz)?
POUT(r, t, Ax)PIN(s, t, Cz)
POUT(s, t, Cz)=
?
A,B,r
?
x,y
?(Ax ? ByCz)?
POUT(r, t, Ax)PIN(r, s, By)
For efficiency reasons, we use a coarse-to-fine prun-
ing scheme like that of Caraballo and Charniak (1998).
For a given sentence, we first run the inside-outside
algorithm using the baseline (unannotated) grammar,
 74
 76
 78
 80
 82
 84
 86
 88
 90
 200  400  600  800  1000
F1
Total number of grammar symbols
50% Merging and Smoothing
50% Merging
Splitting but no Merging
Flat Training
Figure 3: Hierarchical training leads to better parame-
ter estimates. Merging reduces the grammar size sig-
nificantly, while preserving the accuracy and enabling
us to do more SM cycles. Parameter smoothing leads
to even better accuracy for grammars with high com-
plexity.
producing a packed forest representation of the poste-
rior symbol probabilities for each span. For example,
one span might have a posterior probability of 0.8 of
the symbol NP, but e?10 for PP. Then, we parse with the
larger annotated grammar, but, at each span, we prune
away any symbols whose posterior probability under
the baseline grammar falls below a certain threshold
(e?8 in our experiments). Even though our baseline
grammar has a very low accuracy, we found that this
pruning barely impacts the performance of our better
grammars, while significantly reducing the computa-
tional cost. For a grammar with 479 subcategories (4
SM cycles), lowering the threshold to e?15 led to an F1
improvement of 0.13% (89.03 vs. 89.16) on the devel-
opment set but increased the parsing time by a factor of
16.
3 Analysis
So far, we have presented a split-merge method for
learning to iteratively subcategorize basic symbols
like NP and VP into automatically induced subsym-
bols (subcategories in the original sense of Chomsky
(1965)). This approach gives parsing accuracies of up
to 90.7% on the development set, substantially higher
than previous symbol-splitting approaches, while start-
ing from an extremely simple base grammar. However,
in general, any automatic induction system is in dan-
ger of being entirely uninterpretable. In this section,
we examine the learned grammars, discussing what is
learned. We focus particularly on connections with the
linguistically motivated annotations of Klein and Man-
ning (2003), which we do generally recover.
Inspecting a large grammar by hand is difficult, but
fortunately, our baseline grammar has less than 100
nonterminal symbols, and even our most complicated
grammar has only 1043 total (sub)symbols. It is there-
437
VBZ
VBZ-0 gives sells takes
VBZ-1 comes goes works
VBZ-2 includes owns is
VBZ-3 puts provides takes
VBZ-4 says adds Says
VBZ-5 believes means thinks
VBZ-6 expects makes calls
VBZ-7 plans expects wants
VBZ-8 is ?s gets
VBZ-9 ?s is remains
VBZ-10 has ?s is
VBZ-11 does Is Does
NNP
NNP-0 Jr. Goldman INC.
NNP-1 Bush Noriega Peters
NNP-2 J. E. L.
NNP-3 York Francisco Street
NNP-4 Inc Exchange Co
NNP-5 Inc. Corp. Co.
NNP-6 Stock Exchange York
NNP-7 Corp. Inc. Group
NNP-8 Congress Japan IBM
NNP-9 Friday September August
NNP-10 Shearson D. Ford
NNP-11 U.S. Treasury Senate
NNP-12 John Robert James
NNP-13 Mr. Ms. President
NNP-14 Oct. Nov. Sept.
NNP-15 New San Wall
JJS
JJS-0 largest latest biggest
JJS-1 least best worst
JJS-2 most Most least
DT
DT-0 the The a
DT-1 A An Another
DT-2 The No This
DT-3 The Some These
DT-4 all those some
DT-5 some these both
DT-6 That This each
DT-7 this that each
DT-8 the The a
DT-9 no any some
DT-10 an a the
DT-11 a this the
CD
CD-0 1 50 100
CD-1 8.50 15 1.2
CD-2 8 10 20
CD-3 1 30 31
CD-4 1989 1990 1988
CD-5 1988 1987 1990
CD-6 two three five
CD-7 one One Three
CD-8 12 34 14
CD-9 78 58 34
CD-10 one two three
CD-11 million billion trillion
PRP
PRP-0 It He I
PRP-1 it he they
PRP-2 it them him
RBR
RBR-0 further lower higher
RBR-1 more less More
RBR-2 earlier Earlier later
IN
IN-0 In With After
IN-1 In For At
IN-2 in for on
IN-3 of for on
IN-4 from on with
IN-5 at for by
IN-6 by in with
IN-7 for with on
IN-8 If While As
IN-9 because if while
IN-10 whether if That
IN-11 that like whether
IN-12 about over between
IN-13 as de Up
IN-14 than ago until
IN-15 out up down
RB
RB-0 recently previously still
RB-1 here back now
RB-2 very highly relatively
RB-3 so too as
RB-4 also now still
RB-5 however Now However
RB-6 much far enough
RB-7 even well then
RB-8 as about nearly
RB-9 only just almost
RB-10 ago earlier later
RB-11 rather instead because
RB-12 back close ahead
RB-13 up down off
RB-14 not Not maybe
RB-15 n?t not also
Table 1: The most frequent three words in the subcategories of several part-of-speech tags.
fore relatively straightforward to review the broad be-
havior of a grammar. In this section, we review a
randomly-selected grammar after 4 SM cycles that pro-
duced an F1 score on the development set of 89.11. We
feel it is reasonable to present only a single grammar
because all the grammars are very similar. For exam-
ple, after 4 SM cycles, the F1 scores of the 4 trained
grammars have a variance of only 0.024, which is tiny
compared to the deviation of 0.43 obtained by Mat-
suzaki et al (2005)). Furthermore, these grammars
allocate splits to nonterminals with a variance of only
0.32, so they agree to within a single latent state.
3.1 Lexical Splits
One of the original motivations for lexicalization of
parsers is the fact that part-of-speech (POS) tags are
usually far too general to encapsulate a word?s syntac-
tic behavior. In the limit, each word may well have
its own unique syntactic behavior, especially when, as
in modern parsers, semantic selectional preferences are
lumped in with traditional syntactic trends. However,
in practice, and given limited data, the relationship be-
tween specific words and their syntactic contexts may
be best modeled at a level more fine than POS tag but
less fine than lexical identity.
In our model, POS tags are split just like any other
grammar symbol: the subsymbols for several tags are
shown in Table 1, along with their most frequent mem-
bers. In most cases, the categories are recognizable as
either classic subcategories or an interpretable division
of some other kind.
Nominal categories are the most heavily split (see
Table 2), and have the splits which are most semantic
in nature (though not without syntactic correlations).
For example, plural common nouns (NNS) divide into
the maximum number of categories (16). One cate-
gory consists primarily of dates, whose typical parent
is an NP subsymbol whose typical parent is a root S,
essentially modeling the temporal noun annotation dis-
cussed in Klein and Manning (2003). Another cate-
gory specializes in capitalized words, preferring as a
parent an NP with an S parent (i.e. subject position).
A third category specializes in monetary units, and
so on. These kinds of syntactico-semantic categories
are typical, and, given distributional clustering results
like those of Schuetze (1998), unsurprising. The sin-
gular nouns are broadly similar, if slightly more ho-
mogenous, being dominated by categories for stocks
and trading. The proper noun category (NNP, shown)
also splits into the maximum 16 categories, including
months, countries, variants of Co. and Inc., first names,
last names, initials, and so on.
Verbal categories are also heavily split. Verbal sub-
categories sometimes reflect syntactic selectional pref-
erences, sometimes reflect semantic selectional prefer-
ences, and sometimes reflect other aspects of verbal
syntax. For example, the present tense third person
verb subsymbols (VBZ) are shown. The auxiliaries get
three clear categories: do, have, and be (this pattern
repeats in other tenses), as well a fourth category for
the ambiguous ?s. Verbs of communication (says) and
438
NNP 62 CC 7 WP$ 2 NP 37 CONJP 2
JJ 58 JJR 5 WDT 2 VP 32 FRAG 2
NNS 57 JJS 5 -RRB- 2 PP 28 NAC 2
NN 56 : 5 ? 1 ADVP 22 UCP 2
VBN 49 PRP 4 FW 1 S 21 WHADVP 2
RB 47 PRP$ 4 RBS 1 ADJP 19 INTJ 1
VBG 40 MD 3 TO 1 SBAR 15 SBARQ 1
VB 37 RBR 3 $ 1 QP 9 RRC 1
VBD 36 WP 2 UH 1 WHNP 5 WHADJP 1
CD 32 POS 2 , 1 PRN 4 X 1
IN 27 PDT 2 ? 1 NX 4 ROOT 1
VBZ 25 WRB 2 SYM 1 SINV 3 LST 1
VBP 19 -LRB- 2 RP 1 PRT 2
DT 17 . 2 LS 1 WHPP 2
NNPS 11 EX 2 # 1 SQ 2
Table 2: Number of latent annotations determined by
our split-merge procedure after 6 SM cycles
propositional attitudes (beleives) that tend to take in-
flected sentential complements dominate two classes,
while control verbs (wants) fill out another.
As an example of a less-split category, the superla-
tive adjectives (JJS) are split into three categories,
corresponding principally to most, least, and largest,
with most frequent parents NP, QP, and ADVP, respec-
tively. The relative adjectives (JJR) are split in the same
way. Relative adverbs (RBR) are split into a different
three categories, corresponding to (usually metaphor-
ical) distance (further), degree (more), and time (ear-
lier). Personal pronouns (PRP) are well-divided into
three categories, roughly: nominative case, accusative
case, and sentence-initial nominative case, which each
correlate very strongly with syntactic position. As an-
other example of a specific trend which was mentioned
by Klein and Manning (2003), adverbs (RB) do contain
splits for adverbs under ADVPs (also), NPs (only), and
VPs (not).
Functional categories generally show fewer splits,
but those splits that they do exhibit are known to be
strongly correlated with syntactic behavior. For exam-
ple, determiners (DT) divide along several axes: defi-
nite (the), indefinite (a), demonstrative (this), quantifi-
cational (some), negative polarity (no, any), and var-
ious upper- and lower-case distinctions inside these
types. Here, it is interesting to note that these distinc-
tions emerge in a predictable order (see Figure 2 for DT
splits), beginning with the distinction between demon-
stratives and non-demonstratives, with the other dis-
tinctions emerging subsequently; this echoes the result
of Klein and Manning (2003), where the authors chose
to distinguish the demonstrative constrast, but not the
additional ones learned here.
Another very important distinction, as shown in
Klein and Manning (2003), is the various subdivi-
sions in the preposition class (IN). Learned first is
the split between subordinating conjunctions like that
and proper prepositions. Then, subdivisions of each
emerge: wh-subordinators like if, noun-modifying
prepositions like of, predominantly verb-modifying
ones like from, and so on.
Many other interesting patterns emerge, including
ADVP
ADVP-0 RB-13 NP-2 RB-13 PP-3 IN-15 NP-2
ADVP-1 NP-3 RB-10 NP-3 RBR-2 NP-3 IN-14
ADVP-2 IN-5 JJS-1 RB-8 RB-6 RB-6 RBR-1
ADVP-3 RBR-0 RB-12 PP-0 RP-0
ADVP-4 RB-3 RB-6 ADVP-2 SBAR-8 ADVP-2 PP-5
ADVP-5 RB-5 NP-3 RB-10 RB-0
ADVP-6 RB-4 RB-0 RB-3 RB-6
ADVP-7 RB-7 IN-5 JJS-1 RB-6
ADVP-8 RB-0 RBS-0 RBR-1 IN-14
ADVP-9 RB-1 IN-15 RBR-0
SINV
SINV-0 VP-14 NP-7 VP-14 VP-15 NP-7 NP-9
VP-14 NP-7 .-0
SINV-1 S-6 ,-0 VP-14 NP-7 .-0
S-11 VP-14 NP-7 .-0
Table 3: The most frequent three productions of some
latent annotations.
many classical distinctions not specifically mentioned
or modeled in previous work. For example, the wh-
determiners (WDT) split into one class for that and an-
other for which, while the wh-adverbs align by refer-
ence type: event-based how and why vs. entity-based
when and where. The possesive particle (POS) has one
class for the standard ?s, but another for the plural-only
apostrophe. As a final example, the cardinal number
nonterminal (CD) induces various categories for dates,
fractions, spelled-out numbers, large (usually financial)
digit sequences, and others.
3.2 Phrasal Splits
Analyzing the splits of phrasal nonterminals is more
difficult than for lexical categories, and we can merely
give illustrations. We show some of the top productions
of two categories in Table 3.
A nonterminal split can be used to model an other-
wise uncaptured correlation between that symbol?s ex-
ternal context (e.g. its parent symbol) and its internal
context (e.g. its child symbols). A particularly clean ex-
ample of a split correlating external with internal con-
texts is the inverted sentence category (SINV), which
has only two subsymbols, one which usually has the
ROOT symbol as its parent (and which has sentence fi-
nal puncutation as its last child), and a second subsym-
bol which occurs in embedded contexts (and does not
end in punctuation). Such patterns are common, but of-
ten less easy to predict. For example, possesive NPs get
two subsymbols, depending on whether their possessor
is a person / country or an organization. The external
correlation turns out to be that people and countries are
more likely to possess a subject NP, while organizations
are more likely to possess an object NP.
Nonterminal splits can also be used to relay infor-
mation between distant tree nodes, though untangling
this kind of propagation and distilling it into clean ex-
amples is not trivial. As one example, the subsym-
bol S-12 (matrix clauses) occurs only under the ROOT
symbol. S-12?s children usually include NP-8, which
in turn usually includes PRP-0, the capitalized nomi-
native pronouns, DT-{1,2,6} (the capitalized determin-
439
ers), and so on. This same propagation occurs even
more frequently in the intermediate symbols, with, for
example, one subsymbol of NP symbol specializing in
propagating proper noun sequences.
Verb phrases, unsurprisingly, also receive a full set
of subsymbols, including categories for infinitive VPs,
passive VPs, several for intransitive VPs, several for
transitive VPs with NP and PP objects, and one for
sentential complements. As an example of how lexi-
cal splits can interact with phrasal splits, the two most
frequent rewrites involving intransitive past tense verbs
(VBD) involve two different VPs and VBDs: VP-14 ?
VBD-13 and VP-15 ? VBD-12. The difference is that
VP-14s are main clause VPs, while VP-15s are sub-
ordinate clause VPs. Correspondingly, VBD-13s are
verbs of communication (said, reported), while VBD-
12s are an assortment of verbs which often appear in
subordinate contexts (did, began).
Other interesting phenomena also emerge. For ex-
ample, intermediate symbols, which in previous work
were very heavily, manually split using a Markov pro-
cess, end up encoding processes which are largely
Markov, but more complex. For example, some classes
of adverb phrases (those with RB-4 as their head) are
?forgotten? by the VP intermediate grammar. The rele-
vant rule is the very probable VP-2 ? VP-2 ADVP-6;
adding this ADVP to a growing VP does not change the
VP subsymbol. In essense, at least a partial distinction
between verbal arguments and verbal adjucts has been
learned (as exploited in Collins (1999), for example).
4 Conclusions
By using a split-and-merge strategy and beginning with
the barest possible initial structure, our method reli-
ably learns a PCFG that is remarkably good at pars-
ing. Hierarchical split/merge training enables us to
learn compact but accurate grammars, ranging from ex-
tremely compact (an F1 of 78% with only 147 sym-
bols) to extremely accurate (an F1 of 90.2% for our
largest grammar with only 1043 symbols). Splitting
provides a tight fit to the training data, while merging
improves generalization and controls grammar size. In
order to overcome data fragmentation and overfitting,
we smooth our parameters. Smoothing allows us to
add a larger number of annotations, each specializing
in only a fraction of the data, without overfitting our
training set. As one can see in Table 4, the resulting
parser ranks among the best lexicalized parsers, beat-
ing those of Collins (1999) and Charniak and Johnson
(2005).8 Its F1 performance is a 27% reduction in er-
ror over Matsuzaki et al (2005) and Klein and Man-
ning (2003). Not only is our parser more accurate, but
the learned grammar is also significantly smaller than
that of previous work. While this all is accomplished
with only automatic learning, the resulting grammar is
8Even with the Viterbi parser our best grammar achieves
88.7/88.9 LP/LR.
? 40 words LP LR CB 0CB
Klein and Manning (2003) 86.9 85.7 1.10 60.3
Matsuzaki et al (2005) 86.6 86.7 1.19 61.1
Collins (1999) 88.7 88.5 0.92 66.7
Charniak and Johnson (2005) 90.1 90.1 0.74 70.1
This Paper 90.3 90.0 0.78 68.5
all sentences LP LR CB 0CB
Klein and Manning (2003) 86.3 85.1 1.31 57.2
Matsuzaki et al (2005) 86.1 86.0 1.39 58.3
Collins (1999) 88.3 88.1 1.06 64.0
Charniak and Johnson (2005) 89.5 89.6 0.88 67.6
This Paper 89.8 89.6 0.92 66.3
Table 4: Comparison of our results with those of others.
human-interpretable. It shows most of the manually in-
troduced annotations discussed by Klein and Manning
(2003), but also learns other linguistic phenomena.
References
G. Ball and D. Hall. 1967. A clustering technique for sum-
marizing multivariate data. Behavioral Science.
S. Caraballo and E. Charniak. 1998. New figures of merit
for best?first probabilistic chart parsing. In Computational
Lingusitics, p. 275?298.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In ACL?05,
p. 173?180.
E. Charniak. 1996. Tree-bank grammars. In AAAI ?96, p.
1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired parser. In
NAACL ?00, p. 132?139.
D. Chiang and D. Bikel. 2002. Recovering latent information
in treebanks. In Computational Linguistics.
N. Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press.
M. Collins. 1999. Head-Driven Statistical Models for Natu-
ral Language Parsing. Ph.D. thesis, U. of Pennsylvania.
J. Goodman. 1996. Parsing algorithms and metrics. In ACL
?96, p. 177?183.
J. Henderson. 2004. Discriminative training of a neural net-
work statistical parser. In ACL ?04.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. ACL ?03, p. 423?430.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic
CFG with latent annotations. In ACL ?05, p. 75?82.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL ?92, p. 128?135.
D. Prescher. 2005. Inducing head-driven PCFGs with la-
tent heads: Refining a tree-bank grammar for parsing. In
ECML?05.
H. Schuetze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
S. Sekine and M. J. Collins. 1997. EVALB bracket scoring
program. http://nlp.cs.nyu.edu/evalb/.
K. Sima?an. 1992. Computatoinal complexity of probabilis-
tic disambiguation. Grammars, 5:125?151.
A. Stolcke and S. Omohundro. 1994. Inducing probabilistic
grammars by bayesian model merging. In Grammatical
Inference and Applications, p. 106?118.
440
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761?768,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An End-to-End Discriminative Approach to Machine Translation
Percy Liang Alexandre Bouchard-Co?te? Dan Klein Ben Taskar
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, bouchard, klein, taskar}@cs.berkeley.edu
Abstract
We present a perceptron-style discriminative ap-
proach to machine translation in which large feature
sets can be exploited. Unlike discriminative rerank-
ing approaches, our system can take advantage of
learned features in all stages of decoding. We first
discuss several challenges to error-driven discrim-
inative approaches. In particular, we explore dif-
ferent ways of updating parameters given a training
example. We find that making frequent but smaller
updates is preferable to making fewer but larger up-
dates. Then, we discuss an array of features and
show both how they quantitatively increase BLEU
score and how they qualitatively interact on spe-
cific examples. One particular feature we investi-
gate is a novel way to introduce learning into the
initial phrase extraction process, which has previ-
ously been entirely heuristic.
1 Introduction
The generative, noisy-channel paradigm has his-
torically served as the foundation for most of the
work in statistical machine translation (Brown et
al., 1994). At the same time, discriminative meth-
ods have provided substantial improvements over
generative models on a wide range of NLP tasks.
They allow one to easily encode domain knowl-
edge in the form of features. Moreover, param-
eters are tuned to directly minimize error rather
than to maximize joint likelihood, which may not
correspond well to the task objective.
In this paper, we present an end-to-end dis-
criminative approach to machine translation. The
proposed system is phrase-based, as in Koehn et
al. (2003), but uses an online perceptron training
scheme to learn model parameters. Unlike mini-
mum error rate training (Och, 2003), our system is
able to exploit large numbers of specific features
in the same manner as static reranking systems
(Shen et al, 2004; Och et al, 2004). However,
unlike static rerankers, our system does not rely
on a baseline translation system. Instead, it up-
dates based on its own n-best lists. As parameter
estimates improve, the system produces better n-
best lists, which can in turn enable better updates
in future training iterations. In this paper, we fo-
cus on two aspects of the problem of discrimina-
tive translation: the inherent difficulty of learning
from reference translations, and the challenge of
engineering effective features for this task.
Discriminative learning from reference transla-
tions is inherently problematic because standard
discriminative methods need to know which out-
puts are correct and which are not. However, a
proposed translation that differs from a reference
translation need not be incorrect. It may differ
in word choice, literalness, or style, yet be fully
acceptable. Pushing our system to avoid such al-
ternate translations is undesirable. On the other
hand, even if a system produces a reference trans-
lation, it may do so by abusing the hidden struc-
ture (sentence segmentation and alignment). We
can therefore never be entirely sure whether or not
a proposed output is safe to update towards. We
discuss this issue in detail in Section 5, where we
show that conservative updates (which push the
system towards a local variant of the current pre-
diction) are more effective than more aggressive
updates (which try to directly update towards the
reference).
The second major contribution of this work is
an investigation of an array of features for our
model. We show how our features quantitatively
increase BLEU score, as well as how they qual-
itatively interact on specific examples. We first
consider learning weights for individual phrases
and part-of-speech patterns, showing gains from
each. We then present a novel way to parameter-
ize and introduce learning into the initial phrase
extraction process. In particular, we introduce
alignment constellation features, which allow us
to weight phrases based on the word alignment
pattern that led to their extraction. This kind of
761
feature provides a potential way to initially extract
phrases more aggressively and then later down-
weight undesirable patterns, essentially learning a
weighted extraction heuristic. Finally, we use POS
features to parameterize a distortion model in a
limited distortion decoder (Zens and Ney, 2004;
Tillmann and Zhang, 2005). We show that over-
all, BLEU score increases from 28.4 to 29.6 on
French-English.
2 Approach
2.1 Translation as structured classification
Machine translation can be seen as a structured
classification task, in which the goal is to learn
a mapping from an input (French) sentence x to
an output (English) sentence y. Given this setup,
discriminative methods allow us to define a broad
class of features ? that operate on (x,y). For ex-
ample, some features would measure the fluency
of y and others would measure the faithfulness of
y as a translation of x.
However, the translation task in this framework
differs from traditional applications of discrimina-
tive structured classification such as POS tagging
and parsing in a fundamental way. Whereas in
POS tagging, there is a one-to-one correspondence
between the words x and the tags y, the correspon-
dence between x and y in machine translation is
not only much more complex, but is in fact un-
known. Therefore, we introduce a hidden corre-
spondence structure h and work with the feature
vector ?(x,y,h).
The phrase-based model of Koehn et al (2003)
is an instance of this framework. In their model,
the correspondence h consists of (1) the segmen-
tation of the input sentence into phrases, (2) the
segmentation of the output sentence into the same
number of phrases, and (3) a bijection between
the input and output phrases. The feature vec-
tor ?(x,y,h) contains four components: the log
probability of the output sentence y under a lan-
guage model, the score of translating x into y
based on a phrase table, a distortion score, and a
length penalty.1 In Section 6, we vastly increase
the number of features to take advantage of the full
power of discriminative training.
Another example of this framework is the hier-
archical model of Chiang (2005). In this model
the correspondence h is a synchronous parse tree
1More components can be added to the feature vector if
additional language models or phrase tables are available.
over input and output sentences, and features in-
clude the scores of various productions used in the
tree.
Given features ? and a corresponding set of pa-
rameters w, a standard classification rule f is to
return the highest scoring output sentence y, max-
imizing over correspondences h:
f(x;w) = argmax
y,h
w ? ?(x,y,h). (1)
In the phrase-based model, computing the
argmax exactly is intractable, so we approximate
f with beam decoding.
2.2 Perceptron-based training
To tune the parameters w of the model, we use the
averaged perceptron algorithm (Collins, 2002) be-
cause of its efficiency and past success on various
NLP tasks (Collins and Roark, 2004; Roark et al,
2004). In principle, w could have been tuned by
maximizing conditional probability or maximiz-
ing margin. However, these two options require
either marginalization or numerical optimization,
neither of which is tractable over the space of out-
put sentences y and correspondences h. In con-
trast, the perceptron algorithm requires only a de-
coder that computes f(x;w).
Recall the traditional perceptron update rule on
an example (xi,yi) is
w? w + ?(xi,yt)? ?(xi,yp), (2)
where yt = yi is the target output and yp =
f(xi;w) = argmaxyw ? ?(xi,y) is the predic-
tion using the current parameters w.
We adapt this update rule to work with hidden
variables as follows:
w? w + ?(xi,yt,ht)??(xi,yp,hp), (3)
where (yp,hp) is the argmax computation in
Equation 1, and (yt,ht) is the target that we up-
date towards. If (yt,ht) is the same argmax com-
putation with the additional constraint that yt =
yi, then Equation 3 can be interpreted as a Viterbi
approximation to the stochastic gradient
EP (h|xi,yi;w)?(xi,yi,h)?EP (y,h|xi;w)?(xi,y,h)
for the following conditional likelihood objective:
P (yi | xi) ?
?
h
exp(w ? ?(xi,yi,h)).
762
      
    	
 

   
 
 


 




      
	

  

 
  
  
	 
   
      
  
	 
   
	
     

  

 
 

 
	 
  
	
     

  

 
  
 
 	
       
	

	 
   
	
   
 
	


	 
   
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 881?888,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Prototype-Driven Grammar Induction
Aria Haghighi
Computer Science Division
University of California Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California Berkeley
klein@cs.berkeley.edu
Abstract
We investigate prototype-driven learning for pri-
marily unsupervised grammar induction. Prior
knowledge is specified declaratively, by providing a
few canonical examples of each target phrase type.
This sparse prototype information is then propa-
gated across a corpus using distributional similar-
ity features, which augment an otherwise standard
PCFG model. We show that distributional features
are effective at distinguishing bracket labels, but not
determining bracket locations. To improve the qual-
ity of the induced trees, we combine our PCFG in-
duction with the CCM model of Klein and Manning
(2002), which has complementary stengths: it iden-
tifies brackets but does not label them. Using only
a handful of prototypes, we show substantial im-
provements over naive PCFG induction for English
and Chinese grammar induction.
1 Introduction
There has been a great deal of work on unsuper-
vised grammar induction, with motivations rang-
ing from scientific interest in language acquisi-
tion to engineering interest in parser construc-
tion (Carroll and Charniak, 1992; Clark, 2001).
Recent work has successfully induced unlabeled
grammatical structure, but has not successfully
learned labeled tree structure (Klein and Manning,
2002; Klein and Manning, 2004; Smith and Eis-
ner, 2004) .
In this paper, our goal is to build a system capa-
ble of producing labeled parses in a target gram-
mar with as little total effort as possible. We in-
vestigate a prototype-driven approach to grammar
induction, in which one supplies canonical ex-
amples of each target concept. For example, we
might specify that we are interested in trees which
use the symbol NP and then list several examples
of prototypical NPs (determiner noun, pronouns,
etc., see figure 1 for a sample prototype list). This
prototype information is similar to specifying an
annotation scheme, which even human annotators
must be provided before they can begin the con-
struction of a treebank. In principle, prototype-
driven learning is just a kind of semi-supervised
learning. However, in practice, the information we
provide is on the order of dozens of total seed in-
stances, instead of a handful of fully parsed trees,
and is of a different nature.
The prototype-driven approach has three
strengths. First, since we provide a set of target
symbols, we can evaluate induced trees using
standard labeled parsing metrics, rather than the
far more forgiving unlabeled metrics described in,
for example, Klein and Manning (2004). Second,
knowledge is declaratively specified in an inter-
pretable way (see figure 1). If a user of the system
is unhappy with its systematic behavior, they can
alter it by altering the prototype information (see
section 7.1 for examples). Third, and related to
the first two, one does not confuse the ability of
the system to learn a consistent grammar with its
ability to learn the grammar a user has in mind.
In this paper, we present a series of experiments
in the induction of labeled context-free trees us-
ing a combination of unlabeled data and sparse
prototypes. We first affirm the well-known re-
sult that simple, unconstrained PCFG induction
produces grammars of poor quality as measured
against treebank structures. We then augment a
PCFGwith prototype features, and show that these
features, when propagated to non-prototype se-
quences using distributional similarity, are effec-
tive at learning bracket labels on fixed unlabeled
trees, but are still not enough to learn good tree
structures without bracketing information. Finally,
we intersect the feature-augmented PCFGwith the
CCM model of Klein and Manning (2002), a high-
quality bracketing model. The intersected model
is able to learn trees with higher unlabeled F1 than
those in Klein and Manning (2004). More impor-
881
tantly, its trees are labeled and can be evaluated
according to labeled metrics. Against the English
Penn Treebank, our final trees achieve a labeled F1
of 65.1 on short sentences, a 51.7% error reduction
over naive PCFG induction.
2 Experimental Setup
The majority of our experiments induced tree
structures from the WSJ section of the English
Penn treebank (Marcus et al, 1994), though see
section 7.4 for an experiment on Chinese. To fa-
cilitate comparison with previous work, we ex-
tracted WSJ-10, the 7,422 sentences which con-
tain 10 or fewer words after the removal of punc-
tuation and null elements according to the scheme
detailed in Klein (2005). We learned models on all
or part of this data and compared their predictions
to the manually annotated treebank trees for the
sentences on which the model was trained. As in
previous work, we begin with the part-of-speech
(POS) tag sequences for each sentence rather than
lexical sequences (Carroll and Charniak, 1992;
Klein and Manning, 2002).
Following Klein and Manning (2004), we report
unlabeled bracket precision, recall, and F1. Note
that according to their metric, brackets of size 1
are omitted from the evaluation. Unlike that work,
all of our induction methods produce trees labeled
with symbols which are identified with treebank
categories. Therefore, we also report labeled pre-
cision, recall, and F1, still ignoring brackets of
size 1.1
3 Experiments in PCFG induction
As an initial experiment, we used the inside-
outside algorithm to induce a PCFG in the
straightforward way (Lari and Young, 1990; Man-
ning and Schu?tze, 1999). For all the experiments
in this paper, we considered binary PCFGs over
the nonterminals and terminals occuring in WSJ-
10. The PCFG rules were of the following forms:
? X ? Y Z, for nonterminal types X,Y, and
Z, with Y 6= X or Z 6= X
? X ? t Y , X ? Y t, for each terminal t
? X ? t t?, for terminals t and t?
For a given sentence S, our CFG generates la-
beled trees T over S.2 Each tree consists of binary
1In cases where multiple gold labels exist in the gold trees,
precision and recall were calculated as in Collins (1999).
2Restricting our CFG to a binary branching grammar re-
sults in an upper bound of 88.1% on unlabeled F1.
productions X(i, j) ? ? over constituent spans
(i, j), where ? is a pair of non-terminal and/or
terminal symbols in the grammar. The generative
probability of a tree T for S is:
PCFG(T, S) =
?
X(i,j)???T
P (?|X)
In the inside-outside algorithm, we iteratively
compute posterior expectations over production
occurences at each training span, then use those
expectations to re-estimate production probabili-
ties. This process is guaranteed to converge to a
local extremum of the data likelihood, but initial
production probability estimates greatly influence
the final grammar (Carroll and Charniak, 1992). In
particular, uniform initial estimates are an (unsta-
ble) fixed point. The classic approach is to add a
small amount of random noise to the initial prob-
abilities in order to break the symmetry between
grammar symbols.
We randomly initialized 5 grammars using tree-
bank non-terminals and trained each to conver-
gence on the first 2000 sentences of WSJ-10.
Viterbi parses were extracted for each of these
2000 sentences according to each grammar. Of
course, the parses? symbols have nothing to anchor
them to our intended treebank symbols. That is, an
NP in one of these grammars may correspond to
the target symbol VP, or may not correspond well
to any target symbol. To evaluate these learned
grammars, we must map the models? phrase types
to target phrase types. For each grammar, we fol-
lowed the common approach of greedily mapping
model symbols to target symbols in the way which
maximizes the labeled F1. Note that this can, and
does, result in mapping multiple model symbols
to the most frequent target symbols. This experi-
ment, labeled PCFG? NONE in figure 4, resulted in
an average labeled F1 of 26.3 and an unlabeled F1
of 45.7. The unlabeled F1 is better than randomly
choosing a tree (34.7), but not better than always
choosing a right branching structure (61.7).
Klein and Manning (2002) suggest that the task
of labeling constituents is significantly easier than
identifying them. Perhaps it is too much to ask
a PCFG induction algorithm to perform both of
these tasks simultaneously. Along the lines of
Pereira and Schabes (1992), we reran the inside-
outside algorithm, but this time placed zero mass
on all trees which did not respect the bracketing
of the gold trees. This constraint does not fully
882
Phrase Prototypes Phrase Prototypes
NP DT NN VP VBN IN NN
JJ NNS VBD DT NN
NNP NNP MD VB CD
S PRP VBD DT NN QP CD CD
DT NN VBD IN DT NN RB CD
DT VBZ DT JJ NN DT CD CD
PP IN NN ADJP RB JJ
TO CD CD JJ
IN PRP JJ CC JJ
ADVP RB RB
RB CD
RB CC RB
VP-INF VB NN NP-INF NN POS
Figure 1: English phrase type prototype list man-
ually specified (The entire supervision for our sys-
tem). The second part of the table is additional
prototypes discussed in section 7.1.
eliminate the structural uncertainty since we are
inducing binary trees and the gold trees are flat-
ter than binary in many cases. This approach of
course achieved the upper bound on unlabeled F1,
because of the gold bracket constraints. However,
it only resulted in an average labeled F1 of 52.6
(experiment PCFG ? GOLD in figure 4). While this
labeled score is an improvement over the PCFG ?
NONE experiment, it is still relatively disappoint-
ing.
3.1 Encoding Prior Knowledge with
Prototypes
Clearly, we need to do something more than
adding structural bias (e.g. bracketing informa-
tion) if we are to learn a PCFG in which the sym-
bols have the meaning and behaviour we intend.
How might we encode information about our prior
knowledge or intentions?
Providing labeled trees is clearly an option. This
approach tells the learner how symbols should re-
cursively relate to each other. Another option is to
provide fully linearized yields as prototypes. We
take this approach here, manually creating a list
of POS sequences typical of the 7 most frequent
categories in the Penn Treebank (see figure 1).3
Our grammar is limited to these 7 phrase types
plus an additional type which has no prototypes
and is unconstrained.4 This list grounds each sym-
3A possible objection to this approach is the introduction
of improper reasearcher bias via specifying prototypes. See
section 7.3 for an experiment utilizing an automatically gen-
erated prototype list with comparable results.
4In our experiments we found that adding prototypes for
more categories did not improve performance and took more
bol in terms of an observable portion of the data,
rather than attempting to relate unknown symbols
to other unknown symbols.
Broadly, we would like to learn a grammar
which explains the observed data (EM?s objec-
tive) but also meets our prior expectations or re-
quirements of the target grammar. How might
we use such a list to constrain the learning of
a PCFG with the inside-outside algorithm? We
might require that all occurences of a prototype
sequence, say DT NN, be constituents of the cor-
responding type (NP). However, human-elicited
prototypes are not likely to have the property that,
when they occur, they are (nearly) always con-
stituents. For example, DT NN is a perfectly rea-
sonable example of a noun phrase, but is not a con-
stituent when it is part of a longer DT NN NN con-
stituent. Therefore, when summing over trees with
the inside-outside algorithm, we could require a
weaker property: whenever a prototype sequence
is a constituent it must be given the label specified
in the prototype file.5 This constraint is enough to
break the symmetry between the model labels, and
therefore requires neither random initialization for
training, nor post-hoc mapping of labels for eval-
uation. Adding prototypes in this way and keep-
ing the gold bracket constraint gave 59.9 labeled
F1. The labeled F1 measure is again an improve-
ment over naive PCFG induction, but is perhaps
less than we might expect given that the model has
been given bracketing information and has proto-
types as a form of supervision to direct it.
In response to a prototype, however, we may
wish to conclude something stronger than a con-
straint on that particular POS sequence. We might
hope that sequences which are similar to a proto-
type in some sense are generally given the same
label as that prototype. For example, DT NN is a
noun phrase prototype, the sequence DT JJ NN is
another good candidate for being a noun phrase.
This kind of propagation of constraints requires
that we have a good way of defining and detect-
ing similarity between POS sequences.
3.2 Phrasal Distributional Similarity
A central linguistic argument for constituent types
is substitutability: phrases of the same type appear
time. We note that we still evaluate against all phrase types
regardless of whether or not they are modeled by our gram-
mar.
5Even this property is likely too strong: prototypes may
have multiple possible labels, for example DT NN may also
be a QP in the English treebank.
883
Yield Prototype Skew KL Phrase Type Skew KL
DT JJ NN DT NN 0.10 NP 0.39
IN DT VBG NN IN NN 0.24 PP 0.45
DT NN MD VB DT NNS PRP VBD DT NN 0.54 S 0.58
CC NN IN NN 0.43 PP 0.71
MD NNS PRP VBD DT NN 1.43 NONE -
Figure 2: Yields along with most similar proto-
types and phrase types, guessed according to (3).
in similar contexts and are mutually substitutable
(Harris, 1954; Radford, 1988). For instance, DT
JJ NN and DT NN occur in similar contexts, and
are indeed both common NPs. This idea has been
repeatedly and successfully operationalized using
various kinds of distributional clustering, where
we define a similarity measure between two items
on the basis of their immediate left and right con-
texts (Schu?tze, 1995; Clark, 2000; Klein and Man-
ning, 2002).
As in Clark (2001), we characterize the distribu-
tion of a sequence by the distribution of POS tags
occurring to the left and right of that sequence in
a corpus. Each occurence of a POS sequence ?
falls in a context x ? y, where x and y are the ad-
jacent tags. The distribution over contexts x ? y
for a given ? is called its signature, and is denoted
by ?(?). Note that ?(?) is composed of context
counts from all occurences, constitiuent and dis-
tituent, of ?. Let ?c(?) denote the context dis-
tribution for ? where the context counts are taken
only from constituent occurences of ?. For each
phrase type in our grammar,X , define ?c(X) to be
the context distribution obtained from the counts
of all constituent occurences of type X:
?c(X) = Ep(?|X) ?c(?) (1)
where p(?|X) is the distribution of yield types for
phrase type X . We compare context distributions
using the skewed KL divergence:
DSKL(p, q) = DKL(p??p + (1? ?)q)
where ? controls how much of the source distribu-
tions is mixed in with the target distribution.
A reasonable baseline rule for classifying the
phrase type of a POS yield is to assign it to the
phrase from which it has minimal divergence:
type(?) = argmin
X
DSKL(?c(?), ?c(X)) (2)
However, this rule is not always accurate, and,
moreover, we do not have access to ?c(?) or
?c(X). We chose to approximate ?c(X) us-
ing the prototype yields for X as samples from
p(?|X). Letting proto(X) denote the (few) pro-
totype yields for phrase type X , we define ??(X):
??(X) =
1
|proto(X)|
?
??proto(X)
?(?)
Note ??(X) is an approximation to (1) in sev-
eral ways. We have replaced an expectation over
p(?|X) with a uniform weighting of proto(X),
and we have replaced ?c(?) with ?(?) for each
term in that expectation. Because of this, we will
rely only on high confidence guesses, and allow
yields to be given a NONE type if their divergence
from each ??(X) exceeds a fixed threshold t. This
gives the following alternative to (2):
type(?) = (3)
{
NONE, if minX DSKL(?(?), ??(X)) < t
argminX DSKL(?(?), ??(X)), otherwise
We built a distributional model implementing
the rule in (3) by constructing ?(?) from context
counts in the WSJ portion of the Penn Treebank
as well as the BLIPP corpus. Each ??(X) was ap-
proximated by a uniform mixture of ?(?) for each
of X?s prototypes ? listed in figure 1.
This method of classifying constituents is very
precise if the threshold is chosen conservatively
enough. For instance, using a threshold of t =
0.75 and ? = 0.1, this rule correctly classifies the
majority label of a constituent-type with 83% pre-
cision, and has a recall of 23% over constituent
types. Figure 2 illustrates some sample yields, the
prototype sequence to which it is least divergent,
and the output of rule (3).
We incorporated this distributional information
into our PCFG induction scheme by adding a pro-
totype feature over each span (i, j) indicating the
output of (3) for the yield ? in that span. Asso-
ciated with each sentence S is a feature map F
specifying, for each (i, j), a prototype feature pij .
These features are generated using an augmented
CFG model, CFG+, given by:6
PCFG+(T, F ) =
?
X(i,j)???T
P (pij |X)P (?|X)
=
?
X(i,j)???T
?CFG+(X ? ?, pij)
6Technically, all features in F must be generated for each
assignment to T , which means that there should be terms in
this equation for the prototype features on distituent spans.
However, we fixed the prototype distribution to be uniform
for distituent spans so that the equation is correct up to a con-
stant depending on F .
884
P (S|ROOT) ? ROOT
S
? P (NP VP|S)
P (P = NONE|S)XXXXX
P (NN NNS|NP)
P (P = NP|NP)
ff
NP
 HHH
NNN
payrolls
NN
Factory
VP
? P (VBD PP|VP)
P (P = VP|VP)aaa
!!!
VBD
fell
PP
? P (IN NN|PP)
P (P = PP|PP)!!! aaa
NN
November
IN
in
Figure 3: Illustration of PCFG augmented with
prototype similarity features.
where ?CFG+(X ? ?, pij) is the local factor for
placing X ? ? on a span with prototype feature
pij . An example is given in figure 3.
For our experiments, we fixed P (pij |X) to be:
P (pij |X) =
{
0.60, if pij = X
uniform, otherwise
Modifying the model in this way, and keeping the
gold bracketing information, gave 71.1 labeled F1
(see experiment PROTO ? GOLD in figure 4), a
40.3% error reduction over naive PCFG induction
in the presence of gold bracketing information.
We note that the our labeled F1 is upper-bounded
by 86.0 due to unary chains and more-than-binary
configurations in the treebank that cannot be ob-
tained from our binary grammar.
We conclude that in the presence of gold bracket
information, we can achieve high labeled accu-
racy by using a CFG augmented with distribu-
tional prototype features.
4 Constituent Context Model
So far, we have shown that, given perfect per-
fect bracketing information, distributional proto-
type features allow us to learn tree structures with
fairly accurate labels. However, such bracketing
information is not available in the unsupervised
case.
Perhaps we don?t actually need bracketing con-
straints in the presence of prototypes and distri-
butional similarity features. However this exper-
iment, labeled PROTO ? NONE in figure 4, gave
only 53.1 labeled F1 (61.1 unlabeled), suggesting
that some amount of bracketing constraint is nec-
essary to achieve high performance.
Fortunately, there are unsupervised systems
which can induce unlabeled bracketings with rea-
sonably high accuracy. One such model is
the constituent-context model (CCM) of Klein
and Manning (2002), a generative distributional
model. For a given sentence S, the CCM generates
a bracket matrix, B, which for each span (i, j), in-
dicates whether or not it is a constituent (Bij = c)
or a distituent (Bij = d). In addition, it generates
a feature map F ?, which for each span (i, j) in S
specifies a pair of features, F ?ij = (yij , cij), where
yij is the POS yield of the span, and cij is the con-
text of the span, i.e identity of the conjoined left
and right POS tags:
PCCM (B,F
?) = P (B)
?
(i,j)
P (yij |Bij)P (cij |Bij)
The distribution P (B) only places mass on brack-
etings which correspond to binary trees. We
can efficiently compute PCCM (B,F ?) (up to
a constant) depending on F ? using local fac-
tors ?CCM (yij , cij) which decomposes over con-
stituent spans:7
PCCM (B,F
?) ?
?
(i,j):Bij=c
P (yij |c)P (cij |c)
P (yij |d)P (cij |d)
=
?
(i,j):Bij=c
?CCM (yij , cij)
The CCM by itself yields an unlabeled F1 of 71.9
on WSJ-10, which is reasonably high, but does not
produce labeled trees.
5 Intersecting CCM and PCFG
The CCM and PCFG models provide complemen-
tary views of syntactic structure. The CCM explic-
itly learns the non-recursive contextual and yield
properties of constituents and distituents. The
PCFG model, on the other hand, does not explic-
itly model properties of distituents but instead fo-
cuses on modeling the hierarchical and recursive
properties of natural language syntax. One would
hope that modeling both of these aspects simulta-
neously would improve the overall quality of our
induced grammar.
We therefore combine the CCM with our feature-
augmented PCFG, denoted by PROTO in exper-
iment names. When we run EM on either of
the models alone, at each iteration and for each
training example, we calculate posteriors over that
7Klein (2005) gives a full presentation.
885
model?s latent variables. For CCM, the latent vari-
able is a bracketing matrix B (equivalent to an un-
labeled binary tree), while for the CFG+ the latent
variable is a labeled tree T . While these latent
variables aren?t exactly the same, there is a close
relationship between them. A bracketing matrix
constrains possible labeled trees, and a given la-
beled tree determines a bracketing matrix. One
way to combine these models is to encourage both
models to prefer latent variables which are com-
patible with each other.
Similar to the approach of Klein and Manning
(2004) on a different model pair, we intersect CCM
and CFG+ by multiplying their scores for any la-
beled tree. For each possible labeled tree over a
sentence S, our generative model for a labeled tree
T is given as follows:
P (T, F, F ?) = (4)
PCFG+(T, F )PCCM (B(T ), F
?)
where B(T ) corresponds to the bracketing ma-
trix determined by T . The EM algorithm for the
product model will maximize:
P (S,F, F ?) =
?
T?T (S)
PCCM (B,F
?)PCFG+(T, F )
=
?
B
PCCM (B,F
?)
?
T?T (B,S)
PCFG+(T, F )
where T (S) is the set of labeled trees consistent
with the sentence S and T (B,S) is the set of la-
beled trees consistent with the bracketing matrix
B and the sentence S. Notice that this quantity in-
creases as the CCM and CFG+ models place proba-
bility mass on compatible latent structures, giving
an intuitive justification for the success of this ap-
proach.
We can compute posterior expectations over
(B, T ) in the combined model (4) using a variant
of the inside-outside algorithm. The local factor
for a binary rule r = X ? Y Z, over span (i, j),
with CCM features F ?ij = (yij , cij) and prototype
feature pij , is given by the product of local factors
for the CCM and CFG+ models:
?(r, (i, j)) = ?CCM (yij , cij)?CFG+(r, pij)
From these local factors, the inside-outside al-
gorithm produces expected counts for each binary
rule, r, over each span (i, j) and split point k, de-
noted by P (r, (i, j), k|S, F, F ?). These posteriors
are sufficient to re-estimate all of our model pa-
rameters.
Labeled Unlabeled
Setting Prec. Rec. F1 Prec. Rec. F1
No Brackets
PCFG ? NONE 23.9 29.1 26.3 40.7 52.1 45.7
PROTO ? NONE 51.8 62.9 56.8 59.6 76.2 66.9
Gold Brackets
PCFG ? GOLD 47.0 57.2 51.6 78.8 100.0 88.1
PROTO ? GOLD 64.8 78.7 71.1 78.8 100.0 88.1
CCM Brackets
CCM - - - 64.2 81.6 71.9
PCFG ? CCM 32.3 38.9 35.3 64.1 81.4 71.8
PROTO ? CCM 56.9 68.5 62.2 68.4 86.9 76.5
BEST 59.4 72.1 65.1 69.7 89.1 78.2
UBOUND 78.8 94.7 86.0 78.8 100.0 88.1
Figure 4: English grammar induction results. The
upper bound on labeled recall is due to unary
chains.
6 CCM as a Bracketer
We tested the product model described in sec-
tion 5 on WSJ-10 under the same conditions as
in section 3. Our initial experiment utilizes no
protoype information, random initialization, and
greedy remapping of its labels. This experiment,
PCFG ? CCM in figure 4, gave 35.3 labeled F1,
compared to the 51.6 labeled F1 with gold brack-
eting information (PCFG ? GOLD in figure 4).
Next we added the manually specified proto-
types in figure 1, and constrained the model to give
these yields their labels if chosen as constituents.
This experiment gave 48.9 labeled F1 (73.3 unla-
beled). The error reduction is 21.0% labeled (5.3%
unlabeled) over PCFG ? CCM.
We then experimented with adding distributional
prototype features as discussed in section 3.2 us-
ing a threshold of 0.75 and ? = 0.1. This experi-
ment, PROTO ? CCM in figure 4, gave 62.2 labeled
F1 (76.5 unlabeled). The error reduction is 26.0%
labeled (12.0% unlabeled) over the experiment us-
ing prototypes without the similarity features. The
overall error reduction from PCFG? CCM is 41.6%
(16.7%) in labeled (unlabeled) F1.
7 Error Analysis
The most common type of error by our PROTO ?
CCM system was due to the binary grammar re-
striction. For instance common NPs, such as DT JJ
NN, analyzed as [NP DT [NP JJ NN] ], which pro-
poses additional N constituents compared to the
flatter treebank analysis. This discrepancy greatly,
and perhaps unfairly, damages NP precision (see
figure 6). However, this is error is unavoidable
886
SXXXXXNP
NNP
France
VPXXXXXMD
can
VPhhhhhhh(((((((VB
boast
NPXXXXXNPaaa!!!NPaaa!!!DT
the
NN
lion
POS
?s
NN
share
PPPPPPIN
of
NPHHHJJ
high-priced
NNS
bottles
Shhhhhhhhh(((((((((NNP
France
VPhhhhhhhh((((((((VPXXXXXVPZZProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Tailoring Word Alignments to Syntactic Machine Translation
John DeNero
Computer Science Division
University of California, Berkeley
denero@berkeley.edu
Dan Klein
Computer Science Division
University of California, Berkeley
klein@cs.berkeley.edu
Abstract
Extracting tree transducer rules for syntac-
tic MT systems can be hindered by word
alignment errors that violate syntactic corre-
spondences. We propose a novel model for
unsupervised word alignment which explic-
itly takes into account target language con-
stituent structure, while retaining the robust-
ness and efficiency of the HMM alignment
model. Our model?s predictions improve the
yield of a tree transducer extraction system,
without sacrificing alignment quality. We
also discuss the impact of various posterior-
based methods of reconciling bidirectional
alignments.
1 Introduction
Syntactic methods are an increasingly promising ap-
proach to statistical machine translation, being both
algorithmically appealing (Melamed, 2004; Wu,
1997) and empirically successful (Chiang, 2005;
Galley et al, 2006). However, despite recent
progress, almost all syntactic MT systems, indeed
statistical MT systems in general, build upon crude
legacy models of word alignment. This dependence
runs deep; for example, Galley et al (2006) requires
word alignments to project trees from the target lan-
guage to the source, while Chiang (2005) requires
alignments to induce grammar rules.
Word alignment models have not stood still in re-
cent years. Unsupervised methods have seen sub-
stantial reductions in alignment error (Liang et al,
2006) as measured by the now much-maligned AER
metric. A host of discriminative methods have been
introduced (Taskar et al, 2005; Moore, 2005; Ayan
and Dorr, 2006). However, few of these methods
have explicitly addressed the tension between word
alignments and the syntactic processes that employ
them (Cherry and Lin, 2006; Daume? III and Marcu,
2005; Lopez and Resnik, 2005).
We are particularly motivated by systems like the
one described in Galley et al (2006), which con-
structs translations using tree-to-string transducer
rules. These rules are extracted from a bitext anno-
tated with both English (target side) parses and word
alignments. Rules are extracted from target side
constituents that can be projected onto contiguous
spans of the source sentence via the word alignment.
Constituents that project onto non-contiguous spans
of the source sentence do not yield transducer rules
themselves, and can only be incorporated into larger
transducer rules. Thus, if the word alignment of a
sentence pair does not respect the constituent struc-
ture of the target sentence, then the minimal transla-
tion units must span large tree fragments, which do
not generalize well.
We present and evaluate an unsupervised word
alignment model similar in character and compu-
tation to the HMM model (Ney and Vogel, 1996),
but which incorporates a novel, syntax-aware distor-
tion component which conditions on target language
parse trees. These trees, while automatically gener-
ated and therefore imperfect, are nonetheless (1) a
useful source of structural bias and (2) the same trees
which constrain future stages of processing anyway.
In our model, the trees do not rule out any align-
ments, but rather softly influence the probability of
transitioning between alignment positions. In par-
ticular, transition probabilities condition upon paths
through the target parse tree, allowing the model to
prefer distortions which respect the tree structure.
17
Our model generates word alignments that better
respect the parse trees upon which they are condi-
tioned, without sacrificing alignment quality. Using
the joint training technique of Liang et al (2006)
to initialize the model parameters, we achieve an
AER superior to the GIZA++ implementation of
IBM model 4 (Och and Ney, 2003) and a reduc-
tion of 56.3% in aligned interior nodes, a measure
of agreement between alignments and parses. As a
result, our alignments yield more rules, which better
match those we would extract had we used manual
alignments.
2 Translation with Tree Transducers
In a tree transducer system, as in phrase-based sys-
tems, the coverage and generality of the transducer
inventory is strongly related to the effectiveness of
the translation model (Galley et al, 2006). We will
demonstrate that this coverage, in turn, is related to
the degree to which initial word alignments respect
syntactic correspondences.
2.1 Rule Extraction
Galley et al (2004) proposes a method for extracting
tree transducer rules from a parallel corpus. Given a
source language sentence s, a target language parse
tree t of its translation, and a word-level alignment,
their algorithm identifies the constituents in t which
map onto contiguous substrings of s via the align-
ment. The root nodes of such constituents ? denoted
frontier nodes ? serve as the roots and leaves of tree
fragments that form minimal transducer rules.
Frontier nodes are distinguished by their compat-
ibility with the word alignment. For a constituent c
of t, we consider the set of source words sc that are
aligned to c. If none of the source words in the lin-
ear closure s?c (the words between the leftmost and
rightmost members of sc) aligns to a target word out-
side of c, then the root of c is a frontier node. The
remaining interior nodes do not generate rules, but
can play a secondary role in a translation system.1
The roots of null-aligned constituents are not fron-
tier nodes, but can attach productively to multiple
minimal rules.
1Interior nodes can be used, for instance, in evaluating
syntax-based language models. They also serve to differentiate
transducer rules that have the same frontier nodes but different
internal structure.
Two transducer rules, t1 ? s1 and t2 ? s2,
can be combined to form larger translation units
by composing t1 and t2 at a shared frontier node
and appropriately concatenating s1 and s2. How-
ever, no technique has yet been shown to robustly
extract smaller component rules from a large trans-
ducer rule. Thus, for the purpose of maximizing the
coverage of the extracted translation model, we pre-
fer to extract many small, minimal rules and gen-
erate larger rules via composition. Maximizing the
number of frontier nodes supports this goal, while
inducing many aligned interior nodes hinders it.
2.2 Word Alignment Interactions
We now turn to the interaction between word align-
ments and the transducer extraction algorithm. Con-
sider the example sentence in figure 1A, which
demonstrates how a particular type of alignment er-
ror prevents the extraction of many useful transducer
rules. The mistaken link [la ? the] intervenes be-
tween axe?s and carrie`r, which both align within an
English adjective phrase, while la aligns to a distant
subspan of the English parse tree. In this way, the
alignment violates the constituent structure of the
English parse.
While alignment errors are undesirable in gen-
eral, this error is particularly problematic for a
syntax-based translation system. In a phrase-based
system, this link would block extraction of the
phrases [axe?s sur la carrie`r ? career oriented] and
[les emplois ? the jobs] because the error overlaps
with both. However, the intervening phrase [em-
plois sont ? jobs are] would still be extracted, at
least capturing the transfer of subject-verb agree-
ment. By contrast, the tree transducer extraction
method fails to extract any of these fragments: the
alignment error causes all non-terminal nodes in
the parse tree to be interior nodes, excluding pre-
terminals and the root. Figure 1B exposes the conse-
quences: a wide array of desired rules are lost during
extraction.
The degree to which a word alignment respects
the constituent structure of a parse tree can be quan-
tified by the frequency of interior nodes, which indi-
cate alignment patterns that cross constituent bound-
aries. To achieve maximum coverage of the trans-
lation model, we hope to infer tree-violating align-
ments only when syntactic structures truly diverge.
18
.(A)
(B) (i)
(ii)
S
NP
VP
ADJP
NN VBN
NNS
DT
AUX
T
h
e
j
o
b
s
a
r
e
c
a
r
e
e
r
o
r
i
e
n
t
e
d
.
les
emplois
sont
ax?s
sur
la
carri?re
.
.
Legend
Correct proposed word alignment consistent with 
human annotation.
Proposed word alignment error inconsistent with 
human annotation.
Word alignment constellation that renders the 
root of the relevant constituent to be an interior 
node.
Word alignment constellation that would allow a 
phrase extraction in a phrase-based translation 
system, but which does not correspond to an 
English constituent.
Bold
Italic
Frontier node (agrees with alignment)
Interior node (inconsistent with alignment)
(S (NP (DT[0] NNS[1]) (VP AUX[2] (ADJV NN[3] VBN[4]) .[5]) ? [0] [1] [2] [3] [4] [5]
(S (NP (DT[0] (NNS jobs)) (VP AUX[1] (ADJV NN[2] VBN[3]) .[4]) ? [0] sont [1] [2] [3] [4]
(S (NP (DT[0] (NNS jobs)) (VP (AUX are) (ADJV NN[1] VBN[2]) .[3]) ? [0] emplois sont [1] [2] [3]
(S NP[0] VP[1] .[2]) ? [0] [1] [2]
(S (NP (DT[0] NNS[1]) VP[2] .[3]) ? [0] [1] [2] [3]
(S (NP (DT[0] (NNS jobs)) VP[2] .[3]) ? [0] emplois [2] [3]
(S (NP (DT[0] (NNS jobs)) (VP AUX[1] ADJV[2]) .[3]) ? [0] emplois [1] [2] [3]
(S (NP (DT[0] (NNS jobs)) (VP (AUX are) ADJV[1]) .[2]) ? [0] emplois sont [1] [2]
Figure 1: In this transducer extraction example, (A) shows a proposed alignment from our test set with
an alignment error that violates the constituent structure of the English sentence. The resulting frontier
nodes are printed in bold; all nodes would be frontier nodes under a correct alignment. (B) shows a small
sample of the rules extracted under the proposed alignment, (ii), and the correct alignment, (i) and (ii). The
single alignment error prevents the extraction of all rules in (i) and many more. This alignment pattern was
observed in our test set and corrected by our model.
3 Unsupervised Word Alignment
To allow for this preference, we present a novel con-
ditional alignment model of a foreign (source) sen-
tence f = {f1, ..., fJ} given an English (target) sen-
tence e = {e1, ..., eI} and a target tree structure t.
Like the classic IBM models (Brown et al, 1994),
our model will introduce a latent alignment vector
a = {a1, ..., aJ} that specifies the position of an
aligned target word for each source word. Formally,
our model describes p(a, f|e, t), but otherwise bor-
rows heavily from the HMM alignment model of
Ney and Vogel (1996).
The HMM model captures the intuition that the
alignment vector a will in general progress across
the sentence e in a pattern which is mostly local, per-
haps with a few large jumps. That is, alignments are
locally monotonic more often than not.
Formally, the HMM model factors as:
p(a, f|e) =
J?
j=1
pd(aj |aj? , j)p`(fj |eaj )
where j? is the position of the last non-null-aligned
source word before position j, p` is a lexical transfer
model, and pd is a local distortion model. As in all
such models, the lexical component p` is a collec-
tion of unsmoothed multinomial distributions over
19
foreign words.
The distortion model pd(aj |aj? , j) is a distribu-
tion over the signed distance aj ? aj? , typically
parameterized as a multinomial, Gaussian or expo-
nential distribution. The implementation that serves
as our baseline uses a multinomial distribution with
separate parameters for j = 1, j = J and shared
parameters for all 1 < j < J . Null alignments have
fixed probability at any position. Inference over a
requires only the standard forward-backward algo-
rithm.
3.1 Syntax-Sensitive Distortion
The broad and robust success of the HMM align-
ment model underscores the utility of its assump-
tions: that word-level translations can be usefully
modeled via first-degree Markov transitions and in-
dependent lexical productions. However, its distor-
tion model considers only string distance, disregard-
ing the constituent structure of the English sentence.
To allow syntax-sensitive distortion, we consider
a new distortion model of the form pd(aj |aj? , j, t).
We condition on t via a generative process that tran-
sitions between two English positions by traversing
the unique shortest path ?(aj? ,aj ,t) through t from
aj? to aj . We constrain ourselves to this shortest
path using a staged generative process.
Stage 1 (POP(n?), STOP(n?)): Starting in the leaf
node at aj? , we choose whether to STOP or
POP from child to parent, conditioning on the
type of the parent node n?. Upon choosing
STOP, we transition to stage 2.
Stage 2 (MOVE(n?, d)): Again, conditioning on the
type of the parent n? of the current node n, we
choose a sibling n? based on the signed distance
d = ?n?(n) ? ?n?(n?), where ?n?(n) is the index
of n in the child list of n?. Zero distance moves
are disallowed. After exactly one MOVE, we
transition to stage 3.
Stage 3 (PUSH(n, ?n(n?))): Given the current node
n, we select one of its children n?, conditioning
on the type of n and the position of the child
?n(n?). We continue to PUSH until reaching a
leaf.
This process is a first-degree Markov walk
through the tree, conditioning on the current node
Stage 1: { Pop(VBN), Pop(ADJP), Pop(VP), Stop(S) }
Stage 2: { Move(S, -1) }
Stage 3: { Push(NP, 1), Push(DT, 1) }
S
NP
VP
ADJP
NN VBN
NNS
DT
AUX
The jobs are career oriented .
.
Figure 2: An example sequence of staged tree tran-
sitions implied by the unique shortest path from the
word oriented (aj? = 5) to the word the (aj = 1).
and its immediate surroundings at each step. We en-
force the property that ?(aj? ,aj ,t) be unique by stag-
ing the process and disallowing zero distance moves
in stage 2. Figure 2 gives an example sequence of
tree transitions for a small parse tree.
The parameterization of this distortion model fol-
lows directly from its generative process. Given a
path ?(aj? ,aj ,t) with r = k +m+3 nodes including
the two leaves, the nearest common ancestor, k in-
tervening nodes on the ascent and m on the descent,
we express it as a triple of staged tree transitions that
include k POPs, a STOP, a MOVE, and m PUSHes:
?
?
{POP(n2), ..., POP(nk+1), STOP(nk+2)}
{MOVE (nk+2, ?(nk+3)? ?(nk+1))}
{PUSH (nk+3, ?(nk+4)) , ..., PUSH (nr?1, ?(nr))}
?
?
Next, we assign probabilities to each tree transi-
tion in each stage. In selecting these distributions,
we aim to maintain the original HMM?s sensitivity
to target word order:
? Selecting POP or STOP is a simple Bernoulli
distribution conditioned upon a node type.
? We model both MOVE and PUSH as multino-
mial distributions over the signed distance in
positions (assuming a starting position of 0 for
PUSH), echoing the parameterization popular
in implementations of the HMM model.
This model reduces to the classic HMM distor-
tion model given minimal English trees of only uni-
formly labeled pre-terminals and a root node. The
classic 0-distance distortion would correspond to the
20
00.2
0.40.6
-2 -1 0 1 2 3 4 5Li
kelihood HMMSyntactic
T
h
i
s
w
o
u
l
d
r
e
l
i
e
v
e
t
h
e
p
r
e
s
s
u
r
e
o
n
o
i
l
.
S
VB
DT .
MD VP
VP
NP PP
DT NN IN NN
Figure 3: For this example sentence, the learned dis-
tortion distribution of pd(aj |aj? , j, t) resembles its
counterpart pd(aj |aj? , j) of the HMM model but re-
flects the constituent structure of the English tree t.
For instance, the short path from relieve to on gives
a high transition likelihood.
STOP probability of the pre-terminal label; all other
distances would correspond to MOVE probabilities
conditioned on the root label, and the probability of
transitioning to the terminal state would correspond
to the POP probability of the root label.
As in a multinomial-distortion implementation of
the classic HMM model, we must sometimes artifi-
cially normalize these distributions in the deficient
case that certain jumps extend beyond the ends of
the local rules. For this reason, MOVE and PUSH
are actually parameterized by three values: a node
type, a signed distance, and a range of options that
dictates a normalization adjustment.
Once each tree transition generates a score, their
product gives the probability of the entire path, and
thereby the cost of the transition between string po-
sitions. Figure 3 shows an example learned distribu-
tion that reflects the structure of the given parse.
With these derivation steps in place, we must ad-
dress a handful of special cases to complete the gen-
erative model. We require that the Markov walk
from leaf to leaf of the English tree must start and
end at the root, using the following assumptions.
1. Given no previous alignment, we forego stages
1 and 2 and begin with a series of PUSHes from
the root of the tree to the desired leaf.
2. Given no subsequent alignments, we skip
stages 2 and 3 after a series of POPs including
a pop conditioned on the root node.
3. If the first choice in stage 1 is to STOP at the
current leaf, then stage 2 and 3 are unneces-
sary. Hence, a choice to STOP immediately is
a choice to emit another foreign word from the
current English word.
4. We flatten unary transitions from the tree when
computing distortion probabilities.
5. Null alignments are treated just as in the HMM
model, incurring a fixed cost from any position.
This model can be simplified by removing all con-
ditioning on node types. However, we found this
variant to slightly underperform the full model de-
scribed above. Intuitively, types carry information
about cross-linguistic ordering preferences.
3.2 Training Approach
Because our model largely mirrors the genera-
tive process and structure of the original HMM
model, we apply a nearly identical training proce-
dure to fit the parameters to the training data via the
Expectation-Maximization algorithm. Och and Ney
(2003) gives a detailed exposition of the technique.
In the E-step, we employ the forward-backward
algorithm and current parameters to find expected
counts for each potential pair of links in each train-
ing pair. In this familiar dynamic programming ap-
proach, we must compute the distortion probabilities
for each pair of English positions.
The minimal path between two leaves in a tree can
be computed efficiently by first finding the path from
the root to each leaf, then comparing those paths to
find the nearest common ancestor and a path through
it ? requiring time linear in the height of the tree.
Computing distortion costs independently for each
pair of words in the sentence imposed a computa-
tional overhead of roughly 50% over the original
HMM model. The bulk of this increase arises from
the fact that distortion probabilities in this model
must be computed for each unique tree, in contrast
21
to the original HMM which has the same distortion
probabilities for all sentences of a given length.
In the M-step, we re-estimate the parameters of
the model using the expected counts collected dur-
ing the E-step. All of the component distributions
of our lexical and distortion models are multinomi-
als. Thus, upon assuming these expectations as val-
ues for the hidden alignment vectors, we maximize
likelihood of the training data simply by comput-
ing relative frequencies for each component multi-
nomial. For the distortion model, an expected count
c(aj , aj?) is allocated to all tree transitions along the
path ?(aj? ,aj ,t). These allocations are summed and
normalized for each tree transition type to complete
re-estimation. The method of re-estimating the lexi-
cal model remains unchanged.
Initialization of the lexical model affects perfor-
mance dramatically. Using the simple but effective
joint training technique of Liang et al (2006), we
initialized the model with lexical parameters from a
jointly trained implementation of IBM Model 1.
3.3 Improved Posterior Inference
Liang et al (2006) shows that thresholding the pos-
terior probabilities of alignments improves AER rel-
ative to computing Viterbi alignments. That is, we
choose a threshold ? (typically ? = 0.5), and take
a = {(i, j) : p(aj = i|f, e) > ?}.
Posterior thresholding provides computationally
convenient ways to combine multiple alignments,
and bidirectional combination often corrects for
errors in individual directional alignment models.
Liang et al (2006) suggests a soft intersection of a
model m with a reverse model r (foreign to English)
that thresholds the product of their posteriors at each
position:
a = {(i, j) : pm(aj = i|f, e) ? pr(ai = j|f, e) > ?} .
These intersected alignments can be quite sparse,
boosting precision at the expense of recall. We
explore a generalized version to this approach by
varying the function c that combines pm and pr:
a = {(i, j) : c(pm, pr) > ?}. If c is the max func-
tion, we recover the (hard) union of the forward and
reverse posterior alignments. If c is the min func-
tion, we recover the (hard) intersection. A novel,
high performing alternative is the soft union, which
we evaluate in the next section:
c(pm, pr) =
pm(aj = i|f, e) + pr(ai = j|f, e)
2
.
Syntax-alignment compatibility can be further
promoted with a simple posterior decoding heuristic
we call competitive thresholding. Given a threshold
and a matrix c of combined weights for each pos-
sible link in an alignment, we include a link (i, j)
only if its weight cij is above-threshold and it is con-
nected to the maximum weighted link in both row i
and column j. That is, only the maximum in each
column and row and a contiguous enclosing span of
above-threshold links are included in the alignment.
3.4 Related Work
This proposed model is not the first variant of the
HMM model that incorporates syntax-based distor-
tion. Lopez and Resnik (2005) considers a sim-
pler tree distance distortion model. Daume? III and
Marcu (2005) employs a syntax-aware distortion
model for aligning summaries to documents, but
condition upon the roots of the constituents that are
jumped over during a transition, instead of those that
are visited during a walk through the tree. In the case
of syntactic machine translation, we want to condi-
tion on crossing constituent boundaries, even if no
constituents are skipped in the process.
4 Experimental Results
To understand the behavior of this model, we com-
puted the standard alignment error rate (AER) per-
formance metric.2 We also investigated extraction-
specific metrics: the frequency of interior nodes ? a
measure of how often the alignments violate the con-
stituent structure of English parses ? and a variant of
the CPER metric of Ayan and Dorr (2006).
We evaluated the performance of our model on
both French-English and Chinese-English manually
aligned data sets. For Chinese, we trained on the
FBIS corpus and the LDC bilingual dictionary, then
tested on 491 hand-aligned sentences from the 2002
2The hand-aligned test data has been annotated with both
sure alignments S and possible alignments P , with S ? P , ac-
cording to the specifications described in Och and Ney (2003).
With these alignments, we compute AER for a proposed align-
ment A as:
?
1? |A?S|+|A?P ||A|+|S|
?
? 100%.
22
French Precision Recall AER
Classic HMM 93.9 93.0 6.5
Syntactic HMM 95.2 91.5 6.4
GIZA++ 96.0 86.1 8.6
Chinese Precision Recall AER
Classic HMM 81.6 78.8 19.8
Syntactic HMM 82.2 76.8 20.5
GIZA++? 61.9 82.6 29.7
Table 1: Alignment error rates (AER) for 100k train-
ing sentences. The evaluated alignments are a soft
union for French and a hard union for Chinese, both
using competitive thresholding decoding. ?From
Ayan and Dorr (2006), grow-diag-final heuristic.
NIST MT evaluation set. For French, we used the
Hansards data from the NAACL 2003 Shared Task.3
We trained on 100k sentences for each language.
4.1 Alignment Error Rate
We compared our model to the original HMM
model, identical in implementation to our syntac-
tic HMM model save the distortion component.
Both models were initialized using the same jointly
trained Model 1 parameters (5 iterations), then
trained independently for 5 iterations. Both models
were then combined with an independently trained
HMM model in the opposite direction: f ? e.4 Ta-
ble 1 summarizes the results; the two models per-
form similarly. The main benefit of our model is the
effect on rule extraction, discussed below.
We also compared our French results to the pub-
lic baseline GIZA++ using the script published for
the NAACL 2006 Machine Translation Workshop
Shared Task.5 Similarly, we compared our Chi-
nese results to the GIZA++ results in Ayan and
Dorr (2006). Our models substantially outperform
GIZA++, confirming results in Liang et al (2006).
Table 2 shows the effect on AER of competitive
thresholding and different combination functions.
3Following previous work, we developed our system on the
37 provided validation sentences and the first 100 sentences of
the corpus test set. We used the remainder as a test set.
4Null emission probabilities were fixed to 1|e| , inversely pro-
portional to the length of the English sentence. The decoding
threshold was held fixed at ? = 0.5.
5Training includes 16 iterations of various IBM models and
a fixed null emission probability of .01. The output of running
GIZA++ in both directions was combined via intersection.
French w/o CT with CT
Hard Intersection (Min) 8.4 8.4
Hard Union (Max) 12.3 7.7
Soft Intersection (Product) 6.9 7.1
Soft Union (Average) 6.7 6.4
Chinese w/o CT with CT
Hard Intersection (Min) 27.4 27.4
Hard Union (Max) 25.0 20.5
Soft Intersection (Product) 25.0 25.2
Soft Union (Average) 21.1 21.6
Table 2: Alignment error rates (AER) by decoding
method for the syntactic HMM model. The compet-
itive thresholding heuristic (CT) is particularly help-
ful for the hard union combination method.
The most dramatic effect of competitive threshold-
ing is to improve alignment quality for hard unions.
It also impacts rule extraction substantially.
4.2 Rule Extraction Results
While its competitive AER certainly speaks to the
potential utility of our syntactic distortion model, we
proposed the model for a different purpose: to mini-
mize the particularly troubling alignment errors that
cross constituent boundaries and violate the struc-
ture of English parse trees. We found that while the
HMM and Syntactic models have very similar AER,
they make substantially different errors.
To investigate the differences, we measured the
degree to which each set of alignments violated the
supplied parse trees, by counting the frequency of
interior nodes that are not null aligned. Figure 4
summarizes the results of the experiment for French:
the Syntactic distortion with competitive threshold-
ing reduces tree violations substantially. Interior
node frequency is reduced by 56% overall, with
the most dramatic improvement observed for clausal
constituents. We observed a similar 50% reduction
for the Chinese data.
Additionally, we evaluated our model with the
transducer analog to the consistent phrase error rate
(CPER) metric of Ayan and Dorr (2006). This evalu-
ation computes precision, recall, and F1 of the rules
extracted under a proposed alignment, relative to the
rules extracted under the gold-standard sure align-
ments. Table 3 shows improvements in F1 by using
23
Reduction 
(percent)
NP
54.1
14.6
VP
46.3
10.3
PP
52.4
6.3
S
77.5
4.8
SBAR
58.0
1.9
Non-
Terminals
53.1
41.1
All
56.3
100.0
Corpus
Frequency
0.05.010.0
15.020.025.0
30.0
Interior 
Node Fr
equency
(percent
) HMM Model Syntactic Model + CT
Corpus frequency:
Reduction (percent): 38.9 47.2 45.3 54.8 59.7 43.7 45.1
14.6 10.3 6.3 4.8 1.9 41.1 100
Figure 4: The syntactic distortion model with com-
petitive thresholding decreases the frequency of in-
terior nodes for each type and the whole corpus.
the syntactic HMM model and competitive thresh-
olding together. Individually, each of these changes
contributes substantially to this increase. Together,
their benefits are partially, but not fully, additive.
5 Conclusion
In light of the need to reconcile word alignments
with phrase structure trees for syntactic MT, we have
proposed an HMM-like model whose distortion is
sensitive to such trees. Our model substantially re-
duces the number of interior nodes in the aligned
corpus and improves rule extraction while nearly
retaining the speed and alignment accuracy of the
HMM model. While it remains to be seen whether
these improvements impact final translation accu-
racy, it is reasonable to hope that, all else equal,
alignments which better respect syntactic correspon-
dences will be superior for syntactic MT.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond aer:
An extensive analysis of word alignments and their impact
on mt. In ACL.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In ACL.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL.
Hal Daume? III and Daniel Marcu. 2005. Induction of word and
phrase alignments for automatic document summarization.
Computational Linguistics, 31(4):505?530, December.
French Prec. Recall F1
Classic HMM Baseline 40.9 17.6 24.6
Syntactic HMM + CT 33.9 22.4 27.0
Relative change -17% 27% 10%
Chinese Prec. Recall F1
HMM Baseline (hard) 66.1 14.5 23.7
HMM Baseline (soft) 36.7 39.1 37.8
Syntactic + CT (hard) 48.0 41.6 44.6
Syntactic + CT (soft) 32.9 48.7 39.2
Relative change? 31% 6% 18%
Table 3: Relative to the classic HMM baseline, our
syntactic distortion model with competitive thresh-
olding improves the tradeoff between precision and
recall of extracted transducer rules. Both French
aligners were decoded using the best-performing
soft union combiner. For Chinese, we show aligners
under both soft and hard union combiners. ?Denotes
relative change from the second line to the third line.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In HLT-NAACL.
A. Lopez and P. Resnik. 2005. Improved hmm alignment mod-
els for languages with scarce resources. In ACL WPT-05.
I. Dan Melamed. 2004. Algorithms for syntax-aware statistical
machine translation. In Proceedings of the Conference on
Theoretical and Methodological Issues in Machine Transla-
tion.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In EMNLP.
Hermann Ney and Stephan Vogel. 1996. Hmm-based word
alignment in statistical translation. In COLING.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
discriminative matching approach to word alignment. In
EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
24
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848?855,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Aria Haghighi and Dan Klein
Computer Science Division
UC Berkeley
{aria42, klein}@cs.berkeley.edu
Abstract
We present an unsupervised, nonparamet-
ric Bayesian approach to coreference reso-
lution which models both global entity iden-
tity across a corpus as well as the sequen-
tial anaphoric structure within each docu-
ment. While most existing coreference work
is driven by pairwise decisions, our model
is fully generative, producing each mention
from a combination of global entity proper-
ties and local attentional state. Despite be-
ing unsupervised, our system achieves a 70.3
MUC F1 measure on the MUC-6 test set,
broadly in the range of some recent super-
vised results.
1 Introduction
Referring to an entity in natural language can
broadly be decomposed into two processes. First,
speakers directly introduce new entities into dis-
course, entities which may be shared across dis-
courses. This initial reference is typically accom-
plished with proper or nominal expressions. Second,
speakers refer back to entities already introduced.
This anaphoric reference is canonically, though of
course not always, accomplished with pronouns, and
is governed by linguistic and cognitive constraints.
In this paper, we present a nonparametric generative
model of a document corpus which naturally con-
nects these two processes.
Most recent coreference resolution work has fo-
cused on the task of deciding which mentions (noun
phrases) in a document are coreferent. The domi-
nant approach is to decompose the task into a col-
lection of pairwise coreference decisions. One then
applies discriminative learning methods to pairs of
mentions, using features which encode properties
such as distance, syntactic environment, and so on
(Soon et al, 2001; Ng and Cardie, 2002). Although
such approaches have been successful, they have
several liabilities. First, rich features require plen-
tiful labeled data, which we do not have for corefer-
ence tasks in most domains and languages. Second,
coreference is inherently a clustering or partitioning
task. Naive pairwise methods can and do fail to pro-
duce coherent partitions. One classic solution is to
make greedy left-to-right linkage decisions. Recent
work has addressed this issue in more global ways.
McCallum and Wellner (2004) use graph partion-
ing in order to reconcile pairwise scores into a final
coherent clustering. Nonetheless, all these systems
crucially rely on pairwise models because cluster-
level models are much harder to work with, combi-
natorially, in discriminative approaches.
Another thread of coreference work has focused
on the problem of identifying matches between
documents (Milch et al, 2005; Bhattacharya and
Getoor, 2006; Daume and Marcu, 2005). These
methods ignore the sequential anaphoric structure
inside documents, but construct models of how and
when entities are shared between them.1 These
models, as ours, are generative ones, since the fo-
cus is on cluster discovery and the data is generally
unlabeled.
In this paper, we present a novel, fully genera-
tive, nonparametric Bayesian model of mentions in a
document corpus. Our model captures both within-
and cross-document coreference. At the top, a hi-
erarchical Dirichlet process (Teh et al, 2006) cap-
1Milch et al (2005) works with citations rather than dis-
courses and does model the linear structure of the citations.
848
tures cross-document entity (and parameter) shar-
ing, while, at the bottom, a sequential model of
salience captures within-document sequential struc-
ture. As a joint model of several kinds of discourse
variables, it can be used to make predictions about
either kind of coreference, though we focus experi-
mentally on within-document measures. To the best
of our ability to compare, our model achieves the
best unsupervised coreference performance.
2 Experimental Setup
We adopt the terminology of the Automatic Context
Extraction (ACE) task (NIST, 2004). For this paper,
we assume that each document in a corpus consists
of a set of mentions, typically noun phrases. Each
mention is a reference to some entity in the domain
of discourse. The coreference resolution task is to
partition the mentions according to referent. Men-
tions can be divided into three categories, proper
mentions (names), nominal mentions (descriptions),
and pronominal mentions (pronouns).
In section 3, we present a sequence of increas-
ingly enriched models, motivating each from short-
comings of the previous. As we go, we will indicate
the performance of each model on data from ACE
2004 (NIST, 2004). In particular, we used as our
development corpus the English translations of the
Arabic and Chinese treebanks, comprising 95 docu-
ments and about 3,905 mentions. This data was used
heavily for model design and hyperparameter selec-
tion. In section 5, we present final results for new
test data from MUC-6 on which no tuning or devel-
opment was performed. This test data will form our
basis for comparison to previous work.
In all experiments, as is common, we will assume
that we have been given as part of our input the true
mention boundaries, the head word of each mention
and the mention type (proper, nominal, or pronom-
inal). For the ACE data sets, the head and mention
type are given as part of the mention annotation. For
the MUC data, the head was crudely chosen to be
the rightmost mention token, and the mention type
was automatically detected. We will not assume
any other information to be present in the data be-
yond the text itself. In particular, unlike much re-
lated work, we do not assume gold named entity
recognition (NER) labels; indeed we do not assume
observed NER labels or POS tags at all. Our pri-
?
?
K
?
K
Zi
H i
J
I
?
?
?
?
?
Zi
H i
I
J
(a) (b)
Figure 1: Graphical model depiction of document level en-
tity models described in sections 3.1 and 3.2 respectively. The
shaded nodes indicate observed variables.
mary performance metric will be the MUC F1 mea-
sure (Vilain et al, 1995), commonly used to evalu-
ate coreference systems on a within-document basis.
Since our system relies on sampling, all results are
averaged over five random runs.
3 Coreference Resolution Models
In this section, we present a sequence of gener-
ative coreference resolution models for document
corpora. All are essentially mixture models, where
the mixture components correspond to entities. As
far as notation, we assume a collection of I docu-
ments, each with Ji mentions. We use random vari-
ables Z to refer to (indices of) entities. We will use
?z to denote the parameters for an entity z, and ?
to refer to the concatenation of all such ?z . X will
refer somewhat loosely to the collection of variables
associated with a mention in our model (such as the
head or gender). We will be explicit about X and ?z
shortly.
Our goal will be to find the setting of the entity
indices which maximize the posterior probability:
Z? = argmax
Z
P (Z|X) = argmax
Z
P (Z,X)
= argmax
Z
?
P (Z,X,?) dP (?)
where Z,X, and ? denote all the entity indices, ob-
served values, and parameters of the model. Note
that we take a Bayesian approach in which all pa-
rameters are integrated out (or sampled). The infer-
ence task is thus primarily a search problem over the
index labels Z.
849
(a)
(b)
(c)
The Weir Group
1
, whose
2
  headquarters
3
 is in the US
4
, is a large, specialized corporation
5
 investing in the area of electricity 
generation. This  power plant
6
, which
7
  will be situated in Rudong
8
, Jiangsu
9
, has an annual generation capacity of 2.4 million kilowatts.  
The Weir Group
1
, whose
1
  headquarters
2
 is in the US
3
, is a large, specialized corporation
4
 investing in the area of electricity 
generation. This  power plant
5
, which
1
  will be situated in Rudong
6
, Jiangsu
7
, has an annual generation capacity of 2.4 million kilowatts.  
The Weir Group
1
, whose
1
  headquarters
2
 is in the US
3
, is a large, specialized corporation
4
 investing in the area of electricity 
generation. This  power plant
5
, which
5
  will be situated in Rudong
6
, Jiangsu
7
, has an annual generation capacity of 2.4 million kilowatts.  
Figure 2: Example output from various models. The output from (a) is from the infinite mixture model of section 3.2. It incorrectly
labels both boxed cases of anaphora. The output from (b) uses the pronoun head model of section 3.3. It correctly labels the first
case of anaphora but incorrectly labels the second pronominal as being coreferent with the dominant document entity The Weir
Group. This error is fixed by adding the salience feature component from section 3.4 as can be seen in (c).
3.1 A Finite Mixture Model
Our first, overly simplistic, corpus model is the stan-
dard finite mixture of multinomials shown in fig-
ure 1(a). In this model, each document is indepen-
dent save for some global hyperparameters. Inside
each document, there is a finite mixture model with
a fixed numberK of components. The distribution ?
over components (entities) is a draw from a symmet-
ric Dirichlet distribution with concentration ?. For
each mention in the document, we choose a compo-
nent (an entity index) z from ?. Entity z is then asso-
ciated with a multinomial emission distribution over
head words with parameters ?hZ , which are drawn
from a symmetric Dirichlet over possible mention
heads with concentration ?H .2 Note that here the X
for a mention consists only of the mention head H .
As we enrich our models, we simultaneously de-
velop an accompanying Gibbs sampling procedure
to obtain samples from P (Z|X).3 For now, all heads
H are observed and all parameters (? and ?) can be
integrated out analytically: for details see Teh et al
(2006). The only sampling is for the values of Zi,j ,
the entity index of mention j in document i. The
relevant conditional distribution is:4
P (Zi,j |Z?i,j ,H) ? P (Zi,j |Z?i,j)P (Hi,j |Z,H?i,j)
where Hi,j is the head of mention j in document i.
Expanding each term, we have the contribution of
the prior:
P (Zi,j = z|Z?i,j) ? nz + ?
2In general, we will use a subscripted ? to indicate concen-
tration for finite Dirichlet distributions. Unless otherwise spec-
ified, ? concentration parameters will be set to e?4 and omitted
from diagrams.
3One could use the EM algorithm with this model, but EM
will not extend effectively to the subsequent models.
4Here, Z?i,j denotes Z? {Zi,j}
where nz is the number of elements of Z?i,j with
entity index z. Similarly we have for the contribu-
tion of the emissions:
P (Hi,j = h|Z,H?i,j) ? nh,z + ?H
where nh,z is the number of times we have seen head
h associated with entity index z in (Z,H?i,j).
3.2 An Infinite Mixture Model
A clear drawback of the finite mixture model is the
requirement that we specify a priori a number of en-
tities K for a document. We would like our model
to select K in an effective, principled way. A mech-
anism for doing so is to replace the finite Dirichlet
prior on ? with the non-parametric Dirichlet process
(DP) prior (Ferguson, 1973).5 Doing so gives the
model in figure 1(b). Note that we now list an in-
finite number of mixture components in this model
since there can be an unbounded number of entities.
Rather than a finite ? with a symmetric Dirichlet
distribution, in which draws tend to have balanced
clusters, we now have an infinite ?. However, most
draws will have weights which decay exponentially
quickly in the prior (though not necessarily in the
posterior). Therefore, there is a natural penalty for
each cluster which is actually used.
With Z observed during sampling, we can inte-
grate out ? and calculate P (Zi,j |Z?i,j) analytically,
using the Chinese restaurant process representation:
P (Zi,j = z|Z?i,j) ?
{
?, if z = znew
nz, otherwise
(1)
where znew is a new entity index not used in Z?i,j
and nz is the number of mentions that have entity in-
dex z. Aside from this change, sampling is identical
5We do not give a detailed presentation of the Dirichlet pro-
cess here, but see Teh et al (2006) for a presentation.
850
PERS : 0.97,   LOC : 0.01,  ORG: 0.01,  MISC: 0.01 
Entity Type
SING: 0.99, PLURAL: 0.01
Number
     MALE: 0.98, FEM: 0.01, NEUTER: 0.01
Gender
Bush : 0.90,   President : 0.06,  .....
Head
?t
?h
?n
?g
X =
Z Z
M T N G
H
? ??
(a) (b)
Figure 3: (a) An entity and its parameters. (b)The head model
described in section 3.3. The shaded nodes indicate observed
variables. The mention type determines which set of parents are
used. The dependence of mention variable on entity parameters
? and pronoun head model ? is omitted.
to the finite mixture case, though with the number
of clusters actually occupied in each sample drifting
upwards or downwards.
This model yielded a 54.5 F1 on our develop-
ment data.6 This model is, however, hopelessly
crude, capturing nothing of the structure of coref-
erence. Its largest empirical problem is that, un-
surprisingly, pronoun mentions such as he are given
their own clusters, not labeled as coreferent with any
non-pronominal mention (see figure 2(a)).
3.3 Pronoun Head Model
While an entity-specific multinomial distribution
over heads makes sense for proper, and some nom-
inal, mention heads, it does not make sense to gen-
erate pronominal mentions this same way. I.e., all
entities can be referred to by generic pronouns, the
choice of which depends on entity properties such as
gender, not the specific entity.
We therefore enrich an entity?s parameters ? to
contain not only a distribution over lexical heads
?h, but also distributions (?t, ?g, ?n) over proper-
ties, where ?t parametrizes a distribution over en-
tity types (PER, LOC, ORG, MISC), and ?g for gen-
der (MALE, FEMALE, NEUTER), and ?n for number
(SG, PL).7 We assume each of these property distri-
butions is drawn from a symmetric Dirichlet distri-
bution with small concentration parameter in order
to encourage a peaked posterior distribution.
6See section 4 for inference details.
7It might seem that entities should simply have, for exam-
ple, a gender g rather than a distribution over genders ?g . There
are two reasons to adopt the softer approach. First, one can
rationalize it in principle, for entities like cars or ships whose
grammatical gender is not deterministic. However, the real rea-
son is that inference is simplified. In any event, we found these
property distributions to be highly determinized in the posterior.
?
?
?
?
?
Z1 Z 3
L 1
S 1
T
1
 
N
1
G
1
M
1
 =
NAM
Z 2
L 2
S 2
N
2
G
2
M
2
 =
NOM
T
2
H
2 =
"president"
H
1 =
"Bush"
H
3 =
"he"
N
2 =
SG
G
2 =
MALE
M
3
 =
PRO
T
2
L 3
S 3
?
I
Figure 4: Coreference model at the document level with entity
properties as well salience lists used for mention type distri-
butions. The diamond nodes indicate deterministic functions.
Shaded nodes indicate observed variables. Although it appears
that each mention head node has many parents, for a given men-
tion type, the mention head depends on only a small subset. De-
pendencies involving parameters ? and ? are omitted.
Previously, when an entity z generated a mention,
it drew a head word from ?hz . It now undergoes a
more complex and structured process. It first draws
an entity type T , a gender G, a number N from the
distributions ?t, ?g, and ?n, respectively. Once the
properties are fetched, a mention type M is chosen
(proper, nominal, pronoun), according to a global
multinomial (again with a symmetric Dirichlet prior
and parameter ?M ). This corresponds to the (tem-
porary) assumption that the speaker makes a random
i.i.d. choice for the type of each mention.
Our head model will then generate a head, con-
ditioning on the entity, its properties, and the men-
tion type, as shown in figure 3(b). If M is not a
pronoun, the head is drawn directly from the en-
tity head multinomial with parameters ?hz . Other-
wise, it is drawn based on a global pronoun head dis-
tribution, conditioning on the entity properties and
parametrized by ?. Formally, it is given by:
P (H|Z, T,G,N,M,?,?) =
{
P (H|T,G,N,?), if M =PRO
P (H|?hZ), otherwise
Although we can observe the number and gen-
der draws for some mentions, like personal pro-
nouns, there are some for which properties aren?t
observed (e.g., it). Because the entity prop-
erty draws are not (all) observed, we must now
sample the unobserved ones as well as the en-
tity indices Z. For instance, we could sample
851
Salience Feature Pronoun Proper Nominal
TOP 0.75 0.17 0.08
HIGH 0.55 0.28 0.17
MID 0.39 0.40 0.21
LOW 0.20 0.45 0.35
NONE 0.00 0.88 0.12
Table 1: Posterior distribution of mention type given salience
by bucketing entity activation rank. Pronouns are preferred for
entities which have high salience and non-pronominal mentions
are preferred for inactive entities.
Ti,j , the entity type of pronominal mention j in
document i, using, P (Ti,j |Z,N,G,H,T?i,j) ?
P (Ti,j |Z)P (Hi,j |T,N,G,H), where the posterior
distributions on the right hand side are straight-
forward because the parameter priors are all finite
Dirichlet. Sampling G and N are identical.
Of course we have prior knowledge about the re-
lationship between entity type and pronoun head
choice. For example, we expect that he is used for
mentions with T = PERSON. In general, we assume
that for each pronominal head we have a list of com-
patible entity types, which we encode via the prior
on ?. We assume ? is drawn from a Dirichlet distri-
bution where each pronoun head is given a synthetic
count of (1 + ?P ) for each (t, g, n) where t is com-
patible with the pronoun and given ?P otherwise.
So, while it will be possible in the posterior to use
he to refer to a non-person, it will be biased towards
being used with persons.
This model gives substantially improved predic-
tions: 64.1 F1 on our development data. As can be
seen in figure 2(b), this model does correct the sys-
tematic problem of pronouns being considered their
own entities. However, it still does not have a pref-
erence for associating pronominal references to en-
tities which are in any way local.
3.4 Adding Salience
We would like our model to capture how mention
types are generated for a given entity in a robust and
somewhat language independent way. The choice of
entities may reasonably be considered to be indepen-
dent given the mixing weights ?, but how we realize
an entity is strongly dependent on context (Ge et al,
1998).
In order to capture this in our model, we enrich
it as shown in figure 4. As we proceed through a
document, generating entities and their mentions,
we maintain a list of the active entities and their
saliences, or activity scores. Every time an entity is
mentioned, we increment its activity score by 1, and
every time we move to generate the next mention,
all activity scores decay by a constant factor of 0.5.
This gives rise to an ordered list of entity activations,
L, where the rank of an entity decays exponentially
as new mentions are generated. We call this list a
salience list. Given a salience list, L, each possible
entity z has some rank on this list. We discretize
these ranks into five buckets S: TOP (1), HIGH (2-
3), MID (4-6), LOW (7+), and NONE. Given the entity
choices Z, both the list L and buckets S are deter-
ministic (see figure 4). We assume that the mention
type M is conditioned on S as shown in figure 4.
We note that correctly sampling an entity now re-
quires that we incorporate terms for how a change
will affect all future salience values. This changes
our sampling equation for existing entities:
P (Zi,j = z|Z?i,j) ? nz
?
j??j
P (Mi,j? |Si,j? ,Z) (2)
where the product ranges over future mentions in the
document and Si,j? is the value of future salience
feature given the setting of all entities, including set-
ting the current entity Zi,j to z. A similar equation
holds for sampling a new entity. Note that, as dis-
cussed below, this full product can be truncated as
an approximation.
This model gives a 71.5 F1 on our development
data. Table 1 shows the posterior distribution of the
mention type given the salience feature. This model
fixes many anaphora errors and in particular fixes the
second anaphora error in figure 2(c).
3.5 Cross Document Coreference
One advantage of a fully generative approach is that
we can allow entities to be shared between docu-
ments in a principled way, giving us the capacity to
do cross-document coreference. Moreover, sharing
across documents pools information about the prop-
erties of an entity across documents.
We can easily link entities across a corpus by as-
suming that the pool of entities is global, with global
mixing weights ?0 drawn from a DP prior with
concentration parameter ?. Each document uses
852
??
?
?
?
Z1 Z 3
L 1
S 1
T
1 
N
1
G
1
M
1
 =
NAM
Z 2
L 2
S 2
N
2
G
2
M
2
 =
NOM
T
2
H
2 =
"president"
H
1 =
"Bush"
H
3 =
"he"
N
2 =
SG
G
2 =
MALE
M
3
 =
PRO
T
2
L 3
S 3
?0
?
?
?
I
Figure 5: Graphical depiction of the HDP coreference model
described in section 3.5. The dependencies between the global
entity parameters ? and pronoun head parameters ? on the men-
tion observations are not depicted.
the same global entities, but each has a document-
specific distribution ?i drawn from a DP centered on
?0 with concentration parameter ?. Up to the point
where entities are chosen, this formulation follows
the basic hierarchical Dirichlet process prior of Teh
et al (2006). Once the entities are chosen, our model
for the realization of the mentions is as before. This
model is depicted graphically in figure 5.
Although it is possible to integrate out ?0 as we
did the individual ?i, we instead choose for ef-
ficiency and simplicity to sample the global mix-
ture distribution ?0 from the posterior distribution
P (?0|Z).8 The mention generation terms in the
model and sampler are unchanged.
In the full hierarchical model, our equation (1) for
sampling entities, ignoring the salience component
of section 3.4, becomes:
P (Zi,j = z|Z?i,j , ?0)?
{
??u0 , if z = znew
nz + ??z0 , otherwise
where ?z0 is the probability of the entity z under the
sampled global entity distribution and ?u0 is the un-
known component mass of this distribution.
The HDP layer of sharing improves the model?s
predictions to 72.5 F1 on our development data. We
should emphasize that our evaluation is of course
per-document and does not reflect cross-document
coreference decisions, only the gains through cross-
document sharing (see section 6.2).
8We do not give the details here; see Teh et al (2006) for de-
tails on how to implement this component of the sampler (called
?direct assignment? in that reference).
4 Inference Details
Up until now, we?ve discussed Gibbs sampling, but
we are not interested in sampling from the poste-
rior P (Z|X), but in finding its mode. Instead of
sampling directly from the posterior distribution, we
instead sample entities proportionally to exponen-
tiated entity posteriors. The exponent is given by
exp cik?1 , where i is the current round number (start-
ing at i = 0), c = 1.5 and k = 20 is the total num-
ber of sampling epochs. This slowly raises the pos-
terior exponent from 1.0 to ec. In our experiments,
we found this procedure to outperform simulated an-
nealing. We also found sampling the T , G, and N
variables to be particularly inefficient, so instead we
maintain soft counts over each of these variables and
use these in place of a hard sampling scheme. We
also found that correctly accounting for the future
impact of salience changes to be particularly ineffi-
cient. However, ignoring those terms entirely made
negligible difference in final accuracy.9
5 Final Experiments
We present our final experiments using the full
model developed in section 3. As in section 3, we
use true mention boundaries and evaluate using the
MUC F1 measure (Vilain et al, 1995). All hyper-
parameters were tuned on the development set only.
The document concentration parameter ? was set by
taking a constant proportion of the average number
of mentions in a document across the corpus. This
number was chosen to minimize the squared error
between the number of proposed entities and true
entities in a document. It was not tuned to maximize
the F1 measure. A coefficient of 0.4 was chosen.
The global concentration coefficient ? was chosen
to be a constant proportion of ?M , where M is the
number of documents in the corpus. We found 0.15
to be a good value using the same least-square pro-
cedure. The values for these coefficients were not
changed for the experiments on the test sets.
5.1 MUC-6
Our main evaluation is on the standard MUC-6 for-
mal test set.10 The standard experimental setup for
9This corresponds to truncating equation (2) at j? = j.
10Since the MUC data is not annotated with mention types,
we automatically detect this information in the same way as Luo
853
Dataset Num Docs. Prec. Recall F1
MUC-6 60 80.8 52.8 63.9
+DRYRUN-TRAIN 251 79.1 59.7 68.0
+ENGLISH-NWIRE 381 80.4 62.4 70.3
Dataset Prec. Recall F1
ENGLISH-NWIRE 66.7 62.3 64.2
ENGLISH-BNEWS 63.2 61.3 62.3
CHINESE-NWIRE 71.6 63.3 67.2
CHINESE-BNEWS 71.2 61.8 66.2
(a) (b)
Table 2: Formal Results: Our system evaluated using the MUC model theoretic measure Vilain et al (1995). The table in (a) is
our performance on the thirty document MUC-6 formal test set with increasing amounts of training data. In all cases for the table,
we are evaluating on the same thirty document test set which is included in our training set, since our system in unsupervised. The
table in (b) is our performance on the ACE 2004 training sets.
this data is a 30/30 document train/test split. Train-
ing our system on all 60 documents of the training
and test set (as this is in an unsupervised system,
the unlabeled test documents are present at train-
ing time), but evaluating only on the test documents,
gave 63.9 F1 and is labeled MUC-6 in table 2(a).
One advantage of an unsupervised approach is
that we can easily utilize more data when learning a
model. We demonstrate the effectiveness of this fact
by evaluating on the MUC-6 test documents with in-
creasing amounts of unannotated training data. We
first added the 191 documents from the MUC-6
dryrun training set (which were not part of the train-
ing data for official MUC-6 evaluation). This model
gave 68.0 F1 and is labeled +DRYRUN-TRAIN in ta-
ble 2(a). We then added the ACE ENGLISH-NWIRE
training data, which is from a different corpora than
the MUC-6 test set and from a different time period.
This model gave 70.3 F1 and is labeled +ENGLISH-
NWIRE in table 2(a).
Our results on this test set are surprisingly com-
parable to, though slightly lower than, some recent
supervised systems. McCallum and Wellner (2004)
report 73.4 F1 on the formal MUC-6 test set, which
is reasonably close to our best MUC-6 number of
70.3 F1. McCallum and Wellner (2004) also report
a much lower 91.6 F1 on only proper nouns men-
tions. Our system achieves a 89.8 F1 when evalu-
ation is restricted to only proper mentions.11 The
et al (2004). A mention is proper if it is annotated with NER
information. It is a pronoun if the head is on the list of En-
glish pronouns. Otherwise, it is a nominal mention. Note we do
not use the NER information for any purpose but determining
whether the mention is proper.
11The best results we know on the MUC-6 test set using the
standard setting are due to Luo et al (2004) who report a 81.3
F1 (much higher than others). However, it is not clear this is a
comparable number, due to the apparent use of gold NER fea-
tures, which provide a strong clue to coreference. Regardless, it
is unsurprising that their system, which has many rich features,
would outperform ours.
HEAD ENT TYPE GENDER NUMBER
Bush: 1.0 PERS MALE SG
AP: 1.0 ORG NEUTER PL
viacom: 0.64, company: 0.36 ORG NEUTER SG
teamsters: 0.22, union: 0.78, MISC NEUTER PL
Table 3: Frequent entities occurring across documents along
with head distribution and mode of property distributions.
closest comparable unsupervised system is Cardie
and Wagstaff (1999) who use pairwise NP distances
to cluster document mentions. They report a 53.6 F1
on MUC6 when tuning distance metric weights to
maximize F1 on the development set.
5.2 ACE 2004
We also performed experiments on ACE 2004 data.
Due to licensing restrictions, we did not have access
to the ACE 2004 formal development and test sets,
and so the results presented are on the training sets.
We report results on the newswire section (NWIRE
in table 2b) and the broadcast news section (BNEWS
in table 2b). These datasets include the prenomi-
nal mention type, which is not present in the MUC-
6 data. We treated prenominals analogously to the
treatment of proper and nominal mentions.
We also tested our system on the Chinese
newswire and broadcast news sections of the ACE
2004 training sets. Our relatively higher perfor-
mance on Chinese compared to English is perhaps
due to the lack of prenominal mentions in the Chi-
nese data, as well as the presence of fewer pronouns
compared to English.
Our ACE results are difficult to compare exactly
to previous work because we did not have access
to the restricted formal test set. However, we can
perform a rough comparison between our results on
the training data (without coreference annotation) to
supervised work which has used the same training
data (with coreference annotation) and evaluated on
the formal test set. Denis and Baldridge (2007) re-
854
port 67.1 F1 and 69.2 F1 on the English NWIRE and
BNEWS respectively using true mention boundaries.
While our system underperforms the supervised sys-
tems, its accuracy is nonetheless promising.
6 Discussion
6.1 Error Analysis
The largest source of error in our system is between
coreferent proper and nominal mentions. The most
common examples of this kind of error are appos-
itive usages e.g. George W. Bush, president of the
US, visited Idaho. Another error of this sort can be
seen in figure 2, where the corporation mention is
not labeled coreferent with the The Weir Groupmen-
tion. Examples such as these illustrate the regular (at
least in newswire) phenomenon that nominal men-
tions are used with informative intent, even when the
entity is salient and a pronoun could have been used
unambiguously. This aspect of nominal mentions is
entirely unmodeled in our system.
6.2 Global Coreference
Since we do not have labeled cross-document coref-
erence data, we cannot evaluate our system?s cross-
document performance quantitatively. However, in
addition to observing the within-document gains
from sharing shown in section 3, we can manually
inspect the most frequently occurring entities in our
corpora. Table 3 shows some of the most frequently
occurring entities across the English ACE NWIRE
corpus. Note that Bush is the most frequent entity,
though his (and others?) nominal cluster president
is mistakenly its own entity. Merging of proper and
nominal clusters does occur as can be seen in table 3.
6.3 Unsupervised NER
We can use our model to for unsupervised NER
tagging: for each proper mention, assign the mode
of the generating entity?s distribution over entity
types. Note that in our model the only way an en-
tity becomes associated with an entity type is by
the pronouns used to refer to it.12 If we evaluate
our system as an unsupervised NER tagger for the
proper mentions in the MUC-6 test set, it yields a
12Ge et al (1998) exploit a similar idea to assign gender to
proper mentions.
per-label accuracy of 61.2% (on MUC labels). Al-
though nowhere near the performance of state-of-
the-art systems, this result beats a simple baseline of
always guessing PERSON (the most common entity
type), which yields 46.4%. This result is interest-
ing given that the model was not developed for the
purpose of inferring entity types whatsoever.
7 Conclusion
We have presented a novel, unsupervised approach
to coreference resolution: global entities are shared
across documents, the number of entities is deter-
mined by the model, and mentions are generated by
a sequential salience model and a model of pronoun-
entity association. Although our system does not
perform quite as well as state-of-the-art supervised
systems, its performance is in the same general
range, despite the system being unsupervised.
References
I. Bhattacharya and L. Getoor. 2006. A latent dirichlet model
for unsupervised entity resolution. SIAM conference on data
mining.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. EMNLP.
Hal Daume and Daniel Marcu. 2005. A Bayesian model for su-
pervised clustering with the Dirichlet process prior. JMLR.
Pascal Denis and Jason Baldridge. 2007. Global, joint determi-
nation of anaphoricity and coreference resolution using inte-
ger programming. HLT-NAACL.
Thomas Ferguson. 1973. A bayesian analysis of some non-
parametric problems. Annals of Statistics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical
approach to anaphora resolution. Sixth Workshop on Very
Large Corpora.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the bell tree. ACL.
Andrew McCallum and Ben Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. NIPS.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. Blog: Proba-
bilistic models with unknown objects. IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. ACL.
NIST. 2004. The ACE evaluation plan.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics.
Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei.
2006. Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101.
Marc Vilain, John Burger, John Aberdeen, Dennis Connolly,
and Lynette Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. MUC-6.
855
Proceedings of ACL-08: HLT, pages 771?779,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Bilingual Lexicons from Monolingual Corpora
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,pliang,tberg,klein }@cs.berkeley.edu
Abstract
We present a method for learning bilingual
translation lexicons from monolingual cor-
pora. Word types in each language are charac-
terized by purely monolingual features, such
as context counts and orthographic substrings.
Translations are induced using a generative
model based on canonical correlation analy-
sis, which explains the monolingual lexicons
in terms of latent matchings. We show that
high-precision lexicons can be learned in a va-
riety of language pairs and from a range of
corpus types.
1 Introduction
Current statistical machine translation systems use
parallel corpora to induce translation correspon-
dences, whether those correspondences be at the
level of phrases (Koehn, 2004), treelets (Galley et
al., 2006), or simply single words (Brown et al,
1994). Although parallel text is plentiful for some
language pairs such as English-Chinese or English-
Arabic, it is scarce or even non-existent for most
others, such as English-Hindi or French-Japanese.
Moreover, parallel text could be scarce for a lan-
guage pair even if monolingual data is readily avail-
able for both languages.
In this paper, we consider the problem of learning
translations from monolingual sources alone. This
task, though clearly more difficult than the standard
parallel text approach, can operate on language pairs
and in domains where standard approaches cannot.
We take as input two monolingual corpora and per-
haps some seed translations, and we produce as out-
put a bilingual lexicon, defined as a list of word
pairs deemed to be word-level translations. Preci-
sion and recall are then measured over these bilin-
gual lexicons. This setting has been considered be-
fore, most notably in Koehn and Knight (2002) and
Fung (1995), but the current paper is the first to use
a probabilistic model and present results across a va-
riety of language pairs and data conditions.
In our method, we represent each language as a
monolingual lexicon (see figure 2): a list of word
types characterized by monolingual feature vectors,
such as context counts, orthographic substrings, and
so on (section 5). We define a generative model over
(1) a source lexicon, (2) a target lexicon, and (3) a
matching between them (section 2). Our model is
based on canonical correlation analysis (CCA)1 and
explains matched word pairs via vectors in a com-
mon latent space. Inference in the model is done
using an EM-style algorithm (section 3).
Somewhat surprisingly, we show that it is pos-
sible to learn or extend a translation lexicon us-
ing monolingual corpora alone, in a variety of lan-
guages and using a variety of corpora, even in the
absence of orthographic features. As might be ex-
pected, the task is harder when no seed lexicon is
provided, when the languages are strongly diver-
gent, or when the monolingual corpora are from dif-
ferent domains. Nonetheless, even in the more diffi-
cult cases, a sizable set of high-precision translations
can be extracted. As an example of the performance
of the system, in English-Spanish induction with our
best feature set, using corpora derived from topically
similar but non-parallel sources, the system obtains
89.0% precision at 33% recall.
1See Hardoon et al (2003) for an overview.
771
state
society
enlarge-
ment
control
import-
ance
sociedad
estado
amplifi-
caci?n
import-
ancia
control
.
.
.
.
.
.
s
t
m
Figure 1: Bilingual lexicon induction: source word types
s are listed on the left and target word types t on the
right. Dashed lines between nodes indicate translation
pairs which are in the matching m.
2 Bilingual Lexicon Induction
As input, we are given a monolingual corpus S (a
sequence of word tokens) in a source language and
a monolingual corpus T in a target language. Let
s = (s1, . . . , snS ) denote nS word types appearing
in the source language, and t = (t1, . . . , tnT ) denote
word types in the target language. Based on S and
T , our goal is to output a matching m between s
and t. We represent m as a set of integer pairs so
that (i, j) ?m if and only if si is matched with tj .
2.1 Generative Model
We propose the following generative model over
matchings m and word types (s, t), which we call
matching canonical correlation analysis (MCCA).
MCCA model
m ? MATCHING-PRIOR [matching m]
For each matched edge (i, j) ?m:
?z
i,j
? N (0, I
d
) [latent concept]
?f
S
(s
i
) ? N (W
S
z
i,j
,?
S
) [source features]
?f
T
(t
i
) ? N (W
T
z
i,j
,?
T
) [target features]
For each unmatched source word type i:
?f
S
(s
i
) ? N (0, ?
2
I
d
S
) [source features]
For each unmatched target word type j:
?f
T
(t
j
) ? N (0, ?
2
I
d
T
) [target features]
First, we generate a matching m ?M, whereM
is the set of matchings in which each word type is
matched to at most one other word type.2 We take
MATCHING-PRIOR to be uniform overM.3
Then, for each matched pair of word types (i, j) ?
m, we need to generate the observed feature vectors
of the source and target word types, fS(si) ? RdS
and fT (tj) ? RdT . The feature vector of each word
type is computed from the appropriate monolin-
gual corpus and summarizes the word?s monolingual
characteristics; see section 5 for details and figure 2
for an illustration. Since si and tj are translations of
each other, we expect fS(si) and fT (tj) to be con-
nected somehow by the generative process. In our
model, they are related through a vector zi,j ? Rd
representing the shared, language-independent con-
cept.
Specifically, to generate the feature vectors, we
first generate a random concept zi,j ? N (0, Id),
where Id is the d ? d identity matrix. The source
feature vector fS(si) is drawn from a multivari-
ate Gaussian with mean WSzi,j and covariance ?S ,
where WS is a dS ? d matrix which transforms the
language-independent concept zi,j into a language-
dependent vector in the source space. The arbitrary
covariance parameter ?S  0 explains the source-
specific variations which are not captured by WS ; it
does not play an explicit role in inference. The target
fT (tj) is generated analogously using WT and ?T ,
conditionally independent of the source given zi,j
(see figure 2). For each of the remaining unmatched
source word types si which have not yet been gen-
erated, we draw the word type features from a base-
line normal distribution with variance ?2IdS , with
hyperparameter ?2  0; unmatched target words
are similarly generated.
If two word types are truly translations, it will be
better to relate their feature vectors through the la-
tent space than to explain them independently via
the baseline distribution. However, if a source word
type is not a translation of any of the target word
types, we can just generate it independently without
requiring it to participate in the matching.
2Our choice ofM permits unmatched word types, but does
not allow words to have multiple translations. This setting facil-
itates comparison to previous work and admits simpler models.
3However, non-uniform priors could encode useful informa-
tion, such as rank similarities.
772
1.0
1.0
20.0
5.0
100.0
50.0
.
.
.
Source 
Space
Canonical 
Space
R
d
s
R
d
t
1.0
1.0
.
.
.
1.0
Target 
Space
R
d
1.0
{
{
O
r
t
h
o
g
r
a
p
h
i
c
 
F
e
a
t
u
r
e
s
C
o
n
t
e
x
t
u
a
l
 
F
e
a
t
u
r
e
s
time
tiempo
#ti
#ti
ime
mpo
me#
pe#
change
dawn
period
necessary
40.0
65.0
120.0
45.0
suficiente
per?odo
mismo
adicional
s
i
t
j
z
f
S
(s
i
)
f
T
(t
j
)
Figure 2: Illustration of our MCCA model. Each latent concept z
i,j
originates in the canonical space. The observed
word vectors in the source and target spaces are generated independently given this concept.
3 Inference
Given our probabilistic model, we would like to
maximize the log-likelihood of the observed data
(s, t):
`(?) = log p(s, t; ?) = log
?
m
p(m, s, t; ?)
with respect to the model parameters ? =
(WS ,WT ,?S ,?T ).
We use the hard (Viterbi) EM algorithm as a start-
ing point, but due to modeling and computational
considerations, we make several important modifi-
cations, which we describe later. The general form
of our algorithm is as follows:
Summary of learning algorithm
E-step: Find the maximum weighted (partial) bi-
partite matching m ?M
M-step: Find the best parameters ? by performing
canonical correlation analysis (CCA)
M-step Given a matching m, the M-step opti-
mizes log p(m, s, t; ?) with respect to ?, which can
be rewritten as
max
?
?
(i,j)?m
log p(si, tj ; ?). (1)
This objective corresponds exactly to maximizing
the likelihood of the probabilistic CCA model pre-
sented in Bach and Jordan (2006), which proved
that the maximum likelihood estimate can be com-
puted by canonical correlation analysis (CCA). In-
tuitively, CCA finds d-dimensional subspaces US ?
R
dS?d of the source and UT ? RdT?d of the tar-
get such that the components of the projections
U
>
S fS(si) and U
>
T fT (tj) are maximally correlated.
4
US and UT can be found by solving an eigenvalue
problem (see Hardoon et al (2003) for details).
Then the maximum likelihood estimates are as fol-
lows: WS = CSSUSP 1/2, WT = CTTUTP 1/2,
?S = CSS ?WSW
>
S , and ?T = CTT ?WTW
>
T ,
where P is a d? d diagonal matrix of the canonical
correlations, CSS = 1|m|
?
(i,j)?m fS(si)fS(si)
> is
the empirical covariance matrix in the source do-
main, and CTT is defined analogously.
E-step To perform a conventional E-step, we
would need to compute the posterior over all match-
ings, which is #P-complete (Valiant, 1979). On the
other hand, hard EM only requires us to compute the
best matching under the current model:5
m = argmax
m?
log p(m?, s, t; ?). (2)
We cast this optimization as a maximum weighted
bipartite matching problem as follows. Define the
edge weight between source word type i and target
word type j to be
wi,j = log p(si, tj ; ?) (3)
? log p(si; ?)? log p(tj ; ?),
4Since dS and dT can be quite large in practice and of-
ten greater than |m|, we use Cholesky decomposition to re-
represent the feature vectors as |m|-dimensional vectors with
the same dot products, which is all that CCA depends on.
5If we wanted softer estimates, we could use the agreement-
based learning framework of Liang et al (2008) to combine two
tractable models.
773
which can be loosely viewed as a pointwise mutual
information quantity. We can check that the ob-
jective log p(m, s, t; ?) is equal to the weight of a
matching plus some constant C:
log p(m, s, t; ?) =
?
(i,j)?m
wi,j + C. (4)
To find the optimal partial matching, edges with
weight wi,j < 0 are set to zero in the graph and the
optimal full matching is computed inO((nS+nT )3)
time using the Hungarian algorithm (Kuhn, 1955). If
a zero edge is present in the solution, we remove the
involved word types from the matching.6
Bootstrapping Recall that the E-step produces a
partial matching of the word types. If too few
word types are matched, learning will not progress
quickly; if too many are matched, the model will be
swamped with noise. We found that it was helpful
to explicitly control the number of edges. Thus, we
adopt a bootstrapping-style approach that only per-
mits high confidence edges at first, and then slowly
permits more over time. In particular, we compute
the optimal full matching, but only retain the high-
est weighted edges. As we run EM, we gradually
increase the number of edges to retain.
In our context, bootstrapping has a similar moti-
vation to the annealing approach of Smith and Eisner
(2006), which also tries to alter the space of hidden
outputs in the E-step over time to facilitate learn-
ing in the M-step, though of course the use of boot-
strapping in general is quite widespread (Yarowsky,
1995).
4 Experimental Setup
In section 5, we present developmental experiments
in English-Spanish lexicon induction; experiments
6Empirically, we obtained much better efficiency and even
increased accuracy by replacing these marginal likelihood
weights with a simple proxy, the distances between the words?
mean latent concepts:
wi,j = A? ||z
?
i ? z
?
j ||2, (5)
where A is a thresholding constant, z?i = E(zi,j | fS(si)) =
P 1/2U>S fS(si), and z
?
j is defined analogously. The increased
accuracy may not be an accident: whether two words are trans-
lations is perhaps better characterized directly by how close
their latent concepts are, whereas log-probability is more sensi-
tive to perturbations in the source and target spaces.
are presented for other languages in section 6. In
this section, we describe the data and experimental
methodology used throughout this work.
4.1 Data
Each experiment requires a source and target mono-
lingual corpus. We use the following corpora:
? EN-ES-W: 3,851 Wikipedia articles with both
English and Spanish bodies (generally not di-
rect translations).
? EN-ES-P: 1st 100k sentences of text from the
parallel English and Spanish Europarl corpus
(Koehn, 2005).
? EN-ES(FR)-D: English: 1st 50k sentences of
Europarl; Spanish (French): 2nd 50k sentences
of Europarl.7
? EN-CH-D: English: 1st 50k sentences of Xin-
hua parallel news corpora;8 Chinese: 2nd 50k
sentences.
? EN-AR-D: English: 1st 50k sentences of 1994
proceedings of UN parallel corpora;9 Ara-
bic: 2nd 50k sentences.
? EN-ES-G: English: 100k sentences of English
Gigaword; Spanish: 100k sentences of Spanish
Gigaword.10
Note that even when corpora are derived from par-
allel sources, no explicit use is ever made of docu-
ment or sentence-level alignments. In particular, our
method is robust to permutations of the sentences in
the corpora.
4.2 Lexicon
Each experiment requires a lexicon for evaluation.
Following Koehn and Knight (2002), we consider
lexicons over only noun word types, although this
is not a fundamental limitation of our model. We
consider a word type to be a noun if its most com-
mon tag is a noun in our monolingual corpus.11 For
7Note that the although the corpora here are derived from a
parallel corpus, there are no parallel sentences.
8LDC catalog # 2002E18.
9LDC catalog # 2004E13.
10These corpora contain no parallel sentences.
11We use the Tree Tagger (Schmid, 1994) for all POS tagging
except for Arabic, where we use the tagger described in Diab et
al. (2004).
774
 0.6 0.65 0.7 0.75 0.8 0.85
 0.9 0.95 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8Precision Recall EN-ES-PEN-ES-W
Figure 3: Example precision/recall curve of our system
on EN-ES-P and EN-ES-W settings. See section 6.1.
all languages pairs except English-Arabic, we ex-
tract evaluation lexicons from the Wiktionary on-
line dictionary. As we discuss in section 7, our ex-
tracted lexicons have low coverage, particularly for
proper nouns, and thus all performance measures are
(sometimes substantially) pessimistic. For English-
Arabic, we extract a lexicon from 100k parallel sen-
tences of UN parallel corpora by running the HMM
intersected alignment model (Liang et al, 2008),
adding (s, t) to the lexicon if s was aligned to t at
least three times and more than any other word.
Also, as in Koehn and Knight (2002), we make
use of a seed lexicon, which consists of a small, and
perhaps incorrect, set of initial translation pairs. We
used two methods to derive a seed lexicon. The
first is to use the evaluation lexicon Le and select
the hundred most common noun word types in the
source corpus which have translations in Le. The
second method is to heuristically induce, where ap-
plicable, a seed lexicon using edit distance, as is
done in Koehn and Knight (2002). Section 6.2 com-
pares the performance of these two methods.
4.3 Evaluation
We evaluate a proposed lexicon Lp against the eval-
uation lexicon Le using the F1 measure in the stan-
dard fashion; precision is given by the number of
proposed translations contained in the evaluation
lexicon, and recall is given by the fraction of pos-
sible translation pairs proposed.12 Since our model
12We should note that precision is not penalized for (s, t) if
s does not have a translation in Le, and recall is not penalized
for failing to recover multiple translations of s.
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ?- 47.4
ORTHO 76.0 81.3 80.1 52.3 55.0
CONTEXT 91.1 81.3 80.2 65.3 58.0
MCCA 87.2 89.7 89.0 89.7 72.0
Table 1: Performance of EDITDIST and our model with
various features sets on EN-ES-W. See section 5.
naturally produces lexicons in which each entry is
associated with a weight based on the model, we can
give a full precision/recall curve (see figure 3). We
summarize these curves with both the best F1 over
all possible thresholds and various precisions px at
recalls x. All reported numbers exclude evaluation
on the seed lexicon entries, regardless of how those
seeds are derived or whether they are correct.
In all experiments, unless noted otherwise, we
used a seed of size 100 obtained from Le and
considered lexicons between the top n = 2, 000
most frequent source and target noun word types
which were not in the seed lexicon; each system
proposed an already-ranked one-to-one translation
lexicon amongst these n words. Where applica-
ble, we compare against the EDITDIST baseline,
which solves a maximum bipartite matching prob-
lem where edge weights are normalized edit dis-
tances. We will use MCCA (for matching CCA) to
denote our model using the optimal feature set (see
section 5.3).
5 Features
In this section, we explore feature representations of
word types in our model. Recall that fS(?) and fT (?)
map source and target word types to vectors in RdS
and RdT , respectively (see section 2). The features
used in each representation are defined identically
and derived only from the appropriate monolingual
corpora. For a concrete example of a word type to
feature vector mapping, see figure 2.
5.1 Orthographic Features
For closely related languages, such as English and
Spanish, translation pairs often share many ortho-
graphic features. One direct way to capture ortho-
graphic similarity between word pairs is edit dis-
tance. Running EDITDIST (see section 4.3) on EN-
775
ES-W yielded 61.1 p0.33, but precision quickly de-
grades for higher recall levels (see EDITDIST in ta-
ble 1). Nevertheless, when available, orthographic
clues are strong indicators of translation pairs.
We can represent orthographic features of a word
type w by assigning a feature to each substring of
length ? 3. Note that MCCA can learn regular or-
thographic correspondences between source and tar-
get words, which is something edit distance cannot
capture (see table 5). Indeed, running our MCCA
model with only orthographic features on EN-ES-
W, labeled ORTHO in table 1, yielded 80.1 p0.33, a
31% error-reduction over EDITDIST in p0.33.
5.2 Context Features
While orthographic features are clearly effective for
historically related language pairs, they are more
limited for other language pairs, where we need to
appeal to other clues. One non-orthographic clue
that word types s and t form a translation pair is
that there is a strong correlation between the source
words used with s and the target words used with t.
To capture this information, we define context fea-
tures for each word type w, consisting of counts of
nouns which occur within a window of size 4 around
w. Consider the translation pair (time, tiempo)
illustrated in figure 2. As we become more con-
fident about other translation pairs which have ac-
tive period and periodico context features, we
learn that translation pairs tend to jointly generate
these features, which leads us to believe that time
and tiempo might be generated by a common un-
derlying concept vector (see section 2).13
Using context features alone on EN-ES-W, our
MCCA model (labeled CONTEXT in table 1) yielded
a 80.2 p0.33. It is perhaps surprising that context fea-
tures alone, without orthographic information, can
yield a best-F1comparable to EDITDIST.
5.3 Combining Features
We can of course combine context and orthographic
features. Doing so yielded 89.03 p0.33 (labeled
MCCA in table 1); this represents a 46.4% error re-
duction in p0.33 over the EDITDIST baseline. For the
remainder of this work, we will use MCCA to refer
13It is important to emphasize, however, that our current
model does not directly relate a word type?s role as a partici-
pant in the matching to that word?s role as a context feature.
(a) Corpus Variation
Setting p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES-G 75.0 71.2 68.3 ?- 49.0
EN-ES-W 87.2 89.7 89.0 89.7 72.0
EN-ES-D 91.4 94.3 92.3 89.7 63.7
EN-ES-P 97.3 94.8 93.8 92.9 77.0
(b) Seed Lexicon Variation
Corpus p0.1 p0.25 p0.33 p0.50 Best-F1
EDITDIST 58.6 62.6 61.1 ? 47.4
MCCA 91.4 94.3 92.3 89.7 63.7
MCCA-AUTO 91.2 90.5 91.8 77.5 61.7
(c) Language Variation
Languages p0.1 p0.25 p0.33 p0.50 Best-F1
EN-ES 91.4 94.3 92.3 89.7 63.7
EN-FR 94.5 89.1 88.3 78.6 61.9
EN-CH 60.1 39.3 26.8 ?- 30.8
EN-AR 70.0 50.0 31.1 ?- 33.1
Table 2: (a) varying type of corpora used on system per-
formance (section 6.1), (b) using a heuristically chosen
seed compared to one taken from the evaluation lexicon
(section 6.2), (c) a variety of language pairs (see sec-
tion 6.3).
to our model using both orthographic and context
features.
6 Experiments
In this section we examine how system performance
varies when crucial elements are altered.
6.1 Corpus Variation
There are many sources from which we can derive
monolingual corpora, and MCCA performance de-
pends on the degree of similarity between corpora.
We explored the following levels of relationships be-
tween corpora, roughly in order of closest to most
distant:
? Same Sentences: EN-ES-P
? Non-Parallel Similar Content: EN-ES-W
? Distinct Sentences, Same Domain: EN-ES-D
? Unrelated Corpora: EN-ES-G
Our results for all conditions are presented in ta-
ble 2(a). The predominant trend is that system per-
formance degraded when the corpora diverged in
776
content, presumably due to context features becom-
ing less informative. However, it is notable that even
in the most extreme case of disjoint corpora from
different time periods and topics (e.g. EN-ES-G),
we are still able to recover lexicons of reasonable
accuracy.
6.2 Seed Lexicon Variation
All of our experiments so far have exploited a small
seed lexicon which has been derived from the eval-
uation lexicon (see section 4.3). In order to explore
system robustness to heuristically chosen seed lexi-
cons, we automatically extracted a seed lexicon sim-
ilarly to Koehn and Knight (2002): we ran EDIT-
DIST on EN-ES-D and took the top 100 most con-
fident translation pairs. Using this automatically de-
rived seed lexicon, we ran our system on EN-ES-
D as before, evaluating on the top 2,000 noun word
types not included in the automatic lexicon.14 Us-
ing the automated seed lexicon, and still evaluat-
ing against our Wiktionary lexicon, MCCA-AUTO
yielded 91.8 p0.33 (see table 2(b)), indicating that
our system can produce lexicons of comparable ac-
curacy with a heuristically chosen seed. We should
note that this performance represents no knowledge
given to the system in the form of gold seed lexicon
entries.
6.3 Language Variation
We also explored how system performance varies
for language pairs other than English-Spanish. On
English-French, for the disjoint EN-FR-D corpus
(described in section 4.1), MCCA yielded 88.3 p0.33
(see table 2(c) for more performance measures).
This verified that our model can work for another
closely related language-pair on which no model de-
velopment was performed.
One concern is how our system performs on lan-
guage pairs where orthographic features are less ap-
plicable. Results on disjoint English-Chinese and
English-Arabic are given as EN-CH-D and EN-AR
in table 2(c), both using only context features. In
these cases, MCCA yielded much lower precisions
of 26.8 and 31.0 p0.33, respectively. For both lan-
guages, performance degraded compared to EN-ES-
14Note that the 2,000 words evaluated here were not identical
to the words tested on when the seed lexicon is derived from the
evaluation lexicon.
(a) English-Spanish
Rank Source Target Correct
1. education educaci?n Y
2. pacto pact Y
3. stability estabilidad Y
6. corruption corrupci?n Y
7. tourism turismo Y
9. organisation organizaci?n Y
10. convenience conveniencia Y
11. syria siria Y
12. cooperation cooperaci?n Y
14. culture cultura Y
21. protocol protocolo Y
23. north norte Y
24. health salud Y
25. action reacci?n N
(b) English-French
Rank Source Target Correct
3. xenophobia x?nophobie Y
4. corruption corruption Y
5. subsidiarity subsidiarit? Y
6. programme programme-cadre N
8. traceability tra?abilit? Y
(c) English-Chinese
Rank Source Target Correct
1. prices ? Y
2. network ? Y
3. population ? Y
4. reporter ? N
5. oil ? Y
Table 3: Sample output from our (a) Spanish, (b) French,
and (c) Chinese systems. We present the highest con-
fidence system predictions, where the only editing done
is to ignore predictions which consist of identical source
and target words.
D and EN-FR-D, presumably due in part to the
lack of orthographic features. However, MCCA still
achieved surprising precision at lower recall levels.
For instance, at p0.1, MCCA yielded 60.1 and 70.0
on Chinese and Arabic, respectively. Figure 3 shows
the highest-confidence outputs in several languages.
6.4 Comparison To Previous Work
There has been previous work in extracting trans-
lation pairs from non-parallel corpora (Rapp, 1995;
Fung, 1995; Koehn and Knight, 2002), but gener-
ally not in as extreme a setting as the one consid-
ered here. Due to unavailability of data and speci-
ficity in experimental conditions and evaluations, it
is not possible to perform exact comparisons. How-
777
(a) Example Non-Cognate Pairs
health salud
traceability rastreabilidad
youth juventud
report informe
advantages ventajas
(b) Interesting Incorrect Pairs
liberal partido
Kirkhope Gorsel
action reaccio?n
Albanians Bosnia
a.m. horas
Netherlands Bretan?a
Table 4: System analysis on EN-ES-W: (a) non-cognate
pairs proposed by our system, (b) hand-selected represen-
tative errors.
(a) Orthographic Feature
Source Feat. Closest Target Feats. Example Translation
#st #es, est (statue, estatua)
ty# ad#, d# (felicity, felicidad)
ogy g??a, g?? (geology, geolog??a)
(b) Context Feature
Source Feat. Closest Context Features
party partido, izquierda
democrat socialistas, demo?cratas
beijing pek??n, kioto
Table 5: Hand selected examples of source and target fea-
tures which are close in canonical space: (a) orthographic
feature correspondences, (b) context features.
ever, we attempted to run an experiment as similar
as possible in setup to Koehn and Knight (2002), us-
ing English Gigaword and German Europarl. In this
setting, our MCCA system yielded 61.7% accuracy
on the 186 most confident predictions compared to
39% reported in Koehn and Knight (2002).
7 Analysis
We have presented a novel generative model for
bilingual lexicon induction and presented results un-
der a variety of data conditions (section 6.1) and lan-
guages (section 6.3) showing that our system can
produce accurate lexicons even in highly adverse
conditions. In this section, we broadly characterize
and analyze the behavior of our system.
We manually examined the top 100 errors in the
English-Spanish lexicon produced by our system
on EN-ES-W. Of the top 100 errors: 21 were cor-
rect translations not contained in the Wiktionary
lexicon (e.g. pintura to painting), 4 were
purely morphological errors (e.g. airport to
aeropuertos), 30 were semantically related (e.g.
basketball to be?isbol), 15 were words with
strong orthographic similarities (e.g. coast to
costas), and 30 were difficult to categorize and
fell into none of these categories. Since many of
our ?errors? actually represent valid translation pairs
not contained in our extracted dictionary, we sup-
plemented our evaluation lexicon with one automat-
ically derived from 100k sentences of parallel Eu-
roparl data. We ran the intersected HMM word-
alignment model (Liang et al, 2008) and added
(s, t) to the lexicon if s was aligned to t at least
three times and more than any other word. Evaluat-
ing against the union of these lexicons yielded 98.0
p0.33, a significant improvement over the 92.3 us-
ing only the Wiktionary lexicon. Of the true errors,
the most common arose from semantically related
words which had strong context feature correlations
(see table 4(b)).
We also explored the relationships our model
learns between features of different languages. We
projected each source and target feature into the
shared canonical space, and for each projected
source feature we examined the closest projected
target features. In table 5(a), we present some of
the orthographic feature relationships learned by our
system. Many of these relationships correspond to
phonological and morphological regularities such as
the English suffix ing mapping to the Spanish suf-
fix g??a. In table 5(b), we present context feature
correspondences. Here, the broad trend is for words
which are either translations or semantically related
across languages to be close in canonical space.
8 Conclusion
We have presented a generative model for bilingual
lexicon induction based on probabilistic CCA. Our
experiments show that high-precision translations
can be mined without any access to parallel corpora.
It remains to be seen how such lexicons can be best
utilized, but they invite new approaches to the statis-
tical translation of resource-poor languages.
778
References
Francis R. Bach and Michael I. Jordan. 2006. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, University of California, Berkeley.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of arabic text: From raw text to
base phrase chunks. In HLT-NAACL.
Pascale Fung. 1995. Compiling bilingual lexicon entries
from a non-parallel english-chinese corpus. In Third
Annual Workshop on Very Large Corpora.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
David R. Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2003. Canonical correlation analysis an
overview with application to learning methods. Tech-
nical Report CSD-TR-03-02, Royal Holloway Univer-
sity of London.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of ACL Workshop on Unsupervised Lexical
Acquisition.
P. Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
P. Liang, D. Klein, and M. I. Jordan. 2008. Agreement-
based learning. In NIPS.
Reinhard Rapp. 1995. Identifying word translation in
non-parallel texts. In ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In International Conference
on New Methods in Language Processing.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
L. G. Valiant. 1979. The complexity of computing
the permanent. Theoretical Computer Science, 8:189?
201.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL.
779
Proceedings of ACL-08: HLT, pages 879?887,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Analyzing the Errors of Unsupervised Learning
Percy Liang Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang,klein}@cs.berkeley.edu
Abstract
We identify four types of errors that unsu-
pervised induction systems make and study
each one in turn. Our contributions include
(1) using a meta-model to analyze the incor-
rect biases of a model in a systematic way,
(2) providing an efficient and robust method
of measuring distance between two parameter
settings of a model, and (3) showing that lo-
cal optima issues which typically plague EM
can be somewhat alleviated by increasing the
number of training examples. We conduct
our analyses on three models: the HMM, the
PCFG, and a simple dependency model.
1 Introduction
The unsupervised induction of linguistic structure
from raw text is an important problem both for un-
derstanding language acquisition and for building
language processing systems such as parsers from
limited resources. Early work on inducing gram-
mars via EM encountered two serious obstacles: the
inappropriateness of the likelihood objective and the
tendency of EM to get stuck in local optima. With-
out additional constraints on bracketing (Pereira and
Shabes, 1992) or on allowable rewrite rules (Carroll
and Charniak, 1992), unsupervised grammar learn-
ing was ineffective.
Since then, there has been a large body of work
addressing the flaws of the EM-based approach.
Syntactic models empirically more learnable than
PCFGs have been developed (Clark, 2001; Klein
and Manning, 2004). Smith and Eisner (2005) pro-
posed a new objective function; Smith and Eis-
ner (2006) introduced a new training procedure.
Bayesian approaches can also improve performance
(Goldwater and Griffiths, 2007; Johnson, 2007;
Kurihara and Sato, 2006).
Though these methods have improved induction
accuracy, at the core they all still involve optimizing
non-convex objective functions related to the like-
lihood of some model, and thus are not completely
immune to the difficulties associated with early ap-
proaches. It is therefore important to better under-
stand the behavior of unsupervised induction sys-
tems in general.
In this paper, we take a step back and present
a more statistical view of unsupervised learning in
the context of grammar induction. We identify four
types of error that a system can make: approxima-
tion, identifiability, estimation, and optimization er-
rors (see Figure 1). We try to isolate each one in turn
and study its properties.
Approximation error is caused by a mis-match
between the likelihood objective optimized by EM
and the true relationship between sentences and their
syntactic structures. Our key idea for understand-
ing this mis-match is to ?cheat? and initialize EM
with the true relationship and then study the ways
in which EM repurposes our desired syntactic struc-
tures to increase likelihood. We present a meta-
model of the changes that EM makes and show how
this tool can shed some light on the undesired biases
of the HMM, the PCFG, and the dependency model
with valence (Klein and Manning, 2004).
Identifiability error can be incurred when two dis-
tinct parameter settings yield the same probabil-
ity distribution over sentences. One type of non-
identifiability present in HMMs and PCFGs is label
symmetry, which even makes computing a mean-
ingful distance between parameters NP-hard. We
present a method to obtain lower and upper bounds
on such a distance.
Estimation error arises from having too few train-
ing examples, and optimization error stems from
879
EM getting stuck in local optima. While it is to be
expected that estimation error should decrease as the
amount of data increases, we show that optimization
error can also decrease. We present striking experi-
ments showing that if our data actually comes from
the model family we are learning with, we can some-
times recover the true parameters by simply run-
ning EM without clever initialization. This result
runs counter to the conventional attitude that EM is
doomed to local optima; it suggests that increasing
the amount of data might be an effective way to par-
tially combat local optima.
2 Unsupervised models
Let x denote an input sentence and y denote the un-
observed desired output (e.g., a parse tree). We con-
sider a model family P = {p?(x,y) : ? ? ?}. For
example, if P is the set of all PCFGs, then the pa-
rameters ? would specify all the rule probabilities of
a particular grammar. We sometimes use ? and p?
interchangeably to simplify notation. In this paper,
we analyze the following three model families:
In the HMM, the input x is a sequence of words
and the output y is the corresponding sequence of
part-of-speech tags.
In the PCFG, the input x is a sequence of POS
tags and the output y is a binary parse tree with yield
x. We represent y as a multiset of binary rewrites of
the form (y ? y1 y2), where y is a nonterminal and
y1, y2 can be either nonterminals or terminals.
In the dependency model with valence (DMV)
(Klein and Manning, 2004), the input x =
(x1, . . . , xm) is a sequence of POS tags and the out-
put y specifies the directed links of a projective de-
pendency tree. The generative model is as follows:
for each head xi, we generate an independent se-
quence of arguments to the left and to the right from
a direction-dependent distribution over tags. At each
point, we stop with a probability parametrized by the
direction and whether any arguments have already
been generated in that direction. See Klein and Man-
ning (2004) for a formal description.
In all our experiments, we used the Wall Street
Journal (WSJ) portion of the Penn Treebank. We bi-
narized the PCFG trees and created gold dependency
trees according to the Collins head rules. We trained
45-state HMMs on all 49208 sentences, 11-state
PCFGs on WSJ-10 (7424 sentences) and DMVs
on WSJ-20 (25523 sentences) (Klein and Manning,
2004). We ran EM for 100 iterations with the pa-
rameters initialized uniformly (always plus a small
amount of random noise). We evaluated the HMM
and PCFG by mapping model states to Treebank
tags to maximize accuracy.
3 Decomposition of errors
Now we will describe the four types of errors (Fig-
ure 1) more formally. Let p?(x,y) denote the distri-
bution which governs the true relationship between
the input x and output y. In general, p? does not
live in our model family P . We are presented with
a set of n unlabeled examples x(1), . . . ,x(n) drawn
i.i.d. from the true p?. In unsupervised induction,
our goal is to approximate p? by some model p? ? P
in terms of strong generative capacity. A standard
approach is to use the EM algorithm to optimize
the empirical likelihood E? log p?(x).1 However, EM
only finds a local maximum, which we denote ??EM,
so there is a discrepancy between what we get (p??EM)
and what we want (p?).
We will define this discrepancy later, but for now,
it suffices to remark that the discrepancy depends
on the distribution over y whereas learning depends
only on the distribution over x. This is an important
property that distinguishes unsupervised induction
from more standard supervised learning or density
estimation scenarios.
Now let us walk through the four types of er-
ror bottom up. First, ??EM, the local maximum
found by EM, is in general different from ?? ?
argmax? E? log p?(x), any global maximum, which
we could find given unlimited computational re-
sources. Optimization error refers to the discrep-
ancy between ?? and ??EM.
Second, our training data is only a noisy sam-
ple from the true p?. If we had infinite data, we
would choose an optimal parameter setting under the
model, ??2 ? argmax? E log p?(x), where now the
expectation E is taken with respect to the true p? in-
stead of the training data. The discrepancy between
??2 and ?? is the estimation error.
Note that ??2 might not be unique. Let ?
?
1 denote
1Here, the expectation E?f(x) def= 1n
Pn
i=1 f(x
(i)) denotes
averaging some function f over the training data.
880
p? = true model
Approximation error (Section 4)
??1 = Best(argmax? E log p?(x))
Identifiability error (Section 5)
??2 ? argmax? E log p?(x)
Estimation error (Section 6)
?? ? argmax? E? log p?(x)
Optimization error (Section 7)
??EM= EM(E? log p?(x)) P
Figure 1: The discrepancy between what we get (??EM)
and what we want (p?) can be decomposed into four types
of errors. The box represents our model family P , which
is the set of possible parametrized distributions we can
represent. Best(S) returns the ? ? S which has the small-
est discrepancy with p?.
the maximizer of E log p?(x) that has the smallest
discrepancy with p?. Since ??1 and ?
?
2 have the same
value under the objective function, we would not be
able to choose ??1 over ?
?
2, even with infinite data or
unlimited computation. Identifiability error refers to
the discrepancy between ??1 and ?
?
2.
Finally, the model family P has fundamental lim-
itations. Approximation error refers to the discrep-
ancy between p? and p??1 . Note that ?
?
1 is not nec-
essarily the best in P . If we had labeled data, we
could find a parameter setting in P which is closer
to p? by optimizing joint likelihood E log p?(x,y)
(generative training) or even conditional likelihood
E log p?(y | x) (discriminative training).
In the remaining sections, we try to study each of
the four errors in isolation. In practice, since it is
difficult to work with some of the parameter settings
that participate in the error decomposition, we use
computationally feasible surrogates so that the error
under study remains the dominant effect.
4 Approximation error
We start by analyzing approximation error, the dis-
crepancy between p? and p??1 (the model found by
optimizing likelihood), a point which has been dis-
20 40 60 80 100iteration
-18.4
-18.0
-17.6
-17.2
-16.7
log
-lik
elih
ood
20 40 60 80 100iteration
0.2
0.4
0.6
0.8
1.0
Lab
eled
F 1
Figure 2: For the PCFG, when we initialize EM with the
supervised estimate ??gen, the likelihood increases but the
accuracy decreases.
cussed by many authors (Merialdo, 1994; Smith and
Eisner, 2005; Haghighi and Klein, 2006).2
To confront the question of specifically how
the likelihood diverges from prediction accuracy,
we perform the following experiment: we ini-
tialize EM with the supervised estimate3 ??gen =
argmax? E? log p?(x,y), which acts as a surrogate
for p?. As we run EM, the likelihood increases but
the accuracy decreases (Figure 2 shows this trend
for the PCFG; the HMM and DMV models behave
similarly). We believe that the initial iterations of
EM contain valuable information about the incor-
rect biases of these models. However, EM is chang-
ing hundreds of thousands of parameters at once in a
non-trivial way, so we need a way of characterizing
the important changes.
One broad observation we can make is that the
first iteration of EM reinforces the systematic mis-
takes of the supervised initializer. In the first E-step,
the posterior counts that are computed summarize
the predictions of the supervised system. If these
match the empirical counts, then the M-step does not
change the parameters. But if the supervised system
predicts too many JJs, for example, then the M-step
will update the parameters to reinforce this bias.
4.1 A meta-model for analyzing EM
We would like to go further and characterize the
specific changes EM makes. An initial approach is
to find the parameters that changed the most dur-
ing the first iteration (weighted by the correspond-
2Here, we think of discrepancy between p and p? as the error
incurred when using p? for prediction on examples generated
from p; in symbols, E(x,y)?ploss(y, argmaxy? p
?(y? | x)).
3For all our models, the supervised estimate is solved in
closed form by taking ratios of counts.
881
ing expected counts computed in the E-step). For
the HMM, the three most changed parameters are
the transitions 2:DT?8:JJ, START?0:NNP, and
8:JJ?3:NN.4 If we delve deeper, we can see that
2:DT?3:NN (the parameter with the 10th largest
change) fell and 2:DT?8:JJ rose. After checking
with a few examples, we can then deduce that some
nouns were retagged as adjectives. Unfortunately,
this type of ad-hoc reasoning requires considerable
manual effort and is rather subjective.
Instead, we propose using a general meta-model
to analyze the changes EM makes in an automatic
and objective way. Instead of treating parameters as
the primary object of study, we look at predictions
made by the model and study how they change over
time. While a model is a distribution over sentences,
a meta-model is a distribution over how the predic-
tions of the model change.
Let R(y) denote the set of parts of a predic-
tion y that we are interested in tracking. Each part
(c, l) ? R(y) consists of a configuration c and a lo-
cation l. For a PCFG, we define a configuration to
be a rewrite rule (e.g., c = PP?IN NP), and a loca-
tion l = [i, k, j] to be a span [i, j] split at k, where
the rewrite c is applied.
In this work, each configuration is associated with
a parameter of the model, but in general, a configu-
ration could be a larger unit such as a subtree, allow-
ing one to track more complex changes. The size of
a configuration governs how much the meta-model
generalizes from individual examples.
Let y(i,t) denote the model prediction on the i-th
training example after t iterations of EM. To sim-
plify notation, we write Rt = R(y(i,t)). The meta-
model explains how Rt became Rt+1.5
In general, we expect a part in Rt+1 to be ex-
plained by a part in Rt that has a similar location
and furthermore, we expect the locations of the two
parts to be related in some consistent way. The meta-
model uses two notions to formalize this idea: a dis-
tance d(l, l?) and a relation r(l, l?). For the PCFG,
d(l, l?) is the number of positions among i,j,k that
are the same as the corresponding ones in l?, and
r((i, k, j), (i?, k?, j?)) = (sign(i ? i?), sign(j ?
4Here 2:DT means state 2 of the HMM, which was greedily
mapped to DT.
5If the same part appears in both Rt and Rt+1, we remove
it from both sets.
j?), sign(k ? k?)) is one of 33 values. We define a
migration as a triple (c, c?, r(l, l?)); this is the unit of
change we want to extract from the meta-model.
Our meta-model provides the following genera-
tive story of how Rt becomes Rt+1: each new part
(c?, l?) ? Rt+1 chooses an old part (c, l) ? Rt with
some probability that depends on (1) the distance be-
tween the locations l and l? and (2) the likelihood of
the particular migration. Formally:
pmeta(Rt+1 | Rt) =
?
(c?,l?)?Rt+1
?
(c,l)?Rt
Z?1l? e
??d(l,l?)p(c? | c, r(l, l?)),
where Zl =
?
(c,l)?Rt e
??d(l,l?) is a normalization
constant, and ? is a hyperparameter controlling the
possibility of distant migrations (set to 3 in our ex-
periments).
We learn the parameters of the meta-model with
an EM algorithm similar to the one for IBM model
1. Fortunately, the likelihood objective is convex, so
we need not worry about local optima.
4.2 Results of the meta-model
We used our meta-model to analyze the approxima-
tion errors of the HMM, DMV, and PCFG. For these
models, we initialized EM with the supervised es-
timate ??gen and collected the model predictions as
EM ran. We then trained the meta-model on the pre-
dictions between successive iterations. The meta-
model gives us an expected count for each migra-
tion. Figure 3 lists the migrations with the highest
expected counts.
From these migrations, we can see that EM tries
to explain x better by making the corresponding y
more regular. In fact, many of the HMM migra-
tions on the first iteration attempt to resolve incon-
sistencies in gold tags. For example, noun adjuncts
(e.g., stock-index), tagged as both nouns and adjec-
tives in the Treebank, tend to become consolidated
under adjectives, as captured by migration (B). EM
also re-purposes under-utilized states to better cap-
ture distributional similarities. For example, state 24
has migrated to state 40 (N), both of which are now
dominated by proper nouns. State 40 initially con-
tained only #, but was quickly overrun with distribu-
tionally similar proper nouns such as Oct. and Chap-
ter, which also precede numbers, just as # does.
882
Iteration 0?1(A) START 4:NN24:NNP(B) 4:NN8:JJ 4:NN(C) 24:NNP 24:NNP36:NNPS
Iteration 1?2(D) 4:NN8:JJ 4:NN(E) START 4:NN24:NNP(F) 8:JJ11:RB 27:TO
Iteration 2?3(G) 24:NNP8:JJ U.S.(H) 24:NNP8:JJ 4:NN(I) 3:DT 24:NNP8:JJ
Iteration 3?4(J) 11:RB32:RP up(K) 24:NNP8:JJ U.S.(L) 19:, 11:RB32:RP
Iteration 4?5(M) 24:NNP34:$ 15:CD(N) 2:IN 24:NNP40:NNP(O) 11:RB32:RP down(a) Top HMM migrations. Example: migration (D) means a NN?NN transition is replaced by JJ?NN.
Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5
(A) DT NN NN (D) NNP NNP NNP (G) DT JJ NNS (J) DT JJ NN (M) POS JJ NN
(B) JJ NN NN (E) NNP NNP NNP (H) MD RB VB (K) DT NNP NN (N) NNS RB VBP
(C) NNP NNP (F) DT NNP NNP (I) VBP RB VB (L) PRP$ JJ NN (O) NNS RB VBD
(b) Top DMV migrations. Example: migration (A) means a DT attaches to the closer NN.
Iteration 0?1 Iteration 1?2 Iteration 2?3 Iteration 3?4 Iteration 4?5
(A) RB 1:VP
4:S
RB 1:VP1:VP
(D) NNP 0:NP
0:NP
NNP NNP0:NP
(G) DT 0:NP
0:NP
DT NN0:NP
(J) TO VB
1:VP
TO VB2:PP
(M) CD NN
0:NP
CD NN3:ADJP
(B) 0:NP 2:PP
0:NP
1:VP 2:PP1:VP
(E) VBN 2:PP
1:VP
1:VP 2:PP1:VP
(H) 0:NP 1:VP
4:S
0:NP 1:VP4:S
(K) MD 1:VP
1:VP
MD VB1:VP
(N) VBD 0:NP
1:VP
VBD 3:ADJP1:VP
(C) VBZ 0:NP
1:VP
VBZ 0:NP1:VP
(F) 0:NP 1:VP
4:S
0:NP 1:VP4:S
(I) TO VB
1:VP
TO VB2:PP
(L) NNP NNP
0:NP
NNP NNP6:NP
(O) 0:NP NN
0:NP
0:NP NN0:NP(c) Top PCFG migrations. Example: migration (D) means a NP?NNPNP rewrite is replaced by NP?NNPNNP,where the new NNP right child spans less than the old NP right child.
Figure 3: We show the prominent migrations that occur during the first 5 iterations of EM for the HMM, DMV, and
PCFG, as recovered by our meta-model. We sort the migrations across each iteration by their expected counts under
the meta-model and show the top 3. Iteration 0 corresponds to the correct outputs. Blue indicates the new iteration,
red indicates the old.
DMV migrations also try to regularize model pre-
dictions, but in a different way?in terms of the
number of arguments. Because the stop probability
is different for adjacent and non-adjacent arguments,
it is statistically much cheaper to generate one argu-
ment rather than two or more. For example, if we
train a DMV on only DT JJ NN, it can fit the data
perfectly by using a chain of single arguments, but
perfect fit is not possible if NN generates both DT
and JJ (which is the desired structure); this explains
migration (J). Indeed, we observed that the variance
of the number of arguments decreases with more EM
iterations (for NN, from 1.38 to 0.41).
In general, low-entropy conditional distributions
are preferred. Migration (H) explains how adverbs
now consistently attach to verbs rather than modals.
After a few iterations, the modal has committed
itself to generating exactly one verb to the right,
which is statistically advantageous because there
must be a verb after a modal, while the adverb is op-
tional. This leaves the verb to generate the adverb.
The PCFG migrations regularize categories in a
manner similar to the HMM, but with the added
complexity of changing bracketing structures. For
example, sentential adverbs are re-analyzed as VP
adverbs (A). Sometimes, multiple migrations ex-
plain the same phenomenon.6 For example, migra-
tions (B) and (C) indicate that PPs that previously
attached to NPs are now raised to the verbal level.
Tree rotation is another common phenomenon, lead-
ing to many left-branching structures (D,G,H). The
migrations that happen during one iteration can also
trigger additional migrations in the next. For exam-
ple, the raising of the PP (B,C) inspires more of the
6We could consolidate these migrations by using larger con-
figurations, but at the risk of decreased generalization.
883
same raising (E). As another example, migration (I)
regularizes TO VB infinitival clauses into PPs, and
this momentum carries over to the next iteration with
even greater force (J).
In summary, the meta-model facilitates our anal-
yses by automatically identifying the broad trends.
We believe that the central idea of modeling the er-
rors of a system is a powerful one which can be used
to analyze a wide range of models, both supervised
and unsupervised.
5 Identifiability error
While approximation error is incurred when likeli-
hood diverges from accuracy, identifiability error is
concerned with the case where likelihood is indiffer-
ent to accuracy.
We say a set of parameters S is identifiable (in
terms of x) if p?(x) 6= p??(x) for every ?, ?? ? S
where ? 6= ??.7 In general, identifiability error is
incurred when the set of maximizers of E log p?(x)
is non-identifiable.8
Label symmetry is perhaps the most familiar ex-
ample of non-identifiability and is intrinsic to mod-
els with hidden labels (HMM and PCFG, but not
DMV). We can permute the hidden labels without
changing the objective function or even the nature
of the solution, so there is no reason to prefer one
permutation over another. While seemingly benign,
this symmetry actually presents a serious challenge
in measuring discrepancy (Section 5.1).
Grenager et al (2005) augments an HMM to al-
low emission from a generic stopword distribution at
any position with probability q. Their model would
definitely not be identifiable if q were a free param-
eter, since we can set q to 0 and just mix in the stop-
word distribution with each of the other emission
distributions to obtain a different parameter setting
yielding the same overall distribution. This is a case
where our notion of desired structure is absent in the
likelihood, and a prior over parameters could help
break ties.
7For our three model families, ? is identifiable in terms of
(x,y), but not in terms of x alone.
8We emphasize that non-identifiability is in terms of x, so
two parameter settings could still induce the same marginal dis-
tribution on x (weak generative capacity) while having different
joint distributions on (x,y) (strong generative capacity). Recall
that discrepancy depends on the latter.
The above non-identifiabilities apply to all param-
eter settings, but another type of non-identifiability
concerns only the maximizers of E log p?(x). Sup-
pose the true data comes from a K-state HMM. If
we attempt to fit an HMM with K + 1 states, we
can split any one of the K states and maintain the
same distribution on x. Or, if we learn a PCFG on
the same HMM data, then we can choose either the
left- or right-branching chain structures, which both
mimic the true HMM equally well.
5.1 Permutation-invariant distance
KL-divergence is a natural measure of discrepancy
between two distributions, but it is somewhat non-
trivial to compute?for our three recursive models, it
requires solving fixed point equations, and becomes
completely intractable in face of label symmetry.
Thus we propose a more manageable alternative:
d?(? || ?
?)
def
=
?
j ?j |?j ? ?
?
j |
?
j ?j
, (1)
where we weight the difference between the j-th
component of the parameter vectors by ?j , the j-
th expected sufficient statistic with respect to p?
(the expected counts computed in the E-step).9 Un-
like KL, our distance d? is only defined on distri-
butions in the model family and is not invariant to
reparametrization. Like KL, d? is asymmetric, with
the first argument holding the status of being the
?true? parameter setting. In our case, the parameters
are conditional probabilities, so 0 ? d?(? || ??) ? 1,
so we can interpret d? as an expected difference be-
tween these probabilities.
Unfortunately, label symmetry can wreak havoc
on our distance measure d?. Suppose we want to
measure the distance between ? and ??. If ?? is
simply ? with the labels permuted, then d?(? || ??)
would be substantial even though the distance ought
to be zero. We define a revised distance to correct
for this by taking the minimum distance over all la-
bel permutations:
D?(? || ?
?) = min
pi
d?(? ||pi(?
?)), (2)
9Without this factor, rarely used components could con-
tribute to the sum as much as frequently used ones, thus, making
the distance overly pessimistic.
884
where pi(??) denotes the parameter setting result-
ing from permuting the labels according to pi. (The
DMV has no label symmetries, so just d? works.)
For mixture models, we can compute D?(? || ??)
efficiently as follows. Note that each term in the
summation of (1) is associated with one of the K
labels. We can form aK?K matrixM , where each
entry Mij is the distance between the parameters in-
volving label i of ? and label j of ??. D?(? || ??) can
then be computed by finding a maximum weighted
bipartite matching on M using the O(K3) Hungar-
ian algorithm (Kuhn, 1955).
For models such as the HMM and PCFG, com-
putingD? is NP-hard, since the summation in d? (1)
contains both first-order terms which depend on one
label (e.g., emission parameters) and higher-order
terms which depend on more than one label (e.g.,
transitions or rewrites). We cannot capture these
problematic higher-order dependencies in M .
However, we can bound D?(? || ??) as follows.
We create M using only first-order terms and find
the best matching (permutation) to obtain a lower
bound D? and an associated permutation pi0 achiev-
ing it. Since D?(? || ??) takes the minimum over all
permutations, d?(? ||pi(??)) is an upper bound for
any pi, in particular for pi = pi0. We then use a local
search procedure that changes pi to further tighten
the upper bound. Let D? denote the final value.
6 Estimation error
Thus far, we have considered approximation and
identifiability errors, which have to do with flaws of
the model. The remaining errors have to do with
how well we can fit the model. To focus on these
errors, we consider the case where the true model is
in our family (p? ? P). To keep the setting as real-
istic as possible, we do supervised learning on real
labeled data to obtain ?? = argmax? E? log p(x,y).
We then throw away our real data and let p? = p?? .
Now we start anew: sample new artificial data from
??, learn a model using this artificial data, and see
how close we get to recovering ??.
In order to compute estimation error, we need to
compare ?? with ??, the global maximizer of the like-
lihood on our generated data. However, we cannot
compute ?? exactly. Let us therefore first consider the
simpler supervised scenario. Here, ??gen has a closed
form solution, so there is no optimization error. Us-
ing our distanceD? (defined in Section 5.1) to quan-
tify estimation error, we see that, for the HMM, ??gen
quickly approaches ?? as we increase the amount of
data (Table 1).
# examples 500 5K 50K 500K
D?(?? || ??gen) 0.003 6.3e-4 2.7e-4 8.5e-5
D?(?? || ??gen) 0.005 0.001 5.2e-4 1.7e-4
D?(?? || ??gen-EM) 0.022 0.018 0.008 0.002
D?(?? || ??gen-EM) 0.049 0.039 0.016 0.004
Table 1: Lower and upper bounds on the distance from
the true ?? for the HMM as we increase the number of
examples.
In the unsupervised case, we use the following
procedure to obtain a surrogate for ??: initialize EM
with the supervised estimate ??gen and run EM for
100 iterations. Let ??gen-EM denote the final param-
eters, which should be representative of ??. Table 1
shows that the estimation error of ??gen-EM is an order
of magnitude higher than that of ??gen, which is to ex-
pected since ??gen-EM does not have access to labeled
data. However, this error can also be driven down
given a moderate number of examples.
7 Optimization error
Finally, we study optimization error, which is the
discrepancy between the global maximizer ?? and
??EM, the result of running EM starting from a uni-
form initialization (plus some small noise). As be-
fore, we cannot compute ??, so we use ??gen-EM as a
surrogate. Also, instead of comparing ??gen-EM and ??
with each other, we compare each of their discrep-
ancies with respect to ??.
Let us first consider optimization error in terms
of prediction error. The first observation is that
there is a gap between the prediction accuracies
of ??gen-EM and ??EM, but this gap shrinks consider-
ably as we increase the number of examples. Fig-
ures 4(a,b,c) support this for all three model fami-
lies: for the HMM, both ??gen-EM and ??EM eventually
achieve around 90% accuracy; for the DMV, 85%.
For the PCFG, ??EM still lags ??gen-EM by 10%, but we
believe that more data can further reduce this gap.
Figure 4(d) shows that these trends are not par-
ticular to artificial data. On real WSJ data, the gap
885
500 5K 50K 500K# examples
0.6
0.7
0.8
0.9
1.0
Acc
ura
cy
500 5K 50K 500K# examples
0.6
0.7
0.8
0.9
1.0
Dir
ect
ed
F 1
500 5K 50K# examples
0.5
0.6
0.8
0.9
1.0
Lab
eled
F 1
1K 3K 10K 40K# examples
0.3
0.4
0.6
0.7
0.8
Acc
ura
cy
(a) HMM (artificial data) (b) DMV (artificial data) (c) PCFG (artificial data) (d) HMM (real data)
500 5K 50K 500K# examples
0.02
0.05
0.07
0.1
0.12
D ?(
?? |
|?) ??gen-EM
??EM (rand 1)
??EM (rand 2)
??EM (rand 3)
20 40 60 80 100iteration
-173.3
-171.4
-169.4
-167.4
-165.5
log
-lik
elih
ood
20 40 60 80 100iteration
0.2
0.4
0.6
0.8
1.0
Acc
ura
cy
Sup. init.
Unif. init.
(e) HMM (artificial data) (f) HMM log-likelihood/accuracy on 500K examples
Figure 4: Compares the performance of ??EM (EM with a uniform initialization) against ??gen-EM (EM initialized with the
supervised estimate) on (a?c) various models, (d) real data. (e) measures distance instead of accuracy and (f) shows a
sample EM run.
between ??gen-EM and ??EM also diminishes for the
HMM. To reaffirm the trends, we also measure dis-
tance D?. Figure 4(e) shows that the distance from
??EM to the true parameters ?? decreases, but the gap
between ??gen-EM and ??EM does not close as deci-
sively as it did for prediction error.
It is quite surprising that by simply running EM
with a neutral initialization, we can accurately learn
a complex model with thousands of parameters. Fig-
ures 4(f,g) show how both likelihood and accuracy,
which both start quite low, improve substantially
over time for the HMM on artificial data.
Carroll and Charniak (1992) report that EM fared
poorly with local optima. We do not claim that there
are no local optima, but only that the likelihood sur-
face that EM is optimizing can become smoother
with more examples. With more examples, there is
less noise in the aggregate statistics, so it might be
easier for EM to pick out the salient patterns.
Srebro et al (2006) made a similar observation
in the context of learning Gaussian mixtures. They
characterized three regimes: one where EM was suc-
cessful in recovering the true clusters (given lots of
data), another where EM failed but the global opti-
mum was successful, and the last where both failed
(without much data).
There is also a rich body of theoretical work on
learning latent-variable models. Specialized algo-
rithms can provably learn certain constrained dis-
crete hidden-variable models, some in terms of weak
generative capacity (Ron et al, 1998; Clark and
Thollard, 2005; Adriaans, 1999), others in term of
strong generative capacity (Dasgupta, 1999; Feld-
man et al, 2005). But with the exception of Das-
gupta and Schulman (2007), there is little theoretical
understanding of EM, let alne on complex model
families such as the HMM, PCFG, and DMV.
8 Conclusion
In recent years, many methods have improved unsu-
pervised induction, but these methods must still deal
with the four types of errors we have identified in
this paper. One of our main contributions of this pa-
per is the idea of using the meta-model to diagnose
the approximation error. Using this tool, we can bet-
ter understand model biases and hopefully correct
for them. We also introduced a method for mea-
suring distances in face of label symmetry and ran
experiments exploring the effectiveness of EM as a
function of the amount of data. Finally, we hope that
setting up the general framework to understand the
errors of unsupervised induction systems will aid the
development of better methods and further analyses.
886
References
P. W. Adriaans. 1999. Learning shallow context-free lan-
guages under simple distributions. Technical report,
Stanford University.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Workshop Notes for Statistically-Based NLP
Techniques, pages 1?13.
A. Clark and F. Thollard. 2005. PAC-learnability
of probabilistic deterministic finite state automata.
JMLR, 5:473?497.
A. Clark. 2001. Unsupervised induction of stochastic
context free grammars with distributional clustering.
In CoNLL.
S. Dasgupta and L. Schulman. 2007. A probabilistic
analysis of EM for mixtures of separated, spherical
Gaussians. JMLR, 8.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
FOCS.
J. Feldman, R. O?Donnell, and R. A. Servedio. 2005.
Learning mixtures of product distributions over dis-
crete domains. In FOCS, pages 501?510.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
ACL.
T. Grenager, D. Klein, and C. D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-based gram-
mar induction. In ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In EMNLP/CoNLL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly, 2:83?97.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
B. Merialdo. 1994. Tagging English text with a prob-
abilistic model. Computational Linguistics, 20:155?
171.
F. Pereira and Y. Shabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In ACL.
D. Ron, Y. Singer, and N. Tishby. 1998. On the learnabil-
ity and usage of acyclic probabilistic finite automata.
Journal of Computer and System Sciences, 56:133?
152.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
N. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In ACL.
N. Srebro, G. Shakhnarovich, and S. Roweis. 2006. An
investigation of computational and informational lim-
its in Gaussian mixture clustering. In ICML, pages
865?872.
887
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25?28,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Complexity of Phrase Alignment Problems
John DeNero and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
{denero, klein}@cs.berkeley.edu
Abstract
Many phrase alignment models operate over
the combinatorial space of bijective phrase
alignments. We prove that finding an optimal
alignment in this space is NP-hard, while com-
puting alignment expectations is #P-hard. On
the other hand, we show that the problem of
finding an optimal alignment can be cast as
an integer linear program, which provides a
simple, declarative approach to Viterbi infer-
ence for phrase alignment models that is em-
pirically quite efficient.
1 Introduction
Learning in phrase alignment models generally re-
quires computing either Viterbi phrase alignments
or expectations of alignment links. For some re-
stricted combinatorial spaces of alignments?those
that arise in ITG-based phrase models (Cherry and
Lin, 2007) or local distortion models (Zens et al,
2004)?inference can be accomplished using poly-
nomial time dynamic programs. However, for more
permissive models such as Marcu and Wong (2002)
and DeNero et al (2006), which operate over the full
space of bijective phrase alignments (see below), no
polynomial time algorithms for exact inference have
been exhibited. Indeed, Marcu and Wong (2002)
conjectures that none exist. In this paper, we show
that Viterbi inference in this full space is NP-hard,
while computing expectations is #P-hard.
On the other hand, we give a compact formula-
tion of Viterbi inference as an integer linear program
(ILP). Using this formulation, exact solutions to the
Viterbi search problem can be found by highly op-
timized, general purpose ILP solvers. While ILP
is of course also NP-hard, we show that, empir-
ically, exact solutions are found very quickly for
most problem instances. In an experiment intended
to illustrate the practicality of the ILP approach, we
show speed and search accuracy results for aligning
phrases under a standard phrase translation model.
2 Phrase Alignment Problems
Rather than focus on a particular model, we describe
four problems that arise in training phrase alignment
models.
2.1 Weighted Sentence Pairs
A sentence pair consists of two word sequences, e
and f. A set of phrases {eij} contains all spans eij
from between-word positions i to j of e. A link is an
aligned pair of phrases, denoted (eij , fkl).1
Let a weighted sentence pair additionally include
a real-valued function ? : {eij}?{fkl} ? R, which
scores links. ?(eij , fkl) can be sentence-specific, for
example encoding the product of a translation model
and a distortion model for (eij , fkl). We impose no
additional restrictions on ? for our analysis.
2.2 Bijective Phrase Alignments
An alignment is a set of links. Given a weighted
sentence pair, we will consider the space of bijective
phrase alignments A: those a ? {eij} ? {fkl} that
use each word token in exactly one link. We first
define the notion of a partition: unionsqiSi = T means Si
are pairwise disjoint and cover T . Then, we can for-
mally define the set of bijective phrase alignments:
A =
?
?
?
a :
?
(eij ,fkl)?a
eij = e ;
?
(eij ,fkl)?a
fkl = f
?
?
?
1As in parsing, the position between each word is assigned
an index, where 0 is to the left of the first word. In this paper,
we assume all phrases have length at least one: j > i and l > k.
25
Both the conditional model of DeNero et al
(2006) and the joint model of Marcu and Wong
(2002) operate in A, as does the phrase-based de-
coding framework of Koehn et al (2003).
2.3 Problem Definitions
For a weighted sentence pair (e, f, ?), let the score
of an alignment be the product of its link scores:
?(a) =
?
(eij ,fkl)?a
?(eij , fkl).
Four related problems involving scored alignments
arise when training phrase alignment models.
OPTIMIZATION, O: Given (e, f, ?), find the high-
est scoring alignment a.
DECISION, D: Given (e, f, ?), decide if there is an
alignment a with ?(a) ? 1.
O arises in the popular Viterbi approximation to
EM (Hard EM) that assumes probability mass is
concentrated at the mode of the posterior distribu-
tion over alignments. D is the corresponding deci-
sion problem for O, useful in analysis.
EXPECTATION, E: Given a weighted sentence pair
(e, f, ?) and indices i, j, k, l, compute
?
a ?(a)
over all a ? A such that (eij , fkl) ? a.
SUM, S: Given (e, f, ?), compute
?
a?A ?(a).
E arises in computing sufficient statistics for
re-estimating phrase translation probabilities (E-
step) when training models. The existence of a
polynomial time algorithm for E implies a poly-
nomial time algorithm for S , because A =
?|e|
j=1
?|f|?1
k=0
?|f|
l=k+1 {a : (e0j , fkl) ? a,a ? A} .
3 Complexity of Inference in A
For the space A of bijective alignments, problems E
and O have long been suspected of being NP-hard,
first asserted but not proven in Marcu and Wong
(2002). We give a novel proof that O is NP-hard,
showing that D is NP-complete by reduction from
SAT, the boolean satisfiability problem. This re-
sult holds despite the fact that the related problem of
finding an optimal matching in a weighted bipartite
graph (the ASSIGNMENT problem) is polynomial-
time solvable using the Hungarian algorithm.
3.1 Reducing Satisfiability to D
A reduction proof of NP-completeness gives a con-
struction by which a known NP-complete problem
can be solved via a newly proposed problem. From a
SAT instance, we construct a weighted sentence pair
for which alignments with positive score correspond
exactly to the SAT solutions. Since SAT is NP-
complete and our construction requires only poly-
nomial time, we conclude that D is NP-complete.2
SAT: Given vectors of boolean variables v = (v)
and propositional clauses3 C = (C), decide
whether there exists an assignment to v that si-
multaneously satisfies each clause in C.
For a SAT instance (v,C), we construct f to con-
tain one word for each clause, and e to contain sev-
eral copies of the literals that appear in those clauses.
? scores only alignments from clauses to literals that
satisfy the clauses. The crux of the construction lies
in ensuring that no variable is assigned both true and
false. The details of constructing such a weighted
sentence pair wsp(v,C) = (e, f, ?), described be-
low, are also depicted in figure 1.
1. f contains a word for each C, followed by an
assignment word for each variable, assign(v).
2. e contains c(`) consecutive words for each lit-
eral `, where c(`) is the number of times that `
appears in the clauses.
Then, we set ?(?, ?) = 0 everywhere except:
3. For all clauses C and each satisfying literal `,
and each one-word phrase e in e containing `,
?(e, fC) = 1. fC is the one-word phrase con-
taining C in f.
4. The assign(v) words in f align to longer phrases
of literals and serve to consistently assign each
variable by using up inconsistent literals. They
also align to unused literals to yield a bijection.
Let ek[`] be the phrase in e containing all literals
` and k negations of `. fassign(v) is the one-word
phrase for assign(v). Then, ?(ek[`], fassign(v)) =
1 for ` ? {v, v?} and all applicable k.
2Note that D is trivially in NP: given an alignment a, it is
easy to determine whether or not ?(a) ? 1.
3A clause is a disjunction of literals. A literal is a bare vari-
able vn or its negation v?n. For instance, v2? v?7? v?9 is a clause.
26
v1
? v
2
? v
3
v?
1
? v
2
? v?
3
v?
1
? v?
2
? v?
3
v?
1
? v?
2
? v
3
v
1
v?
1
v?
2
v?
3
v
3
v
2
v?
1
v?
1
v
2
v?
2
v
3
v?
3
v
1
v?
1
v?
2
v?
3
v
3
v
2
v?
1
v?
1
v
2
v?
2
v
3
v?
3
(a) (b) (c)
assign(v
1
)
assign(v
2
)
assign(v
3
)
(d)
v
1
is true
v
2
is false
v
3
is false
Figure 1: (a) The clauses of an example SAT instance with v = (v
1
, v
2
, v
3
). (b) The weighted sentence pair wsp(v,C)
constructed from the SAT instance. All links that have ? = 1 are marked with a blue horizontal stripe. Stripes in the
last three rows demarcate the alignment options for each assign(vn), which consume all words for some literal. (c) A
bijective alignment with score 1. (d) The corresponding satisfying assignment for the original SAT instance.
Claim 1. If wsp(v,C) has an alignment a with
?(a) ? 1, then (v,C) is satisfiable.
Proof. The score implies that f aligns using all one-
word phrases and ?ai ? a, ?(ai) = 1. By condition
4, each fassign(v) aligns to all v? or all v in e. Then,
assign each v to true if fassign(v) aligns to all v?, and
false otherwise. By condition 3, each C must align
to a satisfying literal, while condition 4 assures that
all available literals are consistent with this assign-
ment to v, which therefore satisfies C.
Claim 2. If (v,C) is satisfiable, then wsp(v,C) has
an alignment a with ?(a) = 1.
Proof. We construct such an alignment a from the
satisfying assignment v. For each C, we choose a
satisfying literal ` consistent with the assignment.
Align fC to the first available ` token in e if the cor-
responding v is true, or the last if v is false. Align
each fassign(v) to all remaining literals for v.
Claims 1 and 2 together show that D is NP-
complete, and therefore that O is NP-hard.
3.2 Reducing Perfect Matching to S
With another construction, we can show that S is #P-
hard, meaning that it is at least as hard as any #P-
complete problem. #P is a class of counting prob-
lems related to NP, and #P-hard problems are NP-
hard as well.
COUNTING PERFECT MATCHINGS, CPM
Given a bipartite graph G with 2n vertices,
count the number of matchings of size n.
For a bipartite graphGwith edge setE = {(vj , vl)},
we construct e and f with n words each, and set
?(ej?1 j , fl?1 l) = 1 and 0 otherwise. The num-
ber of perfect matchings in G is the sum S for
this weighted sentence pair. CPM is #P-complete
(Valiant, 1979), so S (and hence E) is #P-hard.
4 Solving the Optimization Problem
Although O is NP-hard, we present an approach to
solving it using integer linear programming (ILP).
4.1 Previous Inference Approaches
Marcu and Wong (2002) describes an approximation
to O. Given a weighted sentence pair, high scoring
phrases are linked together greedily to reach an ini-
tial alignment. Then, local operators are applied to
hill-climb A in search of the maximum a. This pro-
cedure also approximates E by collecting weighted
counts as the space is traversed.
DeNero et al (2006) instead proposes an
exponential-time dynamic program to systemati-
cally explore A, which can in principle solve either
O or E. In practice, however, the space of align-
ments has to be pruned severely using word align-
ments to control the running time of EM.
Notably, neither of these inference approaches of-
fers any test to know if the optimal alignment is ever
found. Furthermore, they both require small data
sets due to computational expense.
4.2 Alignment via an Integer Program
We cast O as an ILP problem, for which many opti-
mization techniques are well known. First, we in-
27
troduce binary indicator variables ai,j,k,l denoting
whether (eij , fkl) ? a. Furthermore, we introduce
binary indicators ei,j and fk,l that denote whether
some (eij , ?) or (?, fkl) appears in a, respectively. Fi-
nally, we represent the weight function ? as a weight
vector in the program: wi,j,k,l = log ?(eij , fkl).
Now, we can express an integer program that,
when optimized, will yield the optimal alignment of
our weighted sentence pair.
max
?
i,j,k,l
wi,j,k,l ? ai,j,k,l
s.t.
?
i,j:i<x?j
ei,j = 1 ?x : 1 ? x ? |e| (1)
?
k,l:k<y?l
fk,l = 1 ?y : 1 ? y ? |f | (2)
ei,j =
?
k,l
ai,j,k,l ?i, j (3)
fk,l =
?
i,j
ai,j,k,l ?k, l (4)
with the following constraints on index variables:
0 ? i < |e|, 0 < j ? |e|, i < j
0 ? k < |f |, 0 < l ? |f |, k < l .
The objective function is log ?(a) for a implied
by {ai,j,k,l = 1}. Constraint equation 1 ensures that
the English phrases form a partition of e ? each word
in e appears in exactly one phrase ? as does equa-
tion 2 for f. Constraint equation 3 ensures that each
phrase in the chosen partition of e appears in exactly
one link, and that phrases not in the partition are not
aligned (and likewise constraint 4 for f).
5 Applications
The need to find an optimal phrase alignment for a
weighted sentence pair arises in at least two appli-
cations. First, a generative phrase alignment model
can be trained with Viterbi EM by finding optimal
phrase alignments of a training corpus (approximate
E-step), then re-estimating phrase translation param-
eters from those alignments (M-step).
Second, this is an algorithm for forced decoding:
finding the optimal phrase-based derivation of a par-
ticular target sentence. Forced decoding arises in
online discriminative training, where model updates
are made toward the most likely derivation of a gold
translation (Liang et al, 2006).
Sentences per hour on a four-core server 20,000
Frequency of optimal solutions found 93.4%
Frequency of -optimal solutions found 99.2%
Table 1: The solver, tuned for speed, regularly reports
solutions that are within 10?5 of optimal.
Using an off-the-shelf ILP solver,4 we were able
to quickly and reliably find the globally optimal
phrase alignment under ?(eij , fkl) derived from the
Moses pipeline (Koehn et al, 2007).5 Table 1 shows
that finding the optimal phrase alignment is accurate
and efficient.6 Hence, this simple search technique
effectively addresses the intractability challenges in-
herent in evaluating new phrase alignment ideas.
References
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In NAACL-HLT Workshop on Syntax and Structure in
Statistical Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Leslie G. Valiant. 1979. The complexity of computing
the permanent. In Theoretical Computer Science 8.
Richard Zens, Hermann Ney, Taro Watanabeand, and
E. Sumita. 2004. Reordering constraints for phrase
based statistical machine translation. In Coling.
4We used Mosek: www.mosek.com.
5?(eij , fkl) was estimated using the relative frequency of
phrases extracted by the default Moses training script. We eval-
uated on English-Spanish Europarl, sentences up to length 25.
6ILP solvers include many parameters that trade off speed
for accuracy. Substantial speed gains also follow from explicitly
pruning the values of ILP variables based on prior information.
28
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 91?99,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning Semantic Correspondences with Less Supervision
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
A central problem in grounded language acqui-
sition is learning the correspondences between a
rich world state and a stream of text which refer-
ences that world state. To deal with the high de-
gree of ambiguity present in this setting, we present
a generative model that simultaneously segments
the text into utterances and maps each utterance
to a meaning representation grounded in the world
state. We show that our model generalizes across
three domains of increasing difficulty?Robocup
sportscasting, weather forecasts (a new domain),
and NFL recaps.
1 Introduction
Recent work in learning semantics has focused
on mapping sentences to meaning representa-
tions (e.g., some logical form) given aligned sen-
tence/meaning pairs as training data (Ge and
Mooney, 2005; Zettlemoyer and Collins, 2005;
Zettlemoyer and Collins, 2007; Lu et al, 2008).
However, this degree of supervision is unrealistic
for modeling human language acquisition and can
be costly to obtain for building large-scale, broad-
coverage language understanding systems.
A more flexible direction is grounded language
acquisition: learning the meaning of sentences
in the context of an observed world state. The
grounded approach has gained interest in various
disciplines (Siskind, 1996; Yu and Ballard, 2004;
Feldman and Narayanan, 2004; Gorniak and Roy,
2007). Some recent work in the NLP commu-
nity has also moved in this direction by relaxing
the amount of supervision to the setting where
each sentence is paired with a small set of can-
didate meanings (Kate and Mooney, 2007; Chen
and Mooney, 2008).
The goal of this paper is to reduce the amount
of supervision even further. We assume that we are
given a world state represented by a set of records
along with a text, an unsegmented sequence of
words. For example, in the weather forecast do-
main (Section 2.2), the text is the weather report,
and the records provide a structured representation
of the temperature, sky conditions, etc.
In this less restricted data setting, we must re-
solve multiple ambiguities: (1) the segmentation
of the text into utterances; (2) the identification of
relevant facts, i.e., the choice of records and as-
pects of those records; and (3) the alignment of ut-
terances to facts (facts are the meaning represen-
tations of the utterances). Furthermore, in some
of our examples, much of the world state is not
referenced at all in the text, and, conversely, the
text references things which are not represented in
our world state. This increased amount of ambigu-
ity and noise presents serious challenges for learn-
ing. To cope with these challenges, we propose a
probabilistic generative model that treats text seg-
mentation, fact identification, and alignment in a
single unified framework. The parameters of this
hierarchical hidden semi-Markov model can be es-
timated efficiently using EM.
We tested our model on the task of aligning
text to records in three different domains. The
first domain is Robocup sportscasting (Chen and
Mooney, 2008). Their best approach (KRISPER)
obtains 67% F1; our method achieves 76.5%. This
domain is simplified in that the segmentation is
known. The second domain is weather forecasts,
for which we created a new dataset. Here, the
full complexity of joint segmentation and align-
ment arises. Nonetheless, we were able to obtain
reasonable results on this task. The third domain
we considered is NFL recaps (Barzilay and Lap-
ata, 2005; Snyder and Barzilay, 2007). The lan-
guage used in this domain is richer by orders of
magnitude, and much of it does not reference the
world state. Nonetheless, taking the first unsuper-
vised approach to this problem, we were able to
make substantial progress: We achieve an F1 of
53.2%, which closes over half of the gap between
a heuristic baseline (26%) and supervised systems
(68%?80%).
91
Dataset # scenarios |w| |T | |s| |A|
Robocup 1919 5.7 9 2.4 0.8
Weather 22146 28.7 12 36.0 5.8
NFL 78 969.0 44 329.0 24.3
Table 1: Statistics for the three datasets. We report average
values across all scenarios in the dataset: |w| is the number of
words in the text, |T | is the number of record types, |s| is the
number of records, and |A| is the number of gold alignments.
2 Domains and Datasets
Our goal is to learn the correspondence between a
text w and the world state s it describes. We use
the term scenario to refer to such a (w, s) pair.
The text is simply a sequence of words w =
(w1, . . . , w|w|). We represent the world state s as
a set of records, where each record r ? s is de-
scribed by a record type r.t ? T and a tuple of
field values r.v = (r.v1, . . . , r.vm).1 For exam-
ple, temperature is a record type in the weather
domain, and it has four fields: time, min, mean,
and max.
The record type r.t ? T specifies the field type
r.tf ? {INT, STR, CAT} of each field value r.vf ,
f = 1, . . . ,m. There are three possible field
types?integer (INT), string (STR), and categori-
cal (CAT)?which are assumed to be known and
fixed. Integer fields represent numeric properties
of the world such as temperature, string fields rep-
resent surface-level identifiers such as names of
people, and categorical fields represent discrete
concepts such as score types in football (touch-
down, field goal, and safety). The field type de-
termines the way we expect the field value to be
rendered in words: integer fields can be numeri-
cally perturbed, string fields can be spliced, and
categorical fields are represented by open-ended
word distributions, which are to be learned. See
Section 3.3 for details.
2.1 Robocup Sportscasting
In this domain, a Robocup simulator generates the
state of a soccer game, which is represented by
a set of event records. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a pass-
ing event; this type of record has two fields: arg1
(the actor) and arg2 (the recipient). As the game is
progressing, humans interject commentaries about
notable events in the game, e.g., pink1 passes back
to pink5 near the middle of the field. All of the
1To simplify notation, we assume that each record has m
fields, though in practice, m depends on the record type r.t.
fields in this domain are categorical, which means
there is no a priori association between the field
value pink1 and the word pink1. This degree of
flexibility is desirable because pink1 is sometimes
referred to as pink goalie, a mapping which does
not arise from string operations but must instead
be learned.
We used the dataset created by Chen and
Mooney (2008), which contains 1919 scenarios
from the 2001?2004 Robocup finals. Each sce-
nario consists of a single sentence representing a
fragment of a commentary on the game, paired
with a set of candidate records. In the annotation,
each sentence corresponds to at most one record
(possibly one not in the candidate set, in which
case we automatically get that sentence wrong).
See Figure 1(a) for an example and Table 1 for
summary statistics on the dataset.
2.2 Weather Forecasts
In this domain, the world state contains de-
tailed information about a local weather forecast
and the text is a short forecast report (see Fig-
ure 1(b) for an example). To create the dataset,
we collected local weather forecasts for 3,753
cities in the US (those with population at least
10,000) over three days (February 7?9, 2009) from
www.weather.gov. For each city and date, we
created two scenarios, one for the day forecast and
one for the night forecast. The forecasts consist of
hour-by-hour measurements of temperature, wind
speed, sky cover, chance of rain, etc., which rep-
resent the underlying world state.
This world state is summarized by records
which aggregate measurements over selected time
intervals. For example, one of the records states
the minimum, average, and maximum tempera-
ture from 5pm to 6am. This aggregation pro-
cess produced 22,146 scenarios, each containing
|s| = 36 multi-field records. There are 12 record
types, each consisting of only integer and categor-
ical fields.
To annotate the data, we split the text by punc-
tuation into lines and labeled each line with the
records to which the line refers. These lines are
used only for evaluation and are not part of the
model (see Section 5.1 for further discussion).
The weather domain is more complex than the
Robocup domain in several ways: The text w is
longer, there are more candidate records, and most
notably, w references multiple records (5.8 on av-
92
xbadPass(arg1=pink11,arg2=purple3)ballstopped()ballstopped()kick(arg1=pink11)turnover(arg1=pink11,arg2=purple3)
s
w:pink11 makes a bad pass and was picked off by purple3
(a) Robocup sportscasting
. . .rainChance(time=26-30,mode=Def)temperature(time=17-30,min=43,mean=44,max=47)windDir(time=17-30,mode=SE)windSpeed(time=17-30,min=11,mean=12,max=14,mode=10-20)precipPotential(time=17-30,min=5,mean=26,max=75)rainChance(time=17-30,mode=--)windChill(time=17-30,min=37,mean=38,max=42)skyCover(time=17-30,mode=50-75)rainChance(time=21-30,mode=--). . .
s
w:Occasional rain after 3am .Low around 43 .South wind between 11 and 14 mph .Chance of precipitation is 80 % .New rainfall amounts between aquarter and half of an inch possible .
(b) Weather forecasts
. . .rushing(entity=richie anderson,att=5,yds=37,avg=7.4,lg=16,td=0)receiving(entity=richie anderson,rec=4,yds=46,avg=11.5,lg=20,td=0)play(quarter=1,description=richie anderson ( dal ) rushed left side for 13 yards .)defense(entity=eric ogbogu,tot=4,solo=3,ast=1,sck=0,yds=0). . .
s w:. . .Former Jets player Richie Andersonfinished with 37 yards on 5 carriesplus 4 receptions for 46 yards .. . .
(c) NFL recaps
Figure 1: An example of a scenario for each of the three domains. Each scenario consists of a candidate set of records s and a
text w. Each record is specified by a record type (e.g., badPass) and a set of field values. Integer values are in Roman, string
values are in italics, and categorical values are in typewriter. The gold alignments are shown.
erage), so the segmentation of w is unknown. See
Table 1 for a comparison of the two datasets.
2.3 NFL Recaps
In this domain, each scenario represents a single
NFL football game (see Figure 1(c) for an exam-
ple). The world state (the things that happened
during the game) is represented by database tables,
e.g., scoring summary, team comparison, drive
chart, play-by-play, etc. Each record is a database
entry, for instance, the receiving statistics for a cer-
tain player. The text is the recap of the game?
an article summarizing the game highlights. The
dataset we used was collected by Barzilay and La-
pata (2005). The data includes 466 games during
the 2003?2004 NFL season. 78 of these games
were annotated by Snyder and Barzilay (2007),
who aligned each sentence to a set of records.
This domain is by far the most complicated of
the three. Many records corresponding to inconse-
quential game statistics are not mentioned. Con-
versely, the text contains many general remarks
(e.g., it was just that type of game) which are
not present in any of the records. Furthermore,
the complexity of the language used in the re-
cap is far greater than what we can represent us-
ing our simple model. Fortunately, most of the
fields are integer fields or string fields (generally
names or brief descriptions), which provide im-
portant anchor points for learning the correspon-
dences. Nonetheless, the same names and num-
bers occur in multiple records, so there is still un-
certainty about which record is referenced by a
given sentence.
3 Generative Model
To learn the correspondence between a text w and
a world state s, we propose a generative model
p(w | s) with latent variables specifying this cor-
respondence.
Our model combines segmentation with align-
ment. The segmentation aspect of our model is
similar to that of Grenager et al (2005) and Eisen-
stein and Barzilay (2008), but in those two models,
the segments are clustered into topics rather than
grounded to a world state. The alignment aspect
of our model is similar to the HMM model for
word alignment (Ney and Vogel, 1996). DeNero
et al (2008) perform joint segmentation and word
alignment for machine translation, but the nature
of that task is different from ours.
The model is defined by a generative process,
93
which proceeds in three stages (Figure 2 shows the
corresponding graphical model):
1. Record choice: choose a sequence of records
r = (r1, . . . , r|r|) to describe, where each
ri ? s.
2. Field choice: for each chosen record ri, se-
lect a sequence of fields fi = (fi1, . . . , fi|fi|),
where each fij ? {1, . . . ,m}.
3. Word choice: for each chosen field fij ,
choose a number cij > 0 and generate a se-
quence of cij words.
The observed text w is the terminal yield formed
by concatenating the sequences of words of all
fields generated; note that the segmentation of w
provided by c = {cij} is latent. Think of the
words spanned by a record as constituting an ut-
terance with a meaning representation given by the
record and subset of fields chosen.
Formally, our probabilistic model places a dis-
tribution over (r, f , c,w) and factorizes according
to the three stages as follows:
p(r, f , c,w | s) = p(r | s)p(f | r)p(c,w | r, f , s)
The following three sections describe each of
these stages in more detail.
3.1 Record Choice Model
The record choice model specifies a distribu-
tion over an ordered sequence of records r =
(r1, . . . , r|r|), where each record ri ? s. This
model is intended to capture two types of regu-
larities in the discourse structure of language. The
first is salience, that is, some record types are sim-
ply more prominent than others. For example, in
the NFL domain, 70% of scoring records are men-
tioned whereas only 1% of punting records are
mentioned. The second is the idea of local co-
herence, that is, the order in which one mentions
records tend to follow certain patterns. For ex-
ample, in the weather domain, the sky conditions
are generally mentioned first, followed by temper-
ature, and then wind speed.
To capture these two phenomena, we define a
Markov model on the record types (and given the
record type, a record is chosen uniformly from the
set of records with that type):
p(r | s) =
|r|?
i=1
p(ri.t | ri?1.t)
1
|s(ri.t)|
, (1)
where s(t)
def
= {r ? s : r.t = t} and r0.t is
a dedicated START record type.2 We also model
the transition of the final record type to a desig-
nated STOP record type in order to capture regu-
larities about the types of records which are de-
scribed last. More sophisticated models of coher-
ence could also be employed here (Barzilay and
Lapata, 2008).
We assume that s includes a special null record
whose type is NULL, responsible for generating
parts of our text which do not refer to any real
records.
3.2 Field Choice Model
Each record type t ? T has a separate field choice
model, which specifies a distribution over a se-
quence of fields. We want to capture salience
and coherence at the field level like we did at the
record level. For instance, in the weather domain,
the minimum and maximum fields of a tempera-
ture record are mentioned whereas the average is
not. In the Robocup domain, the actor typically
precedes the recipient in passing event records.
Formally, we have a Markov model over the
fields:3
p(f | r) =
|r|?
i=1
|fj |?
j=1
p(fij | fi(j?1)). (2)
Each record type has a dedicated null field with
its own multinomial distribution over words, in-
tended to model words which refer to that record
type in general (e.g., the word passes for passing
records). We also model transitions into the first
field and transitions out of the final field with spe-
cial START and STOP fields. This Markov structure
allows us to capture a few elements of rudimentary
syntax.
3.3 Word Choice Model
We arrive at the final component of our model,
which governs how the information about a par-
ticular field of a record is rendered into words. For
each field fij , we generate the number of words cij
from a uniform distribution over {1, 2, . . . , Cmax},
where Cmax is set larger than the length of the
longest text we expect to see. Conditioned on
2We constrain our inference to only consider record types
t that occur in s, i.e., s(t) 6= ?.
3During inference, we prohibit consecutive fields from re-
peating.
94
s
r
f
c,w
s
r1
f11
w1 ? ? ? w
c11
? ? ?
? ? ? ri
fi1
w ? ? ? w
ci1
? ? ? fi|fi|
w ? ? ? w
ci|fi|
? ? ? rn
? ? ? fn|fn|
w ? ? ? w|w|
cn|fn|
Record choice
Field choice
Word choice
Figure 2: Graphical model representing the generative model. First, records are chosen and ordered from the set s. Then fields
are chosen for each record. Finally, words are chosen for each field. The world state s and the words w are observed, while
(r, f , c) are latent variables to be inferred (note that the number of latent variables itself is unknown).
the fields f , the words w are generated indepen-
dently:4
p(w | r, f , c, s) =
|w|?
k=1
pw(wk | r(k).tf(k), r(k).vf(k)),
where r(k) and f(k) are the record and field re-
sponsible for generating word wk, as determined
by the segmentation c. The word choice model
pw(w | t, v) specifies a distribution over words
given the field type t and field value v. This distri-
bution is a mixture of a global backoff distribution
over words and a field-specific distribution which
depends on the field type t.
Although we designed our word choice model
to be relatively general, it is undoubtedly influ-
enced by the three domains. However, we can
readily extend or replace it with an alternative if
desired; this modularity is one principal benefit of
probabilistic modeling.
Integer Fields (t = INT) For integer fields, we
want to capture the intuition that a numeric quan-
tity v is rendered in the text as a word which
is possibly some other numerical value w due to
stylistic factors. Sometimes the exact value v is
used (e.g., in reporting football statistics). Other
times, it might be customary to round v (e.g., wind
speeds are typically rounded to a multiple of 5).
In other cases, there might just be some unex-
plained error, where w deviates from v by some
noise + = w ? v > 0 or ? = v ? w > 0. We
model + and ? as geometric distributions.5 In
4While a more sophisticated model of words would be
useful if we intended to use this model for natural language
generation, the false independence assumptions present here
matter less for the task of learning the semantic correspon-
dences because we always condition on w.
5Specifically, p(+;?+) = (1 ? ?+)+?1?+, where
?+ is a field-specific parameter; p(?;??) is defined analo-
gously.
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.5
p w(
w|
v=
13)
8 9 10 11 12 13 14 15 16 17 18w
0.1
0.2
0.3
0.4
0.6
p w(
w|
v=
13)
(a) temperature.min (b) windSpeed.min
Figure 3: Two integer field types in the weather domain for
which we learn different distributions over the ways in which
a value v might appear in the text as a word w. Suppose the
record field value is v = 13. Both distributions are centered
around v, as is to be expected, but the two distributions have
different shapes: For temperature.min, almost all the mass
is to the left, suggesting that forecasters tend to report con-
servative lower bounds. For the wind speed, the mass is con-
centrated on 13 and 15, suggesting that forecasters frequently
round wind speeds to multiples of 5.
summary, we allow six possible ways of generat-
ing the word w given v:
v dve5 bvc5 round5(v) v ? ? v + +
Separate probabilities for choosing among these
possibilities are learned for each field type (see
Figure 3 for an example).
String Fields (t = STR) Strings fields are in-
tended to represent values which we expect to be
realized in the text via a simple surface-level trans-
formation. For example, a name field with value
v = Moe Williams is sometimes referenced in the
text by just Williams. We used a simple generic
model of rendering string fields: Let w be a word
chosen uniformly from those in v.
Categorical Fields (t = CAT) Unlike string
fields, categorical fields are not tied down to any
lexical representation; in fact, the identities of the
categorical field values are irrelevant. For each
categorical field f and possible value v, we have a
95
v pw(w | t, v)
0-25 , clear mostly sunny
25-50 partly , cloudy increasing
50-75 mostly cloudy , partly
75-100 of inch an possible new a rainfall
Table 2: Highest probability words for the categorical field
skyCover.mode in the weather domain. It is interesting to
note that skyCover=75-100 is so highly correlated with rain
that the model learns to connect an overcast sky in the world
to the indication of rain in the text.
separate multinomial distribution over words from
which w is drawn. An example of a categori-
cal field is skyCover.mode in the weather domain,
which has four values: 0-25, 25-50, 50-75,
and 75-100. Table 2 shows the top words for
each of these field values learned by our model.
4 Learning and Inference
Our learning and inference methodology is a fairly
conventional application of Expectation Maxi-
mization (EM) and dynamic programming. The
input is a set of scenarios D, each of which is a
text w paired with a world state s. We maximize
the marginal likelihood of our data, summing out
the latent variables (r, f , c):
max
?
?
(w,s)?D
?
r,f ,c
p(r, f , c,w | s; ?), (3)
where ? are the parameters of the model (all the
multinomial probabilities). We use the EM algo-
rithm to maximize (3), which alternates between
the E-step and the M-step. In the E-step, we
compute expected counts according to the poste-
rior p(r, f , c | w, s; ?). In the M-step, we op-
timize the parameters ? by normalizing the ex-
pected counts computed in the E-step. In our ex-
periments, we initialized EM with a uniform dis-
tribution for each multinomial and applied add-0.1
smoothing to each multinomial in the M-step.
As with most complex discrete models, the bulk
of the work is in computing expected counts under
p(r, f , c | w, s; ?). Formally, our model is a hier-
archical hidden semi-Markov model conditioned
on s. Inference in the E-step can be done using a
dynamic program similar to the inside-outside al-
gorithm.
5 Experiments
Two important aspects of our model are the seg-
mentation of the text and the modeling of the co-
herence structure at both the record and field lev-
els. To quantify the benefits of incorporating these
two aspects, we compare our full model with two
simpler variants.
? Model 1 (no model of segmentation or co-
herence): Each record is chosen indepen-
dently; each record generates one field, and
each field generates one word. This model is
similar in spirit to IBM model 1 (Brown et
al., 1993).
? Model 2 (models segmentation but not coher-
ence): Records and fields are still generated
independently, but each field can now gener-
ate multiple words.
? Model 3 (our full model of segmentation and
coherence): Records and fields are generated
according to the Markov chains described in
Section 3.
5.1 Evaluation
In the annotated data, each text w has been di-
vided into a set of lines. These lines correspond
to clauses in the weather domain and sentences in
the Robocup and NFL domains. Each line is an-
notated with a (possibly empty) set of records. Let
A be the gold set of these line-record alignment
pairs.
To evaluate a learned model, we com-
pute the Viterbi segmentation and alignment
(argmaxr,f ,c p(r, f , c | w, s)). We produce a pre-
dicted set of line-record pairsA? by aligning a line
to a record ri if the span of (the utterance corre-
sponding to) ri overlaps the line. The reason we
evaluate indirectly using lines rather than using ut-
terances is that it is difficult to annotate the seg-
mentation of text into utterances in a simple and
consistent manner.
We compute standard precision, recall, and F1
of A? with respect to A. Unless otherwise spec-
ified, performance is reported on all scenarios,
which were also used for training. However, we
did not tune any hyperparameters, but rather used
generic values which worked well enough across
all three domains.
5.2 Robocup Sportscasting
We ran 10 iterations of EM on Models 1?3. Ta-
ble 3 shows that performance improves with in-
creased model sophistication. We also compare
96
Method Precision Recall F1
Model 1 78.6 61.9 69.3
Model 2 74.1 84.1 78.8
Model 3 77.3 84.0 80.5
Table 3: Alignment results on the Robocup sportscasting
dataset.
Method F1
Random baseline 48.0
Chen and Mooney (2008) 67.0
Model 3 75.7
Table 4: F1 scores based on the 4-fold cross-validation
scheme in Chen and Mooney (2008).
our model to the results of Chen and Mooney
(2008) in Table 4.
Figure 4 provides a closer look at the predic-
tions made by each of our three models for a par-
ticular example. Model 1 easily mistakes pink10
for the recipient of a pass record because decisions
are made independently for each word. Model 2
chooses the correct record, but having no model
of the field structure inside a record, it proposes
an incorrect field segmentation (although our eval-
uation is insensitive to this). Equipped with the
ability to prefer a coherent field sequence, Model
3 fixes these errors.
Many of the remaining errors are due to the
garbage collection phenomenon familiar from
word alignment models (Moore, 2004; Liang et
al., 2006). For example, the ballstopped record
occurs frequently but is never mentioned in the
text. At the same time, there is a correlation be-
tween ballstopped and utterances such as pink2
holds onto the ball, which are not aligned to any
record in the annotation. As a result, our model
incorrectly chooses to align the two.
5.3 Weather Forecasts
For the weather domain, staged training was nec-
essary to get good results. For Model 1, we ran
15 iterations of EM. For Model 2, we ran 5 it-
erations of EM on Model 1, followed by 10 it-
erations on Model 2. For Model 3, we ran 5 it-
erations of Model 1, 5 iterations of a simplified
variant of Model 3 where records were chosen in-
dependently, and finally, 5 iterations of Model 3.
When going from one model to another, we used
the final posterior distributions of the former to ini-
Method Precision Recall F1
Model 1 49.9 75.1 60.0
Model 2 67.3 70.4 68.8
Model 3 76.3 73.8 75.0
Table 5: Alignment results on the weather forecast dataset.
[Model 1] r:f :w:
passarg2=pink10pink10 turns the ball over to purple5
[Model 2] r:f :w:
turnoverxpink10 turns the ball over arg2=purple5to purple5
[Model 3] r:f :w:
turnoverarg1=pink10pink10 xturns the ball over to arg2=purple5purple5
Figure 4: An example of predictions made by each of the
three models on the Robocup dataset.
tialize the parameters of the latter.6 We also pro-
hibited utterances in Models 2 and 3 from crossing
punctuation during inference.
Table 5 shows that performance improves sub-
stantially in the more sophisticated models, the
gains being greater than in the Robocup domain.
Figure 5 shows the predictions of the three models
on an example. Model 1 is only able to form iso-
lated (but not completely inaccurate) associations.
By modeling segmentation, Model 2 accounts for
the intermediate words, but errors are still made
due to the lack of Markov structure. Model 3
remedies this. However, unexpected structures
are sometimes learned. For example, the temper-
ature.time=6-21 field indicates daytime, which
happens to be perfectly correlated with the word
high, although high intuitively should be associ-
ated with the temperature.max field. In these cases
of high correlation (Table 2 provides another ex-
ample), it is very difficult to recover the proper
alignment without additional supervision.
5.4 NFL Recaps
In order to scale up our models to the NFL do-
main, we first pruned for each sentence the records
which have either no numerical values (e.g., 23,
23-10, 2/4) nor name-like words (e.g., those that
appear only capitalized in the text) in common.
This eliminated all but 1.5% of the record can-
didates per sentence, while maintaining an ora-
6It is interesting to note that this type of staged training
is evocative of language acquisition in children: lexical asso-
ciations are formed (Model 1) before higher-level discourse
structure is learned (Model 3).
97
[Model 1] r:f :w: cloudy , with a
windDirtime=6-21high near
temperaturemax=6363 .
windDirmode=SEeast southeast wind between
windSpeedmin=55 and
windSpeedmean=911 mph .
[Model 2] r:f :w:
rainChancemode=?cloudy ,
temperaturexwith a time=6-21high near max=6363 .
windDirmode=SEeast southeast wind xbetween 5 and
windSpeedmean=911 mph .
[Model 3] r:f :w:
skyCoverxcloudy ,
temperaturexwith a time=6-21high near max=6363 mean=56.
windDirmode=SEeast southeast xwind between
windSpeedmin=55 max=13and 11 xmph .
Figure 5: An example of predictions made by each of the three models on the weather dataset.
cle alignment F1 score of 88.7. Guessing a single
random record for each sentence yields an F1 of
12.0. A reasonable heuristic which uses weighted
number- and string-matching achieves 26.7.
Due to the much greater complexity of this do-
main, Model 2 was easily misled as it tried with-
out success to find a coherent segmentation of the
fields. We therefore created a variant, Model 2?,
where we constrained each field to generate ex-
actly one word. To train Model 2?, we ran 5 it-
erations of EM where each sentence is assumed
to have exactly one record, followed by 5 itera-
tions where the constraint was relaxed to also al-
low record boundaries at punctuation and the word
and. We did not experiment with Model 3 since
the discourse structure on records in this domain is
not at all governed by a simple Markov model on
record types?indeed, most regions do not refer to
any records at all. We also fixed the backoff prob-
ability to 0.1 instead of learning it and enforced
zero numerical deviation on integer field values.
Model 2? achieved an F1 of 39.9, an improve-
ment over Model 1, which attained 32.8. Inspec-
tion of the errors revealed the following problem:
The alignment task requires us to sometimes align
a sentence to multiple redundant records (e.g.,
play and score) referenced by the same part of the
text. However, our model generates each part of
text from only one record, and thus it can only al-
low an alignment to one record.7 To cope with this
incompatibility between the data and our notion of
semantics, we used the following solution: We di-
vided the records into three groups by type: play,
score, and other. Each group has a copy of the
model, but we enforce that they share the same
segmentation. We also introduce a potential that
couples the presence or absence of records across
7The model can align a sentence to multiple records pro-
vided that the records are referenced by non-overlapping
parts of the text.
Method Precision Recall F1
Random (with pruning) 13.1 11.0 12.0
Baseline 29.2 24.6 26.7
Model 1 25.2 46.9 32.8
Model 2? 43.4 37.0 39.9
Model 2? (with groups) 46.5 62.1 53.2
Graph matching (sup.) 73.4 64.5 68.6
Multilabel global (sup.) 87.3 74.5 80.3
Table 6: Alignment results on the NFL dataset. Graph match-
ing and multilabel are supervised results reported in Snyder
and Barzilay (2007).9
groups on the same segment to capture regular co-
occurrences between redundant records.
Table 6 shows our results. With groups, we
achieve an F1 of 53.2. Though we still trail su-
pervised techniques, which attain numbers in the
68?80 range, we have made substantial progress
over our baseline using an unsupervised method.
Furthermore, our model provides a more detailed
analysis of the correspondence between the world
state and text, rather than just producing a single
alignment decision. Most of the remaining errors
made by our model are due to a lack of calibra-
tion. Sometimes, our false positives are close calls
where a sentence indirectly references a record,
and our model predicts the alignment whereas the
annotation standard does not. We believe that fur-
ther progress is possible with a richer model.
6 Conclusion
We have presented a generative model of corre-
spondences between a world state and an unseg-
mented stream of text. By having a joint model
of salience, coherence, and segmentation, as well
as a detailed rendering of the values in the world
state into words in the text, we are able to cope
with the increased ambiguity that arises in this new
data setting, successfully pushing the limits of un-
supervision.
98
References
R. Barzilay and M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Human Language
Technology and Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 331?338, Vancouver,
B.C.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Linguis-
tics, 34:1?34.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19:263?311.
D. L. Chen and R. J. Mooney. 2008. Learning to sportscast:
A test of grounded language acquisition. In International
Conference on Machine Learning (ICML), pages 128?
135. Omnipress.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Empirical Methods in Natural Language Processing
(EMNLP), pages 314?323, Honolulu, HI.
J. Eisenstein and R. Barzilay. 2008. Bayesian unsupervised
topic segmentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 334?343.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a
neural theory of language. Brain and Language, 89:385?
392.
R. Ge and R. J. Mooney. 2005. A statistical semantic parser
that integrates syntax and semantics. In Computational
Natural Language Learning (CoNLL), pages 9?16, Ann
Arbor, Michigan.
P. Gorniak and D. Roy. 2007. Situated language understand-
ing as filtering perceived affordances. Cognitive Science,
31:197?231.
T. Grenager, D. Klein, and C. D. Manning. 2005. Unsu-
pervised learning of field segmentation models for infor-
mation extraction. In Association for Computational Lin-
guistics (ACL), pages 371?378, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.
R. J. Kate and R. J. Mooney. 2007. Learning language se-
mantics from ambiguous supervision. In Association for
the Advancement of Artificial Intelligence (AAAI), pages
895?900, Cambridge, MA. MIT Press.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agree-
ment. In North American Association for Computational
Linguistics (NAACL), pages 104?111, New York City. As-
sociation for Computational Linguistics.
W. Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer. 2008. A
generative model for parsing natural language to meaning
representations. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 783?792.
R. C. Moore. 2004. Improving IBM word alignment model
1. In Association for Computational Linguistics (ACL),
pages 518?525, Barcelona, Spain. Association for Com-
putational Linguistics.
H. Ney and S. Vogel. 1996. HMM-based word align-
ment in statistical translation. In International Conference
on Computational Linguistics (COLING), pages 836?841.
Association for Computational Linguistics.
J. M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning map-
pings. Cognition, 61:1?38.
B. Snyder and R. Barzilay. 2007. Database-text alignment
via structured multilabel classification. In International
Joint Conference on Artificial Intelligence (IJCAI), pages
1713?1718, Hyderabad, India.
C. Yu and D. H. Ballard. 2004. On the integration of ground-
ing language and learning objects. In Association for the
Advancement of Artificial Intelligence (AAAI), pages 488?
493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification with
probabilistic categorial grammars. In Uncertainty in Arti-
ficial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP/CoNLL), pages 678?687.
99
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923?931,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Better Word Alignments with Supervised ITG Models
Aria Haghighi, John Blitzer, John DeNero and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,blitzer,denero,klein }@cs.berkeley.edu
Abstract
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
1 Introduction
Inversion transduction grammar (ITG) con-
straints (Wu, 1997) provide coherent structural
constraints on the relationship between a sentence
and its translation. ITG has been extensively
explored in unsupervised statistical word align-
ment (Zhang and Gildea, 2005; Cherry and
Lin, 2007a; Zhang et al, 2008) and machine
translation decoding (Cherry and Lin, 2007b;
Petrov et al, 2008). In this work, we investigate
large-scale, discriminative ITG word alignment.
Past work on discriminative word alignment
has focused on the family of at-most-one-to-one
matchings (Melamed, 2000; Taskar et al, 2005;
Moore et al, 2006). An exception to this is the
work of Cherry and Lin (2006), who discrim-
inatively trained one-to-one ITG models, albeit
with limited feature sets. As they found, ITG
approaches offer several advantages over general
matchings. First, the additional structural con-
straint can result in superior alignments. We con-
firm and extend this result, showing that one-to-
one ITG models can perform as well as, or better
than, general one-to-one matching models, either
using heuristic weights or using rich, learned fea-
tures.
A second advantage of ITG approaches is that
they admit a range of training options. As with
general one-to-one matchings, we can optimize
margin-based objectives. However, unlike with
general matchings, we can also efficiently com-
pute expectations over the set of ITG derivations,
enabling the training of conditional likelihood
models. A major challenge in both cases is that
our training alignments are often not one-to-one
ITG alignments. Under such conditions, directly
training to maximize margin is unstable, and train-
ing to maximize likelihood is ill-defined, since the
target algnment derivations don?t exist in our hy-
pothesis class. We show how to adapt both margin
and likelihood objectives to learn good ITG align-
ers.
In the case of likelihood training, two innova-
tions are presented. The simple, two-rule ITG
grammar exponentially over-counts certain align-
ment structures relative to others. Because of this,
Wu (1997) and Zens and Ney (2003) introduced a
normal form ITG which avoids this over-counting.
We extend this normal form to null productions
and give the first extensive empirical comparison
of simple and normal form ITGs, for posterior de-
coding under our likelihood models. Additionally,
we show how to deal with training instances where
the gold alignments are outside of the hypothesis
class by instead optimizing the likelihood of a set
of minimum-loss alignments.
Perhaps the greatest advantage of ITG mod-
els is that they straightforwardly permit block-
923
structured alignments (i.e. phrases), which gen-
eral matchings cannot efficiently do. The need for
block alignments is especially acute in Chinese-
English data, where oracle AERs drop from 10.2
without blocks to around 1.2 with them. Indeed,
blocks are the primary reason for gold alignments
being outside the space of one-to-one ITG align-
ments. We show that placing linear potential func-
tions on many-to-one blocks can substantially im-
prove performance.
Finally, to scale up our system, we give a com-
bination of pruning techniques that allows us to
sum ITG alignments two orders of magnitude
faster than naive inside-outside parsing.
All in all, our discriminatively trained, block
ITG models produce alignments which exhibit
the best AER on the NIST 2002 Chinese-English
alignment data set. Furthermore, they result in
a 1.1 BLEU-point improvement over GIZA++
alignments in an end-to-end Hiero (Chiang, 2007)
machine translation system.
2 Alignment Families
In order to structurally restrict attention to rea-
sonable alignments, word alignment models must
constrain the set of alignments considered. In this
section, we discuss and compare alignment fami-
lies used to train our discriminative models.
Initially, as in Taskar et al (2005) and Moore
et al (2006), we assume the score a of a potential
alignment a) decomposes as
s(a) = ?
(i,j)?a
sij +
?
i/?a
si +
?
j /?a
sj (1)
where sij are word-to-word potentials and si and
sj represent English null and foreign null poten-
tials, respectively.
We evaluate our proposed alignments (a)
against hand-annotated alignments, which are
marked with sure (s) and possible (p) alignments.
The alignment error rate (AER) is given by,
AER(a, s,p) = 1? |a ? s|+ |a ? p||a|+ |s|
2.1 1-to-1 Matchings
The class of at most 1-to-1 alignment match-
ings, A1-1, has been considered in several works
(Melamed, 2000; Taskar et al, 2005; Moore et al,
2006). The alignment that maximizes a set of po-
tentials factored as in Equation (1) can be found
in O(n3) time using a bipartite matching algo-
rithm (Kuhn, 1955).1 On the other hand, summing
over A1-1 is #P -hard (Valiant, 1979).
Initially, we consider heuristic alignment poten-
tials given by Dice coefficients
Dice(e, f) = 2CefCe + Cf
where Cef is the joint count of words (e, f) ap-
pearing in aligned sentence pairs, and Ce and Cf
are monolingual unigram counts.
We extracted such counts from 1.1 million
French-English aligned sentence pairs of Hansards
data (see Section 6.1). For each sentence pair in
the Hansards test set, we predicted the alignment
from A1-1 which maximized the sum of Dice po-
tentials. This yielded 30.6 AER.
2.2 Inversion Transduction Grammar
Wu (1997)?s inversion transduction grammar
(ITG) is a synchronous grammar formalism in
which derivations of sentence pairs correspond to
alignments. In its original formulation, there is a
single non-terminal X spanning a bitext cell with
an English and foreign span. There are three rule
types: Terminal unary productions X ? ?e, f?,
where e and f are an aligned English and for-
eign word pair (possibly with one being null);
normal binary rules X ? X(L)X(R), where the
English and foreign spans are constructed from
the children as ?X(L)X(R), X(L)X(R)?; and in-
verted binary rules X ; X(L)X(R), where the
foreign span inverts the order of the children
?X(L)X(R), X(R)X(L)?.2 In general, we will call
a bitext cell a normal cell if it was constructed with
a normal rule and inverted if constructed with an
inverted rule.
Each ITG derivation yields some alignment.
The set of such ITG alignments,AITG, are a strict
subset of A1-1 (Wu, 1997). Thus, we will view
ITG as a constraint on A1-1 which we will ar-
gue is generally beneficial. The maximum scor-
ing alignment from AITG can be found in O(n6)
time with synchronous CFG parsing; in practice,
we can make ITG parsing efficient using a variety
of pruning techniques. One computational advan-
tage of AITG over A1-1 alignments is that sum-
mation overAITG is tractable. The corresponding
1We shall use n throughout to refer to the maximum of
foreign and English sentence lengths.
2The superscripts on non-terminals are added only to in-
dicate correspondence of child symbols.
924
In
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
 ?
 ?
 ??
 ??
 ??
 ??
 ?
 ?
 ??
 ??
 ??
 ??
I
n
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
(a) Max-Matching Alignment (b) Block ITG Alignment
Figure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively. The 1-1
matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is rendered
as two words in Chinese or that in court is rendered as a single word in Chinese.
dynamic program allows us to utilize likelihood-
based objectives for learning alignment models
(see Section 4).
Using the same heuristic Dice potentials on
the Hansards test set, the maximal scoring align-
ment from AITG yields 28.4 AER?2.4 better
than A1-1 ?indicating that ITG can be beneficial
as a constraint on heuristic alignments.
2.3 Block ITG
An important alignment pattern disallowed by
A1-1 is the many-to-one alignment block. While
not prevalent in our hand-aligned French Hansards
dataset, blocks occur frequently in our hand-
aligned Chinese-English NIST data. Figure 1
contains an example. Extending A1-1 to include
blocks is problematic, because finding a maximal
1-1 matching over phrases is NP-hard (DeNero
and Klein, 2008).
With ITG, it is relatively easy to allow contigu-
ous many-to-one alignment blocks without added
complexity.3 This is accomplished by adding ad-
ditional unary terminal productions aligning a for-
eign phrase to a single English terminal or vice
versa. We will use BITG to refer to this block
ITG variant and ABITG to refer to the alignment
family, which is neither contained in nor contains
A1-1. For this alignment family, we expand the
alignment potential decomposition in Equation (1)
to incorporate block potentials sef and sef which
represent English and foreign many-to-one align-
ment blocks, respectively.
One way to evaluate alignment families is to
3In our experiments we limited the block size to 4.
consider their oracle AER. In the 2002 NIST
Chinese-English hand-aligned data (see Sec-
tion 6.2), we constructed oracle alignment poten-
tials as follows: sij is set to +1 if (i, j) is a sure
or possible alignment in the hand-aligned data, -
1 otherwise. All null potentials (si and sj) are
set to 0. A max-matching under these potentials is
generally a minimal loss alignment in the family.
The oracle AER computed in this was is 10.1 for
A1-1 and 10.2 for AITG. The ABITG alignment
family has an oracle AER of 1.2. These basic ex-
periments show that AITG outperforms A1-1 for
heuristic alignments, and ABITG provide a much
closer fit to true Chinese-English alignments than
A1-1.
3 Margin-Based Training
In this and the next section, we discuss learning
alignment potentials. As input, we have a training
set D = (x1,a?1), . . . , (xn,a?n) of hand-aligned
data, where x refers to a sentence pair. We will as-
sume the score of a alignment is given as a linear
function of a feature vector ?(x,a). We will fur-
ther assume the feature representation of an align-
ment, ?(x,a) decomposes as in Equation (1),
?
(i,j)?a
?ij(x) +
?
i/?a
?i(x) +
?
j /?a
?j(x)
In the framework of loss-augmented margin
learning, we seek a w such that w ? ?(x,a?) is
larger than w ? ?(x,a) + L(a,a?) for all a in an
alignment family, where L(a,a?) is the loss be-
tween a proposed alignment a and the gold align-
ment a?. As in Taskar et al (2005), we utilize a
925
loss that decomposes across alignments. Specif-
ically, for each alignment cell (i, j) which is not
a possible alignment in a?, we incur a loss of 1
when aij 6= a?ij ; note that if (i, j) is a possible
alignment, our loss is indifferent to its presence in
the proposal alignment.
A simple loss-augmented learning pro-
cedure is the margin infused relaxed algo-
rithm (MIRA) (Crammer et al, 2006). MIRA
is an online procedure, where at each time step
t+ 1, we update our weights as follows:
wt+1 = argminw||w ?wt||22 (2)
s.t. w ? ?(x,a?) ? w ? ?(x, a?) + L(a?,a?)
where a? = argmax
a?A
wt ? ?(x,a)
In our data sets, many a? are not in A1-1 (and
thus not in AITG), implying the minimum in-
family loss must exceed 0. Since MIRA oper-
ates in an online fashion, this can cause severe
stability problems. On the Hansards data, the
simple averaging technique described by Collins
(2002) yields a reasonable model. On the Chinese
NIST data, however, where almost no alignment
is in A1-1, the update rule from Equation (2) is
completely unstable, and even the averaged model
does not yield high-quality results.
We instead use a variant of MIRA similar to
Chiang et al (2008). First, rather than update
towards the hand-labeled alignment a?, we up-
date towards an alignment which achieves mini-
mal loss within the family.4 We call this best-
in-class alignment a?p. Second, we perform loss-
augmented inference to obtain a?. This yields the
modified QP,
wt+1 = argminw||w ?wt||22 (3)
s.t. w ? ?(x,a?p) ? w ? ?(x, a?) + L(a,a?p)
where a? = argmax
a?A
wt ? ?(x,a) + ?L(a,a?p)
By setting ? = 0, we recover the MIRA update
from Equation (2). As ? grows, we increase our
preference that a? have high loss (relative to a?p)
rather than high model score. With this change,
MIRA is stable, but still performs suboptimally.
The reason is that initially the score for all align-
ments is low, so we are biased toward only using
very high loss alignments in our constraint. This
slows learning and prevents us from finding a use-
ful weight vector. Instead, in all the experiments
4There might be several alignments which achieve this
minimal loss; we choose arbitrarily among them.
we report here, we begin with ? = 0 and slowly
increase it to ? = 0.5.
4 Likelihood Objective
An alternative to margin-based training is a likeli-
hood objective, which learns a conditional align-
ment distribution Pw(a|x) parametrized as fol-
lows,
logPw(a|x)=w??(x,a)?log
?
a??A
exp(w??(x,a?))
where the log-denominator represents a sum over
the alignment family A. This alignment probabil-
ity only places mass on members ofA. The likeli-
hood objective is given by,
max
w
?
(x,a?)?A
logPw(a?|x)
Optimizing this objective with gradient methods
requires summing over alignments. ForAITG and
ABITG, we can efficiently sum over the set of ITG
derivations inO(n6) time using the inside-outside
algorithm. However, for the ITG grammar pre-
sented in Section 2.2, each alignment has multiple
grammar derivations. In order to correctly sum
over the set of ITG alignments, we need to alter
the grammar to ensure a bijective correspondence
between alignments and derivations.
4.1 ITG Normal Form
There are two ways in which ITG derivations dou-
ble count alignments. First, n-ary productions are
not binarized to remove ambiguity; this results in
an exponential number of derivations for diagonal
alignments. This source of overcounting is con-
sidered and fixed by Wu (1997) and Zens and Ney
(2003), which we briefly review here. The result-
ing grammar, which does not handle null align-
ments, consists of a symbol N to represent a bi-
text cell produced by a normal rule and I for a cell
formed by an inverted rule; alignment terminals
can be either N or I . In order to ensure unique
derivations, we stipulate that a N cell can be con-
structed only from a sequence of smaller inverted
cells I . Binarizing the rule N ? I2+ introduces
the intermediary symbolN (see Figure 2(a)). Sim-
ilarly for inverse cells, we insist an I cell only be
built by an inverted combination of N cells; bina-
rization of I ; N2+ requires the introduction of
the intermediary symbol I (see Figure 2(b)).
Null productions are also a source of double
counting, as there are many possible orders in
926
N ? I2+N ? IN
N ? I
}
N ? IN
I
I
I
N
N
N
(a) Normal Domain Rules
} I ! N2+
I ! NI
I ! NI
I ! N N
N
N
I
I
I
(b) Inverted Domain Rules
N11 ? ??, f?N11
N11 ? N10
N10 ? N10?e, ??
N10 ? N00
}N11 ? ??, f?
?N10
}N10 ? N00?e, ???
}
N00 ? I11N
N ? I11N
N ? I00
N00 ? I+11I00
N00 N10 N10
N11
N
NI11
I11
I00
N00
N11
(c) Normal Domain with Null Rules
}
}
}
I11 ! ??, f?I11
I11 ! I10 I11 ! ??, f?
?I10
I10 ! I10?e, ??
I10 ! I00 I10 ! I00?e, ??
?
I00 ! N+11N00 I
I
N00
N11
N11
I00 ! N11I
I ! N11I
I ! N00
I00
I00 I10 I10
I11
I11
(d) Inverted Domain with Null Rules
Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar
without nulls (presented in Wu (1997) and Zens and Ney (2003)). In (c) and (d), we present a normal form grammar
that accounts for null alignments.
which to attach null alignments to a bitext cell;
we address this by adapting the grammar to force
a null attachment order. We introduce symbols
N00, N10, and N11 to represent whether a normal
cell has taken no nulls, is accepting foreign nulls,
or is accepting English nulls, respectively. We also
introduce symbols I00, I10, and I11 to represent
inverse cells at analogous stages of taking nulls.
As Figures 2 (c) and (d) illustrate, the directions
in which nulls are attached to normal and inverse
cells differ. The N00 symbol is constructed by
one or more ?complete? inverted cells I11 termi-
nated by a no-null I00. By placing I00 in the lower
right hand corner, we allow the larger N00 to un-
ambiguously attach nulls. N00 transitions to the
N10 symbol and accepts any number of ?e, ?? En-
glish terminal alignments. Then N10 transitions to
N11 and accepts any number of ??, f? foreign ter-
minal alignments. An analogous set of grammar
rules exists for the inverted case (see Figure 2(d)
for an illustration). Given this normal form, we
can efficiently compute model expectations over
ITG alignments without double counting.5 To our
knowledge, the alteration of the normal form to
accommodate null emissions is novel to this work.
5The complete grammar adds sentinel symbols to the up-
per left and lower right, and the root symbol is constrained to
be a N00.
4.2 Relaxing the Single Target Assumption
A crucial obstacle for using the likelihood objec-
tive is that a given a? may not be in the alignment
family. As in our alteration to MIRA (Section 3),
we could replace a? with a minimal loss in-class
alignment a?p. However, in contrast to MIRA, the
likelihood objective will implicitly penalize pro-
posed alignments which have loss equal to a?p. We
opt instead to maximize the probability of the set
of alignmentsM(a?) which achieve the same op-
timal in-class loss. Concretely, let m? be the min-
imal loss achievable relative to a? in A. Then,
M(a?) = {a ? A|L(a,a?) = m?}
When a? is an ITG alignment (i.e., m? is 0),
M(a?) consists only of alignments which have all
the sure alignments in a?, but may have some sub-
set of the possible alignments in a?. See Figure 3
for a specific example where m? = 1.
Our modified likelihood objective is given by,
max
w
?
(x,a?)?D
log ?
a?M(a?)
Pw(a|x)
Note that this objective is no longer convex, as it
involves a logarithm of a summation, however we
still utilize gradient-based optimization. Summing
and obtaining feature expectations over M(a?)
can be done efficiently using a constrained variant
927
MIRA Likelihood
1-1 ITG ITG-S ITG-N
Features P R AER P R AER P R AER P R AER
Dice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6
+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4
+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1
Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows
indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar
(Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.
T
h
a
t
i
s
n
o
t
g
o
o
d
e
n
o
u
g
h
 Se
ne
est
pas
 suffisant
a?Gold Alignment Target AlignmentsM(a?)
Figure 3: Often, the gold alignment a? isn?t in our
alignment family, here ABITG. For the likelihood ob-
jective (Section 4.2), we maximize the probability of
the setM(a?) consisting of alignments ABITG which
achieve minimal loss relative to a?. In this example,
the minimal loss is 1, and we have a choice of remov-
ing either of the sure alignments to the English word
not. We also have the choice of whether to include the
possible alignment, yielding 4 alignments inM(a?).
of the inside-outside algorithm where sure align-
ments not present in a? are disallowed, and the
number of missing sure alignments is appended to
the state of the bitext cell.6
One advantage of the likelihood-based objec-
tive is that we can obtain posteriors over individual
alignment cells,
Pw((i, j)|x) =
?
a?A:(i,j)?a
Pw(a|x)
We obtain posterior ITG alignments by including
all alignment cells (i, j) such that Pw((i, j)|x) ex-
ceeds a fixed threshold t. Posterior thresholding
allows us to easily trade-off precision and recall in
our alignments by raising or lowering t.
5 Dynamic Program Pruning
Both discriminative methods require repeated
model inference: MIRA depends upon loss-
augmented Viterbi parsing, while conditional like-
6Note that alignments that achieve the minimal loss would
not introduce any alignments not either sure or possible, so it
suffices to keep track only of the number of sure recall errors.
lihood uses the inside-outside algorithm for com-
puting cell posteriors. Exhaustive computation
of these quantities requires an O(n6) dynamic
program that is prohibitively slow even on small
supervised training sets. However, most of the
search space can safely be pruned using posterior
predictions from a simpler alignment models. We
use posteriors from two jointly estimated HMM
models to make pruning decisions during ITG in-
ference (Liang et al, 2006). Our first pruning tech-
nique is broadly similar to Cherry and Lin (2007a).
We select high-precision alignment links from the
HMM models: those word pairs that have a pos-
terior greater than 0.9 in either model. Then, we
prune all bitext cells that would invalidate more
than 8 of these high-precision alignments.
Our second pruning technique is to prune all
one-by-one (word-to-word) bitext cells that have a
posterior below 10?4 in both HMM models. Prun-
ing a one-by-one cell also indirectly prunes larger
cells containing it. To take maximal advantage of
this indirect pruning, we avoid explicitly attempt-
ing to build each cell in the dynamic program. In-
stead, we track bounds on the spans for which we
have successfully built ITG cells, and we only iter-
ate over larger spans that fall within those bounds.
The details of a similar bounding approach appear
in DeNero et al (2009).
In all, pruning reduces MIRA iteration time
from 175 to 5 minutes on the NIST Chinese-
English dataset with negligible performance loss.
Likelihood training time is reduced by nearly two
orders of magnitude.
6 Alignment Quality Experiments
We present results which measure the quality of
our models on two hand-aligned data sets. Our
first is the English-French Hansards data set from
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003). Here we use the same 337/100
train/test split of the labeled data as Taskar et al
928
MIRA Likelihood
1-1 ITG BITG BITG-S BITG-N
Features P R AER P R AER P R AER P R AER P R AER
Dice, dist,
blcks, dict, lex 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1
+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4
Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment
family. The first row represents our best model without external alignment models and the second row includes
features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N
uses the normal form grammar (Section 4.1).
(2005); we compute external features from the
same unlabeled data, 1.1 million sentence pairs.
Our second is the Chinese-English hand-aligned
portion of the 2002 NIST MT evaluation set. This
dataset has 491 sentences, which we split into a
training set of 150 and a test set of 191. When we
trained external Chinese models, we used the same
unlabeled data set as DeNero and Klein (2007), in-
cluding the bilingual dictionary.
For likelihood based models, we set the L2 reg-
ularization parameter, ?2, to 100 and the thresh-
old for posterior decoding to 0.33. We report re-
sults using the simple ITG grammar (ITG-S, Sec-
tion 2.2) where summing over derivations dou-
ble counts alignments, as well as the normal form
ITG grammar (ITG-N,Section 4.1) which does
not double count. We ran our annealed loss-
augmented MIRA for 15 iterations, beginning
with ? at 0 and increasing it linearly to 0.5. We
compute Viterbi alignments using the averaged
weight vector from this procedure.
6.1 French Hansards Results
The French Hansards data are well-studied data
sets for discriminative word alignment (Taskar et
al., 2005; Cherry and Lin, 2006; Lacoste-Julien
et al, 2006). For this data set, it is not clear
that improving alignment error rate beyond that of
GIZA++ is useful for translation (Ganchev et al,
2008). Table 1 illustrates results for the Hansards
data set. The first row uses dice and the same dis-
tance features as Taskar et al (2005). The first
two rows repeat the experiments of Taskar et al
(2005) and Cherry and Lin (2006), but adding ITG
models that are trained to maximize conditional
likelihood. The last row includes the posterior of
the jointly-trained HMM of Liang et al (2006)
as a feature. This model alone achieves an AER
of 5.4. No model significantly improves over the
HMM alone, which is consistent with the results
of Taskar et al (2005).
6.2 Chinese NIST Results
Chinese-English alignment is a much harder task
than French-English alignment. For example, the
HMM aligner achieves an AER of 20.7 when us-
ing the competitive thresholding heuristic of DeN-
ero and Klein (2007). On this data set, our block
ITG models make substantial performance im-
provements over the HMM, and moreover these
results do translate into downstream improve-
ments in BLEU score for the Chinese-English lan-
guage pair. Because of this, we will briefly de-
scribe the features used for these models in de-
tail. For features on one-by-one cells, we con-
sider Dice, the distance features from (Taskar et
al., 2005), dictionary features, and features for the
50 most frequent lexical pairs. We also trained an
HMM aligner as described in DeNero and Klein
(2007) and used the posteriors of this model as fea-
tures. The first two columns of Table 2 illustrate
these features for ITG and one-to-one matchings.
For our block ITG models, we include all of
these features, along with variants designed for
many-to-one blocks. For example, we include the
average Dice of all the cells in a block. In addi-
tion, we also created three new block-specific fea-
tures types. The first type comprises bias features
for each block length. The second type comprises
features computed from N-gram statistics gathered
from a large monolingual corpus. These include
features such as the number of occurrences of the
phrasal (multi-word) side of a many-to-one block,
as well as pointwise mutual information statistics
for the multi-word parts of many-to-one blocks.
These features capture roughly how ?coherent? the
multi-word side of a block is.
The final block feature type consists of phrase
shape features. These are designed as follows: For
each word in a potential many-to-one block align-
ment, we map an individual word to X if it is not
one of the 25 most frequent words. Some example
features of this type are,
929
? English Block: [the X, X], [in X of, X]
? Chinese Block: [ X, X] [X|, X]
For English blocks, for example, these features
capture the behavior of phrases such as in spite
of or in front of that are rendered as one word in
Chinese. For Chinese blocks, these features cap-
ture the behavior of phrases containing classifier
phrases like? orP, which are rendered as
English indefinite determiners.
The right-hand three columns in Table 2 present
supervised results on our Chinese English data set
using block features. We note that almost all of
our performance gains (relative to both the HMM
and 1-1 matchings) come from BITG and block
features. The maximum likelihood-trained nor-
mal form ITG model outperforms the HMM, even
without including any features derived from the
unlabeled data. Once we include the posteriors
of the HMM as a feature, the AER decreases to
14.4. The previous best AER result on this data set
is 15.9 from Ayan and Dorr (2006), who trained
stacked neural networks based on GIZA++ align-
ments. Our results are not directly comparable
(they used more labeled data, but did not have the
HMM posteriors as an input feature).
6.3 End-To-End MT Experiments
We further evaluated our alignments in an end-to-
end Chinese to English translation task using the
publicly available hierarchical pipeline JosHUa
(Li and Khudanpur, 2008). The pipeline extracts
a Hiero-style synchronous context-free grammar
(Chiang, 2007), employs suffix-array based rule
extraction (Lopez, 2007), and tunes model pa-
rameters with minimum error rate training (Och,
2003). We trained on the FBIS corpus using sen-
tences up to length 40, which includes 2.7 million
English words. We used a 5-gram language model
trained on 126 million words of the Xinhua section
of the English Gigaword corpus, estimated with
SRILM (Stolcke, 2002). We tuned on 300 sen-
tences of the NIST MT04 test set.
Results on the NIST MT05 test set appear in
Table 3. We compared four sets of alignments.
The GIZA++ alignments7 are combined across di-
rections with the grow-diag-final heuristic, which
outperformed the union. The joint HMM align-
ments are generated from competitive posterior
7We used a standard training regimen: 5 iterations of
model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3
iterations of Model 4.
Alignments Translations
Model Prec Rec Rules BLEU
GIZA++ 62 84 1.9M 23.22
Joint HMM 79 77 4.0M 23.05
Viterbi ITG 90 80 3.8M 24.28
Posterior ITG 81 83 4.2M 24.32
Table 3: Results on the NIST MT05 Chinese-English
test set show that our ITG alignments yield improve-
ments in translation quality.
thresholding (DeNero and Klein, 2007). The ITG
Viterbi alignments are the Viterbi output of the
ITG model with all features, trained to maximize
log likelihood. The ITG Posterior alignments
result from applying competitive thresholding to
alignment posteriors under the ITG model. Our
supervised ITG model gave a 1.1 BLEU increase
over GIZA++.
7 Conclusion
This work presented the first large-scale applica-
tion of ITG to discriminative word alignment. We
empirically investigated the performance of con-
ditional likelihood training of ITG word aligners
under simple and normal form grammars. We
showed that through the combination of relaxed
learning objectives, many-to-one block alignment
potential, and efficient pruning, ITG models can
yield state-of-the art word alignments, even when
the underlying gold alignments are highly non-
ITG. Our models yielded the lowest published er-
ror for Chinese-English alignment and an increase
in downstream translation performance.
References
Necip Fazil Ayan and Bonnie Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In ACL.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In ACL.
Colin Cherry and Dekang Lin. 2007a. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In NAACL-HLT 2007.
Colin Cherry and Dekang Lin. 2007b. A scalable in-
version transduction grammar for joint phrasal trans-
lation modeling. In SSST Workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
930
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai S. Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In ACL Short Paper
Track.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In NAACL.
Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008.
Better alignments = better translations? In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word alignment via
quadratic assignment. In NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
SSST Workshop at ACL.
Percy Liang, Dan Klein, and Dan Klein. 2006. Align-
ment by agreement. In NAACL-HLT.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT/NAACL
Workshop on Building and Using Parallel Texts.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In ACL-COLING.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Empirical Methods in Nat-
ural Language Processing.
Andreas Stolcke. 2002. Srilm: An extensible language
modeling toolkit. In ICSLP 2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In NAACL-HLT.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Dan Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment.
In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
931
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958?966,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
K-Best A? Parsing
Adam Pauls and Dan Klein
Computer Science Division
University of California, Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
A? parsing makes 1-best search efficient by
suppressing unlikely 1-best items. Existing k-
best extraction methods can efficiently search
for top derivations, but only after an exhaus-
tive 1-best pass. We present a unified algo-
rithm for k-best A? parsing which preserves
the efficiency of k-best extraction while giv-
ing the speed-ups of A? methods. Our algo-
rithm produces optimal k-best parses under the
same conditions required for optimality in a
1-best A? parser. Empirically, optimal k-best
lists can be extracted significantly faster than
with other approaches, over a range of gram-
mar types.
1 Introduction
Many situations call for a parser to return the k-
best parses rather than only the 1-best. Uses for
k-best lists include minimum Bayes risk decod-
ing (Goodman, 1998; Kumar and Byrne, 2004),
discriminative reranking (Collins, 2000; Char-
niak and Johnson, 2005), and discriminative train-
ing (Och, 2003; McClosky et al, 2006). The
most efficient known algorithm for k-best parsing
(Jime?nez and Marzal, 2000; Huang and Chiang,
2005) performs an initial bottom-up dynamic pro-
gramming pass before extracting the k-best parses.
In that algorithm, the initial pass is, by far, the bot-
tleneck (Huang and Chiang, 2005).
In this paper, we propose an extension of A?
parsing which integrates k-best search with an A?-
based exploration of the 1-best chart. A? pars-
ing can avoid significant amounts of computation
by guiding 1-best search with heuristic estimates
of parse completion costs, and has been applied
successfully in several domains (Klein and Man-
ning, 2002; Klein and Manning, 2003c; Haghighi
et al, 2007). Our algorithm extends the speed-
ups achieved in the 1-best case to the k-best case
and is optimal under the same conditions as a stan-
dard A? algorithm. The amount of work done in
the k-best phase is no more than the amount of
work done by the algorithm of Huang and Chiang
(2005). Our algorithm is also equivalent to stan-
dard A? parsing (up to ties) if it is terminated after
the 1-best derivation is found. Finally, our algo-
rithm can be written down in terms of deduction
rules, and thus falls into the well-understood view
of parsing as weighted deduction (Shieber et al,
1995; Goodman, 1998; Nederhof, 2003).
In addition to presenting the algorithm, we
show experiments in which we extract k-best lists
for three different kinds of grammars: the lexi-
calized grammars of Klein and Manning (2003b),
the state-split grammars of Petrov et al (2006),
and the tree transducer grammars of Galley et al
(2006). We demonstrate that optimal k-best lists
can be extracted significantly faster using our al-
gorithm than with previous methods.
2 A k-Best A? Parsing Algorithm
We build up to our full algorithm in several stages,
beginning with standard 1-best A? parsing and
making incremental modifications.
2.1 Parsing as Weighted Deduction
Our algorithm can be formulated in terms of
prioritized weighted deduction rules (Shieber et
al., 1995; Nederhof, 2003; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
958
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of un-
processed items), as well as a chart of items al-
ready processed. The fundamental operation of
the algorithm is to pop the highest priority item ?
from the agenda, put it into the chart with its cur-
rent weight, and form using deduction rules any
items which can be built by combining ? with
items already in the chart. If new or improved,
resulting items are put on the agenda with priority
given by p(?).
2.2 A? Parsing
The A? parsing algorithm of Klein and Manning
(2003c) can be formulated in terms of weighted
deduction rules (Felzenszwalb and McAllester,
2007). We do so here both to introduce notation
and to build to our final algorithm.
First, we must formalize some notation. As-
sume we have a PCFG1 G and an input sentence
s1 . . . sn of length n. The grammar G has a set of
symbols ?, including a distinguished goal (root)
symbol G. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in G has the form r = A? B C with weight wr
(the negative log-probability of the rule). Edges
are labeled spans e = (A, i, j). Inside derivations
of an edge (A, i, j) are trees rooted at A and span-
ning si+1 . . . sj . The total weight of the best (min-
imum) inside derivation for an edge e is called the
Viterbi inside score ?(e). The goal of the 1-best
A? parsing algorithm is to compute the Viterbi in-
side score of the edge (G, 0, n); backpointers al-
low the reconstruction of a Viterbi parse in the
standard way.
The basic A? algorithm operates on deduc-
tion items I(A, i, j) which represent in a col-
lapsed way the possible inside derivations of edges
(A, i, j). We call these items inside edge items or
simply inside items where clear; a graphical rep-
resentation of an inside item can be seen in Fig-
ure 1(a). The space whose items are inside edges
is called the edge space.
These inside items are combined using the sin-
gle IN deduction schema shown in Table 1. This
schema is instantiated for every grammar rule r
1While we present the algorithm specialized to parsing
with a PCFG, it generalizes to a wide range of hypergraph
search problems as shown in Klein and Manning (2001).
VP
s
3
s
4
s
5
s
1
s
2
... s
6
s
n
...
VP
VBZ NP
DT NN
s
3
s
4
s
5
VP
G
(a) (b)
(c)
VP
VBZ
1
NP
4
DT NN
s
3
s
4
s
5
(e)
VP
6
s
3
s
4
s
5
VBZ NP
DT NN
(d)
Figure 1: Representations of the different types of
items used in parsing. (a) An inside edge item:
I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5).
(c) An inside derivation item: D(TVP, 2, 5) for a tree
TVP. (d) A ranked derivation item: K(VP, 2, 5, 6).
(e) A modified inside derivation item (with back-
pointers to ranked items): D(VP, 2, 5, 3,VP ?
VBZ NP, 1, 4).
in G. For IN, the function g(?) simply sums the
weights of the antecedent items and the gram-
mar rule r, while the priority function p(?) adds
a heuristic to this sum. The heuristic is a bound
on the Viterbi outside score ?(e) of an edge e;
see Klein and Manning (2003c) for details. A
good heuristic allows A? to reach the goal item
I(G, 0, n) while constructing few inside items.
If the heuristic is consistent, then A? guarantees
that whenever an inside item comes off the agenda,
its weight is its true Viterbi inside score (Klein and
Manning, 2003c). In particular, this guarantee im-
plies that the goal item I(G, 0, n) will be popped
with the score of the 1-best parse of the sentence.
Consistency also implies that items are popped off
the agenda in increasing order of bounded Viterbi
scores:
?(e) + h(e)
We will refer to this monotonicity as the order-
ing property of A? (Felzenszwalb and McAllester,
2007). One final property implied by consistency
is admissibility, which states that the heuristic
never overestimates the true Viterbi outside score
for an edge, i.e. h(e) ? ?(e). For the remain-
der of this paper, we will assume our heuristics
are consistent.
2.3 A Naive k-Best A? Algorithm
Due to the optimal substructure of 1-best PCFG
derivations, a 1-best parser searches over the space
of edges; this is the essence of 1-best dynamic
programming. Although most edges can be built
959
Inside Edge Deductions (Used in A? and KA?)
IN: I(B, i, l) : w1 I(C, l, j) : w2
w1+w2+wr+h(A,i,j)?????????????? I(A, i, j) : w1 + w2 + wr
Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic. This schema is
sufficient on its own for 1-best A?, and it is used in KA?. Here, r is the rule A? B C.
Inside Derivation Deductions (Used in NAIVE)
DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2
w1+w2+wr+h(A,i,j)?????????????? D
(
A
TB TC
, i, j
)
: w1 + w2 + wr
Table 2: The deduction schema for building derivations, using a supplied heuristic. TB and TC denote full tree
structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the
space of fully specified inside derivations rather than dynamic programming edges. This schema forms the NAIVE
k-best algorithm.
Outside Edge Deductions (Used in KA?)
OUT-B: I(G, 0, n) : w1
w1??? O(G, 0, n) : 0
OUT-L: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w3+wr+w2??????????? O(B, i, l) : w1 + w3 + wr
OUT-R: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w2+wr+w3??????????? O(C, l, j) : w1 + w2 + wr
Table 3: The deduction schemata for building ouside edge items. The first schema is a base case that constructs an
outside item for the goal (G, 0, n) from the inside item I(G, 0, n). The second two schemata build outside items
in a top-down fashion. Note that for outside items, the completion cost is the weight of an inside item rather than
a value computed by a heuristic.
Delayed Inside Derivation Deductions (Used in KA?)
DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2 O(A, i, j) : w3
w1+w2+wr+w3??????????? D
(
A
TB TC
, i, j
)
: w1 + w2 + wr
Table 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-
tions. The dependency on the outside item O(A, i, j) delays building derivation items until exact Viterbi outside
scores have been computed. This is the final search space for the KA? algorithm.
Ranked Inside Derivation Deductions (Lazy Version of NAIVE)
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2
w1+w2+wr+h(A,i,j)?????????????? D(A, i, j, l, r, u, v) : w1 + w2 + wr
RANK: D1(A, i, j, ?) : w1 . . . Dk(A, i, j, ?) : wk
maxm wm+h(A,i,j)????????????? K(A, i, j, k) : maxm wm
Table 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier
form of the NAIVE algorithm. BUILD builds larger derivations from smaller ones. RANK numbers derivations
for each edge. Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon as
the kth-best inside derivation item for a given edge is removed from the queue. However, it will also still formally
apply (suboptimally) for all derivation items dequeued after the kth. In practice, the RANK schema need not be
implemented explicitly ? one can simply assign a rank to each inside derivation item when it is removed from the
agenda, and directly add the appropriate ranked inside item to the chart.
Delayed Ranked Inside Derivation Deductions (Lazy Version of KA?)
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2 O(A, i, j) : w3
w1+w2+wr+w3??????????? D(A, i, j, l, r, u, v) : w1 + w2 + wr
RANK: D1(A, i, j, ?) : w1 . . . Dk(A, i, j, ?) : wk O(A, i, j) : wk+1
maxm wm+wk+1???????????? K(A, i, j, k) : maxm wm
Table 6: The deduction schemata for building and ranking derivations, using exact outside scores computed from
OUT deductions, used for the lazier form of the KA? algorithm.
960
using many derivations, each inside edge item
will be popped exactly once during parsing, with
a score and backpointers representing its 1-best
derivation.
However, k-best lists involve suboptimal
derivations. One way to compute k-best deriva-
tions is therefore to abandon optimal substructure
and dynamic programming entirely, and to search
over the derivation space, the much larger space
of fully specified trees. The items in this space are
called inside derivation items, or derivation items
where clear, and are of the form D(TA, i, j), spec-
ifying an entire tree TA rooted at symbol A and
spanning si+1 . . . sj (see Figure 1(c)). Derivation
items are combined using the DERIV schema of
Table 2. The goals in this space, representing root
parses, are any derivation items rooted at symbol
G that span the entire input.
In this expanded search space, each distinct
parse has its own derivation item, derivable only
in one way. If we continue to search long enough,
we will pop multiple goal items. The first k which
come off the agenda will be the k-best derivations.
We refer to this approach as NAIVE. It is very in-
efficient on its own, but it leads to the full algo-
rithm.
The correctness of this k-best algorithm follows
from the correctness of A? parsing. The derivation
space of full trees is simply the edge space of a
much larger grammar (see Section 2.5).
Note that the DERIV schema?s priority includes
a heuristic just like 1-best A?. Because of the
context freedom of the grammar, any consistent
heuristic for inside edge items usable in 1-best A?
is also consistent for inside derivation items (and
vice versa). In particular, the 1-best Viterbi out-
side score for an edge is a ?perfect? heuristic for
any derivation of that edge.
While correct, NAIVE is massively inefficient.
In comparison with A? parsing over G, where there
are O(n2) inside items, the size of the derivation
space is exponential in the sentence length. By
the ordering property, we know that NAIVE will
process all derivation items d with
?(d) + h(d) ? ?(gk)
where gk is the kth-best root parse and ?(?) is the
inside score of a derivation item (analogous to ?
for edges).2 Even for reasonable heuristics, this
2The new symbol emphasizes that ? scores a specific
derivation rather than a minimum over a set of derivations.
number can be very large; see Section 3 for empir-
ical results.
This naive algorithm is, of course, not novel, ei-
ther in general approach or specific computation.
Early k-best parsers functioned by abandoning dy-
namic programming and performing beam search
on derivations (Ratnaparkhi, 1999; Collins, 2000).
Huang (2005) proposes an extension of Knuth?s
algorithm (Knuth, 1977) to produce k-best lists
by searching in the space of derivations, which
is essentially this algorithm. While Huang (2005)
makes no explicit mention of a heuristic, it would
be easy to incorporate one into their formulation.
2.4 A New k-Best A? Parser
While NAIVE suffers severe performance degra-
dation for loose heuristics, it is in fact very effi-
cient if h(?) is ?perfect,? i.e. h(e) = ?(e) ?e. In
this case, the ordering property of A? guarantees
that only inside derivation items d satisfying
?(d) + ?(d) ? ?(gk)
will be placed in the chart. The set of derivation
items d satisfying this inequality is exactly the set
which appear in the k-best derivations of (G, 0, n)
(as always, modulo ties). We could therefore use
NAIVE quite efficiently if we could obtain exact
Viterbi outside scores.
One option is to compute outside scores with
exhaustive dynamic programming over the orig-
inal grammar. In a certain sense, described in
greater detail below, this precomputation of exact
heuristics is equivalent to the k-best extraction al-
gorithm of Huang and Chiang (2005). However,
this exhaustive 1-best work is precisely what we
want to use A? to avoid.
Our algorithm solves this problem by integrat-
ing three searches into a single agenda-driven pro-
cess. First, an A? search in the space of inside
edge items with an (imperfect) external heuristic
h(?) finds exact inside scores. Second, exact out-
side scores are computed from inside and outside
items. Finally, these exact outside scores guide the
search over derivations. It can be useful to imag-
ine these three operations as operating in phases,
but they are all interleaved, progressing in order of
their various priorities.
In order to calculate outside scores, we intro-
duce outside items O(A, i, j), which represent
best derivations of G ? s1 . . . si A sj+1 . . . sn;
see Figure 1(b). Where the weights of inside items
961
compute Viterbi inside scores, the weights of out-
side items compute Viterbi outside scores.
Table 3 shows deduction schemata for building
outside items. These schemata are adapted from
the schemata used in the general hierarchical A?
algorithm of Felzenszwalb and McAllester (2007).
In that work, it is shown that such schemata main-
tain the property that the weight of an outside item
is the true Viterbi outside score when it is removed
from the agenda. They also show that outside
items o follow an ordering property, namely that
they are processed in increasing order of
?(o) + ?(o)
This quantity is the score of the best root deriva-
tion which includes the edge corresponding to o.
Felzenszwalb and McAllester (2007) also show
that both inside and outside items can be processed
on the same queue and the ordering property holds
jointly for both types of items.
If we delay the construction of a derivation
item until its corresponding outside item has been
popped, then we can gain the benefits of using an
exact heuristic h(?) in the naive algorithm. We re-
alize this delay by modifying the DERIV deduc-
tion schema as shown in Table 4 to trigger on and
prioritize with the appropriate outside scores.
We now have our final algorithm, which we call
KA?. It is the union of the IN, OUT, and new ?de-
layed? DERIV deduction schemata. In words, our
algorithm functions as follows: we initialize the
agenda with I(si, i ? 1, i) and D(si, i ? 1, i) for
i = 1 . . . n. We compute inside scores in standard
A? fashion using the IN deduction rule, using any
heuristic we might provide to 1-best A?. Once the
inside item I(G, 0, n) is found, we automatically
begin to compute outside scores via the OUT de-
duction rules. Once O(si, i ? 1, i) is found, we
can begin to also search in the space of deriva-
tion items, using the perfect heuristics given by
the just-computed outside scores. Note, however,
that all computation is done with a single agenda,
so the processing of all three types of items is in-
terleaved, with the k-best search possibly termi-
nating without a full inside computation. As with
NAIVE, the algorithm terminates when a k-th goal
derivation is dequeued.
2.5 Correctness
We prove the correctness of this algorithm by a re-
duction to the hierarchical A? (HA?) algorithm of
Felzenszwalb and McAllester (2007). The input
to HA? is a target grammar Gm and a list of gram-
mars G0 . . .Gm?1 in which Gt?1 is a relaxed pro-
jection of Gt for all t = 1 . . .m. A grammar Gt?1
is a projection of Gt if there exists some onto func-
tion pit : ?t 7? ?t?1 defined for all symbols in Gt.
We use At?1 to represent pit(At). A projection is
relaxed if, for every rule r = At ? BtCt with
weight wr there is a rule r? = At?1 ? Bt?1Ct?1
in Gt?1 with weight wr? ? wr.
We assume that our external heuristic function
h(?) is constructed by parsing our input sentence
with a relaxed projection of our target grammar.
This assumption, though often true anyway, is
to allow proof by reduction to Felzenszwalb and
McAllester (2007).3
We construct an instance of HA? as follows: Let
G0 be the relaxed projection which computes the
heuristic. Let G1 be the input grammar G, and let
G2, the target grammar of our HA? instance, be the
grammar of derivations in G formed by expanding
each symbol A in G to all possible inside deriva-
tions TA rooted atA. The rules in G2 have the form
TA ? TB TC with weight given by the weight of
the rule A ? B C. By construction, G1 is a re-
laxed projection of G2; by assumption G0 is a re-
laxed projection of G1. The deduction rules that
describe KA? build the same items as HA? with
same weights and priorities, and so the guarantees
from HA? carry over to KA?.
We can characterize the amount of work done
using the ordering property. Let gk be the kth-best
derivation item for the goal edge g. Our algorithm
processes all derivation items d, outside items o,
and inside items i satisfying
?(d) + ?(d) ? ?(gk)
?(o) + ?(o) ? ?(gk)
?(i) + h(i) ? ?(gk)
We have already argued that the set of deriva-
tion items satisfying the first inequality is the set of
subtrees that appear in the optimal k-best parses,
modulo ties. Similarly, it can be shown that the
second inequality is satisfied only for edges that
appear in the optimal k-best parses. The last in-
equality characterizes the amount of work done in
the bottom-up pass. We compare this to 1-best A?,
which pops all inside items i satisfying
?(i) + h(i) ? ?(g) = ?(g1)
3KA? is correct for any consistent heuristic but a non-
reductive proof is not possible in the present space.
962
Thus, the ?extra? inside items popped in the
bottom-up pass during k-best parsing as compared
to 1-best parsing are those items i satisfying
?(g1) ? ?(i) + h(i) ? ?(gk)
The question of how many items satisfy these
inequalities is empirical; we show in our experi-
ments that it is small for reasonable heuristics. At
worst, the bottom-up phase pops all inside items
and reduces to exhaustive dynamic programming.
Additionally, it is worth noting that our algo-
rithm is naturally online in that it can be stopped
at any k without advance specification.
2.6 Lazy Successor Functions
The global ordering property guarantees that we
will only dequeue derivation fragments of top
parses. However, we will enqueue all combina-
tions of such items, which is wasteful. By ex-
ploiting a local ordering amongst derivations, we
can be more conservative about combination and
gain the advantages of a lazy successor function
(Huang and Chiang, 2005).
To do so, we represent inside derivations not
by explicitly specifying entire trees, but rather
by using ranked backpointers. In this represen-
tation, inside derivations are represented in two
ways, shown in Figure 1(d) and (e). The first
way (d) simply adds a rank u to an edge, giving
a tuple (A, i, j, u). The corresponding item is the
ranked derivation item K(A, i, j, u), which repre-
sents the uth-best derivation of A over (i, j). The
second representation (e) is a backpointer of the
form (A, i, j, l, r, u, v), specifying the derivation
formed by combining the uth-best derivation of
(B, i, l) and the vth-best derivation of (C, l, j) us-
ing rule r = A? B C. The corresponding items
D(A, i, j, l, r, u, v) are the new form of our inside
derivation items.
The modified deduction schemata for the
NAIVE algorithm over these representations are
shown in Table 5. The BUILD schema pro-
duces new inside derivation items from ranked
derivation items, while the RANK schema as-
signs each derivation item a rank; together they
function like DERIV. We can find the k-best list
by searching until K(G, 0, n, k) is removed from
the agenda. The k-best derivations can then
be extracted by following the backpointers for
K(G, 0, n, 1) . . . K(G, 0, n, k). The KA? algo-
rithm can be modified in the same way, shown in
Table 6.
1
5
50
500
Heuristic
Deriva
tion ite
ms pus
hed (m
illions
)
5-split 4-split 3-split 2-split 1-split 0-split
NAIVE
KA*
Figure 2: Number of derivation items enqueued as a
function of heuristic. Heuristics are shown in decreas-
ing order of tightness. The y-axis is on a log-scale.
The actual laziness is provided by addition-
ally delaying the combination of ranked items.
When an item K(B, i, l, u) is popped off the
queue, a naive implementation would loop over
items K(C, l, j, v) for all v, C, and j (and
similarly for left combinations). Fortunately,
little looping is actually necessary: there is
a partial ordering of derivation items, namely,
that D(A, i, j, l, r, u, v) will have a lower com-
puted priority than D(A, i, j, l, r, u ? 1, v) and
D(A, i, j, l, r, u, v ? 1) (Jime?nez and Marzal,
2000). So, we can wait until one of the latter two
is built before ?triggering? the construction of the
former. This triggering is similar to the ?lazy fron-
tier? used by Huang and Chiang (2005). All of our
experiments use this lazy representation.
3 Experiments
3.1 State-Split Grammars
We performed our first experiments with the gram-
mars of Petrov et al (2006). The training pro-
cedure for these grammars produces a hierarchy
of increasingly refined grammars through state-
splitting. We followed Pauls and Klein (2009) in
computing heuristics for the most refined grammar
from outside scores for less-split grammars.
We used the Berkeley Parser4 to learn such
grammars from Sections 2-21 of the Penn Tree-
bank (Marcus et al, 1993). We trained with 6
split-merge cycles, producing 7 grammars. We
tested these grammars on 100 sentences of length
at most 30 of Section 23 of the Treebank. Our
?target grammar? was in all cases the most split
grammar.
4http://berkeleyparser.googlecode.com
963
0 2000 4000 6000 8000 10000
050
00
15000
25000
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
0 2000 4000 6000 8000 10000
050
00
15000
25000
EXH
k
Items 
pushed
 (milli
ons) K BestBottom-up
Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA? and EXH. The
amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases.
Heuristics computed from projections to suc-
cessively smaller grammars in the hierarchy form
successively looser bounds on the outside scores.
This allows us to examine the performance as a
function of the tightness of the heuristic. We first
compared our algorithm KA? against the NAIVE
algorithm. We extracted 1000-best lists using each
algorithm, with heuristics computed using each of
the 6 smaller grammars.
In Figure 2, we evaluate only the k-best extrac-
tion phase by plotting the number of derivation
items and outside items added to the agenda as
a function of the heuristic used, for increasingly
loose heuristics. We follow earlier work (Pauls
and Klein, 2009) in using number of edges pushed
as the primary, hardware-invariant metric for eval-
uating performance of our algorithms.5 While
KA? scales roughly linearly with the looseness of
the heuristic, NAIVE degrades very quickly as the
heuristics get worse. For heuristics given by gram-
mars weaker than the 4-split grammar, NAIVE ran
out of memory.
Since the bottom-up pass of k-best parsing is
the bottleneck, we also examine the time spent
in the 1-best phase of k-best parsing. As a base-
line, we compared KA? to the approach of Huang
and Chiang (2005), which we will call EXH (see
below for more explanation) since it requires ex-
haustive parsing in the bottom-up pass. We per-
formed the exhaustive parsing needed for EXH
in our agenda-based parser to facilitate compar-
ison. For KA?, we included the cost of com-
puting the heuristic, which was done by running
our agenda-based parser exhaustively on a smaller
grammar to compute outside items; we chose the
5We found that edges pushed was generally well corre-
lated with parsing time.
0 2000 4000 6000 8000 10000
020
0
600
1000
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
Figure 4: The performance of KA? for lexicalized
grammars. The performance is dominated by the com-
putation of the heuristic, so that both the bottom-up
phase and the k-best phase are barely visible.
3-split grammar for the heuristic since it gives the
best overall tradeoff of heuristic and bottom-up
parsing time. We separated the items enqueued
into items enqueued while computing the heuris-
tic (not strictly part of the algorithm), inside items
(?bottom-up?), and derivation and outside items
(together ?k-best?). The results are shown in Fig-
ure 3. The cost of k-best extraction is clearly
dwarfed by the the 1-best computation in both
cases. However, KA? is significantly faster over
the bottom-up computations, even when the cost
of computing the heuristic is included.
3.2 Lexicalized Parsing
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003b).
This model is constructed as the product of a
dependency model and the unlexicalized PCFG
model in Klein and Manning (2003a). We
964
0 2000 4000 6000 8000 10000
05
00
1500
2500
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
0 2000 4000 6000 8000 10000
05
00
1500
2500
EXH
k
Items 
pushed
 (milli
ons) K BestBottom-up
Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA? and EXH.
constructed these grammars using the Stanford
Parser.6 The model was trained on Sections 2-20
of the Penn Treebank and tested on 100 sentences
of Section 21 of length at most 30 words.
For this grammar, Klein and Manning (2003b)
showed that a very accurate heuristic can be con-
structed by taking the sum of outside scores com-
puted with the dependency model and the PCFG
model individually. We report performance as a
function of k for KA? in Figure 4. Both NAIVE
and EXH are impractical on these grammars due
to memory limitations. For KA?, computing the
heuristic is the bottleneck, after which bottom-up
parsing and k-best extraction are very fast.
3.3 Tree Transducer Grammars
Syntactic machine translation (Galley et al, 2004)
uses tree transducer grammars to translate sen-
tences. Transducer rules are synchronous context-
free productions that have both a source and a tar-
get side. We examine the cost of k-best parsing in
the source side of such grammars with KA?, which
can be a first step in translation.
We extracted a grammar from 220 million
words of Arabic-English bitext using the approach
of Galley et al (2006), extracting rules with at
most 3 non-terminals. These rules are highly lex-
icalized. About 300K rules are applicable for a
typical 30-word sentence; we filter the rest. We
tested on 100 sentences of length at most 40 from
the NIST05 Arabic-English test set.
We used a simple but effective heuristic for
these grammars, similar to the FILTER heuristic
suggested in Klein and Manning (2003c). We pro-
jected the source projection to a smaller grammar
by collapsing all non-terminal symbols to X, and
6http://nlp.stanford.edu/software/
also collapsing pre-terminals into related clusters.
For example, we collapsed the tags NN, NNS,
NNP, and NNPS to N. This projection reduced
the number of grammar symbols from 149 to 36.
Using it as a heuristic for the full grammar sup-
pressed ? 60% of the total items (Figure 5).
4 Related Work
While formulated very differently, one limiting
case of our algorithm relates closely to the EXH
algorithm of Huang and Chiang (2005). In par-
ticular, if all inside items are processed before any
derivation items, the subsequent number of deriva-
tion items and outside items popped by KA? is
nearly identical to the number popped by EXH in
our experiments (both algorithms have the same
ordering bounds on which derivation items are
popped). The only real difference between the al-
gorithms in this limited case is that EXH places
k-best items on local priority queues per edge,
while KA? makes use of one global queue. Thus,
in addition to providing a method for speeding
up k-best extraction with A?, our algorithm also
provides an alternate form of Huang and Chiang
(2005)?s k-best extraction that can be phrased in a
weighted deduction system.
5 Conclusions
We have presented KA?, an extension of A? pars-
ing that allows extraction of optimal k-best parses
without the need for an exhaustive 1-best pass. We
have shown in several domains that, with an ap-
propriate heuristic, our algorithm can extract k-
best lists in a fraction of the time required by cur-
rent approaches to k-best extraction, giving the
best of both A? parsing and efficient k-best extrac-
tion, in a unified procedure.
965
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning (ICML).
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Human Language Technologies: The An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
ACL).
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The
Annual Conference of the Association for Compu-
tational Linguistics (ACL).
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D.
thesis, Harvard University.
Aria Haghighi, John DeNero, and Dan Klein. 2007.
Approximate factoring for A* search. In Proceed-
ings of HLT-NAACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53?64.
Liang Huang. 2005. Unpublished manuscript.
http://www.cis.upenn.edu/?lhuang3/
knuth.pdf.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In IWPT, pages 123?134.
Dan Klein and Chris Manning. 2002. Fast exact in-
ference with a factored model for natural language
processing,. In Proceedings of NIPS.
Dan Klein and Chris Manning. 2003a. Accurate unlex-
icalized parsing. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).
Dan Klein and Chris Manning. 2003b. Factored A*
search for models over sequences and trees. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence (IJCAI).
Dan Klein and Christopher D. Manning. 2003c. A*
parsing: Fast exact Viterbi parse selection. In
In Proceedings of the Human Language Technol-
ogy Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
Donald Knuth. 1977. A generalization of Dijkstra?s
algorithm. Information Processing Letters, 6(1):1?
5.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 152?159.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computationl Linguis-
tics, 29(1):135?143.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. In Ma-
chine Learning, volume 34, pages 151?5175.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
966
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 141?144,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Asynchronous Binarization for Synchronous Grammars
John DeNero, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, adpauls, klein}@cs.berkeley.edu
Abstract
Binarization of n-ary rules is critical for the effi-
ciency of syntactic machine translation decoding.
Because the target side of a rule will generally
reorder the source side, it is complex (and some-
times impossible) to find synchronous rule bina-
rizations. However, we show that synchronous
binarizations are not necessary in a two-stage de-
coder. Instead, the grammar can be binarized one
way for the parsing stage, then rebinarized in a
different way for the reranking stage. Each indi-
vidual binarization considers only one monolin-
gual projection of the grammar, entirely avoid-
ing the constraints of synchronous binarization
and allowing binarizations that are separately op-
timized for each stage. Compared to n-ary for-
est reranking, even simple target-side binariza-
tion schemes improve overall decoding accuracy.
1 Introduction
Syntactic machine translation decoders search
over a space of synchronous derivations, scoring
them according to both a weighted synchronous
grammar and an n-gram language model. The
rewrites of the synchronous translation gram-
mar are typically flat, n-ary rules. Past work
has synchronously binarized such rules for effi-
ciency (Zhang et al, 2006; Huang et al, 2008).
Unfortunately, because source and target orders
differ, synchronous binarizations can be highly
constrained and sometimes impossible to find.
Recent work has explored two-stage decoding,
which explicitly decouples decoding into a source
parsing stage and a target language model inte-
gration stage (Huang and Chiang, 2007). Be-
cause translation grammars continue to increase
in size and complexity, both decoding stages re-
quire efficient approaches (DeNero et al, 2009).
In this paper, we show how two-stage decoding
enables independent binarizations for each stage.
The source-side binarization guarantees cubic-
time construction of a derivation forest, while an
entirely different target-side binarization leads to
efficient forest reranking with a language model.
Binarizing a synchronous grammar twice inde-
pendently has two principal advantages over syn-
chronous binarization. First, each binarization can
be fully tailored to its decoding stage, optimiz-
ing the efficiency of both parsing and language
model reranking. Second, the ITG constraint on
non-terminal reordering patterns is circumvented,
allowing the efficient application of synchronous
rules that do not have a synchronous binarization.
The primary contribution of this paper is to es-
tablish that binarization of synchronous grammars
need not be constrained by cross-lingual reorder-
ing patterns. We also demonstrate that even sim-
ple target-side binarization schemes improve the
search accuracy of forest reranking with a lan-
guage model, relative to n-ary forest reranking.
2 Asynchronous Binarization
Two-stage decoding consists of parsing and lan-
guage model integration. The parsing stage builds
a pruned forest of derivations scored by the trans-
lation grammar only. In the second stage, this for-
est is reranked by an n-gram language model. We
rerank derivations with cube growing, a lazy beam
search algorithm (Huang and Chiang, 2007).
In this paper, we focus on syntactic translation
with tree-transducer rules (Galley et al, 2006).
These synchronous rules allow multiple adjacent
non-terminals and place no restrictions on rule size
or lexicalization. Two example unlexicalized rules
appear in Figure 1, along with aligned and parsed
training sentences that would have licensed them.
2.1 Constructing Translation Forests
The parsing stage builds a forest of derivations by
parsing with the source-side projection of the syn-
chronous grammar. Each forest node P
ij
com-
pactly encodes all parse derivations rooted by
grammar symbol P and spanning the source sen-
tence from positions i to j. Each derivation of P
ij
is rooted by a rule with non-terminals that each
141
?PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
(a)
(b)
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
yo    ayer    com?    en casa
I       ate   yesterday   at home 
PRP  VBD       NN           PP
S
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
  NN
2
   PP
4
S ?
Figure 1: Two unlexicalized transducer rules (top) and
aligned, parsed training sentences from which they could be
extracted (bottom). The internal structure of English parses
has been omitted, as it is irrelevant to our decoding problem.
anchor to some child nodeC
(t)
k`
, where the symbol
C
(t)
is the tth child in the source side of the rule,
and i ? k < ` ? j.
We build this forest with a CKY-style algorithm.
For each span (i, j) from small to large, and each
symbol P , we iterate over all ways of building a
node P
ij
, first considering all grammar rules with
parent symbol P and then, for each rule, consider-
ing all ways of anchoring its non-terminals to ex-
isting forest nodes. Because we do not incorporate
a language model in this stage, we need only oper-
ate over the source-side projection of the grammar.
Of course, the number of possible anchorings
for a rule is exponential in the number of non-
terminals it contains. The purpose of binarization
during the parsing pass is to make this exponential
algorithm polynomial by reducing rule branching
to at most two non-terminals. Binarization reduces
algorithmic complexity by eliminating redundant
work: the shared substructures of n-ary rules are
scored only once, cached, and reused. Caching is
also commonplace in Early-style parsers that im-
plicitly binarize when applying n-ary rules.
While any binarization of the source side will
give a cubic-time algorithm, the particulars of a
grammar transformation can affect parsing speed
substantially. For instance, DeNero et al (2009)
describe normal forms particularly suited to trans-
ducer grammars, demonstrating that well-chosen
binarizations admit cubic-time parsing algorithms
while introducing very few intermediate grammar
symbols. Binarization choice can also improve
monolingual parsing efficiency (Song et al, 2008).
The parsing stage of our decoder proceeds
by first converting the source-side projection of
the translation grammar into lexical normal form
(DeNero et al, 2009), which allows each rule to
be applied to any span in linear time, then build-
ing a binary-branching translation forest, as shown
in Figure 2(a). The intermediate nodes introduced
during this transformation do not have a target-
side projection or interpretation. They only exist
for the sake of source-side parsing efficiency.
2.2 Collapsing Binarization
To facilitate a change in binarization, we transform
the translation forest into n-ary form. In the n-ary
forest, each hyperedge corresponds to an original
grammar rule, and all nodes correspond to original
grammar symbols, rather than those introduced
during binarizaiton. Transforming the entire for-
est to n-ary form is intractable, however, because
the number of hyperedges would be exponential in
n. Instead, we include only the top k n-ary back-
traces for each forest node. These backtraces can
be enumerated efficiently from the binary forest.
Figure 2(b) illustrates the result.
For efficiency, we follow DeNero et al (2009)
in pruning low-scoring nodes in the n-ary for-
est under the weighted translation grammar. We
use a max-marginal threshold to prune unlikely
nodes, which can be computed through a max-
sum semiring variant of inside-outside (Goodman,
1996; Petrov and Klein, 2007).
Forest reranking with a language model can be
performed over this n-ary forest using the cube
growing algorithm of Huang and Chiang (2007).
Cube growing lazily builds k-best lists of deriva-
tions at each node in the forest by filling a node-
specific priority queue upon request from the par-
ent. N -ary forest reranking serves as our baseline.
2.3 Reranking with Target-Side Binarization
Zhang et al (2006) demonstrate that reranking
over binarized derivations improves search accu-
racy by better exploring the space of translations
within the strict confines of beam search. Binariz-
ing the forest during reranking permits pairs of ad-
jacent non-terminals in the target-side projection
of rules to be rescored at intermediate forest nodes.
This target-side binarization can be performed on-
the-fly: when a node P
ij
is queried for its k-best
list, we binarize its n-ary backtraces.
Suppose P
ij
can be constructed from a rule r
with target-side projection
P ? `
0
C
1
`
1
C
2
`
2
. . . C
n
`
n
where C
1
, . . . , C
n
are non-terminal symbols that
are each anchored to a nodeC
(i)
kl
in the forest, and
`
i
are (possibly empty) sequences of lexical items.
142
yo ayer com? en casa
S
PRP+NN+VBD
PRP+NN
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
PRP+VBD+NN
PRP+VBD
?I ate?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    NN
2
]  PP
4
S ?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    PP
4
]  NN
2
S ?
(a) Parsing stage binarization (b) Collapsed n-ary forest (c) Reranking stage binarization
PRP+VBD+PP
Figure 2: A translation forest as it evolves during two-stage decoding, along with two n-ary rules in the forest that are rebi-
narized. (a) A source-binarized forest constructed while parsing the source sentence with the translation grammar. (b) A flat
n-ary forest constructed by collapsing out the source-side binarization. (c) A target-binarized forest containing two derivations
of the root symbol?the second is dashed for clarity. Both derivations share the node PRP+VBD, which will contain a single
k-best list of translations during language model reranking. One such translation of PRP+VBD is shown: ?I ate?.
We apply a simple left-branching binarization to
r, though in principle any binarization is possible.
We construct a new symbol B and two new rules:
r
1
: B ? `
0
C
1
`
1
C
2
`
2
r
2
: P ? B C
3
`
3
. . . C
n
`
n
These rules are also anchored to forest nodes. Any
C
i
remains anchored to the same node as it was in
the n-ary forest. For the new symbol B, we intro-
duce a new forest nodeB that does not correspond
to any particular span of the source sentence. We
likewise transform the resulting r
2
until all rules
have at most two non-terminal items. The original
rule r from the n-ary forest is replaced by binary
rules. Figure 2(c) illustrates the rebinarized forest.
Language model reranking treats the newly in-
troduced forest nodeB as any other node: building
a k-best derivation list by combining derivations
from C
(1)
and C
(2)
using rule r
1
. These deriva-
tions are made available to the parent of B, which
may be another introduced node (if more binariza-
tion were required) or the original root P
ij
.
Crucially, the ordering of non-terminals in the
source-side projection of r does not play a role
in this binarization process. The intermediate
nodes B may comprise translations of discontigu-
ous parts of the source sentence, as long as those
parts are contained within the span (i, j).
2.4 Reusing Intermediate Nodes
The binarization we describe transforms the for-
est on a rule-by-rule basis. We must consider in-
dividual rules because they may contain different
lexical items and non-terminal orderings. How-
ever, two different rules that can build a node often
share some substructures. For instance, the two
rules in Figure 2 both begin with PRP followed by
VBD. In addition, these symbols are anchored to
the same source-side spans. Thus, binarizing both
rules yields the same intermediate forest node B.
In the case where two intermediate nodes share
the same intermediate rule anchored to the same
forest nodes, they can be shared. That is, we need
only generate one k-best list of derivations, then
use it in derivations rooted by both rules. Sharing
derivation lists in this way provides an additional
advantage of binarization over n-ary forest rerank-
ing. Not only do we assess language model penal-
ties over smaller partial derivations, but repeated
language model evaluations are cached and reused
across rules with common substructure.
3 Experiments
The utility of binarization for parsing is well
known, and plays an important role in the effi-
ciency of the parsing stage of decoding (DeNero et
al., 2009). The benefit of binarization for language
143
Forest Reranked BLEU Model Score
N -ary baseline 58.2 41,543
Left-branching binary 58.5 41,556
Table 1: Reranking a binarized forest improves BLEU by 0.3
and model score by 13 relative to an n-ary forest baseline by
reducing search errors during forest rescoring.
model reranking has also been established, both
for synchronous binarization (Zhang et al, 2006)
and for target-only binarization (Huang, 2007). In
our experiment, we evaluate the benefit of target-
side forest re-binarization in the two-stage decoder
of DeNero et al (2009), relative to reranking n-ary
forests directly.
We translated 300 NIST 2005 Arabic sentences
to English with a large grammar learned from a
220 million word bitext, using rules with up to 6
non-terminals. We used a trigram language model
trained on the English side of this bitext. Model
parameters were tuned withMERT. Beam size was
limited to 200 derivations per forest node.
Table 1 shows a modest increase in model
and BLEU score from left-branching binarization
during language model reranking. We used the
same pruned n-ary forest from an identical parsing
stage in both conditions. Binarization did increase
reranking time by 25% because more k-best lists
are constructed. However, reusing intermediate
edges during reranking binarization reduced bina-
rized reranking time by 37%. We found that on
average, intermediate nodes introduced in the for-
est are used in 4.5 different rules, which accounts
for the speed increase.
4 Discussion
Asynchronous binarization in two-stage decoding
allows us to select an appropriate grammar trans-
formation for each language. The source trans-
formation can optimize specifically for the parsing
stage of translation, while the target-side binariza-
tion can optimize for the reranking stage.
Synchronous binarization is of course a way to
get the benefits of binarizing both grammar pro-
jections; it is a special case of asynchronous bi-
narization. However, synchronous binarization is
constrained by the non-terminal reordering, lim-
iting the possible binarization options. For in-
stance, none of the binarization choices used in
Figure 2 on either side would be possible in a
synchronous binarization. There are rules, though
rare, that cannot be binarized synchronously at all
(Wu, 1997), but can be incorporated in two-stage
decoding with asynchronous binarization.
On the source side, these limited binarization
options may, for example, prevent a binarization
that minimizes intermediate symbols (DeNero et
al., 2009). On the target side, the speed of for-
est reranking depends upon the degree of reuse
of intermediate k-best lists, which in turn depends
upon the manner in which the target-side grammar
projection is binarized. Limiting options may pre-
vent a binarization that allows intermediate nodes
to be maximally reused. In future work, we look
forward to evaluating the wide array of forest bi-
narization strategies that are enabled by our asyn-
chronous approach.
References
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of the Annual Conference of the North American
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syn-
tactic translation models. In Proceedings of the Annual
Conference of the Association for Computational Linguis-
tics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In Pro-
ceedings of the Annual Conference of the Association for
Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight.
2008. Binarization of synchronous context-free gram-
mars. Computational Linguistics.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proceedings of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST).
Slav Petrov and Dan Klein. 2007. Improved inference for un-
lexicalized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguistics.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better
binarization for the CKY parsing. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation.
In Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics.
144
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 23?27,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Core-Tools Statistical NLP Course
Dan Klein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
In the fall term of 2004, I taught a
new statistical NLP course focusing
on core tools and machine-learning al-
gorithms. The course work was or-
ganized around four substantial pro-
gramming assignments in which the
students implemented the important
parts of several core tools, including
language models (for speech rerank-
ing), a maximum entropy classifier, a
part-of-speech tagger, a PCFG parser,
and a word-alignment system. Using
provided scaffolding, students built re-
alistic tools with nearly state-of-the-
art performance in most cases. This
paper briefly outlines the coverage of
the course, the scope of the assign-
ments, and some of the lessons learned
in teaching the course in this way.
1 Introduction
In the fall term of 2004, I taught a new sta-
tistical NLP course at UC Berkeley which cov-
ered the central tools and machine-learning ap-
proaches of NLP. My goal in formulating this
course was to create a syllabus and assignment
set to teach in a relatively short time the impor-
tant aspects, both practical and theoretical, of
what took me years of building research tools to
internalize. The result was a rather hard course
with a high workload. Although the course eval-
uations were very positive, and several of the
students who completed the course were able to
jump right into research projects in my group,
there?s no question that the broad accessibility
of the course, especially for non-CS students,
was limited.
As with any NLP course, there were several
fundamental choice points. First, it?s not possi-
ble to cover both core tools and end-to-end ap-
plications in detail in a single term. Since Marti
Hearst was teaching an applied NLP course dur-
ing the same term, I chose to cover tools and
algorithms almost exclusively (see figure 1 for a
syllabus). The second choice point was whether
to organize the course primarily around linguis-
tic topics or primarily around statistical meth-
ods. I chose to follow linguistic topics because
that order seemed much easier to motivate to the
students (comments on this choice in section 3).
The final fundamental choice I made in decid-
ing how to target this class was to require both
substantial coding and substantial math. This
choice narrowed the audience of the class, but
allowed the students to build realistic systems
which were not just toy implementations.
I feel that the most successful aspect of
this course was the set of assignments, so the
largest section below will be devoted to de-
scribing them. If other researchers are inter-
ested in using any of my materials, they are en-
couraged to contact me or visit my web page
(http://www.cs.berkeley.edu/~klein).
2 Audience
The audience of the class began as a mix of CS
PhD students (mostly AI but some systems stu-
dents), some linguistics graduate students, and
23
a few advanced CS undergrads. What became
apparent after the first homework assignment
(see section 4.2) was that while the CS students
could at least muddle through the course with
weak (or absent) linguistics backgrounds, the
linguistics students were unable to acquire the
math and programming skills quickly enough to
keep up. I have no good ideas about how to ad-
dress this issue. Moreover, even among the CS
students, some of the systems students had trou-
ble with the math and some of the AI/theory
students had issues with coding scalable solu-
tions. The course was certainly not optimized
for broad accessibility, but the approximately
80% of students who stuck it out did what I con-
sidered to be extremely impressive work. For
example, one student built a language model
which took the mass reserved for new words
and distributed it according to a character n-
gram model. Another student invented a non-
iterative word alignment heuristic which out-
performed IBM model 4 on small and medium
training corpora. A third student built a maxent
part-of-speech tagger with a per-word accuracy
of 96.7%, certainly in the state-of-the-art range.
3 Topics
The topics covered in the course are shown in
figure 1. The first week of the course was es-
sentially a history lesson about symbolic ap-
proaches NLP, both to show their strengths (a
full, unified pipeline including predicate logic se-
mantic interpretations, while we still don?t have
a good notion of probabilistic interpretation)
and their weaknesses (many interpretations arise
from just a few rules, ambiguity poorly han-
dled). From there, I discussed statistical ap-
proaches to problems of increasing complexity,
spending a large amount of time on tree and se-
quence models.
As mentioned above, I organized the lectures
around linguistic topics rather than mathemat-
ical methods. However, given the degree to
which the course focused on such foundational
methods, this order was perhaps a mistake. For
example, it meant that simple word alignment
models like IBM models 1 and 2 (Brown et
al., 1990) and the HMM model (Vogel et al,
1996) came many weeks after HMMs were intro-
duced in the context of part-of-speech tagging.
I also separated unsupervised learning into its
own sub-sequence, where I now wish I had pre-
sented the unsupervised approaches to each task
along with the supervised ones.
I assigned readings from Jurafsky and Mar-
tin (2000) and Manning and Schu?tze (1999) for
the first half of the course, but the second half
was almost entirely based on papers from the re-
search literature. This reflected both increasing
sophistication on the part of the students and
insufficient coverage of the latter topics in the
textbooks.
4 Assignments
The key component which characterized this
course was the assignments. Each assignment
is described below. They are available for
use by other instructors. While licensing
issues with the data make it impossible to put
the entirety of the assignment materials on
the web, some materials will be linked from
http://www.cs.berkeley.edu/~klein, and
the rest can be obtained by emailing me.
4.1 Assignment Principles
The assignments were all in Java. In all cases,
I supplied a large amount of scaffolding code
which read in the appropriate data files, con-
structed a placeholder baseline system, and
tested that baseline. The students therefore al-
ways began with a running end-to-end pipeline,
using standard corpora, evaluated in standard
ways. They then swapped out the baseline
placeholder for increasingly sophisticated imple-
mentations. When possible, assignments also
had a toy ?miniTest? mode where rather than
reading in real corpora, a small toy corpus was
loaded to facilitate debugging. Assignments
were graded entirely on the basis of write-ups.
4.2 Assignment 1: Language Modeling
In the first assignment, students built n-gram
language models using WSJ data. Their lan-
guage models were evaluated in three ways by
24
Topics Techniques Lectures
Classical NLP Chart Parsing, Semantic Interpretation 2
Speech and Language Modeling Smoothing 2
Text Categorization Naive-Bayes Models 1
Word-Sense Disambiguation Maximum Entropy Models 1
Part-of-Speech Tagging HMMs and MEMMs 1
Part-of-Speech Tagging CRFs 1
Statistical Parsing PCFGs 1
Statistical Parsing Inference for PCFGs 1
Statistical Parsing Grammar Representations 1
Statistical Parsing Lexicalized Dependency Models 1
Statistical Parsing Other Parsing Models 1
Semantic Representation 2
Information Extraction 1
Coreference 1
Machine Translation Word-to-Word Alignment Models 1
Machine Translation Decoding Word-to-Word Models 1
Machine Translation Syntactic Translation Models 1
Unsupervised Learning Document Clustering 1
Unsupervised Learning Word-Level Clustering 1
Unsupervised Learning Grammar Induction 2
Question Answering 1
Document Summarization 1
Figure 1: Topics Covered. Each lecture was 80 minutes.
the support harness. First, perplexity on held-
out WSJ text was calculated. In this evaluation,
reserving the correct mass for unknown words
was important. Second, their language models
were used to rescore n-best speech lists (supplied
by Brian Roark, see Roark (2001)). Finally, ran-
dom sentences were generatively sampled from
their models, giving students concrete feedback
on how their models did (or did not) capture in-
formation about English. The support code in-
tially provided an unsmoothed unigram model
to get students started. They were then asked
to build several more complex language mod-
els, including at least one higher-order interpo-
lated model, and at least one model using Good-
Turing or held-out smoothing. Beyond these re-
quirements, students were encouraged to acheive
the best possible word error rate and perplexity
figures by whatever means they chose.1 They
were also asked to identify ways in which their
language models missed important trends of En-
1After each assignment, I presented in class an hon-
ors list, consisting of the students who won on any mea-
sure or who had simply built something clever. I initially
worried about how these honors announcements would
be received, but students really seemed to enjoy hearing
what their peers were doing, and most students made the
honors list at some point in the term.
glish and to suggest solutions.
As a second part to assignment 1, students
trained class-conditional n-gram models (at the
character level) to do the proper name identi-
fication task from Smarr and Manning (2002)
(whose data we used). In this task, proper name
strings are to be mapped to one of {drug, com-
pany, movie, person, location}. This turns
out to be a fairly easy task since the different
categories have markedly different character dis-
tributions.2 In the future, I will move this part
of assignment 1 and the matching part of assign-
ment 2 into a new, joint assignment.
4.3 Assignment 2: Maximum Entropy /
POS Tagging
In assignment 2, students first built a general
maximum entropy model for multiclass classi-
fication. The support code provided a crippled
maxent classifier which always returned the uni-
form distribution over labels (by ignoring the
features of the input datum). Students replaced
the crippled bits and got a correct classifier run-
2This assignment could equally well have been done
as a language identification task, but the proper name
data was convenient and led to fun error analysis, since
in good systems the errors are mostly places named after
people, movies with place names as titles, and so on.
25
ning, first on a small toy problem and then on
the proper-name identification problem from as-
signment 1. The support code provided opti-
mization code (an L-BFGS optimizer) and fea-
ture indexing machinery, so students only wrote
code to calculate the maxent objective function
and its derivatives.
The original intention of assignment 2 was
that students then use this maxent classifier as a
building block of a maxent part-of-speech tagger
like that of Ratnaparkhi (1996). The support
code supplied a most-frequent-tag baseline tag-
ger and a greedy lattice decoder. The students
first improved the local scoring function (keep-
ing the greedy decoder) using either an HMM
or maxent model for each timeslice. Once this
was complete, they upgraded the greedy decoder
to a Viterbi decoder. Since students were, in
practice, generally only willing to wait about 20
minutes for an experiment to run, most chose to
discard their maxent classifiers and build gener-
ative HMM taggers. About half of the students?
final taggers exceeded 96% per-word tagging ac-
curacy, which I found very impressive. Students
were only required to build a trigram tagger
of some kind. However, many chose to have
smoothed HMMs with complex emission mod-
els like Brants (2000), while others built maxent
taggers.
Because of the slowness of maxent taggers?
training, I will just ask students to build HMM
taggers next time. Moreover, with the relation
between the two parts of this assignment gone, I
will separate out the proper-name classification
part into its own assignment.
4.4 Assignment 3: Parsing
In assignment 3, students wrote a probabilis-
tic chart parser. The support code read in
and normalized Penn Treebank trees using the
standard data splits, handled binarization of n-
ary rules, and calculated ParsEval numbers over
the development or test sets. A baseline left-
branching parser was provided. Students wrote
an agenda-based uniform-cost parser essentially
from scratch. Once the parser parsed cor-
rectly with the supplied treebank grammar, stu-
dents experimented with horizontal and vertical
markovization (see Klein and Manning (2003))
to improve parsing accuracy. Students were
then free to experiment with speed-ups to the
parser, more complex annotation schemes, and
so on. Most students? parsers ran at reasonable
speeds (around a minute for 40 word sentences)
and got final F1 measures over 82%, which is
substantially higher than an unannotated tree-
bank grammar will produce. While this assign-
ment would appear to be more work than the
others, it actually got the least overload-related
complaints of all the assignments.
In the future, I may instead have students im-
plement an array-based CKY parser (Kasami,
1965), since a better understanding of CKY
would have been more useful than knowing
about agenda-based methods for later parts of
the course. Moreover, several students wanted
to experiment with induction methods which
required summing parsers instead of Viterbi
parsers.
4.5 Assignment 4: Word Alignment
In assignment 4, students built word alignment
systems using the Canadian Hansards training
data and evaluation alignments from the 2003
(and now 2005) shared task in the NAACL
workshop on parallel texts. The support code
provided a monotone baseline aligner and eval-
uation/display code which graphically printed
gold alignments superimposed over guessed
alignments. Students first built a heuristic
aligner (Dice, mutual information-based, or
whatever they could invent) and then built IBM
model 1 and 2 aligners. They then had a choice
of either scaling up the system to learn from
larger training sets or implementing the HMM
alignment model.
4.6 Assignment Observations
For all the assignments, I stressed that the stu-
dents should spend a substantial amount of time
doing error analysis. However, most didn?t, ex-
cept for in assignment 2, where the support code
printed out every error their taggers made, by
default. For this assignment, students actually
provided very good error analysis. In the fu-
ture, I will increase the amount of verbose er-
26
ror output to encourage better error analysis for
the other assignments ? it seemed like students
were reluctant to write code to display errors,
but were happy to look at errors as they scrolled
by.3
A very important question raised by an
anonymous reviewer was how effectively imple-
menting tried-and-true methods feeds into new
research. For students who will not be do-
ing NLP research but want to know how the
basic methods work (realistically, this is most
of the audience), the experience of having im-
plemented several ?classic? approaches to core
tools is certainly appropriate. However, even
for students who intend to do NLP research,
this hands-on tour of established methods has
already shown itself to be very valuable. These
students can pick up any paper on any of these
tasks, and they have a very concrete idea about
what the data sets look like, why people do
things they way they do, and what kinds of er-
ror types and rates one can expect from a given
tool. That?s experience that can take a long time
to acquire otherwise ? it certainly took me a
while. Moreover, I?ve had several students from
the class start research projects with me, and,
in each case, those projects have been in some
way bridged by the course assignments. This
methodology also means that all of the students
working with me have a shared implementation
background, which has facilitated ad hoc collab-
orations on research projects.
5 Conclusions
There are certainly changes I will make when I
teach this course again this fall. I will likely
shuffle the topics around so that word align-
ment comes earlier (closer to HMMs for tagging)
and I will likely teach dynamic programming so-
lutions to parsing and tagging in more depth
than graph-search based methods. Some stu-
dents needed remedial linguistics sections and
other students needed remedial math sections,
and I would hold more such sessions, and ear-
3There was also verbose error reporting for assign-
ment 4, which displayed each sentence?s guessed and gold
alignments in a grid, but since most students didn?t speak
French, this didn?t have the same effect.
lier in the term. However, I will certainly keep
the substantial implementation component of
the course, partially in response to very positive
student feedback on the assignments, partially
from my own reaction to the high quality of stu-
dent work on those assignments, and partially
from how easily students with so much hands-
on experience seem to be able to jump into NLP
research.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In ANLP 6, pages 224?231.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
Dan Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics and Speech Recognition. Prentice Hall, Engle-
wood Cliffs, NJ.
T. Kasami. 1965. An efficient recognition and syn-
tax analysis algorithm for context-free languages.
Technical Report AFCRL-65-758, Air Force Cam-
bridge Research Laboratory, Bedford, MA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In ACL 41, pages
423?430.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In EMNLP 1,
pages 133?142.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27:249?276.
Joseph Smarr and Christopher D. Manning. 2002.
Classifying unknown proper noun phrases without
context. Technical report, Stanford University.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. In COLING 16, pages 836?
841.
27
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 14?20, New York City, June 2006. c?2006 Association for Computational Linguistics
Non-Local Modeling with a Mixture of PCFGs
Slav Petrov Leon Barrett Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, lbarrett, klein}@eecs.berkeley.edu
Abstract
While most work on parsing with PCFGs
has focused on local correlations between
tree configurations, we attempt to model
non-local correlations using a finite mix-
ture of PCFGs. A mixture grammar fit
with the EM algorithm shows improve-
ment over a single PCFG, both in parsing
accuracy and in test data likelihood. We
argue that this improvement comes from
the learning of specialized grammars that
capture non-local correlations.
1 Introduction
The probabilistic context-free grammar (PCFG) for-
malism is the basis of most modern statistical
parsers. The symbols in a PCFG encode context-
freedom assumptions about statistical dependencies
in the derivations of sentences, and the relative con-
ditional probabilities of the grammar rules induce
scores on trees. Compared to a basic treebank
grammar (Charniak, 1996), the grammars of high-
accuracy parsers weaken independence assumptions
by splitting grammar symbols and rules with ei-
ther lexical (Charniak, 2000; Collins, 1999) or non-
lexical (Klein and Manning, 2003; Matsuzaki et al,
2005) conditioning information. While such split-
ting, or conditioning, can cause problems for sta-
tistical estimation, it can dramatically improve the
accuracy of a parser.
However, the configurations exploited in PCFG
parsers are quite local: rules? probabilities may de-
pend on parents or head words, but do not depend
on arbitrarily distant tree configurations. For exam-
ple, it is generally not modeled that if one quantifier
phrase (QP in the Penn Treebank) appears in a sen-
tence, the likelihood of finding another QP in that
same sentence is greatly increased. This kind of ef-
fect is neither surprising nor unknown ? for exam-
ple, Bock and Loebell (1990) show experimentally
that human language generation demonstrates prim-
ing effects. The mediating variables can not only in-
clude priming effects but also genre or stylistic con-
ventions, as well as many other factors which are not
adequately modeled by local phrase structure.
A reasonable way to add a latent variable to a
generative model is to use a mixture of estimators,
in this case a mixture of PCFGs (see Section 3).
The general mixture of estimators approach was first
suggested in the statistics literature by Titterington
et al (1962) and has since been adopted in machine
learning (Ghahramani and Jordan, 1994). In a mix-
ture approach, we have a new global variable on
which all PCFG productions for a given sentence
can be conditioned. In this paper, we experiment
with a finite mixture of PCFGs. This is similar to the
latent nonterminals used in Matsuzaki et al (2005),
but because the latent variable we use is global, our
approach is more oriented toward learning non-local
structure. We demonstrate that a mixture fit with the
EM algorithm gives improved parsing accuracy and
test data likelihood. We then investigate what is and
is not being learned by the latent mixture variable.
While mixture components are difficult to interpret,
we demonstrate that the patterns learned are better
than random splits.
2 Empirical Motivation
It is commonly accepted that the context freedom
assumptions underlying the PCFG model are too
14
VP
VBD
increased
NP
CD
11
NN
%
PP
TO
to
NP
QP
#
#
CD
2.5
CD
billion
PP
IN
from
NP
QP
#
#
CD
2.25
CD
billion
Rule Score
QP? # CD CD 131.6
PRN? -LRB- ADJP -RRB 77.1
VP? VBD NP , PP PP 33.7
VP? VBD NP NP PP 28.4
PRN? -LRB- NP -RRB- 17.3
ADJP? QP 13.3
PP? IN NP ADVP 12.3
NP? NP PRN 12.3
VP? VBN PP PP PP 11.6
ADVP? NP RBR 10.1
Figure 1: Self-triggering: QP? # CD CD. If one British financial occurs in the sentence, the probability of
seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation
for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most
increased in a sentence containing this rule.
strong and that weakening them results in better
models of language (Johnson, 1998; Gildea, 2001;
Klein and Manning, 2003). In particular, certain
grammar productions often cooccur with other pro-
ductions, which may be either near or distant in the
parse tree. In general, there exist three types of cor-
relations: (i) local (e.g. parent-child), (ii) non-local,
and (iii) self correlations (which may be local or
non-local).
In order to quantify the strength of a correlation,
we use a likelihood ratio (LR). For two rules X? ?
and Y? ?, we compute
LR(X? ?, Y? ?) = P(?, ?|X,Y )P(?|X,Y )P(?|X,Y )
This measures how much more often the rules oc-
cur together than they would in the case of indepen-
dence. For rules that are correlated, this score will
be high (? 1); if the rules are independent, it will
be around 1, and if they are anti-correlated, it will be
near 0.
Among the correlations present in the Penn Tree-
bank, the local correlations are the strongest ones;
they contribute 65% of the rule pairs with LR scores
above 90 and 85% of those with scores over 200.
Non-local and self correlations are in general com-
mon but weaker, with non-local correlations con-
tributing approximately 85% of all correlations1 . By
adding a latent variable conditioning all productions,
1Quantifying the amount of non-local correlation is prob-
lematic; most pairs of cooccuring rules are non-local and will,
due to small sample effects, have LR ratios greater than 1 even
if they were truly independent in the limit.
we aim to capture some of this interdependence be-
tween rules.
Correlations at short distances have been cap-
tured effectively in previous work (Johnson, 1998;
Klein and Manning, 2003); vertical markovization
(annotating nonterminals with their ancestor sym-
bols) does this by simply producing a different dis-
tribution for each set of ancestors. This added con-
text leads to substantial improvement in parsing ac-
curacy. With local correlations already well cap-
tured, our main motivation for introducing a mix-
ture of grammars is to capture long-range rule cooc-
currences, something that to our knowledge has not
been done successfully in the past.
As an example, the rule QP? # CD CD, rep-
resenting a quantity of British currency, cooc-
curs with itself 132 times as often as if oc-
currences were independent. These cooccur-
rences appear in cases such as seen in Figure 1.
Similarly, the rules VP? VBD NP PP , S and
VP? VBG NP PP PP cooccur in the Penn Tree-
bank 100 times as often as we would expect if they
were independent. They appear in sentences of a
very particular form, telling of an action and then
giving detail about it; an example can be seen in Fig-
ure 2.
3 Mixtures of PCFGs
In a probabilistic context-free grammar (PCFG),
each rule X? ? is associated with a conditional
probability P(?|X) (Manning and Schu?tze, 1999).
Together, these rules induce a distribution over trees
P(T ). A mixture of PCFGs enriches the basic model
15
VP
VBD
hit
NP
a record
PP
in 1998
,
,
S
VP
VBG
rising
NP
1.7%
PP
after inflation adjustment
PP
to $13,120
S
NP
DT
No
NX
NX
NNS
lawyers
CC
or
NX
NN
tape
NNS
recorders
VP
were present
.
.
(a) (b)
S
S
NP
DT
These
NN
rate
NNS
indications
VP
VBP
are
RB
n?t
ADJP
directly comparable
:
;
S
NP
NN
lending
NNS
practices
VP
VBP
vary
ADVP
widely
PP
by location
.
.
X
X
SYM
**
ADJP
VBN
Projected
(c) (d)
Figure 2: Tree fragments demonstrating coocurrences. (a) and (c) Repeated formulaic structure in one
grammar: rules VP? VBD NP PP , S and VP? VBG NP PP PP and rules VP? VBP RB ADJP
and VP? VBP ADVP PP. (b) Sibling effects, though not parallel structure, rules: NX? NNS and
NX? NN NNS. (d) A special structure for footnotes has rules ROOT? X and X? SYM coocurring
with high probability.
by allowing for multiple grammars, Gi, which we
call individual grammars, as opposed to a single
grammar. Without loss of generality, we can as-
sume that the individual grammars share the same
set of rules. Therefore, each original rule X? ?
is now associated with a vector of probabilities,
P(?|X, i). If, in addition, the individual grammars
are assigned prior probabilities P(i), then the entire
mixture induces a joint distribution over derivations
P(T, i) = P(i)P(T |i) from which we recover a dis-
tribution over trees by summing over the grammar
index i.
As a generative derivation process, we can think
of this in two ways. First, we can imagine G to be
a latent variable on which all productions are con-
ditioned. This view emphasizes that any otherwise
unmodeled variable or variables can be captured by
the latent variable G. Second, we can imagine se-
lecting an individual grammar Gi and then gener-
ating a sentence using that grammar. This view is
associated with the expectation that there are multi-
ple grammars for a language, perhaps representing
different genres or styles. Formally, of course, the
two views are the same.
3.1 Hierarchical Estimation
So far, there is nothing in the formal mixture model
to say that rule probabilities in one component have
any relation to those in other components. However,
we have a strong intuition that many rules, such as
NP? DT NN, will be common in all mixture com-
ponents. Moreover, we would like to pool our data
across components when appropriate to obtain more
reliable estimators.
This can be accomplished with a hierarchical es-
timator for the rule probabilities. We introduce a
shared grammar Gs. Associated to each rewrite is
now a latent variable L = {S, I} which indicates
whether the used rule was derived from the shared
grammar Gs or one of the individual grammars Gi:
P(?|X, i) =
?P(?|X, i, ?= I) + (1? ?)P(?|X, i, ?= S),
where ? ? P (? = I) is the probability of
choosing the individual grammar and can also
be viewed as a mixing coefficient. Note that
P(?|X, i, ?= S) = P(?|X, ?= S), since the shared
grammar is the same for all individual grammars.
This kind of hierarchical estimation is analogous to
that used in hierarchical mixtures of naive-Bayes for
16
text categorization (McCallum et al, 1998).
The hierarchical estimator is most easily de-
scribed as a generative model. First, we choose a
individual grammar Gi. Then, for each nonterminal,
we select a level from the back-off hierarchy gram-
mar: the individual grammar Gi with probability ?,
and the shared grammar Gs with probability 1 ? ?.
Finally, we select a rewrite from the chosen level. To
emphasize: the derivation of a phrase-structure tree
in a hierarchically-estimated mixture of PCFGs in-
volves two kinds of hidden variables: the grammar
G used for each sentence, and the level L used at
each tree node. These hidden variables will impact
both learning and inference in this model.
3.2 Inference: Parsing
Parsing involves inference for a given sentence S.
One would generally like to calculate the most prob-
able parse ? that is, the tree T which has the high-
est probability P(T |S) ??i P(i)P(T |i). How-
ever, this is difficult for mixture models. For a single
grammar we have:
P(T, i) = P(i)
?
X???T
P(?|X, i).
This score decomposes into a product and it is sim-
ple to construct a dynamic programming algorithm
to find the optimal T (Baker, 1979). However, for a
mixture of grammars we need to sum over the indi-
vidual grammars:
?
i
P(T, i) =
?
i
P(i)
?
X???T
P(?|X, i).
Because of the outer sum, this expression unfor-
tunately does not decompose into a product over
scores of subparts. In particular, a tree which maxi-
mizes the sum need not be a top tree for any single
component.
As is true for many other grammar formalisms in
which there is a derivation / parse distinction, an al-
ternative to finding the most probable parse is to find
the most probable derivation (Vijay-Shankar and
Joshi, 1985; Bod, 1992; Steedman, 2000). Instead
of finding the tree T which maximizes
?
i P(T, i),
we find both the tree T and component i which max-
imize P(T, i). The most probable derivation can be
found by simply doing standard PCFG parsing once
for each component, then comparing the resulting
trees? likelihoods.
3.3 Learning: Training
Training a mixture of PCFGs from a treebank is an
incomplete data problem. We need to decide which
individual grammar gave rise to a given observed
tree. Moreover, we need to select a generation path
(individual grammar or shared grammar) for each
rule in the tree. To learn estimate parameters, we
can use a standard Expectation-Maximization (EM)
approach.
In the E-step, we compute the posterior distribu-
tions of the latent variables, which are in this case
both the component G of each sentence and the hier-
archy level L of each rewrite. Note that, unlike dur-
ing parsing, there is no uncertainty over the actual
rules used, so the E-step does not require summing
over possible trees. Specifically, for the variable G
we have
P(i|T ) = P(T, i)?
j P(T, j)
.
For the hierarchy level L we can write
P(? = I|X ? ?, i, T ) =
?P(?|X, ?= I)
?P(?|X, i, ?= I) + (1? ?)P(?|X, ?= S) ,
where we slightly abuse notation since the rule
X ? ? can occur multiple times in a tree T.
In the M-step, we find the maximum-likelihood
model parameters given these posterior assign-
ments; i.e., we find the best grammars given the way
the training data?s rules are distributed between in-
dividual and shared grammars. This is done exactly
as in the standard single-grammar model using rela-
tive expected frequencies. The updates are shown in
Figure 3.3, where T = {T1, T2, . . . } is the training
set.
We initialize the algorithm by setting the assign-
ments from sentences to grammars to be uniform
between all the individual grammars, with a small
random perturbation to break symmetry.
4 Results
We ran our experiments on the Wall Street Jour-
nal (WSJ) portion of the Penn Treebank using the
standard setup: We trained on sections 2 to 21,
and we used section 22 as a validation set for tun-
ing model hyperparameters. Results are reported
17
P(i)?
?
Tk?T P(i|Tk)
?
i
?
Tk?T P(i|Tk)
=
P
Tk?T
P(i|Tk)
k
P(l = I)?
?
Tk?T
?
X???Tk P(? = I|X ? ?)
?
Tk?T |Tk|
P(?|X, i, ? = I)?
?
Tk?T
?
X???Tk P(i|Tk)P(? = I|Tk, i,X ? ?)
?
??
?
Tk?T
?
X????Tk P(i|Tk)P(? = I|Tk, i,X ? ??)
Figure 3: Parameter updates. The shared grammar?s parameters are re-estimated in the same manner.
on all sentences of 40 words or less from section
23. We use a markovized grammar which was an-
notated with parent and sibling information as a
baseline (see Section 4.2). Unsmoothed maximum-
likelihood estimates were used for rule probabili-
ties as in Charniak (1996). For the tagging proba-
bilities, we used maximum-likelihood estimates for
P(tag|word). Add-one smoothing was applied to
unknown and rare (seen ten times or less during
training) words before inverting those estimates to
give P(word|tag). Parsing was done with a sim-
ple Java implementation of an agenda-based chart
parser.
4.1 Parsing Accuracy
The EM algorithm is guaranteed to continuously in-
crease the likelihood on the training set until conver-
gence to a local maximum. However, the likelihood
on unseen data will start decreasing after a number
of iterations, due to overfitting. This is demonstrated
in Figure 4. We use the likelihood on the validation
set to stop training before overfitting occurs.
In order to evaluate the performance of our model,
we trained mixture grammars with various numbers
of components. For each configuration, we used EM
to obtain twelve estimates, each time with a different
random initialization. We show the F1-score for the
model with highest log-likelihood on the validation
set in Figure 4. The results show that a mixture of
grammars outperforms a standard, single grammar
PCFG parser.2
4.2 Capturing Rule Correlations
As described in Section 2, we hope that the mix-
ture model will capture long-range correlations in
2This effect is statistically significant.
the data. Since local correlations can be captured
by adding parent annotation, we combine our mix-
ture model with a grammar in which node probabil-
ities depend on the parent (the last vertical ancestor)
and the closest sibling (the last horizontal ancestor).
Klein and Manning (2003) refer to this grammar as
a markovized grammar of vertical order = 2 and hor-
izontal order = 1. Because many local correlations
are captured by the markovized grammar, there is a
greater hope that observed improvements stem from
non-local correlations.
In fact, we find that the mixture does capture
non-local correlations. We measure the degree to
which a grammar captures correlations by calculat-
ing the total squared error between LR scores of the
grammar and corpus, weighted by the probability
of seeing nonterminals. This is 39422 for a sin-
gle PCFG, but drops to 37125 for a mixture with
five individual grammars, indicating that the mix-
ture model better captures the correlations present
in the corpus. As a concrete example, in the Penn
Treebank, we often see the rules FRAG? ADJP
and PRN? , SBAR , cooccurring; their LR is 134.
When we learn a single markovized PCFG from the
treebank, that grammar gives a likelihood ratio of
only 61. However, when we train with a hierarchi-
cal model composed of a shared grammar and four
individual grammars, we find that the grammar like-
lihood ratio for these rules goes up to 126, which is
very similar to that of the empirical ratio.
4.3 Genre
The mixture of grammars model can equivalently be
viewed as capturing either non-local correlations or
variations in grammar. The latter view suggests that
the model might benefit when the syntactic structure
18
 0  10  20  30  40  50  60
Lo
g 
Li
ke
lih
oo
d
Iteration
Training data
Validation data
Testing data
 79
 79.2
 79.4
 79.6
 79.8
 80
 1  2  3  4  5  6  7  8  9
F1
Number of Component Grammars
Mixture model
Baseline: 1 grammar
(a) (b)
Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to fit on the
same plot). Note that when overfitting occurs the likelihood on the validation and test data starts decreasing
(after 13 iterations). (b) The accuracy of the mixture of grammars model with ? = 0.4 versus the number of
grammars. Note the improvement over a 1-grammar PCFG model.
varies significantly, as between different genres. We
tested this with the Brown corpus, of which we used
8 different genres (f, g, k, l, m, n, p, and r). We fol-
low Gildea (2001) in using the ninth and tenth sen-
tences of every block of ten as validation and test
data, respectively, because a contiguous test section
might not be representative due to the genre varia-
tion.
To test the effects of genre variation, we evalu-
ated various training schemes on the Brown corpus.
The single grammar baseline for this corpus gives
F1 = 79.75, with log likelihood (LL) on the testing
data=-242561. The first test, then, was to estimate
each individual grammar from only one genre. We
did this by assigning sentences to individual gram-
mars by genre, without using any EM training. This
increases the data likelihood, though it reduces the
F1 score (F1 = 79.48, LL=-242332). The increase
in likelihood indicates that there are genre-specific
features that our model can represent. (The lack of
F1 improvement may be attributed to the increased
difficulty of estimating rule probabilities after divid-
ing the already scant data available in the Brown cor-
pus. This small quantity of data makes overfitting
almost certain.)
However, local minima and lack of data cause dif-
ficulty in learning genre-specific features. If we start
with sentences assigned by genre as before, but then
train with EM, both F1 and test data log likelihood
drop (F1 = 79.37, LL=-242100). When we use
EM with a random initialization, so that sentences
are not assigned directly to grammars, the scores go
down even further (F1 = 79.16, LL=-242459). This
indicates that the model can capture variation be-
tween genres, but that maximum training data likeli-
hood does not necessarily give maximum accuracy.
Presumably, with more genre-specific data avail-
able, learning would generalize better. So, genre-
specific grammar variation is real, but it is difficult
to capture via EM.
4.4 Smoothing Effects
While the mixture of grammars captures rule cor-
relations, it may also enhance performance via
smoothing effects. Splitting the data randomly could
produce a smoothed shared grammar, Gs, that is
a kind of held-out estimate which could be supe-
rior to the unsmoothed ML estimates for the single-
component grammar.
We tested the degree of generalization by eval-
uating the shared grammar alone and also a mix-
ture of the shared grammar with the known sin-
gle grammar. Those shared grammars were ex-
tracted after training the mixture model with four in-
dividual grammars. We found that both the shared
grammar alone (F1=79.13, LL=-333278) and the
shared grammar mixed with the single grammar
(F1=79.36, LL=-331546) perform worse than a sin-
19
gle PCFG (F1=79.37, LL=-327658). This indicates
that smoothing is not the primary learning effect
contributing to increased F1.
5 Conclusions
We examined the sorts of rule correlations that may
be found in natural language corpora, discovering
non-local correlations not captured by traditional
models. We found that using a model capable of
representing these non-local features gives improve-
ment in parsing accuracy and data likelihood. This
improvement is modest, however, primarily because
local correlations are so much stronger than non-
local ones.
References
J. Baker. 1979. Trainable grammars for speech recog-
nition. Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America, pages
547?550.
K. Bock and H. Loebell. 1990. Framing sentences. Cog-
nition, 35:1?39.
R. Bod. 1992. A computational model of language per-
formance: Data oriented parsing. International Con-
ference on Computational Linguistics (COLING).
E. Charniak. 1996. Tree-bank grammars. In Proc. of
the 13th National Conference on Artificial Intelligence
(AAAI), pages 1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In Proc. of the Conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
Z. Ghahramani and M. I. Jordan. 1994. Supervised
learning from incomplete data via an EM approach. In
Advances in Neural Information Processing Systems
(NIPS), pages 120?127.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. Conference on Empirical Methods in Natural
Language Processing (EMNLP).
M. Johnson. 1998. Pcfg models of linguistic tree repre-
sentations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. Proc. of the 41st Meeting of the Association
for Computational Linguistics (ACL), pages 423?430.
C. Manning and H. Schu?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. The MIT Press,
Cambridge, Massachusetts.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of the 43rd
Meeting of the Association for Computational Linguis-
tics (ACL), pages 75?82.
A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.
1998. Improving text classification by shrinkage in a
hierarchy of classes. In Int. Conf. on Machine Learn-
ing (ICML), pages 359?367.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
D. Titterington, A. Smith, and U. Makov. 1962. Statisti-
cal Analysis of Finite Mixture Distributions. Wiley.
K. Vijay-Shankar and A. Joshi. 1985. Some computa-
tional properties of tree adjoining grammars. Proc. of
the 23th Meeting of the Association for Computational
Linguistics (ACL), pages 82?93.
20
Proceedings of the Workshop on Statistical Machine Translation, pages 31?38,
New York City, June 2006. c?2006 Association for Computational Linguistics
Why Generative Phrase Models Underperform Surface Heuristics
John DeNero, Dan Gillick, James Zhang, Dan Klein
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Berkeley, CA 94705
{denero, dgillick, jyzhang, klein}@eecs.berkeley.edu
Abstract
We investigate why weights from generative mod-
els underperform heuristic estimates in phrase-
based machine translation. We first propose a sim-
ple generative, phrase-based model and verify that
its estimates are inferior to those given by surface
statistics. The performance gap stems primarily
from the addition of a hidden segmentation vari-
able, which increases the capacity for overfitting
during maximum likelihood training with EM. In
particular, while word level models benefit greatly
from re-estimation, phrase-level models do not: the
crucial difference is that distinct word alignments
cannot all be correct, while distinct segmentations
can. Alternate segmentations rather than alternate
alignments compete, resulting in increased deter-
minization of the phrase table, decreased general-
ization, and decreased final BLEU score. We also
show that interpolation of the two methods can re-
sult in a modest increase in BLEU score.
1 Introduction
At the core of a phrase-based statistical machine
translation system is a phrase table containing
pairs of source and target language phrases, each
weighted by a conditional translation probability.
Koehn et al (2003a) showed that translation qual-
ity is very sensitive to how this table is extracted
from the training data. One particularly surprising
result is that a simple heuristic extraction algorithm
based on surface statistics of a word-aligned training
set outperformed the phrase-based generative model
proposed by Marcu and Wong (2002).
This result is surprising in light of the reverse sit-
uation for word-based statistical translation. Specif-
ically, in the task of word alignment, heuristic ap-
proaches such as the Dice coefficient consistently
underperform their re-estimated counterparts, such
as the IBM word alignment models (Brown et al,
1993). This well-known result is unsurprising: re-
estimation introduces an element of competition into
the learning process. The key virtue of competition
in word alignment is that, to a first approximation,
only one source word should generate each target
word. If a good alignment for a word token is found,
other plausible alignments are explained away and
should be discounted as incorrect for that token.
As we show in this paper, this effect does not pre-
vail for phrase-level alignments. The central differ-
ence is that phrase-based models, such as the ones
presented in section 2 or Marcu and Wong (2002),
contain an element of segmentation. That is, they do
not merely learn correspondences between phrases,
but also segmentations of the source and target sen-
tences. However, while it is reasonable to sup-
pose that if one alignment is right, others must be
wrong, the situation is more complex for segmenta-
tions. For example, if one segmentation subsumes
another, they are not necessarily incompatible: both
may be equally valid. While in some cases, such
as idiomatic vs. literal translations, two segmenta-
tions may be in true competition, we show that the
most common result is for different segmentations
to be recruited for different examples, overfitting the
training data and overly determinizing the phrase
translation estimates.
In this work, we first define a novel (but not rad-
ical) generative phrase-based model analogous to
IBM Model 3. While its exact training is intractable,
we describe a training regime which uses word-
level alignments to constrain the space of feasible
segmentations down to a manageable number. We
demonstrate that the phrase analogue of the Dice co-
efficient is superior to our generative model (a re-
sult also echoing previous work). In the primary
contribution of the paper, we present a series of ex-
periments designed to elucidate what re-estimation
learns in this context. We show that estimates are
overly determinized because segmentations are used
31
in unintuitive ways for the sake of data likelihood.
We comment on both the beneficial instances of seg-
ment competition (idioms) as well as the harmful
ones (most everything else). Finally, we demon-
strate that interpolation of the two estimates can
provide a modest increase in BLEU score over the
heuristic baseline.
2 Approach and Evaluation Methodology
The generative model defined below is evaluated
based on the BLEU score it produces in an end-
to-end machine translation system from English to
French. The top-performing diag-and extraction
heuristic (Zens et al, 2002) serves as the baseline for
evaluation.1 Each approach ? the generative model
and heuristic baseline ? produces an estimated con-
ditional distribution of English phrases given French
phrases. We will refer to the distribution derived
from the baseline heuristic as ?H . The distribution
learned via the generative model, denoted ?EM , is
described in detail below.
2.1 A Generative Phrase Model
While our model for computing ?EM is novel, it
is meant to exemplify a class of models that are
not only clear extensions to generative word align-
ment models, but also compatible with the statistical
framework assumed during phrase-based decoding.
The generative process we modeled produces a
phrase-aligned English sentence from a French sen-
tence where the former is a translation of the lat-
ter. Note that this generative process is opposite to
the translation direction of the larger system because
of the standard noisy-channel decomposition. The
learned parameters from this model will be used to
translate sentences from English to French. The gen-
erative process modeled has four steps:2
1. Begin with a French sentence f.
1This well-known heuristic extracts phrases from a sentence
pair by computing a word-level alignment for the sentence and
then enumerating all phrases compatible with that alignment.
The word alignment is computed by first intersecting the direc-
tional alignments produced by a generative IBM model (e.g.,
model 4 with minor enhancements) in each translation direc-
tion, then adding certain alignments from the union of the di-
rectional alignments based on local growth rules.
2Our notation matches the literature for phrase-based trans-
lation: e is an English word, e? is an English phrase, and e?I1 is a
sequence of I English phrases, and e is an English sentence.
2. Segment f into a sequence of I multi-word
phrases that span the sentence, f? I1 .
3. For each phrase f?i ? f? I1 , choose a correspond-
ing position j in the English sentence and es-
tablish the alignment aj = i, then generate ex-
actly one English phrase e?j from f?i.
4. The sequence e?j ordered by a describes an En-
glish sentence e.
The corresponding probabilistic model for this gen-
erative process is:
P (e|f) =
?
f?I1 ,e?
I
1,a
P (e, f? I1 , e?
I
1, a|f)
=
?
f?I1 ,e?
I
1,a
?(f? I1 |f)
?
f?i?f?I1
?(e?j |f?i)d(aj = i|f)
where P (e, f? I1 , e?
I
1, a|f) factors into a segmentation
model ?, a translation model ? and a distortion
model d. The parameters for each component of this
model are estimated differently:
? The segmentation model ?(f? I1 |f) is assumed to
be uniform over all possible segmentations for
a sentence.3
? The phrase translation model ?(e?j |f?i) is pa-
rameterized by a large table of phrase transla-
tion probabilities.
? The distortion model d(aj = i|f) is a discount-
ing function based on absolute sentence posi-
tion akin to the one used in IBM model 3.
While similar to the joint model in Marcu and Wong
(2002), our model takes a conditional form com-
patible with the statistical assumptions used by the
Pharaoh decoder. Thus, after training, the param-
eters of the phrase translation model ?EM can be
used directly for decoding.
2.2 Training
Significant approximation and pruning is required
to train a generative phrase model and table ? such
as ?EM ? with hidden segmentation and alignment
variables using the expectation maximization algo-
rithm (EM). Computing the likelihood of the data
3This segmentation model is deficient given a maximum
phrase length: many segmentations are disallowed in practice.
32
for a set of parameters (the e-step) involves summing
over exponentially many possible segmentations for
each training sentence. Unlike previous attempts to
train a similar model (Marcu and Wong, 2002), we
allow information from a word-alignment model to
inform our approximation. This approach allowed
us to directly estimate translation probabilities even
for rare phrase pairs, which were estimated heuristi-
cally in previous work.
In each iteration of EM, we re-estimate each
phrase translation probability by summing fractional
phrase counts (soft counts) from the data given the
current model parameters.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i)
=
?
(f,e)
?
f?I1 :f?i?f?
I
1
?
e?I1:e?j?e?
I
1
?
a:aj=i P (e, f?
I
1 , e?
I
1, a|f)
?
f?I1 :f?i?f?
I
1
?
e?I1
?
a P (e, f?
I
1 , e?
I
1, a|f)
This training loop necessitates approximation be-
cause summing over all possible segmentations and
alignments for each sentence is intractable, requiring
time exponential in the length of the sentences. Ad-
ditionally, the set of possible phrase pairs grows too
large to fit in memory. Using word alignments, we
can address both problems.4 In particular, we can
determine for any aligned segmentation (f? I1 , e?
I
1, a)
whether it is compatible with the word-level align-
ment for the sentence pair. We define a phrase pair
to be compatible with a word-alignment if no word
in either phrase is aligned with a word outside the
other phrase (Zens et al, 2002). Then, (f? I1 , e?
I
1, a)
is compatible with the word-alignment if each of its
aligned phrases is a compatible phrase pair.
The training process is then constrained such that,
when evaluating the above sum, only compatible
aligned segmentations are considered. That is, we
allow P (e, f? I1 , e?
I
1, a|f) > 0 only for aligned seg-
mentations (f? I1 , e?
I
1, a) such that a provides a one-
to-one mapping from f? I1 to e?
I
1 where all phrase pairs
(f?aj , e?j) are compatible with the word alignment.
This constraint has two important effects. First,
we force P (e?j |f?i) = 0 for all phrase pairs not com-
patible with the word-level alignment for some sen-
tence pair. This restriction successfully reduced the
4The word alignments used in approximating the e-step
were the same as those used to create the heuristic diag-and
baseline.
total legal phrase pair types from approximately 250
million to 17 million for 100,000 training sentences.
However, some desirable phrases were eliminated
because of errors in the word alignments.
Second, the time to compute the e-step is reduced.
While in principle it is still intractable, in practice
we can compute most sentence pairs? contributions
in under a second each. However, some spurious
word alignments can disallow all segmentations for
a sentence pair, rendering it unusable for training.
Several factors including errors in the word-level
alignments, sparse word alignments and non-literal
translations cause our constraint to rule out approx-
imately 54% of the training set. Thus, the reduced
size of the usable training set accounts for some of
the degraded performance of ?EM relative to ?H .
However, the results in figure 1 of the following sec-
tion show that ?EM trained on twice as much data
as ?H still underperforms the heuristic, indicating a
larger issue than decreased training set size.
2.3 Experimental Design
To test the relative performance of ?EM and ?H ,
we evaluated each using an end-to-end translation
system from English to French. We chose this non-
standard translation direction so that the examples
in this paper would be more accessible to a primar-
ily English-speaking audience. All training and test
data were drawn from the French/English section of
the Europarl sentence-aligned corpus. We tested on
the first 1,000 unique sentences of length 5 to 15 in
the corpus and trained on sentences of length 1 to 60
starting after the first 10,000.
The system follows the structure proposed in
the documentation for the Pharaoh decoder and
uses many publicly available components (Koehn,
2003b). The language model was generated from
the Europarl corpus using the SRI Language Model-
ing Toolkit (Stolcke, 2002). Pharaoh performed de-
coding using a set of default parameters for weight-
ing the relative influence of the language, translation
and distortion models (Koehn, 2003b). A maximum
phrase length of three was used for all experiments.
To properly compare ?EM to ?H , all aspects of
the translation pipeline were held constant except for
the parameters of the phrase translation table. In par-
ticular, we did not tune the decoding hyperparame-
ters for the different phrase tables.
33
Source 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743Iteration 2 0.3735 0.3851 0.3814iteration 3 0.3705 0.384 0.3827Iteration 4 0.3695 0.285 0.3801iteration 5 0.3705 0.284 0.3774interpSource 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743iteration 3 0.3705 0.384 0.3827iteration 3 0.3705 0.384 0.3827
0.360.37
0.380.39
0.40
25k 50k 100kTraining sentences
BLEU
HeuristicIteration 1iteration 3
0%20%
40%60%
80%100%
0 10 20 30 40 50 60Sentence Length
Senten
ces Sk
ipped
Figure 1: Statistical re-estimation using a generative
phrase model degrades BLEU score relative to its
heuristic initialization.
3 Results
Having generated ?H heuristically and ?EM with
EM, we now compare their performance. While the
model and training regimen for ?EM differ from the
model from Marcu and Wong (2002), we achieved
results similar to Koehn et al (2003a): ?EM slightly
underperformed ?H . Figure 1 compares the BLEU
scores using each estimate. Note that the expecta-
tion maximization algorithm for training ?EM was
initialized with the heuristic parameters ?H , so the
heuristic curve can be equivalently labeled as itera-
tion 0.
Thus, the first iteration of EM increases the ob-
served likelihood of the training sentences while si-
multaneously degrading translation performance on
the test set. As training proceeds, performance on
the test set levels off after three iterations of EM. The
system never achieves the performance of its initial-
ization parameters. The pruning of our training regi-
men accounts for part of this degradation, but not all;
augmenting ?EM by adding back in all phrase pairs
that were dropped during training does not close the
performance gap between ?EM and ?H .
3.1 Analysis
Learning ?EM degrades translation quality in large
part because EM learns overly determinized seg-
mentations and translation parameters, overfitting
the training data and failing to generalize. The pri-
mary increase in richness from generative word-
level models to generative phrase-level models is
due to the additional latent segmentation variable.
Although we impose a uniform distribution over
segmentations, it nonetheless plays a crucial role
during training. We will characterize this phe-
nomenon through aggregate statistics and transla-
tion examples shortly, but begin by demonstrating
the model?s capacity to overfit the training data.
Let us first return to the motivation behind in-
troducing and learning phrases in machine transla-
tion. For any language pair, there are contiguous
strings of words whose collocational translation is
non-compositional; that is, they translate together
differently than they would in isolation. For in-
stance, chat in French generally translates to cat in
English, but appeler un chat un chat is an idiom
which translates to call a spade a spade. Introduc-
ing phrases allows us to translate chat un chat atom-
ically to spade a spade and vice versa.
While introducing phrases and parameterizing
their translation probabilities with a surface heuris-
tic allows for this possibility, statistical re-estimation
would be required to learn that chat should never be
translated to spade in isolation. Hence, translating I
have a spade with ?H could yield an error.
But enforcing competition among segmentations
introduces a new problem: true translation ambigu-
ity can also be spuriously explained by the segmen-
tation. Consider the french fragment carte sur la
table, which could translate to map on the table or
notice on the chart. Using these two sentence pairs
as training, one would hope to capture the ambiguity
in the parameter table as:
French English ?(e|f)
carte map 0.5
carte notice 0.5
carte sur map on 0.5
carte sur notice on 0.5
sur on 1.0
... ... ...
table table 0.5
table chart 0.5
Assuming we only allow non-degenerate seg-
mentations and disallow non-monotonic alignments,
this parameter table yields a marginal likelihood
P (f|e) = 0.25 for both sentence pairs ? the intu-
itive result given two independent lexical ambigu-
34
ities. However, the following table yields a likeli-
hood of 0.28 for both sentences:5
French English ?(e|f)
carte map 1.0
carte sur notice on 1.0
carte sur la notice on the 1.0
sur on 1.0
sur la table on the table 1.0
la the 1.0
la table the table 1.0
table chart 1.0
Hence, a higher likelihood can be achieved by al-
locating some phrases to certain translations while
reserving overlapping phrases for others, thereby
failing to model the real ambiguity that exists across
the language pair. Also, notice that the phrase sur
la can take on an arbitrary distribution over any en-
glish phrases without affecting the likelihood of ei-
ther sentence pair. Not only does this counterintu-
itive parameterization give a high data likelihood,
but it is also a fixed point of the EM algorithm.
The phenomenon demonstrated above poses a
problem for generative phrase models in general.
The ambiguous process of translation can be mod-
eled either by the latent segmentation variable or the
phrase translation probabilities. In some cases, opti-
mizing the likelihood of the training corpus adjusts
for the former when we would prefer the latter. We
next investigate how this problem manifests in ?EM
and its effect on translation quality.
3.2 Learned parameters
The parameters of ?EM differ from the heuristically
extracted parameters ?H in that the conditional dis-
tributions over English translations for some French
words are sharply peaked for ?EM compared to flat-
ter distributions generated by ?H . This determinism
? predicted by the previous section?s example ? is
not atypical of EM training for other tasks.
To quantify the notion of peaked distributions
over phrase translations, we compute the entropy of
the distribution for each French phrase according to
5For example, summing over the first translation ex-
pands to 17 (?(map | carte)?(on the table | sur la table)
+?(map | carte)?(on | sur)?(the table | la table)).
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
0 10 20 30 400 - .01
.01 - .5.5 - 1
1 - 1.51.5 - 2
> 2
Entrop
y
% Phrase TranslationsLearnedHeuristic
1E-04 1E-02 1E+00 1E+02',de.lall 'leetlesMost
 Comm
on Fre
nch Ph
rases
EntropyLearned Heuristic
Figure 2: Many more French phrases have very low
entropy under the learned parameterization.
the standard definition.
H(?(e?|f?)) =
?
e?
?(e?|f?) log2 ?(e?|f?)
The average entropy, weighted by frequency, for the
most common 10,000 phrases in the learned table
was 1.55, comparable to 3.76 for the heuristic table.
The difference between the tables becomes much
more striking when we consider the histogram of
entropies for phrases in figure 2. In particular, the
learned table has many more phrases with entropy
near zero. The most pronounced entropy differences
often appear for common phrases. Ten of the most
common phrases in the French corpus are shown in
figure 3.
As more probability mass is reserved for fewer
translations, many of the alternative translations un-
der ?H are assigned prohibitively small probabili-
ties. In translating 1,000 test sentences, for example,
no phrase translation with ?(e?|f?) less than 10?5 was
used by the decoder. Given this empirical threshold,
nearly 60% of entries in ?EM are unusable, com-
pared with 1% in ?H .
3.3 Effects on Translation
While this determinism of ?EM may be desirable
in some circumstances, we found that the ambi-
guity in ?H is often preferable at decoding time.
35
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
01020
3040
0 - .01 .01 - .5 .5 - 1 1 - 1.5 1.5 - 2 > 2Entropy
% Phr
ase 
Transl
ations HeuristicLearned
1E-04 1E-02 1E+00 1E+02 ',.ll 'n 'quequiplusl ' unionC
ommo
n Fren
ch Phr
ases
EntropyLearned Heuristic
Figure 3: Entropy of 10 common French phrases.
Several learned distributions have very low entropy.
In particular, the pattern of translation-ambiguous
phrases receiving spuriously peaked distributions (as
described in section 3.1) introduces new tra slation
errors relative to the baseline. We now investigate
both positive and negative effects of the learning
process.
The issue that motivated training a generative
model is sometimes resolved correctly: for a word
that translates differently alone than in the context
of an idiom, the translation probabilities can more
accurately reflect this. Returning to the previous ex-
ample, the phrase table for chat has been corrected
through the learning process. The heuristic process
gives the incorrect translation spade with 61% prob-
ability, while the statistical learning approach gives
cat with 95% probability.
While such examples of improvement are en-
couraging, the trend of spurious determinism over-
whelms this benefit by introducing errors in four re-
lated ways, each of which will be explored in turn.
1. Useful phrase pairs can be assigned very low
probabilities and therefore become unusable.
2. A proper translation for a phrase can be over-
ridden by another translation with spuriously
high probability.
3. Error-prone, common, ambiguous phrases be-
come active during decoding.
4. The language model cannot distinguish be-
tween different translation options as effec-
tively due to deterministic translation model
distributions.
The first effect follows from our observation in
section 3.2 that many phrase pairs are unusable due
to vanishingly small probabilities. Some of the en-
tries that are made unusable by re-estimation are
helpful at decoding time, evidenced by the fact
that pruning the set of ?EM ?s low-scoring learned
phrases from the original heuristic table reduces
BLEU score by 0.02 for 25k training sentences (be-
low the score for ?EM ).
The second effect is more subtle. Consider the
sentence in figure 4, which to a first approxima-
tion can be translated as a series of cognates, as
demonstrated by the decoding that follows from the
heuristic parameterization ?H .6 Notice also that the
translation probabilities from heuristic extraction are
non-deterministic. On the other hand, the translation
system makes a significant lexical error on this sim-
ple sentence when parameterized by ?EM : the use
of caracte?rise in this context is incorrect. This error
arises from a sharply peaked distribution over En-
glish phrases for caracte?rise.
This example illustrates a recurring problem: er-
rors do not necessarily arise because a correct trans-
lation is not available. Notice that a preferable trans-
lation of degree as degre? is available under both pa-
rameterizations. Degre? is not used, however, be-
cause of the peaked distribution of a competing
translation candidate. In this way, very high prob-
ability translations can effectively block the use of
more appropriate translations at decoding time.
What is furthermore surprising and noteworthy in
this example is that the learned, near-deterministic
translation for caracte?rise is not a common trans-
lation for the word. Not only does the statistical
learning process yield low-entropy translation dis-
tributions, but occasionally the translation with un-
desirably high conditional probability does not have
a strong surface correlation with the source phrase.
This example is not unique; during different initial-
izations of the EM algorithm, we noticed such pat-
6While there is some agreement error and awkwardness, the
heuristic translation is comprehensible to native speakers. The
learned translation incorrectly translates degree, degrading the
translation quality.
36
the situation varies to an
la situation varie d ' une
Heuristically Extracted Phrase Table
Learned Phrase Table
enormous
immense
degree
degr?
situation varies to
la varie d '
an enormous
une immense
degree
caract?rise
the
situation
caracte?rise
English ?(e|f)
degree 0.998
characterises 0.001
characterised 0.001
caracte?rise
English ?(e|f)
characterises 0.49
characterised 0.21
permeate 0.05
features 0.05
typifies 0.05
degr e?
English ?(e|f)
degree 0.49
level 0.38
extent 0.02
amount 0.02
how 0.01
degre?
English ?(e|f)
degree 0.64
level 0.26
extent 0.10
Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.
terns even for common French phrases such as de
and ne.
The third source of errors is closely related: com-
mon phrases that translate in many ways depending
on the context can introduce errors if they have a
spuriously peaked distribution. For instance, con-
sider the lone apostrophe, which is treated as a sin-
gle token in our data set (figure 5). The shape of
the heuristic translation distribution for the phrase is
intuitively appealing, showing a relatively flat dis-
tribution among many possible translations. Such
a distribution has very high entropy. On the other
hand, the learned table translates the apostrophe to
the with probability very near 1.
Heuristic
English ?H(e|f)
our 0.10
that 0.09
is 0.06
we 0.05
next 0.05
Learned
English ?EM (e|f)
the 0.99
, 4.1 ? 10?3
is 6.5 ? 10?4
to 6.3 ? 10?4
in 5.3 ? 10?4
Figure 5: Translation probabilities for an apostro-
phe, the most common french phrase. The learned
table contains a highly peaked distribution.
Such common phrases whose translation depends
highly on the context are ripe for producing transla-
tion errors. The flatness of the distribution of ?H en-
sures that the single apostrophe will rarely be used
during decoding because no one phrase table entry
has high enough probability to promote its use. On
the other hand, using the peaked entry ?EM (the|?)
incurs virtually no cost to the score of a translation.
The final kind of errors stems from interactions
between the language and translation models. The
selection among translation choices via a language
model ? a key virtue of the noisy channel frame-
work ? is hindered by the determinism of the transla-
tion model. This effect appears to be less significant
than the previous three. We should note, however,
that adjusting the language and translation model
weights during decoding does not close the perfor-
mance gap between ?H and ?EM .
3.4 Improvements
In light of the low entropy of ?EM , we could hope to
improve translations by retaining entropy. There are
several strategies we have considered to achieve this.
Broadly, we have tried two approaches: combin-
ing ?EM and ?H via heuristic interpolation methods
and modifying the training loop to limit determin-
ism.
The simplest strategy to increase entropy is to
interpolate the heuristic and learned phrase tables.
Varying the weight of interpolation showed an im-
provement over the heuristic of up to 0.01 for 100k
sentences. A more modest improvement of 0.003 for
25k training sentences appears in table 1.
In another experiment, we interpolated the out-
put of each iteration of EM with its input, thereby
maintaining some entropy from the initialization pa-
rameters. BLEU score increased to a maximum of
0.394 using this technique with 100k training sen-
tences, outperforming the heuristic by a slim margin
of 0.005.
We might address the determinization in ?EM
without resorting to interpolation by modifying the
37
training procedure to retain entropy. By imposing a
non-uniform segmentation model that favors shorter
phrases over longer ones, we hope to prevent the
error-causing effects of EM training outlined above.
In principle, this change will encourage EM to ex-
plain training sentences with shorter sentences. In
practice, however, this approach has not led to an
improvement in BLEU.
Another approach to maintaining entropy during
the training process is to smooth the probabilities
generated by EM. In particular, we can use the fol-
lowing smoothed update equation during the train-
ing loop, which reserves a portion of probability
mass for unseen translations.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i) + kl?1
In the equation above, l is the length of the French
phrase and k is a tuning parameter. This formula-
tion not only serves to reduce very spiked probabili-
ties in ?EM , but also boosts the probability of short
phrases to encourage their use. With k = 2.5, this
smoothing approach improves BLEU by .007 using
25k training sentences, nearly equaling the heuristic
(table 1).
4 Conclusion
Re-estimating phrase translation probabilities using
a generative model holds the promise of improving
upon heuristic techniques. However, the combina-
torial properties of a phrase-based generative model
have unfortunate side effects. In cases of true ambi-
guity in the language pair to be translated, parameter
estimates that explain the ambiguity using segmen-
tation variables can in some cases yield higher data
likelihoods by determinizing phrase translation esti-
mates. However, this behavior in turn leads to errors
at decoding time.
We have also shown that some modest benefit can
be obtained from re-estimation through the blunt in-
strument of interpolation. A remaining challenge is
to design more appropriate statistical models which
tie segmentations together unless sufficient evidence
of true non-compositionality is present; perhaps
such models could properly combine the benefits of
both current approaches.
Estimate BLEU
?H 0.385
?H phrase pairs that also appear in ?EM 0.365
?EM 0.374
?EM with a non-uniform segmentation model 0.374
?EM with smoothing 0.381
?EM with gaps filled in by ?H 0.374
?EM interpolated with ?H 0.388
Table 1: BLEU results for 25k training sentences.
5 Acknowledgments
We would like to thank the anonymous reviewers for
their valuable feedback on this paper.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 1993.
Philipp Koehn. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. USC Information
Sciences Institute, 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Sta-
tistical phrase-based translation. HLT-NAACL, 2003.
Philipp Koehn. Pharaoh: A Beam Search Decoder for
Phrase-Based Statisical Machine Translation Models.
USC Information Sciences Institute, 2003.
Daniel Marcu and William Wong. A phrase-based, joint
probability model for statistical machine translation.
Conference on Empirical Methods in Natual Language
Processing, 2002.
Franz Josef Och and Hermann Ney. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, 2003.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
Improved alignment models for statistical machine
translation. ACL Workshops, 1999.
Andreas Stolcke. Srilm ? an extensible language model-
ing toolkit. Proceedings of the International Confer-
ence on Statistical Language Processing, 2002.
Richard Zens, Franz Josef Och and Hermann Ney.
Phrase-Based Statistical Machine Translation. Annual
German Conference on AI, 2002.
38
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33?39,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parsing German with Latent Variable Grammars
Slav Petrov and Dan Klein
{petrov,klein}@cs.berkeley.edu
University of California at Berkeley
Berkeley, CA 94720
Abstract
We describe experiments on learning latent
variable grammars for various German tree-
banks, using a language-agnostic statistical
approach. In our method, a minimal ini-
tial grammar is hierarchically refined using an
adaptive split-and-merge EM procedure, giv-
ing compact, accurate grammars. The learn-
ing procedure directly maximizes the likeli-
hood of the training treebank, without the use
of any language specific or linguistically con-
strained features. Nonetheless, the resulting
grammars encode many linguistically inter-
pretable patterns and give the best published
parsing accuracies on three German treebanks.
1 Introduction
Probabilistic context-free grammars (PCFGs) under-
lie most high-performance parsers in one way or an-
other (Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005). However, as demonstrated in Char-
niak (1996) and Klein and Manning (2003), a PCFG
which simply takes the empirical rules and probabil-
ities off of a treebank does not perform well. This
naive grammar is a poor one because its context-
freedom assumptions are too strong in some ways
(e.g. it assumes that subject and object NPs share
the same distribution) and too weak in others (e.g.
it assumes that long rewrites do not decompose into
smaller steps). Therefore, a variety of techniques
have been developed to both enrich and generalize
the naive grammar, ranging from simple tree anno-
tation and symbol splitting (Johnson, 1998; Klein
and Manning, 2003) to full lexicalization and intri-
cate smoothing (Collins, 1999; Charniak, 2000).
We view treebank parsing as the search for an
optimally refined grammar consistent with a coarse
training treebank. As a result, we begin with the
provided evaluation symbols (such as NP, VP, etc.)
but split them based on the statistical patterns in
the training trees. A manual approach might take
the symbol NP and subdivide it into one subsymbol
NP?S for subjects and another subsymbol NP?VP
for objects. However, rather than devising linguis-
tically motivated features or splits, we take a fully
automated approach, in which each symbol is split
into unconstrained subsymbols. For example, NP
would be split into NP-1 through NP-8. We use
the Expectation-Maximization (EM) to then fit our
split model to the observed trees; therein the vari-
ous subsymbols will specialize in ways which may
or may not correspond to our linguistic intuitions.
This approach is relatively language independent,
because the hidden subsymbols are induced auto-
matically from the training trees based solely on data
likelihood, though of course it is most applicable to
strongly configurational languages.
In our experiments, we find that we can learn
compact grammars that give the highest parsing ac-
curacies in the 2008 Parsing German shared task.
Our F1-scores of 69.8/84.0 (TIGER/TueBa-D/Z) are
more than four points higher than those of the
second best systems. Additionally, we investigate
the patterns that are learned and show that the la-
tent variable approach recovers linguistically inter-
pretable phenomena. In our analysis, we pay partic-
ular attention to similarities and differences between
33
FRAG
RB
Not
NP
DT
this
NN
year
.
.
(a)
ROOT
FRAG-x
FRAG-x
RB-x
Not
NP-x
DT-x
this
NN-x
year
.-x
.
(b)
Figure 1: (a) The original tree. (b) The binarized tree
with latent variables.
grammars learned from the two treebanks.
2 Latent Variable Parsing
In latent variable parsing (Matsuzaki et al, 2005;
Prescher, 2005; Petrov et al, 2006), we learn
rule probabilities on latent annotations that, when
marginalized out, maximize the likelihood of the
unannotated training trees. We use an automatic ap-
proach in which basic nonterminal symbols are al-
ternately split and merged to maximize the likeli-
hood of the training treebank.
In this section we briefly review the main ideas
in latent variable parsing. This work has been pre-
viously published and we therefore provide only
a short overview. For a more detailed exposi-
tion of the learning algorithm the reader is re-
ferred to Petrov et al (2006). The correspond-
ing inference procedure is described in detail in
Petrov and Klein (2007). The parser, code,
and trained models are available for download at
http://nlp.cs.berkeley.edu.
2.1 Learning
Starting with a simple X-bar grammar, we use the
Expectation-Maximization (EM) algorithm to learn
a new grammar whose nonterminals are subsymbols
of the original evaluation nonterminals. The X-bar
grammar is created by binarizing the treebank trees;
for each local tree rooted at an evaluation nonter-
minal X, we introduce a cascade of new nodes la-
beled X so that each node has at most two children,
see Figure 1. This initialization is the absolute mini-
mum starting grammar that distinguishes the evalua-
tion nonterminals (and maintains separate grammars
for each of them).
In Petrov et al (2006) we show that a hierarchical
split-and-merge strategy learns compact but accurate
grammars, allocating subsymbols adaptively where
they are most effective. Beginning with the base-
line grammar, we repeatedly split and re-train the
grammar. In each iteration, we initialize EM with
the results of the previous round?s grammar, splitting
every previous symbol in two and adding a small
amount of randomness (1%) to break the symme-
try between the various subsymbols. Note that we
split all nonterminal symbols, including the part-of-
speech categories. While creating more latent an-
notations can increase accuracy, it can also lead to
overfitting via oversplitting. Adding subsymbols di-
vides grammar statistics into many bins, resulting in
a tighter fit to the training data. At the same time,
each bin has less support and therefore gives a less
robust estimate of the grammar probabilities. At
some point, the fit no longer generalizes, leading to
overfitting.
To prevent oversplitting, we could measure the
utility of splitting each latent annotation individu-
ally and then split the best ones first. However, not
only is this impractical, requiring an entire training
phase for each new split, but it assumes the contri-
butions of multiple splits are independent. In fact,
extra subsymbols may need to be added to several
nonterminals before they can cooperate to pass in-
formation along the parse tree. This point is cru-
cial to the success of our method: because all splits
are fit simultaneously, local splits can chain together
to propagate information non-locally. We therefore
address oversplitting in the opposite direction; after
training all splits, we measure for each one the loss
in likelihood incurred by removing it. If this loss
is small, the new annotation does not carry enough
useful information and can be removed. Another ad-
vantage of evaluating post-hoc merges is that, unlike
the likelihood gain from splitting, the likelihood loss
from merging can be efficiently approximated.
To summarize, splitting provides an increasingly
tight fit to the training data, while merging improves
generalization and controls grammar size. In order
to further overcome data fragmentation and overfit-
ting, we also smooth our parameters along the split
hierarchy. Smoothing allows us to add a larger num-
ber of annotations, each specializing in only a frac-
tion of the data, without overfitting our training set.
34
2.2 Inference
At inference time, we want to use the learned gram-
mar to efficiently and accurately compute a parse
tree for a give sentence.
For efficiency, we employ a hierarchical coarse-
to-fine inference scheme (Charniak et al, 1998;
Charniak and Johnson, 2005; Petrov and Klein,
2007) which vastly improves inference time with no
loss in test set accuracy. Our method considers the
splitting history of the final grammar, projecting it
onto its increasingly refined prior stages. For each
such projection of the refined grammar, we estimate
the projection?s parameters from the source PCFG
itself (rather than the original treebank), using tech-
niques for infinite tree distributions and iterated fix-
point equations. We then rapidly pre-parse with each
refinement stage in sequence, such that any item
X:[i, j] with sufficiently low posterior probability
triggers the pruning of its further refined variants in
all subsequent finer parses.
Our refined grammars G are over symbols of the
form X-k where X is an evaluation symbol (such as
NP) and k is some indicator of a subsymbol, which
may encode something linguistic like a parent anno-
tation context, but which is formally just an integer.
G therefore induces a derivation distribution over
trees labeled with split symbols. This distribution
in turn induces a parse distribution over (projected)
trees with unsplit evaluation symbols. We have
several choices of how to select a tree given these
posterior distributions over trees. Since computing
the most likely parse tree is NP-complete (Sima?an,
1992), we settle for an approximation that allows us
to (partially) sum out the latent annotation. In Petrov
and Klein (2007) we relate this approximation to
Goodman (1996)?s labeled brackets algorithm ap-
plied to rules and to Matsuzaki et al (2005)?s sen-
tence specific variational approximation. This pro-
cedure is substantially superior to simply erasing the
latent annotations from the the Viterbi derivation.
2.3 Results
In Petrov and Klein (2007) we trained models for
English, Chinese and German using the standard
corpora and setups. We applied our latent variable
model directly to each of the treebanks, without any
? 40 words all
Parser LP LR LP LR
ENGLISH
Charniak et al (2005) 90.1 90.1 89.5 89.6
Petrov and Klein (2007) 90.7 90.5 90.2 89.9
ENGLISH (reranked)
Charniak et al (2005) 92.4 91.6 91.8 91.0
GERMAN (NEGRA)
Dubey (2005) F1 76.3 -
Petrov and Klein (2007) 80.8 80.7 80.1 80.1
CHINESE
Chiang et al (2002) 81.1 78.8 78.0 75.2
Petrov and Klein (2007) 86.9 85.7 84.8 81.9
Table 1: Our split-and-merge latent variable approach
produces the best published parsing performance on
many languages.
language dependent modifications. Specifically, the
same model hyperparameters (merging percentage
and smoothing factor) were used in all experiments.
Table 1 summarizes the results: automatically in-
ducing latent structure is a technique that generalizes
well across language boundaries and results in state
of the art performance for Chinese and German. On
English, the parser is outperformed by the reranked
output of Charniak and Johnson (2005), but it out-
performs their underlying lexicalized parser.
3 Experiments
We conducted experiments on the two treebanks
provided for the 2008 Parsing German shared task.
Both treebanks are annotated collections of Ger-
man newspaper text, covering from similar top-
ics. They are annotated with part-of-speech (POS)
tags, morphological information, phrase structure,
and grammatical functions. TueBa-D/Z addition-
ally uses topological fields to describe fundamental
word order restrictions in German clauses. However,
the treebanks differ significantly in their annotation
schemes: while TIGER relies on crossing branches
to describe long distance relationships, TueBa-D/Z
uses planar tree structures with designated labels
that encode long distance relationships. Addition-
ally, the annotation in TIGER is relatively flat on the
phrasal level, while TueBa-D/Z annotates more in-
ternal phrase structure.
We used the standard splits into training and de-
35
 60
 65
 70
 75
 80
 85
 90
 0  1  2  3  4  5
F1
Split & Merge Iterations
TIGER
TueBa-D/Z
Figure 2: Parsing accuracy improves when the amount of
latent annotation is increased.
velopment set, containing roughly 16,000 training
trees and 1,600 development trees, respectively. All
parsing figures in this section are on the develop-
ment set, evaluating on constituents and grammat-
ical functions using gold part-of-speech tags, un-
less noted otherwise. Note that even when we as-
sume gold evaluation part-of-speech tags, we still
assign probabilities to the different subsymbols of
the provided evaluation tag. The parsing accuracies
in the final results section are the official results of
the 2008 Parsing German shared task.
3.1 Latent Annotation
As described in Section 2.1, we start with a mini-
mal X-Bar grammar and learn increasingly refined
grammars in a hierarchical split-and-merge fashion.
We conjoined the constituency categories with their
grammatical functions, creating initial categories
like NP-PD and NP-OA which were further split
automatically. Figure 2 shows how held-out accu-
racy improves when we add latent annotation. Our
baseline grammars have low F1-scores (63.3/72.8,
TIGER/TueBa-D/Z), but performance increases as
the complexity of latent annotation increases. After
four split-and-merge iterations, performance levels
off. Interestingly, the gap in performance between
the two treebanks increases from 9.5 to 13.4 F1-
points. It appears that the latent variable approach
is better suited for capturing the rich structure of the
TueBa-D/Z treebank.
As languages vary in their phrase-internal head-
TIGER TueBa-D/Z
F1 EX F1 EX
Auto Tags 71.12 28.91 83.18 18.46
Gold Tags 71.74 34.04 85.10 20.98
Table 2: Parsing accuracies (F1-score and exact match)
with gold POS tags and automatic POS tags. Many parse
errors are due to incorrect tagging.
edness, we varied the binarization scheme, but, con-
sistent with our experience in other languages, no-
ticed little difference between right and left bina-
rization. We also experimented with starting from
a more constrained baseline by adding parent and
sibling annotation. Adding initial structural annota-
tion results in a higher baseline performance. How-
ever, since it fragments the grammar, adding latent
annotation has a smaller effect, eventually resulting
in poorer performance compared to starting from a
simple X-Bar grammar. Essentially, the initial gram-
mar is either mis- or oversplit to some degree.
3.2 Part-of-speech tagging
When gold parts-of-speech are not assumed, many
parse errors can be traced back to part-of-speech
(POS) tagging errors. It is therefore interesting to in-
vestigate the influence of tagging errors on the over-
all parsing accuracy. For the shared task, we could
assume gold POS tags: during inference we only al-
lowed (and scored) the different subsymbols of the
correct tags. However, this assumption cannot be
made in a more realistic scenario, where we want to
parse text from an unknown source. Table 2 com-
pares the parsing performance with gold POS tags
and with automatic tagging. While POS tagging er-
rors have little influence on the TIGER treebank,
tagging errors on TueBa-D/Z cause an substantial
number of subsequent parse errors.
3.3 Two pass parsing
In the previous experiments, we conflated the
phrasal categories and grammatical functions into
single initial grammar symbol. An alternative is
to first determine the categorical constituency struc-
ture and then to assign grammatical functions to the
chosen constituents in a separate, second pass. To
achieve this, we trained latent variable grammars
for base constituency parsing by stripping off the
36
grammatical functions. After four rounds of split
and merge training, these grammars achieve very
good constituency accuracies of 85.1/94.1 F1-score
(TIGER/TueBa-D/Z). For the second pass, we es-
timated (but did not split) X-Bar style grammars
on the grammatical functions only. Fixing the con-
stituency structure from the first pass, we used those
to add grammatical functions. Unfortunately, this
approach proved to be inferior to the unified, one
pass approach, giving F1-scores of only 50.0/69.4
(TIGER/TueBa-D/Z). Presumably, the degradation
can be attributed to the fact that grammatical func-
tions model long-distance relations between the con-
stituents, which can only be captured poorly by an
unsplit, highly local X-bar style grammar.
3.4 Final Results
The final results of the shared task evaluation are
shown in Table 3. These results were produced by
a latent variable grammar that was trained for four
split-and-merge iterations, starting from an X-Bar
grammar over conjoined categorical/grammatical
symbols, with a left-branching binarization. Our
automatic latent variable approach serves better for
German disambiguation than the competing ap-
proaches, despite its being very language agnostic.
4 Analysis
In this section, we examine the learned grammars,
discussing what is learned. Because the grammat-
ical functions significantly increase the number of
base categories and make the grammars more diffi-
cult to examine, we show examples from grammars
that were trained for categorical constituency pars-
ing by initially stripping off all grammatical function
annotations.
4.1 Lexical Splits
Since both treebanks use the same part-of-speech
categories, it is easy to compare the learned POS
subcategories. To better understand what is being
learned, we selected two grammars after two split
and merge iterations and examined the word dis-
tributions of the subcategories of various symbols.
The three most likely words for a number of POS
tags are shown in Table 4. Interestingly, the sub-
categories learned from the different treebanks ex-
hibit very similar patterns. For example, in both
cases, the nominal category (NE) has been split
into subcategories for first and last names, abbrevi-
ations and places. The cardinal numbers (CARD)
have been split into subcategories for years, spelled
out numbers, and other numbers. There are of-
ten subcategories distinguishing sentence initial and
sentence medial placement (KOND, PDAT, ART,
APPR, etc.), as well as subcategories capturing case
distinctions (PDAT, ART, etc.).
A quantitative way of analyzing the complexity of
what is learned is to compare the number of subcat-
egories that our split-and-merge procedure has allo-
cated to each category. Table 5 shows the automat-
ically determined number of subcategories for each
POS tag. While many categories have been split into
comparably many of subcategories, the POS tags in
the TIGER treebank have in general been refined
more heavily. This increased refinement can be ex-
plained by our merging criterion. We compute the
loss in likelihood that would be incurred from re-
moving a split, and we merge back the least useful
splits. In this process, lexical and phrasal splits com-
pete with each other. In TueBa-D/Z the phrasal cat-
egories have richer internal structure and therefore
get split more heavily. As a consequence, the lexi-
cal categories are often relatively less refined at any
given stage than in TIGER. Having different merg-
ing thresholds for the lexical and phrasal categories
would eliminate this difference and we might expect
the difference in lexical refinement to become less
pronounced. Of course, because of the different un-
derlying statistics in the two treebanks, we do not
expect the number of subcategories to become ex-
actly equal in any case.
4.2 Phrasal splits
Analyzing the phrasal splits is much more difficult,
as the splits can model internal as well as exter-
nal context (as well as combinations thereof) and,
in general, several splits must be considered jointly
before their patterning can be described. Further-
more, the two treebanks use different annotation
standards and different constituent categories. Over-
all, the phrasal categories of the TueBa-D/Z tree-
bank have been more heavily refined, in order to bet-
ter capture the rich internal structures. In both tree-
banks, the most heavily split categories are the noun,
verb and prepositional phrase categories (NP/NX,
37
TIGER TueBa-D/Z
LP LR F1 LP LR F1
Berkeley Parser 69.23 70.41 69.81 83.91 84.04 83.97
Va?xjo? Parser 67.06 63.40 65.18 76.20 74.56 75.37
Stanford Parser 58.52 57.63 58.07 79.26 79.22 79.24
Table 3: Final test set results of the 2008 Parsing German shared task (labeled precision, labeled recall and F1-score)
on both treebanks (including grammatical functions and using gold part-of-speech tags).
NE
Kohl Klaus SPD Deutschland
Rabin Helmut USA dpa
Lafontaine Peter CDU Bonn
CARD
1996 zwei 000 zwei
1994 drei 100 3
1991 vier 20 2
KOND
Und und sondern und
Doch oder aber oder
Aber aber bis sowie
PDAT
Diese dieser diesem -
Dieser dieses diese -
Dieses diese dieser -
ART
Die der der die
Der des den der
Das Die die den
APPR
In als in von
Von nach von in
Nach vor mit fu?r
PDS
Das dessen das -
Dies deren dies -
Diese die diese -
NE
Milosevic Peter K. Berlin
Mu?ller Wolfgang W. taz
Clinton Klaus de Kosovo
CARD
1998 zwei 500 zwei
1999 drei 100 20
2000 fu?nf 20 18
KOND
Und und sondern und
Aber oder weder Denn
Doch aber sowohl oder
PDAT
Dieser diese diesem dieser
Diese dieser dieser diese
Dieses dieses diesen dieses
ART
Die die die der
die Die der die
Der das den den
APPR
In bis in von
Mit Von auf in
Nach Bis mit fu?r
PDS
dem dessen das Das
das die Das das
jene denen dies diese
Table 4: The three most likely words for several part-of-speech (sub-)categories. The left column corresponds to the
TIGER treebank the right column to the TueBa-D/Z treebank. Similar subcategories are learned for both treebanks.
38
POS Ti Tue
ADJA 32 17
NN 32 32
NE 31 32
ADV 30 15
ADJD 30 19
VVFIN 29 5
VVPP 29 4
APPR 25 24
VVINF 18 7
CARD 18 16
ART 10 7
PIS 9 14
PPER 9 2
PIDAT - 9
POS Ti Tue
PIAT 8 7
VAFIN 8 3
KON 8 8
$[ 7 11
PROAV 7 -
APPRART 6 5
$ 6 2
PDS 5 5
PPOSAT 4 4
$. 4 5
PDAT 4 5
KOUS 4 3
VMFIN 4 1
PRELS 3 1
POS Ti Tue
VVIZU 3 2
VAINF 3 3
PTKNEG 3 1
FM 3 8
PWS 2 2
PWAV 2 5
XY 2 2
TRUNC 2 4
KOUI 2 1
PTKVZ 2 1
VAPP 2 2
KOKOM 2 5
PROP - 2
VVIMP 1 1
POS Ti Tue
VAIMP 1 1
VMPP 1 2
PPOSS 1 1
PRELAT 1 1
NNE 1 -
APPO 1 1
PTKA 1 2
PTKANT 1 2
PWAT 1 2
PRF 1 1
PTKZU 1 1
APZR 1 1
VMINF 1 1
ITJ 1 2
Table 5: Automatically determined number of subcategories for the part-of-speech tags. The left column corresponds
to the TIGER treebank the right column to the TueBa-D/Z treebank. Many categories are split in the same number of
subcategories, but overall the TIGER categories have been more heavily refined.
PP/PX, VP/VX*) as well as the sentential categories
(S/SIMPX). Categories that are rare or that have lit-
tle internal structure, in contrast, have been split
lightly or not at all.
5 Conclusions
We presented a series of experiments on pars-
ing German with latent variable grammars. We
showed that our latent variable approach is very
well suited for parsing German, giving the best
parsing figures on several different treebanks, de-
spite being completely language independent. Ad-
ditionally, we examined the learned grammars
and showed examples illustrating the linguistically
meaningful patterns that were learned. The parser,
code, and models are available for download at
http://nlp.cs.berkeley.edu.
References
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak. 1996. Tree-bank grammars. In AAAI ?96,
pages 1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00, pages 132?139.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In COLING ?02, pages 183?189.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ?05.
J. Goodman. 1996. Parsing algorithms and metrics. ACL
?96.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ?03, pages 423?430.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
D. Prescher. 2005. Inducing head-driven PCFGs with la-
tent heads: Refining a tree-bank grammar for parsing.
In ECML?05.
K. Sima?an. 1992. Computatoinal complexity of proba-
bilistic disambiguation. Grammars, 5:125?151.
39
A* Parsing: Fast Exact Viterbi Parse Selection
Dan Klein
Computer Science Department
Stanford University
Stanford, CA 94305-9040
klein@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
manning@cs.stanford.edu
Abstract
We present an extension of the classic A* search
procedure to tabular PCFG parsing. The use of A*
search can dramatically reduce the time required to
find a best parse by conservatively estimating the
probabilities of parse completions. We discuss vari-
ous estimates and give efficient algorithms for com-
puting them. On average-length Penn treebank sen-
tences, our most detailed estimate reduces the to-
tal number of edges processed to less than 3% of
that required by exhaustive parsing, and a simpler
estimate, which requires less than a minute of pre-
computation, reduces the work to less than 5%. Un-
like best-first and finite-beam methods for achieving
this kind of speed-up, an A* method is guaranteed to
find the most likely parse, not just an approximation.
Our parser, which is simpler to implement than an
upward-propagating best-first parser, is correct for a
wide range of parser control strategies and maintains
worst-case cubic time.
1 Introduction
PCFG parsing algorithms with worst-case cubic-time
bounds are well-known. However, when dealing with
wide-coverage grammars and long sentences, even cu-
bic algorithms can be far too expensive in practice. Two
primary types of methods for accelerating parse selec-
tion have been proposed. Roark (2001) and Ratnaparkhi
(1999) use a beam-search strategy, in which only the best
n parses are tracked at any moment. Parsing time is lin-
ear and can be made arbitrarily fast by reducing n. This
is a greedy strategy, and the actual Viterbi (highest proba-
bility) parse can be pruned from the beam because, while
it is globally optimal, it may not be locally optimal at ev-
ery parse stage. Chitrao and Grishman (1990), Caraballo
and Charniak (1998), Charniak et al (1998), and Collins
(1999) describe best-first parsing, which is intended for
a tabular item-based framework. In best-first parsing,
one builds a figure-of-merit (FOM) over parser items,
and uses the FOM to decide the order in which agenda
items should be processed. This approach also dramat-
ically reduces the work done during parsing, though it,
too, gives no guarantee that the first parse returned is the
actual Viterbi parse (nor does it maintain a worst-case cu-
bic time bound). We discuss best-first parsing further in
section 3.3.
Both of these speed-up techniques are based on greedy
models of parser actions. The beam search greedily
prunes partial parses at each beam stage, and a best-first
FOM greedily orders parse item exploration. If we wish
to maintain optimality in a search procedure, the obvious
thing to try is A* methods (see for example Russell and
Norvig, 1995). We apply A* search to a tabular item-
based parser, ordering the parse items based on a com-
bination of their known internal cost of construction and
a conservative estimate of their cost of completion (see
figure 1). A* search has been proposed and used for
speech applications (Goel and Byrne, 1999, Corazza et
al., 1994); however, it has been little used, certainly in the
recent statistical parsing literature, apparently because of
difficulty in conceptualizing and computing effective ad-
missible estimates. The contribution of this paper is to
demonstrate effective ways of doing this, by precomput-
ing grammar statistics which can be used as effective A*
estimates.
The A* formulation provides three benefits. First, it
substantially reduces the work required to parse a sen-
tence, without sacrificing either the optimality of the an-
swer or the worst-case cubic time bounds on the parser.
Second, the resulting parser is structurally simpler than a
FOM-driven best-first parser. Finally, it allows us to eas-
ily prove the correctness of our algorithm, over a broad
range of control strategies and grammar encodings.
In this paper, we describe two methods of construct-
ing A* bounds for PCFGs. One involves context sum-
marization, which uses estimates of the sort proposed in
Corazza et al (1994), but considering richer summaries.
The other involves grammar summarization, which, to
our knowledge, is entirely novel. We present the esti-
mates that we use, along with algorithms to efficiently
calculate them, and illustrate their effectiveness in a tab-
ular PCFG parsing algorithm, applied to Penn Treebank
sentences.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 40-47
                                                         Proceedings of HLT-NAACL 2003
words
S:[0,n]
start
X
?
?
DT:[0,1] NN:[1,2] VBZ:[2,3]
start
S:[0,3]
NP:[0,2]
VP:[0,2]
(a) (b)
Figure 1: A* edge costs. (a) The cost of an edge X is a com-
bination of the cost to build the edge (the Viterbi inside score
?) and the cost to incorporate it into a root parse (the Viterbi
outside score ?). (b) In the corresponding hypergraph, we have
exact values for the inside score from the explored hyperedges
(solid lines), and use upper bounds on the outside score, which
estimate the dashed hyperedges.
2 An A* Algorithm
An agenda-based PCFG parser operates on parse items
called edges, such as NP:[0,2], which denote a grammar
symbol over a span. The parser maintains two data struc-
tures: a chart or table, which records edges for which
(best) parses have already been found, and an agenda of
newly-formed edges waiting to be processed. The core
loop involves removing an edge from the agenda and
combining that edge with edges already in the chart to
create new edges. For example, NP:[0,2] might be re-
moved from the agenda, and, if there were a rule S ? NP
VP and VP:[2,8] was already entered into the chart, the
edge S:[0,8] would be formed, and added to the agenda if
it were not in the chart already.
The way an A* parser differs from a classic chart
parser is that, like a best-first parser, agenda edges are
processed according to a priority. In best-first parsing,
this priority is called a figure-of-merit (FOM), and is
based on various approximations to P (e|s), the frac-
tion of parses of a sentence s which include an edge e
(though see Goodman (1997) for an alternative notion of
FOM). Edges which seem promising are explored first;
others can wait on the agenda indefinitely. Note that
even if we did know P (e|s) exactly, we still would not
know whether e occurs in any best parse of s. Nonethe-
less, good FOMs empirically lead quickly to good parses.
Best-first parsing aims to find a (hopefully good) parse
quickly, but gives no guarantee that the first parse discov-
ered is the Viterbi parse, nor does it allow one to recog-
nize the Viterbi parse when it is found.
In A* parsing, we wish to construct priorities which
will speed up parsing, yet still guarantee optimality (that
the first parse returned is indeed a best parse). With a
categorical CFG chart parser run to exhaustion, it does
not matter in what order one removes edges from the
agenda; all edges involved in full parses of the sentence
will be constructed at some point. A cubic time bound
follows straightforwardly by simply testing for edge exis-
tence, ensuring that we never process an edge twice. With
PCFG parsing, there is a subtlety involved. In addition to
knowing whether edges can be constructed, we also want
to know the scores of edges? best parses. Therefore, we
record estimates of best-parse scores, updating them as
better parses are found. If, during parsing, we find a new,
better way to construct some edge e that has previously
been entered into the chart, we may also have found a bet-
ter way to construct any edges which have already been
built using e. Best-first parsers deal with this by allowing
an upward propagation, which updates such edges? scores
(Caraballo and Charniak, 1998). If run to exhaustion, all
edges? Viterbi scores will be correct, but the propagation
destroys the cubic time bound of the parser, since in effect
each edge can be processed many times.
In order to ensure optimality, it is sufficient that, for
any edge e, all edges f which are contained in a best
parse of e get removed from the agenda before e itself
does. If we have an edge priority which ensures this or-
dering, we can avoid upward propagation entirely (and
omit the data structures involved in it) and still be sure
that each edge leaves the agenda scored correctly. If the
grammar happens to be in CNF, one way to do this is to
give shorter spans higher priority than longer ones; this
priority essentially gives the CKY algorithm.
Formally, assume we have a PCFG G and a sentence
s = 0wn (we place indices as fenceposts between words).
An inside parse of an edge e = X :[i, j] is a derivation in
G from X to iwj . Let ?G(e, s) denote the log-probability
of a best inside parse of e (its Viterbi inside score).1 We
will drop the G, s, and even e when context permits. Our
parser, like a best-first parser, maintains estimates b(e, s)
of ?(e, s) which begin at ??, only increase over time,
and always represent the score of the best parses of their
edges e discovered so far. Optimality means that for any
e, b(e, s) will equal ?G(e, s) when e is removed from the
agenda.
If one uses b(e, s) to prioritize edges, we show in Klein
and Manning (2001a), that the parser is optimal over ar-
bitrary PCFGs, and a wide range of control strategies.
This is proved using an extension of Dijkstra?s algorithm
to a certain kind of hypergraph associated with parsing,
shown in figure 1(b): parse items are nodes in the hyper-
graph, hyperarcs take sets of parse items to their result
item, and hyperpaths map to parses. Reachability from
start corresponds to parseability, and shortest paths to
Viterbi parses.
1Our use of inside score and outside score evokes the same
picture as talk about inside and outside probabilities, but note
that in this paper inside and outside scores always refer to (a
bound on) the maximum (Viterbi) probability parse inside or
outside some edge, rather than to the sum for all such parses.
Estimate SX SXL SXLR TRUE
Summary (1,6,NP) (1,6,NP,VBZ) (1,6,NP,VBZ,?,?) (entire context)
Best Tree S
PP
IN
?
NP
NP
,
?
NP
DT
?
JJ
?
NN
?
VP
VBD
?
.
?
S
VP
VBZ
VBZ
NP
NP
PP
IN
?
NP
DT
?
NNP
?
NNP
?
NNP
?
NNP
?
S
VP
VBZ
VBZ
NP
NP
NP
,
,
CC
?
NP
DT
?
JJ
?
NN
?
,
?
S
S
VP
VBZ
VBZ
NP
NP
,
,
NP
PRP
PRP
VP
VBZ
VBZ
NP
DT
DT
NN
NN
.
.
Score ?11.3 ?13.9 ?15.1 ?18.1
(a) (b) (c) (d)
Figure 2: Best outside parses given richer summaries of edge context. (a ? SX) Knowing only the edge state (NP) and the left and
right outside spans, (b ? SXL) also knowing the left tag, (c ? SXLR) left and right tags, and (d ? TRUE) the entire outside context.
The hypergraph shown in figure 1(b) shows a parse of
the goal S:[0,3] which includes NP:[0,2].2 This parse can
be split into an inside portion (solid lines) and an outside
portion (dashed lines), as indicated in figure 1(a). The
outside portion is an outside parse: formally, an outside
parse of an edge X :[i, j] in sentence s = 0wn is a deriva-
tion from G?s root symbol to w0iXwjn. We use ?G(e, s)
to denote the score of a best outside parse of e.
Using b(e, s) as the edge priority corresponds to a gen-
eralization of uniform cost search on graphs (Russell and
Norvig, 1995). In the analogous generalization of A*
search, we add to b(e, s) an estimate a(e, s) of the com-
petion cost ?G(e, s) (the cost of the dashed outside parse)
to focus exploration on regions of the graph which appear
to have good total cost.
A* search is correct as long as the estimate a satis-
fies two conditions. First, it must be admissible, meaning
that it must not underestimate the actual log-probability
required to complete the parse. Second, it must be mono-
tonic, meaning that as one builds up a tree, the combined
log-probability ? + a never increases. The proof of this
is very similar to the proof of the uniform-cost case in
Klein and Manning (2001a), and so we omit it for space
reasons (it can be found in Klein and Manning, 2002).
Concretely, we can use b + a as the edge priority, pro-
vided a is an admissible, monotonic estimate of ?. We
will still have a correct algorithm, and even rough heuris-
tics can dramatically cut down the number of edges pro-
cessed (and therefore total work). We next discuss several
estimates, describe how to compute them efficiently, and
show the edge savings when parsing Penn treebank WSJ
sentences.
3 A* Estimates for Parsing
When parsing with a PCFG G, each edge e = X :[i, j]
spans some interval [i, j] of the sentence and is labeled
2The example here shows a bottom-up construction of a
parse tree. However, the present algorithm and estimates work
just as well for top-down chart parsing, given suitable active
items as nodes; see (Klein and Manning, 2001a).
by some grammar symbol (or state) X . Our presentation
assumes that G is a binarized grammar, and so in gen-
eral X may be either a complete state like NP that was
in an original n-ary grammar, or an intermediate state,
like an Earley dotted rule, that is the result of implicit or
explicit grammar binarization. For the edge e, its yield
in s = 0wn is the sequence of terminals that it spans
(iwj). Its context is its state X along with the rest of
the terminals of sentence (0wiXjwn). Scores are log-
probabilities; lower cost is higher log-probability. So, ?>?
or ?better? will mean higher log-probability.
3.1 Context Summary Estimates
One way to construct an admissible estimate is to sum-
marize the context in some way, and to find the score of
the best parse of any context that fits that summary. Let
c(e, s) be the context of e in s. Let ? be a summary func-
tion of contexts. We can then use the context summary
estimate:
a?(e, s) = max
(e?,s?):?(c(e?,s?))=
?(c(e,s))
?G(e?, s?) ? ?G(e, s)
That is, we return the exact Viterbi outside score for some
context, generally not the actual context, whose summary
matches the actual one?s summary. If the number of sum-
maries is reasonable, we can precompute and store the
estimate for each summary once and for all, then retrieve
them in constant time per edge at parse time.
If we give no information in the summary, the estimate
will be constantly 0. This is the trivial estimate NULL,
and corresponds to simply using inside estimates b alone
as priorities. On the other extreme, if each context had
a unique summary, then a(e, s) would be ?G(e, s) itself.
This is the ideal estimate, which we call TRUE. In prac-
tice, of course, precomputing TRUE would not be feasi-
ble.3
3Note that our ideal estimate is not P (e|s) like the ideal
FOM, rather it is P (Tg,e)/P (Te) (where Tg,e is a best parse of
the goal g among those which contain e, and Te is a best parse
of e over the yield of e). That is, we are not estimating parser
choice probabilities, but parse tree probabilities.
We used various intermediate summaries, some illus-
trated in figure 2. S1 specifies only the total number of
words outside e, while S specifies separately the number
to the left and right. SX also specifies e?s label. SXL and
SXR add the tags adjacent to e on the left and right re-
spectively. S1XLR includes both the left and right tags,
but merges the number of words to the left and right.4
As the summaries become richer, the estimates become
sharper. As an example, consider an NP in the context
?VBZ NP , PRP VBZ DT NN .? shown in figure 2.5 The
summary SX reveals only that there is an NP with 1 word
to the left and 6 the right, and gives an estimate of ?11.3.
This score is backed by the concrete parse shown in fig-
ure 2(a). This is a best parse of a context compatible with
what little we specified, but very optimistic. It assumes
very common tags in very common patterns. SXL adds
that the tag to the left is VBZ, and the hypothesis that the
NP is part of a sentence-initial PP must be abandoned;
the best score drops to ?13.9, backed by the parse in fig-
ure 2(b). Specifying the right tag to be ?,? drops the score
further to ?15.1, given by figure 2(c). The actual best
parse is figure 2(d), with a score of ?18.1.
These estimates are similar to quantities calculated in
Corazza et al (1994); in that work, they are interested
in the related problem of finding best completions for
strings which contain gaps. For the SX estimate, for
example, the string would be the edge?s label and two
(fixed-length) gaps. They introduce quantities essentially
the same as our SX estimate to fill gaps, and their one-
word update algorithms are similarly related to those we
use here. The primary difference here is in the application
of these quantities, not their calculation.
3.2 Grammar Projection Estimates
The context summary estimates described above use local
information, combined with span sizes. This gives the
effect that, for larger contexts, the best parses which back
the estimates will have less and less to do with the actual
contexts (and hence will become increasingly optimistic).
Context summary estimates do not pin down the exact
context, but do use the original grammar G. For grammar
projection estimates, we use the exact context, but project
the grammar to some G? which is so much simpler that it
is feasible to first exhaustively parse with G? and then use
the result to guide the search in the full grammar G.
Formally, we have a projection pi which maps gram-
mar states of G (that is, the dotted rules of an Earley-style
parser) to some reduced set. This projection of states in-
duces a projection of rules. If a set R = {r} of rules in
G collide as the rule r? in G?, we give r? the probability
4Merging the left and right outside span sizes in S1XLR was
done solely to reduce memory usage.
5Our examples, and our experiments, use delexicalized sen-
tences from the Penn treebank.
Grammar State
Projection NP CC NP ? ? CC NP CC NP
NULL X X X
SX NP X NP ? ? X NP X NP
XBAR NP CC NP?
F X CC X ? ? CC X CC X
TRUE NP CC NP ? ? CC NP CC NP
Figure 3: Examples of grammar state images under several
grammar projections.
P (r?) = maxr?R P (r). Note that the resulting grammar
G? will not generally be a proper PCFG; it may assign
more than probability 1 to the set of trees it generates. In
fact, it will usually assign infinite mass. However, all that
matters for our purposes is that every tree in G projects
under pi to a tree in G? with the same or higher probabil-
ity, which is true because every rule in G does. There-
fore, we know that ?G(e, s) ? ?G?(e, s). If G? is much
more compact than G, for each new sentence s, we can
first rapidly calculate api = ?G? for all edges, then parse
with G.
The identity projection ? returns G and therefore a? is
TRUE. On the other extreme, a constant projection gives
NULL (if any rewrite has probability 1). In between, we
tried three other grammar projection estimates (examples
in figure 3). First, consider mapping all terminal states to
a single terminal token, but not altering the grammar in
any other way. If we do this projection, then we get the
SX estimate from the last section (collapsing the termi-
nals together effectively hides which terminals are in the
context, but not their number). However, the resulting
grammar is nearly as large as G, and therefore it is much
more efficient to use the precomputed context summary
formulation. Second, for the projection XBAR, we tried
collapsing all the incomplete states of each complete state
to a single state (so NP? ? CC NP and NP? ? PP would
both become NP?). This turned out to be ineffective, since
most productions then had merged probability 1.
For our current grammar, the best estimate of this type
was one we called F, for filter, which collapsed all com-
plete (passive) symbols together, but did not collapse any
terminal symbols. So, for example, a state like NP? ? CC
NP CC NP would become X? ? CC X CC X (see section 3.3
for a description of our grammar encodings). This esti-
mate has an interesting behavior which is complementary
to the context summary estimates. It does not indicate
well when an edge would be moderately expensive to in-
tegrate into a sentence, but it is able to completely elimi-
nate certain edges which are impossible to integrate into
a full parse (for example in this case maybe the two CC
tags required to complete the NP are not present in the
future context).
A close approximation to the F estimate can also be
computed online especially quickly during parsing. Since
TRUE
S1
S
SX
SXRSXL
SXMLR
S1XLR
F
B
BF
NULL
unionsq
unionsq
unionsq
TRUESX
SXL SXR
F
NULL
CONTEXT SUMMARY
GRAMMAR PROJECTION
(a) (b)
Figure 4: (a) The A* estimates form a lattice. Lines indicate
subsumption, unionsq indicates estimates which are the explicit join
of lower estimates. (b) Context summary vs. grammar projec-
tion estimates: some estimates can be cast either way.
we are parsing with the Penn treebank covering gram-
mar, almost any (phrasal) non-terminal can be built over
almost any span. As discussed in Klein and Manning
(2001b), the only source of constraint on what edges can
be built where is the tags in the rules. Therefore, an edge
with a label like NP? ? CC NP CC NP can essentially be
built whenever (and only whenever) two CC tags are in
the edge?s right context, one of them being immediately
to the right. To the extent that this is true, F can be ap-
proximated by simply scanning for the tag configuration
required by a state?s local rule, and returning 0 if it is
present and ?? otherwise. This is the method we used
to implement F; exactly parsing with the projected gram-
mar was much slower and did not result in substantial
improvement.
It is worth explicitly discussing how the F estimate dif-
fers from top-down grammar-driven filtering standardly
used by top-down chart parsers; in the treebank grammar,
there is virtually no top-down filtering to be exploited
(again, see Klein and Manning (2001b)). In a left-to-right
parse, top-down filtering is a prefix licensing condition; F
is more of a sophisticated lookahead condition on suf-
fixes.
The relationships between all of these estimates are
shown in figure 4. The estimates form a join lattice (fig-
ure 4(a)): adding context information to a merged con-
text estimate can only sharpen the individual outside es-
timates. In this sense, for example S ? SX. The lattice
top is TRUE and the bottom is NULL. In addition, the
minimum (unionsq) of a set of admissible estimates is still an
admissible estimate. We can use this to combine our ba-
sic estimates into composite estimates: SXMLR = unionsq (SXL,
SXR) will be valid, and a better estimate than either SXL
or SXR individually. Similarly, B is unionsq (SXMLR, S1XLR).
There are other useful grammar projections, which are
beyond the scope of this paper. First, much recent statisti-
cal parsing work has gotten value from splitting grammar
Original Rules Outside-Trie Rules Inside-Trie Rules
NP ? DT JJ NN 0.3 NP ? XNP? ? NN NN 0.4 NP ? XDT JJ NN 0.3
NP ? DT NN NN 0.1 XNP? ? NN ? DT JJ 0.75 NP ? XDT NN NN 0.1
XNP? ? NN ? DT NN 0.25 XDT JJ ? DT JJ 1.0
XDT NN ? DT NN 1.0
Figure 5: Two trie encodings of rules.
0
10
20
30
40
50
60
70
80
90
100
NU
LL S SX SX
L
S1
XL
R
SX
R
SX
ML
R B
Ed
ge
s 
Bl
o
ck
ed
O-Tries
I-Tries
Figure 6: Fraction of edges saved by using various estimate
methods, for two rule encodings. O-TRIE is a deterministic
right-branching trie encoding (Leermakers, 1992) with weights
pushed left (Mohri, 1997). I-TRIE is a non-deterministic left-
branching trie with weights on rule entry as in Charniak et al
(1998).
states, such as by annotating nodes with their parent and
even grandparent categories (Johnson, 1998). This anno-
tation multiplies out the state space, giving a much larger
grammar, and projecting back to the unannotated state set
can be used as an outside estimate. Second, and perhaps
more importantly, this technique can be applied to lexical
parsing, where the state projections are onto the delex-
icalized PCFG symbols and/or onto the word-word de-
pendency structures. This is particularly effective when
the tree model takes a certain factored form; see Klein
and Manning (2003) for details.
3.3 Parsing Performance
Following (Charniak et al, 1998), we parsed unseen sen-
tences of length 18?26 from the Penn Treebank, using the
grammar induced from the remainder of the treebank.6
We tried all estimates described above.
Rules were encoded as both inside (I) and outside (O)
tries, shown in figure 5. Such an encoding binarizes the
grammar, and compacts it. I-tries are as in Charniak et
al. (1998), where NP? DT JJ NN becomes NP ? XDT JJ
NN and XDT JJ ? DT JJ, and correspond to dropping the
portion of an Earley dotted rule after the dot.7 O-tries,
as in Leermakers (1992), turn NP? DT JJ NN into NP ?
XNP? ? NN NN and XNP? ? NN ? DT JJ, and correspond to
6We chose the data set used by Charniak and coauthors, so
as to facilitate comparison with previous work. We do however
acknowledge that many of our current local estimates are less
effective on longer spans, and so would work less well on 40?
50 word sentences. This is an area of future research.
7In Charniak et al (1998), the binarization is in the reverse
direction; we binarize into a left chain because it is the standard
direction implicit in chart parsers? dotted rules, and the direction
makes little difference in edge counts.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 2000 4000 6000 8000 10000
Edges Processed
Se
n
te
n
ce
s 
Pa
rs
ed
BF
SXF
B
SXR
SXL
SX
S
Figure 7: Number of sentences parsed as more edges are ex-
panded. Sentences are Penn treebank sentences of length 18?26
parsed with the treebank grammar. A typical number of edges
in an exhaustive parse is 150,000. Even relatively simple A*
estimates allow substantial savings.
dropping the portion which precedes the dot. Figure 6
shows the overall savings for several estimates of each
type. The I-tries were superior for the coarser estimates,
while O-tries were superior for the finer estimates. In
addition, only O-tries permit the accelerated version of
F, since they explicitly declare their right requirements.
Additionally, with I-tries, only the top-level intermedi-
ate rules have probability less than 1, while for O-tries,
one can back-weight probability as in (Mohri, 1997), also
shown in figure 5, enabling sub-parts of rare rules to be
penalized even before they are completed.8 For all sub-
sequent results, we discuss only the O-trie numbers.
Figure 8 lists the overall savings for each context sum-
mary estimate, with and without F joined in. We see that
the NULL estimate (i.e., uniform cost search) is not very
effective ? alone it only blocks 11% of the edges. But it
is still better than exhaustive parsing: with it, one stops
parsing when the best parse is found, while in exhaustive
parsing one continues until no edges remain. Even the
simplest non-trivial estimate, S, blocks 40% of the edges,
and the best estimate BF blocks over 97% of the edges, a
speed-up of over 35 times, without sacrificing optimality
or algorithmic complexity.
For comparison to previous FOM work, figure 7
shows, for an edge count and an estimate, the propor-
tion of sentences for which a first parse was found us-
ing at most that many edges. To situate our results, the
FOMs used by (Caraballo and Charniak, 1998) require
10K edges to parse 96% of these sentences, while BF re-
quires only 6K edges. On the other hand, the more com-
plex, tuned FOM in (Charniak et al, 1998) is able to parse
all of these sentences using around 2K edges, while BF
requires 7K edges. Our estimates do not reduce the to-
tal edge count quite as much as the best FOMs can, but
they are in the same range. This is as much as one could
possibly expect, since, crucially, our first parses are al-
8However, context summary estimates which include the
state compensate for this automatically.
Estimate Savings w/ Filter Storage Precomp
NULL 11.2 58.3 0K none
S 40.5 77.8 2.5K 1 min
SX 80.3 95.3 5M 1 min
SXL 83.5 96.1 250M 30 min
S1XLR 93.5 96.5 500M 480 min
SXR 93.8 96.9 250M 30 min
SXMLR 94.3 97.1 500M 60 min
B 94.6 97.3 1G 540 min
Figure 8: The trade-off between online savings and precompu-
tation time.
ways optimal, while the FOM parses need not be (and
indeed sometimes are not).9 Also, our parser never needs
to propagate score changes upwards, and so may be ex-
pected to do less work overall per edge, all else being
equal. This savings is substantial, even if no propaga-
tion is done, because no data structure needs to be cre-
ated to track the edges which are supported by each given
edge (for us, this represents a factor of approximately
2 in memory savings). Moreover, the context summary
estimates require only a single table lookup per edge,
while the accelerated version of F requires only a rapid
quadratic scan of the input per sentence (less than 1% of
parse time per sentence), followed by a table lookup per
edge. The complex FOMs in (Charniak et al, 1998) re-
quire somewhat more online computation to assemble.
It is interesting that SXR is so much more effective than
SXL; this is primarily because of the way that the rules
have been encoded. If we factor the rules in the other
direction, we get the opposite effect. Also, when com-
bined with F, the difference in their performance drops
from 10.3% to 0.8%; F is a right-filter and is partially
redundant when added to SXR, but is orthogonal to SXL.
3.4 Estimate Sharpness
A disadvantage of admissibility for the context summary
estimates is that, necessarily, they are overly optimistic
as to the contents of the outside context. The larger the
outside context, the farther the gap between the true cost
and the estimate. Figure 9 shows average outside esti-
mates for Viterbi edges as span size increases. For small
outside spans, all estimates are fairly good approxima-
tions of TRUE. As the span increases, the approximations
fall behind. Beyond the smallest outside spans, all of the
curves are approximately linear, but the actual value?s
slope is roughly twice that of the estimates. The gap
between our empirical methods and the true cost grows
fairly steadily, but the differences between the empirical
methods themselves stay relatively constant. This reflects
9In fact, the bias from the FOM commonly raises the bracket
accuracy slightly over the Viterbi parses, but that difference nev-
ertheless demonstrates that the first parses are not always the
Viterbi ones. In our experiments, non-optimal pruning some-
times bought slight per-node accuracy gains at the cost of a
slight drop in exact match.
-40
-35
-30
-25
-20
-15
-10
-5
0
2 4 6 8 10 12 14 16 18
OutsideSpan
Av
e
ra
ge
A*
Es
tim
a
te
S
SX
SXR
B
TRUE
Figure 9: The average estimate by outside span length for var-
ious methods. For large outside spans, the estimates differ by
relatively constant amounts.
the nature of these estimates: they have differing local in-
formation in their summaries, but all are equally ignorant
about the more distant context elements. The various lo-
cal environments can be more or less costly to integrate
into a parse, but, within a few words, the local restric-
tions have been incorporated one way or another, and the
estimates are all free to be equally optimistic about the
remainder of the context. The cost to ?package up? the
local restrictions creates their constant differences, and
the shared ignorance about the wider context causes their
same-slope linear drop-off. This suggests that it would
be interesting to explore other, more global, notions of
context. We do not claim that our context estimates are
the best possible ? one could hope to find features of the
context, such as number of verbs to the right or number
of unusual tags in the context, which would partition the
contexts more effectively than adjacent tags, especially as
the outside context grows in size.
3.5 Estimate Computation
The amount of work required to (pre)calculate context
summary estimates depends on how easy it is to effi-
ciently take the max over all parses compatible with each
context summary. The benefit provided by an estimate
will depend on how well the restrictions in that summary
nail down the important features of the full context.
Figure 10 shows recursive pseudocode for the SX es-
timate; the others are similar. To precalculate our A*
estimates efficiently, we used a memoization approach
rather than a dynamic programming approach. This re-
sulted in code comparable in efficiency, but which was
simpler to reason about, and, more importantly, allowed
us to exploit sparseness when present. For example with
left-factored trie encodings, 76% of (state, right tag) com-
binations are simply impossible. Tables which mapped
arguments to returned results were used to memoize each
procedure. In our experiments, we forced these tables to
be filled in a precomputation step, but depending on the
situation it might be advantageous to allow them to fill
as needed, with early parses proceeding slowly while the
outsideSX(state, lspan, rspan)
if (lspan+rspan == 0)
if state is the root then 0 else ??
score = ??
% could have a left sibling
for sibsize in [0,lspan-1]
for (x?y state) in grammar
cost = insideSX(y,sibsize)+
outsideSX(x,lspan-sibsize,rspan)+
logP (x?y state)
score = max(score,cost)
% could have a right sibling
for sibsize in [0,rspan-1]
for (x?state y) in grammar
cost = insideSX(y,sibsize)+
outsideSX(x,lspan,rspan-sibsize)+
logP (x?state y)
score = max(score,cost)
return score;
insideSX(state, span)
if (span == 0)
if state is a terminal then 0 else ??
score = ??
% choose a split point
for split in [1,span-1]
for (state?x y) in grammar
cost = insideSX(x,split)+
insideSX(y,span-split)+
logP (state?x y)
score = max(score,cost)
return score;
Figure 10: Pseudocode for the SX estimate in the case where
the grammar is in CNF. Other estimates and more general gram-
mars are similar.
tables populate.
With the optimal forward estimate TRUE, the actual
distance to the closest goal, we would never expand edges
other than those in best parses, but computing TRUE is as
hard as parsing the sentence in the first place. On the
other hand, no precomputation is needed for NULL. In
between is a trade off of space/time requirements for pre-
computation and the online savings during the parsing of
new sentences. Figure 8 shows the average savings ver-
sus the precomputation time.10 Where on this curve one
chooses to be depends on many factors; 9 hours may be
too much to spend computing B, but an hour for SXMLR
gives nearly the same performance, and the one minute
required for SX is comparable to the I/O time to read the
Penn treebank in our system.
The grammar projection estimate F had to be recom-
puted for each sentence parsed, but took less than 1% of
the total parse time. Although this method alone was less
effective than SX (only 58.3% edge savings), it was ex-
tremely effective in combination with the context sum-
mary methods. In practice, the combination of F and SX
is easy to implement, fast to initialize, and very effective:
10All times are for a Java implementation running on a 2GB
700MHz Intel machine.
one cuts out 95% of the work in parsing at the cost of
one minute of precomputation and 5 Mb of storage for
outside estimates for our grammar.
4 Extension to Other Models
While the A* estimates given here can be used to accel-
erate PCFG parsing, most high-performance parsing has
utilized models over lexicalized trees. These A* methods
can be adapted to the lexicalized case. In Klein and Man-
ning (2003), we apply a pair of grammar projection esti-
mates to a lexicalized parsing model of a certain factored
form. In that model, the score of a lexicalized tree is the
product of the scores of two projections of that tree, one
onto unlexicalized phrase structure, and one onto phrasal-
category-free word-to-word dependency structure. Since
this model has a projection-based form, grammar projec-
tion methods are easy to apply and especially effective,
giving over three orders of magnitude in edge savings.
The total cost per sentence includes the time required for
two exhaustive PCFG parses, after which the A* search
takes only seconds, even for very long sentences. Even
when a lexicalized model is not in this factored form, it
still admits factored grammar projection bounds; we are
currently investigating this case.
5 Conclusions
An A* parser is simpler to build than a best-first parser,
does less work per edge, and provides both an optimality
guarantee and a worst-case cubic time bound. We have
described two general ways of constructing admissible
A* estimates for PCFG parsing and given several specific
estimates. Using these estimates, our parser is capable of
finding the Viterbi parse of an average-length Penn tree-
bank sentence in a few seconds, processing less than 3%
of the edges which would be constructed by an exhaustive
parser.
Acknowledgements.
We would like to Joshua Goodman and Dan Melamed
for advice and discussion about this work. This paper
is based on work supported by the National Science
Foundation (NSF) under Grant No. IIS-0085896,
by the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intelligence
(AQUAINT) Program, by an NSF Graduate Fellowship to
the first author, and by an IBM Faculty Partnership Award
to the second author.
References
Sharon A. Caraballo and Eugene Charniak. 1998. New figures
of merit for best-first probabilistic chart parsing. Computa-
tional Linguistics, 24:275?298.
Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998.
Edge-based best-first chart parsing. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 127?133.
Morgan Kaufmann.
Mahesh V. Chitrao and Ralph Grishman. 1990. Statistical pars-
ing of messages. In Proceedings of the DARPA Speech and
Natural Language Workshop, Hidden Valley, PA, pages 263?
266. Morgan Kaufmann.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Anna Corazza, Renato De Mori, Roberto Gretter, and Giorgio
Satta. 1994. Optimal probabilistic evaluation functions for
search controlled by stochastic context-free grammars. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
16(10):1018?1027.
Vaibhava Goel and William J. Byrne. 1999. Task dependent loss
functions in speech recognition: A-star search over recogni-
tion lattices. In Eurospeech-99, pages 1243?1246.
Joshua Goodman. 1997. Global thresholding and multiple-pass
parsing. In EMNLP 2, pages 11?25.
Mark Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24:613?632.
Dan Klein and Christopher D. Manning. 2001a. Parsing and hy-
pergraphs. In Proceedings of the 7th International Workshop
on Parsing Technologies (IWPT-2001).
Dan Klein and Christopher D. Manning. 2001b. Parsing with
treebank grammars: Empirical bounds, theoretical models,
and the structure of the Penn treebank. In ACL 39/EACL 10,
pages 330?337.
Dan Klein and Christopher D. Manning. 2002. A* parsing:
Fast exact Viterbi parse selection. Technical Report dbpubs/
2002-16, Stanford University, Stanford, CA.
Dan Klein and Christopher D. Manning. 2003. Fast exact in-
ference with a factored model for natural language parsing.
In Advances in Neural Information Processing Systems, vol-
ume 15. MIT Press.
Rene? Leermakers. 1992. A recursive ascent Earley parser. In-
formation Processing Letters, 41:87?91.
Mehryar Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(4):269?
311.
Adwait Ratnaparkhi. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34:151?
175.
Brian Roark. 2001. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27:249?276.
Stuart J. Russell and Peter Norvig. 1995. Artificial Intelligence:
A Modern Approach. Prentice Hall, Englewood Cliffs, NJ.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network
Kristina Toutanova Dan Klein
Computer Science Dept. Computer Science Dept.
Stanford University Stanford University
Stanford, CA 94305-9040 Stanford, CA 94305-9040
kristina@cs.stanford.edu klein@cs.stanford.edu
Christopher D. Manning Yoram Singer
Computer Science Dept. School of Computer Science
Stanford University The Hebrew University
Stanford, CA 94305-9040 Jerusalem 91904, Israel
manning@stanford.edu singer@cs.huji.ac.il
Abstract
We present a new part-of-speech tagger that
demonstrates the following ideas: (i) explicit
use of both preceding and following tag con-
texts via a dependency network representa-
tion, (ii) broad use of lexical features, includ-
ing jointly conditioning on multiple consecu-
tive words, (iii) effective use of priors in con-
ditional loglinear models, and (iv) fine-grained
modeling of unknown word features. Using
these ideas together, the resulting tagger gives
a 97.24% accuracy on the Penn Treebank WSJ,
an error reduction of 4.4% on the best previous
single automatically learned tagging result.
1 Introduction
Almost all approaches to sequence problems such as part-
of-speech tagging take a unidirectional approach to con-
ditioning inference along the sequence. Regardless of
whether one is using HMMs, maximum entropy condi-
tional sequence models, or other techniques like decision
trees, most systems work in one direction through the
sequence (normally left to right, but occasionally right
to left, e.g., Church (1988)). There are a few excep-
tions, such as Brill?s transformation-based learning (Brill,
1995), but most of the best known and most successful
approaches of recent years have been unidirectional.
Most sequence models can be seen as chaining to-
gether the scores or decisions from successive local mod-
els to form a global model for an entire sequence. Clearly
the identity of a tag is correlated with both past and future
tags? identities. However, in the unidirectional (causal)
case, only one direction of influence is explicitly consid-
ered at each local point. For example, in a left-to-right
first-order HMM, the current tag t0 is predicted based on
the previous tag t?1 (and the current word).1 The back-
ward interaction between t0 and the next tag t+1 shows
up implicitly later, when t+1 is generated in turn. While
unidirectional models are therefore able to capture both
directions of influence, there are good reasons for sus-
pecting that it would be advantageous to make informa-
tion from both directions explicitly available for condi-
tioning at each local point in the model: (i) because of
smoothing and interactions with other modeled features,
terms like P(t0|t+1, . . .) might give a sharp estimate of t0
even when terms like P(t+1|t0, . . .) do not, and (ii) jointly
considering the left and right context together might be
especially revealing. In this paper we exploit this idea,
using dependency networks, with a series of local con-
ditional loglinear (aka maximum entropy or multiclass
logistic regression) models as one way of providing ef-
ficient bidirectional inference.
Secondly, while all taggers use lexical information,
and, indeed, it is well-known that lexical probabilities
are much more revealing than tag sequence probabilities
(Charniak et al, 1993), most taggers make quite limited
use of lexical probabilities (compared with, for example,
the bilexical probabilities commonly used in current sta-
tistical parsers). While modern taggers may be more prin-
cipled than the classic CLAWS tagger (Marshall, 1987),
they are in some respects inferior in their use of lexical
information: CLAWS, through its IDIOMTAG module,
categorically captured many important, correct taggings
of frequent idiomatic word sequences. In this work, we
incorporate appropriate multiword feature templates so
that such facts can be learned and used automatically by
1Rather than subscripting all variables with a position index,
we use a hopefully clearer relative notation, where t0 denotes
the current position and t?n and t+n are left and right context
tags, and similarly for words.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 173-180
                                                         Proceedings of HLT-NAACL 2003
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(a) Left-to-Right CMM
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(b) Right-to-Left CMM
w1 w2 w3 . . . . wn
t1 t2 t3 tn
(c) Bidirectional Dependency Network
Figure 1: Dependency networks: (a) the (standard) left-to-right
first-order CMM, (b) the (reversed) right-to-left CMM, and (c)
the bidirectional dependency network.
the model.
Having expressive templates leads to a large number
of features, but we show that by suitable use of a prior
(i.e., regularization) in the conditional loglinear model ?
something not used by previous maximum entropy tag-
gers ? many such features can be added with an overall
positive effect on the model. Indeed, as for the voted per-
ceptron of Collins (2002), we can get performance gains
by reducing the support threshold for features to be in-
cluded in the model. Combining all these ideas, together
with a few additional handcrafted unknown word fea-
tures, gives us a part-of-speech tagger with a per-position
tag accuracy of 97.24%, and a whole-sentence correct
rate of 56.34% on Penn Treebank WSJ data. This is the
best automatically learned part-of-speech tagging result
known to us, representing an error reduction of 4.4% on
the model presented in Collins (2002), using the same
data splits, and a larger error reduction of 12.1% from the
more similar best previous loglinear model in Toutanova
and Manning (2000).
2 Bidirectional Dependency Networks
When building probabilistic models for tag sequences,
we often decompose the global probability of sequences
using a directed graphical model (e.g., an HMM (Brants,
2000) or a conditional Markov model (CMM) (Ratna-
parkhi, 1996)). In such models, the probability assigned
to a tagged sequence of words x = ?t, w? is the product
of a sequence of local portions of the graphical model,
one from each time slice. For example, in the left-to-right
CMM shown in figure 1(a),
P(t, w) =
?
i
P(ti |ti?1, wi )
That is, the replicated structure is a local model
P(t0|t?1, w0).2 Of course, if there are too many con-
ditioned quantities, these local models may have to be
estimated in some sophisticated way; it is typical in tag-
ging to populate these models with little maximum en-
tropy models. For example, we might populate a model
for P(t0|t?1, w0) with a maxent model of the form:
P?(t0|t?1, w0) =
exp(??t0,t?1? + ??t0,w0?)
?
t ?0
exp(??t ?0,t?1? + ??t ?0,w0?)
In this case, the w0 and t?1 can have joint effects on t0, but
there are not joint features involving all three variables
(though there could have been such features). We say that
this model uses the feature templates ?t0, t?1? (previous
tag features) and ?t0, w0? (current word features).
Clearly, both the preceding tag t?1 and following tag
t+1 carry useful information about a current tag t0. Unidi-
rectional models do not ignore this influence; in the case
of a left-to-right CMM, the influence of t?1 on t0 is ex-
plicit in the P(t0|t?1, w0) local model, while the influ-
ence of t+1 on t0 is implicit in the local model at the next
position (via P(t+1|t0, w+1)). The situation is reversed
for the right-to-left CMM in figure 1(b).
From a seat-of-the-pants machine learning perspective,
when building a classifier to label the tag at a certain posi-
tion, the obvious thing to do is to explicitly include in the
local model all predictive features, no matter on which
side of the target position they lie. There are two good
formal reasons to expect that a model explicitly condi-
tioning on both sides at each position, like figure 1(c)
could be advantageous. First, because of smoothing
effects and interaction with other conditioning features
(like the words), left-to-right factors like P(t0|t?1, w0)
do not always suffice when t0 is implicitly needed to de-
termine t?1. For example, consider a case of observation
bias (Klein and Manning, 2002) for a first-order left-to-
right CMM. The word to has only one tag (TO) in the PTB
tag set. The TO tag is often preceded by nouns, but rarely
by modals (MD). In a sequence will to fight, that trend
indicates that will should be a noun rather than a modal
verb. However, that effect is completely lost in a CMM
like (a): P(twill |will, ?star t?) prefers the modal tagging,
and P(TO|to, twill ) is roughly 1 regardless of twill . While
the model has an arrow between the two tag positions,
that path of influence is severed.3 The same problem ex-
ists in the other direction. If we use the symmetric right-
2Throughout this paper we assume that enough boundary
symbols always exist that we can ignore the differences which
would otherwise exist at the initial and final few positions.
3Despite use of names like ?label bias? (Lafferty et al, 2001)
or ?observation bias?, these effects are really just unwanted
explaining-away effects (Cowell et al, 1999, 19), where two
nodes which are not actually in causal competition have been
modeled as if they were.
A B A B A B
(a) (b) (c)
Figure 2: Simple dependency nets: (a) the Bayes? net for
P(A)P(B|A), (b) the Bayes? net for P(A|B)P(B), (c) a bidi-
rectional net with models of P(A|B) and P(B|A), which is not
a Bayes? net.
to-left model, fight will receive its more common noun
tagging by symmetric reasoning. However, the bidirec-
tional model (c) discussed in the next section makes both
directions available for conditioning at all locations, us-
ing replicated models of P(t0|t?1, t+1, w0), and will be
able to get this example correct.4
2.1 Semantics of Dependency Networks
While the structures in figure 1(a) and (b) are well-
understood graphical models with well-known semantics,
figure 1(c) is not a standard Bayes? net, precisely because
the graph has cycles. Rather, it is a more general de-
pendency network (Heckerman et al, 2000). Each node
represents a random variable along with a local condi-
tional probability model of that variable, conditioned on
the source variables of all incoming arcs. In this sense,
the semantics are the same as for standard Bayes? nets.
However, because the graph is cyclic, the net does not
correspond to a proper factorization of a large joint prob-
ability estimate into local conditional factors. Consider
the two-node cases shown in figure 2. Formally, for the
net in (a), we can write P(a, b) = P(a)P(b|a). For (b)
we write P(a, b) = P(b)P(a|b). However, in (c), the
nodes A and B carry the information P(a|b) and P(b|a)
respectively. The chain rule doesn?t allow us to recon-
struct P(a, b) by multiplying these two quantities. Un-
der appropriate conditions, we could reconstruct P(a, b)
from these quantities using Gibbs sampling, and, in gen-
eral, that is the best we can do. However, while recon-
structing the joint probabilities from these local condi-
tional probabilities may be difficult, estimating the local
probabilities themselves is no harder than it is for acyclic
models: we take observations of the local environments
and use any maximum likelihood estimation method we
desire. In our experiments, we used local maxent models,
but if the event space allowed, (smoothed) relative counts
would do.
4The effect of indirect influence being weaker than direct in-
fluence is more pronounced for conditionally structured models,
but is potentially an issue even with a simple HMM. The prob-
abilistic models for basic left-to-right and right-to-left HMMs
with emissions on their states can be shown to be equivalent us-
ing Bayes? rule on the transitions, provided start and end sym-
bols are modeled. However, this equivalence is violated in prac-
tice by the addition of smoothing.
function bestScore()
return bestScoreSub(n + 2, ?end, end, end?);
function bestScoreSub(i + 1, ?ti?1, ti , ti+1?)
% memoization
if (cached(i + 1, ?ti?1, ti , ti+1?))
return cache(i + 1, ?ti?1, ti , ti+1?);
% left boundary case
if (i = ?1)
if (?ti?1, ti , ti+1? == ?star t, star t, star t?)
return 1;
else
return 0;
% recursive case
return maxti?2 bestScoreSub(i, ?ti?2, ti?1, ti ?)?
P(ti |ti?1, ti+1, wi );
Figure 3: Pseudocode for polynomial inference for the first-
order bidirectional CMM (memoized version).
2.2 Inference for Linear Dependency Networks
Cyclic or not, we can view the product of local probabil-
ities from a dependency network as a score:
score(x) =
?
i
P(xi |Pa(xi ))
where Pa(xi ) are the nodes with arcs to the node xi . In the
case of an acyclic model, this score will be the joint prob-
ability of the event x , P(x). In the general case, it will not
be. However, we can still ask for the event, in this case the
tag sequence, with the highest score. For dependency net-
works like those in figure 1, an adaptation of the Viterbi
algorithm can be used to find the maximizing sequence
in polynomial time. Figure 3 gives pseudocode for the
concrete case of the network in figure 1(d); the general
case is similar, and is in fact just a max-plus version of
standard inference algorithms for Bayes? nets (Cowell et
al., 1999, 97). In essence, there is no difference between
inference on this network and a second-order left-to-right
CMM or HMM. The only difference is that, when the
Markov window is at a position i , rather than receiving
the score for P(ti |ti?1, ti?2, wi ), one receives the score
for P(ti?1|ti , ti?2, wi?1).
There are some foundational issues worth mention-
ing. As discussed previously, the maximum scoring se-
quence need not be the sequence with maximum likeli-
hood according to the model. There is therefore a worry
with these models about a kind of ?collusion? where the
model locks onto conditionally consistent but jointly un-
likely sequences. Consider the two-node network in fig-
ure 2(c). If we have the following distribution of ob-
servations (in the form ab) ?11, 11, 11, 12, 21, 33?, then
clearly the most likely state of the network is 11. How-
ever, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1)
= 3/4 ? 3/4 = 9/16, while the score of 33 is 1. An ad-
ditional related problem is that the training set loss (sum
of negative logarithms of the sequence scores) does not
bound the training set error (0/1 loss on sequences) from
Data Set Sect?ns Sent. Tokens Unkn
Training 0?18 38,219 912,344 0
Develop 19?21 5,527 131,768 4,467
Test 22?24 5,462 129,654 3,649
Table 1: Data set splits used.
above. Consider the following training set, for the same
network, with each entire data point considered as a label:
?11, 22?. The relative-frequency model assigns loss 0 to
both training examples, but cannot do better than 50%
error in regenerating the training data labels. These is-
sues are further discussed in Heckerman et al (2000).
Preliminary work of ours suggests that practical use of
dependency networks is not in general immune to these
theoretical concerns: a dependency network can choose a
sequence model that is bidirectionally very consistent but
does not match the data very well. However, this problem
does not appear to have prevented the networks from per-
forming well on the tagging problem, probably because
features linking tags and observations are generally much
sharper discriminators than tag sequence features.
It is useful to contrast this framework with the con-
ditional random fields of Lafferty et al (2001). The
CRF approach uses similar local features, but rather than
chaining together local models, they construct a sin-
gle, globally normalized model. The principal advan-
tage of the dependency network approach is that advan-
tageous bidirectional effects can be obtained without the
extremely expensive global training required for CRFs.
To summarize, we draw a dependency network in
which each node has as neighbors all the other nodes
that we would like to have influence it directly. Each
node?s neighborhood is then considered in isolation and
a local model is trained to maximize the conditional like-
lihood over the training data of that node. At test time,
the sequence with the highest product of local conditional
scores is calculated and returned. We can always find the
exact maximizing sequence, but only in the case of an
acyclic net is it guaranteed to be the maximum likelihood
sequence.
3 Experiments
The part of speech tagged data used in our experiments is
the Wall Street Journal data from Penn Treebank III (Mar-
cus et al, 1994). We extracted tagged sentences from the
parse trees.5 We split the data into training, development,
and test sets as in (Collins, 2002). Table 1 lists character-
5Note that these tags (and sentences) are not identical to
those obtained from the tagged/pos directories of the same disk:
hundreds of tags in the RB/RP/IN set were changed to be more
consistent in the parsed/mrg version. Maybe we were the last to
discover this, but we?ve never seen it in print.
istics of the three splits.6 Except where indicated for the
model BEST, all results are on the development set.
One innovation in our reporting of results is that we
present whole-sentence accuracy numbers as well as the
traditional per-tag accuracy measure (over all tokens,
even unambiguous ones). This is the quantity that most
sequence models attempt to maximize (and has been mo-
tivated over doing per-state optimization as being more
useful for subsequent linguistic processing: one wants to
find a coherent sentence interpretation). Further, while
some tag errors matter much more than others, to a first
cut getting a single tag wrong in many of the more com-
mon ways (e.g., proper noun vs. common noun; noun vs.
verb) would lead to errors in a subsequent processor such
as an information extraction system or a parser that would
greatly degrade results for the entire sentence. Finally,
the fact that the measure has much more dynamic range
has some appeal when reporting tagging results.
The per-state models in this paper are log-linear mod-
els, building upon the models in (Ratnaparkhi, 1996) and
(Toutanova and Manning, 2000), though some models are
in fact strictly simpler. The features in the models are
defined using templates; there are different templates for
rare words aimed at learning the correct tags for unknown
words.7 We present the results of three classes of experi-
ments: experiments with directionality, experiments with
lexicalization, and experiments with smoothing.
3.1 Experiments with Directionality
In this section, we report experiments using log-linear
CMMs to populate nets with various structures, exploring
the relative value of neighboring words? tags. Table 2 lists
the discussed networks. All networks have the same ver-
tical feature templates: ?t0, w0? features for known words
and various ?t0, ? (w1n)? word signature features for all
words, known or not, including spelling and capitaliza-
tion features (see section 3.3).
Just this vertical conditioning gives an accuracy of
93.69% (denoted as ?Baseline? in table 2).8 Condition-
6Tagger results are only comparable when tested not only on
the same data and tag set, but with the same amount of training
data. Brants (2000) illustrates very clearly how tagging perfor-
mance increases as training set size grows, largely because the
percentage of unknown words decreases while system perfor-
mance on them increases (they become increasingly restricted
as to word class).
7Except where otherwise stated, a count cutoff of 2 was used
for common word features and 35 for rare word features (tem-
plates need a support set strictly greater in size than the cutoff
before they are included in the model).
8Charniak et al (1993) noted that such a simple model got
90.25%, but this was with no unknown word model beyond
a prior distribution over tags. Abney et al (1999) raise this
baseline to 92.34%, and with our sophisticated unknown word
model, it gets even higher. The large number of unambiguous
tokens and ones with very skewed distributions make the base-
Model Feature Templates? Features Sentence Token Unkn. Word
Accuracy Accuracy Accuracy
Baseline ? 56,805 26.74% 93.69% 82.61%
L ?t0, t?1? 27,474 41.89% 95.79% 85.49%
R ?t0, t+1? 27,648 36.31% 95.14% 85.65%
L+L2 ?t0, t?1?, ?t0, t?2? 32,935 44.04% 96.05% 85.92%
R+R2 ?t0, t+1?, ?t0, t+2? 33,423 37.20% 95.25% 84.49%
L+R ?t0, t?1?, ?t0, t+1? 32,610 49.50% 96.57% 87.15%
LL ?t0, t?1, t?2? 45,532 44.60% 96.10% 86.48%
RR ?t0, t+1, t+2? 45,446 38.41% 95.40% 85.58%
LR ?t0, t?1, t+1? 45,478 49.30% 96.55% 87.26%
L+LL+LLL ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t?2, t?3? 118,752 45.14% 96.20% 86.52%
R+LR+LLR ?t0, t+1?, ?t0, t?1, t+1?, ?t0, t?1, t?2, t+1? 115,790 51.69% 96.77% 87.91%
L+LL+LR+RR+R ?t0, t?1?, ?t0, t?1, t?2?, ?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2? 81,049 53.23% 96.92% 87.91%
Table 2: Tagging accuracy on the development set with different sequence feature templates. ?All models include the same vertical
word-tag features (?t0, w0? and various ?t0, ? (w1n)?), though the baseline uses a lower cutoff for these features.
Model Feature Templates Support Features Sentence Token Unknown
Cutoff Accuracy Accuracy Accuracy
BASELINE ?t0, w0? 2 6,501 1.63% 60.16% 82.98%
?t0, w0? 0 56,805 26.74% 93.69% 82.61%
3W ?t0, w0?, ?t0, w?1?, ?t0, w+1? 2 239,767 48.27% 96.57% 86.78%
3W+TAGS tag sequences, ?t0, w0?, ?t0, w?1?, ?t0, w+1? 2 263,160 53.83% 97.02% 88.05%
BEST see text 2 460,552 55.31% 97.15% 88.61%
Table 3: Tagging accuracy with different lexical feature templates on the development set.
Model Feature Templates Support Features Sentence Token Unknown
Cutoff Accuracy Accuracy Accuracy
BEST see text 2 460,552 56.34% 97.24% 89.04%
Table 4: Final tagging accuracy for the best model on the test set.
ing on the previous tag as well (model L, ?t0, t?1? fea-
tures) gives 95.79%. The reverse, model R, using the
next tag instead, is slightly inferior at 95.14%. Model
L+R, using both tags simultaneously (but with only the
individual-direction features) gives a much better accu-
racy of 96.57%. Since this model has roughly twice as
many tag-tag features, the fact that it outperforms the uni-
directional models is not by itself compelling evidence
for using bidirectional networks. However, it also out-
performs model L+L2 which adds the ?t0, t?2? second-
previous word features instead of next word features,
which gives only 96.05% (and R+R2 gives 95.25%). We
conclude that, if one wishes to condition on two neigh-
boring nodes (using two sets of 2-tag features), the sym-
metric bidirectional model is superior.
High-performance taggers typically also include joint
three-tag counts in some way, either as tag trigrams
(Brants, 2000) or tag-triple features (Ratnaparkhi, 1996,
Toutanova and Manning, 2000). Models LL, RR, and CR
use only the vertical features and a single set of tag-triple
features: the left tags (t?2, t?1 and t0), right tags (t0, t+1,
t+2), or centered tags (t?1, t0, t+1) respectively. Again,
with roughly equivalent feature sets, the left context is
better than the right, and the centered context is better
than either unidirectional context.
line for this task high, while substantial annotator noise creates
an unknown upper bound on the task.
3.2 Lexicalization
Lexicalization has been a key factor in the advance of
statistical parsing models, but has been less exploited
for tagging. Words surrounding the current word have
been occasionally used in taggers, such as (Ratnaparkhi,
1996), Brill?s transformation based tagger (Brill, 1995),
and the HMM model of Lee et al (2000), but neverthe-
less, the only lexicalization consistently included in tag-
ging models is the dependence of the part of speech tag
of a word on the word itself.
In maximum entropy models, joint features which look
at surrounding words and their tags, as well as joint fea-
tures of the current word and surrounding words are in
principle straightforward additions, but have not been in-
corporated into previous models. We have found these
features to be very useful. We explore here lexicaliza-
tion both alone and in combination with preceding and
following tag histories.
Table 3 shows the development set accuracy of several
models with various lexical features. All models use the
same rare word features as the models in Table 2. The
first two rows show a baseline model using the current
word only. The count cutoff for this feature was 0 in the
first model and 2 for the model in the second row. As
there are no tag sequence features in these models, the ac-
curacy drops significantly if a higher cutoff is used (from
a per tag accuracy of about 93.7% to only 60.2%).
The third row shows a model where a tag is de-
cided solely by the three words centered at the tag po-
sition (3W). As far as we are aware, models of this
sort have not been explored previously, but its accu-
racy is surprisingly high: despite having no sequence
model at all, it is more accurate than a model which uses
standard tag fourgram HMM features (?t0, w0?, ?t0, t?1?,
?t0, t?1, t?2?, ?t0, t?1, t?2, t?3?, shown in Table 2, model
L+LL+LLL).
The fourth and fifth rows show models with bi-
directional tagging features. The fourth model
(3W+TAGS) uses the same tag sequence features as
the last model in Table 2 (?t0, t?1?, ?t0, t?1, t?2?,
?t0, t?1, t+1?, ?t0, t+1?, ?t0, t+1, t+2?) and current, previ-
ous, and next word. The last model has in ad-
dition the feature templates ?t0, w0, t?1?, ?t0, w0, t+1?,
?t0, w?1, w0?, and ?t0, w0, w+1?, and includes the im-
provements in unknown word modeling discussed in sec-
tion 3.3.9 We call this model BEST. BEST has a to-
ken accuracy on the final test set of 97.24% and a sen-
tence accuracy of 56.34% (see Table 4). A 95% confi-
dence interval for the accuracy (using a binomial model)
is (97.15%, 97.33%).
In order to understand the gains from using right con-
text tags and more lexicalization, let us look at an exam-
ple of an error that the enriched models learn not to make.
An interesting example of a common tagging error of the
simpler models which could be corrected by a determinis-
tic fixup rule of the kind used in the IDIOMTAG module
of (Marshall, 1987) is the expression as X as (often, as
far as). This should be tagged as/RB X/{RB,JJ} as/IN in
the Penn Treebank. A model using only current word and
two left tags (model L+L2 in Table 2), made 87 errors on
this expression, tagging it as/IN X as/IN ? since the tag
sequence probabilities do not give strong reasons to dis-
prefer the most common tagging of as (it is tagged as IN
over 80% of the time). However, the model 3W+TAGS,
which uses two right tags and the two surrounding words
in addition, made only 8 errors of this kind, and model
BEST made only 6 errors.
3.3 Unknown word features
Most of the models presented here use a set of un-
known word features basically inherited from (Ratna-
parkhi, 1996), which include using character n-gram pre-
fixes and suffixes (for n up to 4), and detectors for a few
other prominent features of words, such as capitaliza-
tion, hyphens, and numbers. Doing error analysis on un-
known words on a simple tagging model (with ?t0, t?1?,
?t0, t?1, t?2?, and ?w0, t0? features) suggested several ad-
ditional specialized features that can usefully improve
9Thede and Harper (1999) use ?t?1, t0, w0? templates in
their ?full-second order? HMM, achieving an accuracy of
96.86%. Here we can add the opposite tiling and other features.
Smoothed Features Sentence Token Unk. W.
Accuracy Acc. Acc.
yes 45,532 44.60% 96.10% 86.48%
no 45,532 42.81% 95.88% 83.08%
yes 292,649 54.88% 97.10% 88.20%
no 292,649 48.85% 96.54% 85.20%
Table 5: Accuracy with and without quadratic regularization.
performance. By far the most significant is a crude com-
pany name detector which marks capitalized words fol-
lowed within 3 words by a company name suffix like Co.
or Inc. This suggests that further gains could be made by
incorporating a good named entity recognizer as a prepro-
cessor to the tagger (reversing the most common order of
processing in pipelined systems!), and is a good example
of something that can only be done when using a condi-
tional model. Minor gains come from a few additional
features: an allcaps feature, and a conjunction feature of
words that are capitalized and have a digit and a dash in
them (such words are normally common nouns, such as
CFC-12 or F/A-18). We also found it advantageous to
use prefixes and suffixes of length up to 10. Together
with the larger templates, these features contribute to our
unknown word accuracies being higher than those of pre-
viously reported taggers.
3.4 Smoothing
With so many features in the model, overtraining is a dis-
tinct possibility when using pure maximum likelihood es-
timation. We avoid this by using a Gaussian prior (aka
quadratic regularization or quadratic penalization) which
resists high feature weights unless they produce great
score gain. The regularized objective F is:
F(?) =
?
i
log(P?(ti |w, t)) +
?n
j=1
?2j
2? 2
Since we use a conjugate-gradient procedure to maximize
the data likelihood, the addition of a penalty term is eas-
ily incorporated. Both the total size of the penalty and
the partial derivatives with repsect to each ? j are triv-
ial to compute; these are added to the log-likelihood and
log-likelihood derivatives, and the penalized optimization
procedes without further modification.
We have not extensively experimented with the value
of ? 2 ? which can even be set differently for different pa-
rameters or parameter classes. All the results in this paper
use a constant ? 2 = 0.5, so that the denominator disap-
pears in the above expression. Experiments on a simple
model with ? made an order of magnitude higher or lower
both resulted in worse performance than with ? 2 = 0.5.
Our experiments show that quadratic regularization
is very effective in improving the generalization perfor-
mance of tagging models, mostly by increasing the num-
ber of features which could usefully be incorporated. The
Tagger Support cutoff Accuracy
Collins (2002) 0 96.60%
5 96.72%
Model 3W+TAGS variant 1 96.97%
5 96.93%
Table 6: Effect of changing common word feature cutoffs (fea-
tures with support ? cutoff are excluded from the model).
number of features used in our complex models ? in the
several hundreds of thousands, is extremely high in com-
parison with the data set size and the number of features
used in other machine learning domains. We describe two
sets of experiments aimed at comparing models with and
without regularization. One is for a simple model with a
relatively small number of features, and the other is for a
model with a large number of features.
The usefulness of priors in maximum entropy models
is not new to this work: Gaussian prior smoothing is ad-
vocated in Chen and Rosenfeld (2000), and used in all
the stochastic LFG work (Johnson et al, 1999). How-
ever, until recently, its role and importance have not been
widely understood. For example, Zhang and Oles (2001)
attribute the perceived limited success of logistic regres-
sion for text categorization to a lack of use of regular-
ization. At any rate, regularized conditional loglinear
models have not previously been applied to the prob-
lem of producing a high quality part-of-speech tagger:
Ratnaparkhi (1996), Toutanova and Manning (2000), and
Collins (2002) all present unregularized models. Indeed,
the result of Collins (2002) that including low support
features helps a voted perceptron model but harms a max-
imum entropy model is undone once the weights of the
maximum entropy model are regularized.
Table 5 shows results on the development set from two
pairs of experiments. The first pair of models use com-
mon word templates ?t0, w0?, ?t0, t?1, t?2? and the same
rare word templates as used in the models in table 2. The
second pair of models use the same features as model
BEST with a higher frequency cutoff of 5 for common
word features.
For the first pair of models, the error reduction from
smoothing is 5.3% overall and 20.1% on unknown words.
For the second pair of models, the error reduction is
even bigger: 16.2% overall after convergence and 5.8% if
looking at the best accuracy achieved by the unsmoothed
model (by stopping training after 75 iterations; see be-
low). The especially large reduction in unknown word er-
ror reflects the fact that, because penalties are effectively
stronger for rare features than frequent ones, the presence
of penalties increases the degree to which more general
cross-word signature features (which apply to unknown
words) are used, relative to word-specific sparse features
(which do not apply to unknown words).
Secondly, use of regularization allows us to incorporate
features with low support into the model while improving
96,3
96,4
96,5
96,6
96,7
96,8
96,9
97
97,1
97,2
0 100 200 300 400
Training Iterations
Ac
cu
ra
cy

No Smoothing
Smoothing
Figure 4: Accuracy by training iterations, with and without
quadratic regularization.
performance. Whereas Ratnaparkhi (1996) used feature
support cutoffs and early stopping to stop overfitting of
the model, and Collins (2002) contends that including
low support features harms a maximum entropy model,
our results show that low support features are useful in a
regularized maximum entropy model. Table 6 contrasts
our results with those from Collins (2002). Since the
models are not the same, the exact numbers are incompa-
rable, but the difference in direction is important: in the
regularized model, performance improves with the inclu-
sion of low support features.
Finally, in addition to being significantly more accu-
rate, smoothed models train much faster than unsmoothed
ones, and do not benefit from early stopping. For ex-
ample, the first smoothed model in Table 5 required 80
conjugate gradient iterations to converge (somewhat ar-
bitrarily defined as a maximum difference of 10?4 in fea-
ture weights between iterations), while its corresponding
unsmoothed model required 335 iterations, thus training
was roughly 4 times slower.10 The second pair of models
required 134 and 370 iterations respectively. As might
be expected, unsmoothed models reach their highest gen-
eralization capacity long before convergence and accu-
racy on an unseen test set drops considerably with fur-
ther iterations. This is not the case for smoothed mod-
els, as their test set accuracy increases almost monoton-
ically with training iterations.11 Figure 4 shows a graph
of training iterations versus accuracy for the second pair
of models on the development set.
4 Conclusion
We have shown how broad feature use, when combined
with appropriate model regularization, produces a supe-
rior level of tagger performance. While experience sug-
10On a 2GHz PC, this is still an important difference: our
largest models require about 25 minutes per iteration to train.
11In practice one notices some wiggling in the curve, but
the trend remains upward even beyond our chosen convergence
point.
gests that the final accuracy number presented here could
be slightly improved upon by classifier combination, it is
worth noting that not only is this tagger better than any
previous single tagger, but it also appears to outperform
Brill and Wu (1998), the best-known combination tagger
(they report an accuracy of 97.16% over the same WSJ
data, but using a larger training set, which should favor
them).
While part-of-speech tagging is now a fairly well-worn
road, and our ability to win performance increases in
this domain is starting to be limited by the rate of er-
rors and inconsistencies in the Penn Treebank training
data, this work also has broader implications. Across
the many NLP problems which involve sequence mod-
els over sparse multinomial distributions, it suggests that
feature-rich models with extensive lexicalization, bidirec-
tional inference, and effective regularization will be key
elements in producing state-of-the-art results.
Acknowledgements
This work was supported in part by the Advanced Re-
search and Development Activity (ARDA)?s Advanced
Question Answering for Intelligence (AQUAINT) Pro-
gram, by the National Science Foundation under Grant
No. IIS-0085896, and by an IBM Faculty Partnership
Award.
References
Steven Abney, Robert E. Schapire, and Yoram Singer. 1999.
Boosting applied to tagging and PP attachment. In
EMNLP/VLC 1999, pages 38?45.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP 6, pages 224?231.
Eric Brill and Jun Wu. 1998. Classifier combination for
improved lexical disambiguation. In ACL 36/COLING 17,
pages 191?195.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike
Perkowitz. 1993. Equations for part-of-speech tagging. In
AAAI 11, pages 784?789.
Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models. IEEE
Transactions on Speech and Audio Processing, 8(1):37?50.
Kenneth W. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In ANLP 2, pages
136?143.
Michael Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with per-
ceptron algorithms. In EMNLP 2002.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 1999. Probabilistic Networks and
Expert Systems. Springer-Verlag, New York.
David Heckerman, David Maxwell Chickering, Christopher
Meek, Robert Rounthwaite, and Carl Myers Kadie. 2000.
Dependency networks for inference, collaborative filtering
and data visualization. Journal of Machine Learning Re-
search, 1(1):49?75.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In ACL 37, pages 535?541.
Dan Klein and Christopher D. Manning. 2002. Conditional
structure versus conditional estimation in NLP models. In
EMNLP 2002, pages 9?16.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML-2001, pages
282?289.
Sang-Zoo Lee, Jun ichi Tsujii, and Hae-Chang Rim. 2000. Part-
of-speech tagging based on Hidden Markov Model assuming
joint independence. In ACL 38, pages 263?169.
Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkie-
wicz. 1994. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19:313?
330.
Ian Marshall. 1987. Tag selection using probabilistic methods.
In Roger Garside, Geoffrey Sampson, and Geoffrey Leech,
editors, The Computational analysis of English: a corpus-
based approach, pages 42?65. Longman, London.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP 1, pages 133?142.
Scott M. Thede and Mary P. Harper. 1999. Second-order hidden
Markov model for part-of-speech tagging. In ACL 37, pages
175?182.
Kristina Toutanova and Christopher Manning. 2000. Enriching
the knowledge sources used in a maximum entropy part-of-
speech tagger. In EMNLP/VLC 1999, pages 63?71.
Tong Zhang and Frank J. Oles. 2001. Text categorization based
on regularized linear classification methods. Information Re-
trieval, 4:5?31.
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 8
                                                              Proceedings of HLT-NAACL
Parsing with Treebank Grammars: Empirical Bounds, Theoretical
Models, and the Structure of the Penn Treebank
Dan Klein and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
 
klein, manning  @cs.stanford.edu
Abstract
This paper presents empirical studies and
closely corresponding theoretical models of
the performance of a chart parser exhaus-
tively parsing the Penn Treebank with the
Treebank?s own CFG grammar. We show
how performance is dramatically affected by
rule representation and tree transformations,
but little by top-down vs. bottom-up strate-
gies. We discuss grammatical saturation, in-
cluding analysis of the strongly connected
components of the phrasal nonterminals in
the Treebank, and model how, as sentence
length increases, the effective grammar rule
size increases as regions of the grammar
are unlocked, yielding super-cubic observed
time behavior in some configurations.
1 Introduction
This paper originated from examining the empirical
performance of an exhaustive active chart parser us-
ing an untransformed treebank grammar over the Penn
Treebank. Our initial experiments yielded the sur-
prising result that for many configurations empirical
parsing speed was super-cubic in the sentence length.
This led us to look more closely at the structure of
the treebank grammar. The resulting analysis builds
on the presentation of Charniak (1996), but extends
it by elucidating the structure of non-terminal inter-
relationships in the Penn Treebank grammar. On the
basis of these studies, we build simple theoretical
models which closely predict observed parser perfor-
mance, and, in particular, explain the originally ob-
served super-cubic behavior.
We used treebank grammars induced directly from
the local trees of the entire WSJ section of the Penn
Treebank (Marcus et al, 1993) (release 3). For each
length and parameter setting, 25 sentences evenly dis-
tributed through the treebank were parsed. Since we
were parsing sentences from among those from which
our grammar was derived, coverage was never an is-
sue. Every sentence parsed had at least one parse ? the
parse with which it was originally observed.1
The sentences were parsed using an implementa-
tion of the probabilistic chart-parsing algorithm pre-
sented in (Klein and Manning, 2001). In that paper,
we present a theoretical analysis showing an 
	
worst-case time bound for exhaustively parsing arbi-
trary context-free grammars. In what follows, we do
not make use of the probabilistic aspects of the gram-
mar or parser.
2 Parameters
The parameters we varied were:
 Tree Transforms: NOTRANSFORM, NOEMPTIES,
NOUNARIESHIGH, and NOUNARIESLOW.
 Grammar Rule Encodings: LIST, TRIE, or MIN
 Rule Introduction: TOPDOWN or BOTTOMUP
The default settings are shown above in bold face.
We do not discuss all possible combinations of these
settings. Rather, we take the bottom-up parser using an
untransformed grammar with trie rule encodings to be
the basic form of the parser. Except where noted, we
will discuss how each factor affects this baseline, as
most of the effects are orthogonal. When we name a
setting, any omitted parameters are assumed to be the
defaults.
2.1 Tree Transforms
In all cases, the grammar was directly induced from
(transformed) Penn treebank trees. The transforms
used are shown in figure 1. For all settings, func-
tional tags and crossreferencing annotations were
stripped. For NOTRANSFORM, no other modification
was made. In particular, empty nodes (represented as
-NONE- in the treebank) were turned into rules that
generated the empty string (  ), and there was no col-
lapsing of categories (such as PRT and ADVP) as is of-
ten done in parsing work (Collins, 1997, etc.). For
1Effectively ?testing on the training set? would be invalid
if we wished to present performance results such as precision
and recall, but it is not a problem for the present experiments,
which focus solely on the parser load and grammar structure.
TOP
S-HLN
NP-SBJ
-NONE-

VP
VB
Atone
TOP
S
NP
-NONE-

VP
VB
Atone
TOP
S
VP
VB
Atone
TOP
S
Atone
TOP
VB
Atone
(a) (b) (c) (d) (e)
Figure 1: Tree Transforms: (a) The raw tree, (b) NO-
TRANSFORM, (c) NOEMPTIES, (d) NOUNARIES-
HIGH (e) NOUNARIESLOW
NNP
NNS
NNP NNP
NNJJ
NNS JJ
CD
NNCD
NNDT
NNDT NN
NNS DT
JJ DT NN
CC NP  NP
PP NP
SBAR NP
NNSNN
NN
PRP
QP
NNS
NNS
NNS
NNS
NNP
NNP  NN
JJ
CD  NN
NN
DT
NN
JJ  NN
NP  CC  NP
NN
SBAR
PP
PRP
QP

NNS
NNS
NNS
NNP NNP
JJ
NN
CD  NN
NNS DT
JJ  NN
NP  NP CC
PP
SBAR
NN
PRP
QP
NN
LIST TRIE MIN
Figure 2: Grammar Encodings: FSAs for a subset of
the rules for the category NP. Non-black states are
active, non-white states are accepting, and bold transi-
tions are phrasal.
NOEMPTIES, empties were removed by pruning non-
terminals which covered no overt words. For NOUNA-
RIESHIGH, and NOUNARIESLOW, unary nodes were
removed as well, by keeping only the tops and the bot-
toms of unary chains, respectively.2
2.2 Grammar Rule Encodings
The parser operates on Finite State Automata (FSA)
grammar representations. We compiled grammar
rules into FSAs in three ways: LISTs, TRIEs, and
MINimized FSAs. An example of each representa-
tion is given in figure 2. For LIST encodings, each
local tree type was encoded in its own, linearly struc-
tured FSA, corresponding to Earley (1970)-style dot-
ted rules. For TRIE, there was one FSA per cate-
gory, encoding together all rule types producing that
category. For MIN, state-minimized FSAs were con-
structed from the trie FSAs. Note that while the rule
encoding may dramatically affect the efficiency of a
parser, it does not change the actual set of parses for a
given sentence in any way.3
2In no case were the nonterminal-to-word or TOP-to-
nonterminal unaries altered.
3FSAs are not the only method of representing and com-
pacting grammars. For example, the prefix compacted tries
we use are the same as the common practice of ignoring
items before the dot in a dotted rule (Moore, 2000). Another
0
60
120
180
240
300
360
0 10 20 30 40 50
SentenceLength
A
vg
.T
im
e(s
ec
on
ds
)
List-NoTransform
exp3.54r0.999
Trie-NoTransform
exp3.16r0.995
Trie-NoEmpties
exp3.47r0.998
Trie-NoUnariesHigh
exp3.67r0.999
Trie-NoUnariesLow
exp3.65r0.999
Min-NoTransform
exp2.87r0.998
Min-NoUnariesLow
exp3.32r1.000
Figure 3: The average time to parse sentences using
various parameters.
3 Observed Performance
In this section, we outline the observed performance
of the parser for various settings. We frequently speak
in terms of the following:
 span: a range of words in the chart, e.g., [1,3]4
 edge: a category over a span, e.g., NP:[1,3]
 traversal: a way of making an edge from an
active and a passive edge, e.g., NP:[1,3] 
(NP  DT.NN:[1,2] + NN:[2,3])
3.1 Time
The parser has an   	 theoretical time bound,
where  is the number of words in the sentence to be
parsed,  is the number of nonterminal categories in
the grammar and  is the number of (active) states in
the FSA encoding of the grammar. The time bound
is derived from counting the number of traversals pro-
cessed by the parser, each taking 	 time.
In figure 3, we see the average time5 taken per sen-
tence length for several settings, with the empirical ex-
ponent (and correlation  -value) from the best-fit sim-
ple power law model to the right. Notice that most
settings show time growth greater than 

	 .
Although,   	 is simply an asymptotic bound,
there are good explanations for the observed behav-
ior. There are two primary causes for the super-cubic
time values. The first is theoretically uninteresting.
The parser is implemented in Java, which uses garbage
collection for memory management. Even when there
is plenty of memory for a parse?s primary data struc-
tures, ?garbage collection thrashing? can occur when
logical possibility would be trie encodings which compact
the grammar states by common suffix rather than common
prefix, as in (Leermakers, 1992). The savings are less than
for prefix compaction.
4Note that the number of words (or size) of a span is equal
to the difference between the endpoints.
5The hardware was a 700 MHz Intel Pentium III, and we
used up to 2GB of RAM for very long sentences or very
poor parameters. With good parameter settings, the system
can parse 100+ word treebank sentences.
0.0M
5.0M
10.0M
15.0M
20.0M
0 10 20 30 40 50
SentenceLength
A
vg
.T
ra
ve
rs
al
s
NoTransform
exp2.86r1.000
NoEmpties
exp3.28r1.000
NoUnariesHigh
exp3.74r0.999
NoUnariesLow
exp3.83r0.999
0.0M
5.0M
10.0M
15.0M
20.0M
0 10 20 30 40 50
SentenceLength
A
vg
.T
ra
ve
rs
al
s List
exp2.60r0.999
Trie
exp2.86r1.000
Min
exp2.78r1.000
0.994
0.995
0.996
0.997
0.998
0.999
1.000
1.001
1.002
0 10 20 30 40 50
SentenceLength
R
at
io
(T
D/
BU
)
Edges
Traversals
(a) (b) (c)
Figure 4: (a) The number of traversals for different grammar transforms. (b) The number of traversals for different
grammar encodings. (c) The ratio of the number of edges and traversals produced with a top-down strategy over
the number produced with a bottom-up strategy (shown for TRIE-NOTRANSFORM, others are similar).
parsing longer sentences as temporary objects cause
increasingly frequent reclamation. To see past this ef-
fect, which inflates the empirical exponents, we turn to
the actual traversal counts, which better illuminate the
issues at hand. Figures 4 (a) and (b) show the traversal
curves corresponding to the times in figure 3.
The interesting cause of the varying exponents
comes from the ?constant? terms in the theoretical
bound. The second half of this paper shows how
modeling growth in these terms can accurately predict
parsing performance (see figures 9 to 13).
3.2 Memory
The memory bound for the parser is A Generative Constituent-Context Model for Improved Grammar Induction
Dan Klein and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
{klein, manning}@cs.stanford.edu
Abstract
We present a generative distributional model for the
unsupervised induction of natural language syntax
which explicitly models constituent yields and con-
texts. Parameter search with EM produces higher
quality analyses than previously exhibited by un-
supervised systems, giving the best published un-
supervised parsing results on the ATIS corpus. Ex-
periments on Penn treebank sentences of compara-
ble length show an even higher F1 of 71% on non-
trivial brackets. We compare distributionally in-
duced and actual part-of-speech tags as input data,
and examine extensions to the basic model. We dis-
cuss errors made by the system, compare the sys-
tem to previous models, and discuss upper bounds,
lower bounds, and stability for this task.
1 Introduction
The task of inducing hierarchical syntactic structure
from observed yields alone has received a great deal
of attention (Carroll and Charniak, 1992; Pereira and
Schabes, 1992; Brill, 1993; Stolcke and Omohun-
dro, 1994). Researchers have explored this problem
for a variety of reasons: to argue empirically against
the poverty of the stimulus (Clark, 2001), to use in-
duction systems as a first stage in constructing large
treebanks (van Zaanen, 2000), or to build better lan-
guage models (Baker, 1979; Chen, 1995).
In previous work, we presented a conditional
model over trees which gave the best published re-
sults for unsupervised parsing of the ATIS corpus
(Klein and Manning, 2001b). However, it suffered
from several drawbacks, primarily stemming from
the conditional model used for induction. Here, we
improve on that model in several ways. First, we
construct a generative model which utilizes the same
features. Then, we extend the model to allow mul-
tiple constituent types and multiple prior distribu-
tions over trees. The new model gives a 13% reduc-
tion in parsing error on WSJ sentence experiments,
including a positive qualitative shift in error types.
Additionally, it produces much more stable results,
does not require heavy smoothing, and exhibits a re-
liable correspondence between the maximized ob-
jective and parsing accuracy. It is also much faster,
not requiring a fitting phase for each iteration.
Klein and Manning (2001b) and Clark (2001) take
treebank part-of-speech sequences as input. We fol-
lowed this for most experiments, but in section 4.3,
we use distributionally induced tags as input. Perfor-
mance with induced tags is somewhat reduced, but
still gives better performance than previous models.
2 Previous Work
Early work on grammar induction emphasized heu-
ristic structure search, where the primary induction
is done by incrementally adding new productions to
an initially empty grammar (Olivier, 1968; Wolff,
1988). In the early 1990s, attempts were made to do
grammar induction by parameter search, where the
broad structure of the grammar is fixed in advance
and only parameters are induced (Lari and Young,
1990; Carroll and Charniak, 1992).1 However, this
appeared unpromising and most recent work has re-
turned to using structure search. Note that both ap-
proaches are local. Structure search requires ways
of deciding locally which merges will produce a co-
herent, globally good grammar. To the extent that
such approaches work, they work because good lo-
cal heuristics have been engineered (Klein and Man-
ning, 2001a; Clark, 2001).
1On this approach, the question of which rules are included
or excluded becomes the question of which parameters are zero.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 128-135.
                         Proceedings of the 40th Annual Meeting of the Association for
SNP
NN
0 Factory
NNS
1 payrolls
VP
VBD
2 fell
PP
IN
3 in
NN
4 September 5
543210
5
4
3
2
1
0
St
ar
t
End
543210
5
4
3
2
1
0
St
ar
t
End
543210
5
4
3
2
1
0
St
ar
t
EndSpan Label Constituent Context
?0,5? S NN NNS VBD IN NN  ? 
?0,2? NP NN NNS  ? VBD
?2,5? VP VBD IN NN NNS ? 
?3,5? PP IN NN VBD ? 
?0,1? NN NN  ? NNS
?1,2? NNS NNS NN ? VBD
?2,3? VBD VBD NNS ? IN
?3,4? IN IN VBD ? NN
?4,5? NN NNS IN ? 
(a) (b) (c)
Figure 1: (a) Example parse tree with (b) its associated bracketing and (c) the yields and contexts for each constituent span in that
bracketing. Distituent yields and contexts are not shown, but are modeled.
Parameter search is also local; parameters which
are locally optimal may be globally poor. A con-
crete example is the experiments from (Carroll and
Charniak, 1992). They restricted the space of gram-
mars to those isomorphic to a dependency grammar
over the POS symbols in the Penn treebank, and
then searched for parameters with the inside-outside
algorithm (Baker, 1979) starting with 300 random
production weight vectors. Each seed converged to
a different locally optimal grammar, none of them
nearly as good as the treebank grammar, measured
either by parsing performance or data-likelihood.
However, parameter search methods have a poten-
tial advantage. By aggregating over only valid, com-
plete parses of each sentence, they naturally incor-
porate the constraint that constituents cannot cross
? the bracketing decisions made by the grammar
must be coherent. The Carroll and Charniak exper-
iments had two primary causes for failure. First,
random initialization is not always good, or neces-
sary. The parameter space is riddled with local like-
lihood maxima, and starting with a very specific, but
random, grammar should not be expected to work
well. We duplicated their experiments, but used a
uniform parameter initialization where all produc-
tions were equally likely. This allowed the interac-
tion between the grammar and data to break the ini-
tial symmetry, and resulted in an induced grammar
of higher quality than Carroll and Charniak reported.
This grammar, which we refer to as DEP-PCFG will
be evaluated in more detail in section 4. The sec-
ond way in which their experiment was guaranteed
to be somewhat unencouraging is that a delexical-
ized dependency grammar is a very poor model of
language, even in a supervised setting. By the F1
measure used in the experiments in section 4, an in-
duced dependency PCFG scores 48.2, compared to
a score of 82.1 for a supervised PCFG read from
local trees of the treebank. However, a supervised
dependency PCFG scores only 53.5, not much bet-
ter than the unsupervised version, and worse than a
right-branching baseline (of 60.0). As an example of
the inherent shortcomings of the dependency gram-
mar, it is structurally unable to distinguish whether
the subject or object should be attached to the verb
first. Since both parses involve the same set of pro-
ductions, both will have equal likelihood.
3 A Generative Constituent-Context Model
To exploit the benefits of parameter search, we used
a novel model which is designed specifically to en-
able a more felicitous search space. The funda-
mental assumption is a much weakened version of
classic linguistic constituency tests (Radford, 1988):
constituents appear in constituent contexts. A par-
ticular linguistic phenomenon that the system ex-
ploits is that long constituents often have short, com-
mon equivalents, or proforms, which appear in sim-
ilar contexts and whose constituency is easily dis-
covered (or guaranteed). Our model is designed
to transfer the constituency of a sequence directly
to its containing context, which is intended to then
pressure new sequences that occur in that context
into being parsed as constituents in the next round.
The model is also designed to exploit the successes
of distributional clustering, and can equally well be
viewed as doing distributional clustering in the pres-
ence of no-overlap constraints.
3.1 Constituents and Contexts
Unlike a PCFG, our model describes all contigu-
ous subsequences of a sentence (spans), including
empty spans, whether they are constituents or non-
constituents (distituents). A span encloses a se-
quence of terminals, or yield, ?, such as DT JJ NN.
A span occurs in a context x , such as ?VBZ, where
x is the ordered pair of preceding and following ter-
minals ( denotes a sentence boundary). A bracket-
ing of a sentence is a boolean matrix B, which in-
dicates which spans are constituents and which are
not. Figure 1 shows a parse of a short sentence, the
bracketing corresponding to that parse, and the la-
bels, yields, and contexts of its constituent spans.
Figure 2 shows several bracketings of the sen-
tence in figure 1. A bracketing B of a sentence is
non-crossing if, whenever two spans cross, at most
one is a constituent in B. A non-crossing bracket-
ing is tree-equivalent if the size-one terminal spans
and the full-sentence span are constituents, and all
size-zero spans are distituents. Figure 2(a) and (b)
are tree-equivalent. Tree-equivalent bracketings B
correspond to (unlabeled) trees in the obvious way.
A bracketing is binary if it corresponds to a binary
tree. Figure 2(b) is binary. We will induce trees by
inducing tree-equivalent bracketings.
Our generative model over sentences S has two
phases. First, we choose a bracketing B according
to some distribution P(B) and then generate the sen-
tence given that bracketing:
P(S, B) = P(B)P(S|B)
Given B, we fill in each span independently. The
context and yield of each span are independent of
each other, and generated conditionally on the con-
stituency Bi j of that span.
P(S|B) =
?
?i, j ??spans(S) P(?i j , xi j |Bi j )
=
?
?i, j ? P(?i j |Bi j )P(xi j |Bi j)
The distribution P(?i j |Bi j) is a pair of multinomial
distributions over the set of all possible yields: one
for constituents (Bi j = c) and one for distituents
(Bi j = d). Similarly for P(xi j |Bi j ) and contexts.
The marginal probability assigned to the sentence S
is given by summing over all possible bracketings of
S: P(S) =
?
B P(B)P(S|B).2
To induce structure, we run EM over this model,
treating the sentences S as observed and the brack-
etings B as unobserved. The parameters 2 of
2Viewed as a model generating sentences, this model is defi-
cient, placing mass on yield and context choices which will not
tile into a valid sentence, either because specifications for posi-
tions conflict or because yields of incorrect lengths are chosen.
However, we can renormalize by dividing by the mass placed on
proper sentences and zeroing the probability of improper brack-
etings. The rest of the paper, and results, would be unchanged
except for notation to track the renormalization constant.
543210
5
4
3
2
1
0
St
ar
t
End
543210
5
4
3
2
1
0
St
ar
t
End
543210
5
4
3
2
1
0
St
ar
t
End
(a) Tree-equivalent (b) Binary (c) Crossing
Figure 2: Three bracketings of the sentence in figure 1: con-
stituent spans in black. (b) corresponds to the binary parse in
figure 1; (a) does not contain the ?2,5? VP bracket, while (c)
contains a ?0,3? bracket crossing that VP bracket.
the model are the constituency-conditional yield
and context distributions P(?|b) and P(x |b). If
P(B) is uniform over all (possibly crossing) brack-
etings, then this procedure will be equivalent to soft-
clustering with two equal-prior classes.
There is reason to believe that such soft cluster-
ings alone will not produce valuable distinctions,
even with a significantly larger number of classes.
The distituents must necessarily outnumber the con-
stituents, and so such distributional clustering will
result in mostly distituent classes. Clark (2001) finds
exactly this effect, and must resort to a filtering heu-
ristic to separate constituent and distituent clusters.
To underscore the difference between the bracketing
and labeling tasks, consider figure 3. In both plots,
each point is a frequent tag sequence, assigned to
the (normalized) vector of its context frequencies.
Each plot has been projected onto the first two prin-
cipal components of its respective data set. The left
plot shows the most frequent sequences of three con-
stituent types. Even in just two dimensions, the clus-
ters seem coherent, and it is easy to believe that
they would be found by a clustering algorithm in
the full space. On the right, sequences have been
labeled according to whether their occurrences are
constituents more or less of the time than a cutoff
(of 0.2). The distinction between constituent and
distituent seems much less easily discernible.
We can turn what at first seems to be distributional
clustering into tree induction by confining P(B) to
put mass only on tree-equivalent bracketings. In par-
ticular, consider Pbin(B) which is uniform over bi-
nary bracketings and zero elsewhere. If we take this
bracketing distribution, then when we sum over data
completions, we will only involve bracketings which
correspond to valid binary trees. This restriction is
the basis for our algorithm.
NP
VP
PP
Usually a Constituent
Rarely a Constituent
(a) Constituent Types (b) Constituents vs. Distituents
Figure 3: The most frequent yields of (a) three constituent types and (b) constituents and distituents, as context vectors, projected
onto their first two principal components. Clustering is effective at labeling, but not detecting constituents.
3.2 The Induction Algorithm
We now essentially have our induction algorithm.
We take P(B) to be Pbin(B), so that all binary trees
are equally likely. We then apply the EM algorithm:
E-Step: Find the conditional completion likeli-
hoods P(B|S,2) according to the current 2.
M-Step: Fix P(B|S,2) and find the 2? which max-
imizes
?
B P(B|S,2) log P(S, B|2?).
The completions (bracketings) cannot be efficiently
enumerated, and so a cubic dynamic program simi-
lar to the inside-outside algorithm is used to calcu-
late the expected counts of each yield and context,
both as constituents and distituents. Relative fre-
quency estimates (which are the ML estimates for
this model) are used to set 2?.
To begin the process, we did not begin at the E-
step with an initial guess at 2. Rather, we began at
the M-step, using an initial distribution over com-
pletions. The initial distribution was not the uniform
distribution over binary trees Pbin(B). That was un-
desirable as an initial point because, combinatorily,
almost all trees are relatively balanced. On the other
hand, in language, we want to allow unbalanced
structures to have a reasonable chance to be discov-
ered. Therefore, consider the following uniform-
splitting process of generating binary trees over k
terminals: choose a split point at random, then recur-
sively build trees by this process on each side of the
split. This process gives a distribution Psplit which
puts relatively more weight on unbalanced trees, but
only in a very general, non language-specific way.
This distribution was not used in the model itself,
however. It seemed to bias too strongly against bal-
anced structures, and led to entirely linear-branching
structures.
The smoothing used was straightforward. For
each yield ? or context x , we added 10 counts of that
item as a constituent and 50 as a distituent. This re-
flected the relative skew of random spans being more
likely to be distituents. This contrasts with our previ-
ous work, which was sensitive to smoothing method,
and required a massive amount of it.
4 Experiments
We performed most experiments on the 7422 sen-
tences in the Penn treebank Wall Street Journal sec-
tion which contained no more than 10 words af-
ter the removal of punctuation and null elements
(WSJ-10). Evaluation was done by measuring un-
labeled precision, recall, and their harmonic mean
F1 against the treebank parses. Constituents which
could not be gotten wrong (single words and en-
tire sentences) were discarded.3 The basic experi-
ments, as described above, do not label constituents.
An advantage to having only a single constituent
class is that it encourages constituents of one type to
be found even when they occur in a context which
canonically holds another type. For example, NPs
and PPs both occur between a verb and the end of
the sentence, and they can transfer constituency to
each other through that context.
Figure 4 shows the F1 score for various meth-
ods of parsing. RANDOM chooses a tree uniformly
3Since reproducible evaluation is important, a few more
notes: this is different from the original (unlabeled) bracket-
ing measures proposed in the PARSEVAL standard, which did
not count single words as constituents, but did give points for
putting a bracket over the entire sentence. Secondly, bracket la-
bels and multiplicity are just ignored. Below, we also present
results using the EVALB program for comparability, but we note
that while one can get results from it that ignore bracket labels,
it never ignores bracket multiplicity. Both these alternatives
seem less satisfactory to us as measures for evaluating unsu-
pervised constituency decisions.
13
30
48
60
71
82 87
0
20
40
60
80
100
LBR
ANC
H
RAN
DOM
DEP
-
PCF
G
RBR
ANC
H CCM
SUP
-
PCF
G
UBO
UND
 
 



	
Figure 4: F1 for various models on WSJ-10.
0
10
20
30
40
50
60
70
80
90
100
2 3 4 5 6 7 8 9

 
Pe
rc
e
n
t
Accurate Unlexicalized Parsing
Dan Klein
Computer Science Department
Stanford University
Stanford, CA 94305-9040
klein@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
manning@cs.stanford.edu
Abstract
We demonstrate that an unlexicalized PCFG can
parse much more accurately than previously shown,
by making use of simple, linguistically motivated
state splits, which break down false independence
assumptions latent in a vanilla treebank grammar.
Indeed, its performance of 86.36% (LP/LR F1) is
better than that of early lexicalized PCFG models,
and surprisingly close to the current state-of-the-
art. This result has potential uses beyond establish-
ing a strong lower bound on the maximum possi-
ble accuracy of unlexicalized models: an unlexical-
ized PCFG is much more compact, easier to repli-
cate, and easier to interpret than more complex lex-
ical models, and the parsing algorithms are simpler,
more widely understood, of lower asymptotic com-
plexity, and easier to optimize.
In the early 1990s, as probabilistic methods swept
NLP, parsing work revived the investigation of prob-
abilistic context-free grammars (PCFGs) (Booth and
Thomson, 1973; Baker, 1979). However, early re-
sults on the utility of PCFGs for parse disambigua-
tion and language modeling were somewhat disap-
pointing. A conviction arose that lexicalized PCFGs
(where head words annotate phrasal nodes) were
the key tool for high performance PCFG parsing.
This approach was congruent with the great success
of word n-gram models in speech recognition, and
drew strength from a broader interest in lexicalized
grammars, as well as demonstrations that lexical de-
pendencies were a key tool for resolving ambiguities
such as PP attachments (Ford et al, 1982; Hindle and
Rooth, 1993). In the following decade, great success
in terms of parse disambiguation and even language
modeling was achieved by various lexicalized PCFG
models (Magerman, 1995; Charniak, 1997; Collins,
1999; Charniak, 2000; Charniak, 2001).
However, several results have brought into ques-
tion how large a role lexicalization plays in such
parsers. Johnson (1998) showed that the perfor-
mance of an unlexicalized PCFG over the Penn tree-
bank could be improved enormously simply by an-
notating each node by its parent category. The Penn
treebank covering PCFG is a poor tool for parsing be-
cause the context-freedom assumptions it embodies
are far too strong, and weakening them in this way
makes the model much better. More recently, Gildea
(2001) discusses how taking the bilexical probabil-
ities out of a good current lexicalized PCFG parser
hurts performance hardly at all: by at most 0.5% for
test text from the same domain as the training data,
and not at all for test text from a different domain.1
But it is precisely these bilexical dependencies that
backed the intuition that lexicalized PCFGs should be
very successful, for example in Hindle and Rooth?s
demonstration from PP attachment. We take this as a
reflection of the fundamental sparseness of the lex-
ical dependency information available in the Penn
Treebank. As a speech person would say, one mil-
lion words of training data just isn?t enough. Even
for topics central to the treebank?s Wall Street Jour-
nal text, such as stocks, many very plausible depen-
dencies occur only once, for example stocks stabi-
lized, while many others occur not at all, for exam-
ple stocks skyrocketed.2
The best-performing lexicalized PCFGs have in-
creasingly made use of subcategorization3 of the
1There are minor differences, but all the current best-known
lexicalized PCFGs employ both monolexical statistics, which
describe the phrasal categories of arguments and adjuncts that
appear around a head lexical item, and bilexical statistics, or de-
pendencies, which describe the likelihood of a head word taking
as a dependent a phrase headed by a certain other word.
2This observation motivates various class- or similarity-
based approaches to combating sparseness, and this remains a
promising avenue of work, but success in this area has proven
somewhat elusive, and, at any rate, current lexicalized PCFGs
do simply use exact word matches if available, and interpolate
with syntactic category-based estimates when they are not.
3In this paper we use the term subcategorization in the origi-
nal general sense of Chomsky (1965), for where a syntactic cat-
categories appearing in the Penn treebank. Charniak
(2000) shows the value his parser gains from parent-
annotation of nodes, suggesting that this informa-
tion is at least partly complementary to information
derivable from lexicalization, and Collins (1999)
uses a range of linguistically motivated and care-
fully hand-engineered subcategorizations to break
down wrong context-freedom assumptions of the
naive Penn treebank covering PCFG, such as differ-
entiating ?base NPs? from noun phrases with phrasal
modifiers, and distinguishing sentences with empty
subjects from those where there is an overt subject
NP. While he gives incomplete experimental results
as to their efficacy, we can assume that these features
were incorporated because of beneficial effects on
parsing that were complementary to lexicalization.
In this paper, we show that the parsing perfor-
mance that can be achieved by an unlexicalized
PCFG is far higher than has previously been demon-
strated, and is, indeed, much higher than community
wisdom has thought possible. We describe several
simple, linguistically motivated annotations which
do much to close the gap between a vanilla PCFG
and state-of-the-art lexicalized models. Specifically,
we construct an unlexicalized PCFG which outper-
forms the lexicalized PCFGs of Magerman (1995)
and Collins (1996) (though not more recent models,
such as Charniak (1997) or Collins (1999)).
One benefit of this result is a much-strengthened
lower bound on the capacity of an unlexicalized
PCFG. To the extent that no such strong baseline has
been provided, the community has tended to greatly
overestimate the beneficial effect of lexicalization in
probabilistic parsing, rather than looking critically
at where lexicalized probabilities are both needed to
make the right decision and available in the training
data. Secondly, this result affirms the value of lin-
guistic analysis for feature discovery. The result has
other uses and advantages: an unlexicalized PCFG is
easier to interpret, reason about, and improve than
the more complex lexicalized models. The grammar
representation is much more compact, no longer re-
quiring large structures that store lexicalized proba-
bilities. The parsing algorithms have lower asymp-
totic complexity4 and have much smaller grammar
egory is divided into several subcategories, for example divid-
ing verb phrases into finite and non-finite verb phrases, rather
than in the modern restricted usage where the term refers only
to the syntactic argument frames of predicators.
4O(n3) vs. O(n5) for a naive implementation, or vs. O(n4)
if using the clever approach of Eisner and Satta (1999).
constants. An unlexicalized PCFG parser is much
simpler to build and optimize, including both stan-
dard code optimization techniques and the investiga-
tion of methods for search space pruning (Caraballo
and Charniak, 1998; Charniak et al, 1998).
It is not our goal to argue against the use of lex-
icalized probabilities in high-performance probabi-
listic parsing. It has been comprehensively demon-
strated that lexical dependencies are useful in re-
solving major classes of sentence ambiguities, and a
parser should make use of such information where
possible. We focus here on using unlexicalized,
structural context because we feel that this infor-
mation has been underexploited and underappreci-
ated. We see this investigation as only one part of
the foundation for state-of-the-art parsing which em-
ploys both lexical and structural conditioning.
1 Experimental Setup
To facilitate comparison with previous work, we
trained our models on sections 2?21 of the WSJ sec-
tion of the Penn treebank. We used the first 20 files
(393 sentences) of section 22 as a development set
(devset). This set is small enough that there is no-
ticeable variance in individual results, but it allowed
rapid search for good features via continually repars-
ing the devset in a partially manual hill-climb. All of
section 23 was used as a test set for the final model.
For each model, input trees were annotated or trans-
formed in some way, as in Johnson (1998). Given
a set of transformed trees, we viewed the local trees
as grammar rewrite rules in the standard way, and
used (unsmoothed) maximum-likelihood estimates
for rule probabilities.5 To parse the grammar, we
used a simple array-based Java implementation of
a generalized CKY parser, which, for our final best
model, was able to exhaustively parse all sentences
in section 23 in 1GB of memory, taking approxi-
mately 3 sec for average length sentences.6
5The tagging probabilities were smoothed to accommodate
unknown words. The quantity P(tag|word) was estimated
as follows: words were split into one of several categories
wordclass, based on capitalization, suffix, digit, and other
character features. For each of these categories, we took the
maximum-likelihood estimate of P(tag|wordclass). This dis-
tribution was used as a prior against which observed taggings,
if any, were taken, giving P(tag|word) = [c(tag, word) +
? P(tag|wordclass)]/[c(word)+?]. This was then inverted to
give P(word|tag). The quality of this tagging model impacts
all numbers; for example the raw treebank grammar?s devset F1
is 72.62 with it and 72.09 without it.
6The parser is available for download as open source at:
http://nlp.stanford.edu/downloads/lex-parser.shtml
VP
<VP:[VBZ]. . . PP>
<VP:[VBZ]. . . NP>
<VP:[VBZ]>
VBZ
NP
PP
Figure 1: The v=1, h=1 markovization of VP ? VBZ NP PP.
2 Vertical and Horizontal Markovization
The traditional starting point for unlexicalized pars-
ing is the raw n-ary treebank grammar read from
training trees (after removing functional tags and
null elements). This basic grammar is imperfect in
two well-known ways. First, the category symbols
are too coarse to adequately render the expansions
independent of the contexts. For example, subject
NP expansions are very different from object NP ex-
pansions: a subject NP is 8.7 times more likely than
an object NP to expand as just a pronoun. Having
separate symbols for subject and object NPs allows
this variation to be captured and used to improve
parse scoring. One way of capturing this kind of
external context is to use parent annotation, as pre-
sented in Johnson (1998). For example, NPs with S
parents (like subjects) will be marked NP?S, while
NPs with VP parents (like objects) will be NP?VP.
The second basic deficiency is that many rule
types have been seen only once (and therefore have
their probabilities overestimated), and many rules
which occur in test sentences will never have been
seen in training (and therefore have their probabili-
ties underestimated ? see Collins (1999) for analy-
sis). Note that in parsing with the unsplit grammar,
not having seen a rule doesn?t mean one gets a parse
failure, but rather a possibly very weird parse (Char-
niak, 1996). One successful method of combating
sparsity is to markovize the rules (Collins, 1999). In
particular, we follow that work in markovizing out
from the head child, despite the grammar being un-
lexicalized, because this seems the best way to cap-
ture the traditional linguistic insight that phrases are
organized around a head (Radford, 1988).
Both parent annotation (adding context) and RHS
markovization (removing it) can be seen as two in-
stances of the same idea. In parsing, every node has
a vertical history, including the node itself, parent,
grandparent, and so on. A reasonable assumption is
that only the past v vertical ancestors matter to the
current expansion. Similarly, only the previous h
horizontal ancestors matter (we assume that the head
Horizontal Markov Order
Vertical Order h = 0 h = 1 h ? 2 h = 2 h = ?
v = 1 No annotation 71.27 72.5 73.46 72.96 72.62
(854) (3119) (3863) (6207) (9657)
v ? 2 Sel. Parents 74.75 77.42 77.77 77.50 76.91
(2285) (6564) (7619) (11398) (14247)
v = 2 All Parents 74.68 77.42 77.81 77.50 76.81
(2984) (7312) (8367) (12132) (14666)
v ? 3 Sel. GParents 76.50 78.59 79.07 78.97 78.54
(4943) (12374) (13627) (19545) (20123)
v = 3 All GParents 76.74 79.18 79.74 79.07 78.72
(7797) (15740) (16994) (22886) (22002)
Figure 2: Markovizations: F1 and grammar size.
child always matters). It is a historical accident that
the default notion of a treebank PCFG grammar takes
v = 1 (only the current node matters vertically) and
h = ? (rule right hand sides do not decompose at
all). On this view, it is unsurprising that increasing
v and decreasing h have historically helped.
As an example, consider the case of v = 1,
h = 1. If we start with the rule VP ? VBZ NP
PP PP, it will be broken into several stages, each a
binary or unary rule, which conceptually represent
a head-outward generation of the right hand size, as
shown in figure 1. The bottom layer will be a unary
over the head declaring the goal: ?VP: [VBZ]? ?
VBZ. The square brackets indicate that the VBZ is
the head, while the angle brackets ?X? indicates that
the symbol ?X? is an intermediate symbol (equiv-
alently, an active or incomplete state). The next
layer up will generate the first rightward sibling of
the head child: ?VP: [VBZ]. . . NP? ? ?VP: [VBZ]?
NP. Next, the PP is generated: ?VP: [VBZ]. . . PP? ?
?VP: [VBZ]. . . NP? PP. We would then branch off left
siblings if there were any.7 Finally, we have another
unary to finish the VP. Note that while it is con-
venient to think of this as a head-outward process,
these are just PCFG rewrites, and so the actual scores
attached to each rule will correspond to a downward
generation order.
Figure 2 presents a grid of horizontal and verti-
cal markovizations of the grammar. The raw tree-
bank grammar corresponds to v = 1, h = ? (the
upper right corner), while the parent annotation in
(Johnson, 1998) corresponds to v = 2, h = ?, and
the second-order model in Collins (1999), is broadly
a smoothed version of v = 2, h = 2. In addi-
tion to exact nth-order models, we tried variable-
7In our system, the last few right children carry over as pre-
ceding context for the left children, distinct from common prac-
tice. We found this wrapped horizon to be beneficial, and it
also unifies the infinite order model with the unmarkovized raw
rules.
Cumulative Indiv.
Annotation Size F1 1 F1 1 F1
Baseline (v ? 2, h ? 2) 7619 77.77 ? ?
UNARY-INTERNAL 8065 78.32 0.55 0.55
UNARY-DT 8066 78.48 0.71 0.17
UNARY-RB 8069 78.86 1.09 0.43
TAG-PA 8520 80.62 2.85 2.52
SPLIT-IN 8541 81.19 3.42 2.12
SPLIT-AUX 9034 81.66 3.89 0.57
SPLIT-CC 9190 81.69 3.92 0.12
SPLIT-% 9255 81.81 4.04 0.15
TMP-NP 9594 82.25 4.48 1.07
GAPPED-S 9741 82.28 4.51 0.17
POSS-NP 9820 83.06 5.29 0.28
SPLIT-VP 10499 85.72 7.95 1.36
BASE-NP 11660 86.04 8.27 0.73
DOMINATES-V 14097 86.91 9.14 1.42
RIGHT-REC-NP 15276 87.04 9.27 1.94
Figure 3: Size and devset performance of the cumulatively an-
notated models, starting with the markovized baseline. The
right two columns show the change in F1 from the baseline for
each annotation introduced, both cumulatively and for each sin-
gle annotation applied to the baseline in isolation.
history models similar in intent to those described
in Ron et al (1994). For variable horizontal his-
tories, we did not split intermediate states below 10
occurrences of a symbol. For example, if the symbol
?VP: [VBZ]. . . PP PP? were too rare, we would col-
lapse it to ?VP: [VBZ]. . . PP?. For vertical histories,
we used a cutoff which included both frequency and
mutual information between the history and the ex-
pansions (this was not appropriate for the horizontal
case because MI is unreliable at such low counts).
Figure 2 shows parsing accuracies as well as the
number of symbols in each markovization. These
symbol counts include all the intermediate states
which represent partially completed constituents.
The general trend is that, in the absence of further
annotation, more vertical annotation is better ? even
exhaustive grandparent annotation. This is not true
for horizontal markovization, where the variable-
order second-order model was superior. The best
entry, v = 3, h ? 2, has an F1 of 79.74, already
a substantial improvement over the baseline.
In the remaining sections, we discuss other an-
notations which increasingly split the symbol space.
Since we expressly do not smooth the grammar, not
all splits are guaranteed to be beneficial, and not all
sets of useful splits are guaranteed to co-exist well.
In particular, while v = 3, h ? 2 markovization is
good on its own, it has a large number of states and
does not tolerate further splitting well. Therefore,
we base all further exploration on the v ? 2, h ? 2
ROOT
S?ROOT
NP?S
NN
Revenue
VP?S
VBD
was
NP?VP
QP
$
$
CD
444.9
CD
million
,
,
S?VP
VP?S
VBG
including
NP?VP
NP?NP
JJ
net
NN
interest
,
,
CONJP
RB
down
RB
slightly
IN
from
NP?NP
QP
$
$
CD
450.7
CD
million
.
.
Figure 4: An error which can be resolved with the UNARY-
INTERNAL annotation (incorrect baseline parse shown).
grammar. Although it does not necessarily jump out
of the grid at first glance, this point represents the
best compromise between a compact grammar and
useful markov histories.
3 External vs. Internal Annotation
The two major previous annotation strategies, par-
ent annotation and head lexicalization, can be seen
as instances of external and internal annotation, re-
spectively. Parent annotation lets us indicate an
important feature of the external environment of a
node which influences the internal expansion of that
node. On the other hand, lexicalization is a (radi-
cal) method of marking a distinctive aspect of the
otherwise hidden internal contents of a node which
influence the external distribution. Both kinds of an-
notation can be useful. To identify split states, we
add suffixes of the form -X to mark internal content
features, and ?X to mark external features.
To illustrate the difference, consider unary pro-
ductions. In the raw grammar, there are many unar-
ies, and once any major category is constructed over
a span, most others become constructible as well us-
ing unary chains (see Klein and Manning (2001) for
discussion). Such chains are rare in real treebank
trees: unary rewrites only appear in very specific
contexts, for example S complements of verbs where
the S has an empty, controlled subject. Figure 4
shows an erroneous output of the parser, using the
baseline markovized grammar. Intuitively, there are
several reasons this parse should be ruled out, but
one is that the lower S slot, which is intended pri-
marily for S complements of communication verbs,
is not a unary rewrite position (such complements
usually have subjects). It would therefore be natural
to annotate the trees so as to confine unary produc-
tions to the contexts in which they are actually ap-
propriate. We tried two annotations. First, UNARY-
INTERNAL marks (with a -U) any nonterminal node
which has only one child. In isolation, this resulted
in an absolute gain of 0.55% (see figure 3). The
same sentence, parsed using only the baseline and
UNARY-INTERNAL, is parsed correctly, because the
VP rewrite in the incorrect parse ends with an S?VP-
U with very low probability.8
Alternately, UNARY-EXTERNAL, marked nodes
which had no siblings with ?U. It was similar to
UNARY-INTERNAL in solo benefit (0.01% worse),
but provided far less marginal benefit on top of
other later features (none at all on top of UNARY-
INTERNAL for our top models), and was discarded.9
One restricted place where external unary annota-
tion was very useful, however, was at the pretermi-
nal level, where internal annotation was meaning-
less. One distributionally salient tag conflation in
the Penn treebank is the identification of demonstra-
tives (that, those) and regular determiners (the, a).
Splitting DT tags based on whether they were only
children (UNARY-DT) captured this distinction. The
same external unary annotation was even more ef-
fective when applied to adverbs (UNARY-RB), dis-
tinguishing, for example, as well from also). Be-
yond these cases, unary tag marking was detrimen-
tal. The F1 after UNARY-INTERNAL, UNARY-DT,
and UNARY-RB was 78.86%.
4 Tag Splitting
The idea that part-of-speech tags are not fine-grained
enough to abstract away from specific-word be-
haviour is a cornerstone of lexicalization. The
UNARY-DT annotation, for example, showed that the
determiners which occur alone are usefully distin-
guished from those which occur with other nomi-
nal material. This marks the DT nodes with a single
bit about their immediate external context: whether
there are sisters. Given the success of parent anno-
tation for nonterminals, it makes sense to parent an-
notate tags, as well (TAG-PA). In fact, as figure 3
shows, exhaustively marking all preterminals with
their parent category was the most effective single
annotation we tried. Why should this be useful?
Most tags have a canonical category. For example,
NNS tags occur under NP nodes (only 234 of 70855
do not, mostly mistakes). However, when a tag
8Note that when we show such trees, we generally only
show one annotation on top of the baseline at a time. More-
over, we do not explicitly show the binarization implicit by the
horizontal markovization.
9These two are not equivalent even given infinite data.
VP?S
TO
to
VP?VP
VB
see
PP?VP
IN
if
NP?PP
NN
advertising
NNS
works
VP?S
TO?VP
to
VP?VP
VB?VP
see
SBAR?VP
IN?SBAR
if
S?SBAR
NP?S
NN?NP
advertising
VP?S
VBZ?VP
works
(a) (b)
Figure 5: An error resolved with the TAG-PA annotation (of the
IN tag): (a) the incorrect baseline parse and (b) the correct TAG-
PA parse. SPLIT-IN also resolves this error.
somewhat regularly occurs in a non-canonical posi-
tion, its distribution is usually distinct. For example,
the most common adverbs directly under ADVP are
also (1599) and now (544). Under VP, they are n?t
(3779) and not (922). Under NP, only (215) and just
(132), and so on. TAG-PA brought F1 up substan-
tially, to 80.62%.
In addition to the adverb case, the Penn tag set
conflates various grammatical distinctions that are
commonly made in traditional and generative gram-
mar, and from which a parser could hope to get use-
ful information. For example, subordinating con-
junctions (while, as, if ), complementizers (that, for),
and prepositions (of, in, from) all get the tag IN.
Many of these distinctions are captured by TAG-
PA (subordinating conjunctions occur under S and
prepositions under PP), but are not (both subor-
dinating conjunctions and complementizers appear
under SBAR). Also, there are exclusively noun-
modifying prepositions (of ), predominantly verb-
modifying ones (as), and so on. The annotation
SPLIT-IN does a linguistically motivated 6-way split
of the IN tag, and brought the total to 81.19%.
Figure 5 shows an example error in the baseline
which is equally well fixed by either TAG-PA or
SPLIT-IN. In this case, the more common nominal
use of works is preferred unless the IN tag is anno-
tated to allow if to prefer S complements.
We also got value from three other annotations
which subcategorized tags for specific lexemes.
First we split off auxiliary verbs with the SPLIT-
AUX annotation, which appends ?BE to all forms
of be and ?HAVE to all forms of have.10 More mi-
norly, SPLIT-CC marked conjunction tags to indicate
10This is an extended uniform version of the partial auxil-
iary annotation of Charniak (1997), wherein all auxiliaries are
marked as AUX and a -G is added to gerund auxiliaries and
gerund VPs.
whether or not they were the strings [Bb]ut or &,
each of which have distinctly different distributions
from other conjunctions. Finally, we gave the per-
cent sign (%) its own tag, in line with the dollar sign
($) already having its own. Together these three an-
notations brought the F1 to 81.81%.
5 What is an Unlexicalized Grammar?
Around this point, we must address exactly what we
mean by an unlexicalized PCFG. To the extent that
we go about subcategorizing POS categories, many
of them might come to represent a single word. One
might thus feel that the approach of this paper is to
walk down a slippery slope, and that we are merely
arguing degrees. However, we believe that there is a
fundamental qualitative distinction, grounded in lin-
guistic practice, between what we see as permitted
in an unlexicalized PCFG as against what one finds
and hopes to exploit in lexicalized PCFGs. The di-
vision rests on the traditional distinction between
function words (or closed-class words) and content
words (or open class or lexical words). It is stan-
dard practice in linguistics, dating back decades,
to annotate phrasal nodes with important function-
word distinctions, for example to have a CP[for]
or a PP[to], whereas content words are not part of
grammatical structure, and one would not have spe-
cial rules or constraints for an NP[stocks], for exam-
ple. We follow this approach in our model: various
closed classes are subcategorized to better represent
important distinctions, and important features com-
monly expressed by function words are annotated
onto phrasal nodes (such as whether a VP is finite,
or a participle, or an infinitive clause). However, no
use is made of lexical class words, to provide either
monolexical or bilexical probabilities.11
At any rate, we have kept ourselves honest by es-
timating our models exclusively by maximum like-
lihood estimation over our subcategorized gram-
mar, without any form of interpolation or shrink-
age to unsubcategorized categories (although we do
markovize rules, as explained above). This effec-
11It should be noted that we started with four tags in the Penn
treebank tagset that rewrite as a single word: EX (there), WP$
(whose), # (the pound sign), and TO), and some others such
as WP, POS, and some of the punctuation tags, which rewrite
as barely more. To the extent that we subcategorize tags, there
will be more such cases, but many of them already exist in other
tag sets. For instance, many tag sets, such as the Brown and
CLAWS (c5) tagsets give a separate sets of tags to each form of
the verbal auxiliaries be, do, and have, most of which rewrite as
only a single word (and any corresponding contractions).
VP?S
TO
to
VP?VP
VB
appear
NP?VP
NP?NP
CD
three
NNS
times
PP?NP
IN
on
NP?PP
NNP
CNN
JJ
last
NN
night
VP?S
TO
to
VP?VP
VB
appear
NP?VP
NP?NP
CD
three
NNS
times
PP?NP
IN
on
NP?PP
NNP
CNN
NP-TMP?VP
JJ
last
NN?TMP
night
(a) (b)
Figure 6: An error resolved with the TMP-NP annotation: (a)
the incorrect baseline parse and (b) the correct TMP-NP parse.
tively means that the subcategories that we break off
must themselves be very frequent in the language.
In such a framework, if we try to annotate cate-
gories with any detailed lexical information, many
sentences either entirely fail to parse, or have only
extremely weird parses. The resulting battle against
sparsity means that we can only afford to make a few
distinctions which have major distributional impact.
Even with the individual-lexeme annotations in this
section, the grammar still has only 9255 states com-
pared to the 7619 of the baseline model.
6 Annotations Already in the Treebank
At this point, one might wonder as to the wisdom
of stripping off all treebank functional tags, only
to heuristically add other such markings back in to
the grammar. By and large, the treebank out-of-the
package tags, such as PP-LOC or ADVP-TMP, have
negative utility. Recall that the raw treebank gram-
mar, with no annotation or markovization, had an F1
of 72.62% on our development set. With the func-
tional annotation left in, this drops to 71.49%. The
h ? 2, v ? 1 markovization baseline of 77.77%
dropped even further, all the way to 72.87%, when
these annotations were included.
Nonetheless, some distinctions present in the raw
treebank trees were valuable. For example, an NP
with an S parent could be either a temporal NP or a
subject. For the annotation TMP-NP, we retained the
original -TMP tags on NPs, and, furthermore, propa-
gated the tag down to the tag of the head of the NP.
This is illustrated in figure 6, which also shows an
example of its utility, clarifying that CNN last night
is not a plausible compound and facilitating the oth-
erwise unusual high attachment of the smaller NP.
TMP-NP brought the cumulative F1 to 82.25%. Note
that this technique of pushing the functional tags
down to preterminals might be useful more gener-
ally; for example, locative PPs expand roughly the
ROOT
S?ROOT
?
?
NP?S
DT
This
VP?S
VBZ
is
VP?VP
VB
panic
NP?VP
NN
buying
.
!
?
?
ROOT
S?ROOT
?
?
NP?S
DT
This
VP?S-VBF
VBZ
is
NP?VP
NN
panic
NN
buying
.
!
?
?
(a) (b)
Figure 7: An error resolved with the SPLIT-VP annotation: (a)
the incorrect baseline parse and (b) the correct SPLIT-VP parse.
same way as all other PPs (usually as IN NP), but
they do tend to have different prepositions below IN.
A second kind of information in the original
trees is the presence of empty elements. Following
Collins (1999), the annotation GAPPED-S marks S
nodes which have an empty subject (i.e., raising and
control constructions). This brought F1 to 82.28%.
7 Head Annotation
The notion that the head word of a constituent can
affect its behavior is a useful one. However, often
the head tag is as good (or better) an indicator of how
a constituent will behave.12 We found several head
annotations to be particularly effective. First, pos-
sessive NPs have a very different distribution than
other NPs ? in particular, NP ? NP ? rules are only
used in the treebank when the leftmost child is pos-
sessive (as opposed to other imaginable uses like for
New York lawyers, which is left flat). To address this,
POSS-NP marked all possessive NPs. This brought
the total F1 to 83.06%. Second, the VP symbol is
very overloaded in the Penn treebank, most severely
in that there is no distinction between finite and in-
finitival VPs. An example of the damage this con-
flation can do is given in figure 7, where one needs
to capture the fact that present-tense verbs do not
generally take bare infinitive VP complements. To
allow the finite/non-finite distinction, and other verb
type distinctions, SPLIT-VP annotated all VP nodes
with their head tag, merging all finite forms to a sin-
gle tag VBF. In particular, this also accomplished
Charniak?s gerund-VP marking. This was extremely
useful, bringing the cumulative F1 to 85.72%, 2.66%
absolute improvement (more than its solo improve-
ment over the baseline).
12This is part of the explanation of why (Charniak, 2000)
finds that early generation of head tags as in (Collins, 1999)
is so beneficial. The rest of the benefit is presumably in the
availability of the tags for smoothing purposes.
8 Distance
Error analysis at this point suggested that many re-
maining errors were attachment level and conjunc-
tion scope. While these kinds of errors are undoubt-
edly profitable targets for lexical preference, most
attachment mistakes were overly high attachments,
indicating that the overall right-branching tendency
of English was not being captured. Indeed, this ten-
dency is a difficult trend to capture in a PCFG be-
cause often the high and low attachments involve the
very same rules. Even if not, attachment height is
not modeled by a PCFG unless it is somehow ex-
plicitly encoded into category labels. More com-
plex parsing models have indirectly overcome this
by modeling distance (rather than height).
Linear distance is difficult to encode in a PCFG
? marking nodes with the size of their yields mas-
sively multiplies the state space.13 Therefore, we
wish to find indirect indicators that distinguish high
attachments from low ones. In the case of two PPs
following a NP, with the question of whether the
second PP is a second modifier of the leftmost NP
or should attach lower, inside the first PP, the im-
portant distinction is usually that the lower site is a
non-recursive base NP. Collins (1999) captures this
notion by introducing the notion of a base NP, in
which any NP which dominates only preterminals is
marked with a -B. Further, if an NP-B does not have
a non-base NP parent, it is given one with a unary
production. This was helpful, but substantially less
effective than marking base NPs without introducing
the unary, whose presence actually erased a useful
internal indicator ? base NPs are more frequent in
subject position than object position, for example. In
isolation, the Collins method actually hurt the base-
line (absolute cost to F1 of 0.37%), while skipping
the unary insertion added an absolute 0.73% to the
baseline, and brought the cumulative F1 to 86.04%.
In the case of attachment of a PP to an NP ei-
ther above or inside a relative clause, the high NP
is distinct from the low one in that the already mod-
ified one contains a verb (and the low one may be
a base NP as well). This is a partial explanation of
the utility of verbal distance in Collins (1999). To
13The inability to encode distance naturally in a naive PCFG
is somewhat ironic. In the heart of any PCFG parser, the funda-
mental table entry or chart item is a label over a span, for ex-
ample an NP from position 0 to position 5. The concrete use of
a grammar rule is to take two adjacent span-marked labels and
combine them (for example NP[0,5] and VP[5,12] into S[0,12]).
Yet, only the labels are used to score the combination.
Length ? 40 LP LR F1 Exact CB 0 CB
Magerman (1995) 84.9 84.6 1.26 56.6
Collins (1996) 86.3 85.8 1.14 59.9
this paper 86.9 85.7 86.3 30.9 1.10 60.3
Charniak (1997) 87.4 87.5 1.00 62.1
Collins (1999) 88.7 88.6 0.90 67.1
Length ? 100 LP LR F1 Exact CB 0 CB
this paper 86.3 85.1 85.7 28.8 1.31 57.2
Figure 8: Results of the final model on the test set (section 23).
capture this, DOMINATES-V marks all nodes which
dominate any verbal node (V*, MD) with a -V. This
brought the cumulative F1 to 86.91%. We also tried
marking nodes which dominated prepositions and/or
conjunctions, but these features did not help the cu-
mulative hill-climb.
The final distance/depth feature we used was an
explicit attempt to model depth, rather than use
distance and linear intervention as a proxy. With
RIGHT-REC-NP, we marked all NPs which contained
another NP on their right periphery (i.e., as a right-
most descendant). This captured some further at-
tachment trends, and brought us to a final develop-
ment F1 of 87.04%.
9 Final Results
We took the final model and used it to parse sec-
tion 23 of the treebank. Figure 8 shows the re-
sults. The test set F1 is 86.32% for ? 40 words,
already higher than early lexicalized models, though
of course lower than the state-of-the-art parsers.
10 Conclusion
The advantages of unlexicalized grammars are clear
enough ? easy to estimate, easy to parse with, and
time- and space-efficient. However, the dismal per-
formance of basic unannotated unlexicalized gram-
mars has generally rendered those advantages irrel-
evant. Here, we have shown that, surprisingly, the
maximum-likelihood estimate of a compact unlexi-
calized PCFG can parse on par with early lexicalized
parsers. We do not want to argue that lexical se-
lection is not a worthwhile component of a state-of-
the-art parser ? certain attachments, at least, require
it ? though perhaps its necessity has been overstated.
Rather, we have shown ways to improve parsing,
some easier than lexicalization, and others of which
are orthogonal to it, and could presumably be used
to benefit lexicalized parsers as well.
Acknowledgements
This paper is based on work supported in part by the
National Science Foundation under Grant No. IIS-
0085896, and in part by an IBM Faculty Partnership
Award to the second author.
References
James K. Baker. 1979. Trainable grammars for speech recogni-
tion. In D. H. Klatt and J. J. Wolf, editors, Speech Communi-
cation Papers for the 97th Meeting of the Acoustical Society
of America, pages 547?550.
Taylor L. Booth and Richard A. Thomson. 1973. Applying
probability measures to abstract languages. IEEE Transac-
tions on Computers, C-22:442?450.
Sharon A. Caraballo and Eugene Charniak. 1998. New figures
of merit for best-first probabilistic chart parsing. Computa-
tional Linguistics, 24:275?298.
Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998.
Edge-based best-first chart parsing. In Proceedings of the
Sixth Workshop on Very Large Corpora, pages 127?133.
Eugene Charniak. 1996. Tree-bank grammars. In Proc. of
the 13th National Conference on Artificial Intelligence, pp.
1031?1036.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proceedings of the 14th Na-
tional Conference on Artificial Intelligence, pp. 598?603.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL 1, pages 132?139.
Eugene Charniak. 2001. Immediate-head parsing for language
models. In ACL 39.
Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press, Cambridge, MA.
Michael John Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In ACL 34, pages 184?191.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, Univ. of Pennsylvania.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing for
bilexical context-free grammars and head-automaton gram-
mars. In ACL 37, pages 457?464.
Marilyn Ford, Joan Bresnan, and Ronald M. Kaplan. 1982. A
competence-based theory of syntactic closure. In Joan Bres-
nan, editor, The Mental Representation of Grammatical Re-
lations, pages 727?796. MIT Press, Cambridge, MA.
Daniel Gildea. 2001. Corpus variation and parser performance.
In 2001 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Donald Hindle and Mats Rooth. 1993. Structural ambiguity and
lexical relations. Computational Linguistics, 19(1):103?120.
Mark Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24:613?632.
Dan Klein and Christopher D. Manning. 2001. Parsing with
treebank grammars: Empirical bounds, theoretical models,
and the structure of the Penn treebank. In ACL 39/EACL 10.
David M. Magerman. 1995. Statistical decision-tree models for
parsing. In ACL 33, pages 276?283.
Andrew Radford. 1988. Transformational Grammar. Cam-
bridge University Press, Cambridge.
Dana Ron, Yoram Singer, and Naftali Tishby. 1994. The power
of amnesia. Advances in Neural Information Processing Sys-
tems, volume 6, pages 176?183. Morgan Kaufmann.
Corpus-Based Induction of Syntactic Structure:
Models of Dependency and Constituency
Dan Klein
Computer Science Department
Stanford University
Stanford, CA 94305-9040
klein@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
manning@cs.stanford.edu
Abstract
We present a generative model for the unsupervised
learning of dependency structures. We also describe
the multiplicative combination of this dependency model
with a model of linear constituency. The product model
outperforms both components on their respective evalu-
ation metrics, giving the best published figures for un-
supervised dependency parsing and unsupervised con-
stituency parsing. We also demonstrate that the com-
bined model works and is robust cross-linguistically, be-
ing able to exploit either attachment or distributional reg-
ularities that are salient in the data.
1 Introduction
The task of statistically inducing hierarchical syn-
tactic structure over unannotated sentences of nat-
ural language has received a great deal of atten-
tion (Carroll and Charniak, 1992; Pereira and Sch-
abes, 1992; Brill, 1993; Stolcke and Omohundro,
1994). Researchers have explored this problem for
a variety of reasons: to argue empirically against
the poverty of the stimulus (Clark, 2001), to use
induction systems as a first stage in constructing
large treebanks (van Zaanen, 2000), to build better
language models (Baker, 1979; Chen, 1995), and
to examine cognitive issues in language learning
(Solan et al, 2003). An important distinction should
be drawn between work primarily interested in the
weak generative capacity of models, where model-
ing hierarchical structure is only useful insofar as it
leads to improved models over observed structures
(Baker, 1979; Chen, 1995), and work interested in
the strong generative capacity of models, where the
unobserved structure itself is evaluated (van Zaa-
nen, 2000; Clark, 2001; Klein and Manning, 2002).
This paper falls into the latter category; we will be
inducing models of linguistic constituency and de-
pendency with the goal of recovering linguistically
plausible structures. We make no claims as to the
cognitive plausibility of the induction mechanisms
we present here; however, the ability of these sys-
tems to recover substantial linguistic patterns from
surface yields alone does speak to the strength of
support for these patterns in the data, and hence un-
dermines arguments based on ?the poverty of the
stimulus? (Chomsky, 1965).
2 Unsupervised Dependency Parsing
Most recent progress in unsupervised parsing has
come from tree or phrase-structure grammar based
models (Clark, 2001; Klein and Manning, 2002),
but there are compelling reasons to reconsider un-
supervised dependency parsing. First, most state-of-
the-art supervised parsers make use of specific lexi-
cal information in addition to word-class level infor-
mation ? perhaps lexical information could be a use-
ful source of information for unsupervised methods.
Second, a central motivation for using tree struc-
tures in computational linguistics is to enable the
extraction of dependencies ? function-argument and
modification structures ? and it might be more ad-
vantageous to induce such structures directly. Third,
as we show below, for languages such as Chinese,
which have few function words, and for which the
definition of lexical categories is much less clear,
dependency structures may be easier to detect.
2.1 Representation and Evaluation
An example dependency representation of a short
sentence is shown in figure 1(a), where, follow-
ing the traditional dependency grammar notation,
the regent or head of a dependency is marked with
the tail of the dependency arrow, and the dependent
is marked with the arrowhead (Mel?c?uk, 1988). It
will be important in what follows to see that such
a representation is isomorphic (in terms of strong
generative capacity) to a restricted form of phrase
structure grammar, where the set of terminals and
nonterminals is identical, and every rule is of the
form X ? X Y or X ? Y X (Miller, 1999), giving
the isomorphic representation of figure 1(a) shown
in figure 1(b).1 Depending on the model, part-of-
1Strictly, such phrase structure trees are isomorphic not to
flat dependency structures, but to specific derivations of those
NN
Factory
NNS
payrolls
VBD
fell
IN
in
NN
September
ROOT
VBD
NNS
NN
Factory
NNS
payrolls
VBD
VBD
fell
IN
IN
in
NN
September
S
NP
NN
Factory
NNS
payrolls
VP
VBD
fell
PP
IN
in
NN
September
(a) Classical Dependency Structure (b) Dependency Structure as CF Tree (c) CFG Structure
Figure 1: Three kinds of parse structures.
speech categories may be included in the depen-
dency representation, as shown here, or dependen-
cies may be directly between words. Below, we will
assume an additonal reserved nonterminal ROOT,
whose sole dependent is the head of the sentence.
This simplifies the notation, math, and the evalua-
tion metric.
A dependency analysis will always consist of ex-
actly as many dependencies as there are words in the
sentence. For example, in the dependency structure
of figure 1(b), the dependencies are {(ROOT, fell),
(fell, payrolls), (fell, in), (in, September), (payrolls,
Factory)}. The quality of a hypothesized depen-
dency structure can hence be evaluated by accuracy
as compared to a gold-standard dependency struc-
ture, by reporting the percentage of dependencies
shared between the two analyses.
In the next section, we discuss several models of
dependency structure, and throughout this paper we
report the accuracy of various methods at recover-
ing gold-standard dependency parses from various
corpora, detailed here. WSJ is the entire Penn En-
glish Treebank WSJ portion. WSJ10 is the subset
of sentences which contained 10 words or less after
the removal of punctuation. CTB10 is the sentences
of the same length from the Penn Chinese treebank
(v3). NEGRA10 is the same, for the German NE-
GRA corpus, based on the supplied conversion of
the NEGRA corpus into Penn treebank format. In
most of the present experiments, the provided parts-
of-speech were used as the input alphabet, though
we also present limited experimentation with syn-
thetic parts-of-speech.
It is important to note that the Penn treebanks do
not include dependency annotations; however, the
automatic dependency rules from (Collins, 1999)
are sufficiently accurate to be a good benchmark
for unsupervised systems for the time being (though
see below for specific issues). Similar head-finding
rules were used for Chinese experiments. The NE-
GRA corpus, however, does supply hand-annotated
dependency structures.
structures which specify orders of attachment among multiple
dependents which share a common head.
? ? ? ? ? ROOT
Figure 2: Dependency graph with skeleton chosen, but
words not populated.
Where possible, we report an accuracy figure for
both directed and undirected dependencies. Report-
ing undirected numbers has two advantages: first, it
facilitates comparison with earlier work, and, more
importantly, it allows one to partially obscure the
effects of alternate analyses, such as the system-
atic choice between a modal and a main verb for
the head of a sentence (in either case, the two verbs
would be linked, but the direction would vary).
2.2 Dependency Models
The dependency induction task has received rela-
tively little attention; the best known work is Car-
roll and Charniak (1992), Yuret (1998), and Paskin
(2002). All systems that we are aware of operate un-
der the assumption that the probability of a depen-
dency structure is the product of the scores of the
dependencies (attachments) in that structure. De-
pendencies are seen as ordered (head, dependent)
pairs of words, but the score of a dependency can
optionally condition on other characteristics of the
structure, most often the direction of the depen-
dency (whether the arrow points left or right).
Some notation before we present specific mod-
els: a dependency d is a pair ?h, a? of a head and
argument, which are words in a sentence s, in a cor-
pus S. For uniformity of notation with section 4,
words in s are specified as size-one spans of s: for
example the first word would be 0s1. A dependency
structure D over a sentence is a set of dependencies
(arcs) which form a planar, acyclic graph rooted at
the special symbol ROOT, and in which each word
in s appears as an argument exactly once. For a de-
pendency structure D, there is an associated graph
G which represents the number of words and arrows
between them, without specifying the words them-
selves (see figure 2). A graph G and sentence s to-
gether thus determine a dependency structure. The
Model Dir. Undir.
English (WSJ)
Paskin 01 39.7
RANDOM 41.7
Charniak and Carroll 92-inspired 44.7
ADJACENT 53.2
DMV 54.4
English (WSJ10)
RANDOM 30.1 45.6
ADJACENT 33.6 56.7
DMV 43.2 63.7
German (NEGRA10)
RANDOM 21.8 41.5
ADJACENT 32.6 51.2
DMV 36.3 55.8
Chinese (CTB10)
RANDOM 35.9 47.3
ADJACENT 30.2 47.3
DMV 42.5 54.2
Figure 3: Parsing performance (directed and undirected
dependency accuracy) of various dependency models on
various treebanks, along with baselines.
dependency structure is the object generated by all
of the models that follow; the steps in the deriva-
tions vary from model to model.
Existing generative dependency models intended
for unsupervised learning have chosen to first gen-
erate a word-free graph G, then populate the sen-
tence s conditioned on G. For instance, the model of
Paskin (2002), which is broadly similar to the semi-
probabilistic model in Yuret (1998), first chooses a
graph G uniformly at random (such as figure 2),
then fills in the words, starting with a fixed root
symbol (assumed to be at the rightmost end), and
working down G until an entire dependency struc-
ture D is filled in (figure 1a). The corresponding
probabilistic model is
P(D) = P(s, G)
= P(G)P(s|G)
= P(G)
?
(i, j,dir)?G
P(i?1si | j?1s j , dir) .
In Paskin (2002), the distribution P(G) is fixed to be
uniform, so the only model parameters are the con-
ditional multinomial distributions P(a|h, dir) that
encode which head words take which other words
as arguments. The parameters for left and right ar-
guments of a single head are completely indepen-
dent, while the parameters for first and subsequent
arguments in the same direction are identified.
In those experiments, the model above was
trained on over 30M words of raw newswire, using
EM in an entirely unsupervised fashion, and at great
computational cost. However, as shown in figure 3,
the resulting parser predicted dependencies at be-
low chance level (measured by choosing a random
dependency structure). This below-random perfor-
mance seems to be because the model links word
pairs which have high mutual information (such
as occurrences of congress and bill) regardless of
whether they are plausibly syntactically related. In
practice, high mutual information between words is
often stronger between two topically similar nouns
than between, say, a preposition and its object.
One might hope that the problem with this model
is that the actual lexical items are too semanti-
cally charged to represent workable units of syn-
tactic structure. If one were to apply the Paskin
(2002) model to dependency structures parameter-
ized simply on the word-classes, the result would
be isomorphic to the ?dependency PCFG? models
described in Carroll and Charniak (1992). In these
models, Carroll and Charniak considered PCFGs
with precisely the productions (discussed above)
that make them isomorphic to dependency gram-
mars, with the terminal alphabet being simply parts-
of-speech. Here, the rule probabilities are equiva-
lent to P(Y|X, right) and P(Y|X, left) respectively.2
The actual experiments in Carroll and Charniak
(1992) do not report accuracies that we can compare
to, but they suggest that the learned grammars were
of extremely poor quality. With hindsight, however,
the main issue in their experiments appears to be not
their model, but that they randomly initialized the
production (attachment) probabilities. As a result,
their learned grammars were of very poor quality
and had high variance. However, one nice property
of their structural constraint, which all dependency
models share, is that the symbols in the grammar are
not symmetric. Even with a grammar in which the
productions are initially uniform, a symbol X can
only possibly have non-zero posterior likelihood
over spans which contain a matching terminal X.
Therefore, one can start with uniform rewrites and
let the interaction between the data and the model
structure break the initial symmetry. If one recasts
their experiments in this way, they achieve an accu-
racy of 44.7% on the Penn treebank, which is higher
than choosing a random dependency structure, but
lower than simply linking all adjacent words into a
left-headed (and right-branching) structure (53.2%).
A huge limitation of both of the above models is
that they are incapable of encoding even first-order
valence facts. For example, the latter model learns
that nouns to the left of the verb (usually subjects)
2There is another, subtle distinction: in the Paskin work,
a canonical ordering of multiple attachments was fixed, while
in the Carroll and Charniak work all attachment orders are con-
sidered, giving a numerical bias towards structures where heads
take more than one argument.
ih
j
dae
k
h
i
dae
j
he
k
he
i
h
j
he
STOP
i
he
j
dhe
STOP
(a) (b) (c) (d)
Figure 4: Dependency configurations in a lexicalized tree: (a) right attachment, (b) left attachment, (c) right stop, (d)
left stop. h and a are head and argument words, respectively, while i , j , and k are positions between words.
attach to the verb. But then, given a NOUN NOUN
VERB sequence, both nouns will attach to the verb
? there is no way that the model can learn that verbs
have exactly one subject. We now turn to an im-
proved dependency model that addresses this prob-
lem.
3 An Improved Dependency Model
The dependency models discussed above are dis-
tinct from dependency models used inside high-
performance supervised probabilistic parsers in sev-
eral ways. First, in supervised models, a head out-
ward process is modeled (Eisner, 1996; Collins,
1999). In such processes, heads generate a sequence
of arguments outward to the left or right, condition-
ing on not only the identity of the head and direc-
tion of the attachment, but also on some notion of
distance or valence. Moreover, in a head-outward
model, it is natural to model stop steps, where the
final argument on each side of a head is always the
special symbol STOP. Models like Paskin (2002)
avoid modeling STOP by generating the graph skele-
ton G first, uniformly at random, then populating
the words of s conditioned on G. Previous work
(Collins, 1999) has stressed the importance of in-
cluding termination probabilities, which allows the
graph structure to be generated jointly with the ter-
minal words, precisely because it does allow the
modeling of required dependents.
We propose a simple head-outward dependency
model over word classes which includes a model
of valence, which we call DMV (for dependency
model with valence). We begin at the ROOT. In the
standard way, each head generates a series of non-
STOP arguments to one side, then a STOP argument
to that side, then non-STOP arguments to the other
side, then a second STOP.
For example, in the dependency structure in fig-
ure 1, we first generate a single child of ROOT, here
fell. Then we recurse to the subtree under fell. This
subtree begins with generating the right argument
in. We then recurse to the subtree under in (gener-
ating September to the right, a right STOP, and a left
STOP). Since there are no more right arguments af-
ter in, its right STOP is generated, and the process
moves on to the left arguments of fell.
In this process, there are two kinds of deriva-
tion events, whose local probability factors consti-
tute the model?s parameters. First, there is the de-
cision at any point whether to terminate (generate
STOP) or not: PSTOP(STOP|h, dir, ad j). This is a bi-
nary decision conditioned on three things: the head
h, the direction (generating to the left or right of
the head), and the adjacency (whether or not an ar-
gument has been generated yet in the current di-
rection, a binary variable). The stopping decision
is estimated directly, with no smoothing. If a stop
is generated, no more arguments are generated for
the current head to the current side. If the current
head?s argument generation does not stop, another
argument is chosen using: PCHOOSE(a|h, dir). Here,
the argument is picked conditionally on the iden-
tity of the head (which, recall, is a word class) and
the direction. This term, also, is not smoothed in
any way. Adjacency has no effect on the identity
of the argument, only on the likelihood of termina-
tion. After an argument is generated, its subtree in
the dependency structure is recursively generated.
Formally, for a dependency structure D, let
each word h have left dependents depsD(h, l)
and right dependents depsD(h, r). The follow-
ing recursion defines the probability of the frag-
ment D(h) of the dependency tree rooted at h:
P(D(h)) =
?
dir?{l,r}
?
a?depsD(h,dir)
PSTOP(?STOP|h, dir, ad j)
PCHOOSE(a|h, dir)P(D(a))
PSTOP(STOP|h, dir, ad j)
One can view a structure generated by this deriva-
tional process as a ?lexicalized? tree composed of
the local binary and unary context-free configura-
tions shown in figure 4.3 Each configuration equiv-
alently represents either a head-outward derivation
step or a context-free rewrite rule. There are four
such configurations. Figure 4(a) shows a head h
3It is lexicalized in the sense that the labels in the tree are
derived from terminal symbols, but in our experiments the ter-
minals were word classes, not individual lexical items.
taking a right argument a. The tree headed by h
contains h itself, possibly some right arguments of
h, but no left arguments of h (they attach after all
the right arguments). The tree headed by a contains
a itself, along with all of its left and right children.
Figure 4(b) shows a head h taking a left argument a
? the tree headed by h must have already generated
its right stop to do so. Figure 4(c) and figure 4(d)
show the sealing operations, where STOP derivation
steps are generated. The left and right marks on
node labels represent left and right STOPs that have
been generated.4
The basic inside-outside algorithm (Baker, 1979)
can be used for re-estimation. For each sentence
s ? S, it gives us cs(x : i, j), the expected frac-
tion of parses of s with a node labeled x extend-
ing from position i to position j . The model can
be re-estimated from these counts. For example, to
re-estimate an entry of PSTOP(STOP|h, left, non-adj)
according to a current model 2, we calculate two
quantities.5 The first is the (expected) number of
trees headed by he whose rightmost edge i is strictly
left of h. The second is the number of trees headed
by dhe with rightmost edge i strictly left of h. The
ratio is the MLE of that local probability factor:
PSTOP(STOP|h, left, non-adj) =
?
s?S
?
i<loc(h)
?
k c(he : i, k)
?
s?S
?
i<loc(h)
?
k c(dhe : i, k)
This can be intuitively thought of as the relative
number of times a tree headed by h had already
taken at least one argument to the left, had an op-
portunity to take another, but didn?t.6
Initialization is important to the success of any
local search procedure. We chose to initialize EM
not with an initial model, but with an initial guess
at posterior distributions over dependency structures
(completions). For the first-round, we constructed
a somewhat ad-hoc ?harmonic? completion where
all non-ROOT words took the same number of ar-
guments, and each took other words as arguments
in inverse proportion to (a constant plus) the dis-
tance between them. The ROOT always had a single
4Note that the asymmetry of the attachment rules enforces
the right-before-left attachment convention. This is harmless
and arbitrary as far as dependency evaluations go, but imposes
an x-bar-like structure on the constituency assertions made by
this model. This bias/constraint is dealt with in section 5.
5To simplify notation, we assume each word h occurs at
most one time in a given sentence, between indexes loc(h) and
loc(h) + 1).
6As a final note, in addition to enforcing the right-argument-
first convention, we constrained ROOT to have at most a single
dependent, by a similar device.
argument and took each word with equal probabil-
ity. This structure had two advantages: first, when
testing multiple models, it is easier to start them all
off in a common way by beginning with an M-step,
and, second, it allowed us to point the model in the
vague general direction of what linguistic depen-
dency structures should look like.
On the WSJ10 corpus, the DMV model recov-
ers a substantial fraction of the broad dependency
trends: 43.2% of guessed directed dependencies
were correct (63.7% ignoring direction). To our
knowledge, this is the first published result to break
the adjacent-word heuristic (at 33.6% for this cor-
pus). Verbs are the sentence heads, prepositions
take following noun phrases as arguments, adverbs
attach to verbs, and so on. The most common source
of discrepancy between the test dependencies and
the model?s guesses is a result of the model system-
atically choosing determiners as the heads of noun
phrases, while the test trees have the rightmost noun
as the head. The model?s choice is supported by
a good deal of linguistic research (Abney, 1987),
and is sufficiently systematic that we also report the
scores where the NP headship rule is changed to per-
colate determiners when present. On this adjusted
metric, the score jumps hugely to 55.7% directed
(and 67.9% undirected).
This model also works on German and Chinese at
above-baseline levels (55.8% and 54.2% undirected,
respectively), with no modifications whatsoever. In
German, the largest source of errors is also the
systematic postulation of determiner-headed noun-
phrases. In Chinese, the primary mismatch is that
subjects are considered to be the heads of sentences
rather than verbs.
This dependency induction model is reasonably
successful. However, our intuition is still that the
model can be improved by paying more attention
to syntactic constituency. To this end, after briefly
recapping the model of Klein and Manning (2002),
we present a combined model that exploits depen-
dencies and constituencies. As we will see, this
combined model finds correct dependencies more
successfully than the model above, and finds con-
stituents more successfully than the model of Klein
and Manning (2002).
4 Distributional Constituency Induction
In linear distributional clustering, items (e.g., words
or word sequences) are represented by characteristic
distributions over their linear contexts (e.g., multi-
nomial models over the preceding and following
words, see figure 5). These context distributions
are then clustered in some way, often using standard
     
     
     
     
     
Span Label Constituent Context
?0,5? S NN NNS VBD IN NN  ? 
?0,2? NP NN NNS  ? VBD
?2,5? VP VBD IN NN NNS ? 
?3,5? PP IN NN VBD ? 
?0,1? NN NN  ? NNS
?1,2? NNS NNS NN ? VBD
?2,3? VBD VBD NNS ? IN
?3,4? IN IN VBD ? NN
?4,5? NN NNS IN ? 
(a) (b)
Figure 5: The CCM model?s generative process for the
sentence in figure 1. (a) A binary tree-equivalent brack-
eting is chosen at random. (b) Each span generates its
yield and context (empty spans not shown here). Deriva-
tions which are not coherent are given mass zero.
data clustering methods. In the most common case,
the items are words, and one uses distributions over
adjacent words to induce word classes. Previous
work has shown that even this quite simple repre-
sentation allows the induction of quite high quality
word classes, largely corresponding to traditional
parts of speech (Finch, 1993; Schu?tze, 1995; Clark,
2000). A typical pattern would be that stocks and
treasuries both frequently occur before the words
fell and rose, and might therefore be put into the
same class.
Clark (2001) and Klein and Manning (2002)
show that this approach can be successfully used
for discovering syntactic constituents as well. How-
ever, as one might expect, it is easier to cluster
word sequences (or word class sequences) than to
tell how to put them together into trees. In par-
ticular, if one is given all contiguous subsequences
(subspans) from a corpus of sentences, most natu-
ral clusters will not represent valid constituents (to
the extent that constituency of a non-situated se-
quence is even a well-formed notion). For exam-
ple, it is easy enough to discover that DET N and
DET ADJ N are similar and that V PREP DET and
V PREP DET ADJ are similar, but it is much less
clear how to discover that the former pair are gen-
erally constituents while the latter pair are generally
not. In Klein and Manning (2002), we proposed a
constituent-context model (CCM) which solves this
problem by building constituency decisions directly
into the distributional model, by earmarking a sin-
gle cluster d for non-constituents. During the cal-
culation of cluster assignments, only a non-crossing
subset of the observed word sequences can be as-
signed to other, constituent clusters. This integrated
approach is empirically successful.
The CCM works as follows. Sentences are given
as sequences s of word classes (parts-of-speech or
otherwise). One imagines each sentence as a list
of the O(n2) index pairs ?i, j?, each followed by
the corresponding subspan is j and linear context
i?1si ? j s j+1 (see figure 5). The model generates
all constituent-context pairs, span by span.
The first stage is to choose a bracketing B for
the sentence, which is a maximal non-crossing sub-
set of the spans (equivalent to a binary tree). In
the basic model, P(B) is uniform over binary trees.
Then, for each ?i, j?, the subspan and context pair
(i s j , i?1si ? j s j+1) is generated via a class-
conditional independence model:
P(s, B) = P(B)
?
?i, j ?
P(is j |bi j )P(i?1si ? j s j+1|bi j )
That is, all spans guess their sequences and contexts
given only a constituency decision b.7
This is a model P(s, B) over hidden bracketings
and observed sentences, and it is estimated via EM
to maximize the sentence likelihoods P(s) over the
training corpus. Figure 6 shows the accuracy of the
CCM model not only on English but for the Chinese
and German corpora discussed above.8 Results are
reported at convergence; for the English case, F1
is monotonic during training, while for the others,
there is an earlier peak.
Also shown is an upper bound (the target trees are
not all binary and so any all-binary system will over-
propose constituents). Klein and Manning (2002)
gives comparative numbers showing that the basic
CCM outperforms other recent systems on the ATIS
corpus (which many other constituency induction
systems have reported on). While absolute numbers
are hard to compare across corpora, all the systems
compared to in Klein and Manning (2002) parsed
below a right-branching baseline, while the CCM is
substantially above it.
5 A Combined Model
The two models described above have some com-
mon ground. Both can be seen as models over lexi-
calized trees composed of the configurations in fig-
ure 4. For the DMV, it is already a model over these
structures. At the ?attachment? rewrite for the CCM
7As is typical of distributional clustering, positions in the
corpus can get generated multiple times. Since derivations
need not be consistent, the entire model is mass deficient when
viewed as a model over sentences.
8In Klein and Manning (2002), we reported results using
unlabeled bracketing statistics which gave no credit for brack-
ets which spanned the entire sentence (raising the scores) but
macro-averaged over sentences (lowering the scores). The
numbers here hew more closely to the standard methods used
for evaluating supervised parsers, by being micro-averaged and
including full-span brackets. However, the scores are, overall,
approximately the same.
in (a/b), we assign the quantity:
P(isk |true)P(i?1si ? ksk+1|true)
P(isk |false)P(i?1si ? ksk+1|false)
which is the odds ratio of generating the subse-
quence and context for span ?i, k? as a constituent
as opposed to a non-constituent. If we multiply all
trees? attachment scores by
?
?i, j ? P(is j |false)P(i?1si ? j s j+1|false)
the denominators of the odds ratios cancel, and we
are left with each tree being assigned the probability
it would have received under the CCM.9
In this way, both models can be seen as generat-
ing either constituency or dependency structures. Of
course, the CCM will generate fairly random depen-
dency structures (constrained only by bracketings).
Getting constituency structures from the DMV is
also problematic, because the choice of which side
to first attach arguments on has ramifications on
constituency ? it forces x-bar-like structures ? even
though it is an arbitrary convention as far as depen-
dency evaluations are concerned. For example, if
we attach right arguments first, then a verb with a
left subject and a right object will attach the ob-
ject first, giving traditional VPs, while the other at-
tachment order gives subject-verb groups. To avoid
this bias, we alter the DMV in the following ways.
When using the dependency model alone, we allow
each word to have even probability for either gener-
ation order (but in each actual head derivation, only
one order occurs). When using the models together,
better performance was obtained by releasing the
one-side-attaching-first requirement entirely.
In figure 6, we give the behavior of the CCM con-
stituency model and the DMV dependency model
on both constituency and dependency induction.
Unsurprisingly, their strengths are complementary.
The CCM is better at recovering constituency, and
the dependency model is better at recovering depen-
dency structures. It is reasonable to hope that a com-
bination model might exhibit the best of both. In the
supervised parsing domain, for example, scoring a
lexicalized tree with the product of a simple lexical
dependency model and a PCFG model can outper-
form each factor on its respective metric (Klein and
Manning, 2003).
9This scoring function as described is not a generative
model over lexicalized trees, because it has no generation step
at which nodes? lexical heads are chosen. This can be corrected
by multiplying in a ?head choice? factor of 1/(k ? j) at each fi-
nal ?sealing? configuration (d). In practice, this correction fac-
tor was harmful for the model combination, since it duplicated
a strength of the dependency model, badly.
Model UP UR UF1 Dir Undir
English (WSJ10 ? 7422 Sentences)
LBRANCH/RHEAD 25.6 32.6 28.7 33.6 56.7
RANDOM 31.0 39.4 34.7 30.1 45.6
RBRANCH/LHEAD 55.1 70.0 61.7 24.0 55.9
DMV 46.6 59.2 52.1 43.2 62.7
CCM 64.2 81.6 71.9 23.8 43.3
DMV+CCM (POS) 69.3 88.0 77.6 47.5 64.5
DMV+CCM (DISTR.) 65.2 82.8 72.9 42.3 60.4
UBOUND 78.8 100.0 88.1 100.0 100.0
German (NEGRA10 ? 2175 Sentences)
LBRANCH/RHEAD 27.4 48.8 35.1 32.6 51.2
RANDOM 27.9 49.6 35.7 21.8 41.5
RBRANCH/LHEAD 33.8 60.1 43.3 21.0 49.9
DMV 38.4 69.5 49.5 40.0 57.8
CCM 48.1 85.5 61.6 25.5 44.9
DMV+CCM 49.6 89.7 63.9 50.6 64.7
UBOUND 56.3 100.0 72.1 100.0 100.0
Chinese (CTB10 ? 2437 Sentences)
LBRANCH/RHEAD 26.3 48.8 34.2 30.2 43.9
RANDOM 27.3 50.7 35.5 35.9 47.3
RBRANCH/LHEAD 29.0 53.9 37.8 14.2 41.5
DMV 35.9 66.7 46.7 42.5 54.2
CCM 34.6 64.3 45.0 23.8 40.5
DMV+CCM 33.3 62.0 43.3 55.2 60.3
UBOUND 53.9 100.0 70.1 100.0 100.0
Figure 6: Parsing performance of the combined model
on various treebanks, along with baselines.
In the combined model, we score each tree with
the product of the probabilities from the individ-
ual models above. We use the inside-outside algo-
rithm to sum over all lexicalized trees, similar to the
situation in section 3. The tree configurations are
shown in figure 4. For each configuration, the rele-
vant scores from each model are multiplied together.
For example, consider figure 4(a). From the CCM
we must generate isk as a constituent and its cor-
responding context. From the dependency model,
we pay the cost of h taking a as a right argument
(PCHOOSE), as well as the cost of choosing not to
stop (PSTOP). We then running the inside-outside al-
gorithm over this product model. For the results,
we can extract the sufficient statistics needed to re-
estimate both individual models.10
The models in combination were intitialized in
the same way as when they were run individually.
Sufficient statistics were separately taken off these
individual completions. From then on, the resulting
models were used together during re-estimation.
Figure 6 summarizes the results. The combined
model beats the CCM on English F1: 77.6 vs. 71.9.
The figure also shows the combination model?s
score when using word classes which were induced
entirely automatically, using the simplest distribu-
tional clustering method of Schu?tze (1995). These
classes show some degradation, e.g. 72.9 F1, but it
10The product, like the CCM itself, is mass-deficient.
is worth noting that these totally unsupervised num-
bers are better than the performance of the CCM
model of Klein and Manning (2002) running off
of Penn treebank word classes. Again, if we mod-
ify the gold standard so as to make determiners the
head of NPs, then this model with distributional tags
scores 50.6% on directed and 64.8% on undirected
dependency accuracy.
On the German data, the combination again out-
performs each factor alone, though while the com-
bination was most helpful at boosting constituency
quality for English, for German it provided a larger
boost to the dependency structures. Finally, on
the Chinese data, the combination did substantially
boost dependency accuracy over either single factor,
but actually suffered a small drop in constituency.11
Overall, the combination is able to combine the in-
dividual factors in an effective way.
6 Conclusion
We have presented a successful new dependency-
based model for the unsupervised induction of syn-
tactic structure, which picks up the key ideas that
have made dependency models successful in super-
vised statistical parsing work. We proceeded to
show that it works cross-linguistically. We then
demonstrated how this model could be combined
with the previous best constituent-induction model
to produce a combination which, in general, sub-
stantially outperforms either individual model, on
either metric. A key reason that these models are ca-
pable of recovering structure more accurately than
previous work is that they minimize the amount of
hidden structure that must be induced. In particu-
lar, neither model attempts to learn intermediate, re-
cursive categories with no direct connection to sur-
face statistics. Our results here are just on the un-
grounded induction of syntactic structure. Nonethe-
less, we see the investigation of what patterns can
be recovered from corpora as important, both from a
computational perspective and from a philosophical
one. It demonstrates that the broad constituent and
dependency structure of a language can be recov-
ered quite successfully (individually or, more effec-
tively, jointly) from a very modest amount of train-
ing data.
7 Acknowledgements
This work was supported by a Microsoft Gradu-
ate Research Fellowship to the first author and by
11This seems to be partially due to the large number of un-
analyzed fragments in the Chinese gold standard, which leave
a very large fraction of the posited bracketings completely un-
judged.
the Advanced Research and Development Activity
(ARDA)?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program. This work also ben-
efited from an enormous amount of useful feedback,
from many audiences and individuals.
References
Stephen P. Abney. 1987. The English Noun Phrase in its Sentential
Aspect. Ph.D. thesis, MIT.
James K. Baker. 1979. Trainable grammars for speech recognition. In
D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers
for the 97th Meeting of the Acoustical Society of America, pages
547?550.
Eric Brill. 1993. Automatic grammar induction and parsing free text:
A transformation-based approach. In ACL 31, pages 259?265.
Glenn Carroll and Eugene Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from corpora. In Carl
Weir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, edi-
tors, Working Notes of the Workshop Statistically-Based NLP Tech-
niques, pages 1?13. AAAI Press, Menlo Park, CA.
Stanley F. Chen. 1995. Bayesian grammar induction for language
modeling. In ACL 33, pages 228?235.
Noam Chomsky. 1965. Aspects of the Theory of Syntax. MIT Press,
Cambridge, MA.
Alexander Clark. 2000. Inducing syntactic categories by context distri-
bution clustering. In The Fourth Conference on Natural Language
Learning.
Alexander Clark. 2001. Unsupervised induction of stochastic context-
free grammars using distributional clustering. In The Fifth Confer-
ence on Natural Language Learning.
Michael Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
Jason Eisner. 1996. Three new probabilistic models for dependency
parsing: An exploration. In COLING 16, pages 340?345.
Steven Paul Finch. 1993. Finding Structure in Language. Ph.D. thesis,
University of Edinburgh.
Dan Klein and Christopher D. Manning. 2002. A generative
constituent-context model for improved grammar induction. In ACL
40, pages 128?135.
Dan Klein and Christopher D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In Suzanna
Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances
in Neural Information Processing Systems 15, Cambridge, MA.
MIT Press.
Igor Aleksandrovich Mel? c?uk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany, NY.
Philip H. Miller. 1999. Strong Generative Capacity. CSLI Publications,
Stanford, CA.
Mark A. Paskin. 2002. Grammatical bigrams. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in Neural Infor-
mation Processing Systems 14, Cambridge, MA. MIT Press.
Fernando Pereira and Yves Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL 30, pages 128?135.
Hinrich Schu?tze. 1995. Distributional part-of-speech tagging. In EACL
7, pages 141?148.
Zach Solan, Eytan Ruppin, David Horn, and Shimon Edelman. 2003.
Automatic acquisition and efficient representation of syntactic
structures. In Suzanna Becker, Sebastian Thrun, and Klaus Ober-
mayer, editors, Advances in Neural Information Processing Systems
15, Cambridge, MA. MIT Press.
Andreas Stolcke and Stephen M. Omohundro. 1994. Inducing proba-
bilistic grammars by Bayesian model merging. In Grammatical In-
ference and Applications: Proceedings of the Second International
Colloquium on Grammatical Inference. Springer Verlag.
Menno van Zaanen. 2000. ABL: Alignment-based learning. In COL-
ING 18, pages 961?967.
Deniz Yuret. 1998. Discovery of Linguistic Relations Using Lexical
Attraction. Ph.D. thesis, MIT.
Distributional Phrase Structure Induction
Dan Klein and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
 
klein, manning  @cs.stanford.edu
Abstract
Unsupervised grammar induction systems
commonly judge potential constituents on
the basis of their effects on the likelihood
of the data. Linguistic justifications of
constituency, on the other hand, rely on
notions such as substitutability and vary-
ing external contexts. We describe two
systems for distributional grammar induc-
tion which operate on such principles, us-
ing part-of-speech tags as the contextual
features. The advantages and disadvan-
tages of these systems are examined, in-
cluding precision/recall trade-offs, error
analysis, and extensibility.
1 Overview
While early work showed that small, artificial
context-free grammars could be induced with the
EM algorithm (Lari and Young, 1990) or with
chunk-merge systems (Stolcke and Omohundro,
1994), studies with large natural language gram-
mars have shown that these methods of completely
unsupervised acquisition are generally ineffective.
For instance, Charniak (1993) describes experi-
ments running the EM algorithm from random start-
ing points, which produced widely varying gram-
mars of extremely poor quality. Because of these
kinds of results, the vast majority of statistical pars-
ing work has focused on parsing as a supervised
learning problem (Collins, 1997; Charniak, 2000).
It remains an open problem whether an entirely un-
supervised method can either produce linguistically
sensible grammars or accurately parse free text.
However, there are compelling motivations for
unsupervised grammar induction. Building super-
vised training data requires considerable resources,
including time and linguistic expertise. Further-
more, investigating unsupervised methods can shed
light on linguistic phenomena which are implic-
itly captured within a supervised parser?s supervi-
sory information, and, therefore, often not explicitly
modeled in such systems. For example, our system
and others have difficulty correctly attaching sub-
jects to verbs above objects. For a supervised CFG
parser, this ordering is implicit in the given structure
of VP and S constituents, however, it seems likely
that to learn attachment order reliably, an unsuper-
vised system will have to model it explicitly.
Our goal in this work is the induction of high-
quality, linguistically sensible grammars, not pars-
ing accuracy. We present two systems, one which
does not do disambiguation well and one which
does not do it at all. Both take tagged but unparsed
Penn treebank sentences as input.1 To whatever de-
gree our systems parse well, it can be taken as evi-
dence that their grammars are sensible, but no effort
was taken to improve parsing accuracy directly.
There is no claim that human language acquisi-
tion is in any way modeled by the systems described
here. However, any success of these methods is evi-
dence of substantial cues present in the data, which
could potentially be exploited by humans as well.
Furthermore, mistakes made by these systems could
indicate points where human acquisition is likely
not being driven by these kinds of statistics.
2 Approach
At the heart of any iterative grammar induction sys-
tem is a method, implicit or explicit, for deciding
how to update the grammar. Two linguistic criteria
for constituency in natural language grammars form
the basis of this work (Radford, 1988):
1. External distribution: A constituent is a se-
quence of words which appears in various
structural positions within larger constituents.
1The Penn tag and category sets used in examples in this
paper are documented in Manning and Schu?tze (1999, 413).
2. Substitutability: A constituent is a sequence of
words with (simple) variants which can be sub-
stituted for that sequence.
To make use of these intuitions, we use a distribu-
tional notion of context. Let  be a part-of-speech
tag sequence. Every occurence of  will be in some
context  , where  and  are the adjacent tags or
sentence boundaries. The distribution over contexts
in which  occurs is called its signature, which we
denote by 	
 .
Criterion 1 regards constituency itself. Consider
the tag sequences IN DT NN and IN DT. The former
is a canonical example of a constituent (of category
PP), while the later, though strictly more common,
is, in general, not a constituent. Frequency alone
does not distinguish these two sequences, but Crite-
rion 1 points to a distributional fact which does. In
particular, IN DT NN occurs in many environments.
It can follow a verb, begin a sentence, end a sen-
tence, and so on. On the other hand, IN DT is gener-
ally followed by some kind of a noun or adjective.
This example suggests that a sequence?s con-
stituency might be roughly indicated by the entropy
of its signature, 	
 . This turns out to be
somewhat true, given a few qualifications. Figure 1
shows the actual most frequent constituents along
with their rankings by several other measures. Tag
entropy by itself gives a list that is not particularly
impressive. There are two primary causes for this.
One is that uncommon but possible contexts have
little impact on the tag entropy value. Given the
skewed distribution of short sentences in the tree-
bank, this is somewhat of a problem. To correct for
this, let 
 be the uniform distribution over the
observed contexts for  . Using 
 would
have the obvious effect of boosting rare contexts,
and the more subtle effect of biasing the rankings
slightly towards more common sequences. How-
ever, while 	
 presumably converges to some
sensible limit given infinite data, 
 will
not, as noise eventually makes all or most counts
non-zero. Let  be the uniform distribution over all
contexts. The scaled entropy
Combining Heterogeneous Classifiers for Word-Sense Disambiguation
Dan Klein, Kristina Toutanova, H. Tolga Ilhan,
Sepandar D. Kamvar and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040, USA
Abstract
This paper discusses ensembles of simple but het-
erogeneous classifiers for word-sense disambigua-
tion, examining the Stanford-CS224N system en-
tered in the SENSEVAL-2 English lexical sample
task. First-order classifiers are combined by a
second-order classifier, which variously uses ma-
jority voting, weighted voting, or a maximum en-
tropy model. While individual first-order classifiers
perform comparably to middle-scoring teams? sys-
tems, the combination achieves high performance.
We discuss trade-offs and empirical performance.
Finally, we present an analysis of the combination,
examining how ensemble performance depends on
error independence and task difficulty.
1 Introduction
The problem of supervised word sense disambigua-
tion (WSD) has been approached using many differ-
ent classification algorithms, including naive-Bayes,
decision trees, decision lists, and memory-based
learners. While it is unquestionable that certain al-
gorithms are better suited to the WSD problem than
others (for a comparison, see Mooney (1996)), it
seems that, given similar input features, various al-
gorithms exhibit roughly similar accuracies.1 This
was supported by the SENSEVAL-2 results, where a
This paper is based on work supported in part by the Na-
tional Science Foundation under Grants IIS-0085896 and IIS-
9982226, by an NSF Graduate Fellowship, and by the Research
Collaboration between NTT Communication Science Labora-
tories, Nippon Telegraph and Telephone Corporation and CSLI,
Stanford University.
1In fact, we have observed that differences between imple-
mentations of a single classifier type, such as smoothing or win-
dow size, impacted accuracy far more than the choice of classi-
fication algorithm.
large fraction of systems had scores clustered in a
fairly narrow region (Senseval-2, 2001).
We began building our system with 23 supervised
WSD systems, each submitted by a student taking
the natural language processing course (CS224N) at
Stanford University in Spring 2000. Students were
free to implement whatever WSD method they chose.
While most implemented variants of naive-Bayes,
others implemented a range of other methods, in-
cluding n-gram models, vector space models, and
memory-based learners. Taken individually, the best
of these systems would have turned in an accuracy
of 61.2% in the SENSEVAL-2 English lexical sam-
ple task (which would have given it 6th place), while
others would have produced middling to low perfor-
mance. In this paper, we investigate how these clas-
sifiers behave in combination.
In section 2, we discuss the first-order classifiers
and describe our methods of combination. In sec-
tion 3, we discuss performance, analyzing what ben-
efit was found from combination, and when. We also
discuss aspects of the component systems which
substantially influenced overall performance.
2 The System
2.1 Training Procedure
Figure 1 shows the high-level organization of our
system. Individual first-order classifiers each map
lists of context word tokens to word-sense predic-
tions, and are self-contained WSD systems. The first-
order classifiers are combined in a variety of ways
with second-order classifiers. Second-order classi-
fiers are selectors, taking a list of first-order out-
                     July 2002, pp. 74-80.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
rankingorder2nd.
Entropy
Validation
Voting
1 2 3 4 5 6 7 8
MaximumWeighted
Voting
Majority
classifiersorder1st.
2nd. classifiersorder
Chosen
ClassifierFinal classifier
Cross
rankingorder1st.
Figure 1: High-level system organization.
1 Split data into multiple training and held-out parts.
2 Rank first-order classifiers globally (across all words).
3 Rank first-order classifiers locally (per word),
breaking ties with global ranks.
4 For each word w
5 For each size k
6 Choose the ensemble Ew,k to be the top k classifiers
7 For each voting method m
8 Train the (k, m) second-order classifier with Ew,k
9 Rank the second-order classifier types (k, m) globally.
10 Rank the second-order classifier instances locally.
11 Choose the top-ranked second-order classifier for each word.
12 Retrain chosen per-word classifiers on entire training data.
13 Run these classifiers on test data, and evaluate results.
Table 1: The classifier construction process.
puts and choosing from among them. An outline
of the classifier construction process is given in ta-
ble 1. First, the training data was split into training
and held-out sets for each word. This was done us-
ing 5 random bootstrap splits. Each split allocated
75% of the examples to training and 25% to held-
out testing.2 Held-out data was used both to select
the subsets of first-order classifiers to be combined,
and to select the combination methods.
For each word and each training split, the 23 first-
order classifiers were (independently) trained and
tested on held-out data. For each word, the first-
order classifiers were ranked by their average per-
formance on the held-out data, with the most accu-
rate classifiers at the top of the rankings. Ties were
broken by the classifiers? (weighted) average perfo-
mance across all words.
For each word, we then constructed a set of can-
2Bootstrap splits were used rather than standard n-fold
cross-validation for two reasons. First, it allowed us to gen-
erate an arbitrary number of training/held-out pairs while still
leaving substantial held-out data set sizes. Second, this ap-
proach is commonly used in the literature on ensembles. Its
well-foundedness and theoretical properties are discussed in
Breiman (1996). In retrospect, since we did not take proper ad-
vantage of the ability to generate numerous splits, it might have
been just as well to use cross-validation.
didate second-order classifiers. Second-order clas-
sifier types were identified by an ensemble size k
and a combination method m. One instance of each
second-order type was constructed for each word.
We originally considered ensemble sizes k in the
range {1, 3, 5, 7, 9, 11, 13, 15}. For a second-order
classifier with ensemble size k, the ensemble mem-
bers were the top k first-order classifiers according
to the local rank described above.
We combined first-order ensembles using one of
three methods m:
? Majority voting: The sense output by the most
first-order classifiers in the ensemble was chosen.
Ties were broken by sense frequency, in favor of
more frequent senses.
? Weighted voting: Each first-order classifier was
assigned a voting weight (see below). The sense
receiving the greatest total weighted vote was
chosen.
? Maximum entropy: A maximum entropy classifier
was trained (see below) and run on the outputs of
the first-order classifiers.
We considered all pairs of k and m, and so for
each word there were 24 possible second-order clas-
sifiers, though for k = 1 all three values of m are
equivalent and were merged. The k = 1 ensemble,
as well as the larger ensembles (k ? {9, 11, 13, 15}),
did not help performance once we had good first-
order classifier rankings (see section 3.4).
For m = Majority, there are no parameters to set.
For the other two methods, we set the parameters of
the (k, m) second-order classifier for a word w using
the bootstrap splits of the training data for w.
In the same manner as for the first-order classi-
fiers, we then ranked the second-order classifiers.
For each word, there was the local ranking of the
second-order classifiers, given by their (average) ac-
curacy on held-out data. Ties in these rankings were
broken by the average performance of the classifier
type across all words. The top second-order classi-
fier for each word was selected from these tie-broken
rankings.
At this point, all first-order ensemble members
and chosen second-order combination methods were
retrained on the unsplit training data and run on the
final test data.
It is important to stress that each target word was
considered an entirely separate task, and different
first- and second-order choices could be, and were,
made for each word (see the discussion of table 2
below). Aggregate performance across words was
only used for tie-breaking.
2.2 Combination Methods
Our second-order classifiers take training instances
of the form s? = (s, s1, . . . , sk) where s is the correct
sense and each si is the sense chosen by classifier i .
All three of the combination schemes which we used
can be seen as weighted voting, with different ways
of estimating the voting weights ?i of the first-order
voters. In the simplest case, majority voting, we skip
any attempt at statistical estimation and simply set
each ?i to be 1/k.
For the method we actually call ?weighted vot-
ing,? we view the combination output as a mixture
model in which each first-order system is a mixture
component:
P(s|s1, . . . , sk) =
?
i
?i P(s|si )
The conditional probabilties P(s|si ) assign mass
one to the sense si chosen by classifier i . The mix-
ture weights ?i were estimated using EM to max-
imize the likelihood of the second-order training
instances. In testing, the sense with the highest
weighted vote, and hence highest posterior likeli-
hood, is the selected sense.
For the maximum entropy classifier, we have a
different model for the chosen sense s. In this case,
it is an exponential model of the form:
P(s|s1, . . . , sk) =
exp
?
x ?x fx(s, s1, . . . , sk)
?
t exp
?
x ?x fx(t, s1, . . . , sk)
The features fx are functions which are true over
some subset of vectors s?. The original intent was to
design features to recognize and exploit ?sense ex-
pertise? in the individual classifiers. For example,
one classifier might be trustworthy when reporting
a certain sense but less so for other senses. How-
ever, there was not enough data to accurately esti-
mate parameters for such models.3 In fact, we no-
3The number of features was not large: only one for each
(classifier, chosen sense, correct sense) triple. However, most
senses are rarely chosen and rarely correct, so most features
had zero or singleton support.
ticed that, for certain words, simple majority voting
performed better than the maximum entropy model.
It also turned out that the most complex features we
could get value from were features of the form:
fi(s, s1, . . . , sk) = 1 ?? s = si
That is, for each first-order classifier, there is a sin-
gle feature which is true exactly when that classi-
fier is correct. With only these features, the maxi-
mum entropy approach also reduces to a weighted
vote; the s which maximizes the posterior probabil-
ity P(s|s1, . . . , sk) also maximizes the vote:
v(s) =
?
i ?i?(si = s)
The indicators ? are true for exactly one sense, and
correspond to the simple f i defined above.4 The
sense with the largest vote v(s) will be the sense
with the highest posterior probability P(s|s1, . . . sk)
and will be chosen.
For the maximum entropy classifier, we estimate
the weights by maximizing the likelihood of a held-
out set, using the standard IIS algorithm (Berger et
al., 1996). For both weighted schemes, we found
that stopping the iterative procedures before conver-
gence gave better results. IIS was halted after 50
rounds, while EM was halted after a single round.
Both methods were initialized to uniform starting
weights.
More importantly than changing the exact weight
estimates, moving from method to method triggers
broad qualitative changes in what kind of weights
are allowed. With majority voting, classifiers all
have equal, positive weights. With weighted vot-
ing, the weights are no longer required to be equal,
but are still non-negative. With maximum entropy
weighting, this non-negativity constraint is also re-
laxed, allowing classifiers? votes to actually reduce
the score for the sense that classifier has chosen.
Negative weights are in fact assigned quite fre-
quently, and often seem to have the effect of using
poor classifiers as ?error masks? to cancel out com-
mon errors.
As we move from majority voting to weighted
voting to maximum entropy, the estimation becomes
4If the i th classifier returns the correct sense s, then
?(si = s) is 1, otherwise it is zero.
more sophisticated, but also more prone to overfit-
ting. Since solving the overfitting problem is hard,
while choosing between classifiers based on held-
out data is relatively easy, this spectrum gives us a
way to gracefully handle the range of sparsities in
the training corpora for different words.
2.3 Individual Classifiers
While our first-order classifiers implemented a va-
riety of classification algorithms, the differences in
their individual accuracies did not primarily stem
from the algorithm chosen. Rather, implementation
details led to the largest differences. For example,
naive-Bayes classifiers which chose sensible win-
dow sizes, or dynamically chose between window
sizes, tended to outperform those which chose poor
sizes. Generally, the optimal windows were either
of size one (for words with strong local syntactic or
collocational cues) or of very large size (which de-
tected more topical cues). Programs with hard-wired
window sizes of, say, 5, performed poorly. Iron-
ically, such middle-size windows were commonly
chosen by students, but rarely useful; either extreme
was a better design.5
Another implementation choice dramatically af-
fecting performance of naive-Bayes systems was the
amount and type of smoothing. Heavy smoothing
and smoothing which backed off conditional dis-
tributions P(w j |si) to the relevant marginal P(w j)
gave good results, while insufficient smoothing or
backing off to uniform marginals gave substantially
degraded results.6
There is one significant way in which our first-
order classifiers were likely different from other
teams? systems. In the original class project, stu-
dents were guaranteed that the ambiguous word
would appear only in a single orthographic form,
and many of the systems depended on the input sat-
isfying this guarantee. Since this was not true of
the SENSEVAL-2 data, we mapped the ambiguous
5Such window sizes were also apparently chosen by other
SENSEVAL-2 systems, which commonly used ?long distance?
and ?local? features, but defined local as a window size of 3?5
words on each side of the ambiguous word.
6In particular, there is a defective behavior with naive-Bayes
where, when one smooths far too little, the chosen sense is the
one which has occurred with the most words in the context
window. For small training sets of skewed-prior data like the
SENSEVAL-2 sets, this is invariably the common sense, regard-
less of the context words.
words (but not context words) to a citation form.
We suspect that this lost quite a bit of information
and negatively affected the system?s overall perfor-
mance, since there is considerable correlation be-
tween form and sense, especially for verbs. Nev-
ertheless, we have made no attempt to re-engineer
the student systems, and have not thoroughly inves-
tigated how big a difference this stemming made.
3 Results and Discussion
3.1 Results
Table 2 shows the results per word, and table 3
shows results by part-of-speech and overall, for the
SENSEVAL-2 English lexical sample task. It also
shows what second-order classifiers were selected
for each word. 54.2% of the time, we made an opti-
mal second-order classifier choice. When we chose
wrong, we usually made a mistake in either ensem-
ble size or method, rarely both. A wide range of
second-order classifier types were chosen. As an
overview of the benefit of combination, the globally
best single classifier scored 61.2%, the locally best
single classifier (best on test data) scored 62.2%, the
globally best second order classifier (ME-7, best on
test data) scored 63.2%, and our dynamic selection
method scored 63.9%. Section 3.3 examines combi-
nation effectiveness more closely.
3.2 Changes from SENSEVAL-2
The system we originally submitted to the
SENSEVAL-2 competition had an overall accu-
racy of 61.7%, putting it in 4th place in the revised
rankings (among 21 supervised and 28 total sys-
tems). Assuming that our first-order classifiers
were fixed black-boxes, we wanted an idea of how
good our combination and selection methods were.
To isolate the effectiveness of our second-order
classifier choices, we compared our system to an
oracle method (OR-BEST) which chose a word?s
second-order classifier based on test data (rather
than held-out data). The overall accuracy of this
oracle method was 65.4% at the time, a jump of
3.7%.7 This gap was larger than the gap between
the various top-scoring teams? systems. Therefore,
while the test-set performance of the second-order
classifiers is obviously not available, it was clear
7With other changes, OR-BEST rose to 66.1%.
LB Baselines Combination OR UB System
Word ALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACC CL
art-n 28.6 41.8 50.6 52.0 54.1 52.0 58.2 69.4 58.2 WT-5
authority-n 45.7 33.7 61.3 69.6 69.6 65.2 69.6 78.3 66.3 WT-3
bar-n 31.1 39.7 63.7 61.6 69.5 72.2 72.2 81.5 72.2 ME-7
begin-v 50.0 58.6 70.0 83.6 84.3 88.2 88.2 94.6 83.6 MJ-7
blind-a 65.5 83.6 77.8 83.6 83.6 85.5 85.5 90.9 83.6 WT-7
bum-n 71.1 75.6 71.3 75.6 75.6 77.8 77.8 82.2 77.8 ME-7
call-v 1.5 25.8 33.3 25.8 30.3 27.3 34.8 62.1 30.3 WT-7
carry-v 9.1 22.7 27.8 34.8 33.3 33.3 37.9 62.1 33.3 MJ-5
chair-n 76.8 79.7 84.2 82.6 82.6 82.6 82.6 84.1 81.2 ME-3
channel-n 46.6 27.4 61.1 60.3 60.3 65.8 67.1 78.1 67.1 ME-3
child-n 34.4 54.7 57.9 67.2 70.3 70.3 75.0 90.6 71.9 WT-5
church-n 56.2 53.1 63.1 73.4 73.4 75.0 75.0 85.9 73.4 WT-7
circuit-n 52.9 27.1 70.9 65.9 65.9 78.8 78.8 80.0 78.8 ME-5
collaborate-v 90.0 90.0 92.9 90.0 90.0 90.0 90.0 90.0 90.0 WT-5
colorless-a 48.6 65.7 80.0 68.6 68.6 68.6 68.6 82.9 68.6 ME-5
cool-a 15.4 46.2 65.0 57.7 55.8 59.6 59.6 80.8 59.6 ME-5
day-n 36.6 59.3 58.4 69.0 68.3 66.2 69.0 82.8 63.4 WT-3
develop-v 11.6 29.0 35.2 42.0 43.5 42.0 43.5 68.1 42.0 MJ-3
draw-v 4.9 9.8 23.4 29.3 26.8 24.4 29.3 41.5 26.8 WT-5
dress-v 25.4 42.4 49.9 52.5 52.5 55.9 59.3 72.9 55.9 ME-7
drift-v 3.1 25.0 31.7 37.5 37.5 34.4 37.5 65.6 37.5 WT-5
drive-v 16.7 28.6 40.0 45.2 45.2 40.5 45.2 61.9 42.9 MJ-3
dyke-n 85.7 89.3 86.5 89.3 89.3 89.3 92.9 96.4 92.9 WT-3
face-v 82.8 83.9 80.9 83.9 83.9 82.8 83.9 84.9 83.9 WT-5
facility-n 36.2 48.3 70.5 67.2 70.7 65.5 74.1 86.2 70.7 WT-7
faithful-a 56.5 78.3 65.0 78.3 78.3 78.3 82.6 100.0 78.3 MJ-3
fatigue-n 67.4 76.7 83.9 88.4 90.7 90.7 90.7 93.0 90.7 MJ-5
feeling-n 29.4 56.9 76.7 62.7 70.6 72.5 74.5 86.3 72.5 WT-3
find-v 7.4 14.7 37.6 30.9 27.9 30.9 32.4 48.5 32.4 WT-3
fine-a 32.9 38.6 46.9 51.4 57.1 54.3 57.1 67.1 52.9 MJ-3
fit-a 51.7 51.7 87.7 89.7 89.7 86.2 93.1 96.6 93.1 MJ-5
free-a 26.8 39.0 58.2 65.9 65.9 61.0 65.9 74.4 64.6 ME-3
graceful-a 62.1 75.9 81.4 79.3 79.3 79.3 79.3 82.8 79.3 WT-5
green-a 69.1 78.7 80.0 83.0 83.0 83.0 85.1 88.3 83.0 MJ-3
grip-n 25.5 54.9 49.2 60.8 60.8 58.8 74.5 84.3 60.8 MJ-7
hearth-n 46.9 75.0 56.3 75.0 71.9 65.6 75.0 84.4 62.5 WT-3
holiday-n 77.4 83.9 89.7 83.9 83.9 80.6 83.9 87.1 83.9 WT-5
keep-v 19.4 37.3 36.1 38.8 49.3 52.2 52.2 65.7 52.2 WT-5
lady-n 60.4 69.8 67.7 75.5 75.5 77.4 77.4 81.1 75.5 WT-3
leave-v 21.2 31.8 29.1 43.9 53.0 50.0 54.5 68.2 54.5 WT-5
live-v 20.9 50.7 54.6 53.7 59.7 65.7 71.6 77.6 71.6 MJ-3
local-a 15.8 57.9 76.8 71.1 68.4 68.4 71.1 92.1 71.1 MJ-7
match-v 11.9 35.7 30.4 52.4 52.4 57.1 57.1 78.6 47.6 WT-3
material-n 39.1 42.0 56.0 55.1 55.1 50.7 66.7 73.9 66.7 WT-3
mouth-n 15.0 45.0 40.5 53.3 53.3 45.0 56.7 78.3 53.3 MJ-5
nation-n 70.3 70.3 71.1 70.3 70.3 70.3 70.3 70.3 70.3 WT-5
natural-a 18.4 27.2 50.4 49.5 50.5 58.3 58.3 76.7 55.3 WT-3
nature-n 23.9 45.7 51.3 63.0 67.4 65.2 67.4 82.6 60.9 MJ-5
oblique-a 51.7 69.0 73.7 82.8 82.8 82.8 86.2 89.7 79.3 WT-5
play-v 12.1 19.7 35.6 40.9 51.5 50.0 51.5 62.1 51.5 WT-5
post-n 26.6 31.6 66.5 49.4 57.0 65.8 67.1 73.4 67.1 ME-3
pull-v 1.7 21.7 27.7 21.7 21.7 28.3 28.3 46.7 23.3 WT-3
replace-v 28.9 53.3 49.0 57.8 53.3 60.0 60.0 77.8 57.8 MJ-7
restraint-n 35.6 31.1 53.9 71.1 68.9 71.1 71.1 82.2 66.7 ME-5
see-v 29.0 31.9 40.0 42.0 42.0 42.0 42.0 55.1 42.0 MJ-5
sense-n 18.9 22.6 46.3 64.2 60.4 50.9 64.2 79.2 64.2 MJ-7
serve-v 35.3 29.4 54.4 60.8 64.7 66.7 66.7 74.5 62.7 WT-5
simple-a 51.5 51.5 43.0 51.5 51.5 51.5 51.5 54.5 51.5 ME-3
solemn-a 96.0 96.0 89.2 96.0 96.0 96.0 96.0 96.0 96.0 WT-3
spade-n 66.7 63.6 81.8 75.8 75.8 78.8 78.8 81.8 78.8 WT-3
stress-n 7.7 46.2 47.0 43.6 43.6 35.9 51.3 82.1 48.7 WT-5
strike-v 5.6 16.7 32.3 31.5 29.6 29.6 40.7 55.6 31.5 MJ-5
train-v 22.2 30.2 48.3 57.1 57.1 54.0 57.1 76.2 57.1 WT-7
treat-v 36.4 38.6 51.8 54.5 54.5 52.3 54.5 70.5 52.3 WT-3
turn-v 1.5 14.9 38.8 32.8 29.9 32.8 35.8 52.2 31.3 MJ-5
use-v 61.8 65.8 69.6 65.8 65.8 72.4 72.4 75.0 72.4 ME-3
vital-a 84.2 92.1 91.5 92.1 92.1 92.1 92.1 92.1 92.1 WT-5
wander-v 70.0 80.0 83.2 80.0 82.0 82.0 82.0 84.0 80.0 ME-3
wash-v 16.7 25.0 40.0 58.3 58.3 25.0 58.3 83.3 58.3 MJ-7
work-v 10.0 26.7 28.1 43.3 43.3 41.7 45.0 63.3 45.0 WT-3
yew-n 75.0 78.6 81.4 78.6 78.6 78.6 78.6 82.1 78.6 WT-5
Table 2: Results by word for the SENSEVAL-2 English lexi-
cal sample task. Lower bound (LB): ALL is how often all of
the first-orders chose correctly. Baselines (BL): MFS is the
most-frequent-sense baseline, SNG is the best single first-order
classifier as chosen on held-out data for that word. Fixed com-
binations: majority vote (MJ), weighted vote (WT), maximum
entropy (ME). Oracle bound (OR): BEST is the best second-
order classifier as measured on the test data. Upper bound (UB):
SOME is how often at least one first-order classifier produced
the correct answer. Methods which are ensemble-size depen-
dent are shown for k = 7. System choices: ACC is the accuracy
of the selection the system makes based on held-out data. CL is
the 2nd-order classifier selected.
that a more sophisticated or better-tuned method
of selecting combination models could lead to
significant improvement. In fact, changing only
ranking methods, which are discussed further in the
next section, resulted in an increase in final accu-
racy for our system to the current score of 63.9%,
which would have placed it 1st in the SENSEVAL-2
preliminary results or 2nd in the revised results. Our
LB Baselines Combination OR UB System
ALL MFS SNG MJ-7 WT-7 ME-7 BEST SOME ACC
noun 42.5 50.5 63.8 66.4 67.9 67.8 71.9 81.2 69.7
adj. 45.1 57.8 66.7 69.0 69.4 69.9 71.6 81.0 69.9
verb 28.8 40.2 48.7 53.4 54.7 55.8 58.2 71.2 55.7
avg. 46.5 47.5 62.2 61.5 62.7 63.2 68.9 72.0 63.9
Table 3: Results by part-of-speech, and overall.
58
59
60
61
62
63
64
65
1 3 5 7 9 11 13 15
 	
 	   	 
 





 Conditional Structure versus Conditional Estimation in NLP Models
Dan Klein and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305-9040
{klein, manning}@cs.stanford.edu
Abstract
This paper separates conditional parameter estima-
tion, which consistently raises test set accuracy on
statistical NLP tasks, from conditional model struc-
tures, such as the conditional Markov model used
for maximum-entropy tagging, which tend to lower
accuracy. Error analysis on part-of-speech tagging
shows that the actual tagging errors made by the
conditionally structured model derive not only from
label bias, but also from other ways in which the in-
dependence assumptions of the conditional model
structure are unsuited to linguistic sequences. The
paper presents new word-sense disambiguation and
POS tagging experiments, and integrates apparently
conflicting reports from other recent work.
1 Introduction
The success and widespread adoption of probabilis-
tic models in NLP has led to numerous variant meth-
ods for any given task, and it can be difficult to tell
what aspects of a system have led to its relative suc-
cesses or failures. As an example, maximum en-
tropy taggers have achieved very good performance
(Ratnaparkhi, 1998; Toutanova and Manning, 2000;
Lafferty et al, 2001), but almost identical perfor-
mance has also come from finely tuned HMM mod-
els (Brants, 2000; Thede and Harper, 1999). Are any
performance gains due to the sequence model used,
the maximum entropy approach to parameter estima-
tion, or the features employed by the system?
Recent experiments have given conflicting recom-
mendations. Johnson (2001) finds that a condition-
ally trained PCFG marginally outperforms a standard
jointly trained PCFG, but that a conditional shift-
reduce model performs worse than a joint formu-
lation. Lafferty et al (2001) suggest on abstract
grounds that conditional models will suffer from a
phenomenon called label bias (Bottou, 1991) ? see
section 3 ? but is this a significant effect for real NLP
problems?
We suggest that the results in the literature, along
with the new results we present in this work, can be
explained by the following generalizations:
? The ability to include better features in a well-
founded fashion leads to better performance.
? For fixed features, assumptions implicit in the
model structure have a large impact on errors.
? Maximizing the objective being evaluated has a re-
liably positive, but often small, effect.
It is especially important to study these issues us-
ing NLP data sets: NLP tasks are marked by their
complexity and sparsity, and, as we show, conclu-
sions imported from the machine-learning literature
do not always hold in these characteristic contexts.
In previous work, the structure of a model and
the method of parameter estimation were often both
changed simultaneously (for reasons of naturalness
or computational ease), but in this paper we seek to
tease apart the separate effects of these two factors.
In section 2, we take the Naive-Bayes model, ap-
plied to word-sense disambiguation (WSD), and train
it to maximize various objective functions. Our ex-
periments reaffirm that discriminative objectives like
conditional likelihood improve test-set accuracy. In
section 3, we examine two different model structures
for part-of-speech (POS) tagging. There, we ana-
lyze how assumptions latent in conditional structures
lower tagging accuracy and produce strange quali-
tative behaviors. Finally, we discuss related recent
findings by other researchers.
2 Objective Functions: Naive-Bayes
For bag-of-words WSD, we have a corpus D of la-
beled examples (s, o). Each o = ?oi ? is a list of con-
text words, and the corresponding s is the correct
sense of a fixed target word occuring in that context.
A particular model for this task is the familiar multi-
                                            Association for Computational Linguistics.
                       Language Processing (EMNLP), Philadelphia, July 2002, pp. 9-16.
                         Proceedings of the Conference on Empirical Methods in Natural
nomial Naive-Bayes (NB) model (Gale et al, 1992;
McCallum and Nigam, 1998), where we assume con-
ditional independence between each of the oi . This
NB model gives a joint distribution over the s and ?oi ?
variables:
P(s, o) = P(s)
?
i
P(oi |s)
It also implicitly makes conditional predictions:
P(s|o) = P(s, o)/
?
s?
P(s?, o)
In NLP, NB models are typically used in this latter
way to make conditional decisions, such as chosing
the most likely word sense.1
The parameters 2 = ??s; ?o|s? for this model are
the sense priors P(s) and the sense-conditional word
distributions P(o|s). These are typically set using
(smoothed) relative frequency estimators (RFEs):
?s = P(s) = count(s)/|D|
?o|s = P(o|s) = count(s, o)/
?
o?
count(s, o?)
These intuitive relative frequency estimators are the
estimates for 2 which maximize the joint likelihood
(JL) of D according to the NB model:
J L(2, D) =
?
(s,o)?D
P(s, o)
A NB model which has been trained to maximize JL
will be referred to as NB-JL. It is worth emphasiz-
ing that, in NLP applications, the model is typically
trained jointly, then used for its P(s|o) predictions.
We can set the parameters in other ways, without
changing our model. If we are doing classification,
we may not care about JL. Rather, we will want to
minimize whatever kinds of errors we get charged
for. The JL objective is the evaluation criterion for
language modeling, but a decision process? evalua-
tion is more naturally phrased in terms of P(s|o). If
we want to maximize the probability assigned to the
correct labeling of the corpus, the appropriate objec-
tive is conditional likelihood (CL):
C L(2, D) =
?
(s,o)?D
P(s|o)
This focuses on the sense predictions, not the words,
which is what we cared about in the first place.
Figure 1 shows an example of the trade-offs be-
tween JL and CL. Assume there are two classes (1
and 2), two words (a and b), and only 2-word con-
texts. Assume the actual distribution (training and
test) is 3 each of (1, ab) and (1, ba) and one (2, aa)
1A possible use for the joint predictions would be a topic-
conditional unigram language model.
P(s, o) P(s|o) Correct?
s o Counts Actual NB-JL NB-CL Actual NB-JL NB-CL NB-JL NB-CL
1 aa 0 0 3/14 /4 0 3/5 /4
1 ab 3 3/7 3/14 /4 1 1 1 + +
1 ba 3 3/7 3/14 /4 1 1 1 + +
1 bb 0 0 3/14 /4 0 1 1
2 aa 1 1/7 1/7 1 ?  1 2/5 1 ? /4 - +
2 ab 0 0 0 0 0 0 0
2 ba 0 0 0 0 0 0 0
2 bb 0 0 0 0 0 0 0
Limit log prod. -0.44 -0.69 -? 0.00 -0.05 0.00
Accuracy 6/7 7/7
Model P(1) P(2) P(a|1) P(b|1) P(a|2) P(b|2)
NB-JL 6/7 1/7 1/2 1/2 1 0
NB-CL  1- 1/2 1/2 1 0
Figure 1: Example of joint vs. conditional estimation.
for 7 samples. Then, as shown in figure 1, the JL-
maximizing NB model has priors of 6/7 and 1/7, like
the data. The actual (joint) distribution is not in the
family of NB models, and so it cannot be learned per-
fectly. Still, the NB-JL assigns reasonable probabili-
ties to all occurring events. However, its priors cause
it to incorrectly predict that aa belongs to class 1. On
the other hand, maximizing CL will push the prior for
sense 1 arbitrarily close to zero. As a result, its con-
ditional predictions become more accurate at the cost
of its joint prediction. NB-CL joint prediction assigns
vanishing mass to events other than (2, aa), and so its
joint likelihood score gets arbitrarily bad.
There are other objectives (or loss functions). In
the SENSEVAL competition (Kilgarriff, 1998), we
guess sense distributions, and our score is the sum
of the masses assigned to the correct senses. This
objective is the sum of conditional likelihoods (SCL):
SC L(2, D) =
?
(s,o)?D
P(s|o)
SCL is less appropriate that CL when the model is
used as a step in a probabilistic process, rather than
in isolation. CL is more appropriate for filter pro-
cesses, because it highly punishes assigning zero or
near-zero probabilities to observed outcomes.
If we choose single senses and receive a score of
either 1 or 0 on an instance, then we have 0/1-loss
(Friedman, 1997). This gives the ?number correct?
and so we refer to the corresponding objective as ac-
curacy (Acc):
Acc(2, D) =
?
(s,o)?D
?(s = arg maxs? P(s?|o))
In the following experiments, we illustrate that, for
a fixed model structure, it is advantageous to max-
imize objective functions which are similar to the
evaluation criteria. Although in principle we can op-
timize any of the objectives above, in practice some
are harder to optimize than others. As stated above,
JL is trivial to maximize with a NB model. CL and
SCL, since they are continuous in 2, can be opti-
mized by gradient methods. Acc is not continuous
in 2 and is unsuited to direct optimization (indeed,
finding an optimum is NP-complete).
When optimizing an arbitrary function of 2, we
have to make sure that our probabilities remain well-
formed. If we want to have a well-formed joint NB in-
terpretation, we must have non-negative parameters
and the inequalities ?s
?
o ?o|s ? 1 and
?
s ?s ? 1.
If we want to be guaranteed a non-deficient joint in-
terpretation, we can require equality. However, if we
relax the equality then we have a larger feasible space
which may give better values of our objective.
We performed the following WSD experiments
with Naive-Bayes models. We took as data the col-
lection of SENSEVAL-2 English lexical sample WSD
corpora.2 We set the NB model parameters in several
ways. We optimized JL (using the RFEs).3 We also
optimized SCL and (the log of) CL, using a conju-
gate gradient (CG) method (Press et al, 1988).4 For
CL and SCL, we optimized each objective both over
the space of all distributions and over the subspace
of non-deficient models (giving CL? and SCL?). Acc
was not directly optimized.
Unconstrained CL corresponds exactly to a condi-
tional maximum entropy model (Berger et al, 1996;
Lafferty et al, 2001). This particular case, where
there are multiple explanatory variables and a sin-
gle categorical response variable, is also precisely
the well-studied statistical model of (multinomial)
logistic regression (Agresti, 1990). Its optimization
problem is concave (over log parameters) and there-
fore has a unique global maximum. For CL?, SCL,
and SCL?, we are only guaranteed local optima, but
in practice we detected no maxima which were not
2http://www.sle.sharp.co.uk/senseval2/
3Smoothing is an important factor for this task. So that the
various estimates would be smoothed as similarly as possible,
we smoothed implicitly, by adding smoothing data. We added
one instance of each class occurring with the bag containing
each vocabulary word once. This gave the same result as add-
one smoothing on the RFEs for NB-JL, and ensured that NB-
CL would not assign zero conditional probability to any unseen
event. The smoothing data did not, however, result in smoothed
estimates for SCL; any conditional probability will sum to one
over the smoothing instances. For this objective, we added a
penalty term proportional to
?
?2, which ensured that no con-
ditional sense probabilities reached 0 or 1.
4All optimization was done using conjugate gradient as-
cent over log parameters ?i = log ?i , rather than the given
parameters due to sensitivity near zero and improved quality
of quadratic approximations during optimization. Linear con-
straints over ? are not linear in log space, and were enforced
using a quadratic Lagrange penalty method (Bertsekas, 1995).
TRAINING SET
Optimization Acc MacroAcc log J L log C L SCL
NB-JL 86.8 86.2 -22969684.7 -243184.1 4505.9
NB-CL* 98.5 96.2 -23366291.2 -973.0 5101.2
NB-CL 98.5 96.2 -23431010.0 -854.1 5115.1
NB-SCL* 94.2 93.7 -23054768.6 -226187.8 4884.4
NB-SCL 97.3 95.5 -23146735.3 -220145.0 5055.8
TEST SET
Optimization Acc MacroAcc log J L log C L SCL
NB-JL 73.6 55.0 -1816757.1 -55251.5 3695.4
NB-CL* 72.3 53.4 -1954977.1 -19854.1 3566.3
NB-CL 76.2 56.5 -1964498.5 -20498.7 3798.8
NB-SCL* 74.8 57.2 -1841305.0 -43027.8 3754.1
NB-SCL 77.5 59.7 -1872533.0 -33249.7 3890.8
Figure 2: Scores for the NB model trained according to vari-
ous objectives. Scores are usually higher on both training and
test sets for the objective maximized, and discriminative criteria
lead to better test-set accuracy. The best scores are in bold.
global over the feasible region.
Figure 2 shows, for each objective maximized, the
values of all objectives on both the training and test
set. Optimizing for a given objective generally gave
the best score for that objective for both the training
set and the test set. The exception is NB-SCL and NB-
SCL* which have lower SCL score than NB-CL and
NB-CL*. This is due to the penalty used for smooth-
ing the summed models (see fn. 3).
Accuracy is higher when optimizing the discrim-
inative objectives, CL and SCL, than when optimiz-
ing JL (including for macro-averaging, where each
word?s contribution to average accuracy is made
equal). That these estimates beat NB-JL on accu-
racy is unsurprising, since Acc is a discretization of
conditional predictions, not joint ones. This sup-
ports the claim that maximizing conditional likeli-
hood, or other discriminative objectives, improves
test set accuracy for realistic NLP tasks. NB-SCL,
though harder to maximize in general, gives better
test-set accuracy than NB-CL.5 NB-CL* is some-
where between JL and CL for all objectives on the
training data. Its behavior shows that the change
from a standard NB approach (NB-JL) to a maximum
entropy classifier (NB-CL) can be broken into two as-
pects: a change in objective and an abandonment of
a non-deficiency constraint.6 Note that the JL score
for NB-CL*, is not very much lower than for NB-JL,
despite a large change in CL.
It would be too strong to state that maximizing CL
5This difference seems to be partially due to the different
smoothing methods used: Chen and Rosenfeld (1999) show
that quadratic penalties are very effective in practice, while the
smoothing-data method is quite crude.
6If one is only interested in the model?s conditional predic-
tions, there is no reason to disprefer deficient joint models.
-50
0
50
0 50 100 150 200 250
  	

 	  		
 
 
 Named Entity Recognition with Character-Level Models
Dan Klein
 
and Joseph Smarr

and Huy Nguyen
 
and Christopher D. Manning
 
 
Computer Science Dept.

Symbolic Systems Program
Stanford University Stanford University
Stanford, CA 94305-9040 Stanford, CA 94305-2181

klein,htnguyen,manning  @cs.stanford.edu jsmarr@stanford.edu
Abstract
We discuss two named-entity recognition mod-
els which use characters and character  -grams
either exclusively or as an important part of
their data representation. The first model
is a character-level HMM with minimal con-
text information, and the second model is a
maximum-entropy conditional markov model
with substantially richer context features. Our
best model achieves an overall F  of 86.07%
on the English test data (92.31% on the devel-
opment data). This number represents a 25%
error reduction over the same model without
word-internal (substring) features.
1 Introduction
For most sequence-modeling tasks with word-level eval-
uation, including named-entity recognition and part-of-
speech tagging, it has seemed natural to use entire words
as the basic input features. For example, the classic
HMM view of these two tasks is one in which the ob-
servations are words and the hidden states encode class
labels. However, because of data sparsity, sophisti-
cated unknown word models are generally required for
good performance. A common approach is to extract
word-internal features from unknown words, for example
suffix, capitalization, or punctuation features (Mikheev,
1997, Wacholder et al, 1997, Bikel et al, 1997). One
then treats the unknown word as a collection of such fea-
tures. Having such unknown-word models as an add-on
is perhaps a misplaced focus: in these tasks, providing
correct behavior on unknown words is typically the key
challenge.
Here, we examine the utility of taking character se-
quences as a primary representation. We present two
models in which the basic units are characters and char-
acter  -grams, instead of words and word phrases. Ear-
lier papers have taken a character-level approach to
named entity recognition (NER), notably Cucerzan and
Yarowsky (1999), which used prefix and suffix tries,
though to our knowledge incorporating all character  -
grams is new. In section 2, we discuss a character-level
HMM, while in section 3 we discuss a sequence-free
maximum-entropy (maxent) classifier which uses  -gram
substring features. Finally, in section 4 we add additional
features to the maxent model, and chain these models
into a conditional markov model (CMM), as used for tag-
ging (Ratnaparkhi, 1996) or earlier NER work (Borth-
wick, 1999).
2 A Character-Level HMM
Figure 1 shows a graphical model representation of our
character-level HMM. Characters are emitted one at a
time, and there is one state per character. Each state?s
identity depends only on the previous state. Each char-
acter?s identity depends on both the current state and on
the previous 	 characters. In addition to this HMM
view, it may also be convenient to think of the local emis-
sion models as type-conditional  -gram models. Indeed,
the character emission model in this section is directly
based on the  -gram proper-name classification engine
described in (Smarr and Manning, 2002). The primary
addition is the state-transition chaining, which allows the
model to do segmentation as well as classification.
When using character-level models for word-evaluated
tasks, one would not want multiple characters inside a
single word to receive different labels. This can be
avoided in two ways: by explicitly locking state tran-
sitions inside words, or by careful choice of transition
topology. In our current implementation, we do the latter.
Each state is a pair 
 where  is an entity type (such
as PERSON, and including an other type) and  indicates
the length of time the system has been in state  . There-
fore, a state like (PERSON, 2) indicates the second letter
inside a person phrase. The final letter of a phrase is a fol-
lowing space (we insert one if there is none) and the state
is a special final state like (PERSON, F). Additionally,
once  reaches our  -gram history order, it stays there.
We then use empirical, unsmoothed estimates for state-
Description ALL LOC MISC ORG PER
Official Baseline 71.2 80.5 83.5 66.4 55.2
Word-level HMM 74.5 79.5 69.7 67.5 77.6
Char-level, no conx 82.2 86.1 82.2 73.4 84.6
Char-level, context 83.2 86.9 83.0 75.1 85.6
Table 1: HMM F  performance, English development set.

s s s
e
s
cccc
Figure 1: A character-level HMM. The   nodes are char-
acter observations and  nodes are entity types.
state transitions. This annotation and estimation enforces
consistent labellings in practice. For example, (PERSON,
2) can only transition to the next state (PERSON, 3) or the
final state (PERSON, F). Final states can only transition
to beginning states, like (other, 1).
For emissions, we must estimate a quantity of
the form 


   
	

 
 





, for example,



 ffMax-Margin Parsing
Ben Taskar
Computer Science Dept.
Stanford University
btaskar@cs.stanford.edu
Dan Klein
Computer Science Dept.
Stanford University
klein@cs.stanford.edu
Michael Collins
CS and AI Lab
MIT
mcollins@csail.mit.edu
Daphne Koller
Computer Science Dept.
Stanford University
koller@cs.stanford.edu
Christopher Manning
Computer Science Dept.
Stanford University
manning@cs.stanford.edu
Abstract
We present a novel discriminative approach to parsing
inspired by the large-margin criterion underlying sup-
port vector machines. Our formulation uses a factor-
ization analogous to the standard dynamic programs for
parsing. In particular, it allows one to efficiently learn
a model which discriminates among the entire space of
parse trees, as opposed to reranking the top few candi-
dates. Our models can condition on arbitrary features of
input sentences, thus incorporating an important kind of
lexical information without the added algorithmic com-
plexity of modeling headedness. We provide an efficient
algorithm for learning such models and show experimen-
tal evidence of the model?s improved performance over
a natural baseline model and a lexicalized probabilistic
context-free grammar.
1 Introduction
Recent work has shown that discriminative
techniques frequently achieve classification ac-
curacy that is superior to generative techniques,
over a wide range of tasks. The empirical utility
of models such as logistic regression and sup-
port vector machines (SVMs) in flat classifica-
tion tasks like text categorization, word-sense
disambiguation, and relevance routing has been
repeatedly demonstrated. For sequence tasks
like part-of-speech tagging or named-entity ex-
traction, recent top-performing systems have
also generally been based on discriminative se-
quence models, like conditional Markov mod-
els (Toutanova et al, 2003) or conditional ran-
dom fields (Lafferty et al, 2001).
A number of recent papers have consid-
ered discriminative approaches for natural lan-
guage parsing (Johnson et al, 1999; Collins,
2000; Johnson, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Clark and Cur-
ran, 2004; Kaplan et al, 2004; Collins, 2004).
Broadly speaking, these approaches fall into two
categories, reranking and dynamic programming
approaches. In reranking methods (Johnson
et al, 1999; Collins, 2000; Shen et al, 2003),
an initial parser is used to generate a number
of candidate parses. A discriminative model
is then used to choose between these candi-
dates. In dynamic programming methods, a
large number of candidate parse trees are repre-
sented compactly in a parse tree forest or chart.
Given sufficiently ?local? features, the decod-
ing and parameter estimation problems can be
solved using dynamic programming algorithms.
For example, (Johnson, 2001; Geman and John-
son, 2002; Miyao and Tsujii, 2002; Clark and
Curran, 2004; Kaplan et al, 2004) describe ap-
proaches based on conditional log-linear (max-
imum entropy) models, where variants of the
inside-outside algorithm can be used to effi-
ciently calculate gradients of the log-likelihood
function, despite the exponential number of
trees represented by the parse forest.
In this paper, we describe a dynamic pro-
gramming approach to discriminative parsing
that is an alternative to maximum entropy
estimation. Our method extends the max-
margin approach of Taskar et al (2003) to
the case of context-free grammars. The present
method has several compelling advantages. Un-
like reranking methods, which consider only
a pre-pruned selection of ?good? parses, our
method is an end-to-end discriminative model
over the full space of parses. This distinction
can be very significant, as the set of n-best
parses often does not contain the true parse. For
example, in the work of Collins (2000), 41% of
the correct parses were not in the candidate pool
of ?30-best parses. Unlike previous dynamic
programming approaches, which were based on
maximum entropy estimation, our method in-
corporates an articulated loss function which
penalizes larger tree discrepancies more severely
than smaller ones.1
Moreover, like perceptron-based learning, it
requires only the calculation of Viterbi trees,
rather than expectations over all trees (for ex-
ample using the inside-outside algorithm). In
practice, it converges in many fewer iterations
than CRF-like approaches. For example, while
our approach generally converged in 20-30 iter-
ations, Clark and Curran (2004) report exper-
iments involving 479 iterations of training for
one model, and 1550 iterations for another.
The primary contribution of this paper is the
extension of the max-margin approach of Taskar
et al (2003) to context free grammars. We
show that this framework allows high-accuracy
parsing in cubic time by exploiting novel kinds
of lexical information.
2 Discriminative Parsing
In the discriminative parsing task, we want to
learn a function f : X ? Y, where X is a set
of sentences, and Y is a set of valid parse trees
according to a fixed grammar G. G maps an
input x ? X to a set of candidate parses G(x) ?
Y.2
We assume a loss function L : X ? Y ?
Y ? R+. The function L(x, y, y?) measures the
penalty for proposing the parse y? for x when y
is the true parse. This penalty may be defined,
for example, as the number of labeled spans on
which the two trees do not agree. In general we
assume that L(x, y, y?) = 0 for y = y?. Given
labeled training examples (xi, yi) for i = 1 . . . n,
we seek a function f with small expected loss
on unseen sentences.
The functions we consider take the following
linear discriminant form:
fw(x) = arg max
y?G(x)
?w,?(x, y)?,
1This articulated loss is supported by empirical suc-
cess and theoretical generalization bound in Taskar et al
(2003).
2For all x, we assume here that G(x) is finite. The
space of parse trees over many grammars is naturally in-
finite, but can be made finite if we disallow unary chains
and empty productions.
where ??, ?? denotes the vector inner product,
w ? Rd and ? is a feature-vector representation
of a parse tree ? : X ? Y ? Rd (see examples
below).3
Note that this class of functions includes
Viterbi PCFG parsers, where the feature-vector
consists of the counts of the productions used
in the parse, and the parameters w are the log-
probabilities of those productions.
2.1 Probabilistic Estimation
The traditional method of estimating the pa-
rameters of PCFGs assumes a generative gram-
mar that defines P (x, y) and maximizes the
joint log-likelihood ?i log P (xi, yi) (with some
regularization). A alternative probabilistic
approach is to estimate the parameters dis-
criminatively by maximizing conditional log-
likelihood. For example, the maximum entropy
approach (Johnson, 2001) defines a conditional
log-linear model:
Pw(y | x) =
1
Zw(x)
exp{?w,?(x, y)?},
where Zw(x) =
?
y?G(x) exp{?w,?(x, y)?}, and
maximizes the conditional log-likelihood of the
sample, ?i log P (yi | xi), (with some regular-
ization).
2.2 Max-Margin Estimation
In this paper, we advocate a different estima-
tion criterion, inspired by the max-margin prin-
ciple of SVMs. Max-margin estimation has been
used for parse reranking (Collins, 2000). Re-
cently, it has also been extended to graphical
models (Taskar et al, 2003; Altun et al, 2003)
and shown to outperform the standard max-
likelihood methods. The main idea is to forego
the probabilistic interpretation, and directly en-
sure that
yi = arg max
y?G(xi)
?w,?(xi, y)?,
for all i in the training data. We define the
margin of the parameters w on the example i
and parse y as the difference in value between
the true parse yi and y:
?w,?(xi, yi)? ? ?w,?(xi, y)? = ?w,?i,yi ??i,y?,
3Note that in the case that two members y1 and y2
have the same tied value for ?w,?(x, y)?, we assume that
there is some fixed, deterministic way for breaking ties.
For example, one approach would be to assume some
default ordering on the members of Y.
where ?i,y = ?(xi, y), and ?i,yi = ?(xi, yi). In-
tuitively, the size of the margin quantifies the
confidence in rejecting the mistaken parse y us-
ing the function fw(x), modulo the scale of the
parameters ||w||. We would like this rejection
confidence to be larger when the mistake y is
more severe, i.e. L(xi, yi, y) is large. We can ex-
press this desideratum as an optimization prob-
lem:
max ? (1)
s.t. ?w,?i,yi ? ?i,y? ? ?Li,y ?y ? G(xi);
||w||2 ? 1,
where Li,y = L(xi, yi, y). This quadratic pro-
gram aims to separate each y ? G(xi) from
the target parse yi by a margin that is propor-
tional to the loss L(xi, yi, y). After a standard
transformation, in which maximizing the mar-
gin is reformulated as minimizing the scale of
the weights (for a fixed margin of 1), we get the
following program:
min 12?w?
2 + C
?
i
?i (2)
s.t. ?w,?i,yi ? ?i,y? ? Li,y ? ?i ?y ? G(xi).
The addition of non-negative slack variables ?i
allows one to increase the global margin by pay-
ing a local penalty on some outlying examples.
The constant C dictates the desired trade-off
between margin size and outliers. Note that this
formulation has an exponential number of con-
straints, one for each possible parse y for each
sentence i. We address this issue in section 4.
2.3 The Max-Margin Dual
In SVMs, the optimization problem is solved by
working with the dual of a quadratic program
analogous to Eq. 2. For our problem, just as for
SVMs, the dual has important computational
advantages, including the ?kernel trick,? which
allows the efficient use of high-dimensional fea-
tures spaces endowed with efficient dot products
(Cristianini and Shawe-Taylor, 2000). More-
over, the dual view plays a crucial role in cir-
cumventing the exponential size of the primal
problem.
In Eq. 2, there is a constraint for each mistake
y one might make on each example i, which rules
out that mistake. For each mistake-exclusion
constraint, the dual contains a variable ?i,y. In-
tuitively, the magnitude of ?i,y is proportional
to the attention we must pay to that mistake in
order not to make it.
The dual of Eq. 2 (after adding additional
variables ?i,yi and renormalizing by C) is given
by:
max C
?
i,y
?i,yLi,y ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,y
(Ii,y ? ?i,y)?i,y
?
?
?
?
?
?
?
?
?
?
?
?
2
s.t.
?
y
?i,y = 1, ?i; ?i,y ? 0, ?i, y, (3)
where Ii,y = I(xi, yi, y) indicates whether y is
the true parse yi. Given the dual solution ??,
the solution to the primal problem w? is sim-
ply a weighted linear combination of the feature
vectors of the correct parse and mistaken parses:
w? = C
?
i,y
(Ii,y ? ??i,y)?i,y.
This is the precise sense in which mistakes with
large ? contribute more strongly to the model.
3 Factored Models
There is a major problem with both the pri-
mal and the dual formulations above: since each
potential mistake must be ruled out, the num-
ber of variables or constraints is proportional to
|G(x)|, the number of possible parse trees. Even
in grammars without unary chains or empty el-
ements, the number of parses is generally ex-
ponential in the length of the sentence, so we
cannot expect to solve the above problem with-
out any assumptions about the feature-vector
representation ? and loss function L.
For that matter, for arbitrary representa-
tions, to find the best parse given a weight vec-
tor, we would have no choice but to enumerate
all trees and score them. However, our gram-
mars and representations are generally struc-
tured to enable efficient inference. For exam-
ple, we usually assign scores to local parts of
the parse such as PCFG productions. Such
factored models have shared substructure prop-
erties which permit dynamic programming de-
compositions. In this section, we describe how
this kind of decomposition can be done over the
dual ? distributions. The idea of this decom-
position has previously been used for sequences
and other Markov random fields in Taskar et
al. (2003), but the present extension to CFGs
is novel.
For clarity of presentation, we restrict the
grammar to be in Chomsky normal form (CNF),
where all rules in the grammar are of the form
?A ? B C? or ?A ? a?, where A,B and C are
SNP
DT
The
NN
screen
VP
VBD
was
NP
NP
DT
a
NN
sea
PP
IN
of
NP
NN
red
0
1
2
3
4
5
6
0 1 2 3 4 5 6 7
DT
NN
VBD
DT
NN
IN
NN
NP
NP
PP
VP
S
NP
r = ?NP, 3, 5?
q = ?S ? NP VP, 0, 2, 7?
(a) (b)
Figure 1: Two representations of a binary parse tree: (a) nested tree structure, and (b) grid of labeled spans.
non-terminal symbols, and a is some terminal
symbol. For example figure 1(a) shows a tree
in this form.
We will represent each parse as a set of two
types of parts. Parts of the first type are sin-
gle constituent tuples ?A, s, e, i?, consisting of
a non-terminal A, start-point s and end-point
e, and sentence i, such as r in figure 1(b). In
this representation, indices s and e refer to po-
sitions between words, rather than to words
themselves. These parts correspond to the tra-
ditional notion of an edge in a tabular parser.
Parts of the second type consist of CF-rule-
tuples ?A ? B C, s,m, e, i?. The tuple specifies
a particular rule A ? B C, and its position,
including split point m, within the sentence i,
such as q in figure 1(b), and corresponds to the
traditional notion of a traversal in a tabular
parser. Note that parts for a basic PCFG model
are not just rewrites (which can occur multiple
times), but rather anchored items.
Formally, we assume some countable set of
parts, R. We also assume a function R which
maps each object (x, y) ? X ? Y to a finite
subset of R. Thus R(x, y) is the set of parts be-
longing to a particular parse. Equivalently, the
function R(x, y) maps a derivation y to the set
of parts which it includes. Because all rules are
in binary-branching form, |R(x, y)| is constant
across different derivations y for the same input
sentence x. We assume that the feature vector
for a sentence and parse tree (x, y) decomposes
into a sum of the feature vectors for its parts:
?(x, y) =
?
r?R(x,y)
?(x, r).
In CFGs, the function ?(x, r) can be any func-
tion mapping a rule production and its posi-
tion in the sentence x, to some feature vector
representation. For example, ? could include
features which identify the rule used in the pro-
duction, or features which track the rule iden-
tity together with features of the words at po-
sitions s,m, e, and neighboring positions in the
sentence x.
In addition, we assume that the loss function
L(x, y, y?) also decomposes into a sum of local
loss functions l(x, y, r) over parts, as follows:
L(x, y, y?) =
?
r?R(x,y?)
l(x, y, r).
One approach would be to define l(x, y, r) to
be 0 only if the non-terminal A spans words
s . . . e in the derivation y and 1 otherwise. This
would lead to L(x, y, y?) tracking the number of
?constituent errors? in y?, where a constituent is
a tuple such as ?A, s, e, i?. Another, more strict
definition would be to define l(x, y, r) to be 0
if r of the type ?A ? B C, s,m, e, i? is in the
derivation y and 1 otherwise. This definition
would lead to L(x, y, y?) being the number of CF-
rule-tuples in y? which are not seen in y.4
Finally, we define indicator variables I(x, y, r)
which are 1 if r ? R(x, y), 0 otherwise. We
also define sets R(xi) = ?y?G(xi)R(xi, y) for the
training examples i = 1 . . . n. Thus, R(xi) is
the set of parts that is seen in at least one of
the objects {(xi, y) : y ? G(xi)}.
4 Factored Dual
The dual in Eq. 3 involves variables ?i,y for
all i = 1 . . . n, y ? G(xi), and the objec-
tive is quadratic in these ? variables. In addi-
tion, it turns out that the set of dual variables
?i = {?i,y : y ? G(xi)} for each example i is
constrained to be non-negative and sum to 1.
It is interesting that, while the parameters w
lose their probabilistic interpretation, the dual
variables ?i for each sentence actually form a
kind of probability distribution. Furthermore,
the objective can be expressed in terms of ex-
pectations with respect to these distributions:
C
?
i
E?i [Li,y]?
1
2
?
?
?
?
?
?
?
?
?
?
C
?
i
?i,yi ?E?i [?i,y]
?
?
?
?
?
?
?
?
?
?
2
.
We now consider how to efficiently solve
the max-margin optimization problem for a
factored model. As shown in Taskar et al
(2003), the dual in Eq. 3 can be reframed using
?marginal? terms. We will also find it useful to
consider this alternative formulation of the dual.
Given dual variables ?, we define the marginals
?i,r(?) for all i, r, as follows:
?i,r(?i) =
?
y
?i,yI(xi, y, r) = E?i [I(xi, y, r)] .
Since the dual variables ?i form probability dis-
tributions over parse trees for each sentence i,
the marginals ?i,r(?i) represent the proportion
of parses that would contain part r if they were
drawn from a distribution ?i. Note that the
number of such marginal terms is the number
of parts, which is polynomial in the length of
the sentence.
Now consider the dual objective Q(?) in
Eq. 3. It can be shown that the original ob-
jective Q(?) can be expressed in terms of these
4The constituent loss function does not exactly cor-
respond to the standard scoring metrics, such as F1 or
crossing brackets, but shares the sensitivity to the num-
ber of differences between trees. We have not thoroughly
investigated the exact interplay between the various loss
choices and the various parsing metrics. We used the
constituent loss in our experiments.
marginals as Qm(?(?)), where ?(?) is the vector
with components ?i,r(?i), and Qm(?) is defined
as:
C
?
i,r?R(xi)
?i,rli,r ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,r?R(xi)
(Ii,r ? ?i,r)?i,r
?
?
?
?
?
?
?
?
?
?
?
?
2
where li,r = l(xi, yi, r), ?i,r = ?(xi, r) and Ii,r =
I(xi, yi, r).
This follows from substituting the factored
definitions of the feature representation ? and
loss function L together with definition of
marginals.
Having expressed the objective in terms of a
polynomial number of variables, we now turn to
the constraints on these variables. The feasible
set for ? is
? = {? : ?i,y ? 0, ?i, y
?
y
?i,y = 1, ?i}.
Now let ?m be the space of marginal vectors
which are feasible:
?m = {? : ?? ? ? s.t. ? = ?(?)}.
Then our original optimization problem can be
reframed as max???m Qm(?).
Fortunately, in case of PCFGs, the domain
?m can be described compactly with a polyno-
mial number of linear constraints. Essentially,
we need to enforce the condition that the ex-
pected proportions of parses having particular
parts should be consistent with each other. Our
marginals track constituent parts ?A, s, e, i? and
CF-rule-tuple parts ?A ? B C, s,m, e, i? The
consistency constraints are precisely the inside-
outside probability relations:
?i,A,s,e =
?
B,C
s<m<e
?i,A?B C,s,m,e
and
?i,A,s,e =
?
B,C
e<m?ni
?i,B?AC +
?
B,C
0?m<s
?i,B?CA
where ni is the length of the sentence. In ad-
dition, we must ensure non-negativity and nor-
malization to 1:
?i,r ? 0;
?
A
?i,A,0,ni = 1.
The number of variables in our factored dual
for CFGs is cubic in the length of the sentence,
Model P R F1
GENERATIVE 87.70 88.06 87.88
BASIC 87.51 88.44 87.98
LEXICAL 88.15 88.62 88.39
LEXICAL+AUX 89.74 90.22 89.98
Figure 2: Development set results of the various
models when trained and tested on Penn treebank
sentences of length ? 15.
Model P R F1
GENERATIVE 88.25 87.73 87.99
BASIC 88.08 88.31 88.20
LEXICAL 88.55 88.34 88.44
LEXICAL+AUX 89.14 89.10 89.12
COLLINS 99 89.18 88.20 88.69
Figure 3: Test set results of the various models when
trained and tested on Penn treebank sentences of
length ? 15.
while the number of constraints is quadratic.
This polynomial size formulation should be con-
trasted with the earlier formulation in Collins
(2004), which has an exponential number of
constraints.
5 Factored SMO
We have reduced the problem to a polynomial
size QP, which, in principle, can be solved us-
ing standard QP toolkits. However, although
the number of variables and constraints in the
factored dual is polynomial in the size of the
data, the number of coefficients in the quadratic
term in the objective is very large: quadratic in
the number of sentences and dependent on the
sixth power of sentence length. Hence, in our
experiments we use an online coordinate descent
method analogous to the sequential minimal op-
timization (SMO) used for SVMs (Platt, 1999)
and adapted to structured max-margin estima-
tion in Taskar et al (2003).
We omit the details of the structured SMO
procedure, but the important fact about this
kind of training is that, similar to the basic per-
ceptron approach, it only requires picking up
sentences one at a time, checking what the best
parse is according to the current primal and
dual weights, and adjusting the weights.
6 Results
We used the Penn English Treebank for all of
our experiments. We report results here for
each model and setting trained and tested on
only the sentences of length ? 15 words. Aside
from the length restriction, we used the stan-
dard splits: sections 2-21 for training (9753 sen-
tences), 22 for development (603 sentences), and
23 for final testing (421 sentences).
As a baseline, we trained a CNF transforma-
tion of the unlexicalized model of Klein and
Manning (2003) on this data. The resulting
grammar had 3975 non-terminal symbols and
contained two kinds of productions: binary non-
terminal rewrites and tag-word rewrites.5 The
scores for the binary rewrites were estimated us-
ing unsmoothed relative frequency estimators.
The tagging rewrites were estimated with a
smoothed model of P (w|t), also using the model
from Klein and Manning (2003). Figure 3 shows
the performance of this model (generative):
87.99 F1 on the test set.
For the basic max-margin model, we used
exactly the same set of allowed rewrites (and
therefore the same set of candidate parses) as in
the generative case, but estimated their weights
according to the discriminative method of sec-
tion 4. Tag-word production weights were fixed
to be the log of the generative P (w|t) model.
That is, the only change between genera-
tive and basic is the use of the discriminative
maximum-margin criterion in place of the gen-
erative maximum likelihood one. This change
alone results in a small improvement (88.20 vs.
87.99 F1).
On top of the basic model, we first added lex-
ical features of each span; this gave a lexical
model. For a span ?s, e? of a sentence x, the
base lexical features were:
? xs, the first word in the span
? xs?1, the preceding adjacent word
? xe?1, the last word in the span
? xe, the following adjacent word
? ?xs?1, xs?
? ?xe?1, xe?
? xs+1 for spans of length 3
These base features were conjoined with the
span length for spans of length 3 and below,
since short spans have highly distinct behaviors
(see the examples below). The features are lex-
ical in the sense than they allow specific words
5Unary rewrites were compiled into a single com-
pound symbol, so for example a subject-gapped sentence
would have label like s+vp. These symbols were ex-
panded back into their source unary chain before parses
were evaluated.
and word pairs to influence the parse scores, but
are distinct from traditional lexical features in
several ways. First, there is no notion of head-
word here, nor is there any modeling of word-to-
word attachment. Rather, these features pick
up on lexical trends in constituent boundaries,
for example the trend that in the sentence The
screen was a sea of red., the (length 2) span
between the word was and the word of is un-
likely to be a constituent. These non-head lex-
ical features capture a potentially very differ-
ent source of constraint on tree structures than
head-argument pairs, one having to do more
with linear syntactic preferences than lexical
selection. Regardless of the relative merit of
the two kinds of information, one clear advan-
tage of the present approach is that inference in
the resulting model remains cubic, since the dy-
namic program need not track items with distin-
guished headwords. With the addition of these
features, the accuracy jumped past the genera-
tive baseline, to 88.44.
As a concrete (and particularly clean) exam-
ple of how these features can sway a decision,
consider the sentence The Egyptian president
said he would visit Libya today to resume the
talks. The generative model incorrectly consid-
ers Libya today to be a base np. However, this
analysis is counter to the trend of today being a
one-word constituent. Two features relevant to
this trend are: (constituent ? first-word =
today ? length = 1) and (constituent ? last-
word = today ? length = 1). These features rep-
resent the preference of the word today for being
the first and and last word in constituent spans
of length 1.6 In the lexical model, however,
these features have quite large positive weights:
0.62 each. As a result, this model makes this
parse decision correctly.
Another kind of feature that can usefully be
incorporated into the classification process is
the output of other, auxiliary classifiers. For
this kind of feature, one must take care that its
reliability on the training not be vastly greater
than its reliability on the test set. Otherwise,
its weight will be artificially (and detrimentally)
high. To ensure that such features are as noisy
on the training data as the test data, we split
the training into two folds. We then trained the
auxiliary classifiers in jacknife fashion on each
6In this length 1 case, these are the same feature.
Note also that the features are conjoined with only one
generic label class ?constituent? rather than specific con-
stituent types.
fold, and using their predictions as features on
the other fold. The auxiliary classifiers were
then retrained on the entire training set, and
their predictions used as features on the devel-
opment and test sets.
We used two such auxiliary classifiers, giving
a prediction feature for each span (these classi-
fiers predicted only the presence or absence of a
bracket over that span, not bracket labels). The
first feature was the prediction of the genera-
tive baseline; this feature added little informa-
tion, but made the learning phase faster. The
second feature was the output of a flat classi-
fier which was trained to predict whether sin-
gle spans, in isolation, were constituents or not,
based on a bundle of features including the list
above, but also the following: the preceding,
first, last, and following tag in the span, pairs
of tags such as preceding-first, last-following,
preceding-following, first-last, and the entire tag
sequence.
Tag features on the test sets were taken from
a pretagging of the sentence by the tagger de-
scribed in Toutanova et al (2003). While the
flat classifier alone was quite poor (P 78.77 /
R 63.94 / F1 70.58), the resulting max-margin
model (lexical+aux) scored 89.12 F1. To sit-
uate these numbers with respect to other mod-
els, the parser in Collins (1999), which is genera-
tive, lexicalized, and intricately smoothed scores
88.69 over the same train/test configuration.
It is worth considering the cost of this kind of
method. At training time, discriminative meth-
ods are inherently expensive, since they all in-
volve iteratively checking current model perfor-
mance on the training set, which means parsing
the training set (usually many times). In our
experiments, 10-20 iterations were generally re-
quired for convergence (except the basic model,
which took about 100 iterations.) There are
several nice aspects of the approach described
here. First, it is driven by the repeated extrac-
tion, over the training examples, of incorrect
parses which the model currently prefers over
the true parses. The procedure that provides
these parses need not sum over all parses, nor
even necessarily find the Viterbi parses, to func-
tion. This allows a range of optimizations not
possible for CRF-like approaches which must
extract feature expectations from the entire set
of parses.7 Nonetheless, generative approaches
7One tradeoff is that this approach is more inherently
sequential and harder to parallelize.
are vastly cheaper to train, since they must only
collect counts from the training set.
On the other hand, the max-margin approach
does have the potential to incorporate many
new kinds of features over the input, and the
current feature set alows limited lexicalization
in cubic time, unlike other lexicalized models
(including the Collins model which it outper-
forms in the present limited experiments).
7 Conclusion
We have presented a maximum-margin ap-
proach to parsing, which allows a discriminative
SVM-like objective to be applied to the parsing
problem. Our framework permits the use of a
rich variety of input features, while still decom-
posing in a way that exploits the shared sub-
structure of parse trees in the standard way. On
a test set of ? 15 word sentences, the feature-
rich model outperforms both its own natural
generative baseline and the Collins parser on
F1. While like most discriminative models it is
compute-intensive to train, it allows fast pars-
ing, remaining cubic despite the incorporation
of lexical features. This trade-off between the
complexity, accuracy and efficiency of a parsing
model is an important area of future research.
Acknowledgements
This work was supported in part by the Depart-
ment of the Interior/DARPA under contract
number NBCHD030010, a Microsoft Graduate
Fellowship to the second author, and National
Science Foundation grant 0347631 to the third
author.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann.
2003. Hidden markov support vector ma-
chines. In Proc. ICML.
S. Clark and J. R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In
Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguis-
tics (ACL ?04).
M. Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for
natural language parsing. In ICML 17, pages
175?182.
M. Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice
of distribution-free methods. In Harry Bunt,
John Carroll, and Giorgio Satta, editors, New
Developments in Parsing Technology. Kluwer.
N. Cristianini and J. Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cam-
bridge University Press.
S. Geman and M. Johnson. 2002. Dynamic
programming for parsing and estimation of
stochastic unification-based grammars. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceed-
ings of ACL 1999.
M. Johnson. 2001. Joint and conditional es-
timation of tagging and parsing models. In
ACL 39.
R. Kaplan, S. Riezler, T. King, J. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of HLT-NAACL?04).
D. Klein and C. D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423?
430.
J. Lafferty, A. McCallum, and F. Pereira.
2001. Conditional random fields: Probabi-
listic models for segmenting and labeling se-
quence data. In ICML.
Y. Miyao and J. Tsujii. 2002. Maximum
entropy estimation for feature forests. In
Proceedings of Human Language Technology
Conference (HLT 2002).
J. Platt. 1999. Using sparseness and analytic
QP to speed training of support vector ma-
chines. In NIPS.
L. Shen, A. Sarkar, and A. K. Joshi. 2003. Us-
ing ltag based features in parse reranking. In
Proc. EMNLP.
B. Taskar, C. Guestrin, and D. Koller. 2003.
Max margin Markov networks. In NIPS.
K. Toutanova, D. Klein, C. D. Manning, and
Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL 3, pages 252?259.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410?419,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Game-Theoretic Approach to Generating Spatial Descriptions
Dave Golland
UC Berkeley
Berkeley, CA 94720
dsg@cs.berkeley.edu
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
Language is sensitive to both semantic and
pragmatic effects. To capture both effects,
we model language use as a cooperative game
between two players: a speaker, who gener-
ates an utterance, and a listener, who responds
with an action. Specifically, we consider the
task of generating spatial references to ob-
jects, wherein the listener must accurately
identify an object described by the speaker.
We show that a speaker model that acts op-
timally with respect to an explicit, embedded
listener model substantially outperforms one
that is trained to directly generate spatial de-
scriptions.
1 Introduction
Language is about successful communication be-
tween a speaker and a listener. For example, if the
goal is to reference the target object O1 in Figure 1,
a speaker might choose one of the following two ut-
terances:
(a) right of O2 (b) on O3
Although both utterances are semantically correct,
(a) is ambiguous between O1 and O3, whereas (b)
unambiguously identifies O1 as the target object,
and should therefore be preferred over (a). In this
paper, we present a game-theoretic model that cap-
tures this communication-oriented aspect of lan-
guage interpretation and generation.
Successful communication can be broken down
into semantics and pragmatics. Most computational
Figure 1: An example of a 3D model of a room. The
speaker?s goal is to reference the target object O1 by de-
scribing its spatial relationship to other object(s). The
listener?s goal is to guess the object given the speaker?s
description.
work on interpreting language focuses on compo-
sitional semantics (Zettlemoyer and Collins, 2005;
Wong and Mooney, 2007; Piantadosi et al, 2008),
which is concerned with verifying the truth of a sen-
tence. However, what is missing from this truth-
oriented view is the pragmatic aspect of language?
that language is used to accomplish an end goal, as
exemplified by speech acts (Austin, 1962). Indeed,
although both utterances (a) and (b) are semantically
valid, only (b) is pragmatically felicitous: (a) is am-
biguous and therefore violates the Gricean maxim
of manner (Grice, 1975). To capture this maxim, we
develop a model of pragmatics based on game the-
ory, in the spirit of Ja?ger (2008) but extended to the
stochastic setting. We show that Gricean maxims
410
fall out naturally as consequences of the model.
An effective way to empirically explore the prag-
matic aspects of language is to work in the grounded
setting, where the basic idea is to map language to
some representation of the non-linguistic world (Yu
and Ballard, 2004; Feldman and Narayanan, 2004;
Fleischman and Roy, 2007; Chen and Mooney,
2008; Frank et al, 2009; Liang et al, 2009). Along
similar lines, past work has also focused on inter-
preting natural language instructions (Branavan et
al., 2009; Eisenstein et al, 2009; Kollar et al, 2010),
which takes into account the goal of the communi-
cation. This work differs from ours in that it does
not clarify the formal relationship between pragmat-
ics and the interpretation task. Pragmatics has also
been studied in the context of dialog systems. For
instance, DeVault and Stone (2007) present a model
of collaborative language between multiple agents
that takes into account contextual ambiguities.
We present our pragmatic model in a grounded
setting where a speaker must describe a target object
to a listener via spatial description (such as in the
example given above). Though we use some of the
techniques from work on the semantics of spatial de-
scriptions (Regier and Carlson, 2001; Gorniak and
Roy, 2004; Tellex and Roy, 2009), we empirically
demonstrate that having a model of pragmatics en-
ables more successful communication.
2 Language as a Game
To model Grice?s cooperative principle (Grice,
1975), we formulate the interaction between a
speaker S and a listener L as a cooperative game, that
is, one in which S and L share the same utility func-
tion. For simplicity, we focus on the production and
interpretation of single utterances, where the speaker
and listener have access to a shared context. To sim-
plify notation, we suppress writing the dependence
on the context.
The Communication Game
1. In order to communicate a target o to L, S pro-
duces an utterance w chosen according to a
strategy pS(w | o).
2. L interprets w and responds with a guess g ac-
cording to a strategy pL(g | w).
3. S and L collectively get a utility of U(o, g).
o w g
U
speaker listener
ps(w | o) pl(g | w)
target utterance guess
utility
Figure 2: Diagram representing the communication
game. A target, o, is given to the speaker that generates
an utterance w. Based on this utterance, the listener gen-
erates a guess g. If o = g, then both the listener and
speaker get a utility of 1, otherwise they get a utility of 0.
This communication game is described graphi-
on O3
1
near O3
0
right of O2
0
Figure 3: Three instances of the communication game on
the scenario in Figure 1. For each instance, the target o,
utterancew, guess g, and the resulting utilityU are shown
in their respective positions. A utility of 1 is awarded only
when the guess matches the target.
cally in Figure 2. Figure 3 shows several instances of
the communication game being played for the sce-
nario in Figure 1.
Grice?s maxim of manner encourages utterances
to be unambiguous, which motivates the following
utility, which we call (communicative) success:
U(o, g)
def
= I[o = g], (1)
where the indicator function I[o = g] is 1 if o =
g and 0 otherwise. Hence, a utility-maximizing
speaker will attempt to produce unambiguous utter-
ances because they increase the probability that the
listener will correctly guess the target.
411
Given a speaker strategy pS(w | o), a listener
strategy pL(g | w), and a prior distribution over tar-
gets p(o), the expected utility obtained by S and L is
as follows:
EU(S, L) =
?
o,w,g
p(o)pS(w|o)pL(g|w)U(o, g)
=
?
o,w
p(o)pS(w|o)pL(o|w). (2)
3 From Reflex Speaker to Rational
Speaker
Having formalized the language game, we now ex-
plore various speaker and listener strategies. First,
let us consider literal strategies. A literal speaker
(denoted S:LITERAL) chooses uniformly from the
set of utterances consistent with a target object, i.e.,
the ones which are semantically valid;1 a literal lis-
tener (denoted L:LITERAL) guesses an object con-
sistent with the utterance uniformly at random.
In the running example (Figure 1), where the tar-
get object is O1, there are two semantically valid ut-
terances:
(a) right of O2 (b) on O3
S:LITERAL selects (a) or (b) each with probability
1
2 . If S:LITERAL chooses (a), L:LITERAL will guess
the target object O1 correctly with probability 12 ; if
S:LITERAL chooses (b), L:LITERAL will guess cor-
rectly with probability 1. Therefore, the expected
utility EU(S:LITERAL, L:LITERAL) = 34 .
We say S:LITERAL is an example of a reflex
speaker because it chooses an utterance without
taking the listener into account. A general reflex
speaker is depicted in Figure 4(a), where each edge
represents a potential utterance.
Suppose we now have a model of some listener
L. Motivated by game theory, we would optimize
the expected utility (2) given pL(g | w). We call
the resulting speaker S(L) the rational speaker with
respect to listener L. Solving for this strategy yields:
pS(L)(w | o) = I[w = w?], where
w? = argmax
w?
pL(o | w
?). (3)
1Semantic validity is approximated by a set of heuristic rules
(e.g. left is all positions with smaller x-coordinates).
S
w1
o w2
w3
S(L)
w1
o
L
g1
w2 g2
g3
w3
(a) Reflex speaker (b) Rational speaker
Figure 4: (a) A reflex speaker (S) directly selects an ut-
terance based only on the target object. Each edge rep-
resents a different choice of utterance. (b) A rational
speaker (S(L)) selects an utterance based on an embed-
ded model of the listener (L). Each edge in the first layer
represents a different choice the speaker can make, and
each edge in the second layer represents a response of the
listener.
Intuitively, S(L) chooses an utterance, w?, such that,
if listener L were to interpret w?, the probability of
L guessing the target would be maximized.2 The ra-
tional speaker is depicted in Figure 4(b), where, as
before, each edge at the first level represents a possi-
ble choice for the speaker, but there is now a second
layer representing the response of the listener.
To see how an embedded model of the listener
improves communication, again consider our run-
ning example in Figure 1. A speaker can describe
the target object O1 using either w1 = on O3 or
w2 = right of O2. Suppose the embedded listener
is L:LITERAL, which chooses uniformly from the
set of objects consistent with the given utterance.
In this scenario, pL:LITERAL(O1 | w1) = 1 because
w1 unambiguously describes the target object, but
pL:LITERAL(O1 | w2) = 12 . The rational speaker
S(L:LITERAL) would therefore choose w1, achiev-
ing a utility of 1, which is an improvement over the
reflex speaker S:LITERAL?s utility of 34 .
2If there are ties, any distribution over the utterances having
the same utility is optimal.
412
4 From Literal Speaker to Learned
Speaker
In the previous section, we showed that a literal
strategy, one that considers only semantically valid
choices, can be used to directly construct a reflex
speaker S:LITERAL or an embedded listener in a
rational speaker S(L:LITERAL). This section fo-
cuses on an orthogonal direction: improving literal
strategies with learning. Specifically, we construct
learned strategies from log-linear models trained on
human annotations. These learned strategies can
then be used to construct reflex and rational speaker
variants?S:LEARNED and S(L:LEARNED), respec-
tively.
4.1 Training a Log-Linear Speaker/Listener
We train the speaker, S:LEARNED, (similarly, lis-
tener, L:LEARNED) on training examples which
comprise the utterances produced by the human an-
notators (see Section 6.1 for details on how this
data was collected). Each example consists of a 3D
model of a room in a house that specifies the 3D po-
sitions of each object and the coordinates of a 3D
camera. When training the speaker, each example is
a pair (o, w), where o is the input target object and
w is the output utterance. When training the listener,
each example is (w, g), where w is the input utter-
ance and g is the output guessed object.
For now, an utterance w consists of two parts:
? A spatial preposition w.r (e.g., right of) from a
set of possible prepositions.3
? A reference object w.o (e.g., O3) from the set
of objects in the room.
We consider more complex utterances in Section 5.
Both S:LEARNED and L:LEARNED are
parametrized by log-linear models:
pS:LEARNED(w|o; ?S) ? exp{?
>
S ?(o, w)} (4)
pL:LEARNED(g|w; ?L) ? exp{?
>
L ?(g, w)} (5)
where ?(?, ?) is the feature vector (see below), ?S
and ?L are the parameter vectors for speaker and lis-
tener. Note that the speaker and listener use the same
3We chose 10 prepositions commonly used by people to de-
scribe objects in a preliminary data gathering experiment. This
list includes multi-word units, which function equivalently to
prepositions, such as left of.
set of features, but they have different parameters.
Furthermore, the first normalization sums over pos-
sible utterances w while the second normalization
sums over possible objects g in the scene. The two
parameter vectors are trained to optimize the log-
likelihood of the training data under the respective
models.
Features We now describe the features ?(o, w).
These features draw inspiration from Landau and
Jackendoff (1993) and Tellex and Roy (2009).
Each object o in the 3D scene is represented by
its bounding box, which is the smallest rectangular
prism containing o. The following are functions of
the camera, target (or guessed object) o, and the ref-
erence object w.o in the utterance. The full set of
features is obtained by conjoining these functions
with indicator functions of the form I[w.r = r],
where r ranges over the set of valid prepositions.
? Proximity functions measure the distance be-
tween o and w.o. This is implemented as the
minimum over all the pairwise Euclidean dis-
tances between the corners of the bounding
boxes. We also have indicator functions for
whether o is the closest object, among the top
5 closest objects, and among the top 10 closest
objects to w.o.
? Topological functions measure containment be-
tween o and w.o: vol(o ? w.o)/vol(o) and
vol(o ? w.o)/vol(w.o). To simplify volume
computation, we approximate each object by a
bounding box that is aligned with the camera
axes.
? Projection functions measure the relative posi-
tion of the bounding boxes with respect to one
another. Specifically, let v be the vector from
the center of w.o to the center of o. There is a
function for the projection of v onto each of the
axes defined by the camera orientation (see Fig-
ure 5). Additionally, there is a set of indicator
functions that capture the relative magnitude of
these projections. For example, there is a indi-
cator function denoting whether the projection
of v onto the camera?s x-axis is the largest of
all three projections.
413
Figure 5: The projection features are computed by pro-
jecting a vector v extending from the center of the ref-
erence object to the center of the target object onto the
camera axes fx and fy .
5 Handling Complex Utterances
So far, we have only considered speakers and lis-
teners that deal with utterances consisting of one
preposition and one reference object. We now ex-
tend these strategies to handle more complex utter-
ances. Specifically, we consider utterances that con-
form to the following grammar:4
[noun] N ? something | O1 | O2 | ? ? ?
[relation] R ? in front of | on | ? ? ?
[conjunction] NP ? N RP?
[relativization] RP ? R NP
This grammar captures two phenomena of lan-
guage use, conjunction and relativization.
? Conjunction is useful when one spatial relation
is insufficient to disambiguate the target object.
For example, in Figure 1, right of O2 could re-
fer to the vase or the table, but using the con-
junction right of O2 and on O3 narrows down
the target object to just the vase.
? The main purpose of relativization is to refer
to objects without a precise nominal descrip-
tor. With complex utterances, it is possible to
chain relative prepositional phrases, for exam-
ple, using on something right of O2 to refer to
the vase.
4Naturally, we disallow direct reference to the target object.
Given an utterancew, we define its complexity |w|
as the number of applications of the relativization
rule, RP ? R NP, used to produce w. We had only
considered utterances of complexity 1 in previous
sections.
5.1 Example Utterances
To illustrate the types of utterances available under
the grammar, again consider the scene in Figure 1.
Utterances of complexity 2 can be generated ei-
ther using the relativization rule exclusively, or both
the conjunction and relativization rules. The rela-
tivization rule can be used to generate the following
utterances:
? on something that is right of O2
? right of something that is left of O3
Applying the conjunction rule leads to the following
utterances:
? right of O2 and on O3
? right of O2 and under O1
? left of O1 and left of O3
Note that we inserted the words that is after each N
and the word and between every adjacent pair of RPs
generated via the conjunction rule. This is to help a
human listener interpret an utterance.
5.2 Extending the Rational Speaker
Suppose we have a rational speaker S(L) defined in
terms of an embedded listener L which operates over
utterances of complexity 1. We first extend L to in-
terpret arbitrary utterances of our grammar. The ra-
tional speaker (defined in (2)) automatically inherits
this extension.
Compositional semantics allows us to define the
interpretation of complex utterances in terms of sim-
pler ones. Specifically, each node in the parse tree
has a denotation, which is computed recursively
in terms of the node?s children via a set of sim-
ple rules. Usually, denotations are represented as
lambda-calculus functions, but for us, they will be
distributions over objects in the scene. As a base
case for interpreting utterances of complexity 1, we
can use either L:LITERAL or L:LEARNED (defined
in Sections 3 and 4).
414
Given a subtree w rooted at u ? {N, NP, RP}, we
define the denotation of w, JwK, to be a distribution
over the objects in the scene in which the utterance
was generated. The listener strategy pL(g|w) = JwK
is recursively as follows:
? If w is rooted at N with a single child x, then JwK
is the uniform distribution over N (x), the set of
objects consistent with the word x.
? If w is rooted at NP, we recursively compute the
distributions over objects g for each child tree,
multiply the probabilities, and renormalize (Hin-
ton, 1999).
? Ifw is rooted at RP with relation r, we recursively
compute the distribution over objects g? for the
child NP tree. We then appeal to the base case
to produce a distribution over objects g which are
related to g? via relation r.
This strategy is defined formally as follows:
pL(g | w) ?
?
?????
?????
I[g ? N (x)] w = (N x)
k?
j=1
pL(g | wj) w = (NP w1 . . . wk)
?
g?
pL(g | (r, g?))pL(g? | w?) w = (RP (R r)w?)
(6)
Figure 6 shows an example of this bottom-
up denotation computation for the utterance
on something right of O2 with respect to the scene
in Figure 1. The denotation starts with the lowest
NP node JO2K, which places all the mass on O2
in the scene. Moving up the tree, we compute
the denotation of the RP, Jright of O2K, using the
RP case of (6), which results in a distribution that
places equal mass on O1 and O3.5 The denotation
of the N node JsomethingK is a flat distribution over
all the objects in the scene. Continuing up the tree,
the denotation of the NP is computed by taking a
product of the object distributions, and turns out
to be exactly the same split distribution as its RP
child. Finally, the denotation at the root is computed
by applying the base case to on and the resulting
distribution from the previous step.
5It is worth mentioning that this split distribution between
O1 and O3 represents the ambiguity mentioned in Section 3
when discussing the shortcomings of S:LITERAL.
Figure 6: The listener model maps an utterance to a dis-
tribution over objects in the room. Each internal NP or RP
node is a distribution over objects in the room.
Generation So far, we have defined the listener
strategy pL(g | w). Given target o, the rational
speaker S(L) with respect to this listener needs to
compute argmaxw pL(o | w) as dictated by (3). This
maximization is performed by enumerating all utter-
ances of bounded complexity.
5.3 Modeling Listener Confusion
One shortcoming of the previous approach for ex-
tending a listener is that it falsely assumes that a lis-
tener can reliably interpret a simple utterance just as
well as it can a complex utterance.
We now describe a more realistic speaker which
is robust to listener confusion. Let ? ? [0, 1] be
a focus parameter which determines the confusion
level. Suppose we have a listener L. When presented
with an utterance w, for each application of the rela-
tivization rule, we have a 1?? probability of losing
focus. If we stay focused for the entire utterance
(with probability ?|w|), then we interpret the utter-
ance according to pL. Otherwise (with probability
1 ? ?|w|), we guess an object at random according
to prnd(g | w). We then use (3) to define the rational
speaker S(L) with respect the following ?confused
listener? strategy:
p?L(g | w) = ?
|w|pL(g | w) + (1 ? ?
|w|)prnd(g | w).
(7)
As ? ? 0, the confused listener is more likely to
make a random guess, and thus there is a stronger
penalty against using more complex utterances. As
415
? ? 1, the confused listener converges to pL and the
penalty for using complex utterances vanishes.
5.4 The Taboo Setting
Notice that the rational speaker as defined so far
does not make full use of our grammar. Specifi-
cally, the rational speaker will never use the ?wild-
card? noun something nor the relativization rule in
the grammar because an NP headed by the wildcard
something can always be replaced by the object ID
to obtain a higher utility. For instance, in Figure 6,
the NP spanning something right of O2 can be re-
placed by O3.
However, it is not realistic to assume that all ob-
jects can be referenced directly. To simulate scenar-
ios where some objects cannot be referenced directly
(and to fully exercise our grammar), we introduce
the taboo setting. In this setting, we remove from
the lexicon some fraction of the object IDs which are
closest to the target object. Since the tabooed objects
cannot be referenced directly, a speaker must resort
to use of the wildcard something and relativization.
For example, in Figure 7, we enable tabooing
around the target O1. This prevents the speaker from
referring directly to O3, so the speaker is forced to
describe O3 via the relativization rule, for example,
producing something right of O2.
Figure 7: With tabooing enabled around O1, O3 can no
longer be referred to directly (represented by an X).
6 Experiments
We now present our empirical results, showing that
rational speakers, who have embedded models of lis-
Figure 8: Mechanical Turk speaker task: Given the tar-
get object (e.g., O1), a human speaker must choose an
utterance to describe the object (e.g., right of O2).
teners, can communicate more successfully than re-
flex speakers, who do not.
6.1 Setup
We collected 43 scenes (rooms) from the Google
Sketchup 3D Warehouse, each containing an aver-
age of 22 objects (household items and pieces of fur-
niture arranged in a natural configuration). For each
object o in a scene, we create a scenario, which rep-
resents an instance of the communication game with
o as the target object. There are a total of 2,860 sce-
narios, which we split evenly into a training set (de-
noted TR) and a test set (denoted TS).
We created the following two Amazon Mechani-
cal Turk tasks, which enable humans to play the lan-
guage game on the scenarios:
Speaker Task In this task, human annotators play
the role of speakers in the language game. They are
prompted with a target object o and asked to each
produce an utterance w (by selecting a preposition
w.r from a dropdown list and clicking on a reference
objectw.o) that best informs a listener of the identity
of the target object.
For each training scenario o, we asked three
speakers to produce an utterancew. The three result-
ing (o, w) pairs are used to train the learned reflex
speaker (S:LITERAL). These pairs were also used to
train the learned reflex listener (L:LITERAL), where
the target o is treated as the guessed object. See Sec-
tion 4.1 for the details of the training procedure.
Listener Task In this task, human annotators play
the role of listeners. Given an utterance generated by
a speaker (human or not), the human listener must
416
O2
O1
O3
Question: What object is right of           ?O2
Figure 9: Mechanical Turk listener task: a human listener
is prompted with an utterance generated by a speaker
(e.g., right of O2), and asked to click on an object (shown
by the red arrow).
guess the target object that the speaker saw by click-
ing on an object. The purpose of the listener task is
to evaluate speakers, as described in the next section.
6.2 Evaluation
Utility (Communicative Success) We primarily
evaluate a speaker by its ability to communicate suc-
cessfully with a human listener. For each test sce-
nario, we asked three listeners to guess the object.
We use pL:HUMAN(g | w) to denote the distribution
over guessed objects g given prompt w. For exam-
ple, if two of the three listeners guessed O1, then
pL:HUMAN(O1 | w) = 23 . The expected utility (2) is
then computed by averaging the utility (communica-
tive success) over the test scenarios TS:
SUCCESS(S) = EU(S, L:HUMAN) (8)
=
1
|TS|
?
o?TS
?
w
pS(w|o)pL:HUMAN(o|w).
Exact Match As a secondary evaluation metric,
we also measure the ability of our speaker to exactly
match an utterance produced by a human speaker.
Note that since there are many ways of describing
an object, exact match is neither necessary nor suffi-
cient for successful communication.
We asked three human speakers to each pro-
duce an utterance w given a target o. We use
pS:HUMAN(w | o) to denote this distribution; for ex-
ample, pS:HUMAN(right of O2 | o) = 13 if exactly one
of the three speakers uttered right of O2. We then
Speaker Success Exact Match
S:LITERAL [reflex] 4.62% 1.11%
S(L:LITERAL) [rational] 33.65% 2.91%
S:LEARNED [reflex] 38.36% 5.44%
S(L:LEARNED) [rational] 52.63% 14.03%
S:HUMAN 41.41% 19.95%
Table 1: Comparison of various speakers on communica-
tive success and exact match, where only utterances of
complexity 1 are allowed. The rational speakers (with
respect to both the literal listener L:LITERAL and the
learned listener L:LEARNED) perform better than their
reflex counterparts. While the human speaker (composed
of three people) has higher exact match (it is better at
mimicking itself), the rational speaker S(L:LEARNED)
actually achieves higher communicative success than the
human listener.
define the exact match of a speaker S as follows:
MATCH(S) =
1
|TS|
?
o?TS
?
w
pS:HUMAN(w | o)pS(w | o).
(9)
6.3 Reflex versus Rational Speakers
We first evaluate speakers in the setting where only
utterances of complexity 1 are allowed. Table 1
shows the results on both success and exact match.
First, our main result is that the two rational speak-
ers S(L:LITERAL) and S(L:LEARNED), which each
model a listener explicitly, perform significantly bet-
ter than the corresponding reflex speakers, both in
terms of success and exact match.
Second, it is natural that the speakers that in-
volve learning (S:LITERAL and S(L:LITERAL))
outperform the speakers that only consider the
literal meaning of utterances (S:LEARNED and
S(L:LEARNED)), as the former models capture sub-
tler preferences using features.
Finally, we see that in terms of exact match, the
human speaker S:HUMAN performs the best (this
is not surprising because human exact match is es-
sentially the inter-annotator agreement), but in terms
of communicative success, S(L:LEARNED) achieves
a higher success rate than S:HUMAN, suggesting
that the game-theoretic modeling undertaken by the
rational speakers is effective for communication,
which is ultimate goal of language.
Note that exact match is low even for the ?human
speaker?, since there are often many equally good
417
0.2 0.4 0.6 0.8 1.0?
0.49
0.5
0.51
0.52
suc
ces
s
Figure 10: Communicative success as a function of focus
parameter ? without tabooing on TSDEV. The optimal
value of ? is obtained at 0.79.
ways to evoke an object. At the same time, the suc-
cess rates for all speakers are rather low, reflecting
the fundamental difficulty of the setting: sometimes
it is impossible to unambiguously evoke the target
object via short utterances. In the next section, we
show that we can improve the success rate by al-
lowing the speakers to generate more complex utter-
ances.
6.4 Generating More Complex Utterances
We now evaluate the rational speaker
S(L:LEARNED) when it is allowed to generate
utterances of complexity 1 or 2. Recall from
Section 5.3 that the speaker depends on a focus
parameter ?, which governs the embedded listener?s
ability to interpret the utterance. We divided the test
set (TS) in two halves: TSDEV, which we used to
tune the value of ? and TSFINAL, which we used to
evaluate success rates.
Figure 10 shows the communicative success as
a function of ? on TSDEV. When ? is small, the
embedded listener is confused more easily by more
complex utterances; therefore the speaker tends to
choose mostly utterances of complexity 1. As ?
increases, the utterances increase in complexity, as
does the success rate. However, when ? approaches
1, the utterances are too complex and the success
rate decreases. The dependence between ? and av-
erage utterance complexity is shown in Figure 11.
Table 2 shows the success rates on TSFINAL for
? ? 0 (all utterances have complexity 1), ? = 1 (all
utterances have complexity 2), and ? tuned to max-
imize the success rate based on TSDEV. Setting ?
in this manner allows us to effectively balance com-
plexity and ambiguity, resulting in an improvement
in the success rate.
0.2 0.4 0.6 0.8 1.0?
1.2
1.4
1.6
1.8
2.0
ave
rag
e|w
|
Figure 11: Average utterance complexity as a function of
the focus parameter ? on TSDEV. Higher values of ?
yield more complex utterances.
Taboo Success Success Success
Amount (? ? 0) (? = 1) (? = ??) ??
0% 51.78% 50.99% 54.53% 0.79
5% 38.75% 40.83% 43.12% 0.89
10% 29.57% 29.69% 30.30% 0.80
30% 12.40% 13.04% 12.98% 0.81
Table 2: Communicative success (on TSFINAL) of the
rational speaker S(L:LEARNED) for various values of ?
across different taboo amounts. When the taboo amount
is small, small values of ? lead to higher success rates. As
the taboo amount increases, larger values of ? (resulting
in more complex utterances) are better.
7 Conclusion
Starting with the view that the purpose of language
is successful communication, we developed a game-
theoretic model in which a rational speaker gener-
ates utterances by explicitly taking the listener into
account. On the task of generating spatial descrip-
tions, we showed the rational speaker substantially
outperforms a baseline reflex speaker that does not
have an embedded model. Our results therefore sug-
gest that a model of the pragmatics of communica-
tion is an important factor to consider for generation.
Acknowledgements This work was supported by
the National Science Foundation through a Gradu-
ate Research Fellowship to the first two authors. We
also would like to acknowledge Surya Murali, the
designer of the 3D Google Sketchup models, and
thank the anonymous reviewers for their comments.
References
J. L. Austin. 1962. How to do Things with Words: The
William James Lectures delivered at Harvard Univer-
418
sity in 1955. Oxford, Clarendon, UK.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128?135. Omnipress.
David DeVault and Matthew Stone. 2007. Managing
ambiguities across utterances in dialogue.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
J. Feldman and S. Narayanan. 2004. Embodied meaning
in a neural theory of language. Brain and Language,
89:385?392.
M. Fleischman and D. Roy. 2007. Representing inten-
tions in a cognitive model of language acquisition: Ef-
fects of phrase structure on situated verb learning. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
M. C. Frank, N. D. Goodman, and J. B. Tenenbaum.
2009. Using speakers? referential intentions to model
early cross-situational word learning. Psychological
Science, 20(5):578?585.
Peter Gorniak and Deb Roy. 2004. Grounded semantic
composition for visual scenes. In Journal of Artificial
Intelligence Research, volume 21, pages 429?470.
H. P. Grice. 1975. Syntax and Semantics; Logic and
Conversation. 3:Speech Acts:41?58.
G. Hinton. 1999. Products of experts. In International
Conference on Artificial Neural Networks (ICANN).
G. Ja?ger. 2008. Game theory in semantics and pragmat-
ics. Technical report, University of Tu?bingen.
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward
understanding natural language directions. In Human-
Robot Interaction, pages 259?266.
Barbara Landau and Ray Jackendoff. 1993. ?what?
and ?where? in spatial language and spatial cognition.
Behavioral and Brain Sciences, 16(2spatial preposi-
tions analysis, cross linguistic conceptual similarities;
comments/response):217?238.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
T Regier and LA Carlson. 2001. Journal of experimen-
tal psychology. general; grounding spatial language in
perception: an empirical and computational investiga-
tion. 130(2):273?298.
Stefanie Tellex and Deb Roy. 2009. Grounding spatial
prepositions for video search. In ICMI.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967, Prague, Czech Republic.
Association for Computational Linguistics.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. In Asso-
ciation for the Advancement of Artificial Intelligence
(AAAI), pages 488?493, Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
419
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 502?512,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Simple Domain-Independent Probabilistic Approach to Generation
Gabor Angeli
UC Berkeley
Berkeley, CA 94720
gangeli@berkeley.edu
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present a simple, robust generation system
which performs content selection and surface
realization in a unified, domain-independent
framework. In our approach, we break up
the end-to-end generation process into a se-
quence of local decisions, arranged hierar-
chically and each trained discriminatively.
We deployed our system in three different
domains?Robocup sportscasting, technical
weather forecasts, and common weather fore-
casts, obtaining results comparable to state-of-
the-art domain-specific systems both in terms
of BLEU scores and human evaluation.
1 Introduction
In this paper, we focus on the problem of generat-
ing descriptive text given a world state represented
by a set of database records. While existing gen-
eration systems can be engineered to obtain good
performance on particular domains (e.g., Dale et
al. (2003), Green (2006), Turner et al (2009), Re-
iter et al (2005), inter alia), it is often difficult
to adapt them across different domains. Further-
more, content selection (what to say: see Barzilay
and Lee (2004), Foster and White (2004), inter alia)
and surface realization (how to say it: see Ratna-
parkhi (2002), Wong and Mooney (2007), Chen and
Mooney (2008), Lu et al (2009), etc.) are typically
handled separately. Our goal is to build a simple,
flexible system which is domain-independent and
performs content selection and surface realization in
a unified framework.
We operate in a setting in which we are only given
examples consisting of (i) a set of database records
(input) and (ii) example human-generated text de-
scribing some of those records (output). We use the
model of Liang et al (2009) to automatically induce
the correspondences between words in the text and
the actual database records mentioned.
We break up the full generation process into a se-
quence of local decisions, training a log-linear clas-
sifier for each type of decision. We use a simple
but expressive set of domain-independent features,
where each decision is allowed to depend on the en-
tire history of previous decisions, as in the model
of Ratnaparkhi (2002). These long-range contextual
dependencies turn out to be critical for accurate gen-
eration.
More specifically, our model is defined in terms
of three types of decisions. The first type
chooses records from the database (macro content
selection)?for example, wind speed, in the case
of generating weather forecasts. The second type
chooses a subset of fields from a record (micro con-
tent selection)?e.g., the minimum and maximum
temperature. The third type chooses a suitable tem-
plate to render the content (surface realization)?
e.g., winds between [min] and [max] mph; templates
are automatically extracted from training data.
We tested our approach in three domains:
ROBOCUP, for sportscasting (Chen and Mooney,
2008); SUMTIME, for technical weather forecast
generation (Reiter et al, 2005); and WEATHERGOV,
for common weather forecast generation (Liang et
al., 2009). We performed both automatic (BLEU)
and human evaluation. On WEATHERGOV, we
502
s: pass(arg1=purple6, arg2=purple3)kick(arg1=purple3)badPass(arg1=purple3,arg2=pink9)turnover(arg1=purple3,arg2=pink9)
w: purple3 made a bad passthat was picked off by pink9
(a) Robocup
s: temperature(time=5pm-6am,min=48,mean=53,max=61)windSpeed(time=5pm-6am,min=3,mean=6,max=11,mode=0-10)windDir(time=5pm-6am,mode=SSW)gust(time=5pm-6am,min=0,mean=0,max=0)skyCover(time=5pm-9pm,mode=0-25)skyCover(time=2am-6am,mode=75-100)precipPotential(time=5pm-6am,min=2,mean=14,max=20)rainChance(time=5pm-6am,mode=someChance)
w: a 20 percent chance of showers after midnight . increasing clouds ,with a low around 48 southwest wind between 5 and 10 mph
(b) WeatherGov
s: wind10m(time=6am,dir=SW,min=16,max=20,gust min=0,gust max=-)wind10m(time=9pm,dir=SSW,min=28,max=32,gust min=40,gust max=-)wind10m(time=12am,dir=-,min=24,max=28,gust min=36,gust max=-)
w: sw 16 - 20 backing ssw 28 - 32 gusts 40 by mid evening easing 24 - 28 gusts 36 late evening
(c) SumTime
Figure 1: Example scenarios (a scenario is a world state s paired with a text w) for each of the three domains. Each row in the
world state denotes a record. Our generation task is to map a world state s (input) to a text w (output). Note that this mapping
involves both content selection and surface realization.
achieved a BLEU score of 51.5 on the combined task
of content selection and generation, which is more
than a two-fold improvement over a model similar
to that of Liang et al (2009). On ROBOCUP and
SUMTIME, we achieved results comparable to the
state-of-the-art. most importantly, we obtained these
results with a general-purpose approach that we be-
lieve is simpler than current state-of-the-art systems.
2 Setup and Domains
Our goal is to generate a text given a world state.
The world state, denoted s, is represented by a set
of database records. Define T to be a set of record
types, where each record type t ? T is associated
with a set of fields FIELDS(t). Each record r ? s
has a record type r.t ? T and a field value r.v[f ] for
each field f ? FIELDS(t). The text, denoted w, is
represented by a sequence of tokenized words. We
use the term scenario to denote a world state s paired
with a text w.
In this paper, we conducted experiments on three
domains, which are detailed in the following subsec-
tions. Example scenarios for each domain are de-
tailed in Figure 1.
2.1 ROBOCUP: Sportscasting
A world state in the ROBOCUP domain is a set of
event records (meaning representations in the termi-
nology of Chen and Mooney (2008)) generated by
a robot soccer simulator. For example, the record
pass(arg1=pink1,arg2=pink5) denotes a passing
event; records of this type (pass) have two fields:
arg1 (the agent) and arg2 (the recipient). As the
game progresses, human commentators talk about
some of the events in the game, e.g., purple3 made
a bad pass that was picked off by pink9.
We used the dataset created by Chen and Mooney
(2008), which contains 1919 scenarios from the
2001?2004 Robocup finals. Each scenario con-
sists of a single sentence representing a fragment
of a commentary on the game, paired with a set
of candidate records, which were recorded within
five seconds of the commentary. The records in the
ROBOCUP dataset data were aligned by Chen and
Mooney (2008). Each scenario contains on average
|s| = 2.4 records and 5.7 words. See Figure 1(a) for
an example of a scenario. Content selection in this
domain is choosing the single record to talk about,
and surface realization is talking about it.
503
2.2 SUMTIME: Technical Weather Forecasts
Reiter et al (2005) developed a generation system
and created the SUMTIME-METEO corpus, which
consists of marine wind weather forecasts used by
offshore oil rigs, generated by the output of weather
simulators. More specifically, these forecasts de-
scribe various aspects of the wind at different times
during the forecast period.
We used the version of the SUMTIME-METEO
corpus created by Belz (2008). The dataset consists
of 469 scenarios, each containing on average |s| =
2.6 records and 16.2 words. See Figure 1(c) for an
example of a scenario. This task requires no content
selection, only surface realization: The records are
given in some fixed order and the task is to generate
from each of these records in turn; of course, due
to contextual dependencies, these records cannot be
generated independently.
2.3 WEATHERGOV: Common Weather
Forecasts
In the WEATHERGOV domain, the world state con-
tains detailed information about a local weather
forecast (e.g., temperature, rain chance, etc.). The
text is a short forecast report based on this informa-
tion.
We used the dataset created by Liang et al (2009).
The world state is summarized by records which ag-
gregate measurements over selected time intervals.
The dataset consists of 29,528 scenarios, each con-
taining on average |s| = 36 records and 28.7 words.
See Figure 1(b) for an example of a scenario.
While SUMTIME and WEATHERGOV are both
weather domains, there are significant differences
between the two. SUMTIME forecasts are in-
tended to be read by trained meteorologists, and thus
the text is quite abbreviated. On the other hand,
WEATHERGOV texts are intended to be read by the
general public and thus is more English-like. Fur-
thermore, SUMTIME does not require content selec-
tion, whereas content selection is a major focus of
WEATHERGOV. Indeed, on average, only 5 of 36
records are actually mentioned in a WEATHERGOV
scenario. Also, WEATHERGOV is more complex:
The text is more varied, there are multiple record
types, and there are about ten times as many records
in each world state.
Generation Process
for i = 1, 2, . . . :
?choose a record ri ? s
?if ri = STOP: return
?choose a field set Fi ? FIELDS(ri.t)
?choose a template Ti ? TEMPLATES(ri.t, Fi)
Figure 2: Pseudocode for the generation process. The generated
text w is a deterministic function of the decisions.
3 The Generation Process
To model the process of generating a text w from a
world state s, we decompose the generation process
into a sequence of local decisions. There are two as-
pects of this decomposition that we need to specify:
(i) how the decisions are structured; and (ii) what
pieces of information govern the decisions.
The decisions are structured hierarchically into
three types of decisions: (i) record decisions, which
determine which records in the world state to talk
about (macro content selection); (ii) field set deci-
sions, which determine which fields of those records
to mention (micro content selection); and (iii) tem-
plate decisions, which determine the actual words
to use to describe the chosen fields (surface realiza-
tion). Figure 2 shows the pseudocode for the gen-
eration process, while Figure 3 depicts an example
of the generation process on a WEATHERGOV sce-
nario.
Each of these decisions is governed by a set of
feature templates (see Figure 4), which are repre-
sented as functions of the current decision and past
decisions. The feature weights are learned from
training data (see Section 4.3).
We chose a set of generic domain-independent
feature templates, described in the sections below.
These features can, in general, depend on the current
decision and all previous decisions. For example, re-
ferring to Figure 4, R2 features on the record choice
depend on all the previous record decisions, and R5
features depend on the most recent template deci-
sion. This is in contrast with most systems for con-
tent selection (Barzilay and Lee, 2004) and surface
realization (Belz, 2008), where decisions must de-
compose locally according to either a graph or tree.
The ability to use global features in this manner is
504
Worldstate skyCover1: skyCover(time=5pm-6am,mode=50-75)temperature1: temperature(time=5pm-6am,min=44,mean=49,max=60)...
Decisions
Record r1 = skyCover1 r2 = temperature1 r3 = stop
Field set F1 = {mode} F2 = {time,min}
Template T1 = ?mostly cloudy ,? T2 = ?with a low around [min] .?
Text mostly cloudy , with a low around 45 .
Specific active (nonzero) features for highlighted decisions
r2 = temperature1
(R1) Jr2.t = temperature and (r1.t, r0.t) = (skyCover, start)KJr2.t = temperature and (r1.t) = (skyCover)K(R2) Jr2.t = temperature and {r1.t} = {skyCover}K(R3) Jr2.t = temperature and rj .t 6= temperature ?j < 2K(R4) Jr2.t = temperature and r2.v[time] = 5pm-6amKJr2.t = temperature and r2.v[min] = lowKJr2.t = temperature and r2.v[mean] = lowKJr2.t = temperature and r2.v[max] = mediumK
F2 = {time,min} (F1) JF2 = {time,min}K(F2) JF2 = {time,min} and r2.v[time] = 5pm-6amK(F2) JF2 = {time,min} and r2.v[min] = lowK
T2 = ?with a low around [min]?
(W1) JBase(T2) = ?with a low around [min]?KJCoarse(T2) = ?with a [time] around [min]?K(W2) JBase(T2) = ?with a low around [min]? and r2.v[time] = 5pm-6amKJCoarse(T2) = ?with a [time] around [min]? and r2.v[time] = 5pm-6amKJBase(T2) = ?with a low around [min]? and r2.v[min] = lowKJCoarse(T2) = ?with a [time] around [min]? and r2.v[min] = lowK(W3) log plm(with | cloudy ,)
Figure 3: The generation process on an example WEATHERGOV scenario. The figure is divided into two parts: The upper part of
the figure shows the generation of text from the world state via a sequence of seven decisions (in boxes). Three of these decisions
are highlighted and the features that govern these decisions are shown in the lower part of the figure. Note that different decisions
in the generation process would result in different features being active (nonzero).
Feature TemplatesRecord R1? list of last k record types Jri.t = ? and (ri?1.t, . . . , ri?k.t) = ?K for k ? {1, 2}R2 set of previous record types Jri.t = ? and {rj .t : j < i} = ?KR3 record type already generated Jrj .t = ri.t for some j < iKR4 field values Jri.t = ? and ri.v[f ] = ?K for f ? Fields(ri.t)R5? stop under language model (LM) Jri.t = stopK? log plm(stop | previous two words generated)Field set F1? field set JFi = ?KF2 field values JFi = ? and ri.v[f ] = ?K for f ? FiTemplate W1? base/coarse generation template Jh(Ti) = ?K for h ? {Base,Coarse}W2 field values Jh(Ti) = ? and ri.v[f ] = ?K for f ? Fi, h ? {Base,Coarse}W3? first word of template under LM log plm(first word in Ti | previous two words)
Figure 4: Feature templates that govern the record, field set, and template decisions. Each line specifies the name, informal
description, and formal description of a set of features, obtained by ranging ? over possible values (for example, for Jri.t = ?K, ?
ranges over all record types T ). Notation: JeK returns 1 if the expression e is true and 0 if it is false. These feature templates are
domain-independent; that is, they are used to create features automatically across domains. Feature templates marked with ? are
included in our baseline system (Section 5.2).
one of the principal advantages of our approach.
3.1 Record Decisions
Record decisions are responsible for macro content
selection. Each record decision chooses a record ri
from the world state s according to features of the
following types:
R1 captures the discourse coherence aspect of
content selection; for example, we learn that
windSpeed tends to follow windDir (but not al-
505
ways). R2 captures an unordered notion of
coherence?simply which sets of record types are
preferable; for example, we learn that rainChance
is not generated if sleetChance already was men-
tioned. R3 is a coarser version of R2, capturing
how likely it is to propose a record of a type that has
already been generated. R4 captures the important
aspect of content selection that the records chosen
depend on their field values;1 for example, we learn
that snowChance is not chosen unless there is snow.
R5 allows the language model to indicate whether a
STOP record is appropriate; this helps prevent sen-
tences from ending abruptly.
3.2 Field Set Decisions
Field set decisions are responsible for micro con-
tent selection, i.e., which fields of a record are men-
tioned. Each field set decision chooses a subset of
fields Fi from the set of fields FIELDS(ri.t) of the
record ri that was just generated. These decisions
are made based on two types of features:
F1 captures which sets of fields are talked
about together; for example, we learn that {mean}
and {min,max} are preferred field sets for the
windSpeed record. By defining features on the en-
tire field set, we can capture any correlation structure
over the fields; in contrast, Liang et al (2009) gen-
erates a sequence of fields in which a field can only
depend on the previous one.
F2 allows the field set to be chosen based on the
values of the fields, analogously to R4.
3.3 Template Decisions
Template decisions perform surface realization. A
template is a sequence of elements, where each ele-
ment is either a word (e.g., around) or a field (e.g.,
[min]). Given the record ri and field set Fi that we
are generating from, the goal is to choose a template
Ti (Section 4.3.2 describes how we define the set
of possible templates). The features that govern the
choice of Ti are as follows:
W1 captures a priori preferences for generation
templates given field sets. There are two ways
to control this preference, BASE and COARSE.
1We map a numeric field value onto one of five categories
(very-low, low, medium, high, or very-high) based
on its value with respect to the mean and standard deviation of
values of that field in the training data.
BASE(Ti) denotes the template Ti itself, thus allow-
ing us to remember exactly which templates were
useful. To guard against overfitting, we also use
COARSE(Ti), which maps Ti to a coarsened version
of Ti, in which more words are replaced with their
associated fields (see Figure 5 for an example).
W2 captures a dependence on the values of fields
in the field set, and is analogous to R4 and F2. Fi-
nally, W3 contributes a language model probability,
to ensure smooth transitions between templates.
After Ti has been chosen, each field in the tem-
plate is replaced with a word given the correspond-
ing field value in the world state. In particular, a
word is chosen from the parameters learned in the
model of Liang et al (2009). In the example in Fig-
ure 3, the [min] field in T2 has value 44, which is
rendered to the word 45 (rounding and other noisy
deviations are common in the WEATHERGOV do-
main).
4 Learning a Probabilistic Model
Having described all the features, we now present a
conditional probabilistic model over texts w given
world states s (Section 4.1). Section 4.2 describes
how to use the model for generation, and Section 4.3
describes how to learn the model.
4.1 Model
Recall from Section 3 that the generation process
generates r1, F1, T1, r2, F2, T2, . . . , STOP. To unify
notation, denote this sequence of decisions as d =
(d1, . . . , d|d|).
Our probability model is defined as follows:
p(d | s; ?) =
|d|?
j=1
p(dj | d<j ; ?), (1)
where d<j = (d1, . . . , dj?1) is the history of de-
cisions and ? are the model parameters (feature
weights). Note that the text w (the output) is a de-
terministic function of the decisions d. We use the
features described in Section 3 to define a log-linear
model for each decision:
p(dj | d<j , s; ?) =
exp{?j(dj ,d<j , s)>?}
?
d?j?Dj
exp{?j(d?j ,d<j , s)
>?}
, (2)
where ? are all the parameters (feature weights), ?j
is the feature vector for the j-th decision, and Dj is
506
the domain of the j-th decision (either records, field
sets, or templates).
This chaining of log-linear models was used in
Ratnaparkhi (1998) for tagging and parsing, and in
Ratnaparkhi (2002) for surface realization. The abil-
ity to condition on arbitrary histories is a defining
property of these models.
4.2 Using the Model for Generation
Suppose we have learned a model with parameters ?
(how to obtain ? is discussed in Section 4.3). Given
a world state s, we would like to use our model to
generate an output text w via a decision sequence d.
In our experiments, we choose d by sequentially
choosing the best decision in a greedy fashion (until
the STOP record is generated):
dj = argmax
d?j
p(d?j | d<j , s; ?). (3)
Alternatively, instead of choosing the best decision
at each point, we can sample from the distribution:
dj ? p(dj | d<j , s; ?), which provides more diverse
generated texts at the expense of a slight degradation
in quality.
Both greedy search and sampling are very effi-
cient. Another option is to try to find the Viterbi
decision sequence, i.e., the one with the maximum
joint probability: d = argmaxd? p(d
? | s; ?). How-
ever, this computation is intractable due to features
depending arbitrarily on past decisions, making dy-
namic programming infeasible. We tried using beam
search to approximate this optimization, but we ac-
tually found that beam search performed worse than
greedy. Belz (2008) also found that greedy was more
effective than Viterbi for their model.
4.3 Learning
Now we turn our attention to learning the parame-
ters ? of our model. We are given a set of N sce-
narios {(s(i),w(i))}Ni=1 as training data. Note that
our model is defined over the decision sequence d
which contains information not present in w. In Sec-
tions 4.3.1 and 4.3.2, we show how we fill in this
missing information to obtain d(i) for each training
scenario i.
Assuming this missing information is filled, we
end up with a standard supervised learning problem,
which can be solved by maximize the (conditional)
likelihood of the training data:
max
??Rd
?
?
N?
i=1
|d(i)|?
j=1
log p(d(i)j | d
(i)
<j ; ?)
?
???||?||2, (4)
where ? > 0 is a regularization parameter. The ob-
jective function in (4) is optimized using the stan-
dard L-BFGS algorithm (Liu and Nocedal, 1989).
4.3.1 Latent Alignments
As mentioned previously, our training data in-
cludes only the world state s and generated text w,
not the full sequence of decisions d needed for train-
ing. Intuitively, we know what was generated but not
why it was generated.
We use the model of Liang et al (2009) to im-
pute the decisions d. They introduce a generative
model p(a,w|s), where the latent alignment a spec-
ifies (1) the sequence of records that were chosen,
(2) the sequence of fields that were chosen, and (3)
which words in the text were spanned by the chosen
records and fields. The model is learned in an unsu-
pervised manner using EM to produce a observing
only w and s.
An example of an alignment is given in the left
part of Figure 5. This information specifies the
record decisions and a set of fields for each record.
Because the induced alignments can be noisy, we
need to process them to obtain cleaner template de-
cisions. This is the subject of the next section.
4.3.2 Template Extraction
Given an aligned training scenario (Figure 5), we
would like to extract two types of templates.
For each record, an aligned training scenario
specifies a sequence of fields and the text that
is spanned by each field. We create a template
by abstracting fields?that is, replacing the words
spanned by a field by the field itself. We call the
resulting template COARSE. The problem with us-
ing this template directly is that fields can be noisy
due to errors from the unsupervised model.
Therefore, we also create a BASE template which
only abstracts a subset of the fields. In particular,
we define a trigger pattern which specifies a simple
condition under which a field should be abstracted.
For WEATHERGOV, we only abstract fields that
507
Records:Fields:Text:
skyCover1mode=50-75mostly cloudy ,
temperature1xwith a time=17-30low around min=4445 mean=49.
Aligned training scenario
?
skyCover temperatureCoarse ?[mode]? ?with a [time] [min] [mean]?Base ?most cloudy ,? ?with a low around [min] .?
Templates extracted
Figure 5: An example of template extraction from an imperfectly aligned training scenario. Note that these alignments are noisy
(e.g., [mean] aligns to a period). Therefore, for each record (skyCover and temperature in this case), we extract two templates:
(1) a COARSE template, which takes the text spanned by the record and abstracts away all fields in the scenario ([mode], [time],
[min], and [mean] in the example); and (2) a BASE template, which only abstracts away fields whose spanned text matches a simple
pattern (e.g., numbers in WEATHERGOV, corresponding to [min] in the example).
span numbers; for SUMTIME, fields that span num-
bers and wind directions; and for ROBOCUP, fields
that span words starting with purple or pink.
For each record ri, we define Ti so that BASE(Ti)
and COARSE(Ti) are the corresponding two ex-
tracted templates. We restrict Fi to the set of ab-
stracted fields in the COARSE template
5 Experiments
We now present an empirical evaluation of our sys-
tem on our three domains?ROBOCUP, SUMTIME,
and WEATHERGOV.
5.1 Evaluation Metrics
Automatic Evaluation To evaluate surface real-
ization (or, combined content selection and surface
realization), we measured the BLEU score (Papineni
et al, 2002) (the precision of 4-grams with a brevity
penalty) of the system-generated output with respect
to the human-generated output.
To evaluate macro content selection, we measured
the F1 score (the harmonic mean of precision and
recall) of the set of records chosen with respect to
the human-annotated set of records.
Human Evaluation We conducted a human eval-
uation using Amazon Mechanical Turk. For each
domain, we chose 100 scenarios randomly from the
test set. We ran each system under consideration on
each of these scenarios, and presented each resulting
output to 10 evaluators.2 Evaluators were given in-
structions to rank an output on the basis of English
fluency and semantic correctness on the following
scale:
2To minimize bias, we evaluated all the systems at once,
randomly shuffling the outputs of the systems. The evaluators
were not necessarily the same 10 evaluators.
Score English Fluency Semantic Correctness
5 Flawless Perfect
4 Good Near Perfect
3 Non-native Minor Errors
2 Disfluent Major Errors
1 Gibberish Completely Wrong
Evaluators were also given additional domain-
specific information: (1) the background of the
domain (e.g., that SUMTIME reports are techni-
cal weather reports); (2) general properties of the
desired output (e.g., that SUMTIME texts should
mention every record whereas WEATHERGOV texts
need not); and (3) peculiarities of the text (e.g., the
suffix ly in SUMTIME should exist as a separate to-
ken from its stem, or that pink goalie and pink1 have
the same meaning in ROBOCUP).
5.2 Systems
We evaluated the following systems on our three do-
mains:
? HUMAN is the human-generated output.
? OURSYSTEM uses all the features in Figure 4
and is trained according to Section 4.3.
? BASELINE is OURSYSTEM using a subset of
the features (those marked with ? in Fig-
ure 4). In contrast to OURSYSTEM, the in-
cluded features only depend on a local con-
text of decisions in a manner similar to
the generative model of Liang et al (2009)
and the pCRU-greedy system of Belz (2008).
BASELINE also excludes features that depend
on values of the world state.
? The existing state-of-the-art domain-specific
system for each domain.
5.3 ROBOCUP Results
Following the evaluation methodology of Chen and
Mooney (2008), we trained our system on three
508
System F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 78.7 24.8 4.28 ? 0.78 4.15 ? 1.14
OURSYSTEM 79.9 28.8 4.34 ? 0.69 4.17 ? 1.21
WASPER-GEN 72.0 28.7 4.43 ? 0.76 4.27 ? 1.15
HUMAN ? ? 4.43 ? 0.69 4.30 ? 1.07
Table 1: ROBOCUP results. WASPER-GEN is described in
Chen and Mooney (2008). The BLEU is reported on systems
that use fixed human-annotated records (in other words, we
evaluate surface realization given perfect content selection).
Human Records:Fields:Text:
pass1arg1=purple10purple10 xpasses back to arg2=purple9purple9
Baseline Records:Fields:Text:
pass1arg1=purple10purple10 xkicks to arg2=purple9purple9
OurSystem Records:Fields:Text:
pass1arg1=purple10purple10 xpasses to arg2=purple9purple9WASPER-GENRecords:Text purple10 passes to purple9
Figure 6: Outputs of systems on an example ROBOCUP sce-
nario. There are some minor differences between the outputs.
Recall that OURSYSTEM differs from BASELINE mostly in
the addition of feature W2, which captures dependencies be-
tween field values (e.g., purple10) and the template chosen
(e.g., [arg1] passes to [arg2]). This allows us to capture value-
dependent preferences for different realizations (e.g., passes to
over kicks to). Also, HUMAN uses passes back to, but this word
choice requires knowledge of passing records in previous sce-
narios, which none of the systems have access to. It would nat-
ural, however, to add features that would capture these longer-
range dependencies in our framework.
Robocup games and tested on the fourth, averaging
over the four train/test splits. We report the average
test accuracy weighted by the number of scenarios
in a game. First, we evaluated macro content selec-
tion. Table 1 shows that OURSYSTEM significantly
outperforms BASELINE and WASPER-GEN on F1.
To compare with Chen and Mooney (2008) on
surface realization, we fixed each system?s record
decisions to the ones given by the annotated data
and enforced that all the fields of that record are
chosen. Table 1 shows that OURSYSTEM sig-
nificantly outperforms BASELINE and is compara-
ble to WASPER-GEN on BLEU. On human eval-
uation, OURSYSTEM outperforms BASELINE, but
WASPER-GEN outperforms OURSYSTEM. See
Figure 6 for example outputs from the various sys-
tems.
BLEU EnglishFluency
Semantic
Correctness
BASELINE 32.9 4.23 ? 0.71 4.26 ? 0.85
OURSYSTEM 55.1 4.25 ? 0.69 4.27 ? 0.82
OURSYSTEM-CUSTOM 62.3 4.12 ? 0.78 4.33 ? 0.91
pCRU-greedy 63.6 4.18 ? 0.71 4.49 ? 0.73
SUMTIME-Hybrid 52.7 ? ?
HUMAN ? 4.09 ? 0.83 4.37 ? 0.87
Table 2: SUMTIME results. The SUMTIME-Hybrid system
is described in (Reiter et al, 2005); pCRU-greedy, in (Belz,
2008).
5.4 SUMTIME Results
The SUMTIME task only requires micro content se-
lection and surface realization because the sequence
of records to be generated is fixed; only these as-
pects are evaluated. Following the methodology of
Belz (2008), we used five-fold cross validation.
We found that using the unsupervised model of
Liang et al (2009) to automatically produce aligned
training scenarios (Section 4.3.1) was less effec-
tive than it was in the other two domains due to
two factors: (i) there are fewer training examples
in SUMTIME and unsupervised learning typically
works better with a large amount of data; and (ii)
the alignment model does not exploit the temporal
structure in the SUMTIME world state. Therefore,
we used a small set of simple regular expressions to
produce aligned training scenarios.
Table 2 shows that OURSYSTEM signif-
icantly outperforms BASELINE as well as
SUMTIME-Hybrid, a hand-crafted system, on
BLEU. Note that OURSYSTEM is domain-
independent and has not been specifically tuned
to SUMTIME. However, OURSYSTEM is outper-
formed by the state-of-the-art statistical system
pCRU-greedy.
Custom Features One of the advantages of our
feature-based approach is that it is straightforward to
incorporate domain-specific features to capture spe-
cific properties of a domain. To this end, we define
the following set of feature templates in place of our
generic feature templates from Figure 4:
? F1?: Value of time
? F2?: Existence of gusts/wind direction/wind
speeds
? W1?: Change in wind direction (clockwise,
counterclockwise, or none)
509
Human Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late evening
Baseline Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xincreasing min=1010 x- max=1414
OurSystem-Custom Records:Fields:Text:
windDir1dir=nnenne min=1818 x- max=2222 gust-min=30gusts 30
windDir2xgradually decreasing min=1010 x- max=1414 time=12amby late eveningpCRU-greedy Records:Text nne 18 - 22 gusts 30 easing 10 - 14 by late evening
Figure 7: Outputs of systems on an example SUMTIME scenario. Two notable differences between OURSYSTEM-CUSTOM and
BASELINE arise due to OURSYSTEM-CUSTOM?s value-dependent features. For example, OURSYSTEM-CUSTOM can choose
whether to include the time field (windDir2) or not (windDir1), depending on the value of the time (F1?), thereby improving content
selection. OURSYSTEM-CUSTOM also improves surface realization, choosing gradually decreasing over BASELINE?s increasing.
Interestingly, this improvement comes from the joint effort of two features: W2? prefers decreasing over increasing in this case,
and W5? adds the modifier gradually. An important strength of log-linear models is the ability to combine soft preferences from
many features.
? W2?: Change in wind speed
? W3?: Change in wind direction and speed
? W4?: Existence of gust min and/or max
? W5?: Time elapsed since last record
? W6?: Whether wind is a cardinal direction (N,
E, S, W)
The resulting system, which we call
OURSYSTEM-CUSTOM, obtains a BLEU score
which is comparable to pCRU-greedy.
An important aspect of our system that it is flexi-
ble and quick to deploy. According to Belz (2008),
SUMTIME-Hybrid took twelve person-months to
build, while pCRU-greedy took one month. Having
developed OURSYSTEM in a domain-independent
way, we only needed to do simple reformatting upon
receiving the SUMTIME data. Furthermore, it took
only a few days to develop the custom features
above to create OURSYSTEM-CUSTOM, which has
BLEU performance comparable to the state-of-the-
art pCRU-greedy system.
We also conducted human evaluations on the four
systems shown in Table 2. Note that this evalua-
tion is rather difficult for Mechanical Turkers since
SUMTIME texts are rather technical compared to
those in other domains. Interestingly, all systems
outperform HUMAN on English fluency; this result
corroborates the findings of Belz (2008). On se-
mantic correctness, all systems perform comparably
to HUMAN, except pCRU-greedy, which performs
slightly better. See Figure 7 for a comparison of the
outputs generated by the various systems.
F1 BLEU* EnglishFluency
Semantic
Correctness
BASELINE 22.1 22.2 4.07 ? 0.59 3.41 ? 1.16
OURSYSTEM 65.4 51.5 4.12 ? 0.74 4.22 ? 0.89
HUMAN ? ? 4.14 ? 0.71 3.85 ? 0.99
Table 3: WEATHERGOV results. The BLEU score is on joint
content selection and surface realization and is modified to not
penalize numeric deviations of at most 5.
5.5 WEATHERGOV Results
We evaluate the WEATHERGOV corpus on the joint
task of content selection and surface realization.
We split our corpus into 25,000 scenarios for train-
ing, 1,000 for development, and 3,528 for testing.
In WEATHERGOV, numeric field values are often
rounded or noisily perturbed, so it is difficult to gen-
erate precisely matching numbers. Therefore, we
used a modified BLEU score where numbers dif-
fering by at most five are treated as equal. Fur-
thermore, WEATHERGOV is evaluated on the joint
content selection and surface realization task, un-
like ROBOCUP, where content selection and surface
realization were treated separately, and SUMTIME,
where content selection was not applicable.
Table 3 shows the results. We see that
OURSYSTEM substantially outperforms BASELINE,
especially on BLEU score and semantic correctness.
This difference shows that taking non-local context
into account is very important in this domain. This
result is not surprising, since WEATHERGOV is the
most complicated of the three domains, and this
complexity is exactly where non-locality is neces-
510
Human Records:Fields:Text:
skyCover1cover=50-75mostly cloudy x,
temperature1xwith a time=5pm-6amlow xaround min=5957 x.
windDir1mode=ssesouth xwind between
windSpeed1min=75 xand max=1510 xmph .
Baseline Records:Fields:Text:
rainChance2xa chance of showers ,
nonex,
gust1xwith gusts as high as max=2120 xmph .
precipPotential1xchance of precipitation is max=1010 x% .
OurSystem Records:Fields:Text:
skyCover1xmostly cloudy ,
temperature1xwith a low around min=5959 x.
windDir1xsouth wind between
windSpeed1min=77 xand max=1515 xmph .
Figure 8: Outputs of systems on an example WEATHERGOV scenario. Most of the gains of OURSYSTEM over BASELINE come
from improved content selection. For example, BASELINE chooses rainChance because it happens to be the most common first
record type in the training data. However, since OURSYSTEM has features that depend on the value of rainChance (noChance
in this case), it has learned to disprefer talking about rain when there is no rain. Also, OURSYSTEM has additional features on the
entire history of chosen records, which enables it to choose a better sequence of records.
sary. Interestingly, OURSYSTEM even outperforms
HUMAN on semantic correctness, perhaps due to
generating more straightforward renderings of the
world state. Figure 8 describes example outputs for
each system.
6 Related Work
There has been a fair amount of work both on con-
tent selection and surface realization. In content se-
lection, Barzilay and Lee (2004) use an approach
based on local classification with edge-wise scores
between local decisions. Our model, on the other
hand, can capture higher-order constraints to enforce
global coherence.
Liang et al (2009) introduces a generative model
of the text given the world state, and in some ways is
similar in spirit to our model. Although that model
is capable of generation in principle, it was de-
signed for unsupervised induction of hidden align-
ments (which is exactly what we use it for). Even
if combined with a language model, generated text
was much worse than our baseline.
The prominent approach for surface realization
is rendering the text from a grammar. Wong and
Mooney (2007) and Chen and Mooney (2008) use
synchronous grammars that map a logical form, rep-
resented as a tree, into a parse of the text. Soricut
and Marcu (2006) uses tree structures called WIDL-
expressions (the acronym corresponds to four opera-
tions akin to the rewrite rules of a grammar) to repre-
sent the realization process, and, like our approach,
operates in a log-linear framework. Belz (2008) and
Belz and Kow (2009) also perform surface realiza-
tion from a PCFG-like grammar. Lu et al (2009)
uses a conditional random field model over trees.
Other authors have performed surface realization us-
ing various grammar formalisms, for instance CCG
(White et al, 2007), HPSG (Nakanishi et al, 2005),
and LFG (Cahill and van Genabith, 2006).
In each of the above cases, the decomposable
structure of the tree/grammar enables tractability.
However, we saw that it was important to include
features that captured long-range dependencies. Our
model is also similar in spirit to Ratnaparkhi (2002)
in the use of non-local features, but we operate at
three levels of hierarchy to include both content se-
lection and surface realization.
One issue that arises with long-range dependen-
cies is the lack of efficient algorithms for finding the
optimal text. Koller and Striegnitz (2002) perform
surface realization of a flat semantics, which is NP-
hard, so they recast the problem as non-projective
dependency parsing. Ratnaparkhi (2002) uses beam
search to find an approximate solution. We found
that a greedy approach obtained better results than
beam search; Belz (2008) found greedy approaches
to be effective as well.
7 Conclusion
We have developed a simple yet powerful generation
system that combines both content selection and sur-
face realization in a domain independent way. De-
spite our approach being domain-independent, we
were able to obtain performance comparable to the
state-of-the-art across three domains. Additionally,
the feature-based design of our approach makes it
easy to incorporate domain-specific knowledge to
increase performance even further.
511
References
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to genera-
tion and summarization. In Human Language Tech-
nology and North American Association for Computa-
tional Linguistics (HLT/NAACL).
A. Belz and E. Kow. 2009. System building cost vs.
output quality in data-to-text generation. In European
Workshop on Natural Language Generation, pages
16?24.
A. Belz. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-
space models. Natural Language Engineering,
14(4):1?26.
Aoife Cahill and Josef van Genabith. 2006. Robust pcfg-
based generation using automatically acquired LFG
approximations. In Association for Computational
Linguistics (ACL), pages 1033?1040, Morristown, NJ,
USA. Association for Computational Linguistics.
D. L. Chen and R. J. Mooney. 2008. Learning to
sportscast: A test of grounded language acquisition.
In International Conference on Machine Learning
(ICML), pages 128?135.
R. Dale, S. Geldof, and J. Prost. 2003. Coral: using natu-
ral language generation for navigational assistance. In
Australasian computer science conference, pages 35?
44.
M. E. Foster and M. White. 2004. Techniques for text
planning with XSLT. In Workshop on NLP and XML:
RDF/RDFS and OWL in Language Technology, pages
1?8.
N. Green. 2006. Generation of biomedical arguments for
lay readers. In International Natural Language Gen-
eration Conference, pages 114?121.
A. Koller and K. Striegnitz. 2002. Generation as de-
pendency parsing. In Association for Computational
Linguistics (ACL), pages 17?24.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
W. Lu, H. T. Ng, and W. S. Lee. 2009. Natural lan-
guage generation with tree conditional random fields.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 400?409.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Parsing ?05: Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 93?102, Morristown, NJ, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
A. Ratnaparkhi. 1998. Maximum entropy models for nat-
ural language ambiguity resolution. Ph.D. thesis, Uni-
versity of Pennsylvania.
A. Ratnaparkhi. 2002. Trainable approaches to surface
natural language generation and their application to
conversational dialog systems. Computer, Speech &
Language, 16:435?455.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing words in computer-generated weather fore-
casts. Artificial Intelligence, 167:137?169.
R. Soricut and D. Marcu. 2006. Stochastic language
generation using WIDL-expressions and its applica-
tion in machine translation and summarization. In As-
sociation for Computational Linguistics (ACL), pages
1105?1112.
R. Turner, Y. Sripada, and E. Reiter. 2009. Gener-
ating approximate geographic descriptions. In Eu-
ropean Workshop on Natural Language Generation,
pages 42?49.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realization
with CCG. In In Proceedings of the Workshop on Us-
ing Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+MT).
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967.
512
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 313?321,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Simple Effective Decipherment via Combinatorial Optimization
Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, klein}@cs.berkeley.edu
Abstract
We present a simple objective function that
when optimized yields accurate solutions to
both decipherment and cognate pair identifica-
tion problems. The objective simultaneously
scores a matching between two alphabets and
a matching between two lexicons, each in a
different language. We introduce a simple
coordinate descent procedure that efficiently
finds effective solutions to the resulting com-
binatorial optimization problem. Our system
requires only a list of words in both languages
as input, yet it competes with and surpasses
several state-of-the-art systems that are both
substantially more complex and make use of
more information.
1 Introduction
Decipherment induces a correspondence between
the words in an unknown language and the words
in a known language. We focus on the setting where
a close correspondence between the alphabets of the
two languages exists, but is unknown. Given only
two lists of words, the lexicons of both languages,
we attempt to induce the correspondence between
alphabets and identify the cognates pairs present in
the lexicons. The system we propose accomplishes
this by defining a simple combinatorial optimiza-
tion problem that is a function of both the alphabet
and cognate matchings, and then induces correspon-
dences by optimizing the objective using a block co-
ordinate descent procedure.
There is a range of past work that has var-
iously investigated cognate detection (Kondrak,
2001; Bouchard-Co?te? et al, 2007; Bouchard-Co?te?
et al, 2009; Hall and Klein, 2010), character-level
decipherment (Knight and Yamada, 1999; Knight
et al, 2006; Snyder et al, 2010; Ravi and Knight,
2011), and bilingual lexicon induction (Koehn and
Knight, 2002; Haghighi et al, 2008). We consider
a common element, which is a model wherein there
are character-level correspondences and word-level
correspondences, with the word matching parame-
terized by the character one. This approach sub-
sumes a range of past tasks, though of course past
work has specialized in interesting ways.
Past work has emphasized the modeling as-
pect, where here we use a parametrically simplistic
model, but instead emphasize inference.
2 Decipherment as Two-Level
Optimization
Our method represents two matchings, one at the al-
phabet level and one at the lexicon level. A vector of
variables x specifies a matching between alphabets.
For each character i in the source alphabet and each
character j in the target alhabet we define an indi-
cator variable xij that is on if and only if character i
is mapped to character j. Similarly, a vector y rep-
resents a matching between lexicons. For word u in
the source lexicon and word v in the target lexicon,
the indicator variable yuv denotes that u maps to v.
Note that the matchings need not be one-to-one.
We define an objective function on the matching
variables as follows. Let EDITDIST(u, v;x) denote
the edit distance between source word u and target
word v given alphabet matching x. Let the length
of word u be lu and the length of word w be lw.
This edit distance depends on x in the following
way. Insertions and deletions always cost a constant
.1 Substitutions also cost  unless the characters
are matched in x, in which case the substitution is
1In practice we set  = 1lu+lv . lu + lv is the maximumnumber of edit operations between words u and v. This nor-
malization insures that edit distances are between 0 and 1 for
all pairs of words.
313
free. Now, the objective that we will minimize can
be stated simply: ?u
?
v yuv ? EDITDIST(u, v;x),
the sum of the edit distances between the matched
words, where the edit distance function is parame-
terized by the alphabet matching.
Without restrictions on the matchings x and y
this objective can always be driven to zero by either
mapping all characters to all characters, or matching
none of the words. It is thus necessary to restrict
the matchings in some way. Let I be the size of
the source alphabet and J be the size of the target
alphabet. We allow the alphabet matching x to
be many-to-many but require that each character
participate in no more than two mappings and that
the total number of mappings be max(I, J), a
constraint we refer to as restricted-many-to-many.
The requirements can be encoded with the following
linear constraints on x:
?i
?
j
xij ? 2
?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
The lexicon matching y is required to be ? -one-to-
one. By this we mean that y is an at-most-one-to-one
matching that covers proportion ? of the smaller of
the two lexicons. Let U be the size of the source
lexicon and V be this size of the target lexicon.
This requirement can be encoded with the following
linear constraints:
?u
?
v
yuv ? 1
?v
?
u
yuv ? 1
?
u
?
v
yuv = ? min(U, V )
Now we are ready to define the full optimization
problem. The first formulation is called the Implicit
Matching Objective since includes an implicit
minimization over edit alignments inside the com-
putation of EDITDIST.
(1) Implicit Matching Objective:
min
x,y
?
u
?
v
yuv ? EDITDIST(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
In order to get a better handle on the shape of the
objective and to develop an efficient optimization
procedure we decompose each edit distance compu-
tation and re-formulate the optimization problem in
Section 2.2.
2.1 Example
Figure 1 presents both an example matching prob-
lem and a diagram of the variables and objective.
Here, the source lexicon consists of the English
words (cat, bat, cart, rat, cab), and
the source alphabet consists of the characters (a,
b, c, r, t). The target alhabet is (0, 1,
2, 3). We have used digits as symbols in the target
alphabet to make it clear that we treat the alphabets
as disjoint. We have no prior knowledge about any
correspondence between alphabets, or between lexi-
cons.
The target lexicon consists of the words (23,
1233, 120, 323, 023). The bipartite graphs
show a specific setting of the matching variables.
The bold edges correspond to the xij and yuv that
are one. The matchings shown achieve an edit dis-
tance of zero between all matched word pairs ex-
cept for the pair (cat, 23). The best edit align-
ment for this pair is also diagrammed. Here, ?a?
is aligned to ?2?, ?t? is aligned to ?3?, and ?c? is
deleted and therefore aligned to the null position ?#?.
Only the initial deletion has a non-zero cost since
all other alignments correspond to substitutions be-
tween characters that are matched in x.
2.2 Explicit Objective
Computing EDITDIST(u, v;x) requires running a
dynamic program because of the unknown edit
alignments; here we define those alignments z ex-
plicitly, which makes the EDITDIST(u, v;x) easy to
write explicitly at the cost of more variables. How-
ever, by writing the objective in an explicit form that
refers to these edit variables, we are able to describe
a efficient block coordinate descent procedure that
can be used for optimization.
EDITDIST(u, v;x) is computed by minimizing
over the set of monotonic alignments between the
characters of the source word u and the characters
of the target word v. Let un be the character at the
nth position of the source word u, and similarly for
314
a
b
c
r
t
0
1
2
3
Alphabet Matching
Lexicon Matching
xij
cat
bat
rat
cart
cab
1233
120
323
023
23
yuv
Edit Distance
c
a
t
2
3
# # EditDist(u, v;x) =
min
x,y
?
u
?
v
yuv ? EditDist(u, v;x)
s.t. x is restricted-many-to-many
y is ? -one-to-one
Matching Problem
Insertion
? ?
?
Substitution
Deletion
? ?
??
n
?
m
(1? xunvm)zuv,nm
+
?
n
zuv,n# +
?
m
zuv,#m
?s.t.
minzuv
zuv is monotonic
zuv,nm
Figure 1: An example problem displaying source and target lexicons and alphabets, along with specific matchings.
The variables involved in the optimization problem are diagrammed. x are the alphabet matching indicator variables,
y are the lexicon matching indicator variables, and z are the edit alignment indicator variables. The index u refers to
a word in the source lexicon, v refers to word in the target lexicon, i refers to a character in the source alphabet, and
j refers to a character in the target alhabet. n and m refer to positions in source and target words respectively. The
matching objective function is also shown.
vm. Let zuv be the vector of alignment variables
for the edit distance computation between source
word u and target word v, where entry zuv,nm
indicates whether the character at position n of
source word u is aligned to the character at position
m of target word v. Additionally, define variables
zuv,n# and zuv,#m denoting null alignments, which
will be used to keep track of insertions and deletions.
EDITDIST(u, v;x) =
min
zuv
 ?
(
SUB(zuv, x) + DEL(zuv) + INS(zuv)
)
s.t. zuv is monotonic
We define SUB(zuv, x) to be the number of sub-
stitutions between characters that are not matched
in x, DEL(zuv) to be the number of deletions, and
INS(zuv) to be the number of insertions.
SUB(zuv, x) =
?
n,m
(1? xunvm)zuv,nm
DEL(zuv) =
?
n
zuv,n#
INS(zuv) =
?
m
zuv,#m
Notice that the variable zuv,nm being turned on in-
dicates the substitute operation, while a zuv,n# or
zuv,#m being turned on indicates an insert or delete
operation. These variables are digrammed in Fig-
ure 1. The requirement that zuv be a monotonic
alignment can be expressed using linear constraints,
but in our optimization procedure (described in Sec-
tion 3) these constraints need not be explicitly rep-
resented.
Now we can substitute the explicit edit distance
equation into the implicit matching objective (1).
315
Noticing that the mins and sums commute, we arrive
at the explicit form of the matching optimization
problem.
(2) Explicit Matching Objective:
min
x,y,z
[?
u,v
yuv ?  ?
(SUB(zuv, x) + DEL(zuv) + INS(zuv))
]
s.t. x is restricted-many-to-many
y is ? -one-to-one
?uv zuv is monotonic
The implicit and explicit optimizations are the same,
apart from the fact that the explicit optimization now
explicitly represents the edit alignment variables z.
Let the explicit matching objective (2) be denoted
as J(x, y, z). The relaxation of the explicit problem
with 0-1 constraints removed has integer solutions,2
however the objective J(x, y, z) is non-convex. We
thus turn to a block coordinate descent method in the
next section in order to find local optima.
3 Optimization Method
We now state a block coordinate descent procedure
to find local optima of J(x, y, z) under the con-
straints on x, y, and z. This procedure alternates
between updating y and z to their exact joint optima
when x is held fixed, and updating x to its exact op-
timum when y and z are held fixed.
The psuedocode for the procedure is given in Al-
gorithm 1. Note that the function EDITDIST returns
both the min edit distance euv and the argmin edit
alignments zuv. Also note that cij is as defined in
Section 3.2.
3.1 Lexicon Matching Update
Let x, the alphabet matching variable, be fixed. We
consider the problem of optimizing J(x, y, z) over
the lexicon matching variable y and and the edit
alignments z under the constraint that y is ? -one-
to-one and each zuv is monotonic.
2This can be shown by observing that optimizing x when y
and z are held fixed yields integer solutions (shown in Section
3.2), and similarly for the optimization of y and z when x is
fixed (shown in Section 3.1). Thus, every local optimum with
respect to these block coordinate updates has integer solutions.
The global optimum must be one of these local optima.
Algorithm 1 Block Coordinate Descent
Randomly initialize alphabet matching x.
repeat
for all u, v do
(euv, zuv)? EDITDIST(u, v;x)
end for
[Hungarian]
y ? argminy ? -one-to-one
[?
u,v yuveuv
]
[Solve LP]
x? argmaxx restr.-many-to-many
[?
i,j xijcij
]
until convergence
Notice that y simply picks out which edit distance
problems affect the objective. The zuv in each of
these edit distance problems can be optimized in-
dependently. zuv that do not have yuv active have
no effect on the objective, and zuv with yuv active
can be optimized using the standard edit distance dy-
namic program. Thus, in a first step we compute the
U ? V edit distances euv and best monotonic align-
ment variables zuv between all pairs of source and
target words usingU ?V calls to the standard edit dis-
tance dynamic program. Altogether, this takes time
O
(
(
?
u lu) ? (
?
v lv)
).
Now, in a second step we compute the least
weighted ? -one-to-one matching y under the
weights euv. This can be accomplished in time
O(max(U, V )3) using the Hungarian algorithm
(Kuhn, 1955). These two steps produce y and z that
exactly achieve the optimum value of J(x, y, z) for
the given value of x.
3.2 Alphabet Matching Update
Let y and z, the lexicon matching variables and the
edit alignments, be fixed. Now, we find the optimal
alphabet matching variables x subject to the con-
straint that x is restricted-many-to-many.
It makes sense that to optimize J(x, y, z) with re-
spect to x we should prioritize mappings xij that
would mitigate the largest substitution costs in the
active edit distance problems. Indeed, with a little
algebra it can be shown that solving a maximum
weighted matching problem with weights cij that
count potential substitution costs gives the correct
update for x. In particular, cij is the total cost of
substitution edits in the active edit alignment prob-
316
lems that would result if source character i were not
mapped to target character j in the alphabet match-
ing x. This can be written as:
cij =
?
u,v
?
n,m s.t. un=i,vm=j
 ? yuv ? zuv,nm
If x were constrained to be one-to-one, we
could again apply the Hungarian algorithm, this
time to find a maximum weighted matching under
the weights cij . Since we have instead allowed
restricted-many-to-many alphabet matchings we
turn to linear programming for optimizing x. We
can state the update problem as the following linear
program (LP), which is guaranteed to have integer
solutions:
min
x
?
ij
xijcij
s.t. ?i
?
j
xij ? 2, ?j
?
i
xij ? 2
?
i
?
j
xij = max(I, J)
In experiments we used the GNU Linear Program-
ming Toolkit (GLPK) to solve the LP and update
the alphabet matching x. This update yields match-
ing variables x that achieve the optimum value of
J(x, y, z) for fixed y and z.
3.3 Random Restarts
In practice we found that the block coordinate de-
scent procedure can get stuck at poor local optima.
To find better optima, we run the coordinate descent
procedure multiple times, initialized each time with
a random alphabet matching. We choose the local
optimum with the best objective value across all ini-
tializations. This approach yielded substantial im-
provements in achieved objective value.
4 Experiments
We compare our system to three different state-of-
the-art systems on three different data sets. We set
up experiments that allow for as direct a comparison
as possible. In some cases it must be pointed out
that the past system?s goals are different from our
own, and we will be comparing in a different way
than the respective work was intended. The three
systems make use of additional, or slightly different,
sources of information.
4.1 Phonetic Cognate Lexicons
The first data set we evaluate on consists of 583
triples of phonetic transcriptions of cognates in
Spanish, Portuguese, and Italian. The data set was
introduced by Bouchard-Co?te? et al (2007). For a
given pair of languages the task is to determine the
mapping between lexicons that correctly maps each
source word to its cognate in the target lexicon. We
refer to this task and data set as ROMANCE.
Hall and Klein (2010) presented a state-of-the-
art system for the task of cognate identification and
evaluated on this data set. Their model explicitly
represents parameters for phonetic change between
languages and their parents in a phylogenetic tree.
They estimate parameters and infer the pairs of cog-
nates present in all three languages jointly, while we
consider each pair of languages in turn.
Their model has similarities with our own in that
it learns correspondences between the alphabets of
pairs of languages. However, their correspondences
are probabilistic and implicit while ours are hard and
explicit. Their model also differs from our own in
a key way. Notice that the phonetic alphabets for
the three languages are actually the same. Since
phonetic change occurs gradually across languages
a helpful prior on the correspondence is to favor the
identity. Their model makes use of such a prior.
Our model, on the other hand, is unaware of any
prior correspondence between alphabets and does
not make use of this additional information about
phonetic change.
Hall and Klein (2010) also evaluate their model
on lexicons that do not have a perfect cognate map-
ping. This scenario, where not every word in one
language has a cognate in another, is more realistic.
They produced a data set with this property by prun-
ing words from the ROMANCE data set until only
about 75% of the words in each source lexicon have
cognates in each target lexicon. We refer to this task
and data set as PARTIALROMANCE.
4.2 Lexicons Extracted from Corpora
Next, we evaluate our model on a noisier data set.
Here the lexicons in source and target languages
are extracted from corpora by taking the top 2,000
words in each corpus. In particular, we used the En-
glish and Spanish sides of the Europarl parallel cor-
317
pus (Koehn, 2005). To make this set up more real-
istic (though fairly comparable), we insured that the
corpora were non-parallel by using the first 50K sen-
tences on the English side and the second 50K sen-
tences on the Spanish side. To generate a gold cog-
nate matching we used the intersected HMM align-
ment model of Liang et al (2008) to align the full
parallel corpus. From this alignment we extracted a
translation lexicon by adding an entry for each word
pair with the property that the English word was
aligned to the Spanish in over 10% of the alignments
involving the English word. To reduce this transla-
tion lexicon down to a cognate matching we went
through the translation lexicon by hand and removed
any pair of words that we judged to not be cognates.
The resulting gold matching contains cognate map-
pings in the English lexicon for 1,026 of the words
in the Spanish lexicon. This means that only about
50% of the words in English lexicon have cognates
in the Spanish lexicon. We evaluate on this data set
by computing precision and recall for the number of
English words that are mapped to a correct cognate.
We refer to this task and data set as EUROPARL.
On this data set, we compare against the state-of-
the-art orthographic system presented in Haghighi
et al (2008). Haghighi et al (2008) presents sev-
eral systems that are designed to extract transla-
tion lexicons for non-parallel corpora by learning
a correspondence between their monolingual lexi-
cons. Since our system specializes in matching cog-
nates and does not take into account additional infor-
mation from corpus statistics, we compare against
the version of their system that only takes into ac-
count orthographic features and is thus is best suited
for cognate detection. Their system requires a small
seed of correct cognate pairs. From this seed the sys-
tem learns a projection using canonical correlation
analysis (CCA) into a canonical feature space that
allows feature vectors from source words and target
words to be compared. Once in this canonical space,
similarity metrics can be computed and words can be
matched using a bipartite matching algorithm. The
process is iterative, adding cognate pairs to the seed
lexicon gradually and each time re-computing a re-
fined projection. Our system makes no use of a seed
lexicon whatsoever.
Both our system and the system of Haghighi et
al. (2008) must solve bipartite matching problems
between the two lexicons. For this data set, the lexi-
cons are large enough that finding the exact solution
can be slow. Thus, in all experiments on this data
set, we instead use a greedy competitive linking al-
gorithm that runs in time O(U2V 2log(UV )).
Again, for this dataset it is reasonable to expect
that many characters will map to themselves in the
best alphabet matching. The alphabets are not iden-
tical, but are far from disjoint. Neither our system,
nor that of Haghighi et al (2008) make use of this
expectation. As far as both systems are concerned,
the alphabets are disjoint.
4.3 Decipherment
Finally, we evaluate our model on a data set where
a main goal is to decipher an unknown correspon-
dence between alphabets. We attempt to learn a
mapping from the alphabet of the ancient Semitic
language Ugaritic to the alphabet of Hebrew, and
at the same time learn a matching between Hebrew
words in a Hebrew lexicon and their cognates in a
Ugaritic lexicon. This task is related to the task at-
tempted by Snyder et al (2010). The data set con-
sists of a Ugaritic lexicon of 2,214 words, each of
which has a Hebrew cognate, the lexicon of their
2,214 Hebrew cognates, and a gold cognate dictio-
nary for evaluation. We refer to this task and data set
as UGARITIC.
The non-parameteric Bayesian system of Snyder
et al (2010) assumes that the morphology of He-
brew is known, making use of an inventory of suf-
fixes, prefixes, and stems derived from the words
in the Hebrew bible. It attempts to learn a corre-
spondence between the morphology of Ugaritic and
that of Hebrew while reconstructing cognates for
Ugaritic words. This is a slightly different goal than
that of our system, which learns a correspondence
between lexicons. Snyder et al (2010) run their
system on a set 7,386 Ugaritic words, the same set
that we extracted our 2,214 Ugaritic words with He-
brew cognates from. We evaluate the accuracy of the
lexicon matching produced by our system on these
2,214 Ugaritic words, and so do they, measuring the
number of correctly reconstructed cognates.
By restricting the source and target lexicons to
sets of cognates we have made the task easier. This
was necessary, however, because the Ugaritic and
Hebrew corpora used by Snyder et al (2010) are not
318
Model ? Accuracy
Hall and Klein (2010) ? 90.3
MATCHER 1.0 90.1
Table 1: Results on ROMANCE data set. Our system is
labeled MATCHER. We compare against the phylogenetic
cognate detection system of Hall and Klein (2010). We
show the pairwise cognate accuracy across all pairs of
languages from the following set: Spanish, Portuguese,
and Italian.
comparable: only a small proportion of the words
in the Ugaritic lexicon have cognates in the lexicon
composed of the most frequent Hebrew words.
Here, the alphabets really are disjoint. The sym-
bols in both languages look nothing alike. There is
no obvious prior expectation about how the alpha-
bets will be matched. We evaluate against a well-
established correspondence between the alphabets
of Ugaritic and Hebrew. The Ugaritic alphabet con-
tains 30 characters, the Hebrew alphabet contains 22
characters, and the gold matching contains 33 en-
tries. We evaluate the learned alphabet matching by
counting the number of recovered entries from the
gold matching.
Due to the size of the source and target lexicons,
we again use the greedy competitive linking algo-
rithm in place of the exact Hungarian algorithm in
experiments on this data set.
5 Results
We present results on all four datasets ROMANCE,
PARTIALROMANCE, EUROPARL, and UGARITIC.
On the ROMANCE and PARTIALROMANCE data sets
we compare against the numbers published by Hall
and Klein (2010). We ran an implementation of
the orthographic system presented by Haghighi et
al. (2008) on our EUROPARL data set. We com-
pare against the numbers published by Snyder et al
(2010) on the UGARITIC data set. We refer to our
system as MATCHER in result tables and discussion.
5.1 ROMANCE
The results of running our system, MATCHER, on
the ROMANCE data set are shown in Table 1. We
recover 88.9% of the correct cognate mappings on
the pair Spanish and Italian, 85.7% on Italian and
Portuguese, and 95.6% on Spanish and Portuguese.
Model ? Precision Recall F1
Hall and Klein (2010) ? 66.9 82.0 73.6
MATCHER 0.25 99.7 34.0 50.7
0.50 93.8 60.2 73.3
0.75 81.1 78.0 79.5
Table 2: Results on PARTIALROMANCE data set. Our
system is labeled MATCHER. We compare against the
phylogenetic cognate detection system of Hall and Klein
(2010). We show the pairwise cognate precision, recall,
and F1 across all pairs of languages from the following
set: Spanish, Portuguese, and Italian. Note that approx-
imately 75% of the source words in each of the source
lexicons have cognates in each of the target lexicons.
Our average accuracy across all pairs of languages
is 90.1%. The phylogenetic system of Hall and
Klein (2010) achieves an average accuracy of 90.3%
across all pairs of languages. Our system achieves
accuracy comparable to that of the phylogenetic sys-
tem, despite the fact that the phylogenetic system is
substantially more complex and makes use of an in-
formed prior on alphabet correspondences.
The alphabet matching learned by our system is
interesting to analyze. For the pairing of Span-
ish and Portuguese it recovers phonetic correspon-
dences that are well known. Our system learns the
correct cognate pairing of Spanish /bino/ to Por-
tuguese /vinu/. This pair exemplifies two com-
mon phonetic correspondences for Spanish and Por-
tuguese: the Spanish /o/ often transforms to a /u/ in
Portuguese, and Spanish /b/ often transforms to /v/
in Portuguese. Our system, which allows many-to-
many alphabet correspondences, correctly identifies
the mappings /o/? /u/ and /b/? /v/ as well as the
identity mappings /o/? /o/ and /b/? /b/ which are
also common.
5.2 PARTIALROMANCE
In Table 2 we present the results of running our sys-
tem on the PARTIALROMANCE data set. In this data
set, only approximately 75% of the source words in
each of the source lexicons have cognates in each of
the target lexicons. The parameter ? trades off pre-
cision and recall. We show results for three different
settings of ? : 0.25, 0.5, and 0.75.
Our system achieves an average precision across
language pairs of 99.7% at an average recall of
34.0%. For the pairs Italian ? Portuguese, and Span-
319
Model Seed ? Precision Recall F1
Haghighi et al (2008) 20 0.1 72.0 14.0 23.5
20 0.25 63.6 31.0 41.7
20 0.5 44.8 43.7 44.2
50 0.1 90.5 17.6 29.5
50 0.25 75.4 36.7 49.4
50 0.5 56.4 55.0 55.7
MATCHER 0 0.1 93.5 18.2 30.5
0 0.25 83.2 40.5 54.5
0 0.5 56.5 55.1 55.8
Table 3: Results on EUROPARL data set. Our system
is labeled MATCHER. We compare against the bilingual
lexicon induction system of Haghighi et al (2008). We
show the cognate precision, recall, and F1 for the pair of
languages English and Spanish using lexicons extracted
from corpora. Note that approximately 50% of the words
in the English lexicon have cognates in the Spanish lexi-
con.
ish ? Portuguese, our system achieves prefect preci-
sion at recalls of 32.2% and 38.1% respectively. The
best average F1 achieved by our system is 79.5%,
which surpasses the average F1 of 73.6 achieved by
the phylogenetic system of Hall and Klein (2010).
The phylogenetic system observes the phyloge-
netic tree of ancestry for the three languages and
explicitly models cognate evolution and survival in
a ?survival? tree. One might expect the phyloge-
netic system to achieve better results on this data set
where part of the task is identifying which words do
not have cognates. It is surprising that our model
does so well given its simplicity.
5.3 EUROPARL
Table 3 presents results for our system on the EU-
ROPARL data set across three different settings of ? :
0.1, 0.25, and 0.5. We compare against the ortho-
graphic system presented by Haghighi et al (2008),
across the same three settings of ? , and with two dif-
ferent sizes of seed lexicon: 20 and 50. In this data
set, only approximately 50% of the source words
have cognates in the target lexicon.
Our system achieves a precision of 93.5% at a re-
call of 18.2%, and a best F1 of 55.0%. Using a seed
matching of 50 word pairs, the orthographic sys-
tem of Haghighi et al (2008) achieves a best F1 of
55.7%. Using a seed matching of 20 word pairs,
it achieves a best F1 of 44.2%. Our system out-
performs the orthographic system even though the
orthographic system makes use of important addi-
Model ? Lexicon Acc. Alphabet Acc.
Snyder et al (2010) ? 60.4* 29/33*
MATCHER 1.0 90.4 28/33
Table 4: Results on UGARITIC data set. Our system is la-
beled MATCHER. We compare against the decipherment
system of Snyder et al (2010). *Note that results for this
system are on a somewhat different task. In particular, the
MATCHER system assumes the inventories of cognates in
both Hebrew and Ugaritic are known, while the system
of Snyder et al (2010) reconstructs cognates assuming
only that the morphology of Hebrew is known, which is a
harder task. We show cognate pair identification accuracy
and alphabet matching accuracy for Ugaritic and Hebrew.
tional information: a seed matching of correct cog-
nate pairs. The results show that as the size of
this seed is decreased, the performance of the ortho-
graphic system degrades.
5.4 UGARITIC
In Table 4 we present results on the UGARITIC data
set. We evaluate both accuracy of the lexicon match-
ing learned by our system, and the accuracy of the
alphabet matching. Our system achieves a lexicon
accuracy of 90.4% while correctly identifying 28 out
the 33 gold character mappings.
We also present the results for the decipherment
model of Snyder et al (2010) in Table 4. Note that
while the evaluation data sets for our two models
are the same, the tasks are very different. In par-
ticular, our system assumes the inventories of cog-
nates in both Hebrew and Ugaritic are known, while
the system of Snyder et al (2010) reconstructs cog-
nates assuming only that the morphology of Hebrew
is known, which is a harder task. Even so, the re-
sults show that our system is effective at decipher-
ment when semantically similar lexicons are avail-
able.
6 Conclusion
We have presented a simple combinatorial model
that simultaneously incorporates both a matching
between alphabets and a matching between lexicons.
Our system is effective at both the tasks of cognate
identification and alphabet decipherment, requiring
only lists of words in both languages as input.
320
References
A. Bouchard-Co?te?, P. Liang, T.L. Griffiths, and D. Klein.
2007. A probabilistic approach to diachronic phonol-
ogy. In Proc. of EMNLP.
A. Bouchard-Co?te?, T.L. Griffiths, and D. Klein.
2009. Improved reconstruction of protolanguage word
forms. In Proc. of NAACL.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. Proceedings of ACL.
D. Hall and D. Klein. 2010. Finding cognate groups
using phylogenies. In Proc. of ACL.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of
ACL Workshop on Unsupervised Learning in Natural
Language Processing.
K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems. In
Proc. of COLING/ACL.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proc. of ACL
workshop on Unsupervised lexical acquisition.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In Proc. of Machine Trans-
lation Summit.
G. Kondrak. 2001. Identifying Cognates by Phonetic and
Semantic Similarity. In NAACL.
H.W. Kuhn. 1955. The Hungarian method for the assign-
ment problem. Naval research logistics quarterly.
P. Liang, D. Klein, and M.I. Jordan. 2008. Agreement-
based learning. Proc. of NIPS.
S. Ravi and K. Knight. 2011. Bayesian inference for Zo-
diac and other homophonic ciphers. In Proc. of ACL.
B. Snyder, R. Barzilay, and K. Knight. 2010. A statisti-
cal model for lost language decipherment. In Proc. of
ACL.
321
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 344?354,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cognate Recovery
David Hall and Dan Klein
Computer Science Division
University of California at Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
We present a system for the large scale in-
duction of cognate groups. Our model ex-
plains the evolution of cognates as a sequence
of mutations and innovations along a phy-
logeny. On the task of identifying cognates
from over 21,000 words in 218 different lan-
guages from the Oceanic language family, our
model achieves a cluster purity score over
91%, while maintaining pairwise recall over
62%.
1 Introduction
The critical first step in the reconstruction of an
ancient language is the recovery of related cog-
nate words in its descendants. Unfortunately, this
process has largely been a manual, linguistically-
intensive undertaking for any sizable number of de-
scendant languages. The traditional approach used
by linguists?the comparative method?iterates be-
tween positing putative cognates and then identify-
ing regular sound laws that explain correspondences
between those words (Bloomfield, 1938).
Successful computational approaches have been
developed for large-scale reconstruction of phyloge-
nies (Ringe et al, 2002; Daume? III and Campbell,
2007; Daume? III, 2009; Nerbonne, 2010) and an-
cestral word forms of known cognate sets (Oakes,
2000; Bouchard-Co?te? et al, 2007; Bouchard-Co?te?
et al, 2009), enabling linguists to explore deep his-
torical relationships in an automated fashion. How-
ever, computational approaches thus far have not
been able to offer the same kind of scale for iden-
tifying cognates. Previous work in cognate identi-
fication has largely focused on identifying cognates
in pairs of languages (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kondrak,
2001; Mulloni, 2007), with a few recent exceptions
that can find sets in a handful of languages (Bergsma
and Kondrak, 2007; Hall and Klein, 2010).
While it may seem surprising that cognate de-
tection has not successfully scaled to large num-
bers of languages, the task poses challenges not
seen in reconstruction and phylogeny inference. For
instance, morphological innovations and irregular
sound changes can completely obscure relationships
between words in different languages. However, in
the case of reconstruction, an unexplainable word is
simply that: one can still correctly reconstruct its an-
cestor using words from related languages.
In this paper, we present a system that uses two
generative models for large-scale cognate identi-
fication. Both models describe the evolution of
words along a phylogeny according to automatically
learned sound laws in the form of parametric edit
distances. The first is an adaptation of the genera-
tive model of Hall and Klein (2010), and the other
is a new generative model called PARSIM with con-
nections to parsimony methods in computational bi-
ology (Cavalli-Sforza and Edwards, 1965; Fitch,
1971). Our model supports simple, tractable infer-
ence via message passing, at the expense of being
unable to model some cognacy relationships. To
help correct this deficiency, we also describe an ag-
glomerative inference procedure for the model of
Hall and Klein (2010). By using the output of our
system as input to this system, we can find cognate
groups that PARSIM alone cannot recover.
We apply these models to identifying cognate
groups from two language families using the Aus-
tronesian Basic Vocabulary Database (Greenhill et
al., 2008), a catalog of words from about 40% of
the Austronesian languages. We focus on data from
two subfamilies of Austronesian: Formosan and
344
Oceanic. The datasets are by far the largest on
which automated cognate recovery has ever been at-
tempted, with 18 and 271 languages respectively.
On the larger Oceanic data, our model can achieve
cluster purity scores of 91.8%, while maintaining
pairwise recall of 62.1%. We also analyze the mis-
takes of our system, where we find that some of the
erroneous cognate groups our system finds may not
be errors at all. Instead, they may be previously
unknown cognacy relationships that were not anno-
tated in the data.
2 Background
Before we present our model, we first describe ba-
sic facts of the Austronesian language family, along
with a description of the Austronesian Basic Vocab-
ulary Database, which forms the dataset that we use
for our experiments. For far more detailed coverage
of the Austronesian languages, we direct the inter-
ested reader to Blust (2009)?s comprehensive mono-
graph.
2.1 The Austronesian Language Family
The Austronesian language family is one of the
largest in the world, comprising about one-fifth of
the world?s languages. Geographically, it stretches
from its homeland on Formosa (Taiwan) to Mada-
gascar in the west, and as far as Hawai?i and (at one
point) the Easter Islands to the east. Until the ad-
vent of European colonialism spread Indo-European
languages to every continent, Austronesian was the
most widespread of all language families.
Linguistically, the language family is as diverse
as it is large, but a few regularities hold. From
a phonological perspective, two features stand out.
First, the phoneme inventories of these languages
are typically small. For example, it is well-known
that Hawaiian has only 13 phonemes. Moreover, the
phonotactics of these languages are often restrictive.
Sticking with the same example, Hawaiian only al-
lows (C)V syllables: consonants clusters are forbid-
den, and no syllable may end with a consonant.
2.2 The Austronesian Basic Vocabulary
Database
The Austronesian Basic Vocabulary Database
(ABVD) (Greenhill et al, 2008) is an ambitious, on-
going effort to catalog the lexicons and basic facts
about all of the languages in the Austronesian lan-
guage family. It also contains manual reconstruc-
tions for select ancestor languages produced by lin-
guists.
The sample we use?from Bouchard-Co?te? et al
(2009)?contains about 50,000 words across 471
languages spanning all the major divisions of Aus-
tronesian. These words are grouped into cognate
groups and arranged by gloss. For instance, there are
37 distinct cognate groups for the gloss ?tail.? One
of these groups includes the words /ekor/, /ingko/,
/iNkot/, /kiikiPu/, and /PiPina/, among others. Most
of these words have been transcribed into the Inter-
national Phonetic Alphabet, though it appears that
some words are transcribed using the Roman alpha-
bet. For instance, the second word in the example is
likely /iNko/, which is a much more likely sequence
than what is transcribed.
In this sample, there are 6307 such cognate
groups and 210 distinct glosses. The data is
somewhat sparse: fewer than 50% of the possible
gloss/language pairs are present. Moreover, there is
some amount of homoplasy?that is, languages with
a word from more than one cognate group for a given
gloss.
Finally, it is important to note that the ABVD is
still a work in progress: they have data from only
50% of extant Austronesian languages.
2.3 Subfamilies of Austronesian
In this paper we focus on two branches of the Aus-
tronesian language family, one as a development set
and one as a test set. For our development set, we
use the Formosan branch. The languages in this
group are exclusively found on the Austronesian
homeland of Formosa. The family encompasses a
substantial portion of the linguistic diversity of Aus-
tronesian: Blust (2009) argues that Formosan con-
tains 9 of the 10 first-order splits of the Austrone-
sian family. Formosan?s diversity is surprising since
it contains a mere 18 languages. Thus, Formosan is
a smaller development set that nevertheless is repre-
sentative of larger families.
For our final test set, we use the Oceanic sub-
family, which includes almost 50% of the languages
in the Austronesian family, meaning that it repre-
sents around 10% of all languages in the world.
Oceanic also represents a large fraction of the ge-
345
ographic diversity of Austronesian, stretching from
New Zealand in the south to Hawai?i in the north.
Our sample includes 21863 words from 218 lan-
guages in the Oceanic family.
3 Models
In this section we describe two models, one based on
Hall and Klein (2010)?which we call HK10?and
another new model that shares some connection to
parsimony methods in computational biology, which
we call PARSIM. Both are generative models that
describe the evolution of words w` from a set of lan-
guages {`} in a cognate group g along a fixed phy-
logeny T .1 Each cognate group and word is also
associated with a gloss or meaning m, which we as-
sume to be fixed.2 In both models, words evolve
according to regular sound laws ?`, which are spe-
cific to each language. Also, both models will make
use of a language model ?, which is used for gen-
erating words that are not dependent on the word in
the parent language. (We leave ?` and ? as abstract
parameters for now. We will describe them in sub-
sequent sections.)
3.1 HK10
The first model we describe is a small modification
of the phylogenetic model of Hall and Klein (2010).
In HK10, there is an unknown number of cognate
groups G where each cognate group g consists of a
set of words {wg,`}. In each cognate group, words
evolve along a phylogeny, where each word in a lan-
guage is the result of that word evolving from its
parent according to regular sound laws. To model
the fact that not all languages have a cognate in
each group, each language in the tree has an asso-
ciated ?survival? variable Sg,`, where a word may
be lost on that branch (and its descendants) instead
of evolving. Once the words are generated, they are
then ?permuted? so that the cognacy relationships
1Both of these models therefore are insensitive to geo-
graphic and historical factors that cannot be easily approxi-
mated by this tree. See Nichols (1992) for an excellent dis-
cussion of these factors.
2One could easily envision allowing the meaning of a word
to change as well. Modeling this semantic drift has been consid-
ered by Kondrak (2001). In the ABVD, however, any semantic
drift has already been elided, since the database has coarsened
glosses to the extent that there is no meaningful way to model
semantic drift given our data.
M
G
W
For
W
Ata
? ? ?
S
For
S
Ata
S
Pai
S
Squ
S
Ciu
w
pt
w
es
L 
?
w
Pai
w
Pai
w
Pai
w
Pai
w
Pai
w
Pai
W
Pai
W
Pai
S
u
r
v
i
v
a
l
E
v
o
l
u
t
i
o
n
P
e
r
m
u
t
a
t
i
o
n
(a)
I
Ata
I
Pai
I
Squ
I
Ciu
W
Ata
W
Squ
W
Ciu
W
Pai
(b)
W
For
?
?
?
?
Figure 1: Plate diagrams for (a) HK10 (Hall and Klein,
2010) and (b) PARSIM, our new parsimony model, for
a small set of languages. In HK10, words are generated
following a phylogenetic tree according to sound laws ?,
and then ?scrambled? with a permutation pi so that the
original cognate groups are lost. In PARSIM, all words
for each of the M glosses are generated in a single tree,
with innovations I starting new cognate groups. The
languages depicted are Formosan (For), Paiwan (Pai),
Atayalic (Ata), Ciuli Atayalic (Ciu), and Squliq Atayalic
(Squ).
346
are obscured. The task of inference then is to re-
cover the original cognate groups.
The generative process for their model is as fol-
lows:
? For each cognate group g, choose a root word
Wroot ? p(W |?), a language model over
words.
? For each language ` in a pre-order traversal of
the phylogeny:
1. Choose S` ? Bernoulli(?`), indicating
whether or not the word survives.
2. If the word survives, choose W` ?
p(W |?`,Wpar(`)).
3. Otherwise, stop generating words in that
language and its descendants.
? For each language, choose a random permuta-
tion pi of the observed data, and rearrange the
cognates according to this permutation.
We reproduce the graphical model for HK10 for a
small phylogeny in Figure 1a.
Inference in this model is intractable; to perform
inference exactly, one has to reason over all parti-
tions of the data into cognate groups. To address
this problem, Hall and Klein (2010) propose an it-
erative bipartite matching scheme where one lan-
guage is held out from the others, and then words
are assigned to the remaining groups to maximize
the probability of the attachment. That is, for some
language ` and fixed assignments pi?` for the other
languages, they seek an assignment pi` that maxi-
mizes:
pi? = argmax
pi
?
g
log p(w(`,pi`(g))|?, pi,w?`)
Unfortunately, while this approach was effective
with only a few languages (they tested on three), this
algorithm cannot scale to the eighteen languages in
Formosan, let alne the hundreds of languages in
Oceanic. Therefore, we make two simple modifi-
cations. First, we restrict the cognate assignments
to stay within a gloss. Thus, there are many fewer
potential matchings to consider. Second, we use an
agglomerative inference procedure, which greedily
merges cognate groups that result in the greatest gain
in likelihood. That is, for all pairs of cognate groups
ga with words wa and gb with words wb, we com-
pute the score:
log p(wa?b|?)? log p(wa|?)? log p(wb|?)
This score is the difference between the log proba-
bility of generating two cognate groups jointly and
generating them separately. We then merge the two
that generate the highest gain in likelihood. Like
the iterative bipartite matching algorithm described
above, this algorithm is not exact. However, it is
O(n2 log n) (where n is the size of the largest gloss,
which for Oceanic is 153), while the bipartite match-
ing algorithm is O(n3) (Kuhn, 1955).
Actually, the original HK10 is doubly intractable.
They use weighted automata to represent distribu-
tions over strings, but these automata?particularly
if they are non-deterministic?make inference in
any non-trivial graphical model intractable. We dis-
cuss this issue in more detail in Section 6.
3.2 A Parsimony-Inspired Model
We now describe a new model called PARSIM that
supports exact inference tractably, though it sacri-
fices some of the expressive power of HK10. In
our model, each language has at most one word for
each gloss, and this one word changes from one
language to its children according to some edge-
specific Markov process. These changes may either
be mutations, which merely change the surface form
of the word, or innovations, which start a new word
in a new cognate group that is unrelated to the previ-
ous word. Mutations take the form of a conditional
edit operation that models insertions, substitutions,
and deletions that correspond to regular (and, with
lower probability, irregular) sound changes that are
likely to occur between a language and its parent.
Innovations, on the other hand, are generated from a
language model independent of the parent?s word.
Specifically, our generative process takes the fol-
lowing form:
? For each gloss m, choose a root word Wroot ?
?, a language model over words.
? For each language ` in a pre-order traversal of
the phylogeny:
347
malapzo mappo
Ciuli Squliq
purrok
Paiwan
Rukai
pouroukou
Figure 2: A small example of how PARSIM works.
Listed here are the words for ?ten? in four languages
from the Formosan family, along with the tree that ex-
plains them. The dashed line indicates an innovation on
the branch.
1. Choose I` ? Bernoulli(?`), indicating
whether or not the word is an innovation
or a mutation.
2. If it is a mutation, choose W` ?
p(W |?`,Wpar(`)).
3. Otherwise, choose W` ? ?.
We also depict our model as a plate diagram for a
small phylogeny in Figure 1b.
Because there is only one tree per gloss, there
is no assignment problem to consider, which is the
main source of the intractability of HK10. Instead,
pieces of the phylogeny are simply ?cut? into sub-
trees whenever an innovation occurs. Thus, message
passing can be used to perform inference.
As an example of how our process works, con-
sider Figure 2. The Formosan word for ?ten?
probably resembled either /purrok/ or /pouroukou/.
There was an innovation in Ciuli and Squliq?s an-
cestor Atayalic that produced a new word for ten.
This word then mutated separately into the words
/malapzo/ and /mappo/, respectively.
4 Relation to Parsimony
PARSIM is related to the parsimony principle
from computational biology (Cavalli-Sforza and Ed-
wards, 1965; Fitch, 1971), where it is used to search
for phylogenies. When using parsimony, a phy-
logeny is scored according to the derivation that re-
quires the fewest number of changes of state, where
a state is typically thought of as a gene or some other
trait in a species. These genes are typically called
?characters? in the computational biology literature,
and two species would have the same value for a
character if they share the same property that that
state represents.
When inducing phylogenies of languages, a natu-
ral choice for characters are glosses from a restricted
vocabulary like a Swadesh list, and two words are
represented as the same value for a character if they
are cognate (Ringe et al, 2002). Other features can
be used (Daume? III and Campbell, 2007; Daume? III,
2009), but they are not relevant to our discussion.
Consider the small example in Figure 3a with just
four languages. Here, cognacy is encoded using
characters. In this example, at least two changes of
state are required to explain the data: both C and B
must have evolved from A. Therefore, the parsimony
score for this tree is two.
Of course, there is no reason why all changes
should be equally likely. For instance, it might be
extremely likely that B changes into both A and
C, but that A never changes into B or C, and so
weighted variants of parsimony might be neces-
sary (Sankoff and Cedergren, 1983).
With this in mind, PARSIM can be thought of a
weighted variant of parsimony, with two differences.
First, the characters do not indicate ahead of time
which words are related. Instead, the characters are
the words themselves. Second, the transitions be-
tween different states (words) are not uniform. In-
stead, they are weighted by the log probability of
one word changing into another, including both mu-
tations and innovations.
Thus, the task of inference in PARSIM is to find
the most ?parsimonious? explanation for the words
we have observed, which is the same as finding the
most likely derivation. Because the distances be-
tween words (that is, the transition probabilities)
are not known ahead of time, they must instead be
learned, which we discuss in Section 7.3
5 Limitations of the Parsimony Model
Potentially, our parsimony model sacrifices a cer-
tain amount of power to make inference tractable.
Specifically, it cannot model homoplasy, the pres-
ence of more than one word in a language for a given
3It is worth noting that we are not the first to point out a
connection between parsimony and likelihood. Indeed, many
authors in the computational biology literature have formally
demonstrated a connection (Farris, 1973; Felsenstein, 1973).
348
A B C A
A
A
A
(a)
A B B A
{A,B}
{A,B}
(b)
{A,B}
A B B A
A
(c)
A
A
A A
A
(d)
A
A
B B
B
B
B
Figure 3: Trees illustrating parsimony and its limitations. In these trees, there are four languages, with words A, B, and
C in various configurations. (a) The most parsimonious derivation for this tree has all intermediate states as A. There
are thus two changes. (b) An example of homoplasy. Here, given this tree, it seems likely that the ancestral languages
contained both A and B. (c) PARSIM cannot recover the example from (b), and so it encodes two innovations (shown
as dashed lines). (d) The HK10 model can recover this relationship, but this power makes the model intractable.
gloss. Homoplasy can arise for a variety of reasons
in phylogenetic models of cognates, and we describe
some in this section.
Consider the example illustrated in Figure 3b,
where the two central languages share a cognate, as
do the two outer languages. This is the canonical ex-
ample of homoplasy, and PARSIM cannot correctly
recover this grouping. Instead, it can at best only se-
lect group A or group B as the value for the parent,
and leave the other group fragmented as two innova-
tions, as in Figure 3c. On the other hand, HK10 can
recover this relationship (Figure 3d), but this power
is precisely what makes it intractable.
There are two reasons this kind of homoplasy
could arise. The first is that there were indeed two
words in the parent language for this gloss, or that
there were two words with similar meanings and
the two meanings drifted together. Second, the tree
could be an inadequate model of the evolution in
this case. For instance, there could have been a cer-
tain amount of borrowing between two of these lan-
guages, or there was not a single coherent parent lan-
guage, but rather a language continuum that cannot
be explained by any tree.
However, homoplasy seems to be relatively un-
common (though not unheard of) in the Oceanic and
Formosan families. Where it does appear, our model
should simply fail to get one of the cognate groups,
instead explaining all of them via innovation. To
repair this shortcoming, we can simply run the ag-
glomerative clustering procedure for the model of
Hall and Klein (2010), starting from the groups that
PARSIM has recovered. Using this procedure, we
can hopefully recover many of the under-groupings
caused by homoplasy.
6 Inference and Scale
6.1 Inference
In this section we describe the basics of infer-
ence in the PARSIM model. We have a nearly
tree-structured graphical model (Figure 1); it is
not a tree only because of the innovation param-
eters. Therefore, we apply the common trick of
grouping variables to form a tree. Specifically, we
group each word variable W` with its innovation
parameter I`. The distribution of interest is then
p(W`, I`|Wpar(`), ?`, ?`), and the primary operation
is summing out messages ? from the children of a
language and sending a new message to its parent:
?`(wpar(`)) =
?
w`
p(w`|?)
?
`? ? child(`)
?`?(w`)
p(w`|?) = p(w`|I` = 0, wpar(`), ?`)p(I` = 0|?`)
+ p(w`|I` = 1, ?`)P (I` = 1|?`) (1)
The first term involves computing the probability of
the word mutating from its parent, and the second
involves the probability of the child word from a lan-
guage model. We describe the parameters and pro-
cedures for these operations in 7.1.
6.2 Scale
Even though inference by message-passing in our
model is tractable, we needed to make certain con-
cessions to make inference acceptably fast. These
choices mainly affect how we represent distributions
over strings.
349
First, we need to model distributions and mes-
sages over words on the internal nodes of a phy-
logeny. The natural choice in this scenario is to use
weighted finite automata (Mohri et al, 1996). Au-
tomata have been used to successfully model distri-
butions of strings for inferring morphology (Dreyer
and Eisner, 2009) as well as cognate detection (Hall
and Klein, 2010). Even in models that would be
tractable with ?ordinary? messages, inference with
automata quickly becomes intractable, because the
size of the automata grow exponentially with the
number of messages passed. Therefore, approxima-
tions must be used. Dreyer and Eisner (2009) used
a mixture of a k-best list and a unigram language
model, while Hall and Klein (2010) used an approx-
imation procedure that projected complex automata
to simple, tractable automata using a modified KL
divergence.
While either approach could be used here in prin-
ciple, we found that automata machinery was simply
too slow for our application. Instead, we exploit the
intuition that we do not need to accurately recon-
struct the word for any ancestral language. More-
over, it is inefficient to keep track of probabilities for
all strings. Therefore, we only track scores for words
that actually exist in a given gloss, which means that
internal nodes only have mass on those words. That
is, if a gloss has 10 distinct words across all the lan-
guages in our dataset, we pass messages that only
contain information about those 10 words.
Now, this representation?while more efficient
than the automata representations?results in infer-
ence that is still quadratic in the number of words
in a gloss, since we have distributions of the form
p(w`|wpar(`), ?`). Intuitively, it is unlikely that a
word from one distant branch of tree resembles a
word in another branch. Therefore, rather than score
all of these unlikely words, we use a beam where we
only factor in words whose score is at most a fac-
tor of e?10 less than the maximum score. Our initial
experiments found that using a beam provides large
savings in time with little impact on prediction qual-
ity.
7 Learning
PARSIM has three kinds of parameters that we need
to learn: the mutation parameters ?`, the innovation
probabilities ?`, and the global language model ?
for generating new words. We learn these parame-
ters via Expectation Maximization (Dempster et al,
1977), iterating between computing expected counts
and adjusting parameters to maximize the posterior
probability of the parameters. In this section, we de-
scribe those parameters.
7.1 Sound Laws
The core piece of our system is learning the sound
laws associated with each edge. Since the founda-
tion of historical linguists with the neogrammari-
ans, linguists have argued for the regularity of sound
change at the phonemic level (Schleicher, 1861;
Bloomfield, 1938). That is to say, if in some lan-
guage a /t/ changes to a /d/ in some word, it is al-
most certain that it will change in every other place
that has the same surrounding context.
In practice, of course, sound change is not entirely
regular, and complex extralinguistic events can lead
to sound changes that are irregular. For example,
in some cultures in which Oceanic languages are
spoken, the name of the chief is taboo: one cannot
speak his name, nor say any word that sounds too
much like his name. Speakers of these languages
do find ways around this prohibition, often resulting
in sound changes that cannot be explained by sound
laws alone (Keesing and Fifi?i, 1969).
Nevertheless, we find it useful to model sound
change as a largely regular if stochastic process.
We employ a sound change model whose expressive
power is equivalent to that of Hall and Klein (2010),
though with a different parameterization. We model
the evolution of a word w` to its child w`? as a
sequence of unigram edits that include insertions,
deletions, and substitutions. Specifically, we use a
standard three-state pair hidden Markov model that
is closely related to the classic alignment algorithm
of Needleman and Wunsch (1970) (Durbin et al,
2006).
The three states in this HMM correspond to
matches/substitutions, insertions, and deletions. The
transitions are set up such that insertions and dele-
tions cannot be interleaved. This prevents spurious
equivalent alignments, which would cause the model
to assign unnecessarily higher probability to transi-
tions with many insertions and deletions.
Actually learning these parameters involves learn-
350
ing the transition probabilities of this HMM (which
model the overall probability of insertion and dele-
tion) as well as the emission probabilities (which
model the particular edits). Because there are rel-
atively few words for each language (96 on average
in Oceanic), we found it important to tie together
the parameters for the various languages, in contrast
to Hall and Klein (2010) who did not. In our maxi-
mization step, we fit a joint log-linear model for each
language, using features that are both specific to a
language and shared across languages. Our features
included indicators on each substitution, insertion,
and deletion operation, along with an indicator for
the outcome of each edit operation. This last fea-
ture reflects the propensity of a particular phoneme
to appear in a given language at all, no matter what
its ancestral phoneme was. This parameterization
is similar to the one used in the reconstruction sys-
tem of Bouchard-Co?te? et al (2009), except that they
used edit operations that conditioned on the context
of the surrounding word, which is crucial when try-
ing to accurately reconstruct ancestral word forms.
To encourage parameter sharing, we used an `2 reg-
ularization penalty.
7.2 Innovation Parameters
The innovation parameters ?` are parameters for
simple Bernoulli distribution that govern the propen-
sity for a language to start a new word. These pa-
rameters can be learned separately, though due to
data sparsity, we found it better to use a tied param-
eterization as with the sound laws. Specifically, we
fit a log linear model whose features are indicators
on the specific language, as well as a global inno-
vation parameter that is shared across all languages.
As with the sound laws, we used an `2 regularization
penalty to encourage the use of the global innovation
parameter.
7.3 Language Model
Finally, we have a single language model ? that is
also shared across all languages. ? is a simple bi-
gram language model over characters in the Interna-
tional Phonetic Alphabet. ? is used when generating
new words either via innovation or from the root of
the tree.
In principle, we could of course have language
models specific to each language, but because there
Formosan
System Prec Recall F1 Purity
Agg. HK10 77.6 83.2 80.0 84.7
PARSIM 87.8 71.0 78.5 94.6
Combination 85.2 81.3 83.2 92.3
Oceanic
System Prec Recall F1 Purity
PARSIM 84.4 62.1 71.5 91.8
Combination 76.0 73.8 74.9 85.5
Table 1: Results on the Formosan and Oceanic fami-
lies. PARSIM is the new parsimony model in this pa-
per, Agg. HK10 is our agglomerative variant of Hall and
Klein (2010) and Combination uses PARSIM?s output to
seed the agglomerative matcher. For the agglomerative
systems, we report the point with maximal F1 score, but
we also show precision/recall curves. (See Figure 4.)
are so few words per language, we found that
branch-specific language models caused the model
to prefer to innovate at almost every node since the
language models could essentially memorize the rel-
atively small vocabularies of these languages.
8 Experiments
8.1 Cognate Recovery
We ran both PARSIM and our agglomerative ver-
sion of HK10 on the Formosan datasets. For PAR-
SIM, we initialized the mutation parameters ? to a
model that preferred matches to insertions, substi-
tutions and deletions by a factor of e3, innovation
parameters to 0.5, and the language model to a uni-
form distribution over characters. For the agglomer-
ative HK10, we initialized its parameters to the val-
ues found by our model.4
Based on our observations about homoplasy, we
also considered a combined system where we ran
PARSIM, and then seeded the agglomerative cluster-
ing algorithm with the clusters found by PARSIM.
For evaluation, we report a few metrics. First,
we report cluster purity, which is a kind of pre-
cision measure for clusterings. Specifically, each
cluster is assigned to the cognate group that is the
most common cognate word in that group, and then
purity is computed as the fraction of words that
4Attempts to learn parameters directly with the agglomera-
tive clustering algorithm were not effective.
351
0.6 
0.7 
0.8 
0.9 
1 
0.4 0.5 0.6 0.7 0.8 0.9 1 
Pre
cisi
on 
Recall 
Combined System 
PARSIM 
Agg. HK10 
Figure 4: Precision/Recall curves for our systems. The
Combined System starts from PARSIM?s output, so it
has fewer points to plot, and starts from a point with
lower precision. As PARSIM outputs only one result, it
is starred.
are in a cluster whose gold cognate group matches
the cognate group of the cluster. For gold parti-
tions G = {G1, G2, . . . , Gg} and found partitions
F = {F1, F2, . . . , Ff}, we have: purity(G,F ) =
1
N
?
f maxg |Gg?Ff |. We also report pairwise pre-
cision and recall computed over pairs of words.5 Fi-
nally, because agglomerative clustering does not de-
fine a natural ?stopping point? other than when the
likelihood gain decreases to 0?which did not per-
form well in our initial tests?we will report both
a precision/recall curve, as well the maximum pair-
wise F1 obtained by the agglomerative HK10 and
the combined system.
The results are in Table 1. On Formosan, PAR-
SIM has much higher precision and purity than our
agglomerative version of HK10 at its highest point,
though its recall and F1 suffer somewhat. Of course,
the comparison is not quite fair, since we have se-
lected the best possible point for HK10.
However, our combination of the two systems
does even better. By feeding our high-precision re-
sults into the agglomerative system and sacrificing
just a little precision, our combined system achieves
much higher F1 scores than either of the systems
alone.
Next, we also examined precision and recall
curves for the two agglomerative systems on For-
5The main difference between precision and purity is that
pairwise precision is inherently quadratic, meaning that it pe-
nalizes mistakes in large groups much more heavily than mis-
takes in small groups.
mosan, which we have plotted in Figure 4, along
with the one point output by PARSIM.
We then ran PARSIM and the combined system
on the much larger Oceanic dataset. Performance
on all metrics decreased somewhat, but this is to be
expected since there is so much more data. As with
Formosan, PARSIM has higher precision than the
combined system, but it has much lower recall.
8.2 Reconstruction
We also wanted to see how well our cognates could
be used to actually reconstruct the ancestral forms of
words. To do so, we ran a version of Bouchard-Co?te?
et al (2009)?s reconstruction system using both the
cognate groups PARSIM found in the Oceanic lan-
guage family and the gold cognate groups provided
by the ABVD. We then evaluated the average Leven-
shtein distance of the reconstruction for each word
to the reconstruction of that word?s Proto-Oceanic
ancestor provided by linguists. Our evaluation dif-
fers from Bouchard-Co?te? et al (2009) in that they
averaged over cognate groups, which does not make
sense for our task because there are different cognate
groups. Instead, we average over per-modern-word
reconstruction error.
Using this metric, reconstructions using our sys-
tem?s cognates are an average of 2.47 edit opera-
tions from the gold reconstruction, while with gold
cognates the error is 2.19 on average. This repre-
sents an error increase of 12.8%. To see if there
was some pattern to these errors, we also plotted the
fraction of words with each Levenshtein distance for
these reconstructions in Figure 5. While the plots are
similar, the automatic cognates exhibit a longer tail.
Thus, even with automatic cognates, the reconstruc-
tion system can reconstruct words faithfully in many
cases, but in a few instances our system fails.
9 Analysis
We now consider some of the errors made by our
system. Broadly, there are two kinds of mistakes
in a model like ours: those affecting precision and
those affecting recall.
9.1 Precision
Many of our precision errors seem to be due to
our somewhat limited model of sound change. For
instance, the language Pazeh has two words for
352
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ?
Fr
act
ion
 of
 W
ord
s 
Levenshtein Distance 
Automatic Cognates 
Gold Cognates 
Figure 5: Percentage of words with varying levels of
Levenshtein distance from the gold reconstruction. Gold
Cognates were hand-annotated by linguists, while Auto-
matic Cognates were found by our system.
?to sleep:? /mudamai/ and /mid@m/. Somewhat
surprisingly the former word is cognate with Pai-
wan /qmereN/ and Saisiat /maPr@m/ while the lat-
ter is not. Our system, however, makes the mistake
of grouping /mid@m/ with the Paiwan and Saisiat
words. Our system has inferred that the insertions of
/u/ and /ai/ (which are required to bring /mudamai/
into alignment with the Saisiat and Paiwan words)
are less likely than substituting a few vowels and the
consonant /r/ for /d/ (which are required to align
/mid@m/). Perhaps a more sophisticated model of
sound change could correctly learn this relationship.
However, a preliminary inspection of the data
seems to indicate that not all of our precision errors
are actually errors, but rather places where the data
is insufficiently annotated (and indeed, the ABVD is
still a work in progress). For instance, consider the
words for ?meat/flesh? in the Formosan languages:
Squliq /hiP/, Bunun /titiP/, Paiwan /seti/, Kavalan
/PisiP/, CentralAmi /titi/, Our system groups all of
these words except for Squliq /hiP/. However, de-
spite these words? similarity, there are actually three
cognate groups here. One includes Squliq /hiP/ and
Kavalan /PisiP/, another includes just Paiwan /seti/,
and the third includes Bunun /titiP/ and CentralAmi
/titi/. Crucially, these cognate groups do not fol-
low the phylogeny closely. Thus, either there was a
significant amount of borrowing between these lan-
guages, or there was a striking amount of homoplasy
in Proto-Formosan, or these words are in fact mostly
cognate. While a more thorough, linguistically-
informed analysis is needed to ensure that these are
actually cognates, we believe that our system, in
conjunction with a trained Austronesian specialist,
could potentially find many more cognate groups,
speeding up the process of completing the ABVD.
9.2 Recall
Our system can also fail to group words that should
be grouped. One recurring problem seems to
be reduplication, which is a fairly common phe-
nomenon in Austronesian languages. For instance,
there is a cognate group for ?to eat? that includes
Bunun /maun/, Thao /kman/, Favorlang /man/, and
Sediq /manakamakan/, among others. Our system
correctly finds this group, with the exception of
/manakamakan/, which is clearly the result of redu-
plication. Reduplication cannot be modeled using
mere sound laws, and so a more complex transition
model is needed to correctly identify these kinds of
changes.
10 Conclusion
We have presented a new system for automatically
finding cognates across many languages. Our sys-
tem is comprised of two parts. The first, PAR-
SIM, is a new high-precision generative model with
tractable inference. The second, HK10, is a mod-
ification of Hall and Klein (2010) that makes their
approximate inference more efficient. We discuss
certain trade-offs needed to make both models scale,
and demonstrated its performance on the Formosan
and Oceanic language families.
References
Shane Bergsma and Greg Kondrak. 2007. Multilingual
cognate identification using integer linear program-
ming. In RANLP Workshop on Acquisition and Man-
agement of Multilingual Lexicons, Borovets, Bulgaria,
September.
Leonard Bloomfield. 1938. Language. Holt, New York.
R. A. Blust. 2009. The Austronesian languages. Aus-
tralian National University.
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic approach
to diachronic phonology. In EMNLP.
Alexandre Bouchard-Co?te?, Thomas L. Griffiths, and Dan
Klein. 2009. Improved reconstruction of protolan-
guage word forms. In NAACL, pages 65?73.
L. L. Cavalli-Sforza and A. W. F. Edwards. 1965. Analy-
sis of human evolution. In S. J. Geerts Genetics Today,
353
editor, Proceedings of XIth International Congress of
Genetics, 1963, Vol, page 923?933. 3, 3.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational Lin-
guistics (ACL).
Hal Daume? III. 2009. Non-parametric Bayesian areal
linguistics. In NAACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In EMNLP, Singapore, Au-
gust.
R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological sequence analysis. eleventh edition.
James S. Farris. 1973. On Comparing the Shapes of
Taxonomic Trees. Systematic Zoology, 22(1):50?54,
March.
J. Felsenstein. 1973. Maximum likelihood and mini-
mum steps methods for estimating evolutionnary trees
from data on discrete characters. Systematic Zoology,
23:240?249.
W. M. Fitch. 1971. Toward defining the course of evo-
lution: minimal change for a specific tree topology.
Systematic Zoology, 20:406?416.
S.J. Greenhill, R. Blust, and R.D. Gray. 2008. The
Austronesian basic vocabulary database: from bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271?283.
David Hall and Dan Klein. 2010. Finding cognates using
phylogenies. In Association for Computational Lin-
guistics (ACL).
Robert M. Keesing and Jonathan Fifi?i. 1969. Kwaio
word tabooing in its cultural context. Journal of the
Polynesian Society, 78(2):154?177.
Grzegorz Kondrak. 2001. Identifying cognates by pho-
netic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguistics,
20(3):381?417.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
NAACL.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop. John Wiley and Sons.
Andrea Mulloni. 2007. Automatic prediction of cognate
orthography using support vector machines. In ACL,
pages 25?30.
Saul B. Needleman and Christian D. Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
Molecular Biology, 48(3):443 ? 453.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
J. Nichols. 1992. Linguistic diversity in space and time.
University of Chicago Press.
Michael P. Oakes. 2000. Computer estimation of vocab-
ulary in a protolanguage from word lists in four daugh-
ter languages. Quantitative Linguistics, 7(3):233?243.
Don Ringe, Tandy Warnow, and Ann Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100(1):59?129.
D. Sankoff and R. J. Cedergren, 1983. Simultaneuous
comparison of three or more sequences related by a
tree, page 253?263. Addison-Wesley, Reading, MA.
August Schleicher. 1861. A Compendium of the Com-
parative Grammar of the Indo-European, Sanskrit,
Greek and Latin Languages.
354
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1?11, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Transfer Using a Bilingual Lexicon
Greg Durrett, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,adpauls,klein}@cs.berkeley.edu
Abstract
We consider the problem of using a bilingual
dictionary to transfer lexico-syntactic infor-
mation from a resource-rich source language
to a resource-poor target language. In con-
trast to past work that used bitexts to trans-
fer analyses of specific sentences at the token
level, we instead use features to transfer the
behavior of words at a type level. In a dis-
criminative dependency parsing framework,
our approach produces gains across a range
of target languages, using two different low-
resource training methodologies (one weakly
supervised and one indirectly supervised) and
two different dictionary sources (one manu-
ally constructed and one automatically con-
structed).
1 Introduction
Building a high-performing parser for a language
with no existing treebank is still an open problem.
Methods that use no supervision at all (Klein and
Manning, 2004) or small amounts of manual su-
pervision (Haghighi and Klein, 2006; Cohen and
Smith, 2009; Naseem et al2010; Berg-Kirkpatrick
and Klein, 2010) have been extensively studied, but
still do not perform well enough to be deployed
in practice. Projection of dependency links across
aligned bitexts (Hwa et al2005; Ganchev et al
2009; Smith and Eisner, 2009) gives better perfor-
mance, but crucially depends on the existence of
large, in-domain bitexts. A more generally appli-
cable class of methods exploits the notion of univer-
sal part of speech tags (Petrov et al2011; Das and
...   the    senators    demand    strict   new    ethics    rules   ...
      DT      NNS          VBP          JJ       JJ       NNS     NNS   
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
          NN                  VVFIN             NN          APPR    ART       NN
       Unions               demand     abandonment     on       the      reform
Figure 1: Sentences in English and German both contain-
ing words that mean ?demand.? The fact that the English
demand takes nouns on its left and right indicates that the
German verlangen should do the same, correctly suggest-
ing attachments to Verzicht and Gewerkschaften.
Petrov, 2011) to train parsers that can run on any lan-
guage with no adaptation (McDonald et al2011)
or unsupervised adaptation (Cohen et al2011).
While these universal parsers currently constitute
the highest-performing methods for languages with-
out treebanks, they are inherently limited by operat-
ing at the coarse POS level, as lexical features are
vital to supervised parsing models.
In this work, we consider augmenting delexical-
ized parsers by transferring syntactic information
through a bilingual lexicon at the word type level.
These parsers are delexicalized in the sense that, al-
though they receive target language words as input,
their feature sets do not include indicators on those
words. This setting is appropriate when there is too
little target language data to learn lexical features di-
rectly. Our main approach is to add features which
are lexical in the sense that they compute a function
of specific target language words, but are still un-
1
lexical in the sense that all lexical knowledge comes
from the bilingual lexicon and training data in the
source language.
Consider the example English and German sen-
tences shown in Figure 1, and suppose that we wish
to parse the German side without access to a Ger-
man treebank. A delexicalized parser operating at
the part of speech level does not have sufficient in-
formation to make the correct decision about, for ex-
ample, the choice of subcategorization frame for the
verb verlangen. However, demand, a possible En-
glish translation of verlangen, takes a noun on its
left and a noun on its right, an observation that in this
case gives us the information we need. We can fire
features in our German parser on the attachments
of Gewerkschaften and Verzicht to verlangen indi-
cating that similar-looking attachments are attested
in English for an English translation of verlangen.
This allows us to exploit fine-grained lexical cues to
make German parsing decisions even when we have
little or no supervised German data; moreover, this
syntactic transfer is possible even in spite of the fact
that demand and verlangen are not observed in par-
allel context.
Using type-level transfer through a dictionary in
this way allows us to decouple the lexico-syntactic
projection from the data conditions under which we
are learning the parser. After computing feature val-
ues using source language resources and a bilingual
lexicon, our model can be trained very simply us-
ing any appropriate training method for a supervised
parser. Furthermore, because the transfer mecha-
nism is just a set of features over word types, we are
free to derive our bilingual lexicon either from bitext
or from a manually-constructed dictionary, making
our method strictly more general than those of Mc-
Donald et al2011) or Ta?ckstro?m et al2012), who
rely centrally on bitext. This flexibility is potentially
useful for resource-poor languages, where a human-
curated bilingual lexicon may be broader in cover-
age or more robust to noise than a small, domain-
limited bitext. Of course, it is an empirical question
whether transferring type level information about
word behavior is effective; we show that, indeed,
this method compares favorably with other transfer
mechanisms used in past work.
The actual syntactic information that we transfer
consists of purely monolingual lexical attachment
statistics computed on an annotated source language
resource.1 While the idea of using large-scale sum-
mary statistics as parser features has been consid-
ered previously (Koo et al2008; Bansal and Klein,
2011; Zhou et al2011), doing so in a projection set-
ting is novel and forces us to design features suitable
for projection through a bilingual lexicon. Our fea-
tures must also be flexible enough to provide benefit
even in the presence of cross-lingual syntactic dif-
ferences and noise introduced by the bilingual dic-
tionary.
Under two different training conditions and with
two different varieties of bilingual lexicons, we
show that our method of lexico-syntactic projection
does indeed improve the performance of parsers that
would otherwise be agnostic to lexical information.
In all settings, we see statistically significant gains
for a range of languages, with our method providing
up to 3% absolute improvement in unlabeled attach-
ment score (UAS) and 11% relative error reduction.
2 Model
The projected lexical features that we propose in this
work are based on lexicalized versions of features
found in MSTParser (McDonald et al2005), an
edge-factored discriminative parser. We take MST-
Parser to be our underlying parsing model and use it
as a testbed on which to evaluate the effectiveness of
our method for various data conditions.2 By instanti-
ating the basic MSTParser features over coarse parts
of speech, we construct a state-of-the-art delexical-
ized parser in the style of McDonald et al2011),
where feature weights can be directly transferred
from a source language or languages to a desired
target language. When we add projected lexical fea-
tures on top of this baseline parser, we do so in a
way that does not sacrifice this generality: while
our new features take on values that are language-
specific, they interact with the model at a language-
independent level. We therefore have the best of
1Throughout this work, we will use English as the source
language, but it is possible to use any language for which the
appropriate bilingual lexicons and treebanks exist. One might
expect to find the best performance from using a source lan-
guage closely related to the target.
2We train MSTParser using the included implementation of
MIRA (Crammer and Singer, 2001) and use projective decoding
for all experiments described in this paper.
2
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, L 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, L [VERB]?CHILD, DIR 0.711
VERB?Gewerkschaften PARENT? [NOUN] 0.822
??? ??? ???
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
        NOUN               VERB           NOUN        ADP    DET     NOUN
        Unions              demand     abandonment     on       the      reform
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, R 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, R [VERB]?CHILD, DIR 0.521
VERB?Verzicht PARENT?[NOUN] 0.623
??? ??? ???
Figure 2: Computation of features on a dependency arc. DELEX features are indicators over characteristics of depen-
dency links that do not involve the words in the sentence. PROJ features are real-valued analogues of DELEX features
that do contain words. We form a query from each stipulated set of characteristics, compute the values of these queries
heuristically, and then fire a feature based on each query?s signature. Signatures indicate which attachment properties
were considered, which part of the query was lexicalized (shown by brackets here), and the POS of the query word.
This procedure yields a small number of real-valued features that still capture rich lexico-syntactic information.
two worlds in that our features can be learned on
any treebank or treebanks that are available to us,
but still exploit highly specific lexical information
to achieve performance gains over using coarse POS
features alone.
2.1 DELEX Features
Our DELEX feature set consists of all of the unlexi-
calized features in MSTParser, only lightly modified
to improve performance for our setting. McDonald
et al2005) present three basic types of such fea-
tures, ATTACH, INBETWEEN, and SURROUNDING,
which we apply at the coarse POS level. The AT-
TACH features for a given dependency link consist of
indicators of the tags of the head and modifier, sep-
arately as well as together. The INBETWEEN and
SURROUNDING features are indicators on the tags
of the head and modifier in addition to each inter-
vening tag in turn (INBETWEEN) or various com-
binations of tags adjacent to the head or modifier
(SURROUNDING).3
MSTParser by default also includes a copy of
each of these indicator features conjoined with
the direction and distance of the attachment it de-
notes. These extra features are important to getting
3As in Koo et al2008), our feature set contains more
backed-off versions of the SURROUNDING features than are de-
scribed in McDonald et al2005).
good performance out of the baseline model. We
slightly modify the conjunction scheme and expand
it with additional backed-off conjunctions, since
these changes lead to features that empirically trans-
fer better than the MSTParser defaults. Specifically,
we use conjunctions with attachment direction (left
or right), coarsened distance,4 and attachment direc-
tion and coarsened distance combined.
We emphasize again that these baseline features
are entirely standard, and all the DELEX feature set
does is recreate an MSTParser-based analogue of the
direct transfer parser described by McDonald et al
(2011).
2.2 PROJ Features
We will now describe how to compute our projected
lexical features, the PROJ feature set, which con-
stitutes the main contribution of this work. Recall
that we wish our method to be as general as possible
and work under many different training conditions;
in particular, we wish to be able to train our model
on only existing treebanks in other languages when
no target language trees are available (discussed in
Section 3.3), or on only a very small target language
treebank (Section 3.4). It would greatly increase
the power of our model if we were able to include
target-language-lexicalized versions of the ATTACH
4Our five distance buckets are {1, 2, 3?5, 6?10, 11+}.
3
features, but these are not learnable without a large
target language treebank. We instead must augment
our baseline model with a relatively small number of
features that are nonetheless rich enough to transfer
the necessary lexical information.
Our overall approach is sketched in Figure 2,
where we show the features that fire on two pro-
posed edges in a German dependency parse. Fea-
tures on an edge in MSTParser incorporate a sub-
set of observable properties about that edge?s head,
modifier, and context in the sentence. For sets of
properties that do not include a lexical item, such
as VERB?NOUN, we fire an indicator feature from
the DELEX feature set. For those that do include a
lexical item, such as verlangen?NOUN, we form a
query, which resembles a lexicalized indicator fea-
ture. Rather than firing the query as an indicator
feature directly, which would result in a model pa-
rameter for each target word, we fire a broad feature
called an signature whose value reflects the specifics
of the query (computation of these values is dis-
cussed in Section 2.2.2). For example, we abstract
verlangen?NOUN to [VERB]?CHILD, with square
brackets indicating the element that was lexicalized.
Section 2.2.1 discusses this coarsening in more de-
tail. The signatures are agnostic to individual words
and even the language being parsed, so they can be
learned on small amounts of data or data from other
languages.
Our signatures allow us to instantiate features at
different levels of granularity corresponding to the
levels of granularity in the DELEX feature set. When
a small amount of target language data is present,
the variety of signatures available to us means that
we can learn language-specific transfer characteris-
tics: for example, nouns tend to follow prepositions
in both French and English, but the ordering of ad-
jectives with respect to nouns is different. We also
have the capability to train on languages other than
our target language, and while this is expected to be
less effective, it can still teach us to exploit some
syntactic properties, such as similar verb attachment
configurations if we train on a group of SVO lan-
guages distinct from a target SVO language. There-
fore, our feature set manages to provide the training
procedure with choices about how much syntactic
information to transfer at the same time as it prevents
overfitting and provides language independence.
2.2.1 Query and Signature Types
A query is a subset of the following pieces of in-
formation about an edge: parent word, parent POS,
child word, child POS, attachment direction, and
binned attachment distance. It must contain exactly
one word.5 We experimented with properties from
INBETWEEN and SURROUNDING features as well,
but found that these only helped under some circum-
stances and could lead to overfitting.6
A signature contains the following three pieces of
information:
1. The non-empty subset of attachment properties
included in the query
2. Whether we have lexicalized on the parent or
child of the attachment, indicated by brackets
3. The part of speech of the included word
Because either the parent or child POS is included
in the signature, there are three meaningful proper-
ties to potentially condition on, of which we must se-
lect a nonempty subset. Some multiplication shows
that we have 7? 2? 13 = 182 total PROJ features.
As an example, the queries
verlangen? NOUN
verlangen? ADP
sprechen? NOUN
all share the signature [VERB]?CHILD, but
verlangen? NOUN,RIGHT
Verzicht? ADP
VERB ? Verzicht
have [VERB]?CHILD,DIR, [ADP]?CHILD, and
PARENT?[NOUN] as their signatures, respectively.
The level of granularity for signatures is a param-
eter that simply must be engineered. We found some
benefit in actually instantiating two signatures for
every query, one as described above and one that
5Bilexical features are possible in our framework, but we do
not use them here, so for clarity we assume that each query has
one associated word.
6One hypothesis is that features looking at the sentence con-
text are more highly specialized to a given language, since they
examine the parent, the child, and one or more other parts of
speech or words.
4
?demand, DIR
PARENT?demand
demand
Word POS Dir Dist
that
ADP R
3
said
VERB L
7
<root>
ROOT L
6
senators
NOUN L
1
rules
NOUN R
4
We
NOUN L
1
that
ADP R
1
They
NOUN L
1
concessions
NOUN R
1
from
ADP R
2
P
a
r
e
n
t
s
C
h
i
l
d
r
e
n
DIR
Value
L
0.66
R
0.33
PARENT
Value
ADP
0.33
VERB
0.33
ROOT
0.33
  He   reports that   the   senators demand strict new ethics rules [...]
PRON     VERB     ADP     DET       NOUN          VERB        ADJ     ADJ      NOUN  NOUN
   ?      We   demand that these hostilities cease    ,        ?      said [...]
PUNC   PRON       VERB      ADP     DET        NOUN        VERB   PUNC  PUNC   VERB
 They  demand concessions  from   the  Israeli authorities    <root>
  PRON        VERB              NOUN            ADP      DET      ADJ           NOUN               ROOT
???
Figure 3: Computation of query values. For each occurrence of a given source word, we tabulate the attachments it
takes part in (parents and children) and record their properties. We then compute relative frequency counts for each
possible query type to get source language scores, which will later be projected through the dictionary to obtain target
language feature values. Only two query types are shown here, but values are computed for many others as well.
does not condition on the part of speech of the word
in the signature. One can also imagine using more
refined signatures, but we found that this led to over-
fitting in the small training scenarios under consid-
eration.
2.2.2 Query Value Estimation
Each query is given a value according to a gener-
ative heuristic that involves the source training data
and the probabilistic bilingual lexicon.7 For a par-
ticular signature, a query can be written as a tu-
ple (x1, x2, . . . , wt) where wt is the target language
query word and the xi are the values of the included
language-independent attachment properties. The
value this feature takes is given by a simple gener-
ative model: we imagine generating the attachment
properties xi given wt by first generating a source
7Lexicons such as those produced by automatic aligners in-
clude probabilities natively, but obviously human-created lexi-
cons do not. For these dictionaries, we simply assume that each
word translates with uniform probability into each of its pos-
sible translations. Tweaking this method did not substantially
change performance.
word ws from wt based on the bilingual lexicon,
then jointly generating the xi conditioned on ws.
Treating the choice of source translation as a latent
variable to be marginalized out, we have
value = p(x1, x2, . . . |wt)
=
?
ws
p(ws|wt)p(x1, x2, . . . |ws)
The first term of the sum comes directly from our
probabilistic lexicon, and the second we can esti-
mate using the maximum likelihood estimator over
our source language training data:
p(x1, x2, . . . |ws) =
c(x1, x2, . . . , ws)
c(ws)
(1)
where c(?) denotes the count of an event in the
source language data.
The final feature value is actually the logarithm
of this computed value, with a small constant added
before the logarithm is taken to avoid zeroes.
5
3 Experiments
3.1 Data Conditions
Before we describe the details of our experiments,
we sketch the data conditions under which we eval-
uate our method. As described in Section 1, there is
a continuum of lightly supervised parsing methods
from those that make no assumptions (beyond what
is directly encoded in the model), to those that use
a small set of syntactic universals, to those that use
treebanks from resource-rich languages, and finally
to those that use both existing treebanks and bitexts.
Our focus is on parsing when one does not have
access to a full-scale target language treebank, but
one does have access to realistic auxiliary resources.
The first variable we consider is whether we have
access to a small number of target language trees or
only pre-existing treebanks in a number of other lan-
guages; while not our actual target language, these
other treebanks can still serve as a kind of proxy for
learning which features generally transfer useful in-
formation (McDonald et al2011). We notate these
conditions with the following shorthand:
BANKS: Large treebanks in other target languages
SEED: Small treebank in the right target language
Previous work on essentially unsupervised meth-
ods has investigated using a small number of target
language trees (Smith and Eisner, 2009), but the be-
havior of supervised models under these conditions
has not been extensively studied. We will see in
Section 3.4 that with only 100 labeled trees, even
our baseline model can achieve performance equal
to or better than that of the model of McDonald et
al. (2011). A single linguist could plausibly anno-
tate such a number of trees in a short amount of time
for a language of interest, so we believe that this is
an important setting in which to show improvement,
even for a method primarily intended to augment un-
supervised parsing.
In addition, we consider two different sources for
our bilingual lexicon:
AUTOMATIC: Extracted from bitext
MANUAL: Constructed from human annotations
Both bitexts and human-curated bilingual dictionar-
ies are more widely available than complete tree-
banks. Bitexts can provide rich information about
lexical correspondences in terms of how words are
used in practice, but for resource-poor languages,
parallel text may only be available in small quan-
tities, or be domain-limited. We show results of our
method on bilingual dictionaries derived from both
sources, in order to show that it is applicable under a
variety of data conditions and can successfully take
advantage of such resources as are available.
3.2 Datasets
We evaluate our method on a range of languages
taken from the CoNLL shared tasks on multilingual
dependency parsing (Buchholz and Marsi, 2006;
Nivre et al2007). We make use of dependency
treebanks for Danish, German, Greek, Spanish, Ital-
ian, Dutch, Portuguese, and Swedish, all from the
2006 shared task.
For our English resource, we use 500,000 En-
glish newswire sentences from English Gigaword
version 3 (Graff et al2007), parsed with the Berke-
ley Parser (Petrov et al2006) and converted to a
dependency treebank using the head rules of Collins
(1999).8 Our English test set (used in Section 3.4)
consists of the first 300 sentences of section 23 of the
Penn treebank (Marcus et al1993), preprocessed
in the same way. Our model does not use gold fine-
grained POS tags, but we do use coarse POS tags
deterministically generated from the provided gold
fine-grained tags in the style of Berg-Kirkpatrick
and Klein (2010) using the mappings of Petrov et
al. (2011).9 Following McDonald et al2011), we
strip punctuation from all treebanks for the results of
Section 3.3. All results are given in terms of unla-
beled attachment score (UAS), ignoring punctuation
even when it is present.
We use the Europarl parallel corpus (Koehn,
2005) as the bitext from which to extract the AUTO-
MATIC bilingual lexicons. For each target language,
we produce one-to-one alignments on the English-
target bitext by running the Berkeley Aligner (Liang
et al2006) with five iterations of IBM Model 1 and
8Results do not degrade much if one simply uses Sections 2-
21 of the Penn treebank instead. Coverage of rare words in the
treebank is less important when a given word must also appear
in the bilingual lexicon as the translation of an observed German
word in order to be useful.
9Note that even in the absence of gold annotation, such tags
could be produced from bitext using the method of (Das and
Petrov, 2011) or could be read off from a bilingual lexicon.
6
This work Past work
MANUAL AUTOMATIC MPH11* TMU12**
DELEX DELEX+PROJ ? DELEX+PROJ ? Multi-dir Multi-proj ? No clusters X-lingual ?
DA 41.3 43.0 1.67 ? 43.6 2.30 ? 48.9* 0.6* 36.7** 2.0**
DE 58.5 58.7 0.20 59.5 0.94 ? 56.7* -0.1* 48.9** 1.8**
EL 57.9 59.9 1.99 ? 60.5 2.55 ? 60.1* 5.0* 59.5** 3.5**
ES 64.2 65.4 1.20 ? 65.7 1.52 ? 64.2* 0.3* 60.2** 2.7**
IT 65.9 66.5 0.58 67.4 1.54 ? 64.1* 0.9* 64.6** 4.2**
NL 57.0 57.5 0.52 58.8 1.88 ? 55.8* 9.9* 52.8** 1.5**
PT 75.4 77.2 1.83 ? 78.7 3.29 ? 74.0* 1.6* 66.8** 4.2**
SV 64.5 66.1 1.61 ? 66.9 2.34 ? 65.3* 2.7* 55.4** 1.5**
AVG 60.6 61.8 1.20 62.6 2.05 61.1* 2.7* 55.6** 2.7**
Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con-
catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths
in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical
significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates
p < 0.05. We also include the baseline results of McDonald et al2011) and Ta?ckstro?m et al2012) and improve-
ments from their best methods of using bitext and lexical information. These results are not directly comparable to
ours, as indicated by * and **. However, we still see that the performance of our type-level transfer method approaches
that of bitext-based methods, which require complex bilingual training for each new language.
five iterations of the HMM aligner with agreement
training. Our lexicon is then read off based on rel-
ative frequency counts of aligned instances of each
word in the bitext.
We also use our method on bilingual dictionar-
ies constructed in a more conventional way. For
this purpose, we scrape our MANUAL bilingual lex-
icons from English Wiktionary (Wikimedia Founda-
tion, 2012). We mine entries for English words that
explicitly have foreign translations listed as well as
words in each target language that have English def-
initions. We discard all translation entries where
the English side is longer than one word, except
for constructions of the form ?to VERB?, where we
manually remove the ?to? and allow the word to be
defined as the English infinitive. Finally, because
our method requires a dictionary with probability
weights, we assume that each target language word
translates with uniform probability into any of the
candidates that we scrape.
3.3 BANKS
We first evaluate our model under the BANKS data
condition. Following the procedure from McDonald
et al2011), for each language, we train both our
DELEX and DELEX+PROJ features on a concate-
nation of 2000 sentences from each other CoNLL
training set, plus 2000 sentences from the Penn
Treebank. Again, despite the values of our PROJ
queries being sensitive to which language we are
currently parsing, the signatures are language in-
dependent, so discriminative training still makes
sense over such a combined treebank. Training our
PROJ features on the non-English treebanks in this
concatenation can be understood as trying to learn
which lexico-syntactic properties transfer ?univer-
sally,? or at least transfer broadly within the families
of languages we are considering.
Table 1 shows the performance of the DELEX fea-
ture set and the DELEX+PROJ feature set using both
AUTOMATIC and MANUAL bilingual lexicons. Both
methods provide positive gains across the board that
are statistically significant in the vast majority of
cases, though MANUAL is slightly less effective;
we postpone until Section 4.1 the discussion of the
shortcomings of the MANUAL lexicon.
We include for reference the baseline results of
McDonald et al2011) and Ta?ckstro?m et al2012)
(multi-direct transfer and no clusters) and the im-
provements from their best methods using lexi-
cal information (multi-projected transfer and cross-
lingual clusters). We emphasize that these results
are not directly comparable to our own, as we
have different training data (and even different train-
ing languages) and use a different underlying pars-
ing model (MSTParser instead of a transition-based
7
AUTOMATIC
100 train trees 200 train trees 400 train trees
DELEX DELEX+PROJ ? DELEX DELEX+PROJ ? DELEX DELEX+PROJ ?
DA 67.2 69.5 2.32 ? 69.5 72.3 2.77 ? 71.4 74.6 3.16 ?
DE 72.9 73.9 0.97 75.4 76.5 1.09 ? 77.3 78.5 1.25 ?
EL 70.8 72.9 2.07 ? 72.6 74.9 2.30 ? 74.3 76.7 2.41 ?
ES 72.5 73.0 0.46 74.1 75.4 1.29 ? 75.3 77.2 1.81 ?
IT 73.3 75.4 2.13 ? 74.7 77.3 2.54 ? 76.0 78.7 2.74 ?
NL 63.0 65.8 2.82 ? 64.7 67.6 2.86 ? 66.1 69.2 3.06 ?
PT 78.1 79.5 1.45 ? 79.5 81.1 1.66 ? 80.7 82.4 1.63 ?
SV 76.4 78.1 1.69 ? 78.1 80.2 2.02 ? 79.6 81.7 2.07 ?
AVG 71.8 73.5 1.74 73.6 75.7 2.07 75.1 77.4 2.27
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
MANUAL
DA 67.2 68.1 0.88 69.5 70.9 1.44 ? 71.4 73.3 1.92 ?
DE 72.9 73.4 0.44 75.4 76.2 0.77 77.3 78.4 1.12 ?
EL 70.8 71.9 1.06 ? 72.6 74.1 1.48 ? 74.3 75.8 1.56 ?
ES 72.5 71.9 -0.64 74.1 74.3 0.23 75.3 76.4 1.04 ?
IT 73.3 74.3 1.01 ? 74.7 76.4 1.66 ? 76.0 78.0 2.01 ?
NL 63.0 65.4 2.43 ? 64.7 67.5 2.76 ? 66.1 69.0 2.91 ?
PT 78.1 78.2 0.13 79.5 80.1 0.62 80.7 81.5 0.82 ?
SV 76.4 76.6 0.25 78.1 79.1 1.01 ? 79.6 81.0 1.40 ?
AVG 71.8 72.5 0.70 73.6 74.8 1.25 75.1 76.7 1.60
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
Table 2: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on various
small numbers of target language trees (the SEED setting). Values reported are UAS for sentences of all lengths on
our enlarged CoNLL test sets (see text); each value is based on 50 sampled training sets of the given size. Daggers
indicate statistical significance as described in the text. Statistical significance is not reported for averages.
parser (Nivre, 2008)). However, our baseline is com-
petitive with theirs,10 demonstrating that we have
constructed a state-of-the-art delexicalized parser.
Furthermore, our method appears to approach the
performance of previous bitext-based methods, and
because of its flexibility and the freedom from com-
plex cross-lingual training for each new language, it
can be applied in the MANUAL case as well, a capa-
bility which neither of the other methods has.
3.4 SEED
We now turn our attention to the SEED scenario,
where a small number of target language trees are
available for each language we consider. While it
is imaginable to continue to exploit the other tree-
banks in the presence of target language trees, we
found that training our DELEX features on the seed
treebank alone gave higher performance than any
10The baseline of Ta?ckstro?m et al2012) is lower because it
is trained only on English rather than on many languages.
attempt to also use the concatenation of treebanks
from the previous section. This is not too surpris-
ing because, with this number of sentences, there is
already good monolingual coverage of coarse POS
features, and attempting to train features on other
languages can be expected to introduce noise into
otherwise accurate monolingual feature weights.
We train our DELEX+PROJ model with both AU-
TOMATIC and MANUAL lexicons on target language
training sets of size 100, 200, and 400, and give re-
sults for each language in Table 2. The performance
of parsers trained on small numbers of trees can
be highly variable, so we create multiple treebanks
of each size by repeatedly sampling from each lan-
guage?s train treebank, and report averaged results.
Furthermore, this evaluation is not on the standard
CoNLL test sets, but is instead on those test sets with
a few hundred unused training sentences added, the
reason being that some of the CoNLL test sets are
very small (fewer than 200 sentences) and appeared
8
to give highly variable results. To compute statistical
significance, we draw a large number of bootstrap
samples for each training set used, then aggregate all
of their sufficient statistics in order to compute the fi-
nal p-value. We see that our DELEX+PROJ method
gives statistically significant gains at the 95% level
over DELEX for nearly all language and training set
size pairs, giving on average a 9% relative error re-
duction in the 400-tree case.
Because our features are relatively few in number
and capture heuristic information, one question we
might ask is how well they can perform in a non-
projection context. In the last line of the table, we
report gains that are achieved when PROJ features
computed from parsed Gigaword are used directly
on English, with no intermediate dictionary. These
are not comparable to the other values in the table
because we are using our projection strategy mono-
lingually, which removes the barriers of imperfect
lexical correspondence (from using the lexicon) and
imperfect syntactic correspondence (from project-
ing). As one might expect, the gains on English are
far higher than the gains on other languages. This
indicates that performance is chiefly limited by the
need to do cross-lingual feature adaptation, not in-
herently low feature capacity. We delay further dis-
cussion to Section 4.2.
One surprising thing to note is that the gains given
by our PROJ features are in some cases larger here
than in the BANKS setting. This result is slightly
counterintuitive, as our baseline parsers are much
better in this case and so we would expect dimin-
ished returns from our method. We conclude that ac-
curately learning which signatures transfer between
languages is important, and it is easier to learn good
feature weights when some target language data is
available. Further evidence supporting this hypothe-
sis is the fact that the gains are larger and more sig-
nificant on larger training set sizes.
4 Discussion
4.1 AUTOMATIC versus MANUAL
Overall, we see that gains from using our MANUAL
lexicons are slightly lower than those from our AU-
TOMATIC lexicons. One might expect higher per-
formance because scraped bilingual lexicons are not
prone to some of the same noise that exists in auto-
AUTOMATIC MANUAL
Voc OCC Voc OCC
DA 324K 0.91 22K 0.64
DE 320K 0.89 58K 0.55
EL 196K 0.94 23K 0.43
ES 165K 0.89 206K 0.74
IT 158K 0.91 78K 0.65
NL 251K 0.87 50K 0.72
PT 165K 0.85 46K 0.53
SV 307K 0.93 28K 0.60
Table 3: Lexicon statistics for all languages for both
sources of bilingual lexicons. ?Voc? indicates vocabulary
size and ?OCC? indicates open-class coverage, the frac-
tion of open-class tokens in the test treebanks with entries
in our bilingual lexicon.
matic aligners, but this is empirically not the case.
Rather, as we see in Table 3, the low recall of our
MANUAL lexicons on open-class words appears to
be a possible culprit. The coverage gap between
these and the AUTOMATIC lexicons is partially due
to the inconsistent structure of Wiktionary: inflected
German and Greek words often do not have their
own pages, so we miss even common morphologi-
cal variants of verb forms in those languages. The
inflected forms that we do scrape are also mapped
to the English base form rather than the correspond-
ing inflected form in English, which introduces fur-
ther noise. Coverage is substantially higher if we
translate using stems only, but this did not empir-
ically lead to performance improvements, possibly
due to conflating different parts of speech with the
same base form.
One might hypothesize that our uniform weight-
ing scheme in the MANUAL lexicon is another
source of problems, and that bitext-derived weights
are necessary to get high performance. This is not
the case here. Truncating the AUTOMATIC dictio-
nary to at most 20 translations per word and setting
the weights uniformly causes a slight performance
drop, but is still better than our MANUAL lexicon.
This further demonstrates that these problems are
more a limitation of our dictionary than our method.
English Wiktionary is not designed to be a bilingual
dictionary, and while it conveniently provided an
easy way for us to produce lexicons for a wide array
9
Frauen    wollen    weiter     f?r       die     Quote  k?mpfen
   NN     VMFIN    ADV    APPR   ART      NN    VVINF
Women     want     further     for       the     quota     fight
Women    want    to   continue   to    fight   for   the   quota
   NNP      VBP   TO      VB      TO    VB    IN   DT    NN
Figure 4: Example of a German tree and a parallel En-
glish sentence with high levels of syntactic divergence.
The English verb want takes fundamentally different chil-
dren than wollen does, so properties of the sort we present
in Section 2.2 will not transfer effectively.
of languages, it is not the resource that one would
choose if designing a parser for a specific target lan-
guage. Bitext is not necessary for our approach to
work, and results on the AUTOMATIC lexicon sug-
gest that our type-level transfer method can in fact
do much better given a higher quality resource.
4.2 Limitations
While our method does provide consistent gains
across a range of languages, the injection of lexical
information is clearly not sufficient to bridge the gap
between unsupervised and supervised parsers. We
argued in Section 3.4 that the cross-lingual transfer
step of our method imposes a fundamental limitation
on how useful any such approach can be, which we
now investigate further.
In particular, any syntactic divergence, especially
inconsistent divergences like head switching, will
limit the utility of transferred structure. Consider
the German example in Figure 4, with a parallel En-
glish sentence provided. The English tree suggests
that want should attach to an infinitival to, which has
no correlate in German. Even disregarding this, its
grandchild is the verb continue, which is realized in
the German sentence as the adverb weiter. While
it is still broadly true that want and wollen both
have verbal elements located to their right, it is less
clear how to design features that can still take advan-
tage of this while working around the differences we
have described. Therefore, a gap between the per-
formance of our features on English and the perfor-
mance of our projected features, as is observed in
Table 2, is to be expected in the absence of a more
complete model of syntactic divergence.
5 Conclusion
In this work, we showed that lexical attachment pref-
erences can be projected to a target language at the
type level using only a bilingual lexicon, improving
over a delexicalized baseline parser. This method
is broadly applicable in the presence or absence
of target language training trees and with bilingual
lexicons derived from either manually-annotated re-
sources or bitexts. The greatest improvements arise
when the bilingual lexicon has high coverage and a
number of target language trees are available in or-
der to learn exactly what lexico-syntactic properties
transfer from the source language.
In addition, we showed that a well-tuned discrim-
inative model with the correct features can achieve
good performance even on very small training sets.
While unsupervised and existing projection meth-
ods do feature great versatility and may yet pro-
duce state-of-the-art parsers on resource-poor lan-
guages, spending time constructing small supervised
resources appears to be the fastest method to achieve
high performance in these settings.
Acknowledgments
This work was partially supported by an NSF Grad-
uate Research Fellowship to the first author, by a
Google Fellowship to the second author, and by the
NSF under grant 0643742. Thanks to the anony-
mous reviewers for their insightful comments.
References
Mohit Bansal and Dan Klein. 2011. Web-scale Features
for Full-scale Parsing. In Proceedings of ACL, pages
693?702, Portland, Oregon, USA.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic Grammar Induction. In Proceedings of ACL,
pages 1288?1297, Uppsala, Sweden.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of CoNLL, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying in
10
Unsupervised Grammar Induction. In Proceedings of
NAACL, pages 74?82, Boulder, Colorado.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised Structure Prediction with Non-Parallel
Multilingual Guidance. In Proceedings of EMNLP,
pages 50?61, Edinburgh, UK.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Koby Crammer and Yoram Singer. 2001. Ultraconserva-
tive Online Algorithms for Multiclass Problems. Jour-
nal of Machine Learning Research, 3:2003.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of ACL, pages 600?609, Port-
land, Oregon, USA.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency Grammar Induction via Bitext Pro-
jection Constraints. In Proceedings of ACL, pages
369?377, Suntec, Singapore.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Grammar Induction. In Proceedings of CoLING-ACL,
pages 881?888, Sydney, Australia.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
Parsers via Syntactic Projection Across Parallel Texts.
Natural Language Engineering, 11:311?325, Septem-
ber.
Dan Klein and Christopher D. Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of ACL,
pages 479?486.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit X,
pages 79?86, Phuket, Thailand. AAMT.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-Supervised Dependency Parsing. In Pro-
ceedings of ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of NAACL, New
York, New York.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated Cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330, June.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP, pages 62?72, Ed-
inburgh, Scotland, UK.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceed-
ings of EMNLP, pages 1234?1244, Cambridge, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of EMNLP-CoNLL, pages
915?932, Prague, Czech Republic.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34:513?553, December.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of ACL,
pages 433?440, Sydney, Australia.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A Universal Part-of-Speech Tagset. In ArXiv, April.
David A. Smith and Jason Eisner. 2009. Parser Adapta-
tion and Projection with Quasi-Synchronous Grammar
Features. In Proceedings of EMNLP, pages 822?831,
Suntec, Singapore.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual Word Clusters for Direct Trans-
fer of Linguistic Structure. In Proceedings of NAACL,
Montreal, Canada.
Wikimedia Foundation. 2012. Wiktionary. Online at
http://www.wiktionary.org/.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In Proceedings
of ACL, pages 1556?1565, Portland, Oregon, USA.
11
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 863?872, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Transforming Trees to Improve Syntactic Convergence
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
We describe a transformation-based learning
method for learning a sequence of mono-
lingual tree transformations that improve the
agreement between constituent trees and word
alignments in bilingual corpora. Using the
manually annotated English Chinese Transla-
tion Treebank, we show how our method au-
tomatically discovers transformations that ac-
commodate differences in English and Chi-
nese syntax. Furthermore, when transforma-
tions are learned on automatically generated
trees and alignments from the same domain as
the training data for a syntactic MT system,
the transformed trees achieve a 0.9 BLEU im-
provement over baseline trees.
1 Introduction
Monolingually, many Treebank conventions are
more or less equally good. For example, the En-
glish WSJ treebank (Marcus et al1993) attaches
verbs to objects rather than to subjects, and it at-
taches prepositional modifiers outside of all quan-
tifiers and determiners. The former matches most
linguistic theories while the latter does not, but to
a monolingual parser, these conventions are equally
learnable. However, once bilingual data is involved,
such treebank conventions entail constraints on rule
extraction that may not be borne out by semantic
alignments. To the extent that there are simply di-
vergences in the syntactic structure of the two lan-
guages, it will often be impossible to construct syn-
tax trees that are simultaneously in full agreement
with monolingual linguistic theories and with the
alignments between sentences in both languages.
To see this, consider the English tree in Figure 1a,
taken from the English side of the English Chi-
nese Translation Treebank (Bies et al2007). The
lowest VP in this tree is headed by ?select,? which
aligns to the Chinese verb ???.? However, ??
?? also aligns to the other half of the English in-
finitive, ?to,? which, following common English lin-
guistic theory, is outside the VP. Because of this
violating alignment, many syntactic machine trans-
lation systems (Galley et al2004; Huang et al
2006) won?t extract any translation rules for this
constituent. However, by applying a simple trans-
formation to the English tree to set up the infinitive
as its own constituent, we get the tree in Figure 1b,
which may be less well-motivated linguistically, but
which corresponds better to the Chinese-mediated
semantics and permits the extraction of many more
syntactic MT rules.
In this work, we develop a method based on
transformation-based learning (Brill, 1995) for au-
tomatically acquiring a sequence of tree transforma-
tions of the sort in Figure 1. Once the transformation
sequence has been learned, it can be deterministi-
cally applied to any parsed sentences, yielding new
parse trees with constituency structures that agree
better with the bilingual alignments yet remain con-
sistent across the corpus. In particular, we use this
method to learn a transformation sequence for the
English trees in a set of English to Chinese MT train-
ing data. In experiments with a string-to-tree trans-
lation system, we show resulting improvements of
up to 0.9 BLEU.
A great deal of research in syntactic machine
translation has been devoted to handling the inher-
ent syntactic divergence between source and target
languages. Some systems attempt to model the dif-
ferences directly (Yamada and Knight, 2001; Eis-
ner, 2003), but most recent work focuses on reduc-
ing the sensitivity of the rule-extraction procedure
to the constituency decisions made by 1-best syn-
tactic parsers, either by using forest-based methods
863
The
first step is to select team members
S
NP
S
VP
VP
VPTO
VBZ
ADVPVB

?? ? ? ?
(a) Before
The
first step is to select team members
S
NP
S
VP
VP
VP
VBZ

?? ? ? ?
TO+VB
VBTO ADVP
(b) After
Figure 1: An example tree transformation merging a VB node with the TO sibling of its parent VP. Before the trans-
formation (a), the bolded VP cannot be extracted as a translation rule, but afterwards (b), both this VP and the newly
created TO+VB node are extractable.
for learning translation rules (Mi and Huang, 2008;
Zhang et al2009), or by learning rules that en-
code syntactic information but do not strictly ad-
here to constituency boundaries (Zollmann et al
2006; Marton and Resnik, 2008; Chiang, 2010). The
most closely related MT system is that of Zhao et al
(2011), who train a rule extraction system to trans-
form the subtrees that make up individual translation
rules using a manually constructed set of transfor-
mations similar to those learned by our system.
Instead of modifying the MT system to work
around the input annotations, our system modifies
the input itself in order to improve downstream
translation. Most systems of this sort learn how to
modify word alignments to agree better with the syn-
tactic parse trees (DeNero and Klein, 2007; Fossum
et al2008), but there has also been other work di-
rectly related to improving agreement by modifying
the trees. Burkett et al2010) train a bilingual pars-
ing model that uses bilingual agreement features to
improve parsing accuracy. More closely related to
the present work, Katz-Brown et al2011) retrain a
parser to directly optimize a word reordering metric
in order to improve a downstream machine transla-
tion system that uses dependency parses in a prepro-
cessing reordering step. Our system is in the same
basic spirit, using a proxy evaluation metric (agree-
ment with alignments; see Section 2 for details) to
improve performance on a downstream translation
task. However, we are concerned more generally
with the goal of creating trees that are more com-
patible with a wide range of syntactically-informed
translation systems, particularly those that extract
translation rules based on syntactic constituents.
2 Agreement
Our primary goal in adapting parse trees is to im-
prove their agreement with a set of external word
alignments. Thus, our first step is to define an agree-
ment score metric to operationalize this concept.
Central to the definition of our agreement score
is the notion of an extractable node. Intuitively, an
extractable English1 tree node (also often called a
?frontier node? in the literature), is one whose span
aligns to a contiguous span in the foreign sentence.
Formally, we assume a fixed word alignment a =
{(i, j)}, where (i, j) ? a means that English word
i is aligned to foreign word j. For an English span
[k, `] (inclusive), the set of aligned foreign words is:
fset([k, `]) = {j | ? i : k ? i ? `; (i, j) ? a}
We then define the aligned foreign span as:
fspan([k, `]) = [min(fset([k, `])),max(fset([k, `]))]
1For expositional clarity, we will refer to ?English? and ?for-
eign? sentences/trees, but our definitions are in no way language
dependent and apply equally well to any language pair.
864
The aligned English span for a given foreign span
[s, t] is defined analogously:
eset([s, t]) = {i | ? j : s ? j ? t; (i, j) ? a}
espan([s, t]) = [min(eset([s, t])),max(eset([s, t]))]
Finally, we define [k, `] to be extractable if and
only if it has at least one word alignment and its
aligned foreign span aligns back to a subspan of
[k, `]:
fset([k, `]) 6= ? ? espan(fspan([k, `])) ? [k, `]
With this definition of an extractable span, we can
now define the agreement score ga(t) for an English
tree t, conditioned on an alignment a:2
ga(t) =
?
[k,`]?t:
|[k,`]|>1
sign([k, `]) (1)
Where
sign([k, `]) =
{
1 [k, `] is extractable
?1 otherwise
Importantly, the sum in Equation 1 ranges over all
unique spans in t. This is simply to make the met-
ric less gameable, preventing degenerate solutions
such as an arbitrarily long chain of unary produc-
tions over an extractable span. Also, since all indi-
vidual words are generated by preterminal part-of-
speech nodes, the sum skips over all length 1 spans.
As a concrete example of agreement score, we can
return to Figure 1. The tree in Figure 1a has 6 unique
spans, but only 5 are extractable, so the total agree-
ment score is 5 - 1 = 4. After the transformation,
though, the tree in Figure 1b has 6 extractable spans,
so the agreement score is 6.
3 Transformation-Based Learning
Transformation-based learning (TBL) was origi-
nally introduced via the Brill part-of-speech tag-
ger (Brill, 1992) and has since been applied to a wide
variety of NLP tasks, including binary phrase struc-
ture bracketing (Brill, 1993), PP-attachment disam-
biguation (Brill and Resnik, 1994), base NP chunk-
ing (Ramshaw and Marcus, 1995), dialogue act tag-
ging (Samuel et al1998), and named entity recog-
nition (Black and Vasilakopoulos, 2002).
2Unextractable spans are penalized in order to ensure that
space is saved for the formation of extractable ones.
The generic procedure is simple, and requires
only four basic inputs: a set of training sentences, an
initial state annotator, an inventory of atomic trans-
formations, and an evaluation metric. First, you ap-
ply the initial state annotator (here, the source of
original trees) to your training sentences to ensure
that they all begin with a legal annotation. Then,
you test each transformation in your inventory to see
which one will yield the greatest improvement in the
evaluation metric if applied to the training data. You
greedily apply this transformation to the full training
set and then repeat the procedure, applying transfor-
mations until some stopping criterion is met (usu-
ally either a maximum number of transformations,
or a threshold on the marginal improvement in the
evaluation metric).
The output of the training procedure is an ordered
set of transformations. To annotate new data, you
simply label it with the same initial state annotator
and then apply each of the learned transformations
in order. This process has the advantage of being
quite fast (usually linear in the number of transfor-
mations and the length of the sentence; for parsing,
the cost will typically be dominated by the cost of
the initial state annotator), and, unlike the learned
parameters of a statistical model, the set of learned
transformations itself can often be of intrinsic lin-
guistic interest.
For our task, we have already defined the evalua-
tion metric (Section 2) and the initial state annotator
will either be the gold Treebank trees or a Treebank-
trained PCFG parser. Thus, to fully describe our sys-
tem, it only remains to define the set of possible tree
transformations.
4 Tree Transformations
The definition of an atomic transformation consists
of two parts: a rewrite rule and the triggering envi-
ronment (Brill, 1995). Tree transformations are best
illustrated visually, and so for each of our transfor-
mation types, both parts of the definition are repre-
sented schematically in Figures 2-7. We have also
included a real-world example of each type of trans-
formation, taken from the English Chinese Transla-
tion Treebank.
Altogether, we define six types of tree transfor-
mations. Each class of transformation takes be-
865
A... ...
B C
A
... ...
B C
B+C
Type: ARTICULATE
Args:
A: PARENT, B: LEFT, C: RIGHT
(a) Schematic
??
?
S
Other members will arrive in two groups .
? ?
? ? ? ?
VPNP
.
??
?
S
Other members will arrive in two groups .
? ?
? ? ? ?
VPNP
.NP+VP
(b) Example: ARTICULATE?S, NP, VP?
Figure 2: ARTICULATE transformations.
A
...
B
...
C D
A
... ...
C D
Type: FLATTEN
Args:
A: PARENT, B: TARGET
A
...
B
...
D E
A
... ...
E C
Type: FLATTENINCONTEXT
Args:
A: PARENT, B: TARGET,
C: SIBLING, left: DIRECTION
C D
(a) Schematic
?? ??
NP
the China Trade Promotion Council
NML NNP
NNPNNP
NNPDT
?? ??
NP
the China Trade Promotion Council
NNPNNPNNPNNPDT
(b) Example: FLATTENINCONTEXT?NP, NML, NNP, left?
Figure 3: FLATTEN transformations.
A
...
B
C
...
A
...
B
...
C
Type: PROMOTE
Args:
A: GRANDPARENT, B: PARENT,
C: CHILD, left: DIRECTION
(a) Schematic
NP
?? ? ???
IN
by the French player N. Taugia
NP NP
PP
NP
?? ? ???
IN
by the French player N. Taugia
NP
NP
PP
(b) Example: PROMOTE?PP, NP, NP, left?
Figure 4: PROMOTE transformations.
A
...
B C
...
A
...
B
...
C
Type: DEMOTE
Args:
A: PARENT, B: DEMOTER,
C: DEMOTED, left: DIRECTION
......
(a) Schematic
? ?? ?
fly
to Beijing on the 2nd
VP
??
IN NP NPIN
PPVB PP
? ?? ?
fly
to Beijing on the 2nd
VP
??
IN NP NPINVB
PP PP
(b) Example: DEMOTE?VP, PP, VB, right?
Figure 5: DEMOTE transformations.
866
AB
C
D
...
A
B
C
......
Type: TRANSFER
Args:
A: GRANDPARENT, B: AUNT,
C: PARENT, D: TARGET,
left: DIRECTION
...
D
(a) Schematic
serious consequences that cause losses
NP
NP
SBAR
SWHNP
???? ? ?
JJ NNS
serious consequences that cause losses
NP
NP SBAR
S
???? ? ?
WHNPJJ NNS
(b) Example: TRANSFER?NP, NP, SBAR, WHNP, left?
Figure 6: TRANSFER transformations.
A
B C
D
...
A
B+D
C
...
B D
Type: ADOPT
Args:
A: GRANDPARENT, B: AUNT,
C: PARENT, D: TARGET,
left: DIRECTION
(a) Schematic
Sabor also tied with Setangon
S
VP
PP
?? ? ??
RB VBD
NP ADVP
????
VP
Sabor also tied with Setangon
S
PP
?? ? ??
RB
VBD
NP
????
RB+VP
(b) Example: ADOPT?S, VP, ADVP, RB, right?
Figure 7: ADOPT transformations.
tween two and four syntactic category arguments,
and most also take a DIRECTION argument that
can have the value left or right.3 We refer to the
nodes in the schematics whose categories are argu-
ments of the transformation definition as participat-
ing nodes. Basically, a particular transformation is
triggered anywhere in a parse tree where all partici-
pating nodes appear in the configuration shown. The
exact rules for the triggering environment are:
1. Each participating node must appear in the
schematically illustrated relationship to the
others. The non-participating nodes in the
schematic do not have to appear. Similarly, any
number of additional nodes can appear as sib-
lings, parents, or children of the explicitly illus-
trated nodes.
2. Any node that will gain a new child as a re-
sult of the transformation must already have at
least one nonterminal child. We have drawn the
schematics to reflect this, so this condition is
3To save space, the schematic for each of these transforma-
tions is only shown for the left direction, but the right version is
simply the mirror image.
equivalent to saying that any participating node
that is drawn with children must have a phrasal
syntactic category (i.e. it cannot be a POS).
3. Repeated mergings are not allowed. That is, the
newly created nodes that result from an ARTIC-
ULATE or ADOPT transformation cannot then
participate as the LEFT or RIGHT argument of a
subsequent ARTICULATE transformation or as
the AUNT or TARGET argument of a subsequent
ADOPT transformation. This is simply to pre-
vent the unrestrained proliferation of new syn-
tactic categories.
The rewrite rule for a transformation is essentially
captured in the corresponding schematic. Additional
nodes that do not appear in the schematic are gener-
ally handled in the obvious way: unillustrated chil-
dren or parents of illustrated nodes remain in place,
while unillustrated siblings of illustrated nodes are
handled identically to their illustrated siblings. The
only additional part of the rewrite that is not shown
explicitly in the schematics is that if the node in the
PARENT position of a TRANSFER or ADOPT trans-
formation is left childless by the transformation (be-
867
cause the TARGET node was its only child), then it is
deleted from the parse tree. In the case of a transfor-
mation whose triggering environment appears multi-
ple times in a single tree, transformations are always
applied leftmost/bottom-up and exhaustively.4
In principle, our transformation inventory consists
of all possible assignments of syntactic categories to
the arguments of each of the transformation types
(subject to the triggering environment constraints).
In practice, though, we only ever consider trans-
formations whose triggering environments appear in
the training corpus (including new triggering envi-
ronments that appear as the result of earlier trans-
formations). While the theoretical space of possi-
ble transformations is exponentially large, the set
of transformations we actually have to consider is
quite manageable, and empirically grows substan-
tially sublinearly in the size of the training set.
5 Results and Analysis
There are two ways to use this procedure. One is to
apply it to the entire data set, with no separate train-
ing phase. Given that the optimization has no notion
of gold transformations, this procedure is roughly
like an unsupervised learner that clusters its entire
data. Another way is to learn annotations on a sub-
set of data and apply it to new data. We choose the
latter primarily for reasons of efficiency and simplic-
ity: many common use cases are easiest to manage
when annotation systems can be trained once offline
and then applied to new data as it comes in.
Since we intend for our system to be used as
a pre-trained annotator, it is important to ensure
that the learned transformation sequence achieves
agreement score gains that generalize well to un-
seen data. To minimize errors that might be intro-
duced by the noise in automatically generated parses
and word alignments, and to maximize reproducibil-
ity, we conducted our initial experiments on the En-
glish Chinese Translation Treebank. For this dataset,
the initial state annotations (parse trees) were man-
ually created by trained human annotators, as were
the word alignments used to compute the agreement
4The transformation is repeatedly applied at the lowest, left-
most location of the parse tree where the triggering environment
appears, until the triggering environment no longer appears any-
where in the tree.
0 1 
2 3 
4 5 
6 7 
8 9 
0 500 1000 1500 2000 2500 
Aver
age A
greem
ent S
core 
Impr
ovem
ent 
Number of Transformations 
Training Dev 
Figure 8: Transformation results on the English Chinese
Translation Treebank. The value plotted is the average
(per-sentence) improvement in agreement score over the
baseline trees.
Transfor- Total Extractable Agreement
mations Spans Spans Score
0 13.15 9.78 6.40
10 12.57 10.36 8.15
50 13.41 11.38 9.35
200 14.03 11.96 9.89
1584 14.58 12.36 10.15
2471 14.65 12.35 10.06
Table 1: Average span counts and agreement scores on
the English Chinese Translation Treebank development
set. The highest agreement score was attained at 1584
transformations, but most of the improvement happened
much earlier.
score.5 The data was divided into training/dev/test
using the standard Chinese parsing split; we trained
the system on the training set (2261 sentences af-
ter filtering out sentences with missing annotations),
and evaluated on the development set (223 sentences
after filtering).
The improvements in agreement score are shown
in Figure 8, with a slightly more detailed breakdown
at a few fixed points in Table 1. While the system
was able to find up to 2471 transformations that im-
proved the training set agreement score, the major-
ity of the improvement, and especially the majority
of the improvement that generalized to the test set,
5The annotation guidelines for the English side of this Tree-
bank are similar, though not identical, to those for the WSJ
Treebank.
868
1 ARTICULATE?S,NP,VP?
2 FLATTENINCONTEXT?PP,NP,IN,right?
3 PROMOTE?VP,VP,VBN,left?
4 ADOPT?VP,TO,VP,VB,left?
5 ADOPT?PP,VBG,PP,IN,left?
6 FLATTEN?VP,VP?
7 ARTICULATE?VP,VBD,NP?
8 FLATTENINCONTEXT?PP,NML,NNP,left?
9 ARTICULATE?NP,NNP,NNS?
10 ARTICULATE?S,NP,ADVP?
11 TRANSFER?NP,NP,SBAR,WHNP,left?
12 FLATTENINCONTEXT?NP,NML,NNP,left?
13 ARTICULATE?NP,NN,NNS?
14 TRANSFER?NP,NP+,,SBAR,WHNP,left?
15 ADOPT?PP,IN,PP,IN,left?
16 PROMOTE?S,VP,CC+VP,right?
17 ARTICULATE?VP,VBZ,VBN?
18 ARTICULATE?VP,VBD,PP?
19 ARTICULATE?VP,MD,ADVP?
20 ADOPT?PP,SYM,QP,CD,right?
Table 2: The first 20 learned transformations, excluding
those that only merged punctuation or conjunctions with
adjacent phrases. The first 5 are illustrated in Figure 9.
was achieved within the first 200 or so transforma-
tions. We also see from Table 1 that, though the first
few transformations deleted many non-extractable
spans, the overall trend was to produce more finely
articulated trees, with the full transformation se-
quence increasing the number of spans by more than
10%.
As discussed in Section 3, one advantage of TBL
is that the learned transformations can themselves
often be interesting. For this task, some of the high-
est scoring transformations did uninteresting things
like conjoining conjunctions or punctuation, which
are often either unaligned or aligned monotonically
with adjacent phrases. However, by filtering out
all ARTICULATE transformations where either the
LEFT or RIGHT argument is ?CC?, ?-RRB-?, ?,?, or
?.? and taking the top 20 remaining transformations,
we get the list in Table 2, the first 5 of which are
also illustrated in Figure 9. Some of these (e.g. #1,
#7, #10) are additional ways of creating new spans
when English and Chinese phrase structures roughly
agree, but many others do recover known differences
in English and Chinese syntax. For example, many
of these transformations directly address compound
verb forms in English, which tend to align to single
words in Chinese: #3 (past participle constructions),
#4 (infinitive), #6 (all), and #17 (present perfect).
We also see differences between English and Chi-
nese internal NP structure (e.g. #9, #12, #13).
6 Machine Translation
The ultimate goal of our system is to improve
the agreement between the automatically generated
parse trees and word alignments that are used as
training data for syntactic machine translation sys-
tems. Given the amount of variability between the
outputs of different parsers and word aligners (or
even the same systems with different settings), the
best way to improve agreement is to learn a trans-
formation sequence that is specifically tuned for the
same annotators (parsers and word aligners) we are
evaluating with. In particular, we found that though
training on the English Chinese Translation Tree-
bank produces clean, interpretable rules, prelimi-
nary experiments showed little to no improvement
from using these rules for MT, primarily because
actual alignments are not only noisier but also sys-
tematically different from gold ones. Thus, all rules
used for MT experiments were learned from auto-
matically annotated text.
For our Chinese to English translation experi-
ments, we generated word alignments using the
Berkeley Aligner (Liang et al2006) with default
settings. We used an MT pipeline that conditions
on target-side syntax, so our initial state annotator
was the Berkeley Parser (Petrov and Klein, 2007),
trained on a modified English treebank that has been
adapted to match standard MT tokenization and cap-
italization schemes.
As mentioned in Section 5, we could, in principle
train on all 500k sentences of our MT training data.
However, this would be quite slow: each iteration of
the training procedure requires iterating through all
n training sentences6 once for each of the m can-
didate transformations, for a total cost of O(nm)
where m grows (albeit sublinearly) with n. Since the
6By using a simple hashing scheme to keep track of trigger-
ing environments, this cost can be reduced greatly but is still
linear in the number of training sentences.
869
S... ...
NP VP
S
... ...
NP VP
NP+VP
(a) ARTICULATE?S,NP,VP?
PP
...
IN
...
A B
PP
... ...
A BNP IN
(b) FLATTENINCONTEXT?PP,NP,IN,right?
VP
...
VP
VBN
...
VP
...
VP
...
VBN
(c) PROMOTE?VP,VP,VBN,left?
VP
TO VP
VB
...
VP
TO+VB VP
...
TO VB
(d) ADOPT?VP,TO,VP,VB,left?
PP
VBG PP
IN
...
VP
VBG+IN PP
...
VBG IN
(e) ADOPT?PP,VBG,PP,IN,left?
Figure 9: Illustrations of the top 5 transformations from
Table 2.
most useful transformations almost by definition are
ones that are triggered the most frequently, any rea-
sonably sized training set is likely to contain them,
and so it is not actually likely that dramatically in-
creasing the size of the training set will yield partic-
ularly large gains.
Thus, to train our TBL system, we extracted a ran-
dom subset of 3000 sentences to serve as a train-
ing set.7 We also extracted an additional 1000 sen-
tence test set to use for rapidly evaluating agreement
score generalization. Figure 10 illustrates the im-
provements in agreement score for the automatically
annotated data, analogous to Figure 8. The same
general patterns hold, although we do see that the
automatically annotated data is more idiosyncratic
and so more than twice as many transformations are
learned before training set agreement stops improv-
ing, even though the training set sizes are roughly
the same.8 Furthermore, test set generalization in
the automatic annotation setting is a little bit worse,
with later transformations tending to actually hurt
test set agreement.
For our machine translation experiments, we used
the string-to-tree syntactic pipeline included in the
current version of Moses (Koehn et al2007).
Our training bitext was approximately 21.8 mil-
lion words, and the sentences and word alignments
were the same for all experiments; the only differ-
ence between each experiment was the English trees,
for which we tested a range of transformation se-
quence prefixes (including a 0-length prefix, which
just yields the original trees, as a baseline). Since
the transformed trees tended to be more finely artic-
ulated, and increasing the number of unique spans
often helps with rule extraction (Wang et al2007),
we equalized the span count by also testing bina-
rized versions of each set of trees, using the left-
branching and right-branching binarization scripts
included with Moses.9
We tuned on 1000 sentence pairs and tested on
7The sentences were shorter on average than those in the En-
glish Chinese Translation Treebank, so this training set contains
roughly the same number of words as that used in the experi-
ments from Section 5.
8Note that the training set improvement curves don?t actu-
ally flatten out because training halts once no improving trans-
formation exists.
9Binarized trees are guaranteed to have k ? 1 unique spans
for sentences of length k.
870
0 
1 
2 
3 
4 
5 
6 
7 
0 1000 2000 3000 4000 5000 
Aver
age A
greem
ent S
core 
Impr
ovem
ent 
Number of Transformations 
Training Test 
Figure 10: Transformation results on a subset of the MT
training data. The training and test sets are disjoint in
order to measure how well the learned transformation se-
quence generalizes. Once again, we plot the average im-
provement over the baseline trees. Though 5151 transfor-
mations were learned from the training set, the maximum
test set agreement was achieved at 630 transformations,
with an average improvement of 2.60.
642 sentence pairs from the NIST MT04 and MT05
data sets, using the BLEU metric (Papineni et al
2001). As discussed by Clark et al2011), the op-
timizer included with Moses (MERT, Och, 2003) is
not always particularly stable, and results (even on
the tuning set) can vary dramatically across tuning
runs. To mitigate this effect, we first used the Moses
training scripts to extract a table of translation rules
for each set of English trees. Then, for each rule
table, we ran MERT 11 times and selected the pa-
rameters that achieved the maximum tuning BLEU
to use for decoding the test set.
Table 3 shows the results of our translation exper-
iments. The best translation results are achieved by
using the first 139 transformations, giving a BLEU
improvement of more than 0.9 over the strongest
baseline.
7 Conclusion
We have demonstrated a simple but effective pro-
cedure for learning a tree transformation sequence
that improves agreement between parse trees and
word alignments. This method yields clear improve-
ments in the quality of Chinese to English trans-
lation, showing that by manipulating English syn-
tax to converge with Chinese phrasal structure, we
improve our ability to explicitly model the types of
Transfor- Agrmnt BLEU
mations Score None Left Right
0 5.36 31.66 31.81 31.84
32 7.17 32.41 32.17 32.06
58 7.42 32.18 32.68* 32.37
139 7.81 32.20 32.60* 32.77*
630 7.96 32.48 32.06 32.22
5151 7.89 32.13 31.84 32.12
Table 3: Machine translation results. Agreement scores
are taken from the test data used to generate Figure 10.
Note that using 0 transformations just yields the original
baseline trees. The transformation sequence cutoffs at 32,
58, and 139 were chosen to correspond to marginal train-
ing (total) agreement gain thresholds of 50, 25, and 10,
respectively. The cutoff at 630 was chosen to maximize
test agreement score and the cutoff at 5151 maximized
training agreement score. Column headings for BLEU
scores (?None,? ?Left,? ?Right?) refer to the type of bina-
rization used after transformations. Entries marked with
a ?*? show a statistically significant difference (p < 0.05)
from the strongest (right-binarized) baseline, according
to the paired bootstrap (Efron and Tibshirani, 1994).
structural relationships between languages that syn-
tactic MT systems are designed to exploit, even if we
lose some fidelity to the original monolingual anno-
tation standards in the process.
Acknowledgements
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-12-C-0014.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
William J. Black and Argyrios Vasilakopoulos. 2002.
Language independent named entity classification by
modified transformation-based learning and by deci-
sion tree induction. In COLING.
Eric Brill and Philip Resnik. 1994. A transformation-
based approach to prepositional phrase attachment dis-
ambiguation. In COLING.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the workshop on Speech and
Natural Language.
871
Eric Brill. 1993. Automatic grammar induction and pars-
ing free text: A transformation-based approach. In
ACL.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In NAACL:HLT.
David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL:HLT.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Bradley Efron and R. J. Tibshirani. 1994. An Introduc-
tion to the Bootstrap (Chapman & Hall/CRC Mono-
graphs on Statistics & Applied Probability). Chapman
and Hall/CRC.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In HLT-NAACL.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a parser for ma-
chine translation reordering. In EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In ACL:HLT.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.
Franz Josef Och. 2003. Miminal error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. Research report, IBM.
RC22176.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning. In
ACL Workshop on Very Large Corpora.
Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998. Dialogue act tagging with transformation-based
learning. In COLING.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Bi-
narizing syntax trees to improve syntax-based machine
translation accuracy. In EMNLP.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence to
string translation model. In ACL-IJCNLP.
Bing Zhao, Young-Suk Lee, Xiaoqiang Luo, and Liu Li.
2011. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In ACL:HLT.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
872
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 995?1005, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An Empirical Investigation of Statistical Significance in NLP
Taylor Berg-Kirkpatrick David Burkett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dburkett, klein}@cs.berkeley.edu
Abstract
We investigate two aspects of the empirical
behavior of paired significance tests for NLP
systems. First, when one system appears
to outperform another, how does significance
level relate in practice to the magnitude of the
gain, to the size of the test set, to the similar-
ity of the systems, and so on? Is it true that for
each task there is a gain which roughly implies
significance? We explore these issues across
a range of NLP tasks using both large collec-
tions of past systems? outputs and variants of
single systems. Next, once significance lev-
els are computed, how well does the standard
i.i.d. notion of significance hold up in practical
settings where future distributions are neither
independent nor identically distributed, such
as across domains? We explore this question
using a range of test set variations for con-
stituency parsing.
1 Introduction
It is, or at least should be, nearly universal that NLP
evaluations include statistical significance tests to
validate metric gains. As important as significance
testing is, relatively few papers have empirically in-
vestigated its practical properties. Those that do
focus on single tasks (Koehn, 2004; Zhang et al
2004) or on the comparison of alternative hypothe-
sis tests (Gillick and Cox, 1989; Yeh, 2000; Bisani
and Ney, 2004; Riezler and Maxwell, 2005).
In this paper, we investigate two aspects of the
empirical behavior of paired significance tests for
NLP systems. For example, all else equal, larger
metric gains will tend to be more significant. How-
ever, what does this relationship look like and how
reliable is it? What should be made of the conven-
tional wisdom that often springs up that a certain
metric gain is roughly the point of significance for
a given task (e.g. 0.4 F1 in parsing or 0.5 BLEU
in machine translation)? We show that, with heavy
caveats, there are such thresholds, though we also
discuss the hazards in their use. In particular, many
other factors contribute to the significance level, and
we investigate several of them. For example, what
is the effect of the similarity between the two sys-
tems? Here, we show that more similar systems tend
to achieve significance with smaller metric gains, re-
flecting the fact that their outputs are more corre-
lated. What about the size of the test set? For ex-
ample, in designing a shared task it is important to
know how large the test set must be in order for sig-
nificance tests to be sensitive to small gains in the
performance metric. Here, we show that test size
plays the largest role in determining discrimination
ability, but that we get diminishing returns. For ex-
ample, doubling the test size will not obviate the
need for significance testing.
In order for our results to be meaningful, we must
have access to the outputs of many of NLP sys-
tems. Public competitions, such as the well-known
CoNLL shared tasks, provide one natural way to ob-
tain a variety of system outputs on the same test
set. However, for most NLP tasks, obtaining out-
puts from a large variety of systems is not feasible.
Thus, in the course of our investigations, we propose
a very simple method for automatically generating
arbitrary numbers of comparable system outputs and
we then validate the trends revealed by our synthetic
method against data from public competitions. This
methodology itself could be of value in, for exam-
ple, the design of new shared tasks.
Finally, we consider a related and perhaps even
more important question that can only be answered
empirically: to what extent is statistical significance
on a test corpus predictive of performance on other
test corpora, in-domain or otherwise? Focusing on
constituency parsing, we investigate the relationship
between significance levels and actual performance
995
on data from outside the test set. We show that when
the test set is (artificially) drawn i.i.d. from the same
distribution that generates new data, then signifi-
cance levels are remarkably well-calibrated. How-
ever, as the domain of the new data diverges from
that of the test set, the predictive ability of signifi-
cance level drops off dramatically.
2 Statistical Significance Testing in NLP
First, we review notation and standard practice in
significance testing to set up our empirical investi-
gation.
2.1 Hypothesis Tests
When comparing a new system A to a baseline sys-
tem B, we want to know if A is better than B on
some large population of data. Imagine that we sam-
ple a small test set x = x1, . . . , xn on which A
beats B by ?(x). Hypothesis testing guards against
the case where A?s victory over B was an unlikely
event, due merely to chance. We would therefore
like to know how likely it would be that a new, in-
dependent test set x? would show a similar victory
for A assuming that A is no better than B on the
population as a whole; this assumption is the null
hypothesis, denoted H0.
Hypothesis testing consists of attempting to esti-
mate this likelihood, written p(?(X) > ?(x)|H0),
where X is a random variable over possible test sets
of size n that we could have drawn, and ?(x) is a
constant, the metric gain we actually observed. Tra-
ditionally, if p(?(X) > ?(x)|H0) < 0.05, we say
that the observed value of ?(x) is sufficiently un-
likely that we should reject H0 (i.e. accept that A?s
victory was real and not just a random fluke). We
refer to p(?(X) > ?(x)|H0) as p-value(x).
In most cases p-value(x) is not easily computable
and must be approximated. The type of approxi-
mation depends on the particular hypothesis testing
method. Various methods have been used in the NLP
community (Gillick and Cox, 1989; Yeh, 2000; Rie-
zler and Maxwell, 2005). We use the paired boot-
strap1 (Efron and Tibshirani, 1993) because it is one
1Riezler and Maxwell (2005) argue the benefits of approx-
imate randomization testing, introduced by Noreen (1989).
However, this method is ill-suited to the type of hypothesis we
are testing. Our null hypothesis does not condition on the test
data, and therefore the bootstrap is a better choice.
1. Draw b bootstrap samples x(i) of size n by
sampling with replacement from x.
2. Initialize s = 0.
3. For each x(i) increment s if ?(x(i)) > 2?(x).
4. Estimate p-value(x) ? sb
Figure 1: The bootstrap procedure. In all of our experiments
we use b = 106, which is more than sufficient for the bootstrap
estimate of p-value(x) to stabilize.
of the most widely used (Och, 2003; Bisani and Ney,
2004; Zhang et al 2004; Koehn, 2004), and be-
cause it can be easily applied to any performance
metric, even complex metrics like F1-measure or
BLEU (Papineni et al 2002). Note that we could
perform the experiments described in this paper us-
ing another method, such as the paired Student?s t-
test. To the extent that the assumptions of the t-test
are met, it is likely that the results would be very
similar to those we present here.
2.2 The Bootstrap
The bootstrap estimates p-value(x) though a com-
bination of simulation and approximation, drawing
many simulated test sets x(i) and counting how often
A sees an accidental advantage of ?(x) or greater.
How can we get sample test sets x(i)? We lack the
ability to actually draw new test sets from the un-
derlying population because all we have is our data
x. The bootstrap therefore draws each x(i) from x
itself, sampling n items from x with replacement;
these new test sets are called bootstrap samples.
Naively, it might seem like we would then check
how often A beats B by more than ?(x) on x(i).
However, there?s something seriously wrong with
these x(i) as far as the null hypothesis is concerned:
the x(i) were sampled from x, and so their average
?(x(i)) won?t be zero like the null hypothesis de-
mands; the average will instead be around ?(x). If
we ask how many of these x(i) have A winning by
?(x), about half of them will. The solution is a re-
centering of the mean ? we want to know how often
A does more than ?(x) better than expected. We ex-
pect it to beat B by ?(x). Therefore, we count up
how many of the x(i) have A beating B by at least
2?(x).2 The pseudocode is shown in Figure 1.
2Note that many authors have used a variant where the event
tallied on the x(i) is whether ?(x(i)) < 0, rather than ?(x(i)) >
2?(x). If the mean of ?(x(i)) is ?(x), and if the distribution of
?(x(i)) is symmetric, then these two versions will be equivalent.
996
As mentioned, a major benefit of the bootstrap is
that any evaluation metric can be used to compute
?(x).3 We run the bootstrap using several metrics:
F1-measure for constituency parsing, unlabeled de-
pendency accuracy for dependency parsing, align-
ment error rate (AER) for word alignment, ROUGE
score (Lin, 2004) for summarization, and BLEU
score for machine translation.4 We report all met-
rics as percentages.
3 Experiments
Our first goal is to explore the relationship be-
tween metric gain, ?(x), and statistical significance,
p-value(x), for a range of NLP tasks. In order to say
anything meaningful, we will need to see both ?(x)
and p-value(x) for many pairs of systems.
3.1 Natural Comparisons
Ideally, for a given task and test set we could obtain
outputs from all systems that have been evaluated
in published work. For each pair of these systems
we could run a comparison and compute both ?(x)
and p-value(x). While obtaining such data is not
generally feasible, for several tasks there are pub-
lic competitions to which systems are submitted by
many researchers. Some of these competitions make
system outputs publicly available. We obtained sys-
tem outputs from the TAC 2008 workshop on auto-
matic summarization (Dang and Owczarzak, 2008),
the CoNLL 2007 shared task on dependency parsing
(Nivre et al 2007), and the WMT 2010 workshop
on machine translation (Callison-Burch et al 2010).
For cases where the metric linearly decomposes over sentences,
the mean of ?(x(i)) is ?(x). By the central limit theorem, the
distribution will be symmetric for large test sets; for small test
sets it may not.
3Note that the bootstrap procedure given only approximates
the true significance level, with multiple sources of approxima-
tion error. One is the error introduced from using a finite num-
ber of bootstrap samples. Another comes from the assumption
that the bootstrap samples reflect the underlying population dis-
tribution. A third is the assumption that the mean bootstrap gain
is the test gain (which could be further corrected for if the metric
is sufficiently ill-behaved).
4To save time, we can compute ?(x) for each bootstrap sam-
ple without having to rerun the evaluation metric. For our met-
rics, sufficient statistics can be recorded for each sentence and
then sampled along with the sentences when constructing each
x(i) (e.g. size of gold, size of guess, and number correct are suf-
ficient for F1). This makes the bootstrap very fast in practice.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
ROUGE
Different research groups
Same research group
Figure 2: TAC 2008 Summarization: Confidence vs.
ROUGE improvement on TAC 2008 test set for comparisons
between all pairs of the 58 participating systems at TAC 2008.
Comparisons between systems entered by the same research
group and comparisons between systems entered by different
research groups are shown separately.
3.1.1 TAC 2008 Summarization
In our first experiment, we use the outputs of the
58 systems that participated in the TAC 2008 work-
shop on automatic summarization. For each possi-
ble pairing, we compute ?(x) and p-value(x) on the
non-update portion of the TAC 2008 test set (we or-
der each pair so that the gain, ?(x), is always pos-
itive).5 For this task, test instances correspond to
document collections. The test set consists of 48
document collections, each with a human produced
summary. Figure 2 plots the ROUGE gain against
1 ? p-value, which we refer to as confidence. Each
point on the graph corresponds to an individual pair
of systems.
As expected, larger gains in ROUGE correspond
to higher confidences. The curved shape of the plot
is interesting. It suggests that relatively quickly we
reach ROUGE gains for which, in practice, signif-
icance tests will most likely be positive. We might
expect that systems whose outputs are highly corre-
lated will achieve higher confidence at lower met-
ric gains. To test this hypothesis, in Figure 2 we
5In order to run bootstraps between all pairs of systems
quickly, we reuse a random sample counts matrix between boot-
strap runs. As a result, we no longer need to perform quadrat-
ically many corpus resamplings. The speed-up from this ap-
proach is enormous, but one undesirable effect is that the boot-
strap estimation noise between different runs is correlated. As a
remedy, we set b so large that the correlated noise is not visible
in plots.
997
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different research groups
Same research group
Figure 3: CoNLL 2007 Dependency parsing: Confidence vs.
unlabeled dependency accuracy improvement on the Chinese
CoNLL 2007 test set for comparisons between all pairs of the
21 participating systems in CoNLL 2007 shared task. Com-
parisons between systems entered by the same research group
and comparisons between systems entered by different research
groups are shown separately.
separately show the comparisons between systems
entered by the same research group and compar-
isons between systems entered by different research
groups, with the expectation that systems entered by
the same group are likely to have more correlated
outputs. Many of the comparisons between systems
submitted by the same group are offset from the
main curve. It appears that they do achieve higher
confidences at lower metric gains.
Given the huge number of system comparisons in
Figure 2, one obvious question to ask is whether
we can take the results of all these statistical sig-
nificance tests and estimate a ROUGE improvement
threshold that predicts when future statistical sig-
nificance tests will probably be significant at the
p-value(x) < 0.05 level. For example, let?s say we
take all the comparisons with p-value between 0.04
and 0.06 (47 comparisons in all in this case). Each
of these comparisons has an associated metric gain,
and by taking, say, the 95th percentile of these met-
ric gains, we get a potentially useful threshold. In
this case, the computed threshold is 1.10 ROUGE.
What does this threshold mean? Well, based on
the way we computed it, it suggests that if somebody
reports a ROUGE increase of around 1.10 on the ex-
act same test set, there is a pretty good chance that a
statistical significance test would show significance
at the p-value(x) < 0.05 level. After all, 95% of
the borderline significant differences that we?ve al-
ready seen showed an increase of even less than 1.10
ROUGE. If we?re evaluating past work, or are in
some other setting where system outputs just aren?t
available, the threshold could guide our interpreta-
tion of reports containing only summary scores.
That being said, it is important that we don?t over-
interpret the meaning of the 1.10 ROUGE threshold.
We have already seen that pairs of systems submit-
ted by the same research group and by different re-
search groups follow different trends, and we will
soon see more evidence demonstrating the impor-
tance of system correlation in determining the rela-
tionship between metric gain and confidence. Addi-
tionally, in Section 4, we will see that properties of
the test corpus have a large effect on the trend. There
are many factors are at work, and so, of course, met-
ric gain alone will not fully determine the outcome
of a paired significance test.
3.1.2 CoNLL 2007 Dependency Parsing
Next, we run an experiment for dependency pars-
ing. We use the outputs of the 21 systems that par-
ticipated in the CoNLL 2007 shared task on depen-
dency parsing. In Figure 3, we plot, for all pairs,
the gain in unlabeled dependency accuracy against
confidence on the CoNLL 2007 Chinese test set,
which consists of 690 sentences and parses. We
again separate comparisons between systems sub-
mitted by the same research group and those submit-
ted by different groups, although for this task there
were fewer cases of multiple submission. The re-
sults resemble the plot for summarization; we again
see a curve-shaped trend, and comparisons between
systems from the same group (few that they are)
achieve higher confidences at lower metric gains.
3.1.3 WMT 2010 Machine Translation
Our final task for which system outputs are pub-
licly available is machine translation. We run an ex-
periment using the outputs of the 31 systems par-
ticipating in WMT 2010 on the system combination
portion of the German-English WMT 2010 news test
set, which consists of 2,034 German sentences and
English translations. We again run comparisons for
pairs of participating systems. We plot gain in test
BLEU score against confidence in Figure 4. In this
experiment there is an additional class of compar-
998
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different research groups
Same research group
System combination
Figure 4: WMT 2010 Machine translation: Confidence vs.
BLEU improvement on the system combination portion of the
German-English WMT 2010 news test set for comparisons be-
tween pairs of the 31 participating systems at WMT 2010.
Comparisons between systems entered by the same research
group, comparisons between systems entered by different re-
search groups, and comparisons between system combination
entries are shown separately.
isons that are likely to have specially correlated sys-
tems: 13 of the submitted systems are system com-
binations, and each take into account the same set
of proposed translations. We separate comparisons
into three sets: comparisons between non-combined
systems entered by different research groups, com-
parisons between non-combined systems entered by
the same research group, and comparisons between
system-combinations.
We see the same curve-shaped trend we saw for
summarization and dependency parsing. Differ-
ent group comparisons, same group comparisons,
and system combination comparisons form distinct
curves. This indicates, again, that comparisons be-
tween systems that are expected to be specially cor-
related achieve high confidence at lower metric gain
levels.
3.2 Synthetic Comparisons
So far, we have seen a clear empirical effect, but, be-
cause of the limited availability of system outputs,
we have only considered a few tasks. We now pro-
pose a simple method that captures the shape of the
effect, and use it to extend our analysis.
3.2.1 Training Set Resampling
Another way of obtaining many different sys-
tems? outputs is to obtain implementations of a
handful of systems, and then vary some aspect of
the training procedure in order to produce many dif-
ferent systems from each implementation. Koehn
(2004) uses this sort of amplification; he uses a sin-
gle machine translation implementation, and then
trains it from different source languages. We take
a slightly different approach. For each task we pick
some fixed training set. Then we generate resampled
training sets by sampling sentences with replace-
ment from the original. In this way, we can gen-
erate as many new training sets as we like, each of
which is similar to the original, but with some vari-
ation. For each base implementation, we train a new
system on each resampled training set. This results
in slightly tweaked trained systems, and is intended
to very roughly approximate the variance introduced
by incremental system changes during research. We
validate this method by comparing plots obtained by
the synthetic approach with plots obtained from nat-
ural comparisons.
We expect that each new system will be differ-
ent, but that systems originating from the same base
model will be highly correlated. This provides a use-
ful division of comparisons: those between systems
built with the same model, and those between sys-
tems built with different models. The first class can
be used to approximate comparisons of systems that
are expected to be specially correlated, and the latter
for comparisons of systems that are not.
3.2.2 Dependency Parsing
We use three base models for dependency parsing:
MST parser (McDonald et al 2005), Maltparser
(Nivre et al 2006), and the ensemble parser of Sur-
deanu and Manning (2010). We use the CoNLL
2007 Chinese training set, which consists of 57K
sentences. We resample 5 training sets of 57K sen-
tences, 10 training sets of 28K sentences, and 10
training sets of 14K sentences. Together, this yields
a total of 75 system outputs on the CoNLL 2007
Chinese test set, 25 systems for each base model
type. The score ranges of all the base models over-
lap. This ensures that for each pair of model types
we will be able to see comparisons where the metric
gains are small. The results of the pairwise compar-
isons of all 75 system outputs are shown in Figure
5, along with the results of the CoNLL 2007 shared
task system comparisons from Figure 3.
999
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2  2.5  3  3.5  4
1  
-  
p-
va
lue
Unlabeled Acc.
Different model types
Same model type
CoNLL 2007 comparisons
Figure 5: Dependency parsing: Confidence vs. unlabeled de-
pendency accuracy improvement on the Chinese CoNLL 2007
test set for comparisons between all pairs of systems gener-
ated by using resampled training sets to train either MST parser,
Maltparser, or the ensemble parser. Comparisons between sys-
tems generated using the same base model type and compar-
isons between systems generated using different base model
types are shown separately. The CoNLL 2007 shared task com-
parisons from Figure 3 are also shown.
The overlay of the natural comparisons suggests
that the synthetic approach reasonably models the
relationship between metric gain and confidence.
Additionally, the different model type and same
model type comparisons exhibit the behavior we
would expect, matching the curves corresponding to
comparisons between specially correlated systems
and standard comparisons respectively.
Since our synthetic approach yields a large num-
ber of system outputs, we can use the procedure
described in Section 3.1.1 to compute the thresh-
old above which the metric gain is probably signifi-
cant. For comparisons between systems of the same
model type, the threshold is 1.20 unlabeled depen-
dency accuracy. For comparisons between systems
of different model types, the threshold is 1.51 un-
labeled dependency accuracy. These results indi-
cate that the similarity of the systems being com-
pared is an important factor. As mentioned, rules-
of-thumb derived from such thresholds cannot be
applied blindly, but, in special cases where two sys-
tems are known to be correlated, the former thresh-
old should be preferred over the latter. For example,
during development most comparisons are made be-
tween incremental variants of the same system. If
adding a feature to a supervised parser increases un-
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
Different model types
Same model type
WMT 2010 comparisons
Figure 6: Machine translation: Confidence vs. BLEU im-
provement on the system combination portion of the German-
English WMT 2010 news test set for comparisons between all
pairs of systems generated by using resampled training sets to
train either Moses or Joshua. Comparisons between systems
generated using the same base model type and comparisons be-
tween systems generated using different base model types are
shown separately. The WMT 2010 workshop comparisons from
Figure 4 are also shown.
labeled accuracy by 1.3, it is useful to be able to
quickly estimate that the improvement is probably
significant. This still isn?t the full story; we will
soon see that properties of the test set al play a
major role. But first, we carry our analysis to sev-
eral more tasks.
3.2.3 Machine Translation
Our two base models for machine translation
are Moses (Koehn et al 2007) and Joshua (Li et
al., 2009). We use 1.4M sentence pairs from the
German-English portion of the WMT-provided Eu-
roparl (Koehn, 2005) and news commentary corpora
as the original training set. We resample 75 training
sets, 20 of 1.4M sentence pairs, 29 of 350K sentence
pairs, and 26 of 88K sentence pairs. This yields a
total of 150 system outputs on the system combi-
nation portion of the German-English WMT 2010
news test set. The results of the pairwise compar-
isons of all 150 system outputs are shown in Figure
6, along with the results of the WMT 2010 workshop
system comparisons from Figure 4.
The natural comparisons from the WMT 2010
workshop align well with the comparisons between
synthetically varied models. Again, the different
model type and same model type comparisons form
distinct curves. For comparisons between systems
1000
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
AER
Different model types
Same model type
Figure 7: Word alignment: Confidence vs. AER improve-
ment on the Hansard test set for comparisons between all pairs
of systems generated by using resampled training sets to train
either the ITG aligner, the joint HMM aligner, or GIZA++.
Comparisons between systems generated using the same base
model type and comparisons between systems generated using
different base model types are shown separately.
of the same model type the computed p-value <
0.05 threshold is 0.28 BLEU. For comparisons be-
tween systems of different model types the threshold
is 0.37 BLEU.
3.2.4 Word Alignment
Now that we have validated our simple model of
system variation on two tasks, we go on to gen-
erate plots for tasks that do not have competitions
with publicly available system outputs. The first
task is English-French word alignment, where we
use three base models: the ITG aligner of Haghighi
et al(2009), the joint HMM aligner of Liang et al
(2006), and GIZA++ (Och and Ney, 2003). The last
two aligners are unsupervised, while the first is su-
pervised. We train the unsupervised word aligners
using the 1.1M sentence pair Hansard training cor-
pus, resampling 20 training sets of the same size.6
Following Haghighi et al(2009), we train the super-
vised ITG aligner using the first 337 sentence pairs
of the hand-aligned Hansard test set; again, we re-
sample 20 training sets of the same size as the origi-
nal data. We test on the remaining 100 hand-aligned
sentence pairs from the Hansard test set.
Unlike previous plots, the points corresponding
to comparisons between systems with different base
6GIZA++ failed to produce reasonable output when trained
with some of these training sets, so there are fewer than 20
GIZA++ systems in our comparisons.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
Different model types
Same model type
Figure 8: Constituency parsing: Confidence vs. F1 improve-
ment on section 23 of the WSJ corpus for comparisons between
all pairs of systems generated by using resampled training sets
to train either the Berkeley parser, the Stanford parser, or the
Collins parser. Comparisons between systems generated us-
ing the same base model type and comparisons between sys-
tems generated using different base model types are shown sep-
arately.
model types form two distinct curves. It turns out
that the upper curve consists only of comparisons
between ITG and HMM aligners. This is likely due
to the fact that the ITG aligner uses posteriors from
the HMM aligner for some of its features, so the
two models are particularly correlated. Overall, the
spread of this plot is larger than previous ones. This
may be due to the small size of the test set, or possi-
bly some additional variance introduced by unsuper-
vised training. For comparisons between systems of
the same model type the p-value < 0.05 threshold
is 0.50 AER. For comparisons between systems of
different model types the threshold is 1.12 AER.
3.2.5 Constituency Parsing
Finally, before we move on to further types of
analysis, we run an experiment for the task of con-
stituency parsing. We use three base models: the
Berkeley parser (Petrov et al 2006), the Stanford
parser (Klein and Manning, 2003), and Dan Bikel?s
implementation (Bikel, 2004) of the Collins parser
(Collins, 1999). We use sections 2-21 of the WSJ
corpus (Marcus et al 1993), which consists of 38K
sentences and parses, as a training set. We resample
10 training sets of size 38K, 10 of size 19K, and 10
of size 9K, and use these to train systems. We test
on section 23. The results are shown in Figure 8.
For comparisons between systems of the same
1001
model type, the p-value < 0.05 threshold is 0.47
F1. For comparisons between systems of different
model types the threshold is 0.57 F1.
4 Properties of the Test Corpus
For five tasks, we have seen a trend relating met-
ric gain and confidence, and we have seen that the
level of correlation between the systems being com-
pared affects the location of the curve. Next, we
look at how the size and domain of the test set play
a role, and, finally, how significance level predicts
performance on held out data. In this section, we
carry out experiments for both machine translation
and constituency parsing, but mainly focus on the
latter because of the availability of large test corpora
that span more than one domain: the Brown corpus
and the held out portions of the WSJ corpus.
4.1 Varying the Size
Figure 9 plots comparisons for machine translation
on variously sized initial segments of the WMT
2010 news test set. Similarly, Figure 10 plots com-
parisons for constituency parsing on initial segments
of the Brown corpus. As might be expected, the
size of the test corpus has a large effect. For both
machine translation and constituency parsing, the
larger the corpus size, the lower the threshold for
p-value < 0.05 and the smaller the spread of the
plot. At one extreme, the entire Brown corpus,
which consists of approximately 24K sentences, has
a threshold of 0.22 F1, while at the other extreme,
the first 100 sentences of the Brown corpus have a
threshold of 3.00 F1. Notice that we see diminishing
returns as we increase the size of the test set. This
phenomenon follows the general shape of the cen-
tral limit theorem, which predicts that variances of
observed metric gains will shrink according to the
square root of the test size. Even using the entire
Brown corpus as a test set there is a small range
where the result of a paired significance test was not
completely determined by metric gain.
It is interesting to note that for a fixed test size,
the domain has only a small effect on the shape of
the curve. Figure 11 plots comparisons for a fixed
test size, but with various test corpora.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
1  
-  
p-
va
lue
BLEU
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
Figure 9: Machine translation; varying test size: Confidence
vs. BLEU improvement on portions of the German-English
WMT 2010 news test set.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
100 sent.
200 sent.
400 sent.
800 sent.
1600 sent.
3200 sent.
Entire Brown corpus
Figure 10: Constituency parsing; varying test size: Con-
fidence vs. F1 improvement on portions of the Brown corpus.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.5  1  1.5  2
1  
-  
p-
va
lue
F1
WSJ sec 22
WSJ sec 23
WSJ sec 24
Brown corpus
Figure 11: Constituency parsing; varying domain: Confi-
dence vs. F1 improvement on the first 1,600 sentences of sec-
tions 22, 23, and 24 of the WSJ corpus, and the Brown corpus.
1002
4.2 Empirical Calibration across Domains
Now that we have a way of generating outputs for
thousands of pairs of systems, we can check empir-
ically the practical reliability of significance testing.
Recall that the bootstrap p-value(x) is an approxi-
mation to p(?(X) > ?(x)|H0). However, we often
really want to determine the probability that the new
system is better than the baseline on the underlying
test distribution or even the distribution from another
domain. There is no reason a priori to expect these
numbers to coincide.
In our next experiment, we treat the entire Brown
corpus, which consists of 24K sentences, as the true
population of English sentences. For each system
generated in the way described in Section 3.2.5 we
compute F1 on all of Brown. Since we are treat-
ing the Brown corpus as the actual population of En-
glish sentences, for each pair of parsers we can say
that the sign of the F1 difference indicates which is
the truly better system. Now, we repeatedly resam-
ple small test sets from Brown, each consisting of
1,600 sentences, drawn by sampling sentences with
replacement. For each pair of systems, and for each
resampled test set, we compute p-value(x) using the
bootstrap. Out of the 4K bootstraps computed in this
way, 942 had p-value between 0.04 and 0.06, 869
of which agreed with the sign of the F1 difference
we saw on the entire Brown corpus. Thus, 92% of
the significance tests with p-value in a tight range
around 0.05 correctly identified the better system.
This result is encouraging. It suggests that sta-
tistical significance computed using the bootstrap is
reasonably well calibrated. However, test sets are
almost never drawn i.i.d. from the distribution of in-
stances the system will encounter in practical use.
Thus, we also wish to compute how calibration de-
grades as the domain of the test set changes. In an-
other experiment, we look at how significance near
p-value = 0.05 on section 23 of the WSJ corpus
predicts performance on sections 22 and 24 and the
Brown corpus. This time, for each pair of generated
systems we run a bootstrap on section 23. Out of
all these bootstraps, 58 system pairs had p-value be-
tween 0.04 and 0.06. Of these, only 83% had the
same sign of F1 difference on section 23 as they did
on section 22, 71% the had the same sign on sec-
tion 23 as on section 24, and 48% the same sign on
Sec. 23 p-value
% Sys. A > Sys. B
Sec. 22 Sec. 24 Brown
0.00125 - 0.0025 97% 95% 73%
0.0025 - 0.005 92% 92% 60%
0.005 - 0.01 92% 85% 56%
0.01 - 0.02 88% 92% 54%
0.02 - 0.04 87% 78% 51%
0.04 - 0.08 83% 74% 48%
Table 1: Empirical calibration: p-value on section 23 of the
WSJ corpus vs. fraction of comparisons where system A beats
system B on section 22, section 24, and the Brown corpus. Note
that system pairs are ordered so that A always outperforms B on
section 23.
section 23 as on the Brown corpus. This indicates
that reliability degrades as we switch the domain. In
the extreme, achieving a p-value near 0.05 on sec-
tion 23 provides no information about performance
on the Brown corpus.
If we intend to use our system on out-of-domain
data, these results are somewhat discouraging. How
low does p-value(x) have to get before we start get-
ting good information about out-of-domain perfor-
mance? We try to answer this question for this par-
ticular parsing task by running the same domain cal-
ibration experiment for several different ranges of
p-value. The results are shown in Table 1. From
these results, it appears that for constituency pars-
ing, when testing on section 23, a p-value level be-
low 0.00125 is required to reasonably predict perfor-
mance on the Brown corpus.
It should be considered a good practice to include
statistical significance testing results with empiri-
cal evaluations. The bootstrap in particular is easy
to run and makes relatively few assumptions about
the task or evaluation metric. However, we have
demonstrated some limitations of statistical signifi-
cance testing for NLP. In particular, while statistical
significance is usually a minimum necessary condi-
tion to demonstrate that a performance difference is
real, it?s also important to consider the relationship
between test set performance and the actual goals
of the systems being tested, especially if the system
will eventually be used on data from a different do-
main than the test set used for evaluation.
5 Conclusion
We have demonstrated trends relating several im-
portant factors to significance level, which include
1003
both properties of the systems being compared and
properties of the test corpus, and have presented a
simple approach to approximating the response of
these factors for tasks where large numbers of sys-
tem outputs are not available. Our results reveal
that the relationship between metric gain and sta-
tistical significance is complex, and therefore sim-
ple thresholds are not a replacement for significance
tests. Indeed, we strongly advocate the use of statis-
tical significance testing to validate metric gains in
NLP, but also note that informal rules-of-thumb do
arise in popular discussion and that, for some set-
tings when previous systems are unavailable, these
empirical results can supplement less sensitive un-
paired tests (e.g. bar-overlaps-point test) in evalua-
tion of progress. Finally, even formal testing has its
limits. We provide cautionary evidence to this ef-
fect, showing that the information provided by a test
quickly degrades as the target corpus shifts domain.
Acknowledgements
This work was partially supported by NSF fellow-
ships to the first and second authors and by the NSF
under grant 0643742.
References
D.M. Bikel. 2004. Intricacies of collins? parsing model.
Computational Linguistics.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in asr performance evaluation. In
Proc. of ICASSP.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O.F. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proc. of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
H.T. Dang and K. Owczarzak. 2008. Overview of the
tac 2008 update summarization task. In Proc. of Text
Analysis Conference.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall/CRC.
L. Gillick and S.J. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proc. of ICASSP.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch,
S. Khudanpur, L. Schwartz, W.N.G. Thornton,
J. Weese, and O.F. Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proc. of the Fourth Workshop on Statistical Machine
Translation.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Proc. of the Workshop on Text
Summarization.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
The penn treebank. Computational linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of EMNLP.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley, New
York.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
1004
S. Riezler and J.T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing for mt.
In Proc. of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
M. Surdeanu and C.D. Manning. 2010. Ensemble mod-
els for dependency parsing: cheap and good? In Proc.
of NAACL.
A. Yeh. 2000. More accurate tests for the statistical sig-
nificance of result differences. In Proc. of ACL.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system. In Proc. of LREC.
1005
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1048?1059, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Parser Showdown at the Wall Street Corral:
An Empirical Investigation of Error Types in Parser Output
Jonathan K. Kummerfeld? David Hall? James R. Curran? Dan Klein?
?Computer Science Division ? e-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,dlwh,klein}@cs.berkeley.edu james@it.usyd.edu.au
Abstract
Constituency parser performance is primarily
interpreted through a single metric, F-score
on WSJ section 23, that conveys no linguis-
tic information regarding the remaining errors.
We classify errors within a set of linguisti-
cally meaningful types using tree transforma-
tions that repair groups of errors together. We
use this analysis to answer a range of ques-
tions about parser behaviour, including what
linguistic constructions are difficult for state-
of-the-art parsers, what types of errors are be-
ing resolved by rerankers, and what types are
introduced when parsing out-of-domain text.
1 Introduction
Parsing has been a major area of research within
computational linguistics for decades, and con-
stituent parser F-scores on WSJ section 23 have ex-
ceeded 90% (Petrov and Klein, 2007), and 92%
when using self-training and reranking (McClosky
et al 2006; Charniak and Johnson, 2005). While
these results give a useful measure of overall per-
formance, they provide no information about the na-
ture, or relative importance, of the remaining errors.
Broad investigations of parser errors beyond the
PARSEVAL metric (Abney et al 1991) have either
focused on specific parsers, e.g. Collins (2003), or
have involved conversion to dependencies (Carroll
et al 1998; King et al 2003). In all of these cases,
the analysis has not taken into consideration how a
set of errors can have a common cause, e.g. a single
mis-attachment can create multiple node errors.
We propose a new method of error classifica-
tion using tree transformations. Errors in the parse
tree are repaired using subtree movement, node cre-
ation, and node deletion. Each step in the process is
then associated with a linguistically meaningful er-
ror type, based on factors such as the node that is
moved, its siblings, and parents.
Using our method we analyse the output of thir-
teen constituency parsers on newswire. Some of
the frequent error types that we identify are widely
recognised as challenging, such as prepositional
phrase (PP) attachment. However, other significant
types have not received as much attention, such as
clause attachment and modifier attachment.
Our method also enables us to investigate where
reranking and self-training improve parsing. Pre-
viously, these developments were analysed only in
terms of their impact on F-score. Similarly, the chal-
lenge of out-of-domain parsing has only been ex-
pressed in terms of this single objective. We are able
to decompose the drop in performance and show that
a disproportionate number of the extra errors are due
to coordination and clause attachment.
This work presents a comprehensive investigation
of parser behaviour in terms of linguistically mean-
ingful errors. By applying our method to multiple
parsers and domains we are able to answer questions
about parser behaviour that were previously only ap-
proachable through approximate measures, such as
counts of node errors. We show which errors have
been reduced over the past fifteen years of parsing
research; where rerankers are making their gains and
where they are not exploiting the full potential of k-
best lists; and what types of errors arise when mov-
ing out-of-domain. We have released our system1 to
enable future work to apply our methodology.
1http://code.google.com/p/berkeley-parser-analyser/
1048
2 Background
Most attempts to understand the behaviour of con-
stituency parsers have focused on overall evaluation
metrics. The three main methods are intrinsic eval-
uation with PARSEVAL, evaluation on dependencies
extracted from the constituency parse, and evalua-
tion on downstream tasks that rely on parsing.
Intrinsic evaluation with PARSEVAL, which calcu-
lates precision and recall over labeled tree nodes, is
a useful indicator of overall performance, but does
not pinpoint which structures the parser has most
difficulty with. Even when the breakdown for par-
ticular node types is presented (e.g. Collins, 2003),
the interaction between node errors is not taken into
account. For example, a VP node could be missing
because of incorrect PP attachment, a coordination
error, or a unary production mistake. There has been
some work that addresses these issues by analysing
the output of constituency parsers on linguistically
motivated error types, but only by hand on sets of
around 100 sentences (Hara et al 2007; Yu et al
2011). By automatically classifying parse errors we
are able to consider the output of multiple parsers on
thousands of sentences.
The second major parser evaluation method in-
volves extraction of grammatical relations (King et
al., 2003; Briscoe and Carroll, 2006) or dependen-
cies (Lin, 1998; Briscoe et al 2002). These met-
rics have been argued to be more informative and
generally applicable (Carroll et al 1998), and have
the advantage that the breakdown over dependency
types is more informative than over node types.
There have been comparisons of multiple parsers
(Foster and van Genabith, 2008; Nivre et al 2010;
Cer et al 2010), as well as work on finding rela-
tions between errors (Hara et al 2009), and break-
ing down errors by a range of factors (McDonald and
Nivre, 2007). However, one challenge is that results
for constituency parsers are strongly influenced by
the dependency scheme being used and how easy it
is to extract the dependencies from a given parser?s
output (Clark and Hockenmaier, 2002). Our ap-
proach does not have this disadvantage, as we anal-
yse parser output directly.
The third major approach involves extrinsic eval-
uation, where the parser?s output is used in a down-
stream task, such as machine translation (Quirk
and Corston-Oliver, 2006), information extraction
(Miyao et al 2008), textual entailment (Yuret et
al., 2010), or semantic dependencies (Dridan and
Oepen, 2011). While some of these approaches give
a better sense of the impact of parse errors, they re-
quire integration into a larger system, making it less
clear where a given error originates.
The work we present here differs from existing
approaches by directly and automatically classifying
errors into meaningful types. This enables the first
very broad, yet detailed, study of parser behaviour,
evaluating the output of thirteen parsers over thou-
sands of sentences.
3 Parsers
Our evaluation is over a wide range of PTB con-
stituency parsers and their variants from the past fif-
teen years. For all parsers we used the publicly avail-
able version, with the standard parameter settings.
Berkeley (Petrov et al 2006; Petrov and Klein,
2007). An unlexicalised parser with a grammar
constructed with automatic state splitting.
Bikel (2004) implementation of Collins (1997).
BUBS (Dunlop et al 2011; Bodenstab et al
2011). A ?grammar-agnostic constituent
parser,? which uses a Berkeley Parser grammar,
but parses with various pruning techniques to
improve speed, at the cost of accuracy.
Charniak (2000). A generative parser with a max-
imum entropy-inspired model. We also use the
reranker (Charniak and Johnson, 2005), and the
self-trained model (McClosky et al 2006).
Collins (1997). A generative lexicalised parser,
with three models, a base model, a model that
uses subcategorisation frames for head words,
and a model that takes into account traces.
SSN (Henderson, 2003; Henderson, 2004). A sta-
tistical left-corner parser, with probabilities es-
timated by a neural network.
Stanford (Klein and Manning, 2003a; Klein and
Manning, 2003b). We consider both the un-
lexicalised PCFG parser (-U) and the factored
parser (-F), which combines the PCFG parser
with a lexicalised dependency parser.
1049
System F P R Exact Speed
ENHANCED TRAINING / SYSTEMS
Charniak-SR 92.07 92.44 91.70 44.87 1.8
Charniak-R 91.41 91.78 91.04 44.04 1.8
Charniak-S 91.02 91.16 90.89 40.77 1.8
STANDARD PARSERS
Berkeley 90.06 90.30 89.81 36.59 4.2
Charniak 89.71 89.88 89.55 37.25 1.8
SSN 89.42 89.96 88.89 32.74 1.8
BUBS 88.50 88.57 88.43 31.62 27.6
Bikel 88.16 88.23 88.10 32.33 0.8
Collins-3 87.66 87.82 87.50 32.22 2.0
Collins-2 87.62 87.77 87.48 32.51 2.2
Collins-1 87.09 87.29 86.90 30.35 3.3
Stanford-L 86.42 86.35 86.49 27.65 0.7
Stanford-U 85.78 86.48 85.09 28.35 2.7
Table 1: PARSEVAL results on WSJ section 23 for the
parsers we consider. The columns are F-score, precision,
recall, exact sentence match, and speed (sents/sec). Cov-
erage was left out as it was above 99.8% for all parsers.
In the ENHANCED TRAINING / SYSTEMS section we in-
clude the Charniak parser with reranking (R), with a self-
trained model (S), and both (SR).
Table 1 shows the standard performance metrics,
measured on section 23 of the WSJ, using all sen-
tences. Speeds were measured using a Quad-Core
Xeon CPU (2.33GHz 4MB L2 cache) with 16GB
of RAM. These results clearly show the variation in
parsing performance, but they do not show which
constructions are the source of those variations.
4 Error Classification
While the statistics in Table 1 give a sense of over-
all parser performance they do not provide linguisti-
cally meaningful intuition for the source of remain-
ing errors. Breaking down the remaining errors by
node type is not particularly informative, as a sin-
gle attachment error can cause multiple node errors,
many of which are for unrelated node types. For
example, in Figure 1 there is a PP attachment error
that causes seven bracket errors (extra S, NP, PP, and
NP, missing S, NP, and PP). Determining that these
correspond to a PP attachment error from just the la-
bels of the missing and extra nodes is difficult. In
contrast, the approach we describe below takes into
consideration the relations between errors, grouping
them into linguistically meaningful sets.
We classify node errors in two phases. First, we
S
VP
VP
S
NP
PP
NP
PP
in 1986
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(a) Parser output
S
VP
VP
PP
in 1986
S
NP
PP
NP
NNP
Applied
IN
of
NP
chief executive officer
VBN
named
VBD
was
NP
PRP
He
(b) Gold tree
Figure 1: Grouping errors by node type is of limited use-
fulness. In this figure and those that follow the top tree
is the incorrect parse and the bottom tree is the correct
parse. Bold, boxed nodes are either extra (marked in the
incorrect tree) or missing (marked in the correct tree).
This is an example of PP Attachment (in 1986 is too
low), but that is not at all clear from the set of incorrect
nodes (extra S, NP, PP, and NP, missing S, NP, and PP).
find a set of tree transformations that convert the out-
put tree into the gold tree. Second, the transforma-
tion are classified into error types such as PP attach-
ment and coordination. Pseudocode for our method
is shown in Algorithm 1. The tree transformation
stage corresponds to the main loop, while the sec-
ond stage corresponds to the final loop.
4.1 Tree Transformation
The core of our transformation process is a set of op-
erations that move subtrees, create nodes, and delete
nodes. Searching for the shortest path to transform
one tree into another is prohibitively slow.2 We find
2We implemented various search procedures and found sim-
ilar results on the sentences that could be processed in a reason-
1050
Algorithm 1 Tree transformation error classification
U = initial set of node errors
Sort U by the depth of the error in the tree, deepest first
G = ?
repeat
for all errors e ? U do
if e fits an environment template t then
g = new error group
Correct e as specified by t
for all errors f that t corrects do
Remove f from U
Insert f into g
end for
Add g to G
end if
end for
until unable to correct any further errors
for all remaining errors e ? U do
Insert a group into G containing e
end for
for all groups g ? G do
Classify g based on properties of the group
end for
a path by applying a greedy bottom?up approach,
iterating through the errors in order of tree depth.
We match each error with a template based on
nearby tree structure and errors. For example, in
Figure 1 there are four extra nodes that all cover
spans ending at Applied in 1986: S, NP, PP, NP.
There are also three missing nodes with spans end-
ing between Applied and in: PP, NP, and S. Figure 2
depicts these errors as spans, showing that this case
fits three criteria: (1) there are a set of extra spans all
ending at the same point, (2) there are a set of miss-
ing spans all ending at the same point, and (3) the ex-
tra spans cross the missing spans, extending beyond
their end-point. This indicates that the node start-
ing after Applied is attaching too low and should be
moved up, outside all of the extra nodes. Together,
the criteria and transformation form a template.
Once a suitable template is identified we correct
the error by moving subtrees, adding nodes and re-
moving nodes. In the example this is done by mov-
ing the node spanning in 1986 up in the tree until it
is outside of all the extra spans. Since moving the PP
leaves a unary production from an NP to an NP, we
also collapse that level. In total this corrects seven
able amount of time.
named chief executive officer of Applied in 1986
Figure 2: Templates are defined in terms of extra and
missing spans, shown here with unbroken lines above and
dashed lines below, respectively. This is an example of a
set of extra spans that cross a set of missing spans (which
in both cases all end at the same position). If the last two
words are moved, two of the extra spans will match the
two missing spans. The other extra span is deleted during
the move as it creates an NP?NP unary production.
errors, as there are three cases in which an extra node
is present that matches a missing node once the PP
is moved. All of these errors are placed in a single
group and information about the nearby tree struc-
ture before and after the transformation is recorded.
We continue to make passes through the list until
no errors are corrected on a pass. For each remaining
node error an individual error group is created.
The templates were constructed by hand based on
manual analysis of parser output. They cover a range
of combinations of extra and missing spans, with
further variation for whether crossing is occurring
and if so whether the crossing bracket starts or ends
in the middle of the correct bracket. Errors that do
not match any of our templates are left uncorrected.
4.2 Transformation Classification
We began with a large set of node errors, in the first
stage they were placed into groups, one group per
tree transformation used to get from the test tree to
the gold tree. Next we classify each group as one of
the error types below.
PP Attachment Any case in which the transforma-
tion involved moving a Prepositional Phrase, or
the incorrect bracket is over a PP, e.g.
He was (VP named chief executive officer of
fill(NP Applied (PP in 1986)))
where (PP in 1986) should modify the entire
VP, rather than just Applied.
NP Attachment Several cases in which NPs had to
be moved, particularly for mistakes in appos-
itive constructions and incorrect attachments
within a verb phrase, e.g.
The bonds (VP go (PP on sale (NP Oct. 19)))
where Oct. 19 should be an argument of go.
1051
VP
NP
NN
today
NP
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(a) Parser output
VP
NP
VP
NP
NN
today
VBG
appearing
NP
NN
ad
JJ
new
DT
another
VBD
wrote
(b) Gold tree
Figure 3: NP Attachment: today is too high, it should
be the argument of appearing, rather than wrote. This
causes three node errors (extra NP, missing NP and VP).
VP
ADVP
ahead of time
S
VP
VP
PP
about it
VB
think
TO
to
VBD
had
(a) Parser output
VP
S
VP
VP
ADVP
ahead of time
PP
about it
VB
think
TO
to
VBD
had
(b) Gold tree
Figure 4: Modifier Attachment: ahead of time is too
high, it should modify think, not had. This causes six
node errors (extra S, VP, and VP, missing S, VP, and VP).
Modifier Attachment Cases involving incorrectly
placed adjectives and adverbs, including errors
corrected by subtree movement and errors re-
quiring only creation of a node, e.g.
(NP (ADVP even more) severe setbacks)
where there should be an extra ADVP node
over even more severe.
Clause Attachment Any group that involves move-
ment of some form of S node.
VP
S
VP
VP
SBAR
unless the agency . . .
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(a) Parser output
VP
SBAR
unless the agency . . .
S
VP
VP
NP
the RTC to . . .
VB
restrict
TO
to
VBZ
intends
(b) Gold tree
Figure 5: Clause Attachment: unless the agency re-
ceives specific congressional authorization is attaching
too low. This causes six node errors (extra S, VP, and
VP, missing S, VP and VP).
SINV
NP
PP
of major market activity
NP
a breakdown
VBZ
is
VP
VBG
Following
(a) Parser output
SINV
NP
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S
VP
VBG
Following
(b) Gold tree
SINV
:
:
NP-SBJ-1
NP
PP
of major market activity
NP
a breakdown
VBZ
is
S-ADV
VP
VBG
Following
NP-SBJ
-NONE-
*-1
(c) Gold tree with traces and function tags
Figure 6: Two Unary errors, a missing S and a missing
NP. The third tree is the PTB tree before traces and func-
tion tags are removed. Note that the missing NP is over
another NP, a production that does occur widely in the
treebank, particularly over the word it.
1052
NP
PP
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
Mannesmann AG
IN
for
NP
A 16% drop
(a) Parser output
NP
NP
Dresdner AG?s 10% decline
CC
and
NP
PP
NP
Mannesmann AG
IN
for
NP
A 16% drop
(b) Gold tree
Figure 7: Coordination: and Dresdner AG?s 10% de-
cline is too low. This causes four node errors (extra PP
and NP, missing NP and PP).
Unary Mistakes involving unary productions that
are not linked to a nearby error such as a match-
ing extra or missing node. We do not include a
breakdown by unary type, though we did find
that clause labeling (S, SINV, etc) accounted
for a large proportion of the errors.
Coordination Cases in which a conjunction is an
immediate sibling of the nodes being moved, or
is the leftmost or rightmost node being moved.
NP Internal Structure While most NP structure is
not annotated in the PTB, there is some use of
ADJP, NX, NAC and QP nodes. We form a
single group for each NP that has one or more
errors involving these types of nodes.
Different label In many cases a node is present in
the tree that spans the correct set of words, but
has the wrong label, in which case we group the
two node errors, (one extra, one missing), as a
single error.
Single word phrase A range of node errors that
span a single word, with checks to ensure this
is not linked to another error (e.g. one part of a
set of internal noun phrase errors).
Other There is a long tail of other errors. Some
could be placed within the categories above,
but would require far more specific rules.
For many of these error types it would be diffi-
cult to extract a meaningful understanding from only
NP
PP
NP
NNP
Baker
NNP
State
IN
of
NNP
Secretary
(a) Parser output
NP
NNP
Baker
PP
NP
NNP
State
IN
of
NNP
Secretary
(b) Gold tree
Figure 8: NP Internal Structure: Baker is too low, caus-
ing four errors (extra PP and NP, missing PP and NP).
the list of node errors involved. Even for error types
that can be measured by counting node errors or rule
production errors, our approach has the advantage
that we identify groups of errors with a single cause.
For example, a missing unary production may corre-
spond to an extra bracket that contains a subtree that
attached incorrectly.
4.3 Methodology
We used sections 00 and 24 as development data
while constructing the tree transformation and error
group classification methods. All of our examples
in text come from these sections as well, but for all
tables of results we ran our system on section 23.
We chose to run our analysis on section 23 as it is
the only section we are sure was not used in the de-
velopment of any of the parsers, either for tuning or
feature development. Our evaluation is entirely fo-
cused on the errors of the parsers, so unless there is
a particular construction that is unusually prevalent
in section 23, we are not revealing any information
about the test set that could bias future work.
5 Results
Our system enables us to answer questions about
parser behaviour that could previously only be
probed indirectly. We demonstrate its usefulness by
applying it to a range of parsers (here), to reranked
K-best lists of various lengths, and to output for out-
of-domain parsing (following sections).
In Table 2 we consider the breakdown of parser
1053
PP Clause Diff Mod NP 1-Word NP
Parser F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.60 0.38 0.31 0.25 0.25 0.23 0.20 0.14 0.14 0.50
Charniak-RS 92.07
Charniak-R 91.41
Charniak-S 91.02
Berkeley 90.06
Charniak 89.71
SSN 89.42
BUBS 88.63
Bikel 88.16
Collins-3 87.66
Collins-2 87.62
Collins-1 87.09
Stanford-F 86.42
Stanford-U 85.78
Worst 1.12 0.61 0.51 0.39 0.45 0.40 0.42 0.27 0.27 1.13
Table 2: Average number of bracket errors per sentence due to the top ten error types. For instance, Stanford-U
produces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment. The scale for
each column is indicated by the Best and Worst values.
Nodes
Error Type Occurrences Involved Ratio
PP Attachment 846 1455 1.7
Single word phrase 490 490 1.0
Clause Attachment 385 913 2.4
Modifier Attachment 383 599 1.6
Different Label 377 754 2.0
Unary 347 349 1.0
NP Attachment 321 597 1.9
NP Internal Structure 299 352 1.2
Coordination 209 557 2.7
Unary Clause Label 185 200 1.1
VP Attachment 64 159 2.5
Parenthetical Attachment 31 74 2.4
Missing Parenthetical 12 17 1.4
Unclassified 655 734 1.1
Table 3: Breakdown of errors on section 23 for the Char-
niak parser with self-trained model and reranker. Errors
are sorted by the number of times they occur. Ratio is the
average number of node errors caused by each error we
identify (i.e. Nodes Involved / Occurrences).
errors on WSJ section 23. The shaded area of
each bar indicates the frequency of parse errors (i.e.
empty means fewest errors). The area filled in is
determined by the expected number of node errors
per sentence that are attributed to that type of error.
The average number of node errors per sentence for
a completely full bar is indicated by the Worst row,
and the value for a completely empty bar is indicated
by the Best row. Exact error counts are available at
http://code.google.com/p/berkeley-parser-analyser/.
We use counts of node errors to make the con-
tributions of each type of error more interpretable.
As Table 3 shows, some errors typically cause only
a single node error, where as others, such as co-
ordination, generally cause several. This means
that considering counts of error groups would over-
emphasise some error types, e.g. single word phrase
errors are second most important by number of
groups (in Table 3), but seventh by total number of
node errors (in Table 2).
As expected, PP attachment is the largest contrib-
utor to errors, across all parsers. Interestingly, coor-
dination is sixth on the list, though that is partly due
to the fact that there are fewer coordination decisions
to be made in the treebank.3
By looking at the performance of the Collins
parser we can see the development over the past
fifteen years. There has been improvement across
the board, but in some cases, e.g. clause attach-
ment errors and different label errors, the change has
been more limited (24% and 29% reductions respec-
tively). We investigated the breakdown of the differ-
ent label errors by label, but no particular cases of la-
3This is indicated by the frequency of CCs and PPs in sec-
tions 02?21 of the treebank, 16,844 and 95,581 respectively.
These counts are only an indicator of the number of decisions
as the nodes can be used in ways that do not involve a decision,
such as sentences that start with a conjunction.
1054
PP Clause Diff Mod NP 1-Word NP
System K F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.08 0.04 0.08 0.05 0.06 0.04 0.08 0.04 0.04 0.11
1000 98.30
100 97.54
50 97.18
Oracle 20 96.40
10 95.66
5 94.61
2 92.59
1000 92.07
100 92.08
50 92.07
Charniak 20 92.05
10 92.16
5 91.94
2 91.56
1 91.02
Worst 0.66 0.43 0.33 0.26 0.28 0.26 0.23 0.16 0.19 0.60
Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser
with reranking and the self-trained model. The oracle results are determined by taking the parse in each K-best list
with the highest F-score.
bel confusion stand out, and we found that the most
common cases remained the same between Collins
and the top results.
It is also interesting to compare pairs of parsers
that share aspects of their architecture. One such
pair is the Stanford parser, where the factored parser
combines the unlexicalised parser with a lexicalised
dependency parser. The main sources of the 0.64
gain in F-score are PP attachment and coordination.
Another interesting pair is the Berkeley parser and
the BUBS parser, which uses a Berkeley grammar,
but improves speed by pruning. The pruning meth-
ods used in BUBS are particularly damaging for PP
attachment errors and unary errors.
Various comparisons can be made between Char-
niak parser variants. We discuss the reranker be-
low. For the self-trained model McClosky et al
(2006) performed some error analysis, considering
variations in F-score depending on the frequency of
tags such as PP, IN and CC in sentences. Here we
see gains on all error types, though particularly for
clause attachment, modifier attachment and coordi-
nation, which fits with their observations.
5.1 Reranking
The standard dynamic programming approach to
parsing limits the range of features that can be em-
ployed. One way to deal with this issue is to mod-
ify the parser to produce the top K parses, rather
than just the 1-best, then use a model with more so-
phisticated features to choose the best parse from
this list (Collins, 2000). While re-ranking has led to
gains in performance (Charniak and Johnson, 2005),
there has been limited analysis of how effectively
rerankers are using the set of available options. Re-
cent work has explored this question in more depth,
but focusing on how variation in the parameters
impacts performance on standard metrics (Huang,
2008; Ng et al 2010; Auli and Lopez, 2011; Ng
and Curran, 2012).
In Table 4 we present a breakdown over error
types for the Charniak parser, using the self-trained
model and reranker. The oracle results use the parse
in each K-best list with the highest F-score. While
this may not give the true oracle result, as F-score
does not factor over sentences, it gives a close ap-
proximation. The table has the same columns as Ta-
ble 2, but the ranges on the bars now reflect the min
and max for these sets.
While there is improvement on all errors when us-
ing the reranker, there is very little additional gain
beyond the first 5-10 parses. Even for the oracle
results, most of the improvement occurs within the
first 5-10 parses. The limited utility of extra parses
1055
PP Clause Diff Mod NP 1-Word NP
Corpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other
Best 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021
WSJ 23 92.07
Brown-F 85.91
Brown-G 84.56
Brown-K 84.09
Brown-L 83.95
Brown-M 84.65
Brown-N 85.20
Brown-P 84.09
Brown-R 83.60
G-Web Blogs 84.15
G-Web Email 81.18
Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073
Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and
the self-trained model. We use per word error rates here rather than per sentence as there is great variation in average
sentence length across the domains, skewing the per sentence results.
for the reranker may be due to the importance of
the base parser output probability feature (which, by
definition, decreases within the K-best list).
Interestingly, the oracle performance improves
across all error types, even at the 2-best level. This
indicates that the base parser model is not particu-
larly biased against a single error. Focusing on the
rows for K = 2 we can also see two interesting out-
liers. The PP attachment improvement of the ora-
cle is considerably higher than that of the reranker,
particularly compared to the differences for other er-
rors, suggesting that the reranker lacks the features
necessary to make the decision better than the parser.
The other interesting outlier is NP internal structure,
which continues to make improvements for longer
lists, unlike the other error types.
5.2 Out-of-Domain
Parsing performance drops considerably when shift-
ing outside of the domain a parser was trained on
(Gildea, 2001). Clegg and Shepherd (2005) evalu-
ated parsers qualitatively on node types and rule pro-
ductions. Bender et al(2011) designed a Wikipedia
test set to evaluate parsers on dependencies repre-
senting ten specific linguistic phenomena.
To provide a deeper understanding of the er-
rors arising when parsing outside of the newswire
domain, we analyse performance of the Charniak
parser with reranker and self-trained model on the
eight parts of the Brown corpus (Marcus et al
Corpus Description Sentences Av. Length
WSJ 23 Newswire 2416 23.5
Brown F Popular 3164 23.4
Brown G Biographies 3279 25.5
Brown K General 3881 17.2
Brown L Mystery 3714 15.7
Brown M Science 881 16.6
Brown N Adventure 4415 16.0
Brown P Romance 3942 17.4
Brown R Humour 967 22.7
G-Web Blogs Blogs 1016 23.6
G-Web Email E-mail 2450 11.9
Table 6: Variation in size and contents of the domains we
consider. The variation in average sentence lengths skews
the results for errors per sentences, and so in Table 5 we
consider errors per word.
1993), and two parts of the Google Web corpus
(Petrov and McDonald, 2012). Table 6 shows statis-
tics for the corpora. The variation in average sen-
tence lengths skew the results for errors per sen-
tence. To handle this we divide by the number of
words to determine the results in Table 5, rather than
by the number of sentences, as in previous figures.
There are several interesting features in the table.
First, on the Brown datasets, while the general trend
is towards worse performance on all errors, NP in-
ternal structure is a notable exception and in some
cases PP attachment and unaries are as well.
In the other errors we see similar patterns across
the corpora, except humour (Brown R), on which the
parser is particularly bad at coordination and clause
1056
attachment. This makes sense, as the colloquial na-
ture of the text includes more unusual uses of con-
junctions, for example:
She was a living doll and no mistake ? the ...
Comparing the Brown corpora and the Google
Web corpora, there are much larger divergences. We
see a particularly large decrease in NP internal struc-
ture. Looking at some of the instances of this error, it
appears to be largely caused by incorrect handling of
structures such as URLs and phone numbers, which
do not appear in the PTB. There are also some more
difficult cases, for example:
... going up for sale in the next month or do .
where or do is a QP. This typographical error is ex-
tremely difficult to handle for a parser trained only
on well-formed text.
For e-mail there is a substantial drop on single
word phrases. Breaking the errors down by label we
found that the majority of the new errors are miss-
ing or extra NPs over single words. Here the main
problem appears to be temporal expressions, though
there also appear to be a substantial number of errors
that are also at the POS level, such as when NNP is
assigned to ta in this case:
... let you know that I ?m out ta here !
Some of these issues, such as URL handling,
could be resolved with suitable training data. Other
issues, such as ungrammatical language and uncon-
ventional use of words, pose a greater challenge.
6 Conclusion
The single F-score objective over brackets or depen-
dencies obscures important differences between sta-
tistical parsers. For instance, a single attachment er-
ror can lead to one or many mismatched brackets.
We have created a novel tree-transformation
methodology for evaluating parsers that categorises
errors into linguistically meaningful types. Using
this approach, we presented the first detailed exam-
ination of the errors produced by a wide range of
constituency parsers for English. We found that PP
attachment and clause attachment are the most chal-
lenging constructions, while coordination turns out
to be less problematic than previously thought. We
also noted interesting variations in error types for
parsers variants.
We investigated the errors resolved in reranking,
and introduced by changing domains. We found that
the Charniak rerankers improved most error types,
but made little headway on improving PP attach-
ment. Changing domain has an impact on all error
types, except NP internal structure.
We have released our system so that future con-
stituent parsers can be evaluated using our method-
ology. Our analysis provides new insight into the
development of parsers over the past fifteen years,
and the challenges that remain.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was par-
tially supported by a General Sir John Monash Fel-
lowship to the first author, the Office of Naval Re-
search under MURI Grant No. N000140911081, an
NSF Fellowship to the second author, ARC Discov-
ery grant DP1097291, the Capital Markets CRC, and
the NSF under grant 0643742.
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311, Pacific Grove,
California, USA, February.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings of ACL, pages 470?480, Portland, Oregon, USA,
June.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local deep dependencies in a large corpus. In Proceed-
ings of EMNLP, pages 397?408, Edinburgh, United
Kingdom, July.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of ACL, pages
440?449, Portland, Oregon, USA, June.
1057
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of ACL, pages 41?
48, Sydney, Australia, July.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake, 2002. Relational Evaluation Schemes,
pages 4?8. Las Palmas, Canary Islands, Spain, May.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal.
In Proceedings of LREC, pages 447?454, Granada,
Spain, May.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In Proceedings of LREC, Valletta, Malta,
May.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, USA, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL, pages 132?139,
Seattle, Washington, USA, April.
Stephen Clark and Julia Hockenmaier. 2002. Evaluat-
ing a wide-coverage ccg parser. In Proceedings of the
LREC Beyond Parseval Workshop, Las Palmas, Ca-
nary Islands, Spain, May.
Andrew B. Clegg and Adrian J. Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedical
corpus. In Proceedings of the ACL Workshop on Soft-
ware, pages 14?33, Ann Arbor, Michigan, USA, June.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL,
pages 16?23, Madrid, Spain, July.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML, pages
175?182, Palo Alto, California, USA, June.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Rebecca Dridan and Stephan Oepen. 2011. Parser evalu-
ation using elementary dependency matching. In Pro-
ceedings of IWPT, pages 225?230, Dublin, Ireland,
October.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2011. Efficient matrix-encoded grammars and low la-
tency parallelization strategies for cyk. In Proceedings
of IWPT, pages 163?174, Dublin, Ireland, October.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings of LREC, Mar-
rakech, Morocco, May.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of EMNLP, pages 167?202,
Pittsburgh, Pennsylvania, USA, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of IWPT, pages 11?22, Prague,
Czech Republic, June.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2009. Descriptive and empirical approaches to captur-
ing underlying dependencies among parsing errors. In
Proceedings of EMNLP, pages 1162?1171, Singapore,
August.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of NAACL, pages 24?31, Edmonton, Canada,
May.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
ACL, pages 95?102, Barcelona, Spain, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Columbus, Ohio, USA, June.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the 4th Inter-
national Workshop on Linguistically Interpreted Cor-
pora at EACL, Budapest, Hungary, April.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of ACL,
pages 423?430, Sapporo, Japan, July.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In Proceedings of NIPS, pages 3?10,
Vancouver, British Columbia, Canada, December.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of NAACL, pages 152?159, New York, New York,
USA, June.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of EMNLP, pages 122?131,
Prague, Czech Republic, June.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
1058
Proceedings of ACL, pages 46?54, Columbus, Ohio,
USA, June.
Dominick Ng and James R. Curran. 2012. N-best CCG
parsing and reranking. In Proceedings of ACL, Jeju,
South Korea, July.
Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage ccg parser. In Pro-
ceedings of ALTA, pages 90?98, Melbourne, Australia,
December.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
Go?mez-Rodr??guez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of Coling, pages 833?841, Beijing, China, August.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411, Rochester, New York, USA, April.
Slav Petrov and Ryan McDonald. 2012. SANCL Shared
Task. LDC2012E43. Linguistic Data Consortium.
Philadelphia, Philadelphia, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL, pages
433?440, Sydney, Australia, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP,
pages 62?69, Sydney, Australia, July.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
IWPT, pages 48?57, Dublin, Ireland, October.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using textual
entailments. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 51?56, Up-
psala, Sweden, July.
1059
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1146?1156, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Training Factored PCFGs with Expectation Propagation
David Hall and Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
PCFGs can grow exponentially as additional
annotations are added to an initially simple
base grammar. We present an approach where
multiple annotations coexist, but in a factored
manner that avoids this combinatorial explo-
sion. Our method works with linguistically-
motivated annotations, induced latent struc-
ture, lexicalization, or any mix of the three.
We use a structured expectation propagation
algorithm that makes use of the factored struc-
ture in two ways. First, by partitioning the fac-
tors, it speeds up parsing exponentially over
the unfactored approach. Second, it minimizes
the redundancy of the factors during training,
improving accuracy over an independent ap-
proach. Using purely latent variable annota-
tions, we can efficiently train and parse with
up to 8 latent bits per symbol, achieving F1
scores up to 88.4 on the Penn Treebank while
using two orders of magnitudes fewer parame-
ters compared to the na??ve approach. Combin-
ing latent, lexicalized, and unlexicalized anno-
tations, our best parser gets 89.4 F1 on all sen-
tences from section 23 of the Penn Treebank.
1 Introduction
Many high-performance PCFG parsers take an ini-
tially simple base grammar over treebank labels like
NP and enrich it with deeper syntactic features to
improve accuracy. This broad characterization in-
cludes lexicalized parsers (Collins, 1997), unlexical-
ized parsers (Klein and Manning, 2003), and latent
variable parsers (Matsuzaki et al 2005). Figures
1(a), 1(b), and 1(c) show small examples of context-
free trees that have been annotated in these ways.
When multi-part annotations are used in the same
grammar, systems have generally multiplied these
annotations together, in the sense that an NP that
was definite, possessive, and VP-dominated would
have a single unstructured PCFG symbol that en-
coded all three facts. In addition, modulo backoff
or smoothing, that unstructured symbol would of-
ten have rewrite parameters entirely distinct from,
say, the indefinite but otherwise similar variant of
the symbol (Klein and Manning, 2003). Therefore,
when designing a grammar, one would have to care-
fully weigh new contextual annotations. Should a
definiteness annotation be included, doubling the
number of NPs in the grammar and perhaps overly
fragmenting statistics? Or should it be excluded,
thereby losing important distinctions? Klein and
Manning (2003) discuss exactly such trade-offs and
omit annotations that were helpful on their own be-
cause they were not worth the combinatorial or sta-
tistical cost when combined with other annotations.
In this paper, we argue for grammars with fac-
tored annotations, that is, grammars with annota-
tions that have structured component parts that are
partially decoupled. Our annotated grammars can
include both latent and explicit annotations, as illus-
trated in Figure 1(d), and we demonstrate that these
factored grammars outperform parsers with unstruc-
tured annotations.
After discussing the factored representation, we
describe a method for parsing with factored anno-
tations, using an approximate inference technique
called expectation propagation (Minka, 2001). Our
algorithm has runtime linear in the number of an-
notation factors in the grammar, improving on the
na??ve algorithm, which has runtime exponential in
the number of annotations. Our method, the Ex-
pectation Propagation for Inferring Constituency
(EPIC) parser, jointly trains a model over factored
annotations, where each factor naturally leverages
information from other annotation factors and im-
proves on their mistakes.
1146
(a) NP[agenda]
NN[agenda]
agenda
NP[?s]
The president?s
(b) NP[?S]
NN[?NP]
agenda
NP[?NP-Poss-Det]
The president?s
(c) NP[1]
NN[0]
agenda
NP[1]
The president?s
(d) NP[agenda,?S,1]
NN[agenda,?NP,0]
agenda
NP[?s,?NP-Poss-Det,1]
The president?s
Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in Collins (1997);
(b) Unlexicalized annotation like that in Klein and Manning (2003); (c) Latent annotation like that in Matsuzaki et al
(2005); and (d) the factored, mixed annotations we argue for in our paper.
We demonstrate the empirical effectiveness of our
approach in two ways. First, we efficiently train
a latent-variable grammar with 8 disjoint one-bit
latent annotation factors, with scores as high as
89.7 F1 on length ?40 sentences from the Penn
Treebank (Marcus et al 1993). This latent vari-
able parser outscores the best of Petrov and Klein
(2008a)?s comparable parsers while using two or-
ders of magnitude fewer parameters. Second, we
combine our latent variable factors with lexicalized
and unlexicalized annotations, resulting in our best
F1 score of 89.4 on all sentences.
2 Intuitions
Modern theories of grammar such as HPSG (Pollard
and Sag, 1994) and Minimalism (Chomsky, 1992)
do not ascribe unstructured conjunctions of anno-
tations to phrasal categories. Rather, phrasal cat-
egories are associated with sequences of metadata
that control their function. For instance, an NP
might have annotations to the effect that it is sin-
gular, masculine, and nominative, with perhaps fur-
ther information about its animacy or other aspects
of the head noun. Thus, it is appealing for a gram-
mar to be able to model these (somewhat) orthog-
onal notions, but most models have no mechanism
to encourage this. As a notable exception, Dreyer
and Eisner (2006) tried to capture this kind of insight
by allowing factored annotations to pass unchanged
from parent label to child label, though they were not
able to demonstrate substantial gains in accuracy.
Moreover, there has been to our knowledge no at-
tempt to employ both latent and non-latent annota-
tions at the same time. There is good reason for this:
lexicalized or highly annotated grammars like those
of Collins (1997) or Klein and Manning (2003) have
a very large number of states and an even larger
number of rules. Further annotating these rules with
latent annotations would produce an infeasibly large
grammar. Nevertheless, it is a shame to sacrifice ex-
pert annotation just to get latent annotations. Thus,
it makes sense to combine these annotation methods
in a way that does not lead to an explosion of the
state space or a fragmentation of statistics.
3 Parsing with Annotations
Suppose we have a raw (binarized) treebank gram-
mar, with productions of the form A ? B C.
The typical process is to then annotate these rules
with additional information, giving rules of the form
A[x] ? B[y] C[z]. In the case of explicit annota-
tions, an x might include information about the par-
ent category, or a head word, or a combination of
things. In the case of latent annotations, x will be
an integer that may or may not correspond to some
linguistic notion. We are interested in the specific
case where each x is actually factored into M dis-
joint parts: A[x1, x2, . . . , xM ]. (See Figure 1(d).)
We call each component of x an annotation factor
or an annotation component.
1147
3.1 Annotation Classes
In this paper, we consider three kinds of annotation
models, representing three of the major traditions in
constituency parsing. Individually, none of our mod-
els are state-of-the-art, instead achieving F1 scores
in the mid-80?s on the Penn Treebank.
The first model is a relatively simple lexicalized
parser. We are not aware of a prior discriminative
lexicalized constituency parser, and it is quite dif-
ferent from the generative models of Collins (1997).
Broadly, it considers features over a binary rule an-
notated with head words: A[h] ? B[h] C[d] and
A[h] ? B[d] C[h], focusing on monolexical rule
features and bilexical dependency features. It is our
best individual model, scoring 87.3 F1 on the devel-
opment set.
The second is similar to the unlexicalized model
of Klein and Manning (2003). This parser starts
from a grammar with labels annotated with sibling
and parent information, and then adds specific an-
notations, such as whether an NP is possessive or
whether a symbol rewrites as a unary. This parser
gets 86.3, tying the original generative version of
Klein and Manning (2003).
Finally, we use a straightforward discriminative
latent variable model much like that of Petrov and
Klein (2008a). Here, each symbol is given a la-
tent annotation, referred to as a substate. Typically,
these substates correlate at least loosely with linguis-
tic phenomena. For instance, NP-1 might be associ-
ated with possessive NPs, while NP-3 might be for
adjuncts. Often, these latent integers are considered
as bit strings, with each bit indicating one latent an-
notation. Prior work in this area has considered the
effect of splitting and merging these states (Petrov et
al., 2006; Petrov and Klein, 2007), as well as ?mul-
tiscale? grammars (Petrov and Klein, 2008b). With
two states (or one bit of annotation), our version of
this parser gets 81.7 F1, edging out the compara-
ble parser of Petrov and Klein (2008a). On the other
hand, our parser gets 83.2 with four states (two bits),
short of the performance of prior work.1
1Much of the difference stems from the different binariza-
tion scheme we employ. We use head-outward binarization,
rather than the left-branching binarization they employed. This
change was to enable integrating lexicalization with our other
models.
3.2 Model Representation
We employ a general exponential family representa-
tion of our grammar. This representation is fairly
general, and?in its generic form?by no means
new, save for the focus on annotation components.
Formally, we begin with a parse tree T over base
symbols for some sentence w, and we decorate the
tree with annotations X , giving a parse tree T [X].
We focus on the case whenX partitions into disjoint
components X = [X1, X2, . . . , XM ]. These com-
ponents are decoupled in the sense that, conditioned
on the coarse tree T , each column of the annota-
tion is independent of every other column. How-
ever, they are crucially not independent conditioned
only on the sentence w. This model is represented
schematically in Figure 2(a).
The conditional probability P(T [X]|w, ?) of an
annotated tree given words is:
P(T [X]|w, ?)
=
?
m fm(T [Xm];w, ?m)?
T ?,X?
?
m fm(T ?[X ?m];w, ?m)
= 1Z(w, ?)
?
m
fm(T [Xm];w, ?m)
(1)
where the factors fm for each model take the form:
fm(T [Xm];w, ?m) = exp
(
?Tm?m(T,Xm,w)
)
Here, Xm is the annotation associated with a partic-
ular model m. ? is a feature function that projects
the raw tree, annotations, and words into a feature
vector. The features ? need to decompose into fea-
tures for each factor fm; we do not allow features
that take into account the annotation from two dif-
ferent components.
We further add a pruning filter that assigns zero
weight to any tree with a constituent that a baseline
unannotated grammar finds sufficiently unlikely, and
a weight of one to any other tree. This filter is similar
to that used in Petrov and Klein (2008a) and allows
for much more efficient training and inference.
Because our model is discriminative, training
takes the form of maximizing the probability of the
training trees given the words. This objective is con-
vex for deterministic annotations, but non-convex
for latent annotations. We (locally) optimize the
1148
flex funlflat f?lat f?unl
(a) (b) (c) 
Full product model Approximate model 
P (T [X]|w; ?) q(T |w)
qlex qlat
qunl
Figure 2: Schematic representation of our model, its approximation, and expectation propagation. (a) The full joint
distribution consists of a product of three grammars with different annotations, here lexicalized, latent, and unlexi-
calized. This model is described in Section 3.2. (b) The core approximation is an anchored PCFG with one factor
corresponding to each annotation component, described in Section 5.1. (c) Fitting the approximation with expectation
propagation, as described in Section 5.3. At the center is the core approximation. During each step, an ?augmented?
distribution qm is created by taking one annotation factor from the full grammar and the rest from the approximate
grammar. For instance, in upper left hand corner the full fLEX is substituted for f?LEX. This new augmented distribution
is projected back to the core approximation. This process is repeated for each factor until convergence.
(non-convex) log conditional likelihood of the ob-
served training data (T (d),w(d)):
`(?) =
?
d
log P(T (d)|w(d), ?)
=
?
d
log
?
X
P(T (d)[X]|w(d), ?)
(2)
Using standard results, the derivative takes the form:
?`(?) =
?
d
E[?(T,X,w)|T (d),w(d)]
?
?
d
E[?(T,X,w)|w(d)]
(3)
The first half of this derivative can be obtained by the
forward/backward-like computation defined by Mat-
suzaki et al(2005), while the second half requires
an inside/outside computation (Petrov and Klein,
2008a). The partition function Z(w, ?) is computed
as a byproduct of the latter computation. Finally,
this objective is regularized, using the L2 norm of ?
as a penalty.
We note that we omit from our parser one major
feature class found in other discriminative parsers,
namely those that use features over the words in the
span (Finkel et al 2008; Petrov and Klein, 2008b).
These features might condition on words on either
side of the split point of a binary rule or take into
account the length of the span. While such features
have proven useful in previous work, they are not the
focus of our current work and so we omit them.
4 The Complexity of Annotated
Grammars
Note that the first term of Equation 3?which is
conditioned on the coarse tree T?factors into M
pieces, one for each of the annotation components.
However, the second term does not factor because it
is conditioned on just the words w. Indeed, na??vely
computing this term requires parsing with the fully
articulated grammar, meaning that inference would
be no more efficient than parsing with non-factored
annotations.
Standard algorithms for parsing run in time
O(G|w|3), where |w| is the length of the sentence,
and G is the size of the grammar, measured in the
number of (binary) rules. Let G0 be the number
of binary rules in the unannotated ?base? grammar.
1149
Suppose that we have M annotation components.
Each annotation component can have up to A primi-
tive annotations per rule. For instance, a latent vari-
able grammar will have A = 8b where b is the num-
ber of bits of annotation. If we compile all annota-
tion components into unstructured annotations, we
can end up with a total grammar size of O(AMG0),
and so in general parsing time scales exponentially
with the number of annotation components. Thus, if
we use latent annotations and the hierarchical split-
ting approach of Petrov et al(2006), then the gram-
mar has size O(8SG0), where S is the number of
times the grammar was split in two. Therefore, the
size of annotated grammars can reach intractable
levels very quickly, particularly in the case of latent
annotations, where all combinations of annotations
are possible.
Petrov (2010) considered an approach to slowing
this growth down by using a set of M independently
trained parsers Pm, and parsed using the product
of the scores from each parser as the score for the
tree. This approach worked largely because train-
ing was intractable: if the training algorithm could
reach the global optimum, then this approach might
have yielded no gain. However, because the opti-
mization technique is local, the same algorithm pro-
duced multiple grammars.
In what follows, we propose another solution that
exploits the factored structure of our grammar with
expectation propagation. Crucially, we are able to
jointly train and parse with all annotation factors,
minimizing redundancy across the models. While
not exact, we will see that expectation propagation
is indeed effective.
5 Factored Inference
The key insight behind the approximate inference
methods we consider here is that the full model is
a product of complex factors that interact in compli-
cated ways, and we will approximate it with a prod-
uct of corresponding simple factors that interact in
simple ways. Since each annotation factor is a rea-
sonable model in both power and complexity on its
own, we can consider them one at a time, replac-
ing all others with their approximations, as shown in
Figure 2(c).
The way we will build these approximations is
with expectation propagation (Minka, 2001). Ex-
pectation propagation (EP) is a general method for
approximate inference that generalizes belief propa-
gation. We describe it here, but we first try to pro-
vide an intuition for how it functions in our system.
We also describe a simplified version of EP, called
assumed density filtering (Boyen and Koller, 1998),
which is somewhat easier to understand and rhetori-
cally convenient. For a more detailed introduction to
EP in general, we direct the reader to either Minka
(2001) or Wainwright and Jordan (2008). Our treat-
ment most resembles the former.
5.1 Factored Approximations
Our goal is to build an approximation that takes in-
formation from all components into account. To be-
gin, we note that each of these components captures
different phenomena: an unlexicalized grammar is
good at capturing structural relationships in a parse
tree (e.g. subject noun phrases have different dis-
tributions than object noun phrases), while a lexi-
calized grammar captures preferred attachments for
different verbs. At the same time, each of these com-
ponent grammars can be thought of as a refinement
of the raw unannotated treebank grammar. By itself,
each of these grammars induces a different poste-
rior distribution over unannotated trees for each sen-
tence. If we can approximate each model?s contri-
bution by using only unannotated symbols, we can
define an algorithm that avoids the exponential over-
head of parsing with the full grammar, and instead
works with each factor in turn.
To do so, we define a sentence specific core
approximation over unannotated trees q(T |w) =
?
m f?m(T,w). Figure 2(b) illustrates this approx-
imation. Here, q(T ) is a product of M structurally
identical factors, one for each of the annotated com-
ponents. We will approximate each model fm by
its corresponding f?m. Thus, there is one color-
coordinated approximate factor for each component
of the model in Figure 2(a).
There are multiple choices for the structure of
these factors, but we focus on anchored PCFGs. An-
chored PCFGs have productions of the form iAj ?
iBk kCj , where i, k, and j are indexes into the sen-
tence. Here, iAj is a symbol representing building
the base symbol A over the span [i, j].
Billott and Lang (1989) introduced anchored
1150
CFGs as ?shared forests,? and Matsuzaki et al
(2005) have previously used these grammars for
finding an approximate one-best tree in a latent vari-
able parser. Note that, even though an anchored
grammar is unannotated, because it is sentence spe-
cific it can represent many complex properties of the
full grammar?s posterior distribution for a given sen-
tence. For example, it might express a preference
for whether a PP token attaches to a particular verb
or to that verb?s object noun phrase in a particular
sentence.
Before continuing, note that a pointwise product
of anchored grammars is still an anchored gram-
mar. The complexity of parsing with a product of
these grammars is therefore no more expensive than
parsing with just one. Indeed, anchoring adds no
inferential cost at all over parsing with an unanno-
tated grammar: the anchored indices i, j, k have to
be computed just to parse the sentence at all. This
property is crucial to EP?s efficiency in our setting.
5.2 Assumed Density Filtering
We now describe a simplified version of EP: parsing
with assumed density filtering (Boyen and Koller,
1998). We would like to train a sequence ofM mod-
els, where each model is trained with knowledge
of the posterior distribution induced by the previous
models. Much as boosting algorithms (Freund and
Schapire, 1995) work by focusing learning on as-
yet-unexplained data points, this approach will en-
courage each model to improve on earlier models,
albeit in a different formal way.
At a high level, assumed density filtering (ADF)
proceeds as follows. First, we have an initially un-
informative q: it assigns the same probability to all
unpruned trees for a given sentence. Then, we fac-
tor in one of the annotated grammars and parse with
this new augmented grammar. This gives us a new
posterior distribution for this sentence over trees an-
notated with just that annotation component. Then,
we can marginalize out the annotations, giving us a
new q that approximates the annotated grammar as
closely as possible without using any annotations.
Once we have incorporated the current model?s com-
ponent, we move on to the next annotated grammar,
augmenting it with the new q, and repeating. In
this way, information from all grammars is incor-
porated into a final posterior distribution over trees
using only unannotated symbols. The algorithm is
then as follows:
? Initialize q(T ) uniformly.
? For each m in sequence:
1. Create the augmented distribution
qm(T[Xm]) ? q(T) ? fm(T[Xm]) and
compute inside and outside scores.
2. Minimize DKL
(
qm(T )||f?m(T )q(T )
)
by
fitting an anchored grammar f?m.
3. Set q(T ) =
?m
m?=1 f?m?(T ).
Step 1 of the inner loop forms an approximate pos-
terior distribution using fm, which is the parsing
model associated with component m, and q, which
is the anchored core approximation to the poste-
rior induced by the first m ? 1 models. Then, the
marginals are computed, and the new posterior dis-
tribution is projected to an anchored grammar, cre-
ating f?m. More intuitively, we create an anchored
PCFG that makes the approximation ?as close as
possible? to the augmented grammar. (We describe
this procedure more precisely in Section 5.4.) Thus,
each term fm is approximated in the context of the
terms that come before it. This contextual approx-
imation is essential: without it, ADF would ap-
proximate the terms independently, meaning that no
information would be shared between the models.
This method would be, in effect, a simple method
for parser combination, not all that dissimilar to the
method proposed by Petrov (2010). Finally, note
that the same inside and outside scores computed in
the loop can be used to compute the expected counts
needed in Equation 3.
Now we consider the runtime complexity of this
algorithm. If the maximum number of annotations
per rule for any factor is A, ADF has complex-
ity O
(
MAG0|w|3
)
when using M factors. In
contrast, parsing with the fully annotated grammar
would have complexityO
(
AMG0|w|3
)
. Critically,
for a latent variable parser with M annotation bits,
the exact algorithm takes time exponential in M ,
while this approximate algorithm takes time linear
in M .
It is worth pausing to consider what this algo-
rithm does during training. At each step, we have
1151
in q an approximation to what the posterior distribu-
tion looks like with the first m? 1 models. In some
places, q will assign high probabilities to spans in the
gold tree, and in some places it will not be so accu-
rate. ?m will be particularly motivated to correct the
latter, because they are less like the gold tree. On the
other hand, ?m will ignore the other ?correct? seg-
ments, because q has already sufficiently captured
them.
5.3 Expectation Propagation
While this sequential algorithm gives us a way to ef-
ficiently combine many kinds of annotations, it is
not a fully joint algorithm: there is no backward
propagation of information from later models to ear-
lier models. Ideally, no model should be privileged
over any other. To correct that, we use EP, which is
essentially the iterative generalization of ADF.
Intuitively, EP cycles among the models, updat-
ing the approximation for that model in turn so that
it closely resembles the predictions made by fm in
the context of all other approximations, as in Fig-
ure 2(c). Thus, each approximate term f?m is cre-
ated using information from all other f?m? , meaning
that the different annotation factors can still ?talk?
to each other. The product of these approximations
q will therefore come to act as an approximation to
the true posterior: it takes into account joint infor-
mation about all annotation components, all within
one tractable anchored grammar.
With that intuition in mind, EP is defined as fol-
lows:
? Initialize contributions f?m to the approximate
posterior q.
? At each step, choose m.
1. Include approximations to all factors other
than m: q\m(T ) =
?
m? 6=m f?m?(T ).
2. Create the augmented distribution by in-
cluding the actual factor for component m
qm(T [Xm]) ? fm(T [Xm])q\m(T )
and compute inside and outside scores.
3. Create a new f?m(T ) that minimizes
DKL
(
qm(T )||f?m(T )q\m(T )
)
.
? Finally, set q(T ) ??m f?m(T ).
Step 2 creates the augmented distribution qm, which
includes fm along with the approximate factors for
all models except the current model. Step 3 creates
a new anchored f?m that has the same marginal dis-
tribution as the true model fm in the context of the
other approximations, just as we did in ADF.
In practice, it is usually better to not recompute
the product of all f?m each time, but instead to main-
tain the full product q(T ) ? ?m f?m and to remove
the appropriate f?m by division. This optimization is
analogous to belief propagation, where messages are
removed from beliefs by division, instead of recom-
puting beliefs on the fly by multiplying all messages.
Schematically, the whole process is illustrated in
Figure 2(c). At each step, one piece of the core
approximation is replaced with the corresponding
component from the full model. This augmented
model is then reapproximated by a new core approx-
imation q after updating the corresponding f?m. This
process repeats until convergence.
5.4 EPIC Parsing
In our parser, EP is implemented as follows. q
and each of the f?m are anchored grammars that as-
sign weights to unannotated rules. The product of
anchored grammars with the annotated factor fm
need not be carried out explicitly. Instead, note
that an anchored grammar is just a function q(A ?
B C, i, k, j) ? R+ that returns a score for every an-
chored binary rule. This function can be easily in-
tegrated into the CKY algorithm for a single anno-
tated grammar by simply multiplying in the value
of q whenever computing the score of the respective
production over some span. The modified inside re-
currence takes the form:
INSIDE(A[x], i, j)
=
?
B,y,C,z
?T?(A[x]? B[y] C[z],w)
?
?
i<k<j
INSIDE(B[y], i, k) ? INSIDE(C[z], k, j)
? q(A? B C, i, k, j)
(4)
Thus, parsing with a pointwise product of an an-
chored grammar and an annotated grammar has no
increased combinatorial cost over parsing with just
the annotated grammar.
1152
To actually perform the projection in step 3 of EP,
we create an anchored grammar from inside and out-
side probabilities. First, we compute the expected
number of times the rule iAj ? iBk kCj occurs,
and then then we locally normalize for each sym-
bol iAj . This actually creates the new q distribution,
and so we have to divide out q\m This process mini-
mizes KL divergence subject to the local normaliza-
tion constraints.
All in all, this gives an algorithm that takes time
O
(
IMAG0|w|3
)
, where I is the maximum num-
ber of iterations, M is the number of models, and
A is the maximum number of annotations for any
given rule.
5.5 Other Inference Algorithms
To our knowledge, expectation propagation has been
used only once in the NLP community; Daume? III
and Marcu (2006) employed an unstructured ver-
sion in a Bayesian model of extractive summariza-
tion. Therefore, it is worth describing how EP dif-
fers from more familiar techniques.
EP can be thought of as a more flexible gen-
eralization of belief propagation, which has been
used several times in NLP (Smith and Eisner, 2008;
Niehues and Vogel, 2008; Cromie`res and Kurohashi,
2009; Burkett and Klein, 2012). In particular, EP al-
lows for the arbitrary choice of messages (the f?m),
meaning that we can use structured messages like
anchored PCFGs.
Mean field (Saul and Jordan, 1996) is another ap-
proximate inference technique that allows for struc-
tured approximations (Xing et al 2003; Burkett et
al., 2010), but here the natural version of mean field
for our model would still be intractable. However,
it is possible to adapt mean field into allowing for
tractable updates that are similar to the ones we pro-
posed. We do not pursue that approach here.
Dual decomposition (Dantzig and Wolfe, 1960;
Komodakis et al 2007) has recently become pop-
ular in the community (Rush et al 2010; Koo et
al., 2010). In fact, EP can be seen as a particular
kind of dual decomposition of the log normalization
constant logZ(w, ?) that is optimized with message
passing rather than (sub-)gradient descent or LP re-
laxations. Indeed, Minka (2001) argues that the EP
objective is more efficiently optimized with message
passing than with gradient updates. This assertion
should be examined for the structured models com-
mon in NLP, but that is beyond the scope of this pa-
per.
Finally, note that EP, like belief propagation but
unlike mean field, is not guaranteed to converge,
though in practice it usually seems to. In our exper-
iments, typically three or four iterations are enough
for almost all sentences to reach convergence, and
we found no loss in cutting off the number of itera-
tions to four.
6 Experiments
In what follows, we describe three experiments.
First, in a small experiment, we examine how effec-
tive the different inference algorithms are for both
training and testing. Second, we scale up our latent
variable model into successively larger products. Fi-
nally, we present a selection of the many possible
model combinations, showing that combining latent
and expert annotation can be quite effective.
6.1 Experimental Setup
For our experiments, we trained and tested on the
Penn Treebank using the standard splits: sections 2-
21 were training, 22 development, and 23 testing.
In preliminary experiments, we report development
set F1 on sentences up to length 40. For our final
test set experiment, we report F1 on sentences from
section 23 up to length 40, as well as all sentences
from that section. Scores reported are computed us-
ing EVALB (Sekine and Collins, 1997). We binarize
trees using Collins? head rules (Collins, 1997).
Each discriminative parser was trained using the
Adaptive Gradient variant of Stochastic Gradient
Descent (Duchi et al 2010). Smaller models were
seeded from larger models. That is, before training
a grammar of 5 models with 1 latent bit each, we
started with weights from a parser with 4 factored
bits. Initial experiments suggested this step did not
affect final performance, but greatly decreased to-
tal training time, especially for the latent variable
parsers. For extracting a one-best tree, we use a
version of the Max-Recall algorithm of Goodman
(1996). When using EP or ADF, we initialized
the core approximation q to the uniform distribution
over unpruned trees.
1153
Parsing
Training ADF EP Exact Petrov
ADF 84.3 84.5 84.5 82.5
EP 84.1 84.6 84.5 78.7
Exact 83.8 84.5 84.9 81.5
Indep. 82.3 82.1 82.2 82.6
Table 1: The effect of algorithm choice for training and
parsing on a product of two 2-state parsers on F1. Petrov
is the product parser of Petrov (2010), and Indep. refers
to independently trained models. For comparison, a four-
state parser achieves a score of 83.2.
When counting parameters, we consider the num-
ber of parameters per binary rule. Hence, a single
four-state latent model would have 64 (= 43) param-
eters per rule, while a product of 5 two-state models
would have just 40 (= 5 ? 23).
6.2 Comparison of Inference Algorithms
In our first experiment, we test the relative perfor-
mance of the various approximate inference meth-
ods at both train and test time. In order to include
exact inference, we necessarily need to look at a
smaller scale example for which exact inference is
still feasible. We examined development perfor-
mance for training and inference on a small product
of two parsers, each with two latent states per sym-
bol.
During training, we have several options. We can
use exact training by parsing with the fully articu-
lated product of both grammars, or, we can instead
use EP, ADF, or independent training. At test time,
we can parse using the full product of both gram-
mars, or, we can instead use EP, ADF, or we can use
the method of Petrov (2010) wherein we multiply
the parsers together in an ad hoc fashion.
The results are in Table 1. The best reported score,
unsurprisingly, is for using exact training and pars-
ing, but using EP for training and parsing results in
a relatively small loss of 0.3 F1. ADF, however, suf-
fers a loss of 0.6 F1 over Exact when used for train-
ing and parsing. Otherwise, Exact and EP seem to
perform fairly similarly at parse time for all training
conditions.
In general, there seems to be a gain for using the
same method for training and testing. Each test-
ing method performs at its best when using models
trained with the same method. Moreover, except for
ADF, the converse holds true: the grammars trained
80 
82 
84 
86 
88 
90 
1 2 3 4 5 6 7 8 
F1
 
Number of Models 
Figure 3: Development F1 plotted against the number M
of one-bit latent annotation components. The best gram-
mar has 6 one-bit annotations, with 89.7 F1.
with a given parsing method are best decoded using
the same method.
Oddly, using Petrov (2010)?s method does not
seem to work well at all for jointly trained models,
except for ADF. Similarly, joint parsing underper-
forms Petrov (2010)?s method when using indepen-
dently trained models. Likely, the joint parsing al-
gorithms are miscalibrating the redundant informa-
tion present in the two independently-trained mod-
els, while the two jointly-trained components come
to depend on each other. In fact, the F1 scores for
the two separate models of the EP parser are in the
60?s.
As expected, ADF does not perform as well as
EP. Therefore, we exclude it from our subsequent
experiments, focusing exclusively on EP.
6.3 Latent Variable Experiments
Most of the previous work in latent variable parsing
has focused on splitting smaller unstructured anno-
tations into larger unstructured annotations. Here,
we consider training a joint model consisting of a
large number of disjoint one-bit (i.e. two-state) la-
tent variable annotations. Specifically, we consider
the performance of products of up to 8 one-bit anno-
tations.
In Figure 3, we show development F1 as a func-
tion of the number of latent bits. Improvement is
roughly linear up to 3 components. Performance
levels off afterwards, with the top performing sys-
tem scoring 89.7 F1. Nevertheless, these parsers
outperform the comparable parsers of Petrov and
Klein (2008a) (89.3), even though our six-bit parser
has many fewer effective parameters per binary rule:
1154
Models F1, ? 40 F1, All
Lexicalized 87.3 86.5
Unlexicalized 86.3 85.4
3xLatent 88.6 87.6
Lex+Unlex 90.2 89.5
Lex+Lat 90.0 89.4
Unlex+Lat 90.0 89.4
Lex+Unlex+Lat 90.2 89.7
Table 2: Development F1 score for various model com-
binations for sentences less than length 40 and all sen-
tences. 3xLatent refers to a latent annotation model with
3 factored latent bits.
48 instead of the 4096 in their best parser. We also
ran our best system on Section 23, where it gets 89.1
and 88.4 on sentences less than length 40 and on all
sentences, respectively. This result compares favor-
ably to the 88.8/88.3 of Petrov and Klein (2008a).
6.4 Heterogeneous Models
We now consider factored models with different
kinds of annotations. Specifically, we tested gram-
mars comprising all subsets of {Lexicalized, Unlex-
icalized, Latent}. We used a model with 3 factored
bits as our representative of the latent variable class,
because it was closest in performance to the other
models. Of course, other smaller and larger combi-
nations are possible, but we found this selection to
be representative.
The development results are in Table 2. Unsur-
prisingly, adding more kinds of annotations helps for
the most part, though the combination of all three
components is not much better than a combination
of just the lexicalized and unlexicalized models. In-
deed, our best systems involved combining the lexi-
calized model with some other model. This is proba-
bly because the lexicalized model can represent very
different syntactic relationships than the latent and
unlexicalized models, meaning there is more diver-
sity in the joint model?s capacity when using combi-
nations involving the lexicalized annotations.
Finally, we ran our best system (the fully com-
bined one) on Section 23 of the Penn Treebank. It
scored 90.1/89.4 F1 on length 40 and all sentences
respectively, slightly edging out the 90.0/89.3 F1
of Petrov and Klein (2008a). However, it is not
quite as good at exact match: 37.7/35.3 vs 40.1/37.7.
Note, though, that their parser makes use of span
features, which deliver a gain of +0.3/0.2F1 respec-
tively, while ours does not. We suspect that similar
gains could be had by incorporating these features,
but we leave that for future work.
7 Conclusion
Factored representations capture a fundamental lin-
guistic insight: grammatical categories are not
monolithic, unanalyzable entities. Instead, they are
composed of numerous facets that together govern
how categories combine into parse trees.
We have developed a new model for grammars
with factored annotations and presented two meth-
ods for parsing with these grammars. Our ex-
periments have demonstrated that our approach
produces higher performance parsers with many
fewer parameters. Moreover, our model works
with both latent and explicit annotations, allowing
us to combine linguistic knowledge with machine
learning. Finally, our source code is available at
http://nlp.cs.berkeley.edu/Software.shtml.
Acknowledgments
We would like to thank Slav Petrov, David Burkett,
Adam Pauls, Greg Durrett and the anonymous re-
viewers for helpful comments. We would also like
to thank Daphne Koller for originally suggesting the
assumed density filtering approach. This work was
partially supported by BBN under DARPA contract
HR0011-12-C-0014, and by an NSF fellowship to
the first author.
References
Sylvie Billott and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association for
Computational Linguistics, pages 143?151, Vancou-
ver, British Columbia, Canada, June.
Xavier Boyen and Daphne Koller. 1998. Tractable in-
ference for complex stochastic processes. In Proceed-
ings of the 14th Conference on Uncertainty in Artificial
Intelligence?UAI 1998, pages 33?42. San Francisco:
Morgan Kaufmann.
David Burkett and Dan Klein. 2012. Fast inference in
phrase extraction models with belief propagation. In
NAACL.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In NAACL.
1155
Noam Chomsky. 1992. A minimalist program for lin-
guistic theory, volume 1. MIT Working Papers in Lin-
guistics, MIT, Cambridge Massachusetts.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16?23.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
G. B. Dantzig and P. Wolfe. 1960. Decomposition
principle for linear programs. Operations Research,
8:101?111.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL), Sydney, Australia.
Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In EMNLP, pages
317?326, July.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In ACL 2008, pages 959?967.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In ACL, pages 177?183.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL, pages 423?430.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In ICCV, pages 1?8.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
Thomas P. Minka. 2001. Expectation propagation for
approximate Bayesian inference. In UAI, pages 362?
369.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 18?25, June.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT, April.
Slav Petrov and Dan Klein. 2008a. Discriminative log-
linear grammars with latent variables. In NIPS, pages
1153?1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing. In
EMNLP, pages 867?876, Honolulu, Hawaii, October.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July.
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Los Angeles, California, June.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In EMNLP, pages 1?11, Cambridge, MA,
October.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS 1995.
Satoshi Sekine and Michael J. Collins. 1997. Evalb ?
bracket scoring program.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156, Honolulu, October.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Eric P. Xing, Michael I. Jordan, and Stuart J. Russell.
2003. A generalized mean field algorithm for varia-
tional inference in exponential families. In UAI, pages
583?591.
1156
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 265?277,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Error-Driven Analysis of Challenges in Coreference Resolution
Jonathan K. Kummerfeld and Dan Klein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720, USA
{jkk,klein}@cs.berkeley.edu
Abstract
Coreference resolution metrics quantify errors
but do not analyze them. Here, we consider
an automated method of categorizing errors in
the output of a coreference system into intu-
itive underlying error types. Using this tool,
we first compare the error distributions across
a large set of systems, then analyze common
errors across the top ten systems, empirically
characterizing the major unsolved challenges
of the coreference resolution task.
1 Introduction
Metrics produce measurements that concisely sum-
marize performance on the full range of error types,
and for coreference resolution there has been ex-
tensive work on developing effective metrics (Luo,
2005; Recasens and Hovy, 2011). However, it is also
valuable to tease apart the errors to understand their
relative importance.
Previous investigations of coreference errors have
focused on quantifying the importance of subtasks
such as named entity recognition and anaphoricity
detection, typically by measuring accuracy improve-
ments when partial gold annotations are provided
(Stoyanov et al, 2009; Pradhan et al, 2011; Prad-
han et al, 2012). For coreference resolution the
drawback of this approach is that decisions are often
interdependent, and so even partial gold information
is extremely informative. Also, previous work only
considered errors by counting links, which does not
capture certain errors in a natural way, e.g. when
a system incorrectly divides a large entity into two
parts, each with multiple mentions. Recent work has
considered some of these issues, but only with small
scale manual analysis (Holen, 2013).
We present a new tool that automatically classifies
errors in the standard output of any coreference res-
olution system. Our approach is to identify changes
that convert the system output into the gold annota-
tions, and map the steps in the conversion onto lin-
guistically intuitive error types. Since our tool uses
only system output, we are able to classify errors
made by systems of any architecture, including both
systems that use link-based inference and systems
that use global inference methods.
Using our tool we perform two studies to un-
derstand similarities and differences between sys-
tems. First, we compare the error distributions on
coreference resolution of all of the systems from the
CoNLL 2011 shared task plus several publicly avail-
able systems. This comparison adds to the analy-
sis from the shared task by illustrating the substan-
tial variation in the types of errors different systems
make. Second, we investigate the aggregate behav-
ior of ten state-of-the-art systems, providing a de-
tailed characterization of each error type. This in-
vestigation identifies key outstanding challenges and
presents the impact that solving each of them would
have in terms of changes in the standard coreference
resolution metrics.
We find that the best systems are not best across
all error types, that a large proportion of span errors
are due to superficial parse differences, and that the
biggest performance loss is on missed entities that
contain a small number of mentions.
This work presents a comprehensive investiga-
tion of common errors in coreference resolution,
identifying particular issues worth focusing on in
future research. Our analysis tool is available at
code.google.com/p/berkeley-coreference-analyser/.
265
2 Background
Most coreference work focuses on accuracy im-
provements, as measured by metrics such as MUC
(Vilain et al, 1995), B3 (Bagga and Baldwin, 1998),
CEAF (Luo, 2005), and BLANC (Recasens and
Hovy, 2011). The only common forms of further
analysis are results for anaphoricity detection and
scores for each mention type (nominal, pronoun,
proper). Two exceptions are: the detailed analysis of
the Reconcile system by Stoyanov et al (2009), and
the multi-system comparisons in the CoNLL shared
task reports (Pradhan et al, 2011, 2012).
A common approach to performance analysis is to
calculate scores for nominals, pronouns and proper
names separately, but this is a very coarse division
(Ng and Cardie, 2002; Haghighi and Klein, 2009).
More fine consideration of some subtasks does oc-
cur, for example, anaphoricity detection, which has
been recognized as a key challenge in coreference
resolution for decades and regularly has separate re-
sults reported (Paice and Husk, 1987; Sobha et al,
2011; Yuan et al, 2012; Bjo?rkelund and Farkas,
2012; Zhekova et al, 2012). Some work has also
included anecdotal discussion of specific error types
or manual classification of a small set of errors, but
these approaches do not effectively quantify the rel-
ative impact of different errors (Chen and Ng, 2012;
Martschat et al, 2012; Haghighi and Klein, 2009).
In a recent paper, Holen (2013) presented a detailed
manual analysis that considered a more comprehen-
sive set of error types, but their focus was on explor-
ing the shortcomings of current metrics, rather than
understanding the behavior of current systems.
The detailed investigation presented by Stoyanov
et al (2009) is the closest to the work we present
here. First, they measured accuracy improvements
when their system was given gold annotations for
three subtasks of coreference resolution: mention
detection, named entity recognition, and anaphoric-
ity detection. To isolate other types of errors they de-
fined resolution classes, based on both the type of a
mention, and properties of possible antecedents (for
example, nominals that have a possible antecedent
that is an exact string match). For each resolution
class they measured performance while giving the
system gold annotations for all other classes. While
this approach is effective at characterizing variations
President Clinton1 is questioning the legitimacy
of George W. Bush?s election victory. Speaking
last night to Democratic supporters in Chicago,
he said Bush won the election only because Re-
publicans stopped the vote-counting in Florida,
and Mr. Clinton1 praised Al Gore?s campaign
manager, Bill Daley, for the way he handled the
election. ?I2 want to thank Bill Daley for his ex-
emplary service as Secretary of Commerce. He
was brilliant. I2 think he did a brilliant job in
leading Vice President Gore to victory myself2.?
Figure 1: Two coreference errors. Mentions are under-
lined and subscripts indicate entities. One error is a men-
tion missing from the system output, he. The other is the
division of references to Bill Clinton into two entities.
between the nine classes they defined, it misses the
cascade effect of errors that only occur when all
mentions are being resolved at once.
The only multi-system comparisons are the
CoNLL task reports (Pradhan et al, 2011, 2012),
which explored the impact of mention detection and
anaphoricity detection through subtasks with differ-
ent types of gold annotation. With a large set of sys-
tems, and well controlled experimental conditions,
the tasks provided a great snapshot of progress in the
field, which we aim to supplement by characterizing
the major outstanding sources of error.
This work adds to previous investigations by pro-
viding a comprehensive and detailed analysis of er-
rors. Our tool can automatically analyze any sys-
tem?s output, giving a reliable estimate of the rela-
tive importance of different error types.
3 Error Classification
When inspecting the output of coreference resolu-
tion systems, several types of errors become imme-
diately apparent: entities that have been divided into
pieces, spurious entities, non-referential pronouns
that have been assigned antecedents, and so on. Our
goal in this work is to automatically assign intuitive
labels like these to errors in system output.
A simple approach, refining results by measur-
ing the accuracy of subsets of the mentions, can be
misleading. For example, in Figure 1, we can in-
tuitively see two pronoun related mistakes: a miss-
ing mention (he), and a divided entity where the two
pieces are the blue pronouns (I2, I2, myself2) and the
red proper names (President Clinton1, Mr. Clinton1).
266
Simply counting the number of incorrect pronoun
links would miss the distinction between the two
types of mistakes present.
One question in designing an error analysis tool
like ours is whether to operate on just system output,
or to also consider intermediate system decisions.
We focused on using system output because other
methods cannot uniformly apply to the full range of
coreference resolution decoding methods, from link
based methods to global inference methods.
Our overall approach is to transform the sys-
tem output into the gold annotations, then map
the changes made in the conversion process to er-
rors. The transformation process is presented in Sec-
tion 3.1 and Figure 2, and the mapping process is
described in Section 3.2 and Figure 3.
3.1 Transformations
The first part of our error classification process de-
termines the changes needed to transform the system
output into the gold annotations. This five stage pro-
cess is described below, and an abstract example is
presented in Figure 2.
1. Alter Span transforms an incorrect system
mention into a gold mention that has the same
head token. In Figure 2 this stage is demon-
strated by a mention in the leftmost entity,
which has its span altered, indicated by the
change from an X to a light blue circle.
2. Split breaks the system entities into pieces,
each containing mentions from a single gold
entity. In Figure 2 there are three changes in
this stage: the leftmost entity is split into a red
piece and a light blue piece, the middle entity
is split into a dark red piece and an X, and the
rightmost entity is split into singletons.
3. Remove deletes every mention that is not
present in the gold annotations. In Figure 2 this
means the four singleton X?s are removed.
4. Introduce creates a singleton entity for each
mention that is missing from the system output.
In Figure 2 this stage involves the introduction
of a light blue mention and two white mentions.
5. Merge combines entities to form the final,
completely correct, set of entities. In Figure 2
the two red entities are merged, the singleton
Mentions
Spurious mention
Entity
1. Alter Span
2. Split
3. Remove
4. Introduce
5. Merge
X
X
X
XX
X X
X
X
X X
X
X
Gold entities indicated using common shading
X
Key
System
Output
Gold
Entities
Figure 2: Abstract example of the transformation process
that converts system output (at the top) to gold annota-
tions (at the bottom).
blue entity is merged with the rest of the blue
entity, and the two white mentions are merged.
267
Operation(s) Error System Gold
i) Alter Span Span error Gorbachev Soviet leader Gorbachev
ii)
Multiple Introduces
Missing Entity
- the pills
and Merges - the tranquilizing pills
iii)
Multiple Splits
Extra Entity
human rights -
and Removes Human Rights -
Introduce
and Merge
the Arab region the Arab region
iv) Missing Mention the region the region
- it
Split and
Remove
her story her story
v) Extra Mention this this
it -
vi) Merge Divided Entity
Iraq1 Iraq1
this nation2 this nation1
the nation2 the nation1
its1 its1
vii) Split Conflated Entities
Mohammed Rashid1 Mohammed Rashid1
the Rashid case1 the Rashid case2
Rashid1 Rashid1
the case1 the case2
Figure 3: Examples of the error types. In examples (i) - (iv) and (vi) the system output contains a single entity. When
multiple entities are involved, they are marked with subscripts. Mentions are in the order in which they appear in the
text. All examples are from system output on the dev set of the CoNLL task.
One subtle point in the split stage is how to record
an entity being split into several pieces. This could
either be a single operation, one entity being split
into N pieces, or N ?1 operations, each involving a
single piece being split off from the rest of the entity.
We use the second approach, as it fits more naturally
with the error mapping we describe in the follow-
ing section. Similarly, for the merge operation, we
record N entities being merged as N?1 operations.
3.2 Mapping
The operations in Section 3.1 are mapped onto seven
error types. In some cases, a single change maps
onto a single error, while in others a single error rep-
resents several closely related operations from adja-
cent stages in the error correction process. The map-
ping is described below and in Figure 3.
1. Span Error. Each Alter Span operation is
mapped to a Span Error, e.g. in Figure 3(i), the
system mention Gorbachev is replaced by the
annotated mention Soviet leader Gorbachev.
2. Missing Entity. A set of Introduce and Merge
operations that forms an entirely new entity,
e.g. the white entity in Figure 2, and the pills
in Figure 3(ii). This error is still assigned if
the new entity includes pronouns that were al-
ready present in the system output. The rea-
soning for this is that most pronouns in the cor-
pus are coreferent, so including just the pro-
nouns from an entity is not meaningfully dif-
ferent from missing the entity entirely.
3. Extra Entity. A set of Split and Remove oper-
ations that completely remove an entity, e.g. the
rightmost entity in Figure 2, and Figure 3(iii).
As for the Missing Entity error type, this error
is still assigned if the original entity contained
pronouns that were valid.
4. Missing Mention. An Introduce and a Merge
that apply to the same mention, e.g. it in Fig-
ure 3(iv), and the blue mention in Figure 2.
5. Extra Mention. A Split and a Remove that ap-
ply to the same mention, e.g. it in Figure 3(v),
and the X in the red entity in Figure 2.
6. Divided Entity. Each remaining Merge oper-
ation is mapped to a Divided Entity error, e.g.
Figure 3(vi), and the red entity in Figure 2.
7. Conflated Entities. Each remaining Split op-
eration is mapped to a Conflated Entity error,
e.g. Figure 3(vii), and the blue and red entities
in Figure 2.
268
4 Methodology
Our tool processes the CoNLL task output, with no
other information required. During development,
and when choosing examples for this paper, we
used the development set of the CoNLL shared task
(Hovy et al, 2006; Pradhan et al, 2007; Pradhan et
al., 2011). The results we present in the rest of the
paper are all for the test set. Using the development
set would have been misleading, as the entrants in
the shared task used it to tune their systems.
4.1 Systems
We analyzed all of the 2011 CoNLL task systems, as
well as several publicly available systems. For the
shared task systems we used the output data from
the task itself, provided by the organizers. For the
publicly available systems we used the default con-
figurations. Finally, we included another run of the
Stanford system, with their OntoNotes-tuned param-
eters (STANFORD-T).
The publicly available systems we used are:
BERKELEY (Durrett and Klein, 2013), IMS
(Bjo?rkelund and Farkas, 2012), STANFORD (Lee
et al, 2013), RECONCILE (Stoyanov et al, 2010),
BART (Versley et al, 2008), UIUC (Bengtson and
Roth, 2008), and CHERRYPICKER (Rahman and
Ng, 2009). The systems from the shared task are
listed in Table 1 and in the references.
5 Broad System Comparison
Table 1 presents the frequency of errors for each sys-
tem and F-Scores for standard metrics1 on the test
set of the 2011 CoNLL shared task. Each bar is
filled in proportion to the number of errors the sys-
tem made, with a full bar corresponding to the num-
ber of errors listed in the bottom row.
The metrics provide an effective overall rank-
ing, as the systems with high scores generally make
fewer errors. However, the metrics do not convey
the significant variation in the types of errors sys-
tems make. For example, YANG and CHARTON are
assigned almost the same scores, but YANG makes
more than twice as many Extra Mention errors.
1CEAF and BLANC are not included as the most recent ver-
sion of the CoNLL scorer (v5) is incorrect, and there are no
standard implementations available.
The most frequent error across all systems is Di-
vided Entity. Unlike parsing errors (Kummerfeld et
al., 2012), improvements are not monotonic, with
better systems often making more errors of one type
when decreasing the frequency of another type.
One outlier is the Irwin et al (2011) system,
which makes very few mistakes in five categories,
but many in the last two. This reflects a high pre-
cision, low recall approach, where clusters are only
formed when there is high confidence.
The third section of Table 1 shows results for sys-
tems that were run with gold noun phrase span in-
formation. This reduces all errors slightly, though
most noticeably Extra Mention, Missing Mention,
and Span Error. On inspection of the remaining
Span Errors we found that many are due to incon-
sistencies regarding the inclusion of the possessive.
The final section of the table shows results for sys-
tems that were provided with the set of mentions that
are coreferent. In this setting, three of the error types
are not present, but there are still Missing Mentions
and Missing Entities because systems do not always
choose an antecedent, leaving a mention as a single-
ton, which is then ignored.
While this broad comparison gives a complete
view of the range of errors present, it is still a coarse
representation. In the next section, we characterize
the common errors on a finer level by breaking down
each error type by a range of properties.
6 Common Errors
To investigate the aggregate state of the art, in this
section we consider results averaged over the top
ten systems: CAI, CHANG, IMS, NUGUES, SAN-
TOS, SAPENA, SONG, STANFORD-T, STOYANOV,
URYUPINA-OPEN.2 These systems represent a broad
range of approaches, all of which are effective.
In each section below, we focus on one or two
error types, characterizing the mistakes by a range
of properties. We then consider a few questions that
apply across multiple error types.
6.1 Span Errors
To characterize the Span Errors, we considered the
text that is in the gold mention, but not the system
2For systems that occur multiple times in Table 1, we only
use the best instance. The BERKELEY system was not included
as it had not been published at submission time.
269
Metric F-Scores Span Conflated Extra Extra Divided Missing Missing
System Mention MUC B3 Error Entities Mention Entity Entity Mention Entity
PUBLICLY AVAILABLE SYSTEMS
BERKELEY 75.57 66.43 66.17
IMS 72.96 64.71 64.73
STANFORD-T 71.21 61.40 63.06
STANFORD 58.56 48.37 56.42
RECONCILE 46.45 49.40 54.90
BART 56.61 46.00 52.56
UIUC 50.60 45.21 52.88
CHERRYPICKER 41.10 40.71 51.39
CONLL, PREDICTED MENTIONS
LEE-OPEN 70.94 61.03 62.96
LEE 70.70 59.56 61.88
SAPENA 43.20 59.54 61.28
SONG 67.26 59.95 60.08
CHANG 64.86 57.13 61.75
CAI-OPEN 67.45 57.86 60.89
NUGUES 68.96 58.61 59.75
URYUPINA-OPEN 68.39 57.63 58.74
SANTOS 65.45 56.65 59.48
STOYANOV 67.78 58.43 57.35
HAO 64.30 54.46 55.82
YANG 63.93 52.31 55.85
CHARTON 64.36 52.49 55.61
KLENNER-OPEN 62.28 49.86 55.62
SOBHA 64.83 50.48 54.85
ZHOU 62.31 48.96 53.42
KOBDANI 61.03 48.62 53.00
ZHANG 61.13 47.88 52.76
XINXIN 61.92 46.62 51.50
KUMMERFELD 62.72 42.70 50.05
IRWIN-OPEN 35.27 27.21 44.29
ZHEKOVA 48.29 24.08 41.42
IRWIN 26.67 19.98 42.73
CONLL, GOLD NP SPANS
LEE-OPEN 75.39 65.39 65.88
LEE 75.16 63.90 64.70
NUGUES 72.42 62.12 61.67
CHANG 67.91 59.77 62.97
SANTOS 67.80 59.52 61.35
STOYANOV 70.29 61.53 59.07
SONG 66.68 55.48 58.04
KOBDANI 66.08 53.94 55.82
ZHANG 64.89 51.64 54.77
ZHEKOVA 62.67 35.22 45.80
CONLL, GOLD MENTIONS
LEE-OPEN 90.93 81.56 75.95
CHANG 99.97 82.52 73.68
Most Errors 2410 3849 2744 5290 4789 2026 3237
Table 1: Counts for each error type on the test set of the 2011 CoNLL task. Bars indicate the number of errors, with
white as zero and fully filled as the number in the Most Errors row. -OPEN indicates a system using external resources.
270
Type Missing Extra
NP 65.8 45.0
POS 12.4 96.9
, 71.2 22.4
SBAR 55.9 1.9
PP 46.2 10.3
DT 17.0 35.9
Total 271.1 224.6
Table 2: Counts of Span Errors grouped by the label over
the extra/missing part of the mention.
mention (missing text), and vice versa (extra text).
We then found nodes in the gold parse that cov-
ered just this extra/missing text, e.g. in Figure 3(i)
we would consider the node over Soviet leader. In
Table 2 we show the most frequent parse nodes.
Some of these differences are superficial, such as
the possessive and the punctuation. Others, such as
the missing PP and SBAR cases, may be due to parse
errors. Of the system mentions involved in span er-
rors, 27.0% do not correspond to a node in the gold
parse. The frequency of punctuation errors could
also be parse related, because punctuation is not con-
sidered in the standard parser evaluation.
Overall it seems that span errors can best be dealt
with by improving parsing, though it is not possi-
ble to completely eliminate these errors because of
inconsistent annotations.
6.2 Extra Mention and Missing Mention
We consider Extra and Missing Mentions together
as they mirror each other, forming a precision-recall
tradeoff, where a high precision system will have
fewer Extra Mentions and more Missing Mentions,
and a high recall system will have the opposite.
Table 3 divides these errors by the type of men-
tion involved and presents some of the most fre-
quent Extra Mentions and Missing Mentions. For
the corpus statistics we count as mentions all NP
spans in the gold parse plus any word tagged with
PRP, WP, WDT, or WRB (following the definition
of gold mention boundaries for the CoNLL tasks).
The mentions it and you are the most common
errors, matching observations from several of the
papers cited in Section 2. However, there is a sur-
prising imbalance between Extra and Missing cases,
e.g. it accounts for a third of the extra errors, but
only 12% of the Missing errors. This imbalance may
Av. Errors Corpus Stats
Mention Extra Missing Count % Coref.
Proper Name 281.6 297.7 6915 59.0
Nominal 484.2 516.5 33328 15.9
Pronoun 390.7 323.3 9926 69.7
it 130.4 38.9 1211 57.1
you 85.2 55.9 1028 44.9
we 39.6 19.6 691 64.7
us 23.2 3.2 242 23.6
that 13.8 13.4 2010 11.5
they 9.6 39.5 738 94.3
their 8.6 21.5 410 95.1
Total 1156.5 1137.5 50169 32.5
Table 3: Counts of Missing and Extra Mention errors by
mention type, and the most common mentions.
Proper Name Nominal
Extra Missing Extra Missing
Text match 145.2 163.6 171.2 96.1
Head match 56.8 70.7 149.6 166.0
Other 79.6 63.4 163.4 254.4
NER Matches 143.4 174.4 23.0 32.0
NER Differs 6.6 6.1 2.4 0.0
NER Unknown 131.6 117.2 458.8 484.5
Total 281.6 297.7 484.2 516.5
Table 4: Counts of Extra and Missing Mentions, grouped
by properties of the mention and the entity it is in.
be the result of systems being tuned to the metrics,
which seem to penalize Missing Mentions more than
Extra Mentions (shown in Section 6.7).
In Table 4 we consider the Extra Mention er-
rors and Missing Mention errors involving proper
names and nominals. The top section counts errors
in which the mention involved in the error has an
exact string match with a mention in the cluster, or
whether it has just a head match. The second sec-
tion of the table considers the named entity anno-
tations in OntoNotes, counting how often the men-
tion?s type matches the type of the cluster.
In all cases shown in the table it appears that sys-
tems are striking a balance between these two types
of errors. One exception may be the use of exact
string matching for nominals, which seems to be bi-
ased towards Extra Mentions.
For these two error types, our observations agree
with previous work: the most common specific error
is the identification of pleonastic pronouns, named
entity types are of limited use, and head matching is
already being used about as effectively as it can be.
271
Composition Av. Errors
Name Nom Pro Extra Missing
0 1 1 70.7 271.6
1 0 1 13.2 28.1
1 1 0 26.6 86.2
2 0 0 61.3 89.3
0 2 0 512.0 347.9
0 0 2 110.9 13.6
3+ 0 0 14.7 14.4
0 3+ 0 154.8 65.9
0 0 3+ 91.0 18.1
Other 51.8 216.4
Total 1107.0 1151.5
Table 5: Counts of Extra and Missing Entity errors,
grouped by the composition of the entity (Names, Nomi-
nals, Pronouns).
Match Type Extra Missing
Proper Name 51.4 42.2
Exact Nominal 338.3 49.5
Pronoun 141.9 10.3
Head
Proper Name 14.4 27.3
Nominal 234.7 129.0
Proper Name 10.2 34.2
None Nominal 92.8 235.3
Pronoun 60.0 21.4
Table 6: Counts of Extra and Missing Entity errors
grouped by properties of the mentions in the entity.
6.3 Extra Entities and Missing Entities
In this section, we consider the errors that involve an
entire entity that was either missing from the system
output or does not exist in the annotations.
Table 5 counts these errors based on the compo-
sition of the entity. There are several noticeable dif-
ferences between the two error types, e.g. for entities
containing one nominal and one pronoun (row 0 1 1)
there are far more Missing errors than Extra errors,
while entities containing two pronouns (row 0 0 2)
have the opposite trend.
It is clear that entities consisting of a single type
of mention are the primary source of these errors,
accounting for 85.3% of the Extra Entity errors,
and 47.7% of Missing Entity errors. Table 6 shows
counts for these cases divided into three groups:
when all mentions are identical, when all mentions
have the same head, and the rest.
Nominals are the most frequent type in Table 6,
and have the greatest variation across the three sec-
Mention Extra Missing
that 6.9 99.7
it 47.7 47.8
this 0.9 36.2
they 3.8 29.1
their 2.1 23.5
them 0.9 13.8
Any pronoun 83.9 299.7
Table 7: Counts of common Missing and Extra Entity
errors where the entity has just two mentions: a pronoun
and either a nominal or a proper name.
tions of the table. For the Extra column, Exact match
cases are a major challenge, accounting for over half
of the nominal errors. These errors include cases
like the example below, where two mentions are not
considered coreferent because they are generic:
everybody tends to mistake the part for the whole.
Here, mistaking the part for the whole is ...
For missing entities we see the opposite trend,
with Exact match cases accounting for less than 12%
of nominal errors. Instead, cases with no match are
the greatest challenge, such as this example, which
requires semantic knowledge to correctly resolve:
The charges related to her sale of ImClone stock.
She sold the share a day before ...
The other common case in Table 5 is an entity
containing a pronoun and a nominal. In Table 7 we
present the most frequent pronouns for this case and
the similar case involving a pronoun and a name.
One way of interpreting these errors is from
the perspective of the pronoun, which is either
incorrectly coreferent (Extra), or incorrectly non-
coreferent (Missing). From this perspective, these
errors are similar in nature to those described by Ta-
ble 3. However, the distribution of errors is quite dif-
ferent, with it being balanced here where previously
it skewed heavily towards extra mentions, while that
was balanced in Table 3 but is skewed towards being
part of Missing Entities here.
Extra Entity errors and Missing Entity errors are
particularly challenging because they are dominated
by entities that are either just nominals, or a nominal
and a pronoun, and for these cases the string match-
ing features are often misleading. This implies that
reducing Extra Entity and Missing Entity errors will
require the use of discourse, context, and semantics.
272
Incorrect Part Rest of Entity Av. Errors
Na No Pr Na No Pr Conflated Divided
- - 1+ - - 1+ 312.7 69.9
- - 1+ - 1+ 1+ 238.5 179.8
- - 1+ - 1+ - 189.6 549.3
- 1+ - - 1+ - 181.5 156.5
- - 1+ 1+ 1+ 1+ 143.6 181.5
- - 1+ 1+ - 1+ 109.7 150.5
- - 1+ 1+ - - 60.0 136.5
Other 454.8 657.7
Total 1690.4 2081.7
Table 8: Counts of Conflated and Divided entities errors
grouped by the Name / Nominal / Pronoun composition
of the parts involved.
6.4 Conflated Entities and Divided Entities
Table 8 breaks down the Conflated Entities errors
and Divided Entity errors by the composition of the
part being split/merged and the rest of the entity in-
volved. Each 1+ indicates that at least one mention
of that type is present (Name / Nominal / Pronoun).
Clearly pronouns being placed incorrectly is the
biggest issue here, with almost all of the common
errors involving a part with just pronouns. It is also
clear that not having proper names in the rest of
the entity presents a challenge. One particularly no-
ticeable issue involves entities composed entirely of
pronouns, which are often created by systems con-
flating the pronouns of two entities together.
Table 8 aggregates errors by the presence of dif-
ferent types of mentions. Aggregating instead by the
exact composition of the incorrect part being con-
flated or divided we found that instances with a part
containing a single pronoun account for 38.9% of
conflated cases and 35.8% of divided cases.
Finally, it is worth noting that in many cases a part
is both conflated with the wrong entity, and divided
from its true entity. Only 12.6% of Conflated Entity
errors led to a complete gold entity with no other er-
rors, and only 21.3% of Divided Entity errors came
from parts that were not involved in another error.
Conflated Entities and Divided Entities are domi-
nated by pronoun link errors: cases where a pronoun
was placed in the wrong entity. Finding finer charac-
terizations of these errors is difficult, as almost any
division produces sparse counts, reflecting the long
tail of mistakes that make up these two error types.
Gold System Decision Count
Cataphoric
Same referent 10.6
Different referent 13.4
Not cataphoric 208.2
Not present 42.8
Not cataphoric Cataphoric 46.2
Not present Cataphoric 186.8
Table 9: Occurrence of mistakes involving cataphora.
6.5 Cataphora
Cataphora (when an anaphor precedes its an-
tecedent) is a pronoun-specific problem that does
not fit easily in the common left-to-right coreference
resolution approach. In the CoNLL test set, 2.8% of
the pronouns are cataphoric. In Table 9 we show
how well systems handle this challenge by counting
mentions based on whether they are cataphoric in
the annotations, are cataphoric in the system output,
and whether the antecedents match.
Systems handle cataphora poorly, missing almost
all of the true instances, and introducing a large
number of extra cases. However, this issue is a fairly
small part of the task, with limited metric impact.
6.6 Entity Properties
Gender, number, person, and named entity type are
properties commonly used in coreference resolution
systems. In some cases, two mentions with differ-
ent properties are placed in the same entity. Some
of these cases are correct, such as variation in per-
son between mentions inside and outside of quotes.
However, many of these cases are errors. In Table 11
we present the percentage of entities that contain
mentions with properties of more than one type. For
named entity types we considered the annotations in
OntoNotes; for the other properties we derive them
from the pronouns in each cluster.
For all of the properties, there are many entities
that we could not assign a value to, either because
no named entity information was available, or be-
cause no pronouns with an unambiguous value for
the property were present. For named entity infor-
mation, OntoNotes only has annotations for 68% of
gold entities, suggesting that named entity taggers
are of limited usefulness, matching observations on
the MUC and ACE corpora (Stoyanov et al, 2009).
The results in the ?Gold? column of Table 11 in-
273
Mentions MUC B3
Error type P R F P R F P R F
Span Error 2.8 2.8 2.7 2.8 2.8 2.8 1.0 2.0 1.6
Conflated Entities 1.7 0.0 0.8 9.9 0.0 4.5 15.9 0.0 6.2
Extra Mention 5.5 0.0 2.6 6.4 0.0 3.0 5.3 0.0 2.2
Extra Entity 15.3 0.0 7.0 11.4 0.0 5.2 6.1 0.0 2.4
Divided Entity 1.8 6.8 4.3 5.7 16.8 10.9 -10.0 21.6 4.5
Missing Mention 1.8 7.0 4.4 3.2 9.2 6.1 -1.3 7.3 3.4
Missing Entity 3.8 16.2 9.8 5.3 13.7 9.3 1.7 11.4 7.0
Table 10: Average accuracy improvement if all errors of a particular type are corrected. Each row in the lower section
is calculated independently, relative to the change after the span errors have been corrected. Some values are negative
because the merge operations involved in fixing the errors are applying to clusters that contain mentions from more
than one gold entity.
Property System Gold
Named Entity 1.7% 0.7%
Gender 0.8% 0.1%
Number 2.1% 0.8%
Person 6.4% 5.1%
Table 11: Percentage of entities that contain mentions
with properties that disagree.
dicate possible errors in the annotations, e.g. in the
0.7% of entities with a mixture of named entity types
there may be mistakes in the coreference annota-
tions, or mistakes in the named entity annotations.3
However, even after taking into consideration cases
where the mixture is valid and cases of annotation
errors, current systems are placing mentions with
different properties in the same clusters.
6.7 Impact of Errors on Metric Scores
Table 10 shows the performance impact of correct-
ing errors of each type. The Span Error row gives
improvements over the original scores, while all
other rows are relative to the scores after Span Er-
rors are corrected.4 By fixing each of the other error
types in isolation, we can get a sense of the gain if
just that error type is addressed. However, it also
means some mentions are incorrectly placed in the
same cluster, causing some negative scores.
Interaction between the error types and the way
the metrics are defined means that the deltas do not
3This kind of cross-annotation analysis may be a useful way
of detecting annotation errors.
4This difference was necessary as the later errors make
changes relative to the state of the entities after the Span Errors
are corrected, e.g. in Figure 2 a blue and red entity is split that
previously contained an X instead of one of the blue mentions.
add up to the overall average gap in performance, but
it is still clear that every error type has a noticeable
impact. Missing Entity errors have the most sub-
stantial impact, which reflects the precision oriented
nature of many coreference resolution systems.
7 Conclusion
While the improvement of metrics and the organiza-
tion of shared tasks have been crucial for progress
in coreference resolution, there is much insight to be
gained by performing a close analysis of errors.
We have presented a new means of automatically
classifying coreference errors that provides an ex-
haustive view of error types. Using our tool we have
analyzed the output of a large set of coreference res-
olution systems and investigated the common chal-
lenges across state-of-the-art systems.
We find that there is considerable variability in
the distribution of errors, and the best systems are
not best across all error types. No single source
of errors stands out as the most substantial chal-
lenge today. However, it is worth noting that
while confidence measures can be used to reduce
precision-related errors, no system has been able to
effectively address the recall-related errors, such as
Missed Entities. Our analysis tool is available at
code.google.com/p/berkeley-coreference-analyser/.
Acknowledgments
We would like to thank the CoNLL task organizers
for providing us with system outputs. This work was
supported by a General Sir John Monash fellowship
to the first author and by BBN under DARPA con-
tract HR0011-12-C-0014.
274
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of The First International Conference on Language
Resources and Evaluation Workshop on Linguistics
Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 294?303.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 49?55.
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50.
Jie Cai, Eva Mujdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 56?60.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Nick Rizzolo, Mark Sammons, and Dan Roth. 2011.
Inference protocols for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 40?44.
Eric Charton and Michel Gagnon. 2011. Poly-co: a mul-
tilayer perceptron approach for coreference detection.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 97?101.
Chen Chen and Vincent Ng. 2012. Combining the best of
two worlds: A hybrid approach to multilingual coref-
erence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 56?63.
Weipeng Chen, Muyu Zhang, and Bing Qin. 2011.
Coreference resolution system using maximum en-
tropy classifier. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 127?130.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods in
Natural Language Processing.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161.
Gordana Ilic Holen. 2013. Critical reflections on evalu-
ation practices in coreference resolution. In Proceed-
ings of the 2013 NAACL HLT Student Research Work-
shop, pages 1?7.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92.
Manfred Klenner and Don Tuggener. 2011. An incre-
mental model for coreference resolution with restric-
tive antecedent accessibility. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 81?85.
Hamidreza Kobdani and Hinrich Schuetze. 2011. Super-
vised coreference resolution with sucre. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 71?
75.
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett,
and Dan Klein. 2011. Mention detection: Heuristics
for the ontonotes annotations. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 102?106.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of er-
ror types in parser output. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1048?1059.
Sobha Lalitha Devi, Pattabhi Rao, Vijay Sundar Ram R,
M. C S, and A. A. 2011. Hybrid approach for corefer-
ence resolution. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 93?96.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanfords multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4).
Xinxin Li, Xuan Wang, and Shuhan Qi. 2011. Coref-
erence resolution with loose transitivity constraints.
275
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 107?111.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25?
32.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 100?106.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 104?111.
Cicero Nogueira dos Santos and Davi Lopes Carvalho.
2011. Rule and tree ensembles for unrestricted
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 51?55.
C. D. Paice and G. D. Husk. 1987. Towards the auto-
matic recognition of anaphoric features in english text:
the impersonal pronoun ?it?. Computer Speech & Lan-
guage, 2(2):109?132.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Un-
restricted coreference: Identifying entities and events
in ontonotes. In Proceedings of the International Con-
ference on Semantic Computing, pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the
15th Conference on Computational Natural Language
Learning (CoNLL 2011), pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-2012
shared task: Modeling multilingual unrestricted coref-
erence in ontonotes. In Joint Conference on EMNLP
and CoNLL - Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the rand index for coreference evaluation. Natural
Language Engineering, 17:485?510, 9.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39.
Lalitha Devi. Sobha, RK. Rao. Pattabhi, R. Vijay Sundar
Ram, CS. Malarkodi, and A. Akilandeswari. 2011.
Hybrid approach for coreference resolution. In Pro-
ceedings of the Fifteenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 93?96.
Yang Song, Houfeng Wang, and Jing Jiang. 2011. Link
type based pre-cluster pair model for coreference reso-
lution. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 131?135.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656?664.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proceedings of the
ACL 2010 Conference Short Papers, pages 156?161.
Veselin Stoyanov, Uday Babbar, Pracheer Gupta, and
Claire Cardie. 2011. Reconciling ontonotes: Unre-
stricted coreference resolution in ontonotes with rec-
oncile. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 122?126.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
a modular toolkit for coreference resolution. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technologies: Demo Session, pages 9?12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Uunderstanding Conference,
pages 45?52.
Hao Xiong, Linfeng Song, Fandong Meng, Yang Liu,
Qun Liu, and Yajuan Lv. 2011. Ets: An error tolera-
ble system for coreference resolution. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 76?80.
Yaqin Yang, Nianwen Xue, and Peter Anick. 2011. A
machine learning-based coreference detection system
276
for ontonotes. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 117?121.
Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,
Liping Ge, Zengjian Liu, Meng Liao, and Xianbo
Si. 2012. A mixed deterministic model for corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 76?82.
Desislava Zhekova and Sandra Ku?bler. 2011. Ubiu: A
robust system for resolving unrestricted coreference.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning: Shared Task,
pages 112?116.
Desislava Zhekova, Sandra Ku?bler, Joshua Bonner,
Marwa Ragheb, and Yu-Yin Hsu. 2012. Ubiu for mul-
tilingual coreference resolution in ontonotes. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 88?94.
Huiwei Zhou, Yao Li, Degen Huang, Yan Zhang, Chun-
long Wu, and Yuansheng Yang. 2011. Combin-
ing syntactic and semantic features by svm for unre-
stricted coreference resolution. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 66?70.
277
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 874?878,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decipherment with a Million Random Restarts
Taylor Berg-Kirkpatrick Dan Klein
Computer Science Division
University of California, Berkeley
{tberg,klein}@cs.berkeley.edu
Abstract
This paper investigates the utility and effect of
running numerous random restarts when us-
ing EM to attack decipherment problems. We
find that simple decipherment models are able
to crack homophonic substitution ciphers with
high accuracy if a large number of random
restarts are used but almost completely fail
with only a few random restarts. For partic-
ularly difficult homophonic ciphers, we find
that big gains in accuracy are to be had by run-
ning upwards of 100K random restarts, which
we accomplish efficiently using a GPU-based
parallel implementation. We run a series of
experiments using millions of random restarts
in order to investigate other empirical proper-
ties of decipherment problems, including the
famously uncracked Zodiac 340.
1 Introduction
What can a million restarts do for decipherment?
EM frequently gets stuck in local optima, so running
between ten and a hundred random restarts is com-
mon practice (Knight et al, 2006; Ravi and Knight,
2011; Berg-Kirkpatrick and Klein, 2011). But, how
important are random restarts and how many random
restarts does it take to saturate gains in accuracy?
We find that the answer depends on the cipher. We
look at both Zodiac 408, a famous homophonic sub-
stitution cipher, and a more difficult homophonic ci-
pher constructed to match properties of the famously
unsolved Zodiac 340. Gains in accuracy saturate af-
ter only a hundred random restarts for Zodiac 408,
but for the constructed cipher we see large gains
in accuracy even as we scale the number of ran-
dom restarts up into the hundred thousands. In both
cases the difference between few and many random
restarts is the difference between almost complete
failure and successful decipherment.
We also find that millions of random restarts can
be helpful for performing exploratory analysis. We
look at some empirical properties of decipherment
problems, visualizing the distribution of local op-
tima encountered by EM both in a successful deci-
pherment of a homophonic cipher and in an unsuc-
cessful attempt to decipher Zodiac 340. Finally, we
attack a series of ciphers generated to match proper-
ties of Zodiac 340 and use the results to argue that
Zodiac 340 is likely not a homophonic cipher under
the commonly assumed linearization order.
2 Decipherment Model
Various types of ciphers have been tackled by the
NLP community with great success (Knight et al,
2006; Snyder et al, 2010; Ravi and Knight, 2011).
Many of these approaches learn an encryption key
by maximizing the score of the decrypted message
under a language model. We focus on homophonic
substitution ciphers, where the encryption key is a
1-to-many mapping from a plaintext alphabet to a
cipher alphabet. We use a simple method introduced
by Knight et al (2006): the EM algorithm (Demp-
ster et al, 1977) is used to learn the emission pa-
rameters of an HMM that has a character trigram
language model as a backbone and the ciphertext
as the observed sequence of emissions. This means
that we learn a multinomial over cipher symbols for
each plaintext character, but do not learn transition
874
parameters, which are fixed by the language model.
We predict the deciphered text using posterior de-
coding in the learned HMM.
2.1 Implementation
Running multiple random restarts means running
EM to convergence multiple times, which can be
computationally intensive; luckily, restarts can be
run in parallel. This kind of parallelism is a good
fit for the Same Instruction Multiple Thread (SIMT)
hardware paradigm implemented by modern GPUs.
We implemented EM with parallel random restarts
using the CUDA API (Nickolls et al, 2008). With a
GPU workstation,1 we can complete a million ran-
dom restarts roughly a thousand times more quickly
than we can complete the same computation with a
serial implementation on a CPU.
3 Experiments
We ran experiments on several homophonic sub-
stitution ciphers: some produced by the infamous
Zodiac killer and others that were automatically
generated to be similar to the Zodiac ciphers. In
each of these experiments, we ran numerous random
restarts; and in all cases we chose the random restart
that attained the highest model score in order to pro-
duce the final decode.
3.1 Experimental Setup
The specifics of how random restarts are produced
is usually considered a detail; however, in this work
it is important to describe the process precisely. In
order to generate random restarts, we sampled emis-
sion parameters by drawing uniformly at random
from the interval [0, 1] and then normalizing. The
corresponding distribution on the multinomial emis-
sion parameters is mildly concentrated at the center
of the simplex.2
For each random restart, we ran EM for 200 itera-
1We used a single workstation with three NVIDIA GTX 580
GPUs. These are consumer graphics cards introduced in 2011.
2We also ran experiments where emission parameters were
drawn from Dirichlet distributions with various concentration
parameter settings. We noticed little effect so long as the distri-
bution did not favor the corners of the simplex. If the distribu-
tion did favor the corners of the simplex, decipherment results
deteriorated sharply.
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  10  100  1000  10000  100000  1e+06
-1530
-1520
-1510
-1500
-1490
-1480
-1470
-1460
Ac
cur
acy
Log likelihood
Number of random restarts
Log likelihood
Accuracy
Figure 1: Zodiac 408 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
tions.3 We found that smoothing EM was important
for good performance. We added a smoothing con-
stant of 0.1 to the expected emission counts before
each M-step. We tuned this value on a small held
out set of automatically generated ciphers.
In all experiments we used a trigram character
language model that was linearly interpolated from
character unigram, bigram, and trigram counts ex-
tracted from both the Google N-gram dataset (Brants
and Franz, 2006) and a small corpus (about 2K
words) of plaintext messages authored by the Zodiac
killer.4
3.2 An Easy Cipher: Zodiac 408
Zodiac 408 is a homophonic cipher that is 408 char-
acters long and contains 54 different cipher sym-
bols. Produced by the Zodiac killer, this cipher was
solved, manually, by two amateur code-breakers a
week after its release to the public in 1969. Ravi and
Knight (2011) were the first to crack Zodiac 408 us-
ing completely automatic methods.
In our first experiment, we compare a decode of
Zodiac 408 using one random restart to a decode us-
ing 100 random restarts. Random restarts have high
3While this does not guarantee convergence, in practice 200
iterations seems to be sufficient for the problems we looked at.
4The interpolation between n-gram orders is uniform, and
the interpolation between corpora favors the Zodiac corpus with
weight 0.9.
875
variance, so when we present the accuracy corre-
sponding to a given number of restarts we present an
average over many bootstrap samples, drawn from
a set of one million random restarts. If we attack
Zodiac 408 with a single random restart, on aver-
age we achieve an accuracy of 18%. If we instead
use 100 random restarts we achieve a much better
average accuracy of 90%. The accuracies for vari-
ous numbers of random restarts are plotted in Fig-
ure 1. Based on these results, we expect accuracy
to increase by about 72% when using 100 random
restarts instead of a single random restart; however,
using more than 100 random restarts for this partic-
ular cipher does not appear to be useful.
Also in Figure 1, we plot a related graph, this time
showing the effect that random restarts have on the
achieved model score. By construction, the (maxi-
mum) model score must increase as we increase the
number of random restarts. We see that it quickly
saturates in the same way that accuracy did.
This raises the question: have we actually
achieved the globally optimal model score or have
we only saturated the usefulness of random restarts?
We can?t prove that we have achieved the global op-
timum,5 but we can at least check that we have sur-
passed the model score achieved by EM when it is
initialized with the gold encryption key. On Zodiac
408, if we initialize with the gold key, EM finds
a local optimum with a model score of ?1467.4.
The best model score over 1M random restarts is
?1466.5, which means we have surpassed the gold
initialization.
The accuracy after gold initialization was 92%,
while the accuracy of the best local optimum was
only 89%. This suggests that the global optimum
may not be worth finding if we haven?t already
found it. From Figure 1, it appears that large in-
creases in likelihood are correlated with increases
in accuracy, but small improvements to high like-
lihoods (e.g. the best local optimum versus the gold
initialization) may not to be.
5ILP solvers can be used to globally optimize objectives
corresponding to short 1-to-1 substitution ciphers (Ravi and
Knight, 2008) (though these objectives are slightly different
from the likelihood objectives faced by EM), but we find that
ILP encodings for even the shortest homophonic ciphers cannot
be optimized in any reasonable amount of time.
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 1  10  100  1000  10000  100000  1e+06
-1310
-1305
-1300
-1295
-1290
-1285
-1280
Ac
cur
acy
Log likelihood
Number of random restarts
Log likelihood
Accuracy
Figure 2: Synth 340 cipher. Accuracy by best model score and
best model score vs. number of random restarts. Bootstrapped
from 1M random restarts.
3.3 A Hard Cipher: Synth 340
What do these graphs look like for a harder cipher?
Zodiac 340 is the second cipher released by the Zo-
diac killer, and it remains unsolved to this day. How-
ever, it is unknown whether Zodiac 340 is actually a
homophonic cipher. If it were a homophonic cipher
we would certainly expect it to be harder than Zo-
diac 408 because Zodiac 340 is shorter (only 340
characters long) and at the same time has more ci-
pher symbols: 63. For our next experiment we gen-
erate a cipher, which we call Synth 340, to match
properties of Zodiac 340; later we will generate mul-
tiple such ciphers.
We sample a random consecutive sequence of 340
characters from our small Zodiac corpus and use
this as our message (and, of course, remove this se-
quence from our language model training data). We
then generate an encryption key by assigning each
of 63 cipher symbols to a single plain text charac-
ter so that the number of cipher symbols mapped to
each plaintext character is proportional to the fre-
quency of that character in the message (this bal-
ancing makes the cipher more difficult). Finally, we
generate the actual ciphertext by randomly sampling
a cipher token for each plain text token uniformly at
random from the cipher symbols allowed for that to-
ken under our generated key.
In Figure 2, we display the same type of plot, this
time for Synth 340. For this cipher, there is an abso-
876
 0
 10000
 20000
 30000
 40000
 50000
 60000
-1340 -1330 -1320 -1310 -1300 -1290 -1280 -1270
Fr
eq
ue
nc
y
Log likelihood
42%
ntyouldli 59%
veautital
74%
veautiful
Figure 3: Synth 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
Several peaks are labeled with their average accuracy and a
snippet of a decode. The gold snippet is ?beautiful.?
lute gain in accuracy of about 9% between 100 ran-
dom restarts and 100K random restarts. A similarly
large gain is seen for model score as we scale up the
number of restarts. This means that, even after tens
of thousands of random restarts, EM is still finding
new local optima with better likelihoods. It also ap-
pears that, even for a short cipher like Synth 340,
likelihood and accuracy are reasonably coupled.
We can visualize the distribution of local optima
encountered by EM across 1M random restarts by
plotting a histogram. Figure 3 shows, for each range
of likelihood, the number of random restarts that
led to a local optimum with a model score in that
range. It is quickly visible that a few model scores
are substantially more likely than all the rest. This
kind of sparsity might be expected if there were
a small number of local optima that EM was ex-
tremely likely to find. We can check whether the
peaks of this histogram each correspond to a single
local optimum or whether each is composed of mul-
tiple local optima that happen to have the same like-
lihood. For the histogram bucket corresponding to a
particular peak, we compute the average relative dif-
ference between each multinomial parameter and its
mean. The average relative difference for the highest
peak in Figure 3 is 0.8%, and for the second highest
peak is 0.3%. These values are much smaller than
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
-1330 -1320 -1310 -1300 -1290 -1280
Fr
eq
ue
nc
y
Log likelihood
Figure 4: Zodiac 340 cipher. Histogram of the likelihoods of the
local optima encountered by EM across 1M random restarts.
the average relative difference between the means of
these two peaks, 40%, indicating that the peaks do
correspond to single local optima or collections of
extremely similar local optima.
There are several very small peaks that have the
highest model scores (the peak with the highest
model score has a frequency of 90 which is too
small to be visible in Figure 3). The fact that these
model scores are both high and rare is the reason we
continue to see improvements to both accuracy and
model score as we run numerous random restarts.
The two tallest peaks and the peak with highest
model score are labeled with their average accuracy
and a small snippet of a decode in Figure 3. The
gold snippet is the word ?beautiful.?
3.4 An Unsolved Cipher: Zodiac 340
In a final experiment, we look at the Zodiac 340
cipher. As mentioned, this cipher has never been
cracked and may not be a homphonic cipher or even
a valid cipher of any kind. The reading order of
the cipher, which consists of a grid of symbols, is
unknown. We make two arguments supporting the
claim that Zodiac 340 is not a homophonic cipher
with row-major reading order: the first is statistical,
based on the success rate of attempts to crack similar
synthetic ciphers; the second is qualitative, compar-
ing distributions of local optimum likelihoods.
If Zodiac 340 is a homophonic cipher should we
877
expect to crack it? In order to answer this question
we generate 100 more ciphers in the same way we
generated Synth 340. We use 10K random restarts to
attack each cipher, and compute accuracies by best
model score. The average accuracy across these 100
ciphers was 75% and the minimum accuracy was
36%. All but two of the ciphers were deciphered
with more than 51% accuracy, which is usually suf-
ficient for a human to identify a decode as partially
correct.
We attempted to crack Zodiac 340 using a row-
major reading order and 1M random restarts, but the
decode with best model score was nonsensical. This
outcome would be unlikely if Zodiac 340 were like
our synthetic ciphers, so Zodiac 340 is probably not
a homophonic cipher with a row-major order. Of
course, it could be a homophonic cipher with a dif-
ferent reading order. It could also be the case that
a large number of salt tokens were inserted, or that
some other assumption is incorrect.
In Figure 4, we show the histogram of model
scores for the attempt to crack Zodiac 340. We note
that this histogram is strikingly different from the
histogram for Synth 340. Zodiac 340?s histogram is
not as sparse, and the range of model scores is much
smaller. The sparsity of Synth 340?s histogram (but
not Zodiac 340?s histogram) is typical of histograms
corresponding to our set of 100 generated ciphers.
4 Conclusion
Random restarts, often considered a footnote of ex-
perimental design, can indeed be useful on scales
beyond that generally used in past work. In particu-
lar, we found that the initializations that lead to the
local optima with highest likelihoods are sometimes
very rare, but finding them can be worthwhile; for
the problems we looked at, local optima with high
likelihoods also achieved high accuracies. While the
present experiments are on a very specific unsuper-
vised learning problem, it is certainly reasonable to
think that large-scale random restarts have potential
more broadly.
In addition to improving search, large-scale
restarts can also provide a novel perspective when
performing exploratory analysis, here letting us ar-
gue in support for the hypothesis that Zodiac 340 is
not a row-major homophonic cipher.
References
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Sim-
ple effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium, Catalog Num-
ber LDC2009T25.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the 2006 Annual Meeting
of the Association for Computational Linguistics.
John Nickolls, Ian Buck, Michael Garland, and Kevin
Skadron. 2008. Scalable parallel programming with
CUDA. Queue.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian inference
for Zodiac and other homophonic ciphers. In Proceed-
ings of the 2011 Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 2010 Annual Meeting of
the Association for Computational Linguistics.
878
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898?1907,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A multi-Teraflop Constituency Parser using GPUs
John Canny David Hall Dan Klein
UC Berkeley
Berkeley, CA, 94720
canny@berkeley.edu, dlwh,klein@cs.berkeley.edu
Abstract
Constituency parsing with rich grammars re-
mains a computational challenge. Graph-
ics Processing Units (GPUs) have previously
been used to accelerate CKY chart evalua-
tion, but gains over CPU parsers were mod-
est. In this paper, we describe a collection of
new techniques that enable chart evaluation at
close to the GPU?s practical maximum speed
(a Teraflop), or around a half-trillion rule eval-
uations per second. Net parser performance
on a 4-GPU system is over 1 thousand length-
30 sentences/second (1 trillion rules/sec), and
400 general sentences/second for the Berkeley
Parser Grammar. The techniques we introduce
include grammar compilation, recursive sym-
bol blocking, and cache-sharing.
1 Introduction
Constituency parsing with high accuracy (e.g. latent
variable) grammars remains a computational chal-
lenge. The O(Gs3) complexity of full CKY pars-
ing for a grammar with G rules and sentence length
s, is daunting. Even with a host of pruning heuris-
tics, the high cost of constituency parsing limits its
uses. The most recent Berkeley latent variable gram-
mar for instance, has 1.7 million rules and requires
about a billion rule evaluations for inside scoring of
a single length-30 sentence. GPUs have previously
been used to accelerate CKY evaluation, but gains
over CPU parsers were modest. e.g. in Yi et al
(2011) a GPU parser is described for the Berkeley
Parser grammar which achieves 5 sentences per sec-
ond on the first 1000 sentences of Penn Treebank
section 22 Marcus et al (1993), which is compa-
rable with the best CPU parsers Petrov and Klein
(2007). Our parser achieves 120 sentences/second
per GPU for this sentence set, and over 250 sen-
tences/sec on length ? 30 sentences. These results
use a Berkeley Grammar approximately twice as big
as Yi et al (2011), an apparent 50x improvement.
On a 4-GPU system, we achieve 1000 sentences/sec
for length ? 30 sentences. This is 2 orders of mag-
nitude faster than CPU implementations that rely
heavily on pruning, and 5 orders of magnitude faster
than full CKY evaluation on a CPU.
Key to these results is a collection of new tech-
niques that enable GPU parsing at close to the
GPU?s practical maximum speed (a Teraflop for re-
cent GPUs), or around a half-trillion rule evaluations
per second. The techniques are:
1. Grammar compilation, which allows register-
to-register code for application of grammar
rules. This gives an order of magnitude (10x)
speedup over alternative approaches that use
shared memory.
2. Symbol/rule blocking of the grammar to re-
spect register, constant and instruction cache
limits. This is precondition for 1 above, and
the details of the partitioning have a big (> 4x)
effect on performance.
3. Sub-block partitioning to distribute rules across
the stream processors of the GPU and allow L2
cache acceleration. A factor of 2 improvement.
The code generated by our parser comes close to the
theoretical limits of the GPU. 80% of grammar rules
1898
are evaluated using a single-cycle register-to-register
instruction.
2 GPU Design Principles
In this paper, we focus on the architecture of recent
NVIDIA R? GPUs, though many of the principles we
describe here can be applied to other GPUs (e.g.
those made by AMD R?.) The current NVIDIA R?
KeplerTM series GPU contains between 2 and 16
?stream processors? or SMX?s which share an L2
cache interfacing to the GPUs main memory Anony-
mous (2013). The SMXs in turn comprise 192
cores which share a memory which is partitioned
into ?shared memory? and L1 cache. Shared mem-
ory supports relatively fast communication between
threads in an SMX. Communication between SMXs
has to pass through slower main memory.
The execution of instructions within SMXs is vir-
tualized and pipelined - i.e. it is not a simple task to
count processors, although there are nominally 192
in the KeplerTM series. Register storage is not at-
tached to cores, instead registers are associated in
blocks of 63 or 255 (depending on KeplerTM sub-
architecture) with running threads. Because of this,
it is usually easier for the programmer to think of
the SMXes as 1024 thread processors. These 1024
threads are grouped into 32 groups of 32 threads
called warps. Each warp of threads shares a program
counter and executes code in lock-step. However,
execution is not SIMD - all threads do not execute all
instructions. When the warp encounters a branching
instruction, all branches that are satisfied by some
thread will be executed in sequence. Each thread
only executes the instructions for its own branch,
and idles for the others. NVIDIA R? calls this model
SIMT (Single Instruction, Multiple Threads). Ex-
ecution of diverging branches by a warp is called
warp divergence. While it simplifies programming,
warp divergence understandably hurts performance
and our first goal is to avoid it.
GPUs are generally optimized for single-
precision floating point arithmetic in support of
rendering and simulation. Table 1 shows instruction
throughput (number of instructions that are executed
per cycle on each SMX). The KeplerTM series has
two architectural sub-generations (3.0 and 3.5) with
significant differences in double-precision support.
Data from Anonymous (2012) and NVIDIA (2012).
Instruction type
Architecture
3.0 3.5
Shared memory word access 32 32
FP arithmetic +,-,*,FMA 192 192
DP arithmetic +,-,*,FMA 8 64
Integer +,- 160 160
Integer *,FMA 32 32
float sin, exp, log,... 32 32
Table 1: Instructions per cycle per SMX in generation 3.0
and 3.5 KeplerTM devices
In the table, FP is floating point, DP is double
precision, and FMA is a single-cycle floating-piont
fused multiply-add used in most matrix and vector
operations (A? A+B ?C). Note next that floating
point (single precision) operations are extremely fast
and there is an FPU for each of the 192 processors.
Double precision floating point is 3x slower on high-
end 3.5 GPUS, and much slower (24x) on the com-
modity 3.0 machines. While integer addition is fast,
integer multiply is much slower. Perhaps most sur-
prising is the speed of single-precision transcenden-
tal function evaluation, log, exp, sin, cos, tan, etc.,
which are as fast as shared memory accesses or in-
teger multiplication, and which amount to a quarter-
trillion transcendental evaluations per second on a
GTX-680/K10.
PCFG grammar evaluation nominally requires
two multiplications and an addition per rule (section
4) which can be written:
Sij,m =
?
k=1...j; n,p?Q
Sik,nS(k+1)j,pcmnp (1)
i.e. the CKY node scores are sums of products of
pairs of scores and a weight. This suggests that at
least in principle, it?s possible to achieve a trillion
rule evaluations per second on a 680 or K10 device,
using a * and an FMA operation for each rule. That
assumes we are doing register-to-register operations
however. If we worked through shared memory (first
line of the table), we would be limited to about 80
billion evaluations/sec, 20 times slower. The anal-
ysis underscores that high performance for parsing
on a GPU is really a challenge of data movement.
We next review the different storage types and their
1899
bandwidths, since prudent use of, and movement be-
tween storage types is the key to performance.
2.1 Memory Types and Speeds
There are six types of storage on the GPU which
matter for us. For each type, we give the capac-
ity and aggregate bandwidth on a typical device (a
GTX-680 or K10 running at 1GHz).
Register files These are virtualized and associated
with threads rather than processors. 256kB per
SMX. Each thread in architecture 3.0 devices can
access 63 32-bit registers, or 255 registers for 3.5
devices. Aggregate bandwidth 40 TB/s.
Shared memory/L1 cache is shared by all threads
in an SMX. 64kB per SMX partitioned into shared
memory and cache functions. Aggregate bandwidth
about 1 TB/s.
Constant memory Each SMX has a 48kB read/only
cache separate from the L1 cache. It can store gram-
mar constants and has much higher bandwidth than
shared memory. Broadast bandwidth 13 TB/s.
Instruction cache is 8 KB per SMX. Aggregate
bandwidth 13 TB/s.
L2 cache is 0.5-1.5 MB, shared between all SMXs.
Aggregate bandwidth 500 GB/s.
Global memory is 2-6GB typically, and is shared
by all SMXs. GPUs use a particularly fast form of
SDRAM (compared to CPUs) but it is still much
slower than the other memory types above. Ag-
gregate bandwidth about 160 GB/s.
There is one more very important principle: Coa-
lesced main memory access. From the above it can
be seen that main memory access is much slower
than other memories and can easily bottleneck the
calculations. The figure above (160 GB/s) for main
memory access assumes such access is coalesced.
Each thread in the GPU has a thread and a block
number which determines where it runs on the hard-
ware. Consecutively-numbered threads should ac-
cess consecutive main memory locations for fast
memory access.
These parameters suggest a set of design princi-
ples for peak performance:
1. Maximize use of registers for symbol scores,
and minimize use of shared memory (in fact we
will not use it at all).
2. Maximize use of constant memory for rule
weights, and minimize use of shared memory.
3. Partition the rule set into blocks that respect the
limits on number of registers, constant memory
(needed for grammar rules probabilities) and
instruction cache limits.
4. Minimize main memory access and use L2
cache to speed it up.
Lets look in more detail at how to achieve this.
3 Anatomy of an Efficient GPU Parser
High performance on the GPU requires us to mini-
mize code divergence. This suggests that we do not
use a lexicalized grammar or a grammar that is sen-
sitive to the position of a span within the sentence.
These kinds of grammars?while highly accurate?
have irregular memory access patterns that conflict
with SIMD execution. Instead, an unlexicalized ap-
proach like that of Johnson (2011) or Klein and
Manning (2003), or a latent variable approach like
that of Matsuzaki et al (2005) or Petrov et al (2006)
are more appropriate. We opt for the latter kind: la-
tent variable grammars are fairly small, and their ac-
curacies rival lexicalized approaches.
Our GPU-ized inside algorithm maintains two
data structures: parse charts that store scores for
each labeled span, as usual, and a ?workspace? that
is used to actually perform the updates of the in-
side algorithm. Schematically, this memory lay-
out is represented in Figure 1. A queue is main-
tained CPU-side that enqueues work items of the
form (s, p, l, r), where s is a sentence, and p, l, and
r specify the index in the parse chart for parent, left
child, and right child, respectively. The outer loop
proceeds in increasing span length (or height of par-
ent node scores to be computed). Next the algorithm
iterates over the available sentences. Then it iterates
over the parent nodes at the current length in that
sentences, and finally over all split points for the cur-
rent parent node. In each case, work items are sent to
the queue with that span for all possible split points.
When the queue is full?or when there are no
more work items of that length?the queue is flushed
1900
Figure 1: The architecture of the system. Parse charts are
stored in triangular arrays laid out consecutively in mem-
ory. Scores for left and right children are transposed and
copied into the ?workspace? array, and the inside updates
are calculated for the parent. Scores are then pushed back
to the appropriate cell in the parse charts, maxing them
with scores that are already there. Transposition ensures
that reads and writes are coalesced.
to the GPU, which executes three steps. First, the
scores for each left and right child are copied into
the corresponding column in the workspace. Then
inside updates are applied in parallel for all cells to
get parent scores. Then parents are entered back to
their appropriate cells in the parse charts. This is
typically a many-to one atomic reduction (either a
sum for probability scores, or a max for max-sum
log probability scores). This process repeats until
all span lengths have been processed.
3.1 The Inside Updates
The high-level goal of our parser is to use SIMD
parallelism to evaluate the same rule across many
spans (1024 threads are currently used to process
8192 spans in each kernel). This approaches allows
us to satisfy the GPU performance desiderata from
the previous section. As discussed in section 5 each
GPU kernel actually processes a small subset of the
symbols and rules for the grammar, and kernels are
executed in sequence until the entire grammar has
been processed. Each thread iterates over the rules
in the same order, reading in symbols from the left
child and right child arrays in main memory as nec-
essary.
The two-dimensional work arrays must be stored
in ?symbol-major? order for this to work. That is,
the parent VP for one work item is stored next to the
parent VP for the next work item, while the VP sym-
bol for the first work item is stored on the next ?row?
of the work array. The reason the workspace cells
are stored in ?symbol-major? order is to maximize
coalesced access: each thread in the SMX accesses
the same symbol for a different work item in paral-
lel, and those work items are in consecutive memory
locations.
3.2 The Copy-transpose Operations
Unlike the workspace arrays, the arrays for the parse
charts are stored in ?span-major? order, transposed
from how they are stored in the workspace arrays.
That is, for a given span, the NP symbol is next
to the same span?s VP symbol (for example). This
order accelerates both symbol loading and Viterbi
search later on. It requires a transpose-copy in-
stead of ?non-transposed? copy to move from chart
to workspace arrays and back again, but note that a
non-transposed copy (or direct access to the chart
by the GPU compute kernel) would probably be
slower. The reason is that any linear ordering of
cells in the triangle table will produce short seg-
ments (less than 32 words and often less than 16)
of consecutive memory locations. This will lead to
many non-coalesced memory accesses. By contrast
the span-major representation always uses vectors
whose lengths equals the number of symbols (500-
1000), and these can be accessed almost entirely
with coalesced operations. The copy-transpose op-
erations are quite efficient (the transpose itself is
much faster than the I/O), and come close to the 160
GB/s GPU main memory limit.
The reverse copy-transpose (from parent
workspace cells to chart) is typically many-to-one,
since parent scores derive from multiple splits. They
are implemented using atomic reduce operations
(either atomic sum or atomic max) to ensure data
consistency.
At the heart of our approach is the use of grammar
compilation and symbol/rule blocking, described
next.
1901
4 Grammar Compilation
Each rule in a probabilistic context-free grammar
can be evaluated with an update of the form:
Sij,m =
?
k=1...j; n,p?Q
Sik,nS(k+1)j,pcmnp (2)
where Sij,m is the score for symbol m as a generator
of the span of words from position i to j in the in-
put sentence, cmnp is the probability that symbol m
generates the binary symbol pair n, p, and Q is the
set of symbols. The scores will be stored in a CKY
chart indexed by the span ij and the symbol m.
To evaluate (2) as fast as possible, we want to
use register variables which are limited in number.
The location indices i, j, k can be moved outside the
GPU kernel to reduce the variable count. We use
symbols P , L and R for respectively the score of the
parent, left child and right child in the CKY chart.
Then the core relation in (2) can be written as:
Pm =
?
n,p?Q
LnRpcmnp (3)
In the KeplerTM architecture, register arguments are
non-indexed, i.e. one cannot access register 3 as an
array variable R[i] with i=31. So in order to use
register storage for maximum speed, we must open-
code the grammar. Symbols like L3, R17 are en-
coded as variables L003 and R017, and each rule
must appear as a line of C code:
P043 += L003*R017*0.023123f;
P019 += L012*R123*6.21354e-7f;
: : : :
Open-coding the grammar likely has a host of per-
formance advantages. It allows both compiler and
hardware to ?see? what arguments are coming and
schedule the operations earlier than a ?grammar
as data? approach. Note that we show here the
sum-product code for computing inner/outer symbol
probabilities. For Viterbi parse extraction we replace
+,* with max,+ and work on log scores.
L and R variables must be loaded from main
memory, while P-values are initialized to zero and
then atomically combined (sum or max) with P-
values in memory. Loads are performed as late as
1Even if indexing were possible, it is extremely unlikely that
such accesses could complete in a single cycle
possible, that is, a load instruction will immediately
precede the first use of a symbol:
float R031 = right[tid+65*stride];
P001 += L001*R031*1.338202e-001f;
where tid is the thread ID plus an offset, and stride
is the row dimension of the workspace (typically
8192), and right is the main memory array of right
symbol scores. Similarly, atomic updates to P-
values occur as early as possible, right after the last
update to a value:
G020 += L041*R008*6.202160e-001f;
atomicAdd(&par[tid+6*stride],G020);
These load/store strategies minimize the active life
of each variable and allow reuse of register variables
for symbols whose lifetimes do not overlap. This
will be critical to successful blocking, described in
the next section.
4.1 Common subexpressions
One interesting discovery made by the compiler was
that the same L,R pair is repeated in several rules. In
hindsight, this is obvious because the symbols in this
grammar are splits of base symbols, and so splits of
the parent symbol will be involved in rules with each
pair of L,R splits. The compiler recognized this by
turning the L,R pair into a common subexpression
in a register. i.e. the compiler converts
P008 += L041*R008*6.200769e-001f;
P009 += L041*R008*6.201930e-001f;
P010 += L041*R008*6.202160e-001f;
into
float LRtmp = L041*R008;
P008 += LRtmp*6.200769e-001f;
P009 += LRtmp*6.201930e-001f;
P010 += LRtmp*6.202160e-001f;
and inspection of the resulting assembly code shows
that each rule is compiled into a single fused
multiply-add of LRtmp and a value from con-
stant memory into the P symbol register. This al-
lows grammar evaluation to approach the theoretical
Gflop limit of the GPU. For this to occur, the rules
need to be sorted with matching L,R pairs consecu-
tive. The compiler does not discover this constraint
otherwise or reorder instructions to make it possible.
1902
4.2 Exploiting L2 cache
Finally, we have to generate code to evaluate distinct
minor cube rulesets on each of the 8 SMXes con-
currently in order to benefit from the L2 cache, as
described in the next section. CUDATM (NVIDIA?s
GPU programming Platform) does not allow direct
control of SMX target, but we can achieve this by
running the kernel as 8 thread blocks and then test-
ing the block ID within the kernel and dispatching to
one of 8 blocks of rules. The CUDATM scheduler
will execute each thread block on a different SMX
which gives the desired distribution of code.
5 Symbol and Rule Blocking
The grammar formula (3) is very sparse. i.e. most
productions are impossible and most cmnp are zero.
For the Berkeley grammar used here, only 0.2% of
potential rules occur. Normally this would be bad
news for performance because it suggests low vari-
able re-use. However, the update relation is a tensor
rather than a matrix product. The re-use rate is deter-
mined by the number of rules in which a particular
symbol occurs, which is actually very high (more
than 1000 on average).
The number of symbols is about 1100 in this
grammar, and only a fraction can be stored in a
thread?s register set at one time (which is either 63 or
255 registers). To compute all productions we will
need to break the calculation into smaller groups of
variables that can fit in the available register space.
We can visualize this geometrically in figure 2.
The vectors of symbols P , L and R form the lead-
ing edges of this cube. The cube will be partitioned
into smaller subcubes indexed by subsets of those
symbols, and containing all the rules that apply be-
tween those symbols. The partitioning is chosen so
that the symbols in that subset can fit into available
register storage. In addition, the partitioning is cho-
sen to induce the same number of rules in each cube
- otherwise different code paths in the kernel will run
longer than others, and reduce overall performance.
This figure is a simplification - in order to balance
the number of rules in each subcube, the partition-
ing is not uniform in number of symbols as the figure
suggests.
As can be seen in figure 2, cube partitioning has
two levels. The original P-L-R cube is first par-
 
P R 
L 
Figure 2: Partition of the cube of symbol combinations
into major subcubes (left) and minor subcubes (right).
titioned into ?major? cubes, which are then parti-
tioned into ?minor? cubes (2x2x2 in the figure). A
major cube holds the symbols and rules that are ex-
ecuted in a single GPU kernel. The minor cubes in a
major cube hold the symbols and rules that are exe-
cuted in a particular SMX. For the GTX-680 or K10
with 8 SMXs, this allows different SMXs to concur-
rently work on different 2x2x2 subcubes in a major
cube. This arrangement substantially reduces main
memory bandwidth through the L2 cache (which is
shared between SMXes). Each symbol in a major
cube will be loaded just once from main memory,
but loaded into (up to) 4 different SMXes through
the L2 cache. Subcube division for caching in our
experiments roughly doubled the kernel?s speed.
However, simple partitioning will not work. e.g.
if we blocked into groups of 20 P, L, R symbols
(in order to fit into 60 registers), we would need
1100/20 = 55 blocks along each edge, and a total of
553 ? 160, 000 cells. Each symbol would need to
be loaded 552 = 3025 times, there would be almost
no symbol re-use. Throughput would be limited by
main memory speed to about 100 Gflops, an order
of magnitude slower than our target. Instead, we
use a rule partitioning scheme that creates as small a
symbol footprint as possible in each cube. We use a
spectral method to do this.
Before describing the spectral method we men-
tion an optimization that drops the symbol count by
2. Symbols are either terminal or non-terminal, and
in the Berkeley latent variable grammar there are
roughly equal numbers of them (503 non-terminals
and 631 terminals). All binary rules involve a non-
terminal parent. L and R symbols may be either ter-
minal or non-terminal, so there are 4 distinct types
1903
of rules depending on the L, R, types. We handle
each of these cases with a different kernel, which
rougly halves the number of rules along each edge
(it is either 503 or 631 along each edge). Further-
more, these kernels are called in different contexts,
and a different number of times. e.g. XX (L, R, both
non-terminal) kernels are called O(s3) times for sen-
tences of length s because both L, R children can oc-
cur at every position in the chart. XT and TX kernels
(with one terminal and one non-terminal symbol) are
called only O(s2) times since one of L or R must be
at the base of the chart. Finally TT kernels (both L
and R are terminals) will be called O(s) times. Per-
formance is therefore dominated by the XX kernel.
5.1 Spectral Partitioning
We explored a number of partitioning schemes for
both symbol and rule partitioning. In the end we
settled on a spectral symbol partitioning scheme.
Each symbol is a node in the graph to be parti-
tioned. Each node is assigned a feature vector de-
signed to match it to other nodes with similar sym-
bols occuring in many rules. There was considerable
evolution of this feature set to improve partitioning.
In the end the vector for a particular P symbol is
a = (a1, 0.1 ? a2, 0.1 ? a3) where a1 is a vector
whose elements are indexed by L, R pairs and whose
values represent the number of rules involving both
those symbols (and the parent symbol P), a2 encodes
L symbols and counts the number of rules contain-
ing that L symbol and P, and a3 encodes the R sym-
bols and counts rules containing that R symbol and
P. This feature vector produces a high similarity be-
tween P symbols that exactly share many L,R pairs
and lower similarity for shared L and R.
A spectral clustering/partitioning algorithm ap-
proximately minimizes the total edge weight of
graph cuts. In our case, the total weight of a cut is
to first order the product of the number of L,R pairs
that occur on each side of the cut, and to second or-
der the count of individual L and R pairs that span
the cut. Let S and T be the counts for a particular
LR pair or feature, then we are trying to minimize
the product S*T while keeping the sum S+T, which
is the total occurences of the feature on both sides of
the partition, constant. Such a product is minimized
when one of S or T is zero. Since many symbols
are involved, this typically does not happen to an in-
dividual symbol, but this heuristic is successful at
making the individual symbol or LR pair distribu-
tions across the cuts as unbalanced as possible. i.e.
one side of the cut has very few instances of a given
symbol. The number of instances of a symbol is an
upper bound on the number of subcells in which than
symbol occurs, and therefore on the number of times
it needs to be loaded from memory. Repeating this
operation recursively to produce a 3d cell decom-
position also concentrates each symbol in relatively
few cells, and so tends to reduce the total register
count per cell.
In a bit more detail, from the vectors a above we
construct a matrix A whose columns are the fea-
ture vectors for each P symbol. Next we construct
the symmetric normalized Laplacian L for the adja-
cency matrix ATA. We then compute the eigende-
composition of L, and extract the eigenvector cor-
responding to the second-smallest eigenvalue. Each
node in the graph is assign a real weight from the
corresponding element of this eigenvector. We sort
by these weights, and partition the symbols using
this sort order. We tried both recursive binary par-
titioning, and partitioning into k intervals using the
original sort order, and obtained better results with
the latter.
Partitioning is applied in order P, L, R to gener-
ate the major cubes of the rule/symbol partition, and
then again to generate minor cubes. This partition-
ing is far more efficient than a naive partitioning.
The XX ruleset for our Berkeley grammar has about
343,000 rules over a 5033 cube of non-terminal sym-
bols. The optimal PxLxR cube decomposition (op-
timal in net kernel throughput) for this ruleset was
6x2x2 for major cubes, and then 2x2x2 for minor
cubes. This requires 6x2x2=24 GPU kernels, each
of which encodes 2x2x2=8 code blocks (recall that
each of the 8 SMXs executes a different code block
from the same kernel)2. Most importantly the reload
rate (the mean number of major cells containing a
given symbol, or the mean number of times a sym-
bol needs to be reloaded from main memory) drops
to about 6 (vs. 3000 for naive partitioning). This
is very significant. Each symbol is used on average
343, 000/501 ? 6000 times overall by the XX ker-
2This cube decomposition also respects the constant cache
and instruction cache limits
1904
nel. Dropping the reload factor to 6 means that for
every 1 main memory load of a symbol, there are
approximately 1000 register or L2 cache reuses. A
little further calculation shows that L2 cache items
are used a little more than twice, so the register reuse
rate within kernel code blocks is close to 500 on av-
erage. This is what allows teraflop range speeds.
Note that while the maximum number of registers
per thread in the GTX-680 or K10 is 63, the aver-
age number of variables per minor cube is over 80
for our best-performing kernel, showing a number
of variables have non-overlapping lifetimes. Sorting
rules lexicographically by (L,R,P) does a good job
of minimizing variable lifetime overlap. However
the CUDATM compiler reorders variables anyway
with slightly worse performance on average (there
seems to be no way around this, other than generat-
ing assembly code directly).
6 GPU Viterbi Parse Extraction
In sequential programs for chart generation, it is pos-
sible to compute and save a pointer to the best split
point and score at each node in the chart. However,
here the scores at each node are computed with fine-
grained parallelism. The best split point and score
cannot be computed until all scores are available.
Thus there is a separate Viterbi step after chart scor-
ing.
The gap between GPU and CPU performance
is large enough that CPU Viterbi search was a
bottleneck, even though it requires asymptotically
less work (O(Gs2) worst case, O(Gs) typical) vs
O(Gs3) to compute the CKY scores. Therefore
we wrote a non-recusive GPU-based Viterbi search.
Current GPUs support ?high-level? recursion, but
there is no stack in the SMX. A recursive pro-
gram must create software stack space in either
shared memory or main memory which serious per-
formance impact on small function calls. Instead,
we use an iterative version of Viterbi parse extrac-
tion which uses pre-allocated array storage to store
its output, and such that the partially-complete out-
put array encodes all the information the algorithm
needs to proceed - i.e. the output array is also the
algorithm?s work queue.
Ignoring unaries for the moment, a binary parse
tree for a sentence of length n has 2n ? 1 nodes,
including preterminals, internal nodes, and the root.
We can uniquely represent a tree as an array with
2n ? 1 elements. In this representation, each index
corresponds to a node in prefix (depth-first) order.
For example, the root is always at position 0, and the
second node will correspond to the root?s left child.
If this second node has a left child, it will be the third
node, otherwise the third node will be the second?s
right sibling.
We can uniquely identify the topology of the tree
by storing the ?width? of each node in this array,
where the width is the number of words governed
by that constituent. For a node at position p, its left
child will always be at p + 1, and its right child will
always be at p+ 2 ?w`, where w` is the width of the
left child. The symbol for each node can obviously
be stored with the height. For unaries, we require
exactly one unary rule per node, with the possibil-
ity that it is the identity rule, and so we store two
nodes: one for the ?pre-unary? symbol, and one for
the ?post-unary.? (Identity unary transitions are re-
moved in post-processing.)
Algorithm 1 Non-recursive Viterbi implementation.
The algorithm proceeds left-to-right in depth-first
order along the array representing the tree.
Input: Sentence length n, parse chart V[i,j]
Output: Array tree of size 2?n?2
tree[0].preunary? ROOT
tree[0].width? n
i? 0 . Current leftmost position for span
for p? 0 to 2?n?2 do
j? i + tree[p].width . Rightmost position
postu? BestUnary(V, tree[p].preunary, i, j)
tree[p].postunary? parent
if tree[p].width = 1 then
i? i + 1
else
lc, rc, k? BestBinary(V, parent, i, j)
tree[p + 1].preunary? lc
tree[p + 1].width? k ? i
tree[p + 2?(k?i)].width? j - k
tree[p + 2?(k?i)].preunary? rc
end if
end for
Armed with this representation, we are ready to
describe algorithm 1. The algorithm proceeds in
1905
left-to-right order along the array. First, the sym-
bol of the root is recorded. Then, for each node in
the tree, we search for the best unary rule continuing
it. If the node is a terminal, then no more nodes can
contain the current word, and so we advance the po-
sition of the left most child. Otherwise, if the node
is a non-terminal, we then find its left and right chil-
dren, entering their respective symbols and widths
into the array representing the tree.
The GPU implementation follows the algorithm
outline above although is somewhat technical. Each
parse tree is handled by a separate thread block
(thread blocks are groups of threads that can com-
municate through shared memory, and run on a sin-
gle SMX). Each thread block includes a number of
threads which are used to rapidly (in partly parallel
fashion) iterate through rulesets and symbol vectors
for the BestBinary and BestUnary operations using
coalesced memory accesses. Each thread block first
loads the complete set of L and R scores for the
current split being explored. Recall that these are
in consecutive memory locations using the ?span-
major? ordering, so these loads are coalesced. Then
the thread block parallel-iterates through the rules
for the current parent symbol, which will be in a con-
tiguous block of memory since the rules are sorted
by parent symbol, and again are coalesced. The
thread block therefore needs storage for all the L, R
symbol scores and in addition working storage pro-
portional to the number of threads (to hold the best
child symbol and its score from each thread). The
number of threads is chosen to maximize speed: too
few will cause each thread to do more work and to
run more slowly. Too many will limit the number of
thread blocks (since the total threads concurrently
running on an SMX is 1024) that can run concur-
rently. We found 128 to be optimum.
With these techniques, Viterbi search consumes
approximately 1% of the parser?s running time. Its
throughput is around 10 Gflops, and it is 50-100x
faster than a CPU reference implementation.
7 Experiments
The parser was tested in an desktop computer with
one Intel E5-2650 processor, 64 GB ram, and
2 GTX-690 dual GPUs (effectively 4 GTX-680
GPUs). The high-level parser code is written in a
matrix library in the Scala language, which access
GPU code through JNI and using the JCUDA wrap-
per library for CUDATM.
XX-kernel throughput was 900 Gflops per GPU
for sum-product calculation (which uses a single
FMA for most rules) and 700 Gflops per GPU for
max-sum calculations (which requires two instruc-
tions for most rules). Net parser throughput in-
cluding max-sum CKY evaluation, Viterbi scoring
traspose-copy etc was between 500 and 600 gi-
gaflops per GPU, or about 2 teraflops total. Parsing
max-length-30 sentences from the Penn Treebank
test set ran at 250 sentences/sec per GPU, or 1000
sentences/sec total. General sentences were parsed
at about half this rate, 120 sentences/sec per GPU,
or 480 sentences/sec for the system.
8 Conclusions and Future Work
We described a new approach to GPU constituency
parsing with surprisingly fast performance, close
to the theoretical limits of the GPU and similar to
dense matrix multiplication which achieves the de-
vices highest practical throughput. The resulting
parser parses 1000 length-30 sentences per second
in a 4-GPU computer. The parser has immediate ap-
plication to parsing and eventually to parser training.
The two highest-priority extensions are:
Addition of pruning: coarse-to-fine score pruning
should be applicable to our GPU design as it is to
CPU parsers. GPU pruning will not be as granu-
lar as CPU pruning and is unlikely to yield as large
speedups (4-5 orders of magnitude are common for
CPU parser pruning). But on the other hand, we
hardly need speedups that large, and 1-2 orders of
magnitude would be very useful.
Direct generation of assembly code. Currently our
code generator produces (> 1.7 million lines, about
same as the number of rules) C source code which
must be compiled into GPU binary code. While it
takes only 8 seconds to generate the source code, it
takes more than an hour to compile it. The com-
piler evidently applies a number of optimizations
that we cannot disable, and this takes time. This
is an obstacle to e.g. using this framework to train
a parser where there would be frequent updates to
the grammar. However, since symbol variables cor-
respond almost one-to-one with registers (modulo
1906
lifetime overlap and reuse, which our code gener-
ator is slightly better at than the compiler), there is
no reason for our code generator not to generate as-
sembly code directly. Presumably assembly code is
much faster to translate into kernel modules than C
source, and hopefully this will lead to much faster
kernel generation.
8.1 Code Release
The code will be released under a BSD-style open
source license once its dependencies are fully in-
tegrated. Pre- and Final releases will be here
https://github.com/jcanny/BIDParse
References
Anonymous. 2012. CUDA C PROGRAMMING
GUIDE. Technical Report PG-02829-001-v5.0. In-
cluded with CUDATM Toolkit.
Anonymous. 2013. NVIDIA?s next generation CUDA
compute architecture: KeplerTM GK110. Technical
report. Included with CUDATM Tootkit.
Mark Johnson. 2011. Parsing in parallel on mul-
tiple cores and gpus. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2011, pages 29?37, Canberra, Australia, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
75?82, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
NVIDIA. 2012. private communication.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on gpus.
In Proceedings of the 2011 Conference on Parsing
Technologies, Dublin, Ireland, October.
1907
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971?1982,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Easy Victories and Uphill Battles in Coreference Resolution
Greg Durrett and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,klein}@cs.berkeley.edu
Abstract
Classical coreference systems encode various
syntactic, discourse, and semantic phenomena
explicitly, using heterogenous features com-
puted from hand-crafted heuristics. In con-
trast, we present a state-of-the-art coreference
system that captures such phenomena implic-
itly, with a small number of homogeneous
feature templates examining shallow proper-
ties of mentions. Surprisingly, our features
are actually more effective than the corre-
sponding hand-engineered ones at modeling
these key linguistic phenomena, allowing us
to win ?easy victories? without crafted heuris-
tics. These features are successful on syntax
and discourse; however, they do not model
semantic compatibility well, nor do we see
gains from experiments with shallow seman-
tic features from the literature, suggesting that
this approach to semantics is an ?uphill bat-
tle.? Nonetheless, our final system1 outper-
forms the Stanford system (Lee et al (2011),
the winner of the CoNLL 2011 shared task)
by 3.5% absolute on the CoNLL metric and
outperforms the IMS system (Bjo?rkelund and
Farkas (2012), the best publicly available En-
glish coreference system) by 1.9% absolute.
1 Introduction
Coreference resolution is a multi-faceted task: hu-
mans resolve references by exploiting contextual
and grammatical clues, as well as semantic infor-
mation and world knowledge, so capturing each of
1The Berkeley Coreference Resolution System is available
at http://nlp.cs.berkeley.edu.
these will be necessary for an automatic system to
fully solve the problem. Acknowledging this com-
plexity, coreference systems, either learning-based
(Bengtson and Roth, 2008; Stoyanov et al, 2010;
Haghighi and Klein, 2010; Rahman and Ng, 2011b)
or rule-based (Haghighi and Klein, 2009; Lee et
al., 2011), draw on diverse information sources and
complex heuristics to resolve pronouns, model dis-
course, determine anaphoricity, and identify seman-
tically compatible mentions. However, this leads to
systems with many heterogenous parts that can be
difficult to interpret or modify.
We build a learning-based, mention-synchronous
coreference system that aims to use the simplest pos-
sible set of features to tackle the various aspects
of coreference resolution. Though they arise from
a small number of simple templates, our features
are numerous, which works to our advantage: we
can both implicitly model important linguistic ef-
fects and capture other patterns in the data that are
not easily teased out by hand. As a result, our data-
driven, homogeneous feature set is able to achieve
high performance despite only using surface-level
document characteristics and shallow syntactic in-
formation. We win ?easy victories? without design-
ing features and heuristics explicitly targeting par-
ticular phenomena.
Though our approach is successful at modeling
syntax, we find semantics to be a much more chal-
lenging aspect of coreference. Our base system
uses only two recall-oriented features on nominal
and proper mentions: head match and exact string
match. Building on these features, we critically eval-
uate several classes of semantic features which intu-
1971
itively should prove useful but have had mixed re-
sults in the literature, and we observe that they are
ineffective for our system. However, these features
are beneficial when gold mentions are provided to
our system, leading us to conclude that the large
number of system mentions extracted by most coref-
erence systems (Lee et al, 2011; Fernandes et al,
2012) means that weak indicators cannot overcome
the bias against making coreference links. Capturing
semantic information in this shallow way is an ?up-
hill battle? due to this structural property of corefer-
ence resolution.
Nevertheless, using a simple architecture and fea-
ture set, our final system outperforms the two best
publicly available English coreference systems, the
Stanford system (Lee et al, 2011) and the IMS sys-
tem (Bjo?rkelund and Farkas, 2012), by wide mar-
gins: 3.5% absolute and 1.9% absolute, respectively,
on the CoNLL metric.
2 Experimental Setup
Throughout this work, we use the datasets from the
CoNLL 2011 shared task2 (Pradhan et al, 2011),
which is derived from the OntoNotes corpus (Hovy
et al, 2006). When applicable, we use the standard
automatic parses and NER tags for each document.
All experiments use system mentions except where
otherwise indicated. For each experiment, we report
MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,
1998), and CEAFe (Luo, 2005), as well as their av-
erage, the CoNLL metric. All metrics are computed
using version 5 of the official CoNLL scorer.3
3 A Mention-Synchronous Framework
We first present the basic architecture of our corefer-
ence system, independent of a feature set. Unlike bi-
nary classification-based coreference systems where
independent binary decisions are made about each
pair (Soon et al, 2001; Bengtson and Roth, 2008;
Versley et al, 2008; Stoyanov et al, 2010), we use a
log-linear model to select at most one antecedent for
2This dataset is identical to the English portion of the
CoNLL 2012 data, except for the absence of a small pivot text.
3Note that this version of the scorer implements a modified
version ofB3, described in Cai and Strube (2010), that was used
for the CoNLL shared tasks. The implementation of CEAFe
is also not exactly as described in Luo et al (2004), but for
completeness we include this metric as well.
each mention or determine that it begins a new clus-
ter (Denis and Baldridge, 2008). In this mention-
ranking or mention-synchronous framework, fea-
tures examine single mentions to evaluate whether
or not they are anaphoric and pairs of mentions to
evaluate whether or not they corefer. While other
work has used this framework as a starting point
for entity-level systems (Luo et al, 2004; Rahman
and Ng, 2009; Haghighi and Klein, 2010; Durrett et
al., 2013), we will show that a mention-synchronous
approach is sufficient to get state-of-the-art perfor-
mance on its own.
3.1 Mention Detection
Our system first identifies a set of predicted men-
tions from text annotated with parses and named en-
tity tags. We extract three distinct types of mentions:
proper mentions from all named entity chunks ex-
cept for those labeled as QUANTITY, CARDINAL, or
PERCENT, pronominal mentions from single words
tagged with PRP or PRP$, and nominal mentions
from all other maximal NP projections. These basic
rules are similar to those of Lee et al (2011), except
that their system uses an additional set of filtering
rules designed to discard instances of pleonastic it,
partitives, certain quantified noun phrases, and other
spurious mentions. In contrast to this highly engi-
neered approach and to systems which use a trained
classifier to compute anaphoricity separately (Rah-
man and Ng, 2009; Bjo?rkelund and Farkas, 2012),
we aim for the highest possible recall of gold men-
tions with a low-complexity method, leaving us with
a large number of spurious system mentions that we
will have to reject later.
3.2 Coreference Model
Figure 1 shows the mention-ranking architecture
that serves as the backbone of our coreference sys-
tem. Assume we have extracted n mentions from
a document x, where x denotes the surface proper-
ties of a document and any precomputed informa-
tion. The ith mention in a document has an asso-
ciated random variable ai taking values in the set
{1, . . . , i?1, NEW}; this variable specifies mention
i?s selected antecedent or indicates that it begins a
new coreference chain. A setting of the ai, denoted
by a = (a1, ..., an), implies a unique set of corefer-
ence chains C that serve as our system output.
1972
[Voters]
1
 agree when [they]
1
 are given a [chance]
2
 to decide if [they]
1
 ...
NEW
False New
Correct
1?
NEW
2?
3?
1?
Correct
a
1
NEW NEW
1?
2?
False Anaphor
False Anaphor
Correct
Wrong Link
False New
Correct
Correct
a
4
a
3
a
2
Figure 1: The basic structure of our coreference model. The ith mention in a document has i possible antecedence
choices: link to one of the i? 1 preceding mentions or begin a new cluster. We place a distribution over these choices
with a log-linear model. Structurally different kinds of errors are weighted differently to optimize for final coreference
loss functions; error types are shown corresponding to the decisions for each mention.
We use a log linear model of the conditional dis-
tribution P (a|x) as follows:
P (a|x) ? exp
(
n?
i=1
w>f(i, ai, x)
)
where f(i, ai, x) is a feature function that examines
the coreference decision ai for mention i with doc-
ument context x. When ai = NEW, the features
fired indicate the suitability of the given mention to
be anaphoric or not; when ai = j for some j, the
features express aspects of the pairwise linkage, and
can examine any relevant attributes of the anaphor
i or the antecedent j, since information about each
mention is contained in x.
Inference in this model is efficient: because
logP (a|x) decomposes linearly over mentions, we
can compute ai = argmaxai P (ai|x) separately
for each mention and return the set of coreference
chains implied by these decisions.
3.3 Learning
During learning, we optimize for conditional log-
likelihood augmented with a parameterized loss
function (Durrett et al, 2013). The main compli-
cating factor in this process is that the supervision
in coreference consists of a gold clustering C? de-
fined over gold mentions. This is problematic for
two reasons: first, because the clustering is defined
over gold mentions rather than our system mentions,
and second, because a clustering does not specify a
full antecedent structure of the sort our model pro-
duces. We can address the first of these problems
by imputing singleton clusters for mentions that do
not appear in the gold standard; our system will then
simply learn to put spurious mentions in their own
clusters. Singletons are always removed before eval-
uation because the OntoNotes corpus does not anno-
tate them, so in this way we can neatly dispose of
spurious mentions. To address the lack of explicit
antecedents in C?, we simply sum over all possible
antecedent structures licensed by the gold clusters.
Formally, we will maximize the conditional log-
likelihood of the set A(C?) of antecedent vectors
a for a document that are consistent with the gold
annotation.4 Consistency for an antecedent choice
ai under gold clusters C? is defined as follows:
1. If ai = j, ai is consistent iff mentions i and j
are present in C? and are in the same cluster.
2. If ai = NEW, ai is consistent off mention i is
not present in C?, or it is present in C? and has
no gold antecedents, or it is present in C? and
none of its gold antecedents are among the set
of system predicted mentions.
Given t training examples of the form (xk, C?k),
we write the following likelihood function:
`(w) =
t?
k=1
log
?
?
?
a?A(C?k)
P ?(a|xk)
?
?+ ??w?1
where P ?(a|xk) ? P (a|xk) exp(l(a,C?k)) with
l(a,C?) being a real-valued loss function. The loss
4Because of this marginalization over latent antecedent
choices, our objective is non-convex.
1973
here plays an analogous role to the loss in struc-
tured max-margin objectives; incorporating it into a
conditional likelihood objective is a technique called
softmax-margin (Gimpel and Smith, 2010).
Our loss function l(a,C?) is a weighted linear
combination of three error types, examples of which
are shown in Figure 1. A false anaphor (FA) error
occurs when ai is chosen to be anaphoric when it
should start a new cluster. A false new (FN) error oc-
curs in the opposite case, when ai wrongly indicates
a new cluster when it should be anaphoric. Finally,
a wrong link (WL) error occurs when the antecedent
chosen for ai is the wrong antecedent (but ai is in-
deed anaphoric). Our final parameterized loss func-
tion is a weighted sum of the counts of these three
error types:
l(a,C?) = ?FAFA(a,C?) + ?FNFN(a,C?) + ?WLWL(a,C?)
where FA(a,C?) gives the number of false anaphor
errors in prediction a with gold chains C? (FN and
WL are analogous). By setting ?FA low and ?FN
high relative to ?WL, we can counterbalance the
high number of singleton mentions and bias the sys-
tem towards making more coreference linkages. We
set (?FA, ?FN, ?WL) = (0.1, 3.0, 1.0) and ? =
0.001 and optimize the objective using AdaGrad
(Duchi et al, 2011).
4 Easy Victories from Surface Features
Our primary goal with this work is to show that a
high-performance coreference system is attainable
with a small number of feature templates that use
only surface-level information sources. These fea-
tures will be general-purpose and capture linguistic
effects to the point where standard heuristic-driven
features are no longer needed in our system.
4.1 SURFACE Features and Conjunctions
Our SURFACE feature set only considers the follow-
ing properties of mentions and mention pairs:
? Mention type (nominal, proper, or pronominal)
? The complete string of a mention
? The semantic head of a mention
? The first word and last word of each mention
Feature name Count
Features on the current mention
[ANAPHORIC] + [HEAD WORD] 41371
[ANAPHORIC] + [FIRST WORD] 18991
[ANAPHORIC] + [LAST WORD] 19184
[ANAPHORIC] + [PRECEDING WORD] 54605
[ANAPHORIC] + [FOLLOWING WORD] 57239
[ANAPHORIC] + [LENGTH] 4304
Features on the antecedent
[ANTECEDENT HEAD WORD] 57383
[ANTECEDENT FIRST WORD] 24239
[ANTECEDENT LAST WORD] 23819
[ANTECEDENT PRECEDING WORD] 53421
[ANTECEDENT FOLLOWING WORD] 55718
[ANTECEDENT LENGTH] 4620
Features on the pair
[EXACT STRING MATCH (T/F)] 47
[HEAD MATCH (T/F)] 46
[SENTENCE DISTANCE, CAPPED AT 10] 2037
[MENTION DISTANCE, CAPPED AT 10] 1680
Table 1: Our SURFACE feature set, which exploits a
small number of surface-level mention properties. Fea-
ture counts for each template are computed over the train-
ing set, and include features generated by our conjunction
scheme (not explicitly shown in the table; see Figure 2),
which yields large numbers of features at varying levels
of expressivity.
? The word immediately preceding and the word
immediately following a mention
? Mention length, in words
? Two distance measures between mentions
(number of sentences and number of mentions)
Table 1 shows the SURFACE feature set. Features
that look only at the current mention fire on all de-
cisions (ai = j or ai = NEW), whereas features
that look at the antecedent in any way (the latter
two groups of features) only fire on pairwise link-
ages (ai 6= NEW).
Two conjunctions of each feature are also in-
cluded: first with the ?type? of the mention be-
ing resolved (either NOMINAL, PROPER, or, if it is
pronominal, the citation form of the pronoun), and
then additionally with the antecedent type (only if
the feature is over a pairwise link). This conjunc-
tion process is shown in Figure 2. Note that features
that just examine the antecedent will end up with
1974
[Voters]
1
 generally agree when [they]
1 
...
NEW
1?
a
2
NEW ? LEN = 1
NEW ? LEN = 1 ? [they]
ANT. HEAD = Voters
ANT. HEAD = Voters ? [they]
ANT. HEAD = Voters ? [they] ? NOM
MENT DIST = 1
MENT DIST = 1 ? [they]
MENT DIST = 1 ? [they] ? NOM
Figure 2: Demonstration of the conjunction scheme we
use. Each feature on anaphoricity is conjoined with the
type (NOMINAL, PROPER, or the citation form if it is a
pronoun) of the mention being resolved. Each feature on
a mention pair is additionally conjoined with the types of
the current and antecedent mentions.
conjunctions that examine properties of the current
mention as well, as shown with the ANT. HEAD fea-
ture in the figure.
Finally, we found it beneficial for our lexical indi-
cator features to only fire on words occurring at least
20 times in the training set; for rare words, we use
the part of speech of the word instead.
The performance of our system is shown in Ta-
ble 2. We contrast our performance with that of
the Stanford system (Lee et al (2011), the winner
of the CoNLL 2011 shared task) and the IMS sys-
tem (Bjo?rkelund and Farkas (2012), the best publicly
available English coreference system). Despite its
simplicity, our SURFACE system is sufficient to out-
perform these sophisticated systems: the Stanford
system uses a cascade of ten rule-based sieves each
of which has customized heuristics, and the IMS
system uses a similarly long pipeline consisting of
a learned referentiality classifier followed by multi-
ple resolvers, which are run in sequence and rely on
the outputs of the previous resolvers as features.
4.2 Data-Driven versus Heuristic-Driven
Features
Why are the SURFACE features sufficient to give
high coreference performance, when they do not
make apparent reference to important linguistic phe-
nomena? The main reason is that they actually do
capture the same phenomena as standard corefer-
MUC B3 CEAFe Avg.
STANFORD 60.46 65.48 47.07 57.67
IMS 62.15 65.57 46.66 58.13
SURFACE 64.39 66.78 49.00 60.06
Table 2: Results for our SURFACE system, the STAN-
FORD system, and the IMS system on the CoNLL 2011
development set. Complete results are shown in Ta-
ble 7. Despite using limited information sources, our sys-
tem is able to substantially outperform the other two, the
two best publicly-available English coreference systems.
Bolded values are significant with p < 0.05 according to
a bootstrap resampling test.
ence features, just implicitly. For example, rather
than having rules targeting person, number, gender,
or animacy of mentions, we use conjunctions with
pronoun identity, which contains this information.
Rather than explicitly writing a feature targeting def-
initeness, our indicators on the first word of a men-
tion will capture this and other effects. And finally,
rather than targeting centering theory (Grosz et al,
1995) with rule-based features identifying syntac-
tic positions (Stoyanov et al, 2010; Haghighi and
Klein, 2010), our features on word context can iden-
tify configurational clues like whether a mention
is preceded or followed by a verb, and therefore
whether it is likely in subject or object position.5
Not only are data-driven features able to capture
the same phenomena as heuristic-driven features,
but they do so at a finer level of granularity, and can
therefore model more patterns in the data. To con-
trast these two types of features, we experiment with
three ablated versions of our system, where we re-
place data-driven features with their heuristic-driven
counterparts:
1. Instead of using an indicator on the first word
of a mention (1STWORD), we instead fire
a feature based on that mention?s manually-
computed definiteness (DEF).
2. Instead of conjoining features on pronominal-
pronominal linkages with the citation form of
5Heuristic-driven approaches were historically more appro-
priate, since past coreference corpora such as MUC and ACE
were smaller and therefore more prone to overfitting feature-
rich models. However, the OntoNotes dataset contains thou-
sands of documents, so having support for features is less of a
concern.
1975
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
?1STWORD 63.32 66.22 47.89 59.14
+DEF?1STWORD 63.79 66.46 48.35 59.53
?PRONCONJ 59.97 63.46 47.94 57.12
+AGR?PRONCONJ 63.54 66.10 48.72 59.45
?CONTEXT 60.88 64.66 47.60 57.71
+POSN?CONTEXT 62.45 65.44 48.08 58.65
+DEF+AGR+POSN 64.55 66.93 48.94 60.14
Table 3: CoNLL metric scores on the development set,
for the three different ablations and replacement features
described in Section 4.2. Feature types are described in
the text; + indicates inclusion of that feature class, ? in-
dicates exclusion. Each individual shallow indicator ap-
pears to do as well at capturing its target phenomenon as
the hand-engineered features, while capturing other infor-
mation as well. Moreover, the hand-engineered features
give no benefit over the SURFACE system.
each pronoun (PRONCONJ), we only conjoin
with a PRONOUN indicator and add features
targeting the person, number, gender, and an-
imacy of the two pronouns (AGR).
3. Instead of using our context features on the
preceding and following word (CONTEXT), we
use manual determinations of when mentions
are in subject, direct object, indirect objection,
or oblique position (POSN).
All rules for computing person, number, gender, an-
imacy, definiteness, and syntactic position are taken
from the system of Lee et al (2011).
Table 3 shows each of the target ablations, as well
as the SURFACE system with the DEF, AGR, and
POSN features added. While the heuristic-driven
feature always help over the corresponding ablated
system, they cannot do the work of the fine-grained
data-driven features. Most tellingly, though, none of
the heuristic-driven features give statistically signifi-
cant improvements on top of the data-driven features
we have already included, indicating that we are at
the point of diminishing returns on modeling those
specific phenomena. While this does not preclude
further engineering to take better advantage of other
syntactic constraints, our simple features represent
an ?easy victory? on this subtask.
5 Uphill Battles on Semantics
In Section 4, we gave a simple set of features that
yielded a high-performance coreference system; this
high performance is possible because features tar-
geting only superficial properties in a fine-grained
way can actually model complex linguistic con-
straints. However, while our existing features cap-
ture syntactic and discourse-level phenomena sur-
prisingly well, they are not effective at capturing se-
mantic phenomena like type compatibility. We will
show that due to structural aspects of the coreference
resolution problem, even a combination of several
shallow semantic features from the literature fails to
adequately model semantics.
5.1 Analysis of the SURFACE System
What can the SURFACE system resolve correctly,
and what errors does it still make? To answer this
question, we will split mentions into several cate-
gories based on their observable properties and the
gold standard coreference information, and exam-
ine our system?s accuracy on each mention subclass
in order to more thoroughly characterize its perfor-
mance.6 These categories represent important dis-
tinctions in terms of the difficulty of mention reso-
lution for our system.
We first split mentions into three categories by
their status in the gold standard: singleton (unanno-
tated in the OntoNotes corpus), starting a new entity
with at least two mentions, or anaphoric. It is impor-
tant to note that while singletons and mentions start-
ing new entities are outwardly similar in that they
have no antecedents, and the prediction should be
the same in either case (NEW), we treat them as dis-
tinct because the factors that impact the coreference
decision differ in the two cases. Mentions that start
new clusters are semantically similar to anaphoric
mentions, but may be marked by heaviness or by a
tendency to be named entities, whereas singletons
may be generic or temporal NPs which might be
thought of as coreferent in a loose sense, but are not
6This method of analysis is similar to that undertaken in
Stoyanov et al (2009) and Rahman and Ng (2011b), though
we split our mentions along different axes, and can simply eval-
uate on accuracy because our decisions do not directly imply
multiple links, as they do in binary classification-based systems
(Stoyanov et al, 2009) or in entity-mention models (Rahman
and Ng, 2011b).
1976
Nominal/Proper
Pronominal
1st w/head 2nd+ w/head
Singleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7K
Starts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3K
Anaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4K
Table 4: Analysis of our SURFACE system on the de-
velopment set. We characterize each predicted mention
by its status in the gold standard (singleton, starting a
new entity, or anaphoric), its type (pronominal or nom-
inal/proper), and by whether its head has appeared as the
head of a previous mention. Each cell shows our sys-
tem?s accuracy on that mention class as well as the size
of the class. The biggest weakness of our system appears
to be its inability to resolve anaphoric mentions with new
heads (bottom-left cell).
included in the OntoNotes dataset due to choices in
the annotation standard.
Second, we divide mentions by their type,
pronominal versus nominal/proper; we then further
subdivide nominals and propers based on whether or
not the head word of the mention has appeared as the
head of a previous mention in the document.
Table 4 shows the results of our analysis. In
each cell, we show the fraction of mentions that
we correctly resolve (i.e., for which we make an
antecedence decision consistent with the gold stan-
dard), as well as the total number of mentions falling
into that cell. First, we observe that there are a sur-
prisingly large number of singleton mentions with
misleading head matches to previous mentions (of-
ten recurring temporal nouns phrases, like July).
The features in our system targeting anaphoricity are
useful for exactly this reason: the more bad head
matches we can rule out based on other criteria, the
more strongly we can rely on head match to make
correct linkages.
Our system is most noticeably poor at resolving
anaphoric mentions whose heads have not appeared
before. The fact that exact match and head match
are our only recall-oriented features on nominals
and propers is starkly apparent here: when we can-
not rely on head match, as is true for this mention
class, we only resolve 7.9% of anaphoric mentions
correctly.7 Many of the mentions in this category
7There are an additional 346 anaphoric nominal/proper men-
tions in the 2nd+ category whose heads only appeared previ-
ously as part of a different cluster; we only resolve 1.7% of
can only be correctly resolved by exploiting world
knowledge, so we will need to include features that
capture this knowledge in some fashion.
5.2 Incorporating Shallow Semantics
As we were able to incorporate syntax with shal-
low features, so too might we hope to incorporate
semantics. However, the semantic information con-
tained even in a coreference corpus of thousands
of documents is insufficient to generalize to unseen
data,8 so system designers have turned to exter-
nal resources such as semantic classes derived from
WordNet (Soon et al, 2001), WordNet hypernymy
or synonymy (Stoyanov et al, 2010), semantic simi-
larity computed from online resources (Ponzetto and
Strube, 2006), named entity type features, gender
and number match using the dataset of Bergsma and
Lin (2006), and features from unsupervised clus-
ters (Hendrickx and Daelemans, 2007; Durrett et al,
2013). In this section, we consider the following
subset of these information sources:
? WordNet hypernymy and synonymy
? Number and gender data for nominals and
propers from Bergsma and Lin (2006)
? Named entity types
? Latent clusters computed from English Giga-
word (Graff et al, 2007), where a latent cluster
label generates each nominal head (excluding
pronouns) and a conjunction of its verbal gov-
ernor and semantic role, if any (Durrett et al,
2013). We use twenty clusters, which include
clusters like president and leader (things which
announce).
Together, we call these the SEM features. We
show results from this expansion of the feature set in
Table 5. When using system mentions, the improve-
ments are not statistically significant on every met-
ric, and are quite marginal given that these features
add information that is intuitively central to corefer-
ence and otherwise unavailable to the system. We
explore the reasons behind this in the next section.
these extremely tricky cases correctly.
8We experimented with bilexical features on head pairs, but
they did not give statistically significant improvements over the
SURFACE features.
1977
MUC B3 CEAFe Avg.
SURFACE 64.39 66.78 49.00 60.06
SURFACE+SEM 64.70 67.27 49.28 60.42
SURFACE (G) 82.80 74.10 68.33 75.08
SURFACE+SEM (G) 84.49 75.65 69.89 76.68
Table 5: CoNLL metric scores on the development set
for our SEM features when added on top of our SURFACE
features. We experiment on both system mentions and
gold mentions. Surprisingly, despite the fact that absolute
performance numbers are much higher on gold mentions
and there is less room for improvement, the semantic fea-
tures help much more than they do on system mentions.
5.3 Analysis of Semantic Features
The main reason that weak semantic cues are not
more effective is the small fraction of positive coref-
erence links present in the training data. From Ta-
ble 4, the number of annotated coreferent spans in
the OntoNotes data is about a factor of five smaller
than the number of system mentions.9 This both
means that most NPs are not coreferent, and for
those that are, choosing the correct links is much
more difficult because of the large number of pos-
sible antecedents. Even head match, which is gen-
erally considered a high-precision indicator (Lee et
al., 2011), would introduce many spurious corefer-
ence arcs if applied too liberally (see Table 4).
In light of this fact, a system needs very strong
evidence to overcome the default hypothesis that a
mention is not coreferent, and a weak indicator will
have such a high ?false positive? rate that it cannot
be relied on (given high weight, this feature would
do more harm than good, by introducing many false
linkages).
To confirm this intuition, we show in the bot-
tom part of Table 5 results when we apply these se-
mantic features on top of our SURFACE system on
gold mentions, where there are no singletons. In the
gold mention setting, we see that the semantic fea-
tures give a consistent improvement on every metric.
Moreover, if we look at a breakdown of errors, the
main improvement the semantic features give us is
on resolution of anaphoric nominals with no head
9This observation is more general than just our system: the
majority of coreference systems, including the winners of the
CoNLL shared tasks (Lee et al, 2011; Fernandes et al, 2012),
opt for high mention recall and resolve a relatively large number
of system mentions.
match: accuracy on the 1601 mentions that fall into
this category improves from 28.0% to 37.9%. On
predicted mentions, by contrast, this category only
improves from 7.9% to 12.2%, a much smaller ab-
solute improvement and one that comes at the ex-
pense of performance on most other resolution class.
The one class that does not get worse, singleton pro-
nouns, actually improves by a similar 4% margin,
indicating that roughly half of the gains we observe
are not even necessarily a result of our features do-
ing what they were designed to do.
Our weak cues do yield some small gains, so there
is hope that better weak indicators of semantic com-
patibility could prove more useful. However, while
extremely high-precision approaches with carefully
engineered features have been shown to be suc-
cessful (Rahman and Ng, 2011a; Bansal and Klein,
2012; Recasens et al, 2013a), we conclude that cap-
turing semantics in a data-driven, shallow manner
remains an uphill battle.
6 FINAL System and Results
While semantic features ended up giving only
marginal benefit, we have demonstrated that nev-
ertheless our SURFACE system is a state-of-the-art
English coreference system. However, there remain
a few natural features that we omitted in order to
keep the system as simple as possible, since they
were orthogonal to the discussion of data-driven
versus heuristic-driven features and do not target
world knowledge. Before giving final results, we
will present a small set of additional features that
consider four additional mention properties beyond
those in Section 4.1:
? Whether two mentions are nested
? Ancestry of each mention head: the depen-
dency parent and grandparent POS tags and arc
directions (shown in Figure 3)
? The speaker of each mention
? Number and gender of each mention as deter-
mined by Bergsma and Lin (2006)
The specific additional features we use are shown
in Table 6. Note that unlike in Section 5, we use
the number and gender information only on the an-
tecedent. Due to our conjunction scheme, both this
1978
ROOT
... sent    it    to  the [president] ... [President Obama] signed ...
VBD PRP TO 
DET NN NNP NNP VBD 
president 
R
TO 
VBD 
R
Obama 
L
VBD 
ROOT 
Figure 3: Demonstration of the ancestry extraction pro-
cess. These features capture more sophisticated configu-
rational information than our context word features do: in
this example, president is in a characteristic indirect ob-
ject position based on its dependency parents, and Obama
is the subject of the main verb of the sentence.
semantic information and the speaker information
can apply in a fine-grained way to different pro-
nouns, and can therefore improve pronoun resolu-
tion substantially; however, these features generally
only improve pronoun resolution.
Full results for our SURFACE and FINAL feature
sets are shown in Table 7. Again, we compare to Lee
et al (2011) and Bjo?rkelund and Farkas (2012).10
Despite our system?s emphasis on one-pass resolu-
tion with as simple a feature set as possible, we are
able to outperform even these sophisticated systems
by a wide margin.
7 Related Work
Many of the individual features we employ in the FI-
NAL feature set have appeared in other coreference
systems (Bjo?rkelund and Nugues, 2011; Rahman
and Ng, 2011b; Fernandes et al, 2012). However,
other authors have often emphasized bilexical fea-
tures on head pairs, whereas our features are heavily
monolexical. For feature conjunctions, other authors
have exploited three classes (Lee et al, 2011) or au-
tomatically learned conjunction schemes (Fernandes
et al, 2012; Lassalle and Denis, 2013), but to our
knowledge we are the first to do fine-grained mod-
eling of every pronoun. Inclusion of a hierarchy of
10Discrepancies between scores here and those printed in
Pradhan et al (2012) arise from two sources: improvements
to the system of Lee et al (2011) since the first CoNLL shared
task, and a fix to the scoring of B3 in the official scorer since
results of the two CoNLL shared tasks were released. Unfor-
tunately, because of this bug in the scoring program, direct
comparison to the printed results of the other highest-scoring
English systems, Fernandes et al (2012) and Martschat et al
(2012), is impossible.
Feature name Count
Features of the SURFACE system 418704
Features on the current mention
[ANAPHORIC] + [CURRENT ANCESTRY] 46047
Features on the antecedent
[ANTECEDENT ANCESTRY] 53874
[ANTECEDENT GENDER] 338
[ANTECEDENT NUMBER] 290
Features on the pair
[HEAD CONTAINED (T/F)] 136
[EXACT STRING CONTAINED (T/F)] 133
[NESTED (T/F)] 355
[DOC TYPE] + [SAME SPEAKER (T/F)] 437
[CURRENT ANCESTRY] + [ANT. ANCESTRY] 2555359
Table 6: FINAL feature set; note that this includes the
SURFACE feature set. As with the features of the SUR-
FACE system, two conjoined variants of each feature
are included: first with the type of the current mention
(NOMINAL, PROPER, or the citation form of the pro-
noun), then with the types of both mentions in the pair.
These conjunctions allow antecedent features on gender
and number to impact pronoun resolution, and they al-
low speaker match to capture effects like I and you being
coreferent when the speakers differ.
features with regularization also means that we or-
ganically get distinctions among different mention
types without having to choose a level of granularity
a priori, unlike the distinct classifiers employed by
Denis and Baldridge (2008).
In terms of architecture, many coreference sys-
tems operate in a pipelined fashion, making par-
tial decisions about coreference or pruning arcs
before full resolution. Some systems use sepa-
rate rule-based and learning-based passes (Chen
and Ng, 2012; Fernandes et al, 2012), a series
of learning-based passes (Bjo?rkelund and Farkas,
2012), or referentiality classifiers that prune the set
of mentions before resolution (Rahman and Ng,
2009; Bjo?rkelund and Farkas, 2012; Recasens et
al., 2013b). By contrast, our system resolves all
mentions in one pass and does not need pruning:
the SURFACE system can train in less than two
hours without any subsampling of coreference arcs,
and rule-based pruning of coreference arcs actually
causes our system to perform less well, since our
features can learn valuable information from these
negative examples.
1979
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
CoNLL 2011 Development Set
STANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67
IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13
SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06
FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58
CoNLL 2011 Test Set
STANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65
IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26
FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13
Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results
of Lee et al (2011) (STANFORD) and Bjo?rkelund and Farkas (2012) (IMS). Starred systems are contributions of this
work. Bolded F1 values represent statistically significant improvements over other systems with p < 0.05 using a
bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer.
8 Conclusion
We have presented a coreference system that uses a
simple, homogeneous set of features in a discrim-
inative learning framework to achieve high perfor-
mance. Large numbers of lexicalized, data-driven
features implicitly model linguistic phenomena such
as definiteness and centering, obviating the need for
heuristic-driven rules explicitly targeting these same
phenomena. Additional semantic features give only
slight benefit beyond head match because they do
not provide strong enough signals of coreference to
improve performance in the system mention setting;
modeling semantic similarity still requires complex
outside information and deep heuristics.
Our system, the Berkeley Coreference
Resolution System, is publicly available at
http://nlp.cs.berkeley.edu.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014 and by
an NSF fellowship for the first author. Thanks
to Sameer Pradhan for helpful discussions regard-
ing the CoNLL scoring program, and thanks to the
anonymous reviewers for their insightful comments.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Seman-
tics from Web Features. In Proceedings of the Associ-
ation for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding the
Value of Features for Coreference Resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
Path-Based Pronoun Resolution. In Proceedings of the
Conference on Computational Linguistics and the As-
sociation for Computational Linguistics.
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-
driven Multilingual Coreference Resolution using Re-
solver Stacking. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
ceedings and Conference on Computational Natural
Language Learning - Shared Task.
Anders Bjo?rkelund and Pierre Nugues. 2011. Exploring
Lexicalized Features for Coreference Resolution. In
Proceedings of the Conference on Computational Nat-
ural Language Learning: Shared Task.
Jie Cai and Michael Strube. 2010. Evaluation Metrics for
End-to-End Coreference Resolution Systems. In Pro-
ceedings of the Special Interest Group on Discourse
and Dialogue.
Chen Chen and Vincent Ng. 2012. Combining the Best
of Two Worlds: A Hybrid Approach to Multilingual
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computational
Natural Language Learning - Shared Task.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
1980
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Greg Durrett, David Hall, and Dan Klein. 2013. Decen-
tralized Entity-Level Modeling for Coreference Reso-
lution. In Proceedings of the Association for Compu-
tational Linguistics.
Eraldo Rezende Fernandes, C??cero Nogueira dos Santos,
and Ruy Luiz Milidiu?. 2012. Latent Structure Per-
ceptron with Feature Induction for Unrestricted Coref-
erence Resolution. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Proceedings and Conference on Computational Nat-
ural Language Learning - Shared Task.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with Cost
Functions. In Proceedings of the North American
Chapter for the Association for Computational Lin-
guistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203?225, June.
Aria Haghighi and Dan Klein. 2009. Simple Coreference
Resolution with Rich Syntactic and Semantic Features.
In Proceedings of Empirical Methods in Natural Lan-
guage Processing.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding Se-
mantic Information: Unsupervised Clusters for Coref-
erence Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Short Papers.
Emmanuel Lassalle and Pascal Denis. 2013. Improving
Pairwise Coreference Models Through Feature Space
Hierarchy Learning. In Proceedings of the Association
for Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s Multi-Pass Sieve Coreference Resolution
System at the CoNLL-2011 Shared Task. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning: Shared Task.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Algo-
rithm Based on the Bell Tree. In Proceedings of the
Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
Multigraph Model for Coreference Resolution. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Proceedings and Con-
ference on Computational Natural Language Learning
- Shared Task.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Association
of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings of
the Conference on Computational Natural Language
Learning: Shared Task.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Altaf Rahman and Vincent Ng. 2011a. Coreference
Resolution with World Knowledge. In Proceedings
of the Association for Computational Linguistics: Hu-
man Language Technologies.
Altaf Rahman and Vincent Ng. 2011b. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intelli-
gence Research, 40(1):469?521, January.
Marta Recasens, Matthew Can, and Daniel Jurafsky.
2013a. Same Referent, Different Words: Unsuper-
vised Mining of Opaque Coreferent Mentions. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013b. The Life and Death of Dis-
course Entities: Identifying Singleton Mentions. In
1981
Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521?544, December.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in Noun Phrase
Coreference Resolution: Making Sense of the State-
of-the-Art. In Proceedings of the Association for Com-
putational Linguistics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence Resolution with Reconcile. In Proceedings of
the Association for Computational Linguistics: Short
Papers.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. BART:
A Modular Toolkit for Coreference Resolution. In
Proceedings of the Association for Computational Lin-
guistics: Demo Session.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Proceed-
ings of the Conference on Message Understanding.
1982
Learning Dependency-Based
Compositional Semantics
Percy Liang?
University of California, Berkeley
Michael I. Jordan??
University of California, Berkeley
Dan Klein?
University of California, Berkeley
Suppose we want to build a system that answers a natural language question by representing its
semantics as a logical form and computing the answer given a structured database of facts. The
core part of such a system is the semantic parser that maps questions to logical forms. Semantic
parsers are typically trained from examples of questions annotated with their target logical forms,
but this type of annotation is expensive.
Our goal is to instead learn a semantic parser from question?answer pairs, where the logical
form is modeled as a latent variable. We develop a new semantic formalism, dependency-based
compositional semantics (DCS) and define a log-linear distribution over DCS logical forms. The
model parameters are estimated using a simple procedure that alternates between beam search
and numerical optimization. On two standard semantic parsing benchmarks, we show that our
system obtains comparable accuracies to even state-of-the-art systems that do require annotated
logical forms.
No rights reserved. This work was authored as part of the Contributor?s official duties as an Employee of
the United States Government and is therefore a work of the United States Government. In accordance with
17 U.S.C. 105, no copyright protection is available for such works under U.S. law.
1. Introduction
One of the major challenges in natural language processing (NLP) is building systems
that both handle complex linguistic phenomena and require minimal human effort. The
difficulty of achieving both criteria is particularly evident in training semantic parsers,
where annotating linguistic expressions with their associated logical forms is expensive
but until recently, seemingly unavoidable. Advances in learning latent-variable models,
however, have made it possible to progressively reduce the amount of supervision
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: pliang@cs.stanford.edu.
?? Computer Science Division and Department of Statistics, University of California, Berkeley, CA 94720,
USA. E-mail: jordan@cs.berkeley.edu.
? Computer Science Division, University of California, Berkeley, CA 94720, USA.
E-mail: klein@cs.berkeley.edu.
Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for
publication: 18 April 2012.
doi:10.1162/COLI a 00127
Computational Linguistics Volume 39, Number 2
required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan
et al 2009; Liang, Jordan, and Klein 2009; Clarke et al 2010; Artzi and Zettlemoyer 2011;
Goldwasser et al 2011). In this article, we develop new techniques to learn accurate
semantic parsers from even weaker supervision.
We demonstrate our techniques on the concrete task of building a system to answer
questions given a structured database of facts; see Figure 1 for an example in the domain
of U.S. geography. This problem of building natural language interfaces to databases
(NLIDBs) has a long history in NLP, starting from the early days of artificial intelligence
with systems such as LUNAR (Woods, Kaplan, and Webber 1972), CHAT-80 (Warren
and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995]
for an overview). We believe NLIDBs provide an appropriate starting point for semantic
parsing because they lead directly to practical systems, and they allow us to temporarily
sidestep intractable philosophical questions on how to represent meaning in general.
Early NLIDBs were quite successful in their respective limited domains, but because
these systems were constructed frommanually built rules, they became difficult to scale
up, both to other domains and to more complex utterances. In response, against the
backdrop of a statistical revolution in NLP during the 1990s, researchers began to build
systems that could learn from examples, with the hope of overcoming the limitations of
rule-based methods. One of the earliest statistical efforts was the CHILL system (Zelle
and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has
been a healthy line of work yielding increasingly more accurate semantic parsers by
using new semantic representations andmachine learning techniques (Miller et al 1996;
Zelle and Mooney 1996; Tang andMooney 2001; Ge andMooney 2005; Kate, Wong, and
Mooney 2005; Zettlemoyer and Collins 2005; Kate andMooney 2006;Wong andMooney
2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007;
Kwiatkowski et al 2010, 2011).
Although statistical methods provided advantages such as robustness and portabil-
ity, however, their application in semantic parsing achieved only limited success. One
of the main obstacles was that these methods depended crucially on having examples
of utterances paired with logical forms, and this requires substantial human effort to
obtain. Furthermore, the annotators must be proficient in some formal language, which
drastically reduces the size of the annotator pool, dampening any hope of acquiring
enough data to fulfill the vision of learning highly accurate systems.
In response to these concerns, researchers have recently begun to explore the pos-
sibility of learning a semantic parser without any annotated logical forms (Clarke et al
Figure 1
The concrete objective: A system that answers natural language questions given a structured
database of facts. An example is shown in the domain of U.S. geography.
390
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 2
Our statistical methodology consists of two steps: (i) semantic parsing (p(z | x;?)): an utterance x
is mapped to a logical form z by drawing from a log-linear distribution parametrized by a
vector ?; and (ii) evaluation ([[z]]w): the logical form z is evaluated with respect to the world w
(database of facts) to deterministically produce an answer y. The figure also shows an example
configuration of the variables around the graphical model. Logical forms z are represented as
labeled trees. During learning, we are given w and (x, y) pairs (shaded nodes) and try to infer
the latent logical forms z and parameters ?.
2010; Artzi and Zettlemoyer 2011; Goldwasser et al 2011; Liang, Jordan, andKlein 2011).
It is in this vein that we develop our present work. Specifically, given a set of (x, y)
example pairs, where x is an utterance (e.g., a question) and y is the corresponding
answer, we wish to learn a mapping from x to y. What makes this mapping particularly
interesting is that it passes through a latent logical form z, which is necessary to capture
the semantic complexities of natural language. Also note that whereas the logical form
z was the end goal in much of earlier work on semantic parsing, for us it is just an
intermediate variable?a means towards an end. Figure 2 shows the graphical model
which captures the learning setting we just described: The question x, answer y, and
world/database w are all observed. We want to infer the logical forms z and the
parameters ? of the semantic parser, which are unknown quantities.
Although liberating ourselves from annotated logical forms reduces cost, it does
increase the difficulty of the learning problem. The core challenge here is program
induction: On each example (x, y), we need to efficiently search over the exponential
space of possible logical forms (programs) z and find ones that produce the target
answer y, a computationally daunting task. There is also a statistical challenge: How
do we parametrize the mapping from utterance x to logical form z so that it can be
learned from only the indirect signal y? To address these two challenges, we must first
discuss the issue of semantic representation. There are two basic questions here: (i) what
391
Computational Linguistics Volume 39, Number 2
should the formal language for the logical forms z be, and (ii) what are the compositional
mechanisms for constructing those logical forms?
The semantic parsing literature has considered many different formal languages
for representing logical forms, including SQL (Giordani and Moschitti 2009), Prolog
(Zelle and Mooney 1996; Tang and Mooney 2001), a simple functional query language
called FunQL (Kate, Wong, and Mooney 2005), and lambda calculus (Zettlemoyer and
Collins 2005), just to name a few. The construction mechanisms are equally diverse, in-
cluding synchronous grammars (Wong and Mooney 2007), hybrid trees (Lu et al 2008),
Combinatory Categorial Grammars (CCG) (Zettlemoyer and Collins 2005), and shift-
reduce derivations (Zelle and Mooney 1996). It is worth pointing out that the choice of
formal language and the construction mechanism are decisions which are really more
orthogonal than is often assumed?the former is concerned with what the logical forms
look like; the latter, with how to generate a set of possible logical forms compositionally
given an utterance. (How to score these logical forms is yet another dimension.)
Existing systems are rarely based on the joint design of the formal language and
the construction mechanism; one or the other is often chosen for convenience from
existing implementations. For example, Prolog and SQL have often been chosen as
formal languages for convenience in end applications, but they were not designed
for representing the semantics of natural language, and, as a result, the construction
mechanism that bridges the gap between natural language and formal language is
generally complex and difficult to learn. CCG (Steedman 2000) is quite popular in
computational linguistics (for example, see Bos et al [2004] and Zettlemoyer and Collins
[2005]). In CCG, logical forms are constructed compositionally using a small handful
of combinators (function application, function composition, and type raising). For a
wide range of canonical examples, CCG produces elegant, streamlined analyses, but
its success really depends on having a good, clean lexicon. During learning, there is
often a great amount of uncertainty over the lexical entries, which makes CCG more
cumbersome. Furthermore, in real-world applications, we would like to handle disflu-
ent utterances, and this further strains CCG by demanding either extra type-raising
rules and disharmonic combinators (Zettlemoyer and Collins 2007) or a proliferation of
redundant lexical entries for each word (Kwiatkowski et al 2010).
To cope with the challenging demands of program induction, we break away from
tradition in favor of a new formal language and construction mechanism, which we call
dependency-based compositional semantics (DCS). The guiding principle behind DCS
is to provide a simple and intuitive framework for constructing and representing logical
forms. Logical forms in DCS are tree structures calledDCS trees. The motivation is two-
fold: (i) DCS trees are meant to parallel syntactic dependency trees, which facilitates
parsing; and (ii) a DCS tree essentially encodes a constraint satisfaction problem, which
can be solved efficiently using dynamic programming to obtain the denotation of a DCS
tree. In addition, DCS provides a mark?execute construct, which provides a uniform
way of dealing with scope variation, a major source of trouble in any semantic for-
malism. The construction mechanism in DCS is a generalization of labeled dependency
parsing, which leads to simple and natural algorithms. To a linguist, DCS might appear
unorthodox, but it is important to keep in mind that our primary goal is effective
program induction, not necessarily to model new linguistic phenomena in the tradition
of formal semantics.
Armed with our new semantic formalism, DCS, we then define a discriminative
probabilistic model, which is depicted in Figure 2. The semantic parser is a log-linear
distribution over DCS trees z given an utterance x. Notably, z is unobserved, and we in-
stead observe only the answer y, which is obtained by evaluating z on a world/database
392
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
w. There are an exponential number of possible trees z, and usually dynamic program-
ming can be used to efficiently search over trees. However, in our learning setting
(independent of the semantic formalism), we must enforce the global constraint that
z produces y. This makes dynamic programming infeasible, so we use beam search
(though dynamic programming is still used to compute the denotation of a fixed DCS
tree). We estimate the model parameters with a simple procedure that alternates be-
tween beam search and optimizing a likelihood objective restricted to those beams. This
yields a natural bootstrapping procedure in which learning and search are integrated.
We evaluated our DCS-based approach on two standard benchmarks, GEO, a U.S.
geography domain (Zelle and Mooney 1996), and JOBS, a job queries domain (Tang and
Mooney 2001). On GEO, we found that our system significantly outperforms previous
work that also learns from answers instead of logical forms (Clarke et al 2010). What
is perhaps a more significant result is that our system obtains comparable accuracies to
state-of-the-art systems that do rely on annotated logical forms. This demonstrates the
viability of training accurate systems with much less supervision than before.
The rest of this article is organized as follows: Section 2 introduces DCS, our new
semantic formalism. Section 3 presents our probabilistic model and learning algorithm.
Section 4 provides an empirical evaluation of our methods. Section 5 situates this work
in a broader context, and Section 6 concludes.
2. Representation
In this section, we present the main conceptual contribution of this work, dependency-
based compositional semantics (DCS), using the U.S. geography domain (Zelle and
Mooney 1996) as a running example. To do this, we need to define the syntax and
semantics of the formal language. The syntax is defined in Section 2.2 and is quite
straightforward: The logical forms in the formal language are simply trees, which we
callDCS trees. In Section 2.3, we give a type-theoretic definition ofworlds (also known
as databases or models) with respect to which we can define the semantics of DCS trees.
The semantics, which is the heart of this article, contains two main ideas: (i) using
trees to represent logical forms as constraint satisfaction problems or extensions thereof,
and (ii) dealing with cases when syntactic and semantic scope diverge (e.g., for general-
ized quantification and superlative constructions) using a new construct which we call
mark?execute. We start in Section 2.4 by introducing the semantics of a basic version
of DCS which focuses only on (i) and then extend it to the full version (Section 2.5) to
account for (ii).
Finally, having fully specified the formal language, we describe a construction
mechanism for mapping a natural language utterance to a set of candidate DCS trees
(Section 2.6).
2.1 Notation
Operations on tuples will play a prominent role in this article. For a sequence1 v =
(v1, . . . , vk), we use |v| = k to denote the length of the sequence. For two sequences u
and v, we use u+ v = (u1, . . . ,u|u|, v1, . . . , v|v|) to denote their concatenation.
1 We use the term sequence to refer to both tuples (v1, . . . , vk ) and arrays [v1, . . . , vk]. For our purposes, there
is no functional difference between tuples and arrays; the distinction is convenient when we start to talk
about arrays of tuples.
393
Computational Linguistics Volume 39, Number 2
For a sequence of positive indices i = (i1, . . . , im), let vi = (vi1 , . . . , vim ) consist of the
components of v specified by i; we call vi the projection of v onto i. We use negative
indices to exclude components: v?i = (v(1,...,|v|)\i). We can also combine sequences of
indices by concatenation: vi,j = vi + vj. Some examples: if v = (a, b, c, d), then v2 = b,
v3,1 = (c, a), v?3 = (a, b, d), v3,?3 = (c, a, b, d).
2.2 Syntax of DCS Trees
The syntax of the DCS formal language is built from two ingredients, predicates and
relations:
 Let P be a set of predicates. We assume that P contains a special null
predicate ?, domain-independent predicates (e.g., count, <, >, and =), and
domain-specific predicates (for the U.S. geography domain, state, river,
border, etc.). Right now, think of predicates as just labels, which have yet
to receive formal semantics.
 LetR be the set of relations. Note that unlike the predicates P , which can
vary across domains, the relationsR are fixed. The full set of relations are
shown in Table 1. For now, just think of relations as labels?their semantics
will be defined in Section 2.4.
The logical forms in DCS are called DCS trees. A DCS tree is a directed rooted tree
in which nodes are labeled with predicates and edges are labeled with relations; each
node also maintains an ordering over its children. Formally:
Definition 1 (DCS trees)
Let Z be the set of DCS trees, where each z ? Z consists of (i) a predicate z.p ? P and (ii)
a sequence of edges z.e = (z.e1, . . . , z.em). Each edge e consists of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We will either draw a DCS tree graphically or write it compactly as ?p; r1 :c1; . . . ; rm :cm?
where p is the predicate at the root node and c1, . . . , cm are its m children connected via
edges labeled with relations r1, . . . , rm, respectively. Figure 3(a) shows an example of a
DCS tree expressed using both graphical and compact formats.
Table 1
Possible relations that appear on edges of DCS trees. Basic DCS uses only the join and aggregate
relations; the full version of DCS uses all of them.
RelationsR
Name Relation Description of semantic function
join
j
j? for j, j
? ? {1, 2, . . . } j-th component of parent = j?-th component of child
aggregate ? parent = set of feasible values of child
extract E mark node for extraction
quantify Q mark node for quantification, negation
compare C mark node for superlatives, comparatives
execute Xi for i ? {1, 2 . . . }? process marked nodes specified by i
394
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 3
(a) An example of a DCS tree (written in both the mathematical and graphical notations). Each
node is labeled with a predicate, and each edge is labeled with a relation. (b) A DCS tree zwith
only join relations encodes a constraint satisfaction problem, represented here as a lambda
calculus formula. For example, the root node label city corresponds to a unary predicate
city(c), the right child node label loc corresponds to a binary predicate loc() (where  is a
pair), and the edge between them denotes the constraint c1 = 1 (where the indices correspond to
the two labels on the edge). (c) The denotation of z is the set of feasible values for the root node.
A DCS tree is a logical form, but it is designed to look like a syntactic dependency
tree, only with predicates in place of words. As we?ll see over the course of this section,
it is this transparency between syntax and semantics provided by DCS which leads to a
simple and streamlined compositional semantics suitable for program induction.
2.3 Worlds
In the context of question answering, the DCS tree is a formal specification of the
question. To obtain an answer, we still need to evaluate the DCS tree with respect to
a database of facts (see Figure 4 for an example). We will use the term world to refer
Figure 4
We use the domain of U.S. geography as a running example. The figure presents an example of a
world w (database) in this domain. A world maps each predicate to a set of tuples. For example,
the depicted world wmaps the predicate loc to the set of pairs of places and their containers.
Note that functions (e.g., population) are also represented as predicates for uniformity. Some
predicates (e.g., count) map to an infinite number of tuples and would be represented implicitly.
395
Computational Linguistics Volume 39, Number 2
to this database (it is sometimes also called a model, but we avoid this term to avoid
confusion with the probabilistic model for learning that we will present in Section 3.1).
Throughout this work, we assume the world is fully observed and fixed, which is a
realistic assumption for building natural language interfaces to existing databases, but
questionable for modeling the semantics of language in general.
2.3.1 Types and Values. To define a world, we start by constructing a set of values V .
The exact set of values depends on the domain (we will continue to use U.S. geog-
raphy as a running example). Briefly, V contains numbers (e.g., 3 ? V), strings (e.g.,
Washington ? V), tuples (e.g., (3,Washington) ? V), sets (e.g., {3,Washington} ? V), and
other higher-order entities.
To be more precise, we construct V recursively. First, define a set of primitive values
V, which includes the following:
 Numeric values. Each value has the form x : t ? V, where x ? R is a real
number and t ? {number, ordinal, percent, length, . . . } is a tag. The tag
allows us to differentiate 3, 3rd, 3%, and 3 miles?this will be important in
Section 2.6.3. We simply write x for the value x :number.
 Symbolic values. Each value has the form x : t ? V, where x is a string (e.g.,
Washington) and t ? {string, city, state, river, . . . } is a tag. Again, the
tag allows us to differentiate, for example, the entitiesWashington :city
andWashington :state.
Now we build the full set of values V from the primitive values V. To define V , we
need a bit more machinery: To avoid logical paradoxes, we construct V in increasing
order of complexity using types (see Carpenter [1998] for a similar construction). The
casual reader can skip this construction without losing any intuition.
Define the set of types T to be the smallest set that satisfies the following properties:
1. The primitive type  ? T ;
2. The tuple type (t1, . . . , tk) ? T for each k ? 0 and each non-tuple type
ti ? T for i = 1, . . . , k; and
3. The set type {t} ? T for each tuple type t ? T .
Note that {}, {{}}, and (()) are not valid types.
For each type t ? T , we construct a corresponding set of values Vt:
1. For the primitive type t = , the primitive values V have already been
specified. Note that these types are rather coarse: Primitive values with
different tags are considered to have the same type .
2. For a tuple type t = (t1, . . . , tk), Vt is the cross product of the values of its
component types:
Vt = {(v1, . . . , vk) : ?i, vi ? Vti} (1)
396
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
3. For a set type t = {t?}, Vt contains all subsets of its element type t?:
Vt = {s : s ? Vt?} (2)
With this last condition, we ensure that all elements of a set must have the
same type. Note that a set is still allowed to have values with different tags
(e.g., {(Washington :city), (Washington :state)} is a valid set, which might
denote the semantics of the utterance things named Washington). Another
distinction is that types are domain-independent whereas tags tend to be
more domain-specific.
Let V = ?t?T Vt be the set of all possible values.
A world maps each predicate to its semantics, which is a set of tuples (see Figure 4
for an example). First, let TTUPLE ? T be the tuple types, which are the ones of the form
(t1, . . . , tk) for some k. Let V{TUPLE} denote all the sets of tuples (with the same type):
V{TUPLE}
def
=
?
t?TTUPLE
V{t} (3)
Now we define a world formally.
Definition 2 (World)
A world w : P ? V{TUPLE} ? {V} is a function that maps each non-null predicate p ?
P\{?} to a set of tuples w(p) ? V{TUPLE} and maps the null predicate ? to the set of all
values (w(?) = V).
For a set of tuplesAwith the same arity, let ARITY(A) = |x|, where x ? A is arbitrary;
if A is empty, then ARITY(A) is undefined. Now for a predicate p ? P and world w,
define ARITYw(p), the arity of predicate pwith respect to w, as follows:
ARITYw(p) =
{
1 if p = ?
ARITY(w(p)) if p = ?
(4)
The null predicate has arity 1 by fiat; the arity of a non-null predicate p is inherited from
the tuples in w(p).
Remarks. In higher-order logic and lambda calculus, we construct function types and
values, whereas in DCS, we construct tuple types and values. The two are equivalent
in representational power, but this discrepancy does point out the fact that lambda
calculus is based on function application, whereas DCS, as we will see, is based on
declarative constraints. The set type {(, )} in DCS corresponds to the function type
? (? bool). In DCS, there is no explicit bool type?it is implicitly represented by
using sets.
2.3.2 Examples. The world w maps each domain-specific predicate to a set of tuples
(usually a finite set backed by a database). For the U.S. geography domain, w has a
397
Computational Linguistics Volume 39, Number 2
predicate that maps to the set of U.S. states (state), another predicate that maps to the
set of pairs of entities and where they are located (loc), and so on:
w(state) = {(California :state), (Oregon :state), . . . } (5)
w(loc) = {(San Francisco :city,California :state), . . . } (6)
. . . (7)
To shorten notation, we use state abbreviations (e.g., CA = California :state).
The world w also specifies the semantics of several domain-independent predicates
(think of these as helper functions), which usually correspond to an infinite set of tuples.
Functions are represented in DCS by a set of input?output pairs. For example, the
semantics of the countt predicate (for each type t ? T ) contains pairs of sets S and their
cardinalities |S|:
w(countt) = {(S, |S|) : S ? V{(t)}} ? V{({(t)},)} (8)
As another example, consider the predicate averaget (for each t ? T ), which takes a
set of key?value pairs (with keys of type t) and returns the average value. For notational
convenience, we treat an arbitrary set of pairs S as a set-valued function: We let S1 = {x :
(x, y) ? S} denote the domain of the function, and abusing notation slightly, we define
the function S(x) = {y : (x, y) ? S} to be the set of values y that co-occur with the given
x. The semantics of averaget contains pairs of sets and their averages:
w(averaget) =
?
?
?
(S, z) : S ? V{(t,)}, z = |S1|
?1
?
x?S1
?
?|S(x)|?1
?
y?S(x)
y
?
?
?
?
?
? V{({(t,)},)}
(9)
Similarly, we can define the semantics of argmint and argmaxt, which each takes a set of
key?value pairs and returns the keys that attain the smallest (largest) value:
w(argmint) =
{
(S, z) : S ? V{(t,)}, z ? argmin
x?S1
min S(x)
}
? V{({(t,)},t)} (10)
w(argmaxt) =
{
(S, z) : S ? V{(t,)}, z ? argmax
x?S1
max S(x)
}
? V{({(t,)},t)} (11)
The extra min and max is needed because S(x) could contain more than one value. We
also impose that w(argmint) contains only (S, z) such that y is numeric for all (x, y) ? S;
thus argmint denotes a partial function (same for argmaxt).
These helper functions are monomorphic: For example, countt only computes
cardinalities of sets of type {(t)}. In practice, we mostly operate on sets of primitives
(t = ). To reduce notation, we omit t to refer to this version: count = count, average =
average, and so forth.
398
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
2.4 Semantics of DCS Trees without Mark?Execute (Basic Version)
The semantics or denotation of a DCS tree z with respect to a world w is denoted zw.
First, we define the semantics of DCS trees with only join relations (Section 2.4.1). In
this case, a DCS tree encodes a constraint satisfaction problem (CSP); this is important
because it highlights the constraint-based nature of DCS and also naturally leads to a
computationally efficient way of computing denotations (Section 2.4.2). We then allow
DCS trees to have aggregate relations (Section 2.4.3). The fragment of DCS which has
only join and aggregate relations is called basic DCS.
2.4.1 Basic DCS Trees as Constraint Satisfaction Problems. Let z be a DCS tree with only
join relations on its edges. In this case, z encodes a CSP as follows: For each node x in z,
the CSP has a variable with value a(x); the collection of these values is referred to as an
assignment a. The predicates and relations of z introduce constraints:
1. a(x) ? w(p) for each node x labeled with predicate p ? P ; and
2. a(x)j = a(y)j? for each edge (x, y) labeled with
j
j? ? R, which says that the
j-th component of a(x) must equal the j?-th component of a(y).
We say that an assignment a is feasible if it satisfies these two constraints. Next, for a node
x, defineV(x) = {a(x) : assignment a is feasible} as the set of feasible values for x?these
are the ones that are consistent with at least one feasible assignment. Finally, we define
the denotation of the DCS tree z with respect to the world w to be zw = V(x0), where
x0 is the root node of z.
Figure 3(a) shows an example of a DCS tree. The corresponding CSP has four vari-
ables c,m, , s.2 In Figure 3(b), we have written the equivalent lambda calculus formula.
The non-root nodes are existentially quantified, the root node c is ?-abstracted, and
all constraints introduced by predicates and relations are conjoined. The ?-abstraction
of c represents the fact that the denotation is the set of feasible values for c (note the
equivalence between the Boolean function ?c.p(c) and the set {c : p(c)}).
Remarks. Note that CSPs only allow existential quantification and conjunction. Why
did we choose this particular logical subset as a starting point, rather than allowing
universal quantification, negation, or disjunction? There seems to be something fun-
damental about this subset, which also appears in Discourse Representation Theory
(DRT) (Kamp and Reyle 1993; Kamp, van Genabith, and Reyle 2005). Briefly, logical
forms in DRT are called Discourse Representation Structures (DRSs), each of which
contains (i) a set of existentially quantified discourse referents (variables), (ii) a set of
conjoined discourse conditions (constraints), and (iii) nested DRSs. If we exclude nested
DRSs, a DRS is exactly a CSP.3 The default existential quantification and conjunction are
quite natural for modeling cross-sentential anaphora: New variables can be added to
2 Technically, the node is c and the variable is a(c), but we use c to denote the variable to simplify notation.
3 Unlike the CSPs corresponding to DCS trees, the CSPs corresponding to DRSs need not be
tree-structured, though economical DRT (Bos 2009) imposes a tree-like restriction on DRSs for
computational reasons.
399
Computational Linguistics Volume 39, Number 2
a DRS and connected to other variables. Indeed, DRT was originally motivated by these
phenomena (see Kamp and Reyle [1993] for more details).4
Tree-structured CSPs can capture unboundedly complex recursive structures?such
as cities in states that border states that have rivers that. . . . Trees are limited, however, in
that they are unable to capture long-distance dependencies such as those arising from
anaphora. For example, in the phrase a state with a river that traverses its capital, its binds
to state, but this dependence cannot be captured in a tree structure. A solution is
to simply add an edge between the its node and the state node that forces the two
nodes to have the same value. The result is still a well-defined CSP, though not a tree-
structured one. The situation would become trickier if we were to integrate the other
relations (aggregate, mark, and execute). We might be able to incorporate some ideas
from Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; White 2006),
given that hybrid logic extends the tree structures of modal logic with nominals, thereby
allowing a node to freely reference other nodes. In this article, however, we will stick to
trees and leave the full exploration of non-trees for future work.
2.4.2 Computation of Join Relations. So far, we have given a declarative definition of the
denotation zw of a DCS tree z with only join relations. Now we will show how to
compute zw efficiently. Recall that the denotation is the set of feasible values for the
root node. In general, finding the solution to a CSP is NP-hard, but for trees, we can
exploit dynamic programming (Dechter 2003). The key is that the denotation of a tree
depends on its subtrees only through their denotations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= w(p) ?
m
?
i=1
{v : vji = tj?i , t ? ciw} (12)
On the right-hand side of Equation (12), the first termw(p) is the set of values that satisfy
the node constraint, and the second term consists of an intersection across all m edges
of {v : vji = tj?i , t ? ciw}, which is the set of values v which satisfy the edge constraint
with respect to some value t for the child ci.
To further flesh out this computation, we express Equation (12) in terms of two
operations: join and project. Join takes a cross product of two sets of tuples and retains
the resulting tuples that match the join constraint:
A j,j? B = {u+ v : u ? A, v ? B,uj = vj?} (13)
Project takes a set of tuples and retains a fixed subset of the components:
A[i] = {vi : v ? A} (14)
The denotation in Equation (12) can now be expressed in terms of these join and project
operations:

?
p;
j1
j?1
:c1; ? ? ? ;
jm
j?m
:cm
?

w
= ((w(p) j1,j?1 c1w)[i] ? ? ? jm,j?m cmw)[i] (15)
4 DRT started the dynamic semantics tradition where meanings are context-change potentials, a natural
way to capture anaphora. The DCS formalism presented here does not deal with anaphora, so we give it
a purely static semantics.
400
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
where i = (1, . . . , ARITYw(p)). Projecting onto i retains only components corresponding
to p.
The time complexity for computing the denotation of a DCS tree zw scales linearly
with the number of nodes, but there is also a dependence on the cost of performing the
join and project operations. For details on howwe optimize these operations and handle
infinite sets of tuples (for predicates such as count), see Liang (2011).
The denotation of DCS trees is defined in terms of the feasible values of a CSP, and
the recurrence in Equation (15) is only one way of computing this denotation. In light of
the extensions to come, however, we now consider Equation (15) as the actual definition
rather than just a computational mechanism. It will still be useful to refer to the CSP in
order to access the intuition of using declarative constraints.
2.4.3 Aggregate Relation. Thus far, we have focused on DCS trees that only use join
relations, which are insufficient for capturing higher-order phenomena in language. For
example, consider the phrase number of major cities. Suppose that number corresponds
to the count predicate, and that major cities maps to the DCS tree ?city; 11 :?major??. We
cannot simply join countwith the root of this DCS tree because count needs to be joined
with the set of major cities (the denotation of ?city; 11 :?major??), not just a single city.
We therefore introduce the aggregate relation (?) that takes a DCS subtree and
reifies its denotation so that it can be accessed by other nodes in its entirety. Consider a
tree ??;? :c?, where the root is connected to a child c via ?. The denotation of the root is
simply the singleton set containing the denotation of c:
??;? :c?w = {(cw)} (16)
Figure 5(a) shows the DCS tree for our running example. The denotation of the
middle node is {(s)}, where s is all major cities. Everything above this node is an
ordinary CSP: s constrains the count node, which in turns constrains the root node to
|s|. Figure 5(b) shows another example of using the aggregate relation ?. Here, the node
right above ? is constrained to be a set of pairs of major cities and their populations.
The average predicate then computes the desired answer.
To represent logical disjunction in natural language, we use the aggregate relation
and two predicates, union and contains, which are defined in the expected way:
w(union) = {(A,B,C) : C = A ? B,A ? V{},B ? V{}} (17)
w(contains) = {(A, x) : x ? A,A ? V{}} (18)
where A,B,C ? V{} are sets of primitive values (see Section 2.3.1). Figure 5(c) shows
an example of a disjunctive construction: We use the aggregate relations to construct
two sets, one containing Oregon, and the other containing states bordering Oregon. We
take the union of these two sets; contains takes the set and reads out an element, which
then constrains the city node.
Remarks. A DCS tree that contains only join and aggregate relations can be viewed as
a collection of tree-structured CSPs connected via aggregate relations. The tree struc-
ture still enables us to compute denotations efficiently based on the recurrences in
Equations (15) and (16).
Recall that a DCS tree with only join relations is a DRS without nested DRSs. The
aggregate relation corresponds to the abstraction operator in DRT and is one way of
401
Computational Linguistics Volume 39, Number 2
Figure 5
Examples of DCS trees that use the aggregate relation (?) to (a) compute the cardinality of a set,
(b) take the average over a set, (c) represent a disjunction over two conditions. The aggregate
relation sets the parent node deterministically to the denotation of the child node. Nodes with
the special null predicate ? are represented as empty nodes.
making nested DRSs. It turns out that the abstraction operator is sufficient to obtain
the full representational power of DRT, and subsumes generalized quantification and
disjunction constructs in DRT. By analogy, we use the aggregate relation to handle
disjunction (Figure 5(c)) and generalized quantification (Section 2.5.6).
DCS restricted to join relations is less expressive than first-order logic because it
does not have universal quantification, negation, and disjunction. The aggregate rela-
tion is analogous to lambda abstraction, and in basic DCS we use the aggregate relation
to implement those basic constructs using higher-order predicates such as not, every,
and union. We can also express logical statements such as generalized quantification,
which go beyond first-order logic.
2.5 Semantics of DCS Trees with Mark?Execute (Full Version)
Basic DCS includes two types of relations, join and aggregate, but it is already quite
expressive. In general, however, it is not enough just to be able to express the meaning
of a sentence using some logical form; we must be able to derive the logical form
compositionally and simply from the sentence.
Consider the superlative constructionmost populous city, which has a basic syntactic
dependency structure shown in Figure 6(a). Figure 6(b) shows that we can in principle
402
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 6
Two semantically equivalent DCS trees are shown in (b) and (c). The DCS tree in (b), which uses
the join and aggregate relations in the basic DCS, does not align well with the syntactic structure
of most populous city (a), and thus is undesirable. The DCS tree in (c), by using the mark?execute
construct, aligns much better, with city rightfully dominating its modifiers. The full version of
DCS allows us to construct (c), which is preferable to (b).
already use a DCS tree with only join and aggregate relations to express the correct
semantics of the superlative construction. Note, however, that the two structures are
quite divergent?the syntactic head is city and the semantic head is argmax. This diver-
gence runs counter to a principal desideratum of DCS, which is to create a transparent
interface between coarse syntax and semantics.
In this section, we introduce mark and execute relations, which will allow us to
use the DCS tree in Figure 6(c) to represent the semantics associated with Figure 6(a);
these two are more similar than (a) and (b). The focus of this section is on this mark?
execute construct?usingmark and execute relations to give proper semantically scoped
denotations to syntactically scoped tree structures.
The basic intuition of the mark?execute construct is as follows: We mark a node
low in the tree with a mark relation; then, higher up in the tree, we invoke it with a
corresponding execute relation (Figure 7). For our example in Figure 6(c), we mark the
population node, which puts the child argmax in a temporary store; when we execute
the city node, we fetch the superlative predicate argmax from the store and invoke it.
This divergence between syntactic and semantic scope arises in other linguistic
contexts besides superlatives, such as quantification and negation. In each of these
cases, the general template is the same: A syntactic modifier low in the tree needs to
have semantic force higher in the tree. A particularly compelling case of this divergence
happenswith quantifier scope ambiguity (e.g., Some river traverses every city5), where the
5 The two meanings are: (i) there is a river x such that x traverses every city; and (ii) for every city x, some
river traverses x.
403
Computational Linguistics Volume 39, Number 2
Figure 7
The template for the mark?execute construct. A mark relation (one of E, Q, C) ?stores? the
modifier. Then an execute relation (of the form Xi for indices i) higher up ?recalls? the
modifier and applies it at the desired semantic point.
quantifiers appear in fixed syntactic positions, but the surface and inverse scope read-
ings correspond to different semantically scoped denotations. Analogously, a single syn-
tactic structure involving superlatives can also yield two different semantically scoped
denotations?the absolute and relative readings (e.g., state bordering the largest state6).
The mark?execute construct provides a unified framework for dealing all these forms
of divergence between syntactic and semantic scope. See Figures 8 and 9 for concrete
examples of this construct.
2.5.1 Denotations.We now formalize the mark?execute construct. We saw that the mark?
execute construct appears to act non-locally, putting things in a store and retrieving
them later. This means that if we want the denotation of a DCS tree to only depend
on the denotations of its subtrees, the denotations need to contain more than the set of
feasible values for the root node, as was the case for basic DCS. We need to augment de-
notations to include information about all marked nodes, because these can be accessed
by an execute relation higher up in the tree.
More specifically, let z be a DCS tree and d = zw be its denotation. The denotation
d consists of n columns. The first column always corresponds to the root node of z,
and the rest of the columns correspond to non-root marked nodes in z. In the example
in Figure 10, there are two columns, one for the root state node and the other for size
node, which is marked by C. The columns are ordered according to a pre-order traversal
of z, so column 1 always corresponds to the root node. The denotation d contains a set of
arrays d.A, where each array represents a feasible assignment of values to the columns
of d; note that we quantify over non-marked nodes, so they do not correspond to any
column in the denotation. For example, in Figure 10, the first array in d.A corresponds to
assigning (OK) to the state node (column 1) and (TX, 2.7e5) to the size node (column 2).
If there are no marked nodes, d.A is basically a set of tuples, which corresponds to a
denotation in basic DCS. For each marked node, the denotation d also maintains a store
6 The two meanings are: (i) a state that borders Alaska (which is the largest state); and (ii) a state with the
highest score, where the score of a state x is the maximum size of any state that x borders (Alaska is
irrelevant here because no states border it).
404
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 8
Examples of DCS trees that use the mark?execute construct with the E and Q mark relations.
(a) The head verb borders, which needs to be returned, has a direct object statesmodified by
which. (b) The quantifier no is syntactically dominated by state but needs to take wider scope.
(c) Two quantifiers yield two possible readings; we build the same basic structure, marking
both quantifiers; the choice of execute relation (X12 versus X21) determines the reading. (d) We
use two mark relations, Q on river for the negation, and E on city to force the quantifier to be
computed for each value of city.
with information to be retrieved when that marked node is executed. A store ? for a
marked node contains the following: (i) the mark relation ?.r (C in the example), (ii) the
base denotation ?.b, which essentially corresponds to denotation of the subtree rooted at
the marked node excluding the mark relation and its subtree (?size?w in the example),
and (iii) the denotation of the child of the mark relation (?argmax?w in the example).
The store of any unmarked nodes is always empty (? = ?).
Definition 3 (Denotations)
Let D be the set of denotations, where each denotation d ? D consists of
 a set of arrays d.A, where each array a = [a1, . . . , an] ? d.A is a sequence of
n tuples for some n ? 0; and
405
Computational Linguistics Volume 39, Number 2
Figure 9
Examples of DCS trees that use the mark?execute construct with the E and C relation. (a,b,c)
Comparatives and superlatives are handled as follows: For each value of the node marked
by E, we compute a number based on the node marked by C; based on this information,
a subset of the values is selected as the possible values of the root node. (d) Analog of quantifier
scope ambiguity for superlatives: The placement of the execute relation determines an absolute
versus relative reading. (e) Interaction between a quantifier and a superlative: The lower execute
relation computes the largest city for each state; the second execute relation invokes most and
enforces that the major constraint holds for the majority of states.
406
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 10
Example of the denotation for a DCS tree (with the compare relation C). This denotation has two
columns, one for each active node?the root node state and the marked node size.
 a sequence of n stores d.? = (d.?1, . . . , d.?n), where each store ? contains a
mark relation ?.r ? {E, Q, C, ?}, a base denotation ?.b ? D ? {?}, and a
child denotation ?.c ? D ? {?}.
Note that denotations are formally defined without reference to DCS trees (just as sets
of tuples were in basic DCS), but it is sometimes useful to refer to the DCS tree that
generates that denotation.
For notational convenience, we write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. Also let
d.ri = d.?i.r, d.bi = d.?i.b, and d.ci = d.?i.c. Let d{?i = x} be the denotation which is
identical to d, except with d.?i = x; d{ri = x}, d{bi = x}, and d{ci = x} are defined
analogously. We also define a project operation for denotations: ??A;???[i] def= ??{ai : a ?
A};?i??. Extending this notation further, we use ? to denote the indices of the non-initial
columns with empty stores (i > 1 such that d.?i = ?). We can then use d[??] to represent
projecting away the non-initial columns with empty stores. For the denotation d in
Figure 10, d[1] keeps column 1, d[??] keeps both columns, and d[2,?2] swaps the two
columns.
In basic DCS, denotations are sets of tuples, which works quite well for repre-
senting the semantics of wh-questions such as What states border Texas? But what about
polar questions such as Does Louisiana border Texas? The denotation should be a simple
Boolean value, which basic DCS does not represent explicitly. Using our new deno-
tations, we can represent Boolean values explicitly using zero-column structures: true
corresponds to a singleton set containing just the empty array (dT = ??{[ ]}??) and false
is the empty set (dF = ?????).
Having described denotations as n-column structures, we now give the formal
mapping from DCS trees to these structures. As in basic DCS, this mapping is defined
recursively over the structure of the tree. We have a recurrence for each case (the first
line is the base case, and each of the others handles a different edge relation):
?p?w = ??{[v] : v ? w(p)}; ??? [base case] (19)

?
p; e;
j
j? :c
?

w
= ?p; e?w 
??
j,j?
cw [join] (20)
?p; e;? :c?w = ?p; e?w 
??
?,? ?
(
cw
)
[aggregate] (21)
407
Computational Linguistics Volume 39, Number 2
?p; e; Xi :c?w = ?p; e?w 
??
?,? xi(cw) [execute] (22)
?p; e; E :c?w =M(?p; e?w, E, cw) [extract] (23)
?p; e; C :c?w =M(?p; e?w, C, cw) [compare] (24)
?p; Q :c; e?w =M(?p; e?w, Q, cw) [quantify] (25)
We define the operations ??
j,j?
,?,Xi, andM in the remainder of this section.
2.5.2 Base Case. Equation (19) defines the denotation for a DCS tree z with a single node
with predicate p. The denotation of z has one column whose arrays correspond to the
tuples w(p); the store for that column is empty.
2.5.3 Join Relations. Equation (20) defines the recurrence for join relations. On the left-
hand side,
?
p; e;
j
j? :c
?
is a DCS tree with p at the root, a sequence of edges e followed by
a final edge with relation
j
j? connected to a child DCS tree c. On the right-hand side, we
take the recursively computed denotation of ?p; e?, the DCS tree without the final edge,
and perform a join-project-inactive operation (notated ??
j,j?
) with the denotation of the
child DCS tree c.
The join-project-inactive operation joins the arrays of the two denotations (this is
the core of the join operation in basic DCS?see Equation (13)), and then projects away
the non-initial empty columns:7
??A;??? ??
j,j?
??A?;???? = ??A??;?+ ????[??],where (26)
A?? = {a+ a? : a ? A, a? ? A?, a1j = a?1j?}
We concatenate all arrays a ? A with all arrays a? ? A? that satisfy the join condition
a1j = a
?
1j? . The sequences of stores are simply concatenated: (?+ ?
?). Finally, any non-
initial columns with empty stores are projected away by applying ?[??].
Note that the join works on column 1; the other columns are carried along for the
ride. As another piece of convenient notation, we use ? to represent all components, so
???,? imposes the join condition that the entire tuple has to agree (a1 = a
?
1).
2.5.4 Aggregate Relations. Equation (21) defines the recurrence for aggregate relations.
Recall that in basic DCS, aggregate (16) simply takes the denotation (a set of tuples) and
puts it into a set. Now, the denotation is not just a set, so we need to generalize this
operation. Specifically, the aggregate operation applied to a denotation forms a set out
of the tuples in the first column for each setting of the rest of the columns:
? (??A;???) = ??A? ? A??;??? (27)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ?i ? {2, . . . ,n}, [ai] ? ?i.b.A[1],??a1, a ? A}
7 The join and project operations are taken from relational algebra.
408
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
The aggregate operation takes the set of arrays A and produces two sets of arrays, A?
andA??, which are unioned (note that the stores do not change). The setA? is the one that
first comes to mind: For every setting of a2, . . . , an, we construct S(a), the set of tuples a
?
1
in the first column which co-occur with a2, . . . , an in A.
There is another case, however: what happens to settings of a2, . . . , an that do not
co-occur with any value of a?1 in A? Then, S(a) = ?, but note that A
? by construction will
not have the desired array [?, a2, . . . , an]. As a concrete example, suppose A = ? and we
have one column (n = 1). Then A? = ?, rather than the desired {[?]}.
Fixing this problem is slightly tricky. There are an infinite number of a2, . . . , anwhich
do not co-occur with any a?1 inA, so for which ones do we actually include [?, a2, . . . , an]?
Certainly, the answer to this question cannot come from A, so it must come from the
stores. In particular, for each column i ? {2, . . . ,n}, we have conveniently stored a base
denotation ?i.b. We consider any ai that occurs in column 1 of the arrays of this base
denotation ([ai] ? ?i.b.A[1]). For this a2, . . . , an, we include [?, a2, . . . , an] in A?? as long as
a2, . . . , an does not co-occur with any a1. An example is given in Figure 11.
The reason for storing base denotations is thus partially revealed: The arrays rep-
resent feasible values of a CSP and can only contain positive information. When we
aggregate, we need to access possibly empty sets of feasible values?a kind of negative
information, which can only be recovered from the base denotations.
Figure 11
An example of applying the aggregate operation, which takes a denotation and aggregates the
values in column 1 for every setting of the other columns. The base denotations (b) are used to
put in {} for values that do not appear in A (in this example, AK, corresponding to the fact that
Alaska does not border any states).
409
Computational Linguistics Volume 39, Number 2
2.5.5 Mark Relations. Equations (23), (24), and (25) each processes a different mark
relation. We define a general mark operation, M(d, r, c) which takes a denotation d, a
mark relation r ? {E, Q, C} and a child denotation c, and sets the store of d in column 1
to be (r, d, c):
M(d, r, c) = d{r1 = r, b1 = d, c1 = c} (28)
The base denotation of the first column b1 is set to the current denotation d. This, in
some sense, creates a snapshot of the current denotation. Figure 12 shows an example
of the mark operation.
2.5.6 Execute Relations. Equation (22) defines the denotation of a DCS tree where the last
edge of the root is an execute relation. Similar to the aggregate case (21), we recurse on
the DCS tree without the last edge (?p; e?) and then join it to the result of applying the
execute operation Xi to the denotation of the child (cw).
The execute operation Xi is the most intricate part of DCS and is what does the
heavy lifting. The operation is parametrized by a sequence of distinct indices i that
specifies the order in which the columns should be processed. Specifically, i indexes into
the subsequence of columns with non-empty stores. We then process this subsequence
of columns in reverse order, where processing a column means performing some op-
erations depending on the stored relation in that column. For example, suppose that
columns 2 and 3 are the only non-empty columns. Then X12 processes column 3 before
column 2. On the other hand, X21 processes column 2 before column 3. We first define
Figure 12
An example of applying the mark operation, which takes a denotation and modifies the store of
the column 1. This information is used by other operations such as aggregate and execute.
410
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 13
An example of applying the execute operation on column 1 with the extract relation E. The
denotation prior to execution consists of two columns: column 1 corresponds to the border
node; column 2 to the state node. The join relations and predicates CA and state constrain the
arrays A in the denotation to include only the states that border California. After execution, the
non-marked column 1 is projected away, leaving only the state column with its store emptied.
the execute operation Xi for a single column i. There are three distinct cases, depending
on the relation stored in column i:
Extraction. For a denotation d with the extract relation E in column i, executing Xi(d)
involves three steps: (i) moving column i to before column 1 (?[i,?i]), (ii) projecting
away non-initial empty columns (?[??]), and (iii) removing the store (?{?1 = ?}):
Xi(d) = d[i,?i][??]{?1 = ?} if d.ri = E (29)
An example is given in Figure 13. There are two main uses of extraction.
1. By default, the denotation of a DCS tree is the set of feasible values of the
root node (which occupies column 1). To return the set of feasible values
of another node, we mark that node with E. Upon execution, the feasible
values of that node move into column 1. Extraction can be used to handle
in situ questions (see Figure 8(a)).
2. Unmarked nodes (those that do not have an edge with a mark relation) are
existentially quantified and have narrower scope than all marked nodes.
Therefore, we can make a node x have wider scope than another node y by
411
Computational Linguistics Volume 39, Number 2
marking x (with E) and executing y before x (see Figure 8(d,e) for
examples). The extract relation E (in fact, any mark relation) signifies
that we want to control the scope of a node, and the execute relation
allows us to set that scope.
Generalized Quantification.Generalized quantifiers are predicates on two sets, a restrictor
A and a nuclear scope B. For example,
w(some) = {(A,B) : A ? B > 0} (30)
w(every) = {(A,B) : A ? B} (31)
w(no) = {(A,B) : A ? B = ?} (32)
w(most) = {(A,B) : |A ? B| > 1
2
|A|} (33)
We think of the quantifier as amodifier which always appears as the child of a Q relation;
the restrictor is the parent. For example, in Figure 8(b), no corresponds to the quantifier
and state corresponds to the restrictor. The nuclear scope should be the set of all states
that Alaska borders. More generally, the nuclear scope is the set of feasible values of the
restrictor node with respect to the CSP that includes all nodes between the mark and
execute relations. The restrictor is also the set of feasible values of the restrictor node,
but with respect to the CSP corresponding to the subtree rooted at that node.8
We implement generalized quantifiers as follows: Let d be a denotation and suppose
we are executing column i. We first construct a denotation for the restrictor dA and a
denotation for the nuclear scope dB. For the restrictor, we take the base denotation in
column i (d.bi)?remember that the base denotation represents a snapshot of the restric-
tor node before the nuclear scope constraints are added. For the nuclear scope, we take
the complete denotation d (which includes the nuclear scope constraints) and extract
column i (d[i,?i][??]{?1 = ?}?see (29)). We then construct dA and dB by applying the
aggregate operation to each. Finally, we join these sets with the quantifier denotation,
stored in d.ci:
xi(d) =
((
d.ci 
??
1,1 dA
)
??2,1 dB
)
[?1] if d.ri = Q,where (34)
dA = ? (d.bi) (35)
dB = ? (d[i,?i][??]{?1 = ?}) (36)
When there is one quantifier, think of the execute relation as performing a syntactic
rewriting operation, as shown in Figure 14(b). For more complex cases, we must defer
to (34).
Figure 8(c) shows an example with two interacting quantifiers. The denotation of
the DCS tree before execution is the same in both readings, as shown in Figure 15. The
8 Defined this way, we can only handle conservative quantifiers, because the nuclear scope will always be
a subset of the restrictor. This design decision is inspired by DRT, where it provides a way of modeling
donkey anaphora. We are not treating anaphora in this work, but we can handle it by allowing pronouns
in the nuclear scope to create anaphoric edges into nodes in the restrictor. These constraints naturally
propagate through the nuclear scope?s CSP without affecting the restrictor.
412
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 14
(a) An example of applying the execute operation on column iwith the quantify relation Q.
Before executing, note that A = {} (because Alaska does not border any states). The restrictor (A)
is the set of all states, and the nuclear scope (B) is empty. Because the pair (A,B) does exist in
w(no), the final denotation is ??{[ ]}?? (which represents true). (b) Although the execute operation
actually works on the denotation, think of it in terms of expanding the DCS tree. We introduce
an extra projection relation [?1], which projects away the first column of the child subtree?s
denotation.
quantifier scope ambiguity is resolved by the choice of execute relation: X12 gives the
surface scope reading, X21 gives the inverse scope reading.
Figure 8(d) shows how extraction and quantification work together. First, the no
quantifier is processed for each city, which is an unprocessed marked node. Here, the
extract relation is a technical trick to give city wider scope.
Comparatives and Superlatives. Comparative and superlative constructions involve com-
paring entities, and for this we rely on a set S of entity?degree pairs (x, y), where x is an
Figure 15
Denotation of Figure 8(c) before the execute relation is applied.
413
Computational Linguistics Volume 39, Number 2
entity and y is a numeric degree. Recall that we can treat S as a function, which maps
an entity x to the set of degrees S(x) associated with x. Note that this set can contain
multiple degrees. For example, in the relative reading of state bordering the largest state,
we would have a degree for the size of each neighboring state.
Superlatives use the argmax and argmin predicates, which are defined in Section 2.3.
Comparatives use the more and less predicates: w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; w(less) is defined analogously:
w(more) = {(S, x, y) : max S(x) > max S(y)} (37)
w(less) = {(S, x, y) : min S(x) < min S(y)} (38)
We use the same mark relation C for both comparative and superlative construc-
tions. In terms of the DCS tree, there are three key parts: (i) the root x, which corresponds
to the entity to be compared, (ii) the child c of a C relation, which corresponds to the
comparative or superlative predicate, and (iii) c?s parent p, which contains the ?degree
information? (which will be described later) used for comparison. We assume that the
root is marked (usually with a relation E). This forces us to compute a comparison
degree for each value of the root node. In terms of the denotation d corresponding to the
DCS tree prior to execution, the entity to be compared occurs in column 1 of the arrays
d.A, the degree information occurs in column i of the arrays d.A, and the denotation of
the comparative or superlative predicate itself is the child denotation at column i (d.ci).
First, we define a concatenating function +i (d), which combines the columns i of d
by concatenating the corresponding tuples of each array in d.A:
+i (??A;???) = ??A?;????, where (39)
A? = {a(1...i1 )\i+ [ai1 + ? ? ?+ ai|i| ]+ a(i1...n)\i : a ? A}
?? = ?(1...i1 )\i+ [?i1 ]+ ?(i1...n)\i
Note that the store of column i1 is kept and the others are discarded. As an example:
+2,1 (??{[(1), (2), (3)], [(4), (5), (6)]};?1,?2,?3??) = ??{[(2, 1), (3)], [(5, 4), (6)]};?2,?3??
(40)
We first create a denotation d? where column i, which contains the degree infor-
mation, is extracted to column 1 (and thus column 2 corresponds to the entity to be
compared). Next, we create a denotation dS whose column 1 contains a set of entity-
degree pairs. There are two types of degree information:
1. Suppose the degree information has arity 2 (ARITY(d.A[i]) = 2). This
occurs, for example, in most populous city (see Figure 9(b)), where column i
is the population node. In this case, we simply set the degree to the
second component of population by projection (???w 
??
1,2 d
?). Now
columns 1 and 2 contain the degrees and entities, respectively. We
concatenate columns 2 and 1 (+2,1 (?)) and aggregate to produce a
denotation dS which contains the set of entity?degree pairs in column 1.
2. Suppose the degree information has arity 1 (ARITY(d.A[i]) = 1). This
occurs, for example, in state bordering the most states (see Figure 9(a)), where
414
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
column i is the lower marked state node. In this case, the degree of an
entity from column 2 is the number of different values that column 1 can
take. To compute this, aggregate the set of values (?
(
d?
)
) and apply the
count predicate. Now with the degrees and entities in columns 1 and 2,
respectively, we concatenate the columns and aggregate again to obtain dS.
Having constructed dS, we simply apply the comparative/superlative predicate which
has been patiently waiting in d.ci. Finally, the store of d?s column 1 was destroyed by the
concatenation operation+2,1 (() ?), so wemust restore it with ?{?1 = d.?1}. The complete
operation is as follows:
xi(d) =
(
???w 
??
1,2
(
d.ci 
??
1,1 dS
))
{?1 = d.?1} if d.?i = C, d.?1 = ?, where (41)
dS =
?
?
?
?
(
+2,1
(
???w 
??
1,2 d
?
))
if ARITY(d.A[i]) = 2
?
(
+2,1
(
???w 
??
1,2
(
?count?w 
??
1,1 ?
(
d?
)
)))
if ARITY(d.A[i]) = 1
(42)
d? = d[i,?i][??]{?1 = ?} (43)
An example of executing the C relation is shown in Figure 16(a). As with executing a
Q relation, for simple cases we can think of executing a C relation as expanding a DCS
tree, as shown in Figure 16(b).
Figure 9(a) and Figure 9(b) show examples of superlative constructions with the ar-
ity 1 and arity 2 types of degree information, respectively. Figure 9(c) shows an example
of a comparative construction. Comparatives and superlatives use the same machinery,
differing only in the predicate: argmax versus ?more; 31 :TX? (more than Texas). But both
predicates have the same template behavior: Each takes a set of entity?degree pairs and
returns any entity satisfying some property. For argmax, the property is obtaining the
highest degree; for more, it is having a degree higher than a threshold. We can handle
generalized superlatives (the five largest or the fifth largest or the 5% largest) as well by
swapping in a different predicate; the execution mechanisms defined in Equation (41)
remain the same.
We saw that the mark?execute machinery allows decisions regarding quantifier
scope to be made in a clean and modular fashion. Superlatives also have scope am-
biguities in the form of absolute versus relative readings. Consider the example in
Figure 9(d). In the absolute reading, we first compute the superlative in a narrow scope
(the largest state is Alaska), and then connect it with the rest of the phrase, resulting in
the empty set (because no states border Alaska). In the relative reading, we consider the
first state as the entity we want to compare, and its degree is the size of a neighboring
state. In this case, the lower state node cannot be set to Alaska because there are no
states bordering it. The result is therefore any state that borders Texas (the largest state
that does have neighbors). The two DCS trees in Figure 9(d) show that we can naturally
account for this form of superlative ambiguity based on where the scope-determining
execute relation is placed without drastically changing the underlying tree structure.
Remarks. These scope divergence issues are not specific to DCS?every serious semantic
formalism must address them. Generative grammar uses quantifier raising to move the
quantifier from its original syntactic position up to the desired semantic position before
semantic interpretation even occurs (Heim and Kratzer 1998). Other mechanisms such
415
Computational Linguistics Volume 39, Number 2
Figure 16
(a) Executing the compare relation C for an example superlative construction (relative reading
of state bordering the largest state from Figure 9(d)). Before executing, column 1 contains the
entity to compare, and column 2 contains the degree information, of which only the second
component is relevant. After executing, the resulting denotation contains a single column with
only the entities that obtain the highest degree (in this case, the states that border Texas). (b) For
this example, think of the execute operation as expanding the original DCS tree, although the
execute operation actually works on the denotation, not the DCS tree. The expanded DCS tree
has the same denotation as the original DCS tree, and syntactically captures the essence of the
execute?compare operation. Going through the relations of the expanded DCS tree from
bottom to top: The X2 relation swaps columns 1 and 2; the join relation keeps only the second
component ((TX, 267K) becomes (267K)); +2,1 concatenates columns 2 and 1 ([(267K), (AR)]
becomes [(AR, 267K)]); ? aggregates these tuples into a set; argmax operates on this set and
returns the elements.
as Montague?s (1973) quantifying in, Cooper storage (Cooper 1975), and Carpenter?s
(1998) scoping constructor handle scope divergence during semantic interpretation.
Roughly speaking, these mechanisms delay application of a quantifier, ?marking? its
spot with a dummy pronoun (as inMontague?s quantifying in) or putting it in a store (as
in Cooper storage), and then ?executing? the quantifier at a later point in the derivation
either by performing a variable substitution or retrieving it from the store. Continuation,
from programming languages, is another solution (Barker 2002; Shan 2004); this sets the
semantics of a quantifier to be a function from its continuation (which captures all the
semantic content of the clause minus the quantifier) to the final denotation of the clause.
416
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Intuitively, continuations reverse the normal evaluation order, allowing a quantifier to
remain in situ but still outscope the rest of the clause. In fact, the mark and execute
relations of DCS are analogous to the shift and reset operators used in continuations.
One of the challenges with allowing flexible scope is that free variables can yield invalid
scopings, a well-known issuewith Cooper storage that the continuation-based approach
solves. Invalid scopings are filtered out by the construction mechanism (Section 2.6).
One difference between mark?execute in DCS and many other mechanisms is that
DCS trees (which contain mark and execute relations) are the final logical forms?the
handling of scope divergence occurs in the computing their denotations. The analog
in the other mechanisms resides in the construction mechanism?the actually final
logical form is quite simple.9 Therefore, we have essentially pushed the inevitable
complexity from the construction mechanism into the semantics of the logical form.
This is a conscious design decision: We want our construction mechanism, which maps
natural language to logical form, to be simple and not burdened with complex linguistic
issues, for our focus is on learning this mapping. Unfortunately, the denotation of our
logical forms (Section 2.5.1) do become more complex than those of lambda calculus
expressions, but we believe this is a reasonable tradeoff to make for our particular
application.
2.6 Construction Mechanism
We have thus far defined the syntax (Section 2.2) and semantics (Section 2.5) of DCS
trees, but we have only vaguely hinted at how these DCS trees might be connected
to natural language utterances by appealing to idealized examples. In this section, we
formally define the construction mechanism for DCS, which takes an utterance x and
produces a set of DCS trees ZL(x).
Because wemotivated DCS trees based on dependency syntax, it might be tempting
to take a dependency parse tree of the utterance, replace the words with predicates, and
attach some relations on the edges to produce a DCS tree. To a first approximation, this
is what we will do, but we need to be a bit more flexible for several reasons: (i) some
nodes in the DCS tree do not have predicates (e.g., children of an E relation or parent
of an Xi relation); (ii) nodes have predicates that do not correspond to words (e.g., in
California cities, there is a implicit loc predicate that bridges CA and city); (iii) some
words might not correspond to any predicates in our world (e.g., please); and (iv) the
DCS tree might not always be aligned with the syntactic structure depending on which
syntactic formalism one ascribes to. Although syntax was the inspiration for the DCS
formalism, we will not actually use it in construction.
It is also worth stressing the purpose of the construction mechanism. In linguistics,
the purpose of the construction mechanism is to try to generate the exact set of valid
logical forms for a sentence. We view the construction mechanism instead as simply a
way of creating a set of candidate logical forms. A separate step defines a distribution
over this set to favor certain logical forms over others. The construction mechanism
should therefore simply overapproximate the set of logical forms. Linguistic constraints
that are normally encoded in the construction mechanism (for example, in CCG, that
the disharmonic pair S/NP and S\NP cannot be coordinated, or that non-indefinite
quantifiers cannot extend their scope beyond clause boundaries) would be instead
9 In the continuation-based approach, this difference corresponds to the difference between assigning a
denotational versus an operational semantics.
417
Computational Linguistics Volume 39, Number 2
encoded as features (Section 3.1.1). Because feature weights are estimated from data,
one can view our approach as automatically learning the linguistic constraints relevant
to our end task.
2.6.1 Lexical Triggers. The construction mechanism assumes a fixed set of lexical triggers
L. Each trigger is a pair (s, p), where s is a sequence of words (usually one) and p is a
predicate (e.g., s = California and p = CA). We use L(s) to denote the set of predicates p
triggered by s ((s, p) ? L). We should think of the lexical triggers L not as pinning down
the precise predicate for each word, but rather as producing an overapproximation.
For example, Lmight contain {(city, city), (city, state), (city, river), . . . }, reflecting our
initial ignorance prior to learning.
We also define a set of trace predicates L(	), which can be introduced without an
overt lexical element. Their name is inspired by trace/null elements in syntax, but they
serve a more practical rather than a theoretical role here. As we shall see in Section 2.6.2,
trace predicates provide more flexibility in the construction of logical forms, allowing
us to insert a predicate based on the partial logical form constructed thus far and assess
its compatibility with the words afterwards (based on features), rather than insisting on
a purely lexically driven formalism. Section 4.1.3 describes the lexical triggers and trace
predicates that we use in our experiments.
2.6.2 Recursive Construction of DCS Trees. Given a set of lexical triggers L, we will now
describe a recursive mechanism for mapping an utterance x = (x1, . . . , xn) to ZL(x), a
set of candidate DCS trees for x. The basic approach is reminiscent of projective labeled
dependency parsing: For each span i..j of the utterance, we build a set of trees Ci,j(x).
The set of trees for the span 0..n is the final result:
ZL(x) = C0,n(x) (44)
Each set of DCS trees Ci,j(x) is constructed recursively by combining the trees of its
subspans Ci,k(x) and Ck?,j(x) for each pair of split points k, k
? (words between k and k?
are ignored). These combinations are then augmented via a function A and filtered via a
function F; these functions will be specified later. Formally, Ci,j(x) is defined recursively
as follows:
Ci,j(x) = F
(
A
(
{?p?i..j : p ? L(xi+1..j)} ?
?
i?k?k?<j
a?Ci,k(x)
b?Ck? ,j(x)
T1(a, b))
))
(45)
This recurrence has two parts:
 The base case: we take the phrase (sequence of words) over span i..j
and look up the set of predicates p in the set of lexical triggers. For each
predicate, we construct a one-node DCS tree. We also extend the definition
of DCS trees in Section 2.2 to allow each node to store the indices of the
span i..j that triggered the predicate at that node; this is denoted by ?p?i..j.
This span information will be useful in Section 3.1.1, where we will need
to talk about how an utterance x is aligned with a DCS tree z.
418
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
 The recursive case: T1(a, b), which we will define shortly, that takes two
DCS trees, a and b, and returns a set of new DCS trees formed by
combining a and b. Figure 17 shows this recurrence graphically.
We now focus on how to combine two DCS trees. Define Td(a, b) as the set of DCS
trees that result by making either a or b the root and connecting the other via a chain of
relations and at most d trace predicates (d is a small integer that keeps the set of DCS
trees manageable):
Td(a, b) = T
?
d
(a, b) ? T?
d
(b, a) (46)
Here, T
?
d
(a, b) is the set of DCS trees where a is the root; for T
?
d
(a, b), b is the root. The
former is defined recursively as follows:
T
?
0 (a, b) = ?, (47)
T
?
d
(a, b) =
?
r?R
p?L()
{?a.p; a.e; r :b? , ?a.p; a.e; r :?? :b??} ? T?
d?1(a, ?p; r :b?)
First, we consider all possible relations r ? R and try appending an edge to a with
relation r and child b (?a.p; a.e; r :b?); an aggregate relation ? can be inserted in addition
(?a.p; a.e; r :?? :b??). Of course, R contains an infinite number of join and execute rela-
tions, but only a small finite number of them make sense: We consider join relations
j
j? only for j ? {1, . . . , ARITY(a.p)} and j
? ? {1, . . . , ARITY(b.p)}, and execute relations Xi
for which i does not contain indices larger than the number of columns of bw. Next,
we further consider all possible trace predicates p ? L(	), and recursively try to connect
Figure 17
An example of the recursive construction of Ci,j(x), a set of DCS trees for span i..j.
419
Computational Linguistics Volume 39, Number 2
Figure 18
Given two DCS trees, a and b, T
?
1 (a, b) and T
?
1 (a, b) are the two sets of DCS trees formed by
combining a and bwith a at the root and b at the root, respectively; one trace predicate can be
inserted in between. In this example, the DCS trees which survive filtering (Section 2.6.3)
are shown.
awith the intermediate ?p; r :b?, now allowing d? 1 additional predicates. See Figure 18
for an example. In the other direction, T
?
d
is defined similarly:
T
?
0 (a, b) = ? (48)
T
?
d
(a, b) =
?
r?R
p?L()
{?b.p; r :a; b.e? , ?b.p; r :?? :a? ; b.e?} ? T?
d?1(a, ?p; r :b?)
Inserting trace predicates allows us to build logical forms with more predicates
than are explicitly triggered by the words. This ability is useful for several reasons.
Sometimes, there is a predicate not overtly expressed, especially in noun compounds
(e.g., California cities). For semantically light words such as prepositions (e.g., for) it is
difficult to enumerate all the possible predicates that they might trigger; it is simpler
computationally to try to insert trace predicates. We can even omit lexical triggers
for transitive verbs such as border because the corresponding predicate border can be
inserted as a trace predicate.
The function T1(a, b) connects two DCS trees via a path of relations and trace predi-
cates. The augmentation function A adds additional relations (specifically, E and/or Xi)
on a single DCS tree:
A(Z) =
?
z?Z
Xi?R
{z, ?z; E :???? , ?Xi :z? , ?Xi :?z; E :?????} (49)
2.6.3 Filtering using Abstract Interpretation. The construction procedure as described thus
far is extremely permissive, generating many DCS trees which are obviously wrong?
for example, ?state; 11 :?>;
2
1 ?3???, which tries to compare a state with the number 3. There
is nothing wrong with this expression syntactically: Its denotation will simply be empty
(with respect to the world). But semantically, this DCS tree is anomalous.
We cannot simply just discard DCS trees with empty denotations, because we
would incorrectly rule out ?state; 11 :?border;
2
1 ?AK???. The difference here is that even
though the denotation is empty in this world, it is possible that it might not be empty
420
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
in a different world where history and geology took another turn, whereas it is simply
impossible to compare cities and numbers.
Now let us quickly flesh out this intuition before falling into a philosophical dis-
cussion about possible worlds. Given a world w, we define an abstract world ?(w),
to be described shortly. We compute the denotation of a DCS tree z with respect to
this abstract world. If at any point in the computation we create an empty denotation,
we judge z to be impossible and throw it away. The filtering function F is defined as
follows:10
F(Z) = {z ? Z : ?z? subtree of z , z??(w).A = ?} (50)
Now we need to define the abstract world ?(w). The intuition is to map concrete
values to abstract values: 3 :length becomes ? :length,Oregon :state becomes ? :state,
and in general, primitive value x : t becomes ? : t. We perform abstraction on tuples
componentwise, so that (Oregon :state, 3 :length) becomes (? :state, ? :length). Our
abstraction of sets is slightly more complex: The empty set maps to the empty set, a set
containing values all with the same abstract value a maps to {a}, and a set containing
values with more than one abstract value maps to {MIXED}. Finally, a world maps each
predicate onto a set of (concrete) tuples; the corresponding abstract world maps each
predicate onto the set of abstract tuples. Formally, the abstraction function is defined as
follows:
?(x : t) = ? : t [primitive value] (51)
?((v1, . . . , vn)) = (?(v1), . . . ,?(vn)) [tuple] (52)
?(A) =
?
?
?
?
?
? if A = ?
{?(x) : x ? A} if |{?(x) : x ? A}| = 1
{MIXED} otherwise
[set] (53)
?(w) = ?p.{?(x) : x ? w(p)} [world] (54)
As an example, the abstract world might look like this:
?(w)(>) = {(? :number, ? :number, ? :number) (55)
(? :length, ? :length, ? :length), . . . }
?(w)(state) = {(? :state)} (56)
?(w)(AK) = {(? :state)} (57)
?(w)(border) = {(? :state, ? :state)} (58)
Now returning to our motivating example at the beginning of this section, we see
that the bad DCS tree has an empty abstract denotation ?state; 11 :?>;
2
1 ?3????(w) =
???; ???. The good DCS tree has a non-empty abstract denotation: ?state; 11 :?border;
2
1 ?AK????(w) = ??{(? :state)}; ???, as desired.
10 To further reduce the search space, F imposes a few additional constraints: for example, limiting the
number of columns to 2, and only allowing trace predicates between arity 1 predicates.
421
Computational Linguistics Volume 39, Number 2
Remarks. Computing denotations on an abstract world is called abstract interpretation
(Cousot and Cousot 1977) and is a very powerful framework commonly used in the
programming languages community. The idea is to obtain information about a program
(in our case, a DCS tree) without running it concretely, but rather just by running it
abstractly. It is closely related to type systems, but the type of abstractions one uses is
often much richer than standard type systems.
2.6.4 Comparison with CCG. We now compare our construction mechanism with CCG
(see Figure 19 for an example). The main difference is that our lexical triggers contain
less information than a lexical entry in a CCG. In CCG, the lexicon would have an entry
such as
major  N/N : ?f.?x.major(x) ? f (x) (59)
which gives detailed information about how this word should interact with its context.
In DCS construction, however, each lexical trigger only has the minimal amount of
information:
major  major (60)
A lexical trigger specifies a pre-theoretic ?meaning? of a word which does not commit
to any formalisms. One advantage of this minimality is that lexical triggers could be
easily obtained from non-expert supervision: One would only have to associate words
with database table names (predicates).
In some sense, the DCS construction mechanism pushes the complexity out of the
lexicon. In linguistics, this complexity usually would end up in the grammar, which
would be undesirable. We do not have to respect this tradeoff, however, because the
Figure 19
Comparison between the construction mechanisms of CCG and DCS. There are three principal
differences: First, in CCG, words are mapped onto lambda calculus expressions; in DCS, words
are just mapped onto predicates. Second, in CCG, lambda calculus expressions are built by
combining (e.g., via function application) two smaller expressions; in DCS, trees are combined
by inserting relations (and possibly other predicates between them). Third, in CCG, all words
map to logical expressions; in DCS, only a small subset of words (e.g., state and Texas) map to
predicates; the rest participate in features for scoring DCS trees.
422
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
construction mechanism only produces an overapproximation, which means it is possi-
ble to have both a simple ?lexicon? and a simple ?grammar.?
There is an important practical rationale for this design decision. During learning,
we never just have one clean lexical entry per word. Rather, there are often many
possible lexical entries (and to handle disfluent utterances or utterances in free word-
order languages, we might actually need many of them [Kwiatkowski et al 2010]):
major  N : ?x.major(x) (61)
major  N/N : ?f.?x.major(x) ? f (x) (62)
major  N\N : ?f.?x.major(x) ? f (x) (63)
. . . (64)
Now think of a DCS lexical trigger major  major as simply a compact representation for
a set of CCG lexical entries. Furthermore, the choice of the lexical entry is made not
at the initial lexical base case, but rather during the recursive construction by inserting
relations between DCS subtrees. It is exactly at this point that the choice can be made,
because after all, the choice is one that depends on context. The general principle is to
compactly represent the indeterminacy until one can resolve it. Compactly representing
a set of CCG lexical entries can also be done within the CCG framework by factoring
lexical entries into a lexeme and a lexical template (Kwiatkowski et al 2011).
Type raising is a combinator in CCG that traditionally converts x to ?f.f (x). In
recent work, Zettlemoyer and Collins (2007) introduced more general type-changing
combinators to allow conversion from one entity into a related entity in general (a
kind of generalized metonymy). For example, in order to parse Boston flights, Boston
is transformed to ?x.to(x, Boston). This type changing is analogous to inserting trace
predicates in DCS, but there is an important distinction: Type changing is a unary
operation and is unconstrained in that it changes logical forms into new ones without
regard for how they will be used downstream. Inserting trace predicates is a binary
operation that is constrained by the two predicates that it is mediating. In the example,
to would only be inserted to combine Boston with flight. This is another instance of
the general principle of delaying uncertain decisions until there is more information.
3. Learning
In Section 2, we defined DCS trees and a construction mechanism for producing a set
of candidate DCS trees given an utterance. We now define a probability distribution
over that set (Section 3.1) and an algorithm for estimating the parameters (Section 3.2).
The number of candidate DCS trees grows exponentially, so we use beam search to
control this growth. The final learning algorithm alternates between beam search and
optimization of the parameters, leading to a natural bootstrapping procedure which
integrates learning and search.
3.1 Semantic Parsing Model
The semantic parsing model specifies a conditional distribution over a set of candi-
date DCS trees C(x) given an utterance x. This distribution depends on a function
?(x, z) ? Rd, which takes a (x, z) pair and extracts a set of local features (see Section 3.1.1
423
Computational Linguistics Volume 39, Number 2
for a full specification). Associated with this feature vector is a parameter vector ? ? Rd.
The inner product between the two vectors, ?(x, z)?, yields a numerical score, which
intuitively measures the compatibility of the utterance x with the DCS tree z. We expo-
nentiate the score and normalize over C(x) to obtain a proper probability distribution:
p(z | x;C,?) = exp{?(x, z)??A(?; x,C)} (65)
A(?; x,C) = log
?
z?C(x)
exp{?(x, z)?} (66)
where A(?; x,C) is the log-partition function with respect to the candidate set function
C(x).
3.1.1 Features.We now define the feature vector ?(x, z) ? Rd, the core part of the seman-
tic parsing model. Each component j = 1, . . . , d of this vector is a feature, and ?(x, z)j
is the number of times that feature occurs in (x, z). Rather than working with indices,
we treat features as symbols (e.g., TRIGGERPRED[states, state]). Each feature captures
some property about (x, z) that abstracts away from the details of the specific instance
and allows us to generalize to new instances that share common features.
The features are organized into feature templates, where each feature template
instantiates a set of features. Figure 20 shows all the feature templates for a concrete
example. The feature templates are as follows:
 PREDHIT contains the single feature PREDHIT, which fires for each
predicate in z.
 PRED contains features {PRED[?(p)] : p ? P}, each of which fires on
?(p), the abstraction of predicate p, where
?(p) =
{
? : t if p = x : t
p otherwise
(67)
The purpose of the abstraction is to abstract away the details of concrete
values such as TX = Texas :state.
 PREDREL contains features {PREDREL[?(p),q] : p ? P ,q ? ({?,?}?
R)?}. A feature fires when a node x has predicate p and is connected via
some path q = (d1, r1), . . . , (dm, rm) to the lowest descendant node ywith
the property that each node between x and y has a null predicate. Each
(d, r) on the path represents an edge labeled with relation r connecting
to a left (d =?) or right (d =?) child. If x has no children, then m = 0.
The most common case is when m = 1, but m = 2 also occurs with the
aggregate and execute relations (e.g., PREDREL[count,? 11? ?] fires
for Figure 5(a)).
 PREDRELPRED contains features {PREDRELPRED[?(p),q,?(p?)] : p, p? ?
P ,q ? ({?,?}?R)?}, which are the same as PREDREL, except that we
include both the predicate p of x and the predicate p? of the descendant
node y. These features do not fire if m = 0.
424
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 20
For each utterance?DCS tree pair (x, z), we define a feature vector ?(x, z), whose j-th component
is the number of times a feature j occurs in (x, z). Each feature has an associated parameter ?j,
which is estimated from data in Section 3.2. The inner product of the feature vector and
parameter vector yields a compatibility score.
 TRIGGERPRED contains features {TRIGGERPRED[s, p] : s ?W?, p ? P},
whereW = {it,Texas, . . . } is the set of words. Each of these features fires
when a span of the utterance with words s triggers the predicate p?more
precisely, when a subtree ?p; e?i..j exists with s = xi+1..j. Note that these
lexicalized features use the predicate p rather than the abstracted
version ?(p).
 TRACEPRED contains features {TRACEPRED[s, p, d] : s ?W?, p ? P , d ?
{?,?}}, each of which fires when a trace predicate p has been inserted
425
Computational Linguistics Volume 39, Number 2
over a word s. The situation is the following: Suppose we have a subtree
a that ends at position k (there is a predicate in a that is triggered by a
phrase with right endpoint k) and another subtree b that begins at k?.
Recall that in the construction mechanism (46), we can insert a trace
predicate p ? L(	) between the roots of a and b. Then, for every word
xj between the spans of the two subtrees ( j = {k+ 1, . . . , k?}), the
feature TRACEPRED[xj, p, d] fires (d =? if b dominates a and d =?
if a dominates b).
 TRACEREL contains features {TRACEREL[s, d, r] : s ?W?, d ? {?,?}, r ?
R}, each of which fires when some trace predicate with parent relation r
has been inserted over a word s.
 TRACEPREDREL contains features {TRACEPREDREL[s, p, d, r] : s ?W?,
p ? P , d ? {?,?}, r ? R}, each of which fires when a predicate p is
connected via child relation r to some trace predicate over a word s.
These features are simple generic patterns which can be applied for modeling
essentially any distribution over sequences and labeled trees?there is nothing spe-
cific to DCS at all. The first half of the feature templates (PREDHIT, PRED, PREDREL,
PREDRELPRED) capture properties of the tree independent of the utterance, and
are similar to those used for syntactic dependency parsing. The other feature tem-
plates (TRIGGERPRED, TRACEPRED, TRACEREL, TRACEPREDREL) connect predicates
in the DCS tree with words in the utterance, similar to those in a model of machine
translation.
3.2 Parameter Estimation
We have now fully specified the details of the graphical model in Figure 2: Section 3.1
described semantic parsing and Section 2 described semantic evaluation. Next, we focus
on the inferential problem of estimating the parameters ? of the model from data.
3.2.1 Objective Function.We assume that our learning algorithm is given a training data
setD containing question?answer pairs (x, y). Because the logical forms are unobserved,
we work with log p(y | x;C,?), the marginal log-likelihood of obtaining the correct
answer y given an utterance x. This marginal log-likelihood sums over all z ? C(x) that
evaluate to y:
log p(y | x;C,?) = log p(z ? Cy(x) | x;C,?) (68)
= A(?; x,Cy)?A(?, x,C), where (69)
Cy(x)
def
= {z ? C(x) : zw = y} (70)
Here, Cy(x) is the set of DCS trees z with denotation y.
We call an example (x, y) ? D feasible if the candidate set of x contains a DCS
tree that evaluates to y (Cy(x) = ?). Define an objective function O(?,C) containing
two terms. The first term is the sum of the marginal log-likelihood over all feasible
426
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
training examples. The second term is a quadratic penalty on the parameters ? with
regularization parameter ?. Formally:
O(?,C) def=
?
(x,y)?D
Cy(x)=?
log p(y | x;C,?)? ?
2
???22 (71)
=
?
(x,y)?D
Cy(x)=?
(A(?; x,Cy)?A(?; x,C))? ?
2
???22
We would like to maximize O(?,C). The log-partition function A(?; ?, ?) is convex,
but O(?,C) is the difference of two log-partition functions and hence is not concave
(nor convex). Thus we resort to gradient-based optimization. A standard result is that
the derivative of the log-partition function is the expected feature vector (Wainwright
and Jordan 2008). Using this, we obtain the gradient of our objective function:11
?O(?,C)
??
=
?
(x,y)?D
Cy(x)=?
(
Ep(z|x;Cy,?)[?(x, z)]? Ep(z|x;C,?)[?(x, z)]
)
? ?? (72)
Updating the parameters in the direction of the gradient would move the parameters
towards the DCS trees that yield the correct answer (Cy) and away from overall can-
didate DCS trees (C). We can use any standard numerical optimization algorithm that
requires only black-box access to a gradient. Section 4.3.4 will discuss the empirical
ramifications of the choice of optimization algorithm.
3.2.2 Algorithm. Given a candidate set function C(x), we can optimize Equation (71) to
obtain estimates of the parameters ?. Ideally, we would use C(x) = ZL(x), the candidate
sets from our construction mechanism in Section 2.6, but we quickly run into the prob-
lem of computing Equation (72) efficiently. Note that ZL(x) (defined in Equation (44))
grows exponentially with the length of x. This by itself is not a show-stopper. Our
features (Section 3.1.1) decompose along the edges of the DCS tree, so it is possible
to use dynamic programming12 to compute the second expectation Ep(z|x;ZL,?)[?(x, z)]
of Equation (72). The problem is computing the first expectation Ep(z|x;ZyL ,?)
[?(x, z)],
which sums over the subset of candidate DCS trees z satisfying the constraint zw = y.
Though this is a smaller set, there is no efficient dynamic program for this set because
the constraint does not decompose along the structure of the DCS tree. Therefore, we
need to approximate ZyL , and, in fact, we will approximate ZL as well so that the two
expectations in Equation (72) are coherent.
Recall that ZL(x) was built by recursively constructing a set of DCS trees Ci,j(x)
for each span i..j. In our approximation, we simply use beam search, which truncates
each Ci,j(x) to include the (at most) K DCS trees with the highest score ?(x, z)
?. We
11 Notation: Ep(x)[f (x)] =
?
x p(x)f (x).
12 The state of the dynamic program would be the span i..j and the head predicate over that span.
427
Computational Linguistics Volume 39, Number 2
let C?i,j,?(x) denote this approximation and define the set of candidate DCS trees with
respect to the beam search:
Z?L,?(x) = C?0,n,?(x) (73)
We now have a chicken-and-egg problem: If we had good parameters ?, we
could generate good candidate sets C(x) using beam search Z?L,?(x). If we had good
candidate sets C(x), we could generate good parameters by optimizing our objective
O(?,C) in Equation (71). This problem leads to a natural solution: simply alternate
between the two steps (Figure 21). This procedure is not guaranteed to converge, due
to the heuristic nature of the beam search, but we have found it to be convergent in
practice.
Finally, we use the trained model with parameters ? to answer new questions x by
choosing the most likely answer y, summing out the latent logical form z:
F?(x)
def
= argmax
y
p(y | x;?, Z?L,?) (74)
= argmax
y
?
z?Z?L,?(x)
zw=y
p(z | x;?, Z?L,?) (75)
4. Experiments
We have now completed the conceptual part of this article?using DCS trees to rep-
resent logical forms (Section 2), and learning a probabilistic model over these trees
(Section 3). In this section, we evaluate and study our approach empirically. Our
main result is that our system can obtain comparable accuracies to state-of-the-art
systems that require annotated logical forms. All the code and data are available at
cs.stanford.edu/~pliang/software/.
4.1 Experimental Set-up
We first describe the data sets (Section 4.1.1) that we use to train and evaluate our
system. We then mention various choices in the model and learning algorithm (Sec-
tion 4.1.2). One of these choices is the lexical triggers, which are further discussed in
Section 4.1.3.
Figure 21
The learning algorithm alternates between updating the candidate sets based on beam search
and updating the parameters using standard numerical optimization.
428
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
4.1.1 Data sets. We tested our methods on two standard data sets, referred to in this
article as GEO and JOBS. These data sets were created by Ray Mooney?s group during
the 1990s and have been used to evaluate semantic parsers for over a decade.
U.S. Geography. The GEO data set, originally created by Zelle and Mooney (1996), con-
tains 880 questions about U.S. geography and a database of facts encoded in Prolog. The
questions in GEO ask about general properties (e.g., area, elevation, and population) of
geographical entities (e.g., cities, states, rivers, andmountains). Across all the questions,
there are 280 word types, and the length of an utterance ranges from 4 to 19 words,
with an average of 8.5 words. The questions involve conjunctions, superlatives, and
negation, but no generalized quantification. Each question is annotated with a logical
form in Prolog, for example:
Utterance: What is the highest point in Florida?
Logical form: answer(A,highest(A,(place(A),loc(A,B),const(B,stateid(florida)))))
Because our approach learns from answers, not logical forms, we evaluated the
annotated logical forms on the provided database to obtain the correct answers.
Recall that a world/database w maps each predicate p ? P to a set of tuples w(p).
Some predicates contain the set of tuples explicitly (e.g., mountain); others can be
derived (e.g., higher takes two entities x and y and returns true if elevation(x) >
elevation(y)). Other predicates are higher-order (e.g., sum, highest) in that they take
other predicates as arguments. We do not use the provided domain-specific higher-
order predicates (e.g., highest), but rather provide domain-independent higher-order
predicates (e.g., argmax) and the ordinary domain-specific predicates (e.g., elevation).
This provides more compositionality and therefore better generalization. Similarly, we
use more and elevation instead of higher. Altogether, P contains 43 predicates plus
one predicate for each value (e.g., CA).
Job Queries. The JOBS data set (Tang and Mooney 2001) contains 640 natural language
queries about job postings. Most of the questions ask for jobs matching various criteria:
job title, company, recruiter, location, salary, languages and platforms used, areas of
expertise, required/desired degrees, and required/desired years of experience. Across
all utterances, there are 388 word types, and the length of an utterance ranges from 2 to
23 words, with an average of 9.8 words.
The utterances are mostly based on conjunctions of criteria, with a sprinkling of
negation and disjunction. Here is an example:
Utterance: Are there any jobs using Java that are not with IBM?
Logical form: answer(A,(job(A),language(A,?java?),?company(A,?IBM?)))
The JOBS data set comes with a database, which we can use as the world w. When
the logical forms are evaluated on this database, however, close to half of the answers
are empty (no jobs match the requested criteria). Therefore, there is a large discrepancy
between obtaining the correct logical form (which has been the focus of most work on
semantic parsing) and obtaining the correct answer (our focus).
To bring these two into better alignment, we generated a random database as
follows: We created m = 100 jobs. For each job j, we go through each predicate p (e.g.,
company) that takes two arguments, a job, and a target value. For each of the possible
target values v, we add (j, v) to w(p) independently with probability ? = 0.8. For exam-
ple, for p = company, j = job37, we might add (job37, IBM) to w(company). The result is
429
Computational Linguistics Volume 39, Number 2
a database with a total of 23 predicates (which includes the domain-independent ones)
in addition to the value predicates (e.g., IBM).
The goal of using randomness is to ensure that two different logical forms will most
likely yield different answers. For example, consider two logical forms:
z1 = ?j.job( j) ? company( j, IBM), (76)
z2 = ?j.job( j) ? language( j, Java). (77)
Under the random construction, the denotation of z1 is S1, a random subset of the jobs,
where each job is included in S1 independently with probability ?, and the denotation
of z2 is S2, which has the same distribution as S1 but importantly is independent of S1.
Therefore, the probability that S1 = S2 is [?
2 + (1? ?)2]m, which is exponentially small
in m. This construction yields a world that is not entirely ?realistic? (a job might have
multiple employers), but it ensures that if we get the correct answer, we probably also
obtain the correct logical form.
4.1.2 Settings. There are a number of settings that control the tradeoffs between compu-
tation, expressiveness, and generalization power of our model, shown here. For now,
we will use generic settings chosen rather crudely; Section 4.3.4 will explore the effect
of changing these settings.
Lexical Triggers The lexical triggers L (Section 2.6.1) define the set of candidate DCS
trees for each utterance. There is a tradeoff between expressiveness and computa-
tional complexity: The more triggers we have, the more DCS trees we can consider
for a given utterance, but then either the candidate sets become too large or beam
search starts dropping the good DCS trees. Choosing lexical triggers is important
and requires additional supervision (Section 4.1.3).
Features Our probabilistic semantic parsing model is defined in terms of feature tem-
plates (Section 3.1.1). Richer features increase expressiveness but also might lead
to overfitting. By default, we include all the feature templates.
Number of training examples (n) An important property of any learning algorithm is
its sample complexity?how many training examples are required to obtain a
certain level of accuracy? By default, all training examples are used.
Number of training iterations (T) Our learning algorithm (Figure 21) alternates be-
tween updating candidate sets and updating parameters for T iterations. We use
T = 5 as the default value.
Beam size (K) The computation of the candidate sets in Figure 21 is based on beam
search where each intermediate state keeps at most K DCS trees. The default value
is K = 100.
Optimization algorithm To optimize the objective functionO(?,C) our default is to use
the standard L-BFGS algorithm (Nocedal 1980) with a backtracking line search for
choosing the step size.
Regularization (?) The regularization parameter ? > 0 in the objective functionO(?,C)
is another knob for controlling the tradeoff between fitting and overfitting. The
default is ? = 0.01.
4.1.3 Lexical Triggers. The lexical trigger set L (Section 2.6.1) is a set of entries (s, p), where
s is a sequence of words and p is a predicate. We run experiments on two sets of lexical
triggers: base triggers LB and augmented triggers LB+P.
430
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Base Triggers. The base trigger set LB includes three types of entries:
 Domain-independent triggers: For each domain-independent predicate
(e.g., argmax), we manually specify a few words associated with that
predicate (e.g., most). The full list is shown at the top of Figure 22.
 Values: For each value x that appears in the world (specifically,
x ? vj ? w(p) for some tuple v, index j, and predicate p), LB contains an
entry (x, x) (e.g., (Boston,Boston :city)). Note that this rule implicitly
specifies an infinite number of triggers.
Regarding predicate names, we do not add entries such as (city, city),
because we want our system to be language-independent. In Turkish,
for instance, we would not have the luxury of lexicographical cues that
associate citywith s?ehir. So we should think of the predicates as just
symbols predicate1, predicate2, and so on. On the other hand, values
in the database are generally proper nouns (e.g., city names) for which
there are generally strong cross-linguistic lexicographic similarities.
 Part-of-speech (POS) triggers:13 For each domain-specific predicate p,
we specify a set of POS tags T. Implicitly, LB contains all pairs (x, p) where
the word x has a POS tag t ? T. For example, for city, we would specify
NN and NNS, which means that any word which is a singular or plural
common noun triggers the predicate city. Note that city triggers city as
desired, but state also triggers city.
The POS triggers for GEO and JOBS domains are shown in the left side of
Figure 22. Note that some predicates such as traverse and loc are
not associated with any POS tags. Predicates corresponding to verbs and
prepositions are not included as overt lexical triggers, but rather included
as trace predicates L(	). In constructing the logical forms, nouns and
adjectives serve as anchor points. Trace predicates can be inserted between
these anchors. This strategy is more flexible than requiring each predicate
to spring from some word.
Augmented Triggers.We nowdefine the augmented trigger set LB+P, which containsmore
domain-specific information than LB. Specifically, for each domain-specific predicate
(e.g., city), we manually specify a single prototype word (e.g., city) associated with
that predicate. Under LB+P, city would trigger only city because city is a prototype
word, but townwould trigger all the NN predicates (city, state, country, etc.) because
it is not a prototype word.
Prototype triggers require only a modest amount of domain-specific supervision
(see the right side of Figure 22 for the entire list for GEO and JOBS). In fact, as we?ll see
in Section 4.2, prototype triggers are not absolutely required to obtain good accuracies,
but they give an extra boost and also improve computational efficiency by reducing the
set of candidate DCS trees.
13 To perform POS tagging, we used the Berkeley Parser (Petrov et al 2006), trained on the WSJ Treebank
(Marcus, Marcinkiewicz, and Santorini 1993) and the Question Treebank (Judge, Cahill, and v. Genabith
2006)?thanks to Slav Petrov for providing the trained parser.
431
Computational Linguistics Volume 39, Number 2
Figure 22
Lexical triggers used in our experiments.
Finally, to determine triggering, we stem all words using the Porter stemmer (Porter
1980), so that mountains triggers the same predicates as mountain. We also decompose
superlatives into two words (e.g., largest is mapped to most large), allowing us to con-
struct the logical form more compositionally.
4.2 Comparison with Other Systems
We now compare our approach with existing methods. We used the same training-test
splits as Zettlemoyer and Collins (2005) (600 training and 280 test examples for GEO,
500 training and 140 test examples for JOBS). For development, we created five random
splits of the training data. For each split, we put 70% of the examples into a development
training set and the remaining 30% into a development test set. The actual test set was
only used for obtaining final numbers.
4.2.1 Systems that Learn from Question?Answer Pairs.We first compare our system (hence-
forth, LJK11) with Clarke et al (2010) (henceforth, CGCR10), which is most similar to
our work in that it also learns from question?answer pairs without using annotated
logical forms. CGCR10 works with the FunQL language and casts semantic parsing as
integer linear programming (ILP). In each iteration, the learning algorithm solves the
432
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 2
Results on GEO with 250 training and 250 test examples. Our system (LJK11 with base triggers
and no logical forms) obtains higher test accuracy than CGCR10, even when CGCR10 is trained
using logical forms.
System Accuracy (%)
CGCR10 w/answers (Clarke et al 2010) 73.2
CGCR10 w/logical forms (Clarke et al 2010) 80.4
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) 84.0
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) 87.6
ILP to predict the logical form for each training example. The examples with correct
predictions are fed to a structural support vector machine (SVM) and the model param-
eters are updated.
Though similar in spirit, there are some important differences between CGCR10
and our approach. They use ILP instead of beam search and structural SVM instead of
log-linear models, but the main difference is which examples are used for learning. Our
approach learns on any feasible example (Section 3.2.1), one where the candidate set
contains a logical form that evaluates to the correct answer. CGCR10 uses a much more
stringent criterion: The highest scoring logical formmust evaluate to the correct answer.
Therefore, for their algorithm to progress, the model already must be non-trivially good
before learning even starts. This is reflected in the amount of prior knowledge and
initialization that CGCR10 uses before learning starts: WordNet features, syntactic parse
trees, and a set of lexical triggers with 1.42 words per non-value predicate. Our system
with base triggers requires only simple indicator features, POS tags, and 0.5 words per
non-value predicate.
CGCR10 created a version of GEO which contains 250 training and 250 test exam-
ples. Table 2 compares the empirical results of this split. We see that our system (LJK11)
with base triggers significantly outperforms CGCR10 (84% vs. 73.2%), and it even
outperforms the version of CGCR10 that is trained using logical forms (84.0% vs. 80.4%).
If we use augmented triggers, we widen the gap by another 3.6 percentage points.14
4.2.2 State-of-the-Art Systems. We now compare our system (LJK11) with state-of-the-
art systems, which all require annotated logical forms (except PRECISE). Here is a brief
overview of the systems:
 COCKTAIL (Tang and Mooney 2001) uses inductive logic programming to
learn rules for driving the decisions of a shift-reduce semantic parser. It
assumes that a lexicon (mapping from words to predicates) is provided.
 PRECISE (Popescu, Etzioni, and Kautz 2003) does not use learning, but
instead relies on matching words to strings in the database using various
heuristics based on WordNet and the Charniak parser. Like our work, it
also uses database type constraints to rule out spurious logical forms. One
of the unique features of PRECISE is that it has 100% precision?it refuses
to parse an utterance which it deems semantically intractable.
14 Note that the numbers for LJK11 differ from those presented in Liang, Jordan, and Klein (2011), which
reports results based on 10 different splits rather than the set-up used by CGCR10.
433
Computational Linguistics Volume 39, Number 2
 SCISSOR (Ge and Mooney 2005) learns a generative probabilistic model
that extends the Collins (1999) models with semantic labels, so
that syntactic and semantic parsing can be done jointly.
 SILT (Kate, Wong, and Mooney 2005) learns a set of transformation rules
for mapping utterances to logical forms.
 KRISP (Kate and Mooney 2006) uses SVMs with string kernels to drive the
local decisions of a chart-based semantic parser.
 WASP (Wong and Mooney 2006) uses log-linear synchronous grammars to
transform utterances into logical forms, starting with word alignments
obtained from the IBM models.
 ?-WASP (Wong and Mooney 2007) extends WASP to work with logical
forms that contain bound variables (lambda abstraction).
 LNLZ08 (Lu et al 2008) learns a generative model over hybrid trees,
which are logical forms augmented with natural language words.
IBM model 1 is used to initialize the parameters, and a discriminative
reranking step works on top of the generative model.
 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear
model over CCG derivations. Starting with a manually constructed
domain-independent lexicon, the training procedure grows the lexicon
by adding lexical entries derived from associating parts of an utterance
with parts of the annotated logical form.
 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra
(disharmonic) combinators to increase the expressive power of the model.
 KZGS10 (Kwiatkowski et al 2010) uses a restricted higher-order
unification procedure, which iteratively breaks up a logical form into
smaller pieces. This approach gradually adds lexical entries of increasing
generality, thus obviating the need for the manually specified templates
used by ZC05 and ZC07 for growing the lexicon. IBM model 1 is used to
initialize the parameters.
 KZGS11 (Kwiatkowski et al 2011) extends KZGS10 by factoring lexical
entries into a template plus a sequence of predicates that fill the slots of
the template. This factorization improves generalization.
With the exception of PRECISE, all other systems require annotated logical forms,
whereas our system learns only from annotated answers. On the other hand, our system
does rely on a fewmanually specified lexical triggers, whereasmany of the later systems
essentially require no manually crafted lexica. For us, the lexical triggers play a crucial
role in the initial stages of learning because they constrain the set of candidate DCS
trees; otherwise we would face a hopelessly intractable search problem. The other
systems induce lexica using unsupervised word alignment (Wong and Mooney 2006,
2007; Kwiatkowski et al 2010, 2011) and/or on-line lexicon learning (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010, 2011). Unfortunately, we cannot use these
automatic techniques because they rely on having annotated logical forms.
Table 3 shows the results for GEO. Semantic parsers are typically evaluated on
the accuracy of the logical forms: precision (the accuracy on utterances which are
434
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 3
Results on GEO: Logical form accuracy (LF) and answer accuracy (Answer) of the various
systems. The first group of systems are evaluated using 10-fold cross-validation on all 880
examples; the second are evaluated on the 680+ 200 split of Zettlemoyer and Collins (2005).
Our system (LJK11) with base triggers obtains comparable accuracy to past work, whereas
with augmented triggers, our system obtains the highest overall accuracy.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 77.5 77.5
SCISSOR (Ge and Mooney 2005) 72.3 ?
SILT (Kate, Wong, and Mooney 2005) 54.1 ?
KRISP (Kate and Mooney 2006) 71.7 ?
WASP (Wong and Mooney 2006) 74.8 ?
?-WASP (Wong and Mooney 2007) 86.6 ?
LNLZ08 (Lu et al 2008) 81.8 ?
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
ZC07 (Zettlemoyer and Collins 2007) 86.1 ?
KZGS10 (Kwiatkowski et al 2010) 88.2 88.9
KZGS11 (Kwiatkowski et al 2010) 88.6 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 87.9
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 91.4
successfully parsed) and recall (the accuracy on all utterances). We only focus on recall
(a lower bound on precision) and simply use the word accuracy to refer to recall.15 Our
system is evaluated only on answer accuracy because our model marginalizes out the
latent logical form. All other systems are evaluated on the accuracy of logical forms. To
calibrate, we also evaluated KZGS10 on answer accuracy and found that it was quite
similar to its logical form accuracy (88.9% vs. 88.2%).16 This does not imply that our
system would necessarily have a high logical form accuracy because multiple logical
forms can produce the same answer, and our system does not receive a training signal
to tease them apart. Even with only base triggers, our system (LJK11) outperforms all
but two of the systems, falling short of KZGS10 by only one percentage point (87.9% vs.
88.9%).17 With augmented triggers, our system takes the lead (91.4% vs. 88.9%).
Table 4 shows the results for JOBS. The two learning-based systems (COCKTAIL
and ZC05) are actually outperformed by PRECISE, which is able to use strong database
type constraints. By exploiting this information and doing learning, we obtain the best
results.
4.3 Empirical Properties
In this section, we try to gain intuition into properties of our approach. All experiments
in this section were performed on random development splits. Throughout this section,
?accuracy? means development test accuracy.
15 Our system produces a logical form for every utterance, and thus our precision is the same as our recall.
16 The 88.2% corresponds to 87.9% in Kwiatkowski et al (2010). The difference is due to using a slightly
newer version of the code.
17 The 87.9% and 91.4% correspond to 88.6% and 91.1% in Liang, Jordan, and Klein (2011). These differences
are due to minor differences in the code.
435
Computational Linguistics Volume 39, Number 2
Table 4
Results on JOBS: Both PRECISE and our system use database type constraints, which results in a
decisive advantage over the other systems. In addition, LJK11 incorporates learning and
therefore obtains the highest accuracies.
System LF (%) Answer (%)
COCKTAIL (Tang and Mooney 2001) 79.4 ?
PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0
ZC05 (Zettlemoyer and Collins 2005) 79.3 ?
LJK11 w/base triggers (Liang, Jordan, and Klein 2011) ? 90.7
LJK11 w/augmented triggers (Liang, Jordan, and Klein 2011) ? 95.0
4.3.1 Error Analysis. To understand the type of errors our system makes, we examined
one of the development runs, which had 34 errors on the test set. We classified these
errors into the following categories (the number of errors in each category is shown in
parentheses):
 Incorrect POS tags (8): GEO is out-of-domain for our POS tagger, so the
tagger makes some basic errors that adversely affect the predicates that
can be lexically triggered. For example, the questionWhat states border
states . . . is tagged as WP VBZ NN NNS . . . , which means that the first states
cannot trigger state. In another example, major river is tagged as NNP
NNP, so these cannot trigger the appropriate predicates either, and thus
the desired DCS tree cannot even be constructed.
 Non-projectivity (3): The candidate DCS trees are defined by a projective
construction mechanism (Section 2.6) that prohibits edges in the DCS
tree from crossing. This means we cannot handle utterances such as
largest city by area, because the desired DCS tree would have city
dominating area dominating argmax. To construct this DCS tree,
we could allow local reordering of the words.
 Unseen words (2): We never saw at least or sea level at training time.
The former has the correct lexical trigger, but not a sufficiently large
feature weight (0) to encourage its use. For the latter, the problem is
more structural: We have no lexical triggers for 0 :length, and only
adding more lexical triggers can solve this problem.
 Wrong lexical triggers (7): Sometimes the error is localized to a single
lexical trigger. For example, the model incorrectly thinksMississippi
is the state rather than the river, and that Rochester is the city in
New York rather than the name, even though there are contextual
cues to disambiguate in these cases.
 Extra words (5): Sometimes, words trigger predicates that should be
ignored. For example, for population density, the first word triggers
population, which is used rather than density.
 Over-smoothing of DCS tree (9): The first half of our features (Figure 20)
are defined on the DCS tree alone; these produce a form of smoothing
that encourages DCS trees to look alike regardless of the words. We found
436
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
several instances where this essential tool for generalization went too
far. For example, in state of Nevada, the trace predicate border is inserted
between the two nouns, because it creates a structure more similar to
that of the common question what states border Nevada?
4.3.2 Visualization of Features.Having analyzed the behavior of our system for individual
utterances, let us move from the token level to the type level and analyze the learned
parameters of our model. We do not look at raw feature weights, because there are
complex interactions between them not represented by examining individual weights.
Instead, we look at expected feature counts, which we think are more interpretable.
Consider a group of ?competing? features J, for example J = {TRIGGERPRED[city,
p] : p ? P}. We define a distribution q(?) over J as follows:
q( j) =
Nj
?
j??J Nj?
, where (78)
Nj =
?
(x,y)?D
Ep(z|x,Z?L,?,?)[?(x, z)]
Think of q( j) as a marginal distribution (because all our features are positive) that
represents the relative frequencies with which the features j ? J fire with respect to
our training data set D and trained model p(z | x, Z?L,?,?). To appreciate the difference
between what this distribution and raw feature weights capture, suppose we had two
features, j1 and j2, which are identical (?(x, z)j1 ? ?(x, z)j2 ). The weights would be split
across the two features, but the features would have the same marginal distribution
(q(j1) = q(j2)). Figure 23 shows some of the feature distributions learned.
4.3.3 Learning, Search, Bootstrapping. Recall from Section 3.2.1 that a training example
is feasible (with respect to our beam search) if the resulting candidate set contains a
DCS tree with the correct answer. Infeasible examples are skipped, but an example may
become feasible in a later iteration. A natural question is how many training examples
are feasible in each iteration. Figure 24 shows the answer: Initially, only around 30% of
the training examples are feasible; this is not surprising given that all the parameters
are zero, so our beam search is essentially unguided. Training on just these examples
improves the parameters, however, and over the next few iterations, the number of
feasible examples steadily increases to around 97%.
In our algorithm, learning and search are deeply intertwined. Search is of course
needed to learn, but learning also improves search. The general approach is similar in
spirit to Searn (Daume, Langford, andMarcu 2009), althoughwe do not have any formal
guarantees at this point.
Our algorithm also has a bootstrapping flavor. The ?easy? examples are processed
first, where easy is defined by the ability of beam search to generate the correct answer.
This bootstrapping occurs quite naturally: Unlikemost bootstrapping algorithms, we do
not have to set a confidence threshold for accepting new training examples, something
that can be quite tricky to do. Instead, our threshold falls out of the discrete nature of
the beam search.
4.3.4 Effect of Various Settings. So far, we have used our approach with default settings
(Section 4.1.2). How sensitive is the approach to these choices? Table 5 shows the impact
of the feature templates. Figure 25 shows the effect of the number of training examples,
437
Computational Linguistics Volume 39, Number 2
Figure 23
Learned feature distributions. In a feature group (e.g., TRIGGERPRED[city, ?]), each feature is
associated with the marginal probability that the feature fires according to Equation (78). Note
that we have successfully learned that citymeans city, but incorrectly learned that sparsemeans
elevation (due to the confounding fact that Alaska is the most sparse state and has the highest
elevation).
number of training iterations, beam size, and regularization parameter. The overall
conclusion is that there are no big surprises: Our default settings could be improved
on slightly, but these differences are often smaller than the variation across different
development splits.
We now consider the choice of optimization algorithm to update the parameters
given candidate sets (see Figure 21). Thus far, we have been using L-BFGS (Nocedal
1980), which is a batch algorithm. Each iteration, we construct the candidate
Figure 24
The fraction of feasible training examples increases steadily as the parameters, and thus the
beam search improves. Each curve corresponds to a run on a different development split.
438
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Table 5
There are two classes of feature templates: lexical features (TRIGGERPRED,TRACE*) and
non-lexical features (PREDREL,PREDRELPRED). The lexical features are relatively much more
important for obtaining good accuracy (76.4% vs. 23.1%), but adding the non-lexical features
makes a significant contribution as well (84.7% vs. 76.4%).
Features Accuracy (%)
PRED 13.4? 1.6
PRED + PREDREL 18.4? 3.5
PRED + PREDREL + PREDRELPRED 23.1? 5.0
PRED + TRIGGERPRED 61.3? 1.1
PRED + TRIGGERPRED + TRACE* 76.4? 2.3
PRED + PREDREL + PREDRELPRED + TRIGGERPRED + TRACE* 84.7? 3.5
sets C(t)(x) for all the training examples before solving the optimization problem
argmax?O(?,C
(t) ). We now consider an on-line algorithm, stochastic gradient descent
(SGD) (Robbins and Monro 1951), which updates the parameters after computing
the candidate set for each example. In particular, we iteratively scan through the
training examples in a random order. For each example (x, y), we compute the
candidate set using beam search. We then update the parameters in the direction of
the gradient of the marginal log-likelihood for that example (see Equation (72)) with
step size t??:
?(t+1) ? ?(t) + t??
(
? log p(y | x; Z?L,?(t) ,?)
??
?
?
?
?=?(t)
)
(79)
The trickiest aspect of using SGD is selecting the correct step size: A small ? leads to
quick progress but also instability; a large ? leads to the opposite. We let L-BFGS and
SGD both take the same number of iterations (passes over the training set). Figure 26
shows that a very small value of ? (less than 0.2) is best for our task, even though
only values between 0.5 and 1 guarantee convergence. Our setting is slightly different
because we are interleaving the SGD updates with beam search, which might also
lead to unpredictable consequences. Furthermore, the non-convexity of the objective
function exacerbates the unpredictability (Liang and Klein 2009). Nonetheless, with
a proper ?, SGD converges much faster than L-BFGS and even to a slightly better
solution.
5. Discussion
The work we have presented in this article addresses three important themes. The
first theme is semantic representation (Section 5.1): How do we parametrize the mapping
from utterances to their meanings? The second theme is program induction (Section 5.2):
How do we efficiently search through the space of logical structures given a weak
feedback signal? Finally, the last theme is grounded language (Section 5.3): Howdowe use
constraints from the world to guide learning of language and conversely use language
to interact with the world?
439
Computational Linguistics Volume 39, Number 2
Figure 25
(a) The learning curve shows test accuracy as the number of training examples increases; about
300 examples suffices to get around 80% accuracy. (b) Although our algorithm is not guaranteed
to converge, the test accuracy is fairly stable (with one exception) with more training
iterations?hardly any overfitting occurs. (c) As the beam size increases, the accuracy increases
monotonically, although the computational burden also increases. There is a small gain from our
default setting of K = 100 to the more expensive K = 300. (d) The accuracy is relatively
insensitive to the choice of the regularization parameter for a wide range of values. In fact, no
regularization is also acceptable. This is probably because the features are simple, and the lexical
triggers and beam search already provide some helpful biases.
5.1 Semantic Representation
Since the late nineteenth century, philosophers and linguists have worked on elucidat-
ing the relationship between an utterance and its meaning. One of the pillars of formal
semantics is Frege?s principle of compositionality, that the meaning of an utterance
is built by composing the meaning of its parts. What these parts are and how they
are composed is the main question. The dominant paradigm, which stems from the
seminal work of Richard Montague (1973) in the early 1970s, states that parts are
lambda calculus expressions that correspond to syntactic constituents, and composition
is function application.
440
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Figure 26
(a) Given the same number of iterations, compared to default batch algorithm (L-BFGS),
the on-line algorithm (stochastic gradient descent) is slightly better for aggressive step
sizes (small ?) and worse for conservative step sizes (large ?). (b) The on-line algorithm
(with an appropriate choice of ?) obtains a reasonable accuracy much faster than L-BFGS.
Consider the compositionality principle from a statistical point of view, where we
construe compositionality as factorization. Factorization, the way a statistical model
breaks into features, is necessary for generalization: It enables us to learn from pre-
viously seen examples and interpret new utterances. Projecting back to Frege?s orig-
inal principle, the parts are the features (Section 3.1.1), and composition is the DCS
construction mechanism (Section 2.6) driven by parameters learned from training
examples.
Taking the statistical view of compositionality, finding a good semantic represen-
tation becomes designing a good statistical model. But statistical modeling must also
deal with the additional issue of language acquisition or learning, which presents
complications: In absorbing training examples, our learning algorithm must inevitably
traverse through intermediate models that are wrong or incomplete. The algorithms
must therefore tolerate this degradation, and do so in a computationally efficient way.
For example, in the line of work on learning probabilistic CCGs (Zettlemoyer and
Collins 2005, 2007; Kwiatkowski et al 2010), many candidate lexical entries must be
entertained for each word even when polysemy does not actually exist (Section 2.6.4).
To improve generalization, the lexicon can be further factorized (Kwiatkowski et al
2011), but this is all done within the constraints of CCG. DCS represents a departure
from this tradition, which replaces a heavily lexicalized constituency-based formalism
with a lightly-lexicalized dependency-based formalism. We can think of DCS as a shift
in linguistic coordinate systems, which makes certain factorizations or features more
accessible. For example, we can define features on paths between predicates in a DCS
tree which capture certain lexical patterns much more easily than in a lambda calculus
expression or a CCG derivation.
DCS has a family resemblance to a semantic representation called natural logic form
(Alshawi, Chang, and Ringgaard 2011), which is also motivated by the benefits of work-
ing with dependency-based logical forms. The goals and the detailed structure of the
two semantic formalisms are different, however. Alshawi, Chang, and Ringgaard (2011)
focus on parsing complex sentences in an open domain where a structured database
or world does not exist. Whereas they do equip their logical forms with a full model-
theoretic semantics, the logical forms are actually closer to dependency trees: Quantifier
scope is left unspecified, and the predicates are simply the words.
441
Computational Linguistics Volume 39, Number 2
Perhaps not immediately apparent is the fact that DCS draws an important idea
from Discourse Representation Theory (DRT) (Kamp and Reyle 1993)?not from the
treatment of anaphora and presupposition which it is known for, but something closer
to its core. This is the idea of having a logical form where all variables are existentially
quantified and constraints are combined via conjunction?a Discourse Representation
Structure (DRS) in DRT, or a basic DCS tree with only join relations. Computationally,
these logical structures conveniently encode CSPs. Linguistically, it appears that existen-
tial quantifiers play an important role and should be treated specially (Kamp and Reyle
1993). DCS takes this core and focuses on semantic compositionality and computation,
whereas DRT focuses more on discourse and pragmatics.
In addition to the statistical view of DCS as a semantic representation, it is use-
ful to think about DCS from the perspective of programming language design. Two
programming languages can be equally expressive, but what matters is how simple it
is to express a desired type of computation in a given language. In some sense, we
designed the DCS formal language to make it easy to represent computations expressed
by natural language. An important part of DCS is themark?execute construct, a uniform
framework for dealing with the divergence between syntactic and semantic scope. This
construct allows us to build simple DCS tree structures and still handle the complexities
of phenomena such as quantifier scope variation. Compared to lambda calculus, think
of DCS as a higher-level programming language tailored to natural language, which
results in simpler programs (DCS trees). Simpler programs are easier for us to work
with and easier for an algorithm to learn.
5.2 Program Induction
Searching over the space of programs is challenging. This is the central computational
challenge of program induction, that of inferring programs (logical forms) from their
behavior (denotations). This problem has been tackled by different communities in
various forms: program induction in AI, programming by demonstration in Human?
Computer Interaction, and program synthesis in programming languages. The core
computational difficulty is that the supervision signal?the behavior?is a complex
function of the program that cannot be easily inverted. What program generated the
output Arizona, Nevada, and Oregon?
Perhaps somewhat counterintuitively, program induction is easier if we infer pro-
grams for not a single task but for multiple tasks. The intuition is that when the tasks
are related, the solution to one task can help another task, both computationally in
navigating the program space and statistically in choosing the appropriate program if
there are multiple feasible possibilities (Liang, Jordan, and Klein 2010). In our semantic
parsing work, we want to infer a logical form for each utterance (task). Clearly the tasks
are related because they use the same vocabulary to talk about the same domain.
Natural language also makes program induction easier by providing side informa-
tion (words) which can be used to guide the search. There have been several papers
that induce programs in this setting: Eisenstein et al (2009) induce conjunctive for-
mulae from natural language instructions, Piantadosi et al (2008) induce first-order
logic formulae using CCG in a small domain assuming observed lexical semantics,
and Clarke et al (2010) induce logical forms in semantic parsing. In the ideal case, the
words would determine the program predicates, and the utterance would determine
the entire program compositionally. But of course, this mapping is not given and must
be learned.
442
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
5.3 Grounded Language
In recent years, there has been an increased interest in connecting language with the
world.18 One of the primary issues in grounded language is alignment?figuring out
what fragments of utterances refer to what aspects of the world. In fact, semantic
parsers trained on examples of utterances and annotated logical form (those discussed
in Section 4.2.2) need to solve the task of aligning words to predicates. Some can learn
from utterances paired with a set of logical forms, one of which is correct (Kate and
Mooney 2007; Chen and Mooney 2008). Liang, Jordan, and Klein (2009) tackle the even
more difficult alignment problem of segmenting and aligning a discourse to a database
of facts, where many parts on either side are irrelevant.
If we know how the world relates to language, we can leverage structure in the
world to guide the learning and interpretation of language.We saw that type constraints
from the database/world reduce the set of candidate logical forms and lead to more
accurate systems (Popescu, Etzioni, and Kautz 2003; Liang, Jordan, and Klein 2011).
Even for syntactic parsing, information from the denotation of an utterance can be
helpful (Schuler 2003).
One of the exciting aspects about using the world for learning language is that
it opens the door to many new types of supervision. We can obtain answers given a
world, which are cheaper to obtain than logical forms (Clarke et al 2010; Liang, Jordan,
and Klein 2011). Other researchers have also pushed in this direction in various ways:
learning a semantic parser based on bootstrapping and estimating the confidence of its
own predictions (Goldwasser et al 2011), learning a semantic parser from user interac-
tions with a dialog system (Artzi and Zettlemoyer 2011), and learning to execute natural
language instructions from just a reward signal using reinforcement learning (Branavan
et al 2009; Branavan, Zettlemoyer, and Barzilay 2010; Branavan, Silver, and Barzilay
2011). In general, supervision from the world is indirectly related to the learning task,
but it is often much more plentiful and natural to obtain.
The benefits can also flow from language to the world. For example, previous work
learned to interpret language to troubleshoot aWindows machine (Branavan et al 2009;
Branavan, Zettlemoyer, and Barzilay 2010), win a game of Civilization (Branavan, Silver,
and Barzilay 2011), play a legal game of solitaire (Eisenstein et al 2009; Goldwasser and
Roth 2011), and navigate a map by following directions (Vogel and Jurafsky 2010; Chen
and Mooney 2011). Even when the objective in the world is defined independently of
language (e.g., in Civilization), language can provide a useful bias towards the non-
linguistic end goal.
6. Conclusions
The main conceptual contribution of this article is a new semantic formalism,
dependency-based compositional semantics (DCS), and techniques to learn a semantic
parser from question?answer pairs where the intermediate logical form (a DCS tree) is
induced in an unsupervised manner. Our final question?answering system was able to
match the accuracies of state-of-the-art systems that learn from annotated logical forms.
There is currently a significant conceptual gap between our question?answering
system (which can be construed as a natural language interface to a database) and
18 Here, world need not refer to the physical world, but could be any virtual world. The point is that the
world has non-trivial structure and exists extra-linguistically.
443
Computational Linguistics Volume 39, Number 2
open-domain question?answering systems. The former focuses on understanding a
question compositionally and computing the answer compositionally, whereas the lat-
ter focuses on retrieving and ranking answers from a large unstructured textual corpus.
The former has depth; the latter has breadth. Developing methods that can both model
the semantic richness of language and scale up to an open-domain setting remains an
open challenge.
We believe that it is possible to push our approach in the open-domain direction.
Neither DCS nor the learning algorithm is tied to having a clean rigid database, which
could instead be a database generated from a noisy information extraction process. The
key is to drive the learning with the desired behavior, the question?answer pairs. The
latent variable is the logical form or program, which just tries to compute the desired
answer by piecing together whatever information is available. Of course, there aremany
open challenges ahead, but with the proper combination of linguistic, statistical, and
computational insight, we hope to eventually build systems with both breadth and
depth.
Acknowledgments
We thank Luke Zettlemoyer and Tom
Kwiatkowski for providing us with data
and answering questions, as well as the
anonymous reviewers for their detailed
feedback. P. L. was supported by an NSF
Graduate Research Fellowship.
References
Alshawi, H., P. Chang, and M. Ringgaard.
2011. Deterministic statistical mapping
of sentences to underspecified
semantics. In International Conference
on Compositional Semantics (IWCS),
pages 15?24, Oxford.
Androutsopoulos, I., G. D. Ritchie, and
P. Thanisch. 1995. Natural language
interfaces to databases?an introduction.
Journal of Natural Language Engineering,
1:29?81.
Artzi, Y. and L. Zettlemoyer. 2011.
Bootstrapping semantic parsers from
conversations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 421?432, Edinburgh.
Baldridge, J. and G. M. Kruijff. 2002.
Coupling CCG with hybrid logic
dependency semantics. In Association
for Computational Linguistics (ACL),
pages 319?326, Philadelphia, PA.
Barker, C. 2002. Continuations and the
nature of quantification. Natural
Language Semantics, 10:211?242.
Bos, J. 2009. A controlled fragment of
DRT. InWorkshop on Controlled Natural
Language, pages 1?5.
Bos, J., S. Clark, M. Steedman, J. R. Curran,
and J. Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG
parser. In International Conference on
Computational Linguistics (COLING),
pages 1240?1246, Geneva.
Branavan, S., H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In
Association for Computational Linguistics and
International Joint Conference on Natural
Language Processing (ACL-IJCNLP),
pages 82?90, Singapore.
Branavan, S., D. Silver, and R. Barzilay. 2011.
Learning to win by reading manuals in a
Monte-Carlo framework. In Association
for Computational Linguistics (ACL),
pages 268?277.
Branavan, S., L. Zettlemoyer, and R. Barzilay.
2010. Reading between the lines: Learning
to map high-level instructions to
commands. In Association for Computational
Linguistics (ACL), pages 1268?1277,
Portland, OR.
Carpenter, B. 1998. Type-Logical Semantics.
MIT Press, Cambridge, MA.
Chen, D. L. and R. J. Mooney. 2008. Learning
to sportscast: A test of grounded language
acquisition. In International Conference on
Machine Learning (ICML), pages 128?135,
Helsinki.
Chen, D. L. and R. J. Mooney. 2011.
Learning to interpret natural language
navigation instructions from observations.
In Association for the Advancement
of Artificial Intelligence (AAAI),
pages 128?135, Cambridge, MA.
Clarke, J., D. Goldwasser, M. Chang,
and D. Roth. 2010. Driving semantic
parsing from the world?s response.
In Computational Natural Language
Learning (CoNLL), pages 18?27,
Uppsala.
444
Liang, Jordan, and Klein Learning Dependency-Based Compositional Semantics
Collins, M. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Cooper, R. 1975.Montague?s semantic theory
and transformational syntax. Ph.D. thesis,
University of Massachusetts at Amherst.
Cousot, P. and R. Cousot. 1977. Abstract
interpretation: A unified lattice model for
static analysis of programs by construction
or approximation of fixpoints. In Principles
of Programming Languages (POPL),
pages 238?252, Los Angeles, CA.
Daume, H., J. Langford, and D. Marcu.
2009. Search-based structured prediction.
Machine Learning Journal (MLJ), 75:297?325.
Dechter, R. 2003. Constraint Processing.
Morgan Kaufmann.
Eisenstein, J., J. Clarke, D. Goldwasser,
and D. Roth. 2009. Reading to learn:
Constructing features from semantic
abstracts. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 958?967, Singapore.
Ge, R. and R. J. Mooney. 2005. A statistical
semantic parser that integrates syntax
and semantics. In Computational Natural
Language Learning (CoNLL), pages 9?16,
Ann Arbor, MI.
Giordani, A. and A. Moschitti. 2009.
Semantic mapping between natural
language questions and SQL queries
via syntactic pairing. In International
Conference on Applications of Natural
Language to Information Systems,
pages 207?221, Saarbru?cken.
Goldwasser, D., R. Reichart, J. Clarke,
and D. Roth. 2011. Confidence driven
unsupervised semantic parsing. In
Association for Computational Linguistics
(ACL), pages 1486?1495, Barcelona.
Goldwasser, D. and D. Roth. 2011. Learning
from natural instructions. In International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1794?1800, Portland, OR.
Heim, I. and A. Kratzer. 1998. Semantics in
Generative Grammar. Wiley-Blackwell,
Oxford.
Judge, J., A. Cahill, and J. v. Genabith.
2006. Question-bank: Creating a
corpus of parse-annotated questions.
In International Conference on Computational
Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 497?504,
Sydney.
Kamp, H. and U. Reyle. 1993. From Discourse
to Logic: An Introduction to the
Model-theoretic Semantics of Natural
Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
Kamp, H., J. van Genabith, and U. Reyle.
2005. Discourse representation theory.
In Handbook of Philosophical Logic,
Kluwer, Dordrecht.
Kate, R. J. and R. J. Mooney. 2006. Using
string-kernels for learning semantic
parsers. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 913?920, Sydney.
Kate, R. J. and R. J. Mooney. 2007.
Learning language semantics from
ambiguous supervision. In Association
for the Advancement of Artificial
Intelligence (AAAI), pages 895?900,
Cambridge, MA.
Kate, R. J., Y. W. Wong, and R. J. Mooney.
2005. Learning to transform natural to
formal languages. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1062?1068.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2010.
Inducing probabilistic CCG grammars
from logical form with higher-order
unification. In Empirical Methods in
Natural Language Processing (EMNLP),
pages1223?1233, Cambridge, MA.
Kwiatkowski, T., L. Zettlemoyer,
S. Goldwater, and M. Steedman. 2011.
Lexical generalization in CCG grammar
induction for semantic parsing. In
Empirical Methods in Natural Language
Processing (EMNLP), pages 1512?1523,
Cambridge, MA.
Liang, P. 2011. Learning Dependency-Based
Compositional Semantics. Ph.D. thesis,
University of California at Berkeley.
Liang, P., M. I. Jordan, and D. Klein. 2009.
Learning semantic correspondences
with less supervision. In Association for
Computational Linguistics and International
Joint Conference on Natural Language
Processing (ACL-IJCNLP), pages 91?99,
Singapore.
Liang, P., M. I. Jordan, and D. Klein. 2010.
Learning programs: A hierarchical
Bayesian approach. In International
Conference on Machine Learning (ICML),
pages 639?646, Haifa.
Liang, P., M. I. Jordan, and D. Klein.
2011. Learning dependency-based
compositional semantics. In Association
for Computational Linguistics (ACL),
pages 590?599, Portland, OR.
Liang, P. and D. Klein. 2009. Online EM for
unsupervised models. In North American
Association for Computational Linguistics
(NAACL), pages 611?619, Boulder, CO.
445
Computational Linguistics Volume 39, Number 2
Lu, W., H. T. Ng, W. S. Lee, and L. S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning
representations. In Empirical Methods in
Natural Language Processing (EMNLP),
pages 783?792, Honolulu, HI.
Marcus, M. P., M. A. Marcinkiewicz, and
B. Santorini. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19:313?330.
Miller, S., D. Stallard, R. Bobrow, and
R. Schwartz. 1996. A fully statistical
approach to natural language interfaces.
In Association for Computational Linguistics
(ACL), pages 55?61, Santa Cruz, CA.
Montague, R. 1973. The proper treatment
of quantification in ordinary English.
In J. Hiutikka, J. Moravcsik, and
P. Suppes, editors, Approaches to Natural
Language, pages 221?242, Dordrecht,
The Netherlands.
Nocedal, J. 1980. Updating quasi-Newton
matrices with limited storage.Mathematics
of Computation, 35:773?782.
Petrov, S., L. Barrett, R. Thibaux, and
D. Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In International Conference on
Computational Linguistics and Association for
Computational Linguistics (COLING/ACL),
pages 433?440, Sydney.
Piantadosi, S. T., N. D. Goodman, B. A. Ellis,
and J. B. Tenenbaum. 2008. A Bayesian
model of the acquisition of compositional
semantics. In Proceedings of the Thirtieth
Annual Conference of the Cognitive Science
Society, pages 1620?1625, Washington, DC.
Popescu, A., O. Etzioni, and H. Kautz. 2003.
Towards a theory of natural language
interfaces to databases. In International
Conference on Intelligent User Interfaces
(IUI), pages 149?157, Miami, FL.
Porter, M. F. 1980. An algorithm for suffix
stripping. Program, 14:130?137.
Robbins, H. and S. Monro. 1951. A stochastic
approximation method. Annals of
Mathematical Statistics, 22(3):400?407.
Schuler, W. 2003. Using model-theoretic
semantic interpretation to guide statistical
parsing and word recognition in a spoken
language interface. In Association for
Computational Linguistics (ACL),
pages 529?536, Sapporo.
Shan, C. 2004. Delimited continuations in
natural language. Technical report, ArXiv.
Available at http://arvix.org/abs/
cs.CL/0404006.
Steedman, M. 2000. The Syntactic Process.
MIT Press, Cambridge, MA.
Tang, L. R. and R. J. Mooney. 2001. Using
multiple clause constructors in inductive
logic programming for semantic parsing.
In European Conference on Machine Learning,
pages 466?477, Freiburg.
Vogel, A. and D. Jurafsky. 2010. Learning
to follow navigational directions.
In Association for Computational Linguistics
(ACL), pages 806?814, Uppsala.
Wainwright, M. and M. I. Jordan. 2008.
Graphical models, exponential families,
and variational inference. Foundations and
Trends in Machine Learning, 1:1?307.
Warren, D. and F. Pereira. 1982. An efficient
easily adaptable system for interpreting
natural language queries. Computational
Linguistics, 8:110?122.
White, M. 2006. Efficient realization of
coordinate structures in combinatory
categorial grammar. Research on Language
and Computation, 4:39?75.
Wong, Y. W. and R. J. Mooney. 2006.
Learning for semantic parsing with
statistical machine translation. In North
American Association for Computational
Linguistics (NAACL), pages 439?446,
New York, NY.
Wong, Y. W. and R. J. Mooney. 2007.
Learning synchronous grammars for
semantic parsing with lambda calculus.
In Association for Computational Linguistics
(ACL), pages 960?967, Prague.
Woods, W. A., R. M. Kaplan, and
B. N. Webber. 1972. The lunar sciences
natural language information system:
Final report. Technical Report 2378,
Bolt Beranek and Newman Inc.,
Cambridge, MA.
Zelle, M. and R. J. Mooney. 1996. Learning to
parse database queries using inductive
logic programming. In Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1050?1055, Cambridge, MA.
Zettlemoyer, L. S. and M. Collins. 2005.
Learning to map sentences to logical
form: Structured classification with
probabilistic categorial grammars.
In Uncertainty in Artificial Intelligence
(UAI), pages 658?666.
Zettlemoyer, L. S. and M. Collins. 2007.
Online learning of relaxed CCG grammars
for parsing to logical form. In Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP/CoNLL), pages 678?687,
Prague.
446

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118?126,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Syntactic Alignment with Inversion Transduction Grammars
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
David Chiang Kevin Knight
Information Sciences Institute
University of Southern California
{chiang,knight}@isi.edu
Abstract
Syntactic machine translation systems cur-
rently use word alignments to infer syntactic
correspondences between the source and tar-
get languages. Instead, we propose an un-
supervised ITG alignment model that directly
aligns syntactic structures. Our model aligns
spans in a source sentence to nodes in a target
parse tree. We show that our model produces
syntactically consistent analyses where possi-
ble, while being robust in the face of syntactic
divergence. Alignment quality and end-to-end
translation experiments demonstrate that this
consistency yields higher quality alignments
than our baseline.
1 Introduction
Syntactic machine translation has advanced signif-
icantly in recent years, and multiple variants cur-
rently achieve state-of-the-art translation quality.
Many of these systems exploit linguistically-derived
syntactic information either on the target side (Gal-
ley et al, 2006), the source side (Huang et al, 2006),
or both (Liu et al, 2009). Still others induce their
syntax from the data (Chiang, 2005). Despite differ-
ences in detail, the vast majority of syntactic meth-
ods share a critical dependence on word alignments.
In particular, they infer syntactic correspondences
between the source and target languages through
word alignment patterns, sometimes in combination
with constraints from parser outputs.
However, word alignments are not perfect indi-
cators of syntactic alignment, and syntactic systems
are very sensitive to word alignment behavior. Even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, while
unaligned words can result in an exponentially large
set of extractable rules to choose from. Researchers
have worked to incorporate syntactic information
into word alignments, resulting in improvements to
both alignment quality (Cherry and Lin, 2006; DeN-
ero and Klein, 2007), and translation quality (May
and Knight, 2007; Fossum et al, 2008).
In this paper, we remove the dependence on word
alignments and instead directly model the syntactic
correspondences in the data, in a manner broadly
similar to Yamada and Knight (2001). In particu-
lar, we propose an unsupervised model that aligns
nodes of a parse tree (or forest) in one language to
spans of a sentence in another. Our model is an in-
stance of the inversion transduction grammar (ITG)
formalism (Wu, 1997), constrained in such a way
that one side of the synchronous derivation respects
a syntactic parse. Our model is best suited to sys-
tems which use source- or target-side trees only.
The design of our model is such that, for divergent
structures, a structurally integrated backoff to flatter
word-level (or null) analyses is available. There-
fore, our model is empirically robust to the case
where syntactic divergence between languages pre-
vents syntactically accurate ITG derivations.
We show that, with appropriate pruning, our
model can be efficiently trained on large parallel cor-
pora. When compared to standard word-alignment-
backed baselines, our model produces more con-
sistent analyses of parallel sentences, leading to
high-count, high-quality transfer rules. End-to-
end translation experiments demonstrate that these
higher quality rules improve translation quality by
1.0 BLEU over a word-alignment-backed baseline.
2 Syntactic Rule Extraction
Our model is intended for use in syntactic transla-
tion models which make use of syntactic parses on
either the target (Galley et al, 2006) or source side
(Huang et al, 2006; Liu et al, 2006). Our model?s
118
SNP
DT* NN NN
VP
VBZ
ADVP
RB VBN
the trade surplus has drastically fallen
??
??
???
 ??
 ?
trade
surplus
drastically
fall
(past)
Figure 1: A single incorrect alignment removes an ex-
tractable node, and hence several desirable rules. We
represent correct extractable nodes in bold, spurious ex-
tractable nodes with a *, and incorrectly blocked ex-
tractable nodes in bold strikethrough.
chief purpose is to align nodes in the syntactic parse
in one language to spans in the other ? an alignment
we will refer to as a ?syntactic? alignment. These
alignments are employed by standard syntactic rule
extraction algorithms, for example, the GHKM al-
gorithm of Galley et al (2004). Following that work,
we will assume parses are present in the target lan-
guage, though our model applies in either direction.
Currently, although syntactic systems make use of
syntactic alignments, these alignments must be in-
duced indirectly from word-level alignments. Pre-
vious work has discussed at length the poor interac-
tion of word-alignments with syntactic rule extrac-
tion (DeNero and Klein, 2007; Fossum et al, 2008).
For completeness, we provide a brief example of this
interaction, but for a more detailed discussion we re-
fer the reader to these presentations.
2.1 Interaction with Word Alignments
Syntactic systems begin rule extraction by first iden-
tifying, for each node in the target parse tree, a
span of the foreign sentence which (1) contains ev-
ery source word that aligns to a target word in the
yield of the node and (2) contains no source words
that align outside that yield. Only nodes for which
a non-empty span satisfying (1) and (2) exists may
form the root or leaf of a translation rule; for that
reason, we will refer to these nodes as extractable
nodes.
Since extractable nodes are inferred based on
word alignments, spurious word alignments can rule
out otherwise desirable extraction points. For exam-
ple, consider the alignment in Figure 1. This align-
ment, produced by GIZA++ (Och and Ney, 2003),
contains 4 correct alignments (the filled circles),
but incorrectly aligns the to the Chinese past tense
marker ? (the hollow circle). This mistaken align-
ment produces the incorrect rule (DT ? the ; ?),
and also blocks the extraction of (VBN ? fallen ;
???).
More high-level syntactic transfer rules are also
ruled out, for example, the ?the insertion rule? (NP
? the NN1 NN2 ; NN1 NN2) and the high-level (S
? NP1 VP2 ; NP1 VP2).
3 A Syntactic Alignment Model
The most common approach to avoiding these prob-
lems is to inject knowledge about syntactic con-
straints into a word alignment model (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et al,
2008).1 While syntactically aware, these models re-
main limited by the word alignment models that un-
derly them.
Here, we describe a model which directly infers
alignments of nodes in the target-language parse tree
to spans of the source sentence. Formally, our model
is an instance of a Synchronous Context-Free Gram-
mar (see Chiang (2004) for a review), or SCFG,
which generates an English (target) parse tree T and
foreign (source) sentence f given a target sentence e.
The generative process underlying this model pro-
duces a derivation d of SCFG rules, from which T
and f can be read off; because we condition on e,
the derivations produce e with probability 1. This
model places a distribution over T and f given by
p(T, f | e) =
?
d
p(d | e) =
?
d
?
r?d
p(r | e)
where the sum is over derivations d which yield T
and f . The SCFG rules r come from one of 4 types,
pictured in Table 1. In general, because our model
can generate English trees, it permits inference over
forests. Although we will restrict ourselves to a sin-
gle parse tree for our experiments, in this section, we
discuss the more general case.
1One notable exception is May and Knight (2007), who pro-
duces syntactic alignments using syntactic rules derived from
word-aligned data.
119
Rule Type Root English Foreign Example Instantiation
TERMINAL E e ft FOUR ? four ;?
UNARY A B fl B fr CD ? FOUR ;  FOUR ?
BINARYMONO A B C fl B fm C fr NP ? NN NN ;  NN ? NN 
BINARYINV A B C fl C fm B fr PP ? IN NP ;? NP  IN 
Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.
Empty word sequences f have been explicitly marked with an .
The first rule type is the TERMINAL production,
which rewrites a terminal symbol2 E as its En-
glish word e and a (possibly empty) sequence of
foreign words ft. Generally speaking, the majority
of foreign words are generated using this rule. It
is only when a straightforward word-to-word corre-
spondence cannot be found that our model resorts to
generating foreign words elsewhere.
We can also rewrite a non-terminal symbol A us-
ing a UNARY production, which on the English side
produces a single symbol B, and on the foreign side
produces the symbol B, with sequences of words fl
to its left and fr to its right.
Finally, there are two binary productions: BINA-
RYMONO rewrites A with two non-terminals B and
C on the English side, and the same non-terminals
B and C in monotonic order on the foreign side,
with sequences of words fl, fr, and fm to the left,
right, and the middle. BINARYINV inverts the or-
der in which the non-terminals B and C are written
on the source side, allowing our model to capture a
large subset of possible reorderings (Wu, 1997).
Derivations from this model have two key prop-
erties: first, the English side of a derivation is con-
strained to form a valid constituency parse, as is re-
quired in a syntax system with target-side syntax;
and second, for each parse node in the English pro-
jection, there is exactly one (possibly empty) con-
tiguous span of the foreign side which was gener-
ated from that non-terminal or one of its descen-
dants. Identifying extractable nodes from a deriva-
tion is thus trivial: any node aligned to a non-empty
foreign span is extractable.
In Figure 2, we show a sample sentence pair frag-
2For notational convenience, we imagine that for each par-
ticular English word e, there is a special preterminal symbol E
which produces it. These symbols E act like any other non-
terminal in the grammar with respect to the parameterization in
Section 3.1. To denote standard non-terminals, we will use A,
B, and C.
PP[0,4]
IN[3,4]
NP[1,3]
DT[1,1]
NNS[1,3]
the[1,1]
elections[1,3]
? ??
?
??
at parliament election
before
before[3,4]
PP
NP IN
NNSDT
0 1 2 3 4
?
PP ? IN NP ; ? NP IN
NP ? DT NNS ; DT NNS
IN ? before ; before
before ? before ; ??
DT ? the ; the
the ? the ; !
NNS ? elections ; elections
elections ? elections ; ?? ??
Figure 2: Top: A synchronous derivation of a small sen-
tence pair fragment under our model. The English pro-
jection of the derivation represents a valid constituency
parse, while the foreign projection is less constrained.
We connect each foreign terminal with a dashed line to
the node in the English side of the synchronous deriva-
tion at which it is generated. The foreign span assigned
to each English node is indicated with indices. All nodes
with non-empty spans, shown in boldface, are extractable
nodes. Bottom: The SCFG rules used in the derivation.
ment as generated by our model. Our model cor-
rectly identifies that the English the aligns to nothing
on the foreign side. Our model also effectively cap-
tures the one-to-many alignment3 of elections to ?
3While our model does not explicitly produce many-to-one
alignments, many-to-one rules can be discovered via rule com-
position (Galley et al, 2006).
120
? ??. Finally, our model correctly analyzes the
Chinese circumposition ? . . .?? (before . . . ). In
this construction, ?? carries the meaning of ?be-
fore?, and thus correctly aligns to before, while ?
functions as a generic preposition, which our model
handles by attaching it to the PP. This analysis per-
mits the extraction of the general rule (PP ? IN1
NP2 ;? NP2 IN1), and the more lexicalized (PP?
before NP ;? NP??).
3.1 Parameterization
In principle, our model could have one parameter for
each instantiation r of a rule type. This model would
have an unmanageable number of parameters, pro-
ducing both computational and modeling issues ? it
is well known that unsupervised models with large
numbers of parameters are prone to degenerate anal-
yses of the data (DeNero et al, 2006). One solution
might be to apply an informed prior with a compu-
tationally tractable inference procedure (e.g. Cohn
and Blunsom (2009) or Liu and Gildea (2009)). We
opt here for the simpler, statistically more robust so-
lution of making independence assumptions to keep
the number of parameters at a reasonable level.
Concretely, we define the probability of the BI-
NARYMONO rule,4
p(r = A? B C; fl B fm C fr|A, eA)
which conditions on the root of the rule A and the
English yield eA, as
pg(A? B C | A, eA) ? pinv(I | B,C)?
pleft(fl | A, eA)?pmid(fm | A, eA)?pright(fr | A, eA)
In words, we assume that the rule probability de-
composes into a monolingual PCFG grammar prob-
ability pg, an inversion probability pinv, and a proba-
bility of left, middle, and right word sequences pleft,
pmid, and pright.5 Because we condition on e, the
monolingual grammar probability pg must form a
distribution which produces e with probability 1.6
4In the text, we only describe the factorization for the BI-
NARYMONO rule. For a parameterization of all rules, we refer
the reader to Table 2.
5All parameters in our model are multinomial distributions.
6A simple case of such a distribution is one which places all
of its mass on a single tree. More complex distributions can be
obtained by conditioning an arbitrary PCFG on e (Goodman,
1998).
We further assume that the probability of produc-
ing a foreign word sequence fl decomposes as:
pleft(fl | A, eA) = pl(|fl| = m | A)
m?
j=1
p(fj | A, eA)
where m is the length of the sequence fl. The pa-
rameter pl is a left length distribution. The prob-
abilities pmid, pright, decompose in the same way,
except substituting a separate length distribution pm
and pr for pl. For the TERMINAL rule, we emit ft
with a similarly decomposed distribution pterm us-
ing length distribution pw.
We define the probability of generating a foreign
word fj as
p(fj | A, eA) =
?
i?eA
1
| eA |
pt(fj | ei)
with i ? eA denoting an index ranging over the in-
dices of the English words contained in eA. The
reader may recognize the above expressions as the
probability assigned by IBM Model 1 (Brown et al,
1993) of generating the words fl given the words eA,
with one important difference ? the length m of the
foreign sentence is often not modeled, so the term
pl(|fl| = m | A) is set to a constant and ignored.
Parameterizing this length allows our model to ef-
fectively control the number of words produced at
different levels of the derivation.
It is worth noting how each parameter affects the
model?s behavior. The pt distribution is a standard
?translation? table, familiar from the IBM Models.
The pinv distribution is a ?distortion? parameter, and
models the likelihood of inverting non-terminals B
and C. This parameter can capture, for example,
the high likelihood that prepositions IN and noun
phrases NP often invert in Chinese due to its use
of postpositions. The non-terminal length distribu-
tions pl, pm, and pr model the probability of ?back-
ing off? and emitting foreign words at non-terminals
when a more refined analysis cannot be found. If
these parameters place high mass on 0 length word
sequences, this heavily penalizes this backoff be-
haviour. For the TERMINAL rule, the length distri-
bution pw parameterizes the number of words pro-
duced for a particular English word e, functioning
similarly to the ?fertilities? employed by IBM Mod-
els 3 and 4 (Brown et al, 1993). This allows us
121
to model, for example, the tendency of English de-
terminers the and a translate to nothing in the Chi-
nese, and of English names to align to multiple Chi-
nese words. In general, we expect an English word
to usually align to one Chinese word, and so we
place a weak Dirichlet prior on on the pe distribution
which puts extra mass on 1-length word sequences.
This is helpful for avoiding the ?garbage collection?
(Moore, 2004) problem for rare words.
3.2 Exploiting Non-Terminal Labels
There are often foreign words that do not correspond
well to any English word, which our model must
also handle. We elected for a simple augmentation
to our model to account for these words. When gen-
erating foreign word sequences f at a non-terminal
(i.e. via the UNARY or BINARY productions), we
also allow for the production of foreign words from
the non-terminal symbol A. We modify p(fj | eA)
from the previous section to allow production of fj
directly from the non-terminal7 A:
p(fj | eA) = pnt ? p(fj | A)
+ (1? pnt) ?
?
i?eA
1
|eA|
pt(fj | ei)
where pnt is a global binomial parameter which con-
trols how often such alignments are made.
This necessitates the inclusion of parameters like
pt(? | NP) into our translation table. Generally,
these parameters do not contain much information,
but rather function like a traditional NULL rooted
at some position in the tree. However, in some
cases, the particular annotation used by the Penn
Treebank (Marcus et al, 1993) (and hence most
parsers) allows for some interesting parameters to
be learned. For example, we found that our aligner
often matched the Chinese word ?, which marks
the past tense (among other things), to the preter-
minals VBD and VBN, which denote the English
simple past and perfect tense. Additionally, Chinese
measure words like ? and ? often align to the CD
(numeral) preterminal. These generalizations can be
quite useful ? where a particular number might pre-
dict a measure word quite poorly, the generalization
that measure words co-occur with the CD tag is very
robust.
7For terminal symbols E, this production is not possible.
3.3 Membership in ITG
The generative process which describes our model
contains a class of grammars larger than the com-
putationally efficient class of ITG grammars. For-
tunately, the parameterization described above not
only reduces the number of parameters to a man-
ageable level, but also introduces independence as-
sumptions which permit synchronous binarization
(Zhang et al, 2006) of our grammar. Any SCFG that
can be synchronously binarized is an ITG, meaning
that our parameterization permits efficient inference
algorithms which we will make use of in the next
section. Although several binarizations are possi-
ble, we give one such binarization and its associated
probabilities in Table 2.
3.4 Robustness to Syntactic Divergence
Generally speaking, ITG grammars have proven
more useful without the monolingual syntactic con-
straints imposed by a target parse tree. When deriva-
tions are restricted to respect a target-side parse tree,
many desirable alignments are ruled out when the
syntax of the two languages diverges, and align-
ment quality drops precipitously (Zhang and Gildea,
2004), though attempts have been made to address
this issue (Gildea, 2003).
Our model is designed to degrade gracefully in
the case of syntactic divergence. Because it can pro-
duce foreign words at any level of the derivation,
our model can effectively back off to a variant of
Model 1 in the case where an ITG derivation that
both respects the target parse tree and the desired
word-level alignments cannot be found.
For example, consider the sentence pair fragment
in Figure 3. It is not possible to produce an ITG
derivation of this fragment that both respects the
English tree and also aligns all foreign words to
their obvious English counterparts. Our model han-
dles this case by attaching the troublesome ?? at
the uppermost VP. This analysis captures 3 of the
4 word-level correspondences, and also permits ex-
traction of abstract rules like (S? NP VP ; NP VP)
and (NP? the NN ; NN).
Unfortunately, this analysis leaves the English
word tomorrow with an empty foreign span, permit-
ting extraction of the incorrect translation (VP ?
announced tomorrow ; ??), among others. Our
122
Rule Type Root English side Foreign side Probability
TERMINAL E e wt pterm(wt | E)
UNARY A Bu wl Bu pg(A ? B | A)pleft(wl | A, eA)
Bu B B wr pright(wr | A, eA)
BINARY A A1 wl A1 pleft(wl | A, eA)
A1 B C1 B C1 pg(A ? B C | A)pinv(I=false | B,C)
A1 B C1 C1 B pg(A ? B C | A)pinv(I=true | B,C)
C1 C2 fm C2 pmid(fm | A, eA)
C2 C C fr pright(fr | A, eA)
Table 2: A synchronous binarization of the SCFG describing our model.
S[0,4]
NP[3,4]
DT[3,3] NN[3,4]
VP[0,3]
VB[2,2]
VP[2,3]
VBN[2,3]
NN[3,3]
VP[2,3]
MD[1,2]
?? ? ?? ??
listannouncewilltomorrow0 1 2 3 4
the[3,3] list[3,4]
be[2,2]
announced[2,3] tomorrow[3,3]
will[1,2]
(a)
Figure 3: The graceful degradation of our model in the
face of syntactic divergence. It is not possible to align
all foreign words with their obvious English counterparts
with an ITG derivation. Instead, our model analyzes as
much as possible, but must resort to emitting ?? high
in the tree.
point here is not that our model?s analysis is ?cor-
rect?, but ?good enough? without resorting to more
computationally complicated models. In general,
our model follows an ?extract as much as possi-
ble? approach. We hypothesize that this approach
will capture important syntactic generalizations, but
it also risks including low-quality rules. It is an em-
pirical question whether this approach is effective,
and we investigate this issue further in Section 5.3.
There are possibilities for improving our model?s
treatment of syntactic divergence. One option is
to allow the model to select trees which are more
consistent with the alignment (Burkett et al, 2010),
which our model can do since it permits efficient in-
ference over forests. The second is to modify the
generative process slightly, perhaps by including the
?clone? operator of Gildea (2003).
4 Learning and Inference
4.1 Parameter Estimation
The parameters of our model can be efficiently
estimated in an unsupervised fashion using the
Expectation-Maximization (EM) algorithm. The E-
step requires the computation of expected counts un-
der our model for each multinomial parameter. We
omit the details of obtaining expected counts for
each distribution, since they can be obtained using
simple arithmetic from a single quantity, namely, the
expected count of a particular instantiation of a syn-
chronous rule r. This expectation is a standard quan-
tity that can be computed in O(n6) time using the
bitext Inside-Outside dynamic program (Wu, 1997).
4.2 Dynamic Program Pruning
While our model permits O(n6) inference over a
forest of English trees, inference over a full forest
would be very slow, and so we fix a single n-ary En-
glish tree obtained from a monolingual parser. How-
ever, it is worth noting that the English side of the
ITG derivation is not completely fixed. Where our
English trees are more than binary branching, we
permit any binarization in our dynamic program.
For efficiency, we also ruled out span alignments
that are extremely lopsided, for example, a 1-word
English span aligned to a 20-word foreign span.
Specifically, we pruned any span alignment in which
one side is more than 5 times larger than the other.
Finally, we employ pruning based on high-
precision alignments from simpler models (Cherry
and Lin, 2007; Haghighi et al, 2009). We com-
pute word-to-word alignments by finding all word
pairs which have a posterior of at least 0.7 according
to both forward and reverse IBM Model 1 parame-
ters, and prune any span pairs which invalidate more
than 3 of these alignments. In total, this pruning re-
123
Span P R F1
Syntactic Alignment 50.9 83.0 63.1
GIZA++ 56.1 67.3 61.2
Rule P R F1
Syntactic Alignment 39.6 40.3 39.9
GIZA++ 46.2 34.7 39.6
Table 3: Alignment quality results for our syntactic
aligner and our GIZA++ baseline.
duced computation from approximately 1.5 seconds
per sentence to about 0.3 seconds per sentence, a
speed-up of a factor of 5.
4.3 Decoding
Given a trained model, we extract a tree-to-string
alignment as follows: we compute, for each node
in the English tree, the posterior probability of a
particular foreign span assignment using the same
dynamic program needed for EM. We then com-
pute the set of span assignments which maximizes
the sum of these posteriors, constrained such that
the foreign span assignments nest in the obvious
way. This algorithm is a natural synchronous gener-
alization of the monolingual Maximum Constituents
Parse algorithm of Goodman (1996).
5 Experiments
5.1 Alignment Quality
We first evaluated our alignments against gold stan-
dard annotations. Our training data consisted of the
2261 manually aligned and translated sentences of
the Chinese Treebank (Bies et al, 2007) and approx-
imately half a million unlabeled sentences of parallel
Chinese-English newswire. The unlabeled data was
subsampled (Li et al, 2009) from a larger corpus by
selecting sentences which have good tune and test
set coverage, and limited to sentences of length at
most 40. We parsed the English side of the train-
ing data with the Berkeley parser.8 For our baseline
alignments, we used GIZA++, trained in the stan-
dard way.9 We used the grow-diag-final alignment
heuristic, as we found it outperformed union in early
experiments.
We trained our unsupervised syntactic aligner on
the concatenation of the labelled and unlabelled
8http://code.google.com/p/berkeleyparser/
95 iterations of model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4.
data. As is standard in unsupervised alignment mod-
els, we initialized the translation parameters pt by
first training 5 iterations of IBM Model 1 using the
joint training algorithm of Liang et al (2006), and
then trained our model for 5 EM iterations. We
extracted syntactic rules using a re-implementation
of the Galley et al (2006) algorithm from both our
syntactic alignments and the GIZA++ alignments.
We handle null-aligned words by extracting every
consistent derivation, and extracted composed rules
consisting of at most 3 minimal rules.
We evaluate our alignments against the gold stan-
dard in two ways. We calculated Span F-score,
which compares the set of extractable nodes paired
with a foreign span, and Rule F-score (Fossum et al,
2008) over minimal rules. The results are shown in
Table 3. By both measures, our syntactic aligner ef-
fectively trades recall for precision when compared
to our baseline, slightly increasing overall F-score.
5.2 Translation Quality
For our translation system, we used a re-
implementation of the syntactic system of Galley et
al. (2006). For the translation rules extracted from
our data, we computed standard features based on
relative frequency counts, and tuned their weights
using MERT (Och, 2003). We also included a
language model feature, using a 5-gram language
model trained on 220 million words of English text
using the SRILM Toolkit (Stolcke, 2002).
For tuning and test data, we used a subset of the
NIST MT04 and MT05 with sentences of length at
most 40. We used the first 1000 sentences of this set
for tuning and the remaining 642 sentences as test
data. We used the decoder described in DeNero et
al. (2009) during both tuning and testing.
We provide final tune and test set results in Ta-
ble 4. Our alignments produce a 1.0 BLEU improve-
ment over the baseline. Our reported syntactic re-
sults were obtained when rules were thresholded by
count; we discuss this in the next section.
5.3 Analysis
As discussed in Section 3.4, our aligner is designed
to extract many rules, which risks inadvertently ex-
tracting low-quality rules. To quantify this, we
first examined the number of rules extracted by our
aligner as compared with GIZA++. After relativiz-
124
Tune Test
Syntactic Alignment 29.78 29.83
GIZA++ 28.76 28.84
GIZA++ high count 25.51 25.38
Table 4: Final tune and test set results for our grammars
extracted using the baseline GIZA++ alignments and our
syntactic aligner. When we filter the GIZA++ grammars
with the same count thresholds used for our aligner (?high
count?), BLEU score drops substantially.
ing to the tune and test set, we extracted approx-
imately 32 million unique rules using our aligner,
but only 3 million with GIZA++. To check that
we were not just extracting extra low-count, low-
quality rules, we plotted the number of rules with
a particular count in Figure 4. We found that while
our aligner certainly extracts many more low-count
rules, it also extracts many more high-count rules.
Of course, high-count rules are not guaranteed
to be high quality. To verify that frequent rules
were better for translation, we experimented with
various methods of thresholding to remove rules
with low count extracted from using aligner. We
found in early development found that removing
low-count rules improved translation performance
substantially. In particular, we settled on the follow-
ing scheme: we kept all rules with a single foreign
terminal on the right-hand side. For entirely lexical
(gapless) rules, we kept all rules occurring at least
3 times. For unlexicalized rules, we kept all rules
occurring at least 20 times per gap. For rules which
mixed gaps and lexical items, we kept all rules oc-
curring at least 10 times per gap. This left us with
a grammar about 600 000 rules, the same grammar
which gave us our final results reported in Table 4.
In contrast to our syntactic aligner, rules extracted
using GIZA++ could not be so aggressively pruned.
When pruned using the same count thresholds, ac-
curacy dropped by more than 3.0 BLEU on the tune
set, and similarly on the test set (see Table 4). To
obtain the accuracy shown in our final results (our
best results with GIZA++), we had to adjust the
count threshold to include all lexicalized rules, all
unlexicalized rules, and mixed rules occurring at
least twice per gap. With these count thresholds, the
GIZA++ grammar contained about 580 000 rules,
roughly the same number as our syntactic grammar.
We also manually searched the grammars for
rules that had high count in the syntactically-
0 200 400 600 800 1000
1e+00
1e+02
1e+04
1e+06
Count
Numbe
r of rul
es with
 count SyntacticGIZA++
Figure 4: Number of extracted translation rules with a
particular count. Grammars extracted from our syntactic
aligner produce not only more low-count rules, but also
more high-count rules than GIZA++.
extracted grammar and low (or 0) count in the
GIZA++ grammar. Of course, we can always
cherry-pick such examples, but a few rules were il-
luminating. For example, for the ? . . .?? con-
struction discussed earlier, our aligner permits ex-
traction of the general rule (PP? IN1 NP2 ;? NP2
IN1) 3087 times, and the lexicalized rule (PP? be-
fore NP ; ? NP ??) 118 times. In constrast, the
GIZA++ grammar extracts the latter only 23 times
and the former not at all. The more complex rule
(NP? NP2 , who S1 , ; S1 ? NP2), which captures
a common appositive construction, was absent from
the GIZA++ grammar but occurred 63 in ours.
6 Conclusion
We have described a syntactic alignment model
which explicitly aligns nodes of a syntactic parse in
one language to spans in another, making it suitable
for use in many syntactic translation systems. Our
model is unsupervised and can be efficiently trained
with a straightforward application of EM. We have
demonstrated that our model can accurately capture
many syntactic correspondences, and is robust in the
face of syntactic divergence between language pairs.
Our aligner permits the extraction of more reliable,
high-count rules when compared to a standard word-
alignment baseline. These high-count rules also pro-
duce improvements in BLEU score.
Acknowledgements
This project is funded in part by the NSF under grant 0643742;
by BBN under DARPA contract HR0011-06-C-0022; and an
NSERC Postgraduate Fellowship. The authors would like to
thank Michael Auli for his input.
125
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007.
English chinese translation treebank v 1.0. web download.
In LDC2007T02.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammar. In
Proceedings of the North American Association for Compu-
tational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In Pro-
ceedings of the Association of Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion transduction
grammar for joint phrasal translation modeling. In Workshop
on Syntax and Structure in Statistical Translation.
David Chiang. 2004. Evaluating grammar formalisms for ap-
plications to natural language processing and biological se-
quence analysis. Ph.D. thesis, University of Pennsylvania.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In The Annual Conference of
the Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of
syntax-directed tree to string grammar induction. In Pro-
ceedings of the Conference on Emprical Methods for Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring word alignments
to syntactic machine translation. In The Annual Conference
of the Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Workshop on Statistical Machine Translation at
NAACL.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of NAACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Us-
ing syntax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proceedings of the Association for Compu-
tational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Association for Computational Linguis-
tics.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, John Blitzer, John Denero, and Dan Klein.
2009. Better word alignments with supervised itg models.
In Proceedings of the Association for Computational Lin-
guistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of CHSLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,
Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,
Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an
open source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2009. Bayesian learning of
phrasal tree-to-string templates. In Proceedings of EMNLP.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-
tree translation with packed forests. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
Jonathan May and Kevin Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of the Con-
ference on Emprical Methods for Natural Language Pro-
cessing.
Robert C. Moore. 2004. Improving ibm word alignment model
1. In The Annual Conference of the Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In ICSLP 2002.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the Association of
Computational Linguistics.
Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment:
supervised or unsupervised? In Proceedings of the Confer-
ence on Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of the North American Chapter of the Associa-
tion for Computational Linguistics.
126
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 127?135,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Joint Parsing and Alignment with Weakly Synchronized Grammars
David Burkett John Blitzer Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,blitzer,klein}@cs.berkeley.edu
Abstract
Syntactic machine translation systems extract
rules from bilingual, word-aligned, syntacti-
cally parsed text, but current systems for pars-
ing and word alignment are at best cascaded
and at worst totally independent of one an-
other. This work presents a unified joint model
for simultaneous parsing and word alignment.
To flexibly model syntactic divergence, we de-
velop a discriminative log-linear model over
two parse trees and an ITG derivation which
is encouraged but not forced to synchronize
with the parses. Our model gives absolute
improvements of 3.3 F1 for English pars-
ing, 2.1 F1 for Chinese parsing, and 5.5 F1
for word alignment over each task?s indepen-
dent baseline, giving the best reported results
for both Chinese-English word alignment and
joint parsing on the parallel portion of the Chi-
nese treebank. We also show an improvement
of 1.2 BLEU in downstream MT evaluation
over basic HMM alignments.
1 Introduction
Current syntactic machine translation (MT) sys-
tems build synchronous context free grammars from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006). Extracting such grammars
requires that bilingual word alignments and mono-
lingual syntactic parses be compatible. Because of
this, much recent work in both word alignment and
parsing has focused on changing aligners to make
use of syntactic information (DeNero and Klein,
2007; May and Knight, 2007; Fossum et al, 2008)
or changing parsers to make use of word align-
ments (Smith and Smith, 2004; Burkett and Klein,
2008; Snyder et al, 2009). In the first case, how-
ever, parsers do not exploit bilingual information.
In the second, word alignment is performed with a
model that does not exploit syntactic information.
This work presents a single, joint model for parsing
and word alignment that allows both pieces to influ-
ence one another simultaneously.
While building a joint model seems intuitive,
there is no easy way to characterize how word align-
ments and syntactic parses should relate to each
other in general. In the ideal situation, each pair
of sentences in a bilingual corpus could be syntacti-
cally parsed using a synchronous context-free gram-
mar. Of course, real translations are almost always
at least partially syntactically divergent. Therefore,
it is unreasonable to expect perfect matches of any
kind between the two sides? syntactic trees, much
less expect that those matches be well explained at
a word level. Indeed, it is sometimes the case that
large pieces of a sentence pair are completely asyn-
chronous and can only be explained monolingually.
Our model exploits synchronization where pos-
sible to perform more accurately on both word
alignment and parsing, but also allows indepen-
dent models to dictate pieces of parse trees and
word alignments when synchronization is impossi-
ble. This notion of ?weak synchronization? is pa-
rameterized and estimated from data to maximize
the likelihood of the correct parses and word align-
ments. Weak synchronization is closely related to
the quasi-synchronous models of Smith and Eis-
ner (2006; 2009) and the bilingual parse reranking
model of Burkett and Klein (2008), but those models
assume that the word alignment of a sentence pair is
known and fixed.
To simultaneously model both parses and align-
127
ments, our model loosely couples three separate
combinatorial structures: monolingual trees in the
source and target languages, and a synchronous ITG
alignment that links the two languages (but is not
constrained to match linguistic syntax). The model
has no hard constraints on how these three struc-
tures must align, but instead contains a set of ?syn-
chronization? features that are used to propagate
influence between the three component grammars.
The presence of synchronization features couples
the parses and alignments, but makes exact inference
in the model intractable; we show how to use a vari-
ational mean field approximation, both for comput-
ing approximate feature expectations during train-
ing, and for performing approximate joint inference
at test time.
We train our joint model on the parallel, gold
word-aligned portion of the Chinese treebank.
When evaluated on parsing and word alignment, this
model significantly improves over independently-
trained baselines: the monolingual parser of Petrov
and Klein (2007) and the discriminative word
aligner of Haghighi et al (2009). It also improves
over the discriminative, bilingual parsing model
of Burkett and Klein (2008), yielding the highest
joint parsing F1 numbers on this data set. Finally,
our model improves word alignment in the context
of translation, leading to a 1.2 BLEU increase over
using HMM word alignments.
2 Joint Parsing and Alignment
Given a source-language sentence, s, and a target-
language sentence, s?, we wish to predict a source
tree t, a target tree t?, and some kind of alignment
a between them. These structures are illustrated in
Figure 1.
To facilitate these predictions, we define a condi-
tional distribution P(t, a, t?|s, s?). We begin with a
generic conditional exponential form:
P(t, a, t?|s, s?) ? exp ?>?(t, a, t?, s, s?) (1)
Unfortunately, a generic model of this form is in-
tractable, because we cannot efficiently sum over
all triples (t, a, t?) without some assumptions about
how the features ?(t, a, t?, s, s?) decompose.
One natural solution is to restrict our candidate
triples to those given by a synchronous context free
grammar (SCFG) (Shieber and Schabes, 1990). Fig-
ure 1(a) gives a simple example of generation from
a log-linearly parameterized synchronous grammar,
together with its features. With the SCFG restric-
tion, we can sum over the necessary structures using
the O(n6) bitext inside-outside algorithm, making
P(t, a, t?|s, s?) relatively efficient to compute expec-
tations under.
Unfortunately, an SCFG requires that all the con-
stituents of each tree, from the root down to the
words, are generated perfectly in tandem. The re-
sulting inability to model any level of syntactic di-
vergence prevents accurate modeling of the individ-
ual monolingual trees. We will consider the run-
ning example from Figure 2 throughout the paper.
Here, for instance, the verb phrase established in
such places as Quanzhou, Zhangzhou, etc. in En-
glish does not correspond to any single node in the
Chinese tree. A synchronous grammar has no choice
but to analyze this sentence incorrectly, either by ig-
noring this verb phrase in English or postulating an
incorrect Chinese constituent that corresponds to it.
Therefore, instead of requiring strict synchroniza-
tion, our model treats the two monolingual trees and
the alignment as separate objects that can vary arbi-
trarily. However, the model rewards synchronization
appropriately when the alignment brings the trees
into correspondence.
3 Weakly Synchronized Grammars
We propose a joint model which still gives probabil-
ities on triples (t, a, t?). However, instead of using
SCFG rules to synchronously enforce the tree con-
straints on t and t?, we only require that each of t
and t? be well-formed under separate monolingual
CFGs.
In order to permit efficient enumeration of all pos-
sible alignments a, we also restrict a to the set of
unlabeled ITG bitrees (Wu, 1997), though again we
do not require that a relate to t or t? in any particular
way. Although this assumption does limit the space
of possible word-level alignments, for the domain
we consider (Chinese-English word alignment), the
reduced space still contains almost all empirically
observed alignments (Haghighi et al, 2009).1 For
1See Section 8.1 for some new terminal productions re-
quired to make this true for the parallel Chinese treebank.
128
NP VP
S
NP
VP
IP
b
0
b
1
b
2
Features
?( (IP, b
0
, S), s, s? )
?( (NP, b
1
, NP), s, s? )
?( (VP, b
2
, VP), s, s? )
NP VP
S
NP
IP
b
0
b
1
b
2
VP
AP
Features
   
   
(IP, s)     
   
(b
0
, s, s?)
   
   
(NP, s)     
   
(b
1
, s, s?)
   
   
(VP, s)     
   
(b
2
, s, s?)
   
   
(S, s?)
      (IP, b
0
)
   
   
(NP, s?)
      (b
0
, S)
   
   
(AP, s?)
      (b
1
, NP)
   
   
(VP, s?)
      (IP, b
0
, S)
Parsing
Alignment
Synchronization
?E
?E
?E
?E
?F
?F
?F
?A
?A
?A
?!
?!
?!"
?!
(a) Synchronous Rule (b) Asynchronous Rule
Figure 1: Source trees, t (right), alignments, a (grid), and target trees, t? (top), and feature decompositions for syn-
chronous (a) and weakly synchronous (b) grammars. Features always condition on bispans and/or anchored syntactic
productions, but weakly synchronous grammars permit more general decompositions.
example, in Figure 2, the word alignment is ITG-
derivable, and each of the colored rectangles is a bi-
span in that derivation.
There are no additional constraints beyond the
independent, internal structural constraints on t, a,
and t?. This decoupling permits derivations like that
in Figure 1(b), where the top-level syntactic nodes
align, but their children are allowed to diverge. With
the three structures separated, our first model is a
completely factored decomposition of (1).
Formally, we represent a source tree t as a set of
nodes {n}, each node representing a labeled span.
Likewise, a target tree t? is a set of nodes {n?}.2 We
represent alignments a as sets of bispans {b}, indi-
cated by rectangles in Figure 1.3 Using this notation,
the initial model has the following form:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)
?
?
(2)
Here ?F (n, s) indicates a vector of source node fea-
tures, ?E(n?, s?) is a vector of target node features,
and ?A(b, s, s?) is a vector of alignment bispan fea-
tures. Of course, this model is completely asyn-
2For expositional clarity, we describe n and n? as labeled
spans only. However, in general, features that depend on n or
n? are permitted to depend on the entire rule, and do in our final
system.
3Alignments a link arbitrary spans of s and s? (including
non-constituents and individual words). We discuss the relation
to word-level alignments in Section 4.
chronous so far, and fails to couple the trees and
alignments at all. To permit soft constraints between
the three structures we are modeling, we add a set of
synchronization features.
For n ? t and b ? a, we say that n b if n and b
both map onto the same span of s. We define b n?
analogously for n? ? t?. We now consider three
different types of synchronization features. Source-
alignment synchronization features ?(n, b) are ex-
tracted whenever n  b. Similarly, target-alignment
features ?(b, n?) are extracted if b  n?. These
features capture phenomena like that of bispan b7
in Figure 2. Here the Chinese noun? synchronizes
with the ITG derivation, but the English projection
of b7 is a distituent. Finally, we extract source-target
features ?./(n, b, n?) whenever nbn?. These fea-
tures capture complete bispan synchrony (as in bi-
span b8) and can be expressed over triples (n, b, n?)
which happen to align, allowing us to reward syn-
chrony, but not requiring it. All of these licensing
conditions are illustrated in Figure 1(b).
With these features added, the final form of the
model is:
P(t, a, t?|s, s?) ? exp
?
?
?
n?t
?>?F (n, s)+
?
b?a
?>?A(b, s, s?)+
?
n??t?
?>?E(n?, s?)+
?
nb
?>?(n, b)+
?
bn?
?>?(b, n?)+
?
nbn?
?>?./(n, b, n?)
?
?
(3)
129
We emphasize that because of the synchronization
features, this final form does not admit any known
efficient dynamic programming for the exact com-
putation of expectations. We will therefore turn to a
variational inference method in Section 6.
4 Features
With the model?s locality structure defined, we
just need to specify the actual feature function,
?. We divide the features into three types: pars-
ing features (?F (n, s) and ?E(n?, s?)), alignment
features (?A(b, s, s?)) and synchronization features
(?(n, b), ?(b, n?), and ?./(n, b, n?)). We detail
each of these in turn here.
4.1 Parsing
The monolingual parsing features we use are sim-
ply parsing model scores under the parser of Petrov
and Klein (2007). While that parser uses heavily re-
fined PCFGs with rule probabilities defined at the
refined symbol level, we interact with its posterior
distribution via posterior marginal probabilities over
unrefined symbols. In particular, to each unrefined
anchored production iAj ? iBkCj , we associate a
single feature whose value is the marginal quantity
log P(iBkCj |iAj , s) under the monolingual parser.
These scores are the same as the variational rule
scores of Matsuzaki et al (2005).4
4.2 Alignment
We begin with the same set of alignment features
as Haghighi et al (2009), which are defined only for
terminal bispans. In addition, we include features on
nonterminal bispans, including a bias feature, fea-
tures that measure the difference in size between
the source and target spans, features that measure
the difference in relative sentence position between
the source and target spans, and features that mea-
sure the density of word-to-word alignment poste-
riors under a separate unsupervised word alignment
model.
4Of course the structure of our model permits any of the
additional rule-factored monolingual parsing features that have
been described in the parsing literature, but in the present work
we focus on the contributions of joint modeling.
4.3 Synchronization
Our synchronization features are indicators for the
syntactic types of the participating nodes. We de-
termine types at both a coarse (more collapsed
than Treebank symbols) and fine (Treebank sym-
bol) level. At the coarse level, we distinguish be-
tween phrasal nodes (e.g. S, NP), synthetic nodes
introduced in the process of binarizing the grammar
(e.g. S?, NP?), and part-of-speech nodes (e.g. NN,
VBZ). At the fine level, we distinguish all nodes
by their exact label. We use coarse and fine types
for both partially synchronized (source-alignment or
target-alignment) features and completely synchro-
nized (source-alignment-target) features. The inset
of Figure 2 shows some sample features. Of course,
we could devise even more sophisticated features by
using the input text itself. As we shall see, however,
our model gives significant improvements with these
simple features alone.
5 Learning
We learn the parameters of our model on the paral-
lel portion of the Chinese treebank. Although our
model assigns probabilities to entire synchronous
derivations of sentences, the parallel Chinese tree-
bank gives alignments only at the word level (1 by
1 bispans in Figure 2). This means that our align-
ment variable a is not fully observed. Because of
this, given a particular word alignment w, we max-
imize the marginal probability of the set of deriva-
tions A(w) that are consistent with w (Haghighi et
al., 2009).5
L(?)=log
?
a?A(wi)
P(ti, a, t?i|si, s?i)
We maximize this objective using standard gradient
methods (Nocedal and Wright, 1999). As with fully
visible log-linear models, the gradient for the ith sen-
tence pair with respect to ? is a difference of feature
expectations:
?L(?) =EP(a|ti,wi,t?i,si,s?i)
[
?(ti, a, t?i, si, s?i)
]
? EP(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
] (4)
5We also learn from non-ITG alignments by maximizing the
marginal probability of the set of minimum-recall error align-
ments in the same way as Haghighi et al (2009)
130
NP
NP
IN
PP
NPIN
PPVBN
VPVBD
VPNP
S
JJ NNS
...
were established in such places as Quanzhou Zhangzhou etc.
?
??
??
?
?
??
?
...
NP
P
NN
NP
PP
VP
VV
AS
NP
VP
b
8
b
7
b
4
Sample Synchronization Features
NP, b8,NP
NN, b7
?!"( ) = CoarseSourceTarget?phrasal, phrasal? : 1
FineSourceTarget?NP,NP? : 1
?!( ) = CoarseSourceAlign?pos? : 1
FineSourceAlign?NN? : 1
Figure 2: An example of a Chinese-English sentence pair with parses, word alignments, and a subset of the full optimal
ITG derivation, including one totally unsynchronized bispan (b4), one partially synchronized bispan (b7), and and fully
synchronized bispan (b8). The inset provides some examples of active synchronization features (see Section 4.3) on
these bispans. On this example, the monolingual English parser erroneously attached the lower PP to the VP headed by
established, and the non-syntactic ITG word aligner misaligned? to such instead of to etc. Our joint model corrected
both of these mistakes because it was rewarded for the synchronization of the two NPs joined by b8.
We cannot efficiently compute the model expecta-
tions in this equation exactly. Therefore we turn next
to an approximate inference method.
6 Mean Field Inference
Instead of computing the model expectations from
(4), we compute the expectations for each sentence
pair with respect to a simpler, fully factored distri-
bution Q(t, a, t?) = q(t)q(a)q(t?). Rewriting Q in
log-linear form, we have:
Q(t, a, t?) ? exp
?
?
?
n?t
?n +
?
b?a
?b +
?
n??t?
?n?
?
?
Here, the ?n, ?b and ?n? are variational parameters
which we set to best approximate our weakly syn-
chronized model from (3):
?? = argmin
?
KL
(
Q?||P?(t, a, t?|s, s?)
)
Once we have found Q, we compute an approximate
gradient by replacing the model expectations with
expectations under Q:
EQ(a|wi)
[
?(ti, a, t?i, si, s?i)
]
? EQ(t,a,t?|si,s?i)
[
?(t, a, t?, si, s?i)
]
Now, we will briefly describe how we compute Q.
First, note that the parameters ? of Q factor along
individual source nodes, target nodes, and bispans.
The combination of the KL objective and our par-
ticular factored form of Q make our inference pro-
cedure a structured mean field algorithm (Saul and
Jordan, 1996). Structured mean field techniques are
well-studied in graphical models, and our adaptation
in this section to multiple grammars follows stan-
dard techniques (see e.g. Wainwright and Jordan,
2008).
Rather than derive the mean field updates for ?,
we describe the algorithm (shown in Figure 3) pro-
cedurally. Similar to block Gibbs sampling, we it-
eratively optimize each component (source parse,
target parse, and alignment) of the model in turn,
conditioned on the others. Where block Gibbs sam-
pling conditions on fixed trees or ITG derivations,
our mean field algorithm maintains uncertainty in
131
Input: sentence pair (s, s?)
parameter vector ?
Output: variational parameters ?
1. Initialize
?0n ? ?
>?F (n, s)
?0b??
>?A(b, s, s?)
?0n???
>?E(n?, s?)
?0n ?
?
t q?0(t)I(n ? t), etc for ?
0
b , ?
0
n?
2. While not converged, for each n, n?, b in
the monolingual and ITG charts
?in ? ?
>
(
?F (n, s) +
?
b,nb ?
i?1
b ?(n, b)+
?
b,nb
?
n?,bn? ?
i?1
b ?
i?1
n? ?./(n, b, n
?)
)
?in ?
?
t q?(t)I(n ? t) (inside-outside)
?ib ? ?
>
(
?A(b, s, s?) +
?
n,nb ?
i?1
n ?(n, b)+
?
n?,bn? ?
i?1
n? ?(b, n
?)+
?
n,nb
?
n?,bn? ?
i?1
n ?
i?1
n? ?./(n, b, n
?)
)
?b ?
?
a q?(a)I(b ? a) (bitext inside-outside)
updates for ?in? , ?
i
n? analogous to ?
i
n, ?
i
n
3. Return variational parameters ?
Figure 3: Structured mean field inference for the weakly
synchronized model. I(n ? t) is an indicator value for
the presence of node n in source tree t.
the form of monolingual parse forests or ITG forests.
The key components to this uncertainty are the
expected counts of particular source nodes, target
nodes, and bispans under the mean field distribution:
?n =
?
t
q?(t)I(n ? t)
?n? =
?
t?
q?(t?)I(n? ? t?)
?b =
?
a
q?(a)I(b ? a)
Since dynamic programs exist for summing over
each of the individual factors, these expectations can
be computed in polynomial time.
6.1 Pruning
Although we can approximate the expectations from
(4) in polynomial time using our mean field distribu-
tion, in practice we must still prune the ITG forests
and monolingual parse forests to allow tractable in-
ference. We prune our ITG forests using the same
basic idea as Haghighi et al (2009), but we em-
ploy a technique that allows us to be more aggres-
sive. Where Haghighi et al (2009) pruned bispans
based on how many unsupervised HMM alignments
were violated, we first train a maximum-matching
word aligner (Taskar et al, 2005) using our super-
vised data set, which has only half the precision er-
rors of the unsupervised HMM. We then prune ev-
ery bispan which violates at least three alignments
from the maximum-matching aligner. When com-
pared to pruning the bitext forest of our model with
Haghighi et al (2009)?s HMM technique, this new
technique allows us to maintain the same level of ac-
curacy while cutting the number of bispans in half.
In addition to pruning the bitext forests, we also
prune the syntactic parse forests using the mono-
lingual parsing model scores. For each unrefined
anchored production iAj ? iBkCj , we com-
pute the marginal probability P(iAj ,i Bk,k Cj |s) un-
der the monolingual parser (these are equivalent to
the maxrule scores from Petrov and Klein 2007). We
only include productions where this probability is
greater than 10?20. Note that at training time, we are
not guaranteed that the gold trees will be included
in the pruned forest. Because of this, we replace the
gold trees ti, t?i with oracle trees from the pruned for-
est, which can be found efficiently using a variant of
the inside algorithm (Huang, 2008).
7 Testing
Once the model has been trained, we still need to
determine how to use it to predict parses and word
alignments for our test sentence pairs. Ideally, given
the sentence pair (s, s?), we would find:
(t?, w?, t??) = argmax
t,w,t?
P(t, w, t?|s, s?)
= argmax
t,w,t?
?
a?A(w)
P(t, a, t?|s, s?)
Of course, this is also intractable, so we once again
resort to our mean field approximation. This yields
the approximate solution:
(t?, w?, t??) = argmax
t,w,t?
?
a?A(w)
Q(t, a, t?)
However, recall that Q incorporates the model?s mu-
tual constraint into the variational parameters, which
132
factor into q(t), q(a), and q(t?). This allows us to
simplify further, and find the maximum a posteriori
assignments under the variational distribution. The
trees can be found quickly using the Viterbi inside
algorithm on their respective qs. However, the sum
for computing w? under q is still intractable.
As we cannot find the maximum probability word
alignment, we provide two alternative approaches
for finding w?. The first is to just find the Viterbi
ITG derivation a? = argmaxa q(a) and then set w?
to contain exactly the 1x1 bispans in a?. The second
method, posterior thresholding, is to compute poste-
rior marginal probabilities under q for each 1x1 cell
beginning at position i, j in the word alignment grid:
m(i, j) =
?
a
q(a)I((i, i+ 1, j, j + 1) ? a)
We then include w(i, j) in w? if m(w(i, j)) > ? ,
where ? is a threshold chosen to trade off precision
and recall. For our experiments, we found that the
Viterbi alignment was uniformly worse than poste-
rior thresholding. All the results from the next sec-
tion use the threshold ? = 0.25.
8 Experiments
We trained and tested our model on the translated
portion of the Chinese treebank (Bies et al, 2007),
which includes hand annotated Chinese and English
parses and word alignments. We separated the data
into three sets: train, dev, and test, according to the
standard Chinese treebank split. To speed up train-
ing, we only used training sentences of length ? 50
words, which left us with 1974 of 2261 sentences.
We measured the results in two ways. First, we
directly measured F1 for English parsing, Chinese
parsing, and word alignment on a held out section of
the hand annotated corpus used to train the model.
Next, we further evaluated the quality of the word
alignments produced by our model by using them as
input for a machine translation system.
8.1 Dataset-specific ITG Terminals
The Chinese treebank gold word alignments include
significantly more many-to-many word alignments
than those used by Haghighi et al (2009). We are
able to produce some of these many-to-many align-
ments by including new many-to-many terminals in
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(a) 2x2
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(b) 2x3
t
h
e
e
n
t
i
r
e
c
o
u
n
t
r
y
i
n
r
e
c
e
n
t
y
e
a
r
s
b
o
t
h
s
i
d
e
s
?
?
?
?
??
?
(c) Gapped 2x3
Figure 4: Examples of phrasal alignments that can be rep-
resented by our new ITG terminal bispans.
our ITG word aligner, as shown in Figure 4. Our
terminal productions sometimes capture non-literal
translation like both sides or in recent years. They
also can allow us to capture particular, systematic
changes in the annotation standard. For example,
the gapped pattern from Figure 4 captures the stan-
dard that English word the is always aligned to the
Chinese head noun in a noun phrase. We featurize
these non-terminals with features similar to those
of Haghighi et al (2009), and all of the alignment
results we report in Section 8.2 (both joint and ITG)
employ these features.
8.2 Parsing and Word Alignment
To compute features that depend on external models,
we needed to train an unsupervised word aligner and
monolingual English and Chinese parsers. The un-
supervised word aligner was a pair of jointly trained
HMMs (Liang et al, 2006), trained on the FBIS cor-
pus. We used the Berkeley Parser (Petrov and Klein,
2007) for both monolingual parsers, with the Chi-
nese parser trained on the full Chinese treebank, and
the English parser trained on a concatenation of the
Penn WSJ corpus (Marcus et al, 1993) and the En-
glish side of train.6
We compare our parsing results to the mono-
lingual parsing models and to the English-Chinese
bilingual reranker of Burkett and Klein (2008),
trained on the same dataset. The results are in
Table 1. For word alignment, we compare to
6To avoid overlap in the data used to train the monolingual
parsers and the joint model, at training time, we used a separate
version of the Chinese parser, trained only on articles 400-1151
(omitting articles in train). For English parsing, we deemed it
insufficient to entirely omit the Chinese treebank data from the
monolingual parser?s training set, as otherwise the monolingual
parser would be trained entirely on out-of-domain data. There-
fore, at training time we used two separate English parsers: to
compute model scores for the first half of train, we used a parser
trained on a concatenation of the WSJ corpus and the second
half of train, and vice versa for the remaining sentences.
133
Test Results
Ch F1 Eng F1 Tot F1
Monolingual 83.6 81.2 82.5
Reranker 86.0 83.8 84.9
Joint 85.7 84.5 85.1
Table 1: Parsing results. Our joint model has the highest
reported F1 for English-Chinese bilingual parsing.
Test Results
Precision Recall AER F1
HMM 86.0 58.4 30.0 69.5
ITG 86.8 73.4 20.2 79.5
Joint 85.5 84.6 14.9 85.0
Table 2: Word alignment results. Our joint model has the
highest reported F1 for English-Chinese word alignment.
the baseline unsupervised HMM word aligner and
to the English-Chinese ITG-based word aligner
of Haghighi et al (2009). The results are in Table 2.
As can be seen, our model makes substantial im-
provements over the independent models. For pars-
ing, we improve absolute F1 over the monolingual
parsers by 2.1 in Chinese, and by 3.3 in English.
For word alignment, we improve absolute F1 by 5.5
over the non-syntactic ITG word aligner. In addi-
tion, our English parsing results are better than those
of the Burkett and Klein (2008) bilingual reranker,
the current top-performing English-Chinese bilin-
gual parser, despite ours using a much simpler set
of synchronization features.
8.3 Machine Translation
We further tested our alignments by using them to
train the Joshua machine translation system (Li and
Khudanpur, 2008). Table 3 describes the results of
our experiments. For all of the systems, we tuned
Rules Tune Test
HMM 1.1M 29.0 29.4
ITG 1.5M 29.9 30.4?
Joint 1.5M 29.6 30.6
Table 3: Tune and test BLEU results for machine transla-
tion systems built with different alignment tools. ? indi-
cates a statistically significant difference between a sys-
tem?s test performance and the one above it.
on 1000 sentences of the NIST 2004 and 2005 ma-
chine translation evaluations, and tested on 400 sen-
tences of the NIST 2006 MT evaluation. Our train-
ing set consisted of 250k sentences of newswire dis-
tributed with the GALE project, all of which were
sub-sampled to have high Ngram overlap with the
tune and test sets. All of our sentences were of
length at most 40 words. When building the trans-
lation grammars, we used Joshua?s default ?tight?
phrase extraction option. We ran MERT for 4 itera-
tions, optimizing 20 weight vectors per iteration on
a 200-best list.
Table 3 gives the results. On the test set, we also
ran the approximate randomization test suggested by
Riezler and Maxwell (2005). We found that our joint
parsing and alignment system significantly outper-
formed the HMM aligner, but the improvement over
the ITG aligner was not statistically significant.
9 Conclusion
The quality of statistical machine translation mod-
els depends crucially on the quality of word align-
ments and syntactic parses for the bilingual training
corpus. Our work presented the first joint model
for parsing and alignment, demonstrating that we
can improve results on both of these tasks, as well
as on downstream machine translation, by allowing
parsers and word aligners to simultaneously inform
one another. Crucial to this improved performance
is a notion of weak synchronization, which allows
our model to learn when pieces of a grammar are
synchronized and when they are not. Although ex-
act inference in the weakly synchronized model is
intractable, we developed a mean field approximate
inference scheme based on monolingual and bitext
parsing, allowing for efficient inference.
Acknowledgements
We thank Adam Pauls and John DeNero for their
help in running machine translation experiments.
We also thank the three anonymous reviewers for
their helpful comments on an earlier draft of this
paper. This project is funded in part by NSF
grants 0915265 and 0643742, an NSF graduate re-
search fellowship, the CIA under grant HM1582-09-
1-0021, and BBN under DARPA contract HR0011-
06-C-0022.
134
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese translation treebank v 1.0.
Web download. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In EMNLP.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008.
Using syntax to improve word alignment for syntax-
based statistical machine translation. In ACL MT
Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuki Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Jon May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In EMNLP.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Workshop on Intrinsic and Extrinsic Eval-
uation Methods for MT and Summarization, ACL.
Lawrence Saul and Michael Jordan. 1996. Exploit-
ing tractable substructures in intractable networks. In
NIPS.
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In ACL.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical Models, Exponential Families, and Varia-
tional Inference. Now Publishers Inc., Hanover, MA,
USA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-AKA syntax aug-
mented machine translation system for IWSLT-06. In
IWSLT.
135
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 385?393,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Coreference Resolution in a Modular, Entity-Centered Model
Aria Haghighi
Computer Science Division
University of California, Berkeley
aria42@cs.berkeley.edu
Dan Klein
Computer Science Division
University of California, Berkeley
klein@cs.berkeley.edu
Abstract
Coreference resolution is governed by syntac-
tic, semantic, and discourse constraints. We
present a generative, model-based approach in
which each of these factors is modularly en-
capsulated and learned in a primarily unsu-
pervised manner. Our semantic representation
first hypothesizes an underlying set of latent
entity types, which generate specific entities
that in turn render individual mentions. By
sharing lexical statistics at the level of abstract
entity types, our model is able to substantially
reduce semantic compatibility errors, result-
ing in the best results to date on the complete
end-to-end coreference task.
1 Introduction
Coreference systems exploit a variety of informa-
tion sources, ranging from syntactic and discourse
constraints, which are highly configurational, to se-
mantic constraints, which are highly contingent on
lexical meaning and world knowledge. Perhaps be-
cause configurational features are inherently easier
to learn from small data sets, past work has often
emphasized them over semantic knowledge.
Of course, all state-of-the-art coreference systems
have needed to capture semantic compatibility to
some degree. As an example of nominal headword
compatibility, a ?president? can be a ?leader? but
cannot be not an ?increase.? Past systems have of-
ten computed the compatibility of specific headword
pairs, extracted either from lexical resources (Ng,
2007; Bengston and Roth, 2008; Rahman and Ng,
2009), web statistics (Yang et al, 2005), or sur-
face syntactic patterns (Haghighi and Klein, 2009).
While the pairwise approach has high precision, it is
neither realistic nor scalable to explicitly enumerate
all pairs of compatible word pairs. A more compact
approach has been to rely on named-entity recog-
nition (NER) systems to give coarse-grained entity
types for each mention (Soon et al, 1999; Ng and
Cardie, 2002). Unfortunately, current systems use
small inventories of types and so provide little con-
straint. In general, coreference errors in state-of-the-
art systems are frequently due to poor models of se-
mantic compatibility (Haghighi and Klein, 2009).
In this work, we take a primarily unsupervised ap-
proach to coreference resolution, broadly similar to
Haghighi and Klein (2007), which addresses this is-
sue. Our generative model exploits a large inven-
tory of distributional entity types, including standard
NER types like PERSON and ORG, as well as more
refined types like WEAPON and VEHICLE. For each
type, distributions over typical heads, modifiers, and
governors are learned from large amounts of unla-
beled data, capturing type-level semantic informa-
tion (e.g. ?spokesman? is a likely head for a PER-
SON). Each entity inherits from a type but captures
entity-level semantic information (e.g. ?giant? may
be a likely head for the Microsoft entity but not all
ORGs). Separately from the type-entity semantic
module, a log-linear discourse model captures con-
figurational effects. Finally, a mention model assem-
bles each textual mention by selecting semantically
appropriate words from the entities and types.
Despite being almost entirely unsupervised, our
model yields the best reported end-to-end results on
a range of standard coreference data sets.
2 Key Abstractions
The key abstractions of our model are illustrated in
Figure 1 and described here.
Mentions: A mention is an observed textual ref-
erence to a latent real-world entity. Mentions are as-
385
Person
[0: 0.30,
 1:0.25,
 2:0.20, ...]
NOM-HEAD
[1: 0.39,
 0:0.18,
 2:0.13, ...]
[Obama: 0.02,
  Smith:0.015,
  Jr.: 0.01, ...]
[president: 0.14,
 painter:0.11,
 senator: 0.10,...]
NAM-HEAD
r ?r fr
NOM-HEAD [president, leader]
NAM-HEAD [Obama, Barack]
r Lr
Barack Obama
NOM-HEAD [painter]
NAM-HEAD [Picasso, Pablo]
r Lr
Pablo Picasso
NN-MOD Mr.
NAM-HEAD Obama
r wr
NOM-HEAD president
r wr
Types
Entities
Mentions
(c)
(b)
(a)
?Mr. Obama? ?the president?
... ... ...
Figure 1: The key abstractions of our model (Section 2).
(a) Mentions map properties (r) to words (wr). (b) Enti-
ties map properties (r) to word lists (Lr). (c) Types map
properties (r) to distributions over property words (?r)
and the fertilities of those distributions (fr). For (b) and
(c), we only illustrate a subset of the properties.
sociated with nodes in a parse tree and are typically
realized as NPs. There are three basic forms of men-
tions: proper (denoted NAM), nominal (NOM), and
pronominal (PRO). We will often describe proper
and nominal mentions together as referring men-
tions.
We represent each mention M as a collection of
key-value pairs. The keys are called properties and
the values are words. For example, the left mention
in Figure 1(a) has a proper head property, denoted
NAM-HEAD, with value ?Obama.? The set of prop-
erties we consider, denoted R, includes several va-
rieties of heads, modifiers, and governors (see Sec-
tion 5.2 for details). Not every mention has a value
for every property.
Entities: An entity is a specific individual or ob-
ject in the world. Entities are always latent in text.
Where a mention has a single word for each prop-
erty, an entity has a list of signature words. For-
mally, entities are mappings from properties r ? R
to lists Lr of ?canonical? words which that entity
uses for that property. For instance in Figure 1(b),
the list of nominal heads for the Barack Obama en-
tity includes ?president.?
Types: Coreference systems often make a men-
tion / entity distinction. We extend this hierarchy
to include types, which represent classes of entities
(PERSON, ORGANIZATION, and so on). Types allow
the sharing of properties across entities and mediate
the generation of entities in our model (Section 3.1).
See Figure 1(c) for a concrete example.
We represent each type ? as a mapping between
properties r and pairs of multinomials (?r, fr). To-
gether, these distributions control the lists Lr for en-
tities of that type. ?r is a unigram distribution of
words that are semantically licensed for property r.
fr is a ?fertility? distribution over the integers that
characterizes entity list lengths. For example, for the
type PERSON, ?r for proper heads is quite flat (there
are many last names) but fr is peaked at 1 (people
have a single last name).
3 Generative Model
We now describe our generative model. At the pa-
rameter level, we have one parameter group for the
types ? = (?, ?1, . . . , ?t), where ? is a multinomial
prior over a fixed number t of types and the {?i} are
the parameters for each individual type, described in
greater detail below. A second group comprises log-
linear parameters pi over discourse choices, also de-
scribed below. Together, these two groups are drawn
according to P (? |?)P (pi|?2), where ? and ?2 are a
small number of scalar hyper-parameters described
in Section 4.
Conditioned on the parameters (? ,pi), a docu-
ment is generated as follows: A semantic module
generates a sequence E of entities. E is in prin-
ciple infinite, though during inference only a finite
number are ever instantiated. A discourse module
generates a vector Z which assigns an entity in-
dex Zi to each mention position i. Finally, a men-
tion generation module independently renders the
sequence of mentions (M) from their underlying en-
tities. The syntactic position and structure of men-
tions are treated as observed, including the mention
forms (pronominal, etc.). We use X to refer to this
ungenenerated information. Our model decomposes
as follows:
P (E,Z,M|? ,pi,X) =
P (E|? ) [Semantic, Section 3.1]
P (Z|pi,X) [Discourse, Section 3.2]
P (M|Z,E, ? ) [Mention, Section 3.3]
We detail each of these components in subsequent
sections.
386
TLr
?
fr ?r
ORG: 0.30
 PERS: 0.22
GPE: 0.18
LOC: 0.15
WEA: 0.12
VEH: 0.09
...
T = PERS
0: 0.30
1: 0.25
2: 0.20
3: 0.18
...
PERS
For T = PERS
president: 0.14
 painter: 0.11
senator: 0.10
minister: 0.09
leader: 0.08
official: 0.06
executive: 0.05
...
president 
leader
official
R
E
Figure 2: Depiction of the entity generation process (Sec-
tion 3.1). Each entity draws a type (T ) from ?, and, for
each property r ? R, forms a word list (Lr) by choosing
a length from T ?s fr distribution and then independently
drawing that many words from T ?s ?r distribution. Ex-
ample values are shown for the person type and the nom-
inal head property (NOM-HEAD).
3.1 Semantic Module
The semantic module is responsible for generating
a sequence of entities. Each entity E is generated
independently and consists of a type indicator T , as
well as a collection {Lr}r?R of word lists for each
property. These elements are generated as follows:
Entity Generation
Draw entity type T ? ?
For each mention property r ? R,
Fetch {(fr, ?r)} for ? T
Draw word list length |Lr| ? fr
Draw |Lr| words from w ? ?r
See Figure 2 for an illustration of this process. Each
word list Lr is generated by first drawing a list
length from fr and then independently populating
that list from the property?s word distribution ?r.1
Past work has employed broadly similar distribu-
tional models for unsupervised NER of proper men-
1There is one exception: the sizes of the proper and nomi-
nal head property lists are jointly generated, but their word lists
are still independently populated.
tions (Collins and Singer, 1999; Elsner et al, 2009).
However, to our knowledge, this is the first work
to incorporate such a model into an entity reference
process.
3.2 Discourse Module
The discourse module is responsible for choosing
an entity to evoke at each of the n mention posi-
tions. Formally, this module generates an entity as-
signment vector Z = (Z1, . . . , Zn), where Zi indi-
cates the entity index for the ith mention position.
Most linguistic inquiry characterizes NP anaphora
by the pairwise relations that hold between a men-
tion and its antecedent (Hobbs, 1979; Kehler et al,
2008). Our discourse module utilizes this pairwise
perspective to define each Zi in terms of an interme-
diate ?antecedent? variable Ai. Ai either points to a
previous antecedent mention position (Ai < i) and
?steals? its entity assignment or begins a new entity
(Ai = i). The choice ofAi is parametrized by affini-
ties spi(i, j;X) between mention positions i and j.
Formally, this process is described as:
Entity Assignment
For each mention position, i = 1, . . . , n,
Draw antecedent position Ai ? {1, . . . , i}:
P (Ai = j|X) ? spi(i, j;X)
Zi =
{
ZAi , if Ai < i
K + 1, otherwise
Here, K denotes the number of entities allocated in
the first i-1 mention positions. This process is an in-
stance of the sequential distance-dependent Chinese
Restaurant Process (DD-CRP) of Blei and Frazier
(2009). During inference, we variously exploit both
the A and Z representations (Section 4).
For nominal and pronoun mentions, there are sev-
eral well-studied anaphora cues, including centering
(Grosz et al, 1995), nearness (Hobbs, 1978), and
deterministic constraints, which have all been uti-
lized in prior coreference work (Soon et al, 1999;
Ng and Cardie, 2002). In order to combine these
cues, we take a log-linear, feature-based approach
and parametrize spi(i, j;X) = exp{pi>fX(i, j)},
where fX(i, j) is a feature vector over mention po-
sitions i and j, and pi is a parameter vector; the fea-
tures may freely condition on X. We utilize the
following features between a mention and an an-
387
tecedent: tree distance, sentence distance, and the
syntactic positions (subject, object, and oblique) of
the mention and antecedent. Features for starting a
new entity include: a definiteness feature (extracted
from the mention?s determiner), the top CFG rule
of the mention parse node, its syntactic role, and a
bias feature. These features are conjoined with the
mention form (nominal or pronoun). Additionally,
we restrict pronoun antecedents to the current and
last two sentences, and the current and last three sen-
tences for nominals. Additionally, we disallow nom-
inals from having direct pronoun antecedents.
In addition to the above, if a mention is in a de-
terministic coreference configuration, as defined in
Haghighi and Klein (2009), we force it to take the
required antecedent. In general, antecedent affini-
ties learn to prefer close antecedents in prominent
syntactic positions. We also learn that new entity
nominals are typically indefinite or have SBAR com-
plements (captured by the CFG feature).
In contrast to nominals and pronouns, the choice
of entity for a proper mention is governed more by
entity frequency than antecedent distance. We cap-
ture this by setting spi(i, j;X) in the proper case to
1 for past positions and to a fixed ? otherwise. 2
3.3 Mention Module
Once the semantic module has generated entities and
the discourse model selects entity assignments, each
mention Mi generates word values for a set of ob-
served properties Ri:
Mention Generation
For each mention Mi, i = 1, . . . , n
Fetch (T, {Lr}r?R) from EZi
Fetch {(fr, ?r)}r?R from ? T
For r ? Ri :
w ? (1? ?r)UNIFORM(Lr) + (?r)?r
For each property r, there is a hyper-parameter ?r
which interpolates between selecting a word from
the entity list Lr and drawing from the underlying
type property distribution ?r. Intuitively, a small
value of ?r indicates that an entity prefers to re-use
2As Blei and Frazier (2009) notes, when marginalizing out
theAi in this trivial case, the DD-CRP reduces to the traditional
CRP (Pitman, 2002), so our discourse model roughly matches
Haghighi and Klein (2007) for proper mentions.
? 1
Person
Organization
[software]
NN-
NOD
ORG
[Microsoft]
[company,
 firm]
NOM-
HEAD
NAM-
HEAD
T
? 2
E1 E2
Z1 Z2 Z3
M1 M2 M3
[Steve,chief, 
Microsoft]
NN-
NOD
PERS
[Ballmer,
 CEO]
[officer,
 executive]
NOM-
HEAD
NAM-
HEAD
T
joined
GOV-
SUBJ
Ballmer
Steve
NN-
HEAD
NAM-
HEAD
joined
GOV-
DOBJ
Microsoft
NAM-
HEAD
became
GOV-
DOBJ
CEO
NAM-
HEAD
? ?
E, ? E, ? E, ?
E,M E,M
E2E1 E1
M M
Figure 3: Depiction of the discourse module (Sec-
tion 3.2); each random variable is annotated with an ex-
ample value. For each mention position, an entity as-
signment (Zi) is made. Conditioned on entities (EZi ),
mentions (Mi) are rendered (Section 3.3). The Y sym-
bol denotes that a random variable is the parent of all Y
random variables.
a small number of words for property r. This is typ-
ically the case for proper and nominal heads as well
as modifiers. At the other extreme, setting ?r to 1
indicates the property isn?t particular to the entity
itself, but rather only on its type. We set ?r to 1
for pronoun heads as well as for the governor of the
head properties.
4 Learning and Inference
Our learning procedure involves finding parame-
ters and assignments which are likely under our
model?s posterior distribution P (E,Z, ? ,pi|M,X).
The model is modularized in such a way that run-
ning EM on all variables simultaneously would be
very difficult. Therefore, we adopt a variational ap-
proach which optimizes various subgroups of the
variables in a round-robin fashion, holding approx-
imations to the others fixed. We first describe the
variable groups, then the updates which optimize
them in turn.
Decomposition: We decompose the entity vari-
388
ables E into types, T, one for each entity, and word
lists, L, one for each entity and property. We decom-
pose the mentions M into referring mentions (prop-
ers and nominals), Mr, and pronominal mentions,
Mp (with sizes nr and np respectively). The en-
tity assignments Z are similarly divided into Zr and
Zp components. For pronouns, rather than use Zp,
we instead work with the corresponding antecedent
variables, denoted Ap, and marginalize over an-
tecedents to obtain Zp.
With these variable groups, we would
like to approximation our model posterior
P (T,L,Zr,Ap, ? ,pi|M,X) using a simple fac-
tored representation. Our variational approximation
takes the following form:
Q(T,L,Zr,Ap, ? ,pi) = ?r(Zr,L)
(
n?
k=1
qk(Tk)
)( np?
i=1
ri(Api )
)
?s(? )?d(pi)
We use a mean field approach to update each of the
RHS factors in turn to minimize the KL-divergence
between the current variational posterior and the
true model posterior. The ?r, ?s, and ?d factors
place point estimates on a single value, just as in
hard EM. Updating these factors involves finding the
value which maximizes the model (expected) log-
likelihood under the other factors. For instance, the
?s factor is a point estimate of the type parameters,
and is updated with:3
?s(? )? argmax? EQ??s lnP (E,Z,M, ? ,pi) (1)
where Q??s denotes all factors of the variational
approximation except for the factor being updated.
The ri (pronoun antecedents) and qk (type indica-
tor) factors maintain a soft approximation and so are
slightly more complex. For example, the ri factor
update takes the standard mean field form:
ri(Api ) ? exp{EQ?ri lnP (E,Z,M, ? ,pi)} (2)
We briefly describe the update for each additional
factor, omitting details for space.
Updating type parameters ?s(? ): The type pa-
rameters ? consist of several multinomial distri-
butions which can be updated by normalizing ex-
pected counts as in the EM algorithm. The prior
3Of course during learning, the argmax is performed over
the entire document collection, rather than a single document.
P (? |?) consists of several finite Dirichlet draws for
each multinomial, which are incorporated as pseu-
docounts.4 Given the entity type variational poste-
riors {qk(?)}, as well as the point estimates of the
L and Zr elements, we obtain expected counts from
each entity?s attribute word lists and referring men-
tion usages.
Updating discourse parameters ?d(pi): The
learned parameters for the discourse module rely on
pairwise antecedent counts for assignments to nom-
inal and pronominal mentions.5 Given these ex-
pected counts, which can be easily obtained from
other factors, the update reduces to a weighted max-
imum entropy problem, which we optimize using
LBFGS. The prior P (pi|?2) is a zero-centered nor-
mal distribution with shared diagonal variance ?2,
which is incorporated via L2 regularization during
optimization.
Updating referring assignments and word lists
?r(Zr,L): The word lists are usually concatena-
tions of the words used in nominal and proper
mentions and so are updated together with the
assignments for those mentions. Updating the
?r(Zr,L) factor involves finding the referring men-
tion entity assignments, Zr, and property word
lists L for instantiated entities which maximize
EQ??r lnP (T,L,Zr,Ap,M, ? ,pi). We actually
only need to optimize over Zr, since for any Zr, we
can compute the optimal set of property word lists
L. Essentially, for each entity we can compute the
Lr which optimizes the probability of the referring
mentions assigned to the entity (indicated by Zr). In
practice, the optimal Lr is just the set of property
words in the assigned mentions. Of course enumer-
ating and scoring all Zr hypotheses is intractable,
so we instead utilize a left-to-right sequential beam
search. Each partial hypothesis is an assignment to a
prefix of mention positions and is scored as though
it were a complete hypothesis. Hypotheses are ex-
tended via adding a new mention to an existing en-
tity or creating a new one. For our experiments, we
limited the number of hypotheses on the beam to the
top fifty and did not notice an improvement in model
score from increasing beam size.
4See software release for full hyper-parameter details.
5Propers have no learned discourse parameters.
389
Updating pronominal antecedents ri(Api ) and en-
tity types qk(Tk): These updates are straightfor-
ward instantiations of the mean-field update (2).
To produce our final coreference partitions, we as-
sign each referring mention to the entity given by the
?r factor and each pronoun to the most likely entity
given by the ri.
4.1 Factor Staging
In order to facilitate learning, some factors are ini-
tially set to fixed heuristic values and only learned
in later iterations. Initially, the assignment factors
?r and {ri} are fixed. For ?r, we use a determin-
istic entity assignment Zr, similar to the Haghighi
and Klein (2009)?s SYN-CONSTR setting: each re-
ferring mention is coreferent with any past men-
tion with the same head or in a deterministic syn-
tactic configuration (appositives or predicative nom-
inatives constructions).6 The {ri} factors are heuris-
tically set to place most of their mass on the closest
antecedent by tree distance. During training, we pro-
ceed in stages, each consisting of 5 iterations:
Stage Learned Fixed B3All
1 ?s, ?d, {qk} {ri},?r 74.6
2 ?s, ?d, {qk}, ?r {ri} 76.3
3 ?s, ?d, {qk}, ?r, {ri} ? 78.0
We evaluate our system at the end of stage using the
B3All metric on the A05CU development set (see
Section 5 for details).
5 Experiments
We considered the challenging end-to-end system
mention setting, where in addition to predicting
mention partitions, a system must identify the men-
tions themselves and their boundaries automati-
cally. Our system deterministically extracts mention
boundaries from parse trees (Section 5.2). We uti-
lized no coreference annotation during training, but
did use minimal prototype information to prime the
learning of entity types (Section 5.3).
5.1 Datasets
For evaluation, we used standard coreference data
sets derived from the ACE corpora:
6Forcing appositive coreference is essential for tying proper
and nominal entity type vocabulary.
? A04CU: Train/dev/test split of the newswire
portion of the ACE 2004 training set7 utilized
in Culotta et al (2007), Bengston and Roth
(2008) and Stoyanov et al (2009). Consists of
90/68/38 documents respectively.
? A05ST: Train/test split of the newswire portion
of the ACE 2005 training set utilized in Stoy-
anov et al (2009). Consists of 57/24 docu-
ments respectively.
? A05RA: Train/test split of the ACE 2005 train-
ing set utilized in Rahman and Ng (2009). Con-
sists of 482/117 documents respectively.
For all experiments, we evaluated on the dev and test
sets above. To train, we included the text of all doc-
uments above, though of course not looking at ei-
ther their mention boundaries or reference annota-
tions in any way. We also trained on the following
much larger unlabeled datasets utilized in Haghighi
and Klein (2009):
? BLLIP: 5k articles of newswire parsed with the
Charniak (2000) parser.
? WIKI: 8k abstracts of English Wikipedia arti-
cles parsed by the Berkeley parser (Petrov et
al., 2006). Articles were selected to have sub-
jects amongst the frequent proper nouns in the
evaluation datasets.
5.2 Mention Detection and Properties
Mention boundaries were automatically detected as
follows: For each noun or pronoun (determined by
parser POS tag), we associated a mention with the
maximal NP projection of that head or that word it-
self if no NP can be found. This procedure recovers
over 90% of annotated mentions on the A05CU dev
set, but also extracts many unannotated ?spurious?
mentions (for instance events, times, dates, or ab-
stract nouns) which are not deemed to be of interest
by the ACE annotation conventions.
Mention properties were obtained from parse
trees using the the Stanford typed dependency ex-
tractor (de Marneffe et al, 2006). The mention prop-
erties we considered are the mention head (anno-
tated with mention type), the typed modifiers of the
head, and the governor of the head (conjoined with
7Due to licensing restriction, the formal ACE test sets are
not available to non-participants.
390
MUC B3All B3None Pairwise F1
System P R F1 P R F1 P R F1 P R F1
ACE2004-STOYANOV-TEST
Stoyanov et al (2009) - - 62.0 - - 76.5 - - 75.4 - - -
Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2 77.4 67.1 71.3 58.3 44.5 50.5
THIS WORK 67.4 66.6 67.0 81.2 73.3 77.0 80.6 75.2 77.3 59.2 50.3 54.4
ACE2005-STOYANOV-TEST
Stoyanov et al (2009) - - 67.4 - - 73.7 - - 72.5 - - -
Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8 81.2 61.6 70.1 66.1 37.9 48.1
THIS WORK 74.6 62.7 68.1 83.2 68.4 75.1 82.7 66.3 73.6 64.3 41.4 50.4
ACE2005-RAHMAN-TEST
Rahman and Ng (2009) 75.4 64.1 69.3 - - - 54.4 70.5 61.4 - - -
Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6 52.0 72.6 60.6 57.0 44.6 50.0
THIS WORK 77.0 66.9 71.6 55.4 74.8 63.8 54.0 74.7 62.7 60.1 47.7 53.0
Table 1: Experimental results with system mentions. All systems except Haghighi and Klein (2009) and current work
are fully supervised. The current work outperforms all other systems, supervised or unsupervised. For comparison pur-
poses, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman
and Ng (2009).
the mention?s syntactic position). We discard deter-
miners, but make use of them in the discourse com-
ponent (Section 3.2) for NP definiteness.
5.3 Prototyping Entity Types
While it is possible to learn type distributions in a
completely unsupervised fashion, we found it use-
ful to prime the system with a handful of important
types. Rather than relying on fully supervised data,
we took the approach of Haghighi and Klein (2006).
For each type of interest, we provided a (possibly-
empty) prototype list of proper and nominal head
words, as well as a list of allowed pronouns. For
instance, for the PERSON type we might provide:
NAM Bush, Gore, Hussein
NOM president, minister, official
PRO he, his, she, him, her, you, ...
The prototypes were used as follows: Any entity
with a prototype on any proper or nominal head
word attribute list (Section 3.1) was constrained to
have the specified type; i.e. the qk factor (Section 4)
places probability one on that single type. Simi-
larly to Haghighi and Klein (2007) and Elsner et al
(2009), we biased these types? pronoun distributions
to the allowed set of pronouns.
In general, the choice of entity types to prime
with prototypes is a domain-specific question. For
experiments here, we utilized the types which are
annotated in the ACE coreference data: person
(PERS), organization (ORG), geo-political entity
(GPE), weapon (WEA), vehicle (VEH), location
(LOC), and facility (FAC). Since the person type
in ACE conflates individual persons with groups
of people (e.g., soldier vs. soldiers), we added
the group (GROUP) type and generated a prototype
specification.
We obtained our prototype list by extracting at
most four common proper and nominal head words
from the newswire portions of the 2004 and 2005
ACE training sets (A04CU and A05ST); we chose
prototype words to be minimally ambiguous with
respect to type.8 When there are not at least three
proper heads for a type (WEA for instance), we
did not provide any proper prototypes and instead
strongly biased the type fertility parameters to gen-
erate empty NAM-HEAD lists.
Because only certain semantic types were anno-
tated under the arbitrary ACE guidelines, there are
many mentions which do not fall into those limited
categories. We therefore prototype (refinements of)
the ACE types and then add an equal number of un-
constrained ?other? types which are automatically
induced. A nice consequence of this approach is
that we can simply run our model on all mentions,
discarding at evaluation time any which are of non-
prototyped types.
5.4 Evaluation
We evaluated on multiple coreference resolution
metrics, as no single one is clearly superior, partic-
8Meaning those headwords were assigned to the target type
for more than 75% of their usages.
391
ularly in dealing with the system mention setting.
We utilized MUC (Vilain et al, 1995), B3All (Stoy-
anov et al, 2009), B3None (Stoyanov et al, 2009),
and Pairwise F1. The B3All and B3None are B3
variants (Bagga and Baldwin, 1998) that differ in
their treatment of spurious mentions. For Pairwise
F1, precision measures how often pairs of predicted
coreferent mentions are in the same annotated entity.
We eliminated any mention pair from this calcula-
tion where both mentions were spurious.9
5.5 Results
Table 1 shows our results. We compared to two
state-of-the-art supervised coreference systems. The
Stoyanov et al (2009) numbers represent their
THRESHOLD ESTIMATION setting and the Rahman
and Ng (2009) numbers represent their highest-
performing cluster ranking model. We also com-
pared to the strong deterministic system of Haghighi
and Klein (2009).10 Across all data sets, our model,
despite being largely unsupervised, consistently out-
performs these systems, which are the best previ-
ously reported results on end-to-end coreference res-
olution (i.e. including mention detection). Perfor-
mance on the A05RA dataset is generally lower be-
cause it includes articles from blogs and web forums
where parser quality is significantly degraded.
While Bengston and Roth (2008) do not report on
the full system mention task, they do report on the
more optimistic setting where mention detection is
performed but non-gold mentions are removed for
evaluation using an oracle. On this more lenient set-
ting, they report 78.4B3 on the A04CU test set. Our
model yields 80.3.
6 Analysis
We now discuss errors and improvements made
by our system. One frequent source of error is
the merging of mentions with explicitly contrasting
modifiers, such as new president and old president.
While it is not unusual for a single entity to admit
multiple modifiers, the particular modifiers new and
old are incompatible in a way that new and popular
9Note that we are still penalized for marking a spurious
mention coreferent with an annotated one.
10Haghighi and Klein (2009) reports on true mentions; here,
we report performance on automatically detected mentions.
are not. Our model does not represent the negative
covariance between these modifiers.
We compared our output to the deterministic sys-
tem of Haghighi and Klein (2009). Many improve-
ments arise from correctly identifying mentions
which are semantically compatible but which do
not explicitly appear in an appositive or predicate-
nominative configuration in the data. For example,
analyst and it cannot corefer in our system because
it is not a likely pronoun for the type PERSON.
While the focus of our model is coreference res-
olution, we can also isolate and evaluate the type
component of our model as an NER system. We
test this component by presenting our learned model
with boundary-annotated non-pronominal entities
from the A05ST dev set and querying their predicted
type variable T . Doing so yields 83.2 entity clas-
sification accuracy under the mapping between our
prototyped types and the coarse ACE types. Note
that this task is substantially more difficult than the
unsupervised NER in Elsner et al (2009) because
the inventory of named entities is larger (7 vs. 3)
and because we predict types over nominal mentions
that are more difficult to judge from surface forms.
In this task, the plurality of errors are confusions be-
tween the GPE (geo-political entity) and ORG entity
types, which have very similar distributions.
7 Conclusion
Our model is able to acquire and exploit knowledge
at either the level of individual entities (?Obama? is
a ?president?) and entity types (?company? can refer
to a corporation). As a result, it leverages semantic
constraints more effectively than systems operating
at either level alone. In conjunction with reasonable,
but simple, factors capturing discourse and syntac-
tic configurational preferences, our entity-centric se-
mantic model lowers coreference error rate substan-
tially, particularly on semantically disambiguated
references, giving a sizable improvement over the
state-of-the-art.11
Acknowledgements: This project is funded in
part by the Office of Naval Research under MURI
Grant No. N000140911081.
11See nlp.cs.berkeley.edu and aria42.com/software.html for
software release.
392
References
A Bagga and B Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Work-
shop (LREC).
Eric Bengston and Dan Roth. 2008. Understanding
the Value of Features for Corefernce Resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
David Blei and Peter I. Frazier. 2009. Dis-
tance Dependent Chinese Restaurant Processes.
http://arxiv.org/abs/0910.1022/.
Eugene Charniak. 2000. Maximum Entropy Inspired
Parser. In North American Chapter of the Association
of Computational Linguistics (NAACL).
Michael Collins and Yoram Singer. 1999. Unsupervised
Models for Named Entity Classification. In Empirical
Methods in Natural Language Processing (EMNLP).
Mike Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
A Culotta, M Wick, R Hall, and A McCallum. 2007.
First-order Probabilistic Models for Coreference Res-
olution. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (NAACL-HLT).
M. C. de Marneffe, B. Maccartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In LREC.
M Elsner, E Charniak, and M Johnson. 2009. Structured
generative models for unsupervised named-entity clus-
tering. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 164?172.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203?225.
Aria Haghighi and Dan Klein. 2006. Prototype-Driven
Learning for Sequence Models. In HLT-NAACL. As-
sociation for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
Coreference Resolution in a Nonparametric Bayesian
Model. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics. Associ-
ation for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple Coreference
Resolution with Rich Syntactic and Semantic Features.
In Proceedings of the 2009 Conference on Empirical
Conference in Natural Language Processing.
J. R. Hobbs. 1978. Resolving Pronoun References. Lin-
gua, 44.
J. R. Hobbs. 1979. Coherence and Coreference. Cogni-
tive Science, 3:67?90.
Andrew Kehler, Laura Kertz, Hannah Rohde, and Jeffrey
Elman. 2008. Coherence and Coreference Revisited.
Vincent Ng and Claire Cardie. 2002. Improving
Machine Learning Approaches to Coreference Res-
olution. In Association of Computational Linguists
(ACL).
Vincent Ng. 2005. Machine Learning for Corefer-
ence Resolution: From Local Classification to Global
Ranking. In Association of Computational Linguists
(ACL).
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 1689?1694.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
J. Pitman. 2002. Combinatorial Stochastic Processes. In
Lecture Notes for St. Flour Summer School.
A Rahman and V Ng. 2009. Supervised models for
coreference resolution. In Proceedings of the 2009
Conference on Empirical Conference in Natural Lan-
guage Processing.
W.H. Soon, H. T. Ng, and D. C. Y. Lim. 1999. A Ma-
chine Learning Approach to Coreference Resolution
of Noun Phrases.
V Stoyanov, N Gilbert, C Cardie, and E Riloff. 2009.
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-art. In Associate of
Computational Linguistics (ACL).
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC-6.
X Yang, J Su, and CL Tan. 2005. Improving pronoun
resolution using statistics-based semantic compatibil-
ity information. In Association of Computational Lin-
guists (ACL).
393
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573?581,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Type-Based MCMC
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Most existing algorithms for learning latent-
variable models?such as EM and existing
Gibbs samplers?are token-based, meaning
that they update the variables associated with
one sentence at a time. The incremental na-
ture of these methods makes them suscepti-
ble to local optima/slow mixing. In this paper,
we introduce a type-based sampler, which up-
dates a block of variables, identified by a type,
which spans multiple sentences. We show im-
provements on part-of-speech induction, word
segmentation, and learning tree-substitution
grammars.
1 Introduction
A long-standing challenge in NLP is the unsu-
pervised induction of linguistic structures, for ex-
ample, grammars from raw sentences or lexicons
from phoneme sequences. A fundamental property
of these unsupervised learning problems is multi-
modality. In grammar induction, for example, we
could analyze subject-verb-object sequences as ei-
ther ((subject verb) object) (mode 1) or (subject
(verb object)) (mode 2).
Multimodality causes problems for token-based
procedures that update variables for one example at
a time. In EM, for example, if the parameters al-
ready assign high probability to the ((subject verb)
object) analysis, re-analyzing the sentences in E-step
only reinforces the analysis, resulting in EM getting
stuck in a local optimum. In (collapsed) Gibbs sam-
pling, if all sentences are already analyzed as ((sub-
ject verb) object), sampling a sentence conditioned
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
2 1 2 2 1
2 2 2 1 2
1 2 1 2 2
(a) token-based (b) sentence-based (c) type-based
Figure 1: Consider a dataset of 3 sentences, each of
length 5. Each variable is labeled with a type (1 or 2). The
unshaded variables are the ones that are updated jointly
by a sampler. The token-based sampler updates the vari-
able for one token at a time (a). The sentence-based sam-
pler updates all variables in a sentence, thus dealing with
intra-sentential dependencies (b). The type-based sam-
pler updates all variables of a particular type (1 in this ex-
ample), thus dealing with dependencies due to common
parameters (c).
on all others will most likely not change its analysis,
resulting in slow mixing.
To combat the problems associated with token-
based algorithms, we propose a new sampling algo-
rithm that operates on types. Our sampler would, for
example, be able to change all occurrences of ((sub-
ject verb) object) to (subject (verb object)) in one
step. These type-based operations are reminiscent of
the type-based grammar operations of early chunk-
merge systems (Wolff, 1988; Stolcke and Omohun-
dro, 1994), but we work within a sampling frame-
work for increased robustness.
In NLP, perhaps the the most simple and popu-
lar sampler is the token-based Gibbs sampler,1 used
in Goldwater et al (2006), Goldwater and Griffiths
(2007), and many others. By sampling only one
1In NLP, this is sometimes referred to as simply the col-
lapsed Gibbs sampler.
573
variable at a time, this sampler is prone to slow mix-
ing due to the strong coupling between variables.
A general remedy is to sample blocks of coupled
variables. For example, the sentence-based sampler
samples all the variables associated with a sentence
at once (e.g., the entire tag sequence). However, this
blocking does not deal with the strong type-based
coupling (e.g., all instances of a word should be
tagged similarly). The type-based sampler we will
present is designed exactly to tackle this coupling,
which we argue is stronger and more important to
deal with in unsupervised learning. Figure 1 depicts
the updates made by each of the three samplers.
We tested our sampler on three models: a
Bayesian HMM for part-of-speech induction (Gold-
water and Griffiths, 2007), a nonparametric
Bayesian model for word segmentation (Goldwater
et al, 2006), and a nonparametric Bayesian model of
tree substitution grammars (Cohn et al, 2009; Post
and Gildea, 2009). Empirically, we find that type-
based sampling improves performance and is less
sensitive to initialization (Section 5).
2 Basic Idea via a Motivating Example
The key technical problem we solve in this paper is
finding a block of variables which are both highly
coupled and yet tractable to sample jointly. This
section illustrates the main idea behind type-based
sampling on a small word segmentation example.
Suppose our dataset x consists of n occurrences
of the sequence a b. Our goal is infer z =
(z1, . . . , zn), where zi = 0 if the sequence is one
word ab, and zi = 1 if the sequence is two, a
and b. We can model this situation with a simple
generative model: for each i = 1, . . . , n, gener-
ate one or two words with equal probability. Each
word is drawn independently based on probabilities
? = (?a, ?b, ?ab) which we endow with a uniform
prior ? ? Dirichlet(1, 1, 1).
We marginalize out ? to get the following standard
expression (Goldwater et al, 2009):
p(z | x) ?
1(m)1(m)1(n?m)
3(n+m)
def
= g(m), (1)
where m =
?n
i=1 zi is the number of two-word se-
quences and a(k) = a(a + 1) ? ? ? (a + k ? 1) is the
200 400 600 8001000
m
-1411.4
-1060.3
-709.1
-358.0
-6.8
log
g(m
)
2 4 6 8 10
iteration
200
400
600
800
1000
m
Token
Type
(a) bimodal posterior (b) sampling run
Figure 2: (a) The posterior (1) is sharply bimodal (note
the log-scale). (b) A run of the token-based and type-
based samplers. We initialize both samplers with m = n
(n = 1000). The type-based sampler mixes instantly
(in fact, it makes independent draws from the posterior)
whereas the token-based sampler requires five passes
through the data before finding the high probability re-
gion m u 0.
ascending factorial.2 Figure 2(a) depicts the result-
ing bimodal posterior.
A token-based sampler chooses one zi to update
according to the posterior p(zi | z?i,x). To illus-
trate the mixing problem, consider the case where
m = n, i.e., all sequences are analyzed as two
words. From (1), we can verify that p(zi = 0 |
z?i,x) = O( 1n). When n = 1000, this means that
there is only a 0.002 probability of setting zi = 0,
a very unlikely but necessary first step to take to es-
cape this local optimum. Indeed, Figure 2(b) shows
how the token-based sampler requires five passes
over the data to finally escape.
Type-based sampling completely eradicates the
local optimum problem in this example. Let us take
a closer look at (1). Note that p(z | x) only depends
on a single integer m, which only takes one of n+ 1
values, not on the particular z. This shows that the
zis are exchangeable. There are
(n
m
)
possible val-
ues of z satisfying m =
?
i zi, each with the same
probability g(m). Summing, we get:
p(m | x) ?
?
z:m=
P
i zi
p(x, z) =
(
n
m
)
g(m). (2)
A sampling strategy falls out naturally: First, sample
the number m via (2). Conditioned on m, choose
2The ascending factorial function arises from marginaliz-
ing Dirichlet distributions and is responsible the rich-gets-richer
phenomenon: the larger n is, more we gain by increasing it.
574
the particular z uniformly out of the
(n
m
)
possibili-
ties. Figure 2(b) shows the effectiveness of this type-
based sampler.
This simple example exposes the fundamental
challenge of multimodality in unsupervised learn-
ing. Both m = 0 and m = n are modes due to the
rich-gets-richer property which arises by virtue of
all n examples sharing the same parameters ?. This
sharing is a double-edged sword: It provides us with
clustering structure but also makes inference hard.
Even though m = n is much worse (by a factor ex-
ponential in n) than m = 0, a na??ve algorithm can
easily have trouble escaping m = n.
3 Setup
We will now present the type-based sampler in full
generality. Our sampler is applicable to any model
which is built out of local multinomial choices,
where each multinomial has a Dirichlet process prior
(a Dirichlet prior if the number of choices is finite).
This includes most probabilistic models in NLP (ex-
cluding ones built from log-linear features).
As we develop the sampler, we will pro-
vide concrete examples for the Bayesian hidden
Markov model (HMM), the Dirichlet process uni-
gram segmentation model (USM) (Goldwater et al,
2006), and the probabilistic tree-substitution gram-
mar (PTSG) (Cohn et al, 2009; Post and Gildea,
2009).
3.1 Model parameters
A model is specified by a collection of multino-
mial parameters ? = {?r}r?R, where R is an in-
dex set. Each vector ?r specifies a distribution over
outcomes: outcome o has probability ?ro.
? HMM: Let K is the number of states. The set
R = {(q, k) : q ? {T,E}, k = 1, . . . ,K}
indexes the K transition distributions {?(T,k)}
(each over outcomes {1, . . . ,K}) and K emis-
sion distributions {?(E,k)} (each over the set of
words).
? USM: R = {0}, and ?0 is a distribution over (an
infinite number of) words.
? PTSG: R is the set of grammar symbols, and
each ?r is a distribution over labeled tree frag-
ments with root label r.
R index set for parameters
? = {?r}r?R multinomial parameters
? = {?r}r?R base distributions (fixed)
S set of sites
b = {bs}s?S binary variables (to be sampled)
z latent structure (set of choices)
z?s choices not depending on site s
zs:b choices after setting bs = b
?zs:b zs:b\z?s: new choices from bs = b
S ? S sites selected for sampling
m # sites in S assigned bs = 1
n = {nro} counts (sufficient statistics of z)
Table 1: Notation used in this paper. Note that there is a
one-to-one mapping between z and (b,x). The informa-
tion relevant for evaluating the likelihood is n. We use
the following parallel notation: n?s = n(z?s),ns:b =
n(zs:b),?ns = n(?zs).
3.2 Choice representation of latent structure z
We represent the latent structure z as a set of local
choices:3
? HMM: z contains elements of the form
(T, i, a, b), denoting a transition from state
a at position i to state b at position i + 1; and
(E, i, a, w), denoting an emission of word w
from state a at position i.
? USM: z contains elements of the form (i, w), de-
noting the generation of word w at character po-
sition i extending to position i+ |w| ? 1.
? PTSG: z contains elements of the form (x, t), de-
noting the generation of tree fragment t rooted at
node x.
The choices z are connected to the parameters ?
as follows: p(z | ?) =
?
z?z ?z.r,z.o. Each choice
z ? z is identified with some z.r ? R and out-
come z.o. Intuitively, choice z was made by drawing
drawing z.o from the multinomial distribution ?z.r.
3.3 Prior
We place a Dirichlet process prior on ?r (Dirichlet
prior for finite outcome spaces): ?r ? DP(?r, ?r),
where ?r is a concentration parameter and ?r is a
fixed base distribution.
3We assume that z contains both a latent part and the ob-
served input x, i.e., x is a deterministic function of z.
575
Let nro(z) = |{z ? z : z.r = r, z.o = o}| be the
number of draws from ?r resulting in outcome o, and
nr? =
?
o nro be the number of times ?r was drawn
from. Let n(z) = {nro(z)} denote the vector of
sufficient statistics associated with choices z. When
it is clear from context, we simply write n for n(z).
Using these sufficient statistics, we can write p(z |
?) =
?
r,o ?
nro(z)
ro .
We now marginalize out ? using Dirichlet-
multinomial conjugacy, producing the following ex-
pression for the likelihood:
p(z) =
?
r?R
?
o (?ro?ro)
(nro(z))
?r(nr?(z))
, (3)
where a(k) = a(a+1) ? ? ? (a+k?1) is the ascending
factorial. (3) is the distribution that we will use for
sampling.
4 Type-Based Sampling
Having described the setup of the model, we now
turn to posterior inference of p(z | x).
4.1 Binary Representation
We first define a new representation of the latent
structure based on binary variables b so that there is
a bijection between z and (b,x); z was used to de-
fine the model, b will be used for inference. We will
use b to exploit the ideas from Section 2. Specifi-
cally, let b = {bs}s?S be a collection of binary vari-
ables indexed by a set of sites S.
? HMM: If the HMM hasK = 2 states, S is the set
of positions in the sequence. For each s ? S , bs
is the hidden state at s. The extension to general
K is considered at the end of Section 4.4.
? USM: S is the set of non-final positions in the
sequence. For each s ? S , bs denotes whether
a word boundary exists between positions s and
s+ 1.
? PTSG: S is the set of internal nodes in the parse
tree. For s ? S, bs denotes whether a tree frag-
ment is rooted at node s.
For each site s ? S, let zs:0 and zs:1 denote the
choices associated with the structures obtained by
setting the binary variable bs = 0 and bs = 1, re-
spectively. Define z?s
def
= zs:0 ? zs:1 to be the set
of choices that do not depend on the value of bs, and
n?s
def
= n(z?s) be the corresponding counts.
? HMM: z?s includes all but the transitions into
and out of the state at s plus the emission at s.
? USM: z?s includes all except the word ending at
s and the one starting at s+ 1 if there is a bound-
ary (bs = 1); except the word covering s if no
boundary exists (bs = 0).
? PTSG: z?s includes all except the tree fragment
rooted at node s and the one with leaf s if bs = 1;
except the single fragment containing s if bs = 0.
4.2 Sampling One Site
A token-based sampler considers one site s at a time.
Specifically, we evaluate the likelihoods of zs:0 and
zs:1 according to (3) and sample bs with probability
proportional to the likelihoods. Intuitively, this can
be accomplished by removing choices that depend
on bs (resulting in z?s), evaluating the likelihood re-
sulting from setting bs to 0 or 1, and then adding the
appropriate choices back in.
More formally, let ?zs:b
def
= zs:b\z?s be the new
choices that would be added if we set bs = b ?
{0, 1}, and let ?ns:b
def
= n(?zs:b) be the corre-
sponding counts. With this notation, we can write
the posterior as follows:
p(bs = b | b\bs) ? (4)
?
r?R
?
o (?ro?ro + n
?s
ro )
(?ns:bro )
(?r + n
?s
r? )
(?ns:br? )
.
The form of the conditional (4) follows from the
joint (3) via two properties: additivity of counts
(ns:b = n?s + ?ns:b) and a simple property of as-
cending factorials (a(k+?) = a(k)(a+ k)(?)).
In practice, most of the entries of ?ns:b are zero.
For the HMM, ns:bro would be nonzero only for
the transitions into the new state (b) at position s
(zs?1 ? b), transitions out of that state (b? zs+1),
and emissions from that state (b? xs).
4.3 Sampling Multiple Sites
We would like to sample multiple sites jointly as in
Section 2, but we cannot choose any arbitrary subset
S ? S, as the likelihood will in general depend on
the exact assignment of bS
def
= {bs}s?S , of which
576
a b c a a b c a b c b
(a) USM
1 1 2 2 1 1 2 2
a b a b c b b e
(b) HMM
a
b
a a
b c
d e
c
d
b c
e
a b
(c) PTSG
Figure 3: The type-based sampler jointly samples all vari-
ables at a set of sites S (in green boxes). Sites in S are
chosen based on types (denoted in red). (a) HMM: two
sites have the same type if they have the same previous
and next states and emit the same word; they conflict un-
less separated by at least one position. (b) USM: two sites
have the same type if they are both of the form ab|c or
abc; note that occurrences of the same letters with other
segmentations do not match the type. (c) PTSG: analo-
gous to the USM, only for tree rather than sequences.
there are an exponential number. To exploit the ex-
changeability property in Section 2, we need to find
sites which look ?the same? from the model?s point
of view, that is, the likelihood only depends on bS
via m
def
=
?
s?S bs.
To do this, we need to define two notions, type and
conflict. We say sites s and s? have the same type if
the counts added by setting either bs or bs? are the
same, that is, ?ns:b = ?ns
?:b for b ? {0, 1}. This
motivates the following definition of the type of site
s with respect to z:
t(z, s)
def
= (?ns:0,?ns:1), (5)
We say that s and s? have the same type if t(z, s) =
t(z, s?). Note that the actual choices added (?zs:b
and ?zs
?:b) are in general different as s and s? cor-
respond to different parts of the latent structure, but
the model only depends on counts and is indifferent
to this. Figure 3 shows examples of same-type sites
for our three models.
However, even if all sites in S have the same
type, we still cannot sample bS jointly, since chang-
ing one bs might change the type of another site s?;
indeed, this dependence is reflected in (5), which
shows that types depend on z. For example, s, s? ?
S conflict when s? = s + 1 in the HMM or when
s and s? are boundaries of one segment (USM) or
one tree fragment (PTSG). Therefore, one additional
concept is necessary: We say two sites s and s? con-
flict if there is some choice that depends on both bs
and bs? ; formally, (z\z?s) ? (z\z?s
?
) 6= ?.
Our key mathematical result is as follows:
Proposition 1 For any set S ? S of non-conflicting
sites with the same type,
p(bS | b\bS) ? g(m) (6)
p(m | b\bS) ?
(
|S|
m
)
g(m), (7)
for some easily computable g(m), where m =
?
s?S bs.
We will derive g(m) shortly, but first note from
(6) that the likelihood for a particular setting of bS
depends on bS only via m as desired. (7) sums
over all
(|S|
m
)
settings of bS with m =
?
s?S bs.
The algorithmic consequences of this result is that
to sample bS , we can first compute (7) for each
m ? {0, . . . , |S|}, sample m according to the nor-
malized distribution, and then choose the actual bS
uniformly subject to m.
Let us now derive g(m) by generalizing (4).
Imagine removing all sites S and their dependent
choices and adding in choices corresponding to
some assignment bS . Since all sites in S are non-
conflicting and of the same type, the count contribu-
tion ?ns:b is the same for every s ? S (i.e., sites
in S are exchangeable). Therefore, the likelihood
of the new assignment bS depends only on the new
counts:
?nS:m
def
= m?ns:1 + (|S| ?m)?ns:0. (8)
Using these new counts in place of the ones in (4),
we get the following expression:
g(m) =
?
r?R
?
o (?ro?ro + nro(z
?S))
(?nS:mro )
?r + nr?(z?S)
(?nS:mr? )
. (9)
4.4 Full Algorithm
Thus far, we have shown how to sample bS given
a set S ? S of non-conflicting sites with the same
type. To complete the description of the type-based
577
Type-Based Sampler
for each iteration t = 1, . . . , T :
?for each pivot site s0 ? S:
??S ? TB(z, s0) (S is the type block centered at s0)
??decrement n and remove from z based on bS
??sample m according to (7)
??sample M ? S with |M | = m uniformly at random
??set bs = I[s ?M ] for each s ? S
??increment n and add to z accordingly
Figure 4: Pseudocode for the general type-based sampler.
We operate in the binary variable representation b of z.
Each step, we jointly sample |S| variables (of the same
type).
sampler, we need to specify how to choose S. Our
general strategy is to first choose a pivot site s0 ? S
uniformly at random and then set S = TB(z, s0) for
some function TB. Call S the type block centered at
s0. The following two criteria on TB are sufficient
for a valid sampler: (A) s0 ? S, and (B) the type
blocks are stable, which means that if we change bS
to any b?S (resulting in a new z
?), the type block cen-
tered at s0 with respect to z? does not change (that
is, TB(z?, s0) = S). (A) ensures ergodicity; (B),
reversibility.
Now we define TB as follows: First set S = {s0}.
Next, loop through all sites s ? S with the same type
as s0 in some fixed order, adding s to S if it does
not conflict with any sites already in S. Figure 4
provides the pseudocode for the full algorithm.
Formally, this sampler cycles over |S| transition
kernels, one for each pivot site. Each kernel (in-
dexed by s0 ? S) defines a blocked Gibbs move,
i.e. sampling from p(bTB(z,s0) | ? ? ? ).
Efficient Implementation There are two oper-
ations we must perform efficiently: (A) looping
through sites with the same type as the pivot site s0,
and (B) checking whether such a site s conflicts with
any site in S. We can perform (B) in O(1) time by
checking if any element of ?zs:bs has already been
removed; if so, there is a conflict and we skip s. To
do (A) efficiently, we maintain a hash table mapping
type t to a doubly-linked list of sites with type t.
There is anO(1) cost for maintaining this data struc-
ture: When we add or remove a site s, we just need
to add or remove neighboring sites s? from their re-
spective linked lists, since their types depend on bs.
For example, in the HMM, when we remove site s,
we also remove sites s?1 and s+1.
For the USM, we use a simpler solution: main-
tain a hash table mapping each word w to a list of
positions where w occurs. Suppose site (position) s
straddles words a and b. Then, to perform (A), we
retrieve the list of positions where a, b, and ab occur,
intersecting the a and b lists to obtain a list of posi-
tions where a b occurs. While this intersection is
often much smaller than the pre-intersected lists, we
found in practice that the smaller amount of book-
keeping balanced out the extra time spent intersect-
ing. We used a similar strategy for the PTSG, which
significantly reduces the amount of bookkeeping.
Skip Approximation Large type blocks mean
larger moves. However, such a block S is also sam-
pled more frequently?once for every choice of a
pivot site s0 ? S. However, we found that empir-
ically, bS changes very infrequently. To eliminate
this apparent waste, we use the following approxi-
mation of our sampler: do not consider s0 ? S as
a pivot site if s0 belongs to some block which was
already sampled in the current iteration. This way,
each site is considered roughly once per iteration.4
Sampling Non-Binary Representations We can
sample in models without a natural binary represen-
tation (e.g., HMMs with with more than two states)
by considering random binary slices. Specifically,
suppose bs ? {1, . . . ,K} for each site s ? S .
We modify Figure 4 as follows: After choosing a
pivot site s0 ? S , let k = bs0 and choose k
? uni-
formly from {1, . . . ,K}. Only include sites in one
of these two states by re-defining the type block to
be S = {s ? TB(z, s0) : bs ? {k, k?}}, and sam-
ple bS restricted to these two states by drawing from
p(bS | bS ? {k, k?}|S|, ? ? ? ). By choosing a random
k? each time, we allow b to reach any point in the
space, thus achieving ergodicity just by using these
binary restrictions.
5 Experiments
We now compare our proposed type-based sampler
to various alternatives, evaluating on marginal like-
4A site could be sampled more than once if it belonged to
more than one type block during the iteration (recall that types
depend on z and thus could change during sampling).
578
lihood (3) and accuracy for our three models:
? HMM: We learned a K = 45 state HMM on
the Wall Street Journal (WSJ) portion of the Penn
Treebank (49208 sentences, 45 tags) for part-of-
speech induction. We fixed ?r to 0.1 and ?r to
uniform for all r.
For accuracy, we used the standard metric based
on greedy mapping, where each state is mapped
to the POS tag that maximizes the number of cor-
rect matches (Haghighi and Klein, 2006). We did
not use a tagging dictionary.
? USM: We learned a USM model on the
Bernstein-Ratner corpus from the CHILDES
database used in Goldwater et al (2006) (9790
sentences) for word segmentation. We fixed ?0 to
0.1. The base distribution ?0 penalizes the length
of words (see Goldwater et al (2009) for details).
For accuracy, we used word token F1.
? PTSG: We learned a PTSG model on sections 2?
21 of the WSJ treebank.5 For accuracy, we used
EVALB parsing F1 on section 22.6 Note this is a
supervised task with latent-variables, whereas the
other two are purely unsupervised.
5.1 Basic Comparison
Figure 5(a)?(c) compares the likelihood and accu-
racy (we use the term accuracy loosely to also in-
clude F1). The initial observation is that the type-
based sampler (TYPE) outperforms the token-based
sampler (TOKEN) across all three models on both
metrics.
We further evaluated the PTSG on parsing. Our
standard treebank PCFG estimated using maximum
likelihood obtained 79% F1. TOKEN obtained an F1
of 82.2%, and TYPE obtained a comparable F1 of
83.2%. Running the PTSG for longer continued to
5Following Petrov et al (2006), we performed an initial pre-
processing step on the trees involving Markovization, binariza-
tion, and collapsing of unary chains; words occurring once are
replaced with one of 50 ?unknown word? tokens, using base
distributions {?r} that penalize the size of trees, and sampling
the hyperparameters (see Cohn et al (2009) for details).
6To evaluate, we created a grammar where the rule proba-
bilities are the mean values under the PTSG distribution: this
involves taking a weighted combination (based on the concen-
tration parameters) of the rule counts from the PTSG samples
and the PCFG-derived base distribution. We used the decoder
of DeNero et al (2009) to parse.
improve the likelihood but actually hurt parsing ac-
curacy, suggesting that the PTSG model is overfit-
ting.
To better understand the gains from TYPE
over TOKEN, we consider three other alterna-
tive samplers. First, annealing (TOKENanneal) is
a commonly-used technique to improve mixing,
where (3) is raised to some inverse temperature.7
In Figure 5(a)?(c), we see that unlike TYPE,
TOKENanneal does not improve over TOKEN uni-
formly: it hurts for the HMM, improves slightly for
the USM, and makes no difference for the PTSG. Al-
though annealing does increase mobility of the sam-
pler, this mobility is undirected, whereas type-based
sampling increases mobility in purely model-driven
directions.
Unlike past work that operated on types (Wolff,
1988; Brown et al, 1992; Stolcke and Omohun-
dro, 1994), type-based sampling makes stochastic
choices, and moreover, these choices are reversible.
Is this stochasticity important? To answer this, we
consider a variant of TYPE, TYPEgreedy: instead
of sampling from (7), TYPEgreedy considers a type
block S and sets bs to 0 for all s ? S if p(bS =
(0, . . . , 0) | ? ? ? ) > p(bS = (1, . . . , 1) | ? ? ? ); else
it sets bs to 1 for all s ? S. From Figure 5(a)?(c),
we see that greediness is disastrous for the HMM,
hurts a little for USM, and makes no difference on
the PTSG. These results show that stochasticity can
indeed be important.
We consider another block sampler, SENTENCE,
which uses dynamic programming to sample all
variables in a sentence (using Metropolis-Hastings
to correct for intra-sentential type-level coupling).
For USM, we see that SENTENCE performs worse
than TYPE and is comparable to TOKEN, suggesting
that type-based dependencies are stronger and more
important to deal with than intra-sentential depen-
dencies.
5.2 Initialization
We initialized all samplers as follows: For the USM
and PTSG, for each site s, we place a boundary (set
bs = 1) with probability ?. For the HMM, we set bs
to state 1 with probability ? and a random state with
7We started with a temperature of 10 and gradually de-
creased it to 1 during the first half of the run, and kept it at 1
thereafter.
579
3 6 9 12
time (hr.)
-1.1e7
-0.9e7
-9.1e6
-7.9e6
-6.7e6
log
-lik
elih
ood
3 6 9 12
time (hr.)
0.1
0.2
0.4
0.5
0.6
acc
ura
cy
2 4 6 8
time (min.)
-3.7e5
-3.2e5
-2.8e5
-2.4e5
-1.9e5
log
-lik
elih
ood Token
Tokenanneal
Typegreedy
Type
Sentence
2 4 6 8
time (min.)
0.1
0.2
0.4
0.5
0.6
F 1
3 6 9 12
time (hr.)
-6.2e6
-6.0e6
-5.8e6
-5.7e6
-5.5e6
log
-lik
elih
ood
(a) HMM (b) USM (c) PTSG
0.2 0.4 0.6 0.8 1.0
?
-7.1e6
-7.0e6
-6.9e6
-6.8e6
-6.7e6
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
acc
ura
cy
0.2 0.4 0.6 0.8 1.0
?
-3.5e5
-3.1e5
-2.7e5
-2.3e5
-1.9e5
log
-lik
elih
ood
0.2 0.4 0.6 0.8 1.0
?
0.2
0.3
0.4
0.5
0.6
F 1
0.2 0.4 0.6 0.8 1.0
?
-5.7e6
-5.6e6
-5.6e6
-5.5e6
-5.5e6
log
-lik
elih
ood
(d) HMM (e) USM (f) PTSG
Figure 5: (a)?(c): Log-likelihood and accuracy over time. TYPE performs the best. Relative to TYPE, TYPEgreedy
tends to hurt performance. TOKEN generally works worse. Relative to TOKEN, TOKENanneal produces mixed results.
SENTENCE behaves like TOKEN. (d)?(f): Effect of initialization. The metrics were applied to the current sample after
15 hours for the HMM and PTSG and 10 minutes for the USM. TYPE generally prefers larger ? and outperform the
other samplers.
probability 1 ? ?. Results in Figure 5(a)?(c) were
obtained by setting ? to maximize likelihood.
Since samplers tend to be sensitive to initializa-
tion, it is important to explore the effect of initial-
ization (parametrized by ? ? [0, 1]). Figure 5(d)?(f)
shows that TYPE is consistently the best, whereas
other samplers can underperform TYPE by a large
margin. Note that TYPE favors ? = 1 in general.
This setting maximizes the number of initial types,
and thus creates larger type blocks and thus enables
larger moves. Larger type blocks also mean more
dependencies that TOKEN is unable to deal with.
6 Related Work and Discussion
Block sampling, on which our work is built, is a clas-
sical idea, but is used restrictively since sampling
large blocks is computationally expensive. Past
work for clustering models maintained tractabil-
ity by using Metropolis-Hastings proposals (Dahl,
2003) or introducing auxiliary variables (Swendsen
and Wang, 1987; Liang et al, 2007). In contrast,
our type-based sampler simply identifies tractable
blocks based on exchangeability.
Other methods for learning latent-variable models
include EM, variational approximations, and uncol-
lapsed samplers. All of these methods maintain dis-
tributions over (or settings of) the latent variables of
the model and update the representation iteratively
(see Gao and Johnson (2008) for an overview in the
context of POS induction). However, these methods
are at the core all token-based, since they only up-
date variables in a single example at a time.8
Blocking variables by type?the key idea of
this paper?is a fundamental departure from token-
based methods. Though type-based changes have
also been proposed (Brown et al, 1992; Stolcke and
Omohundro, 1994), these methods operated greed-
ily, and in Section 5.1, we saw that being greedy led
to more brittle results. By working in a sampling
framework, we were able bring type-based changes
to fruition.
8While EM technically updates all distributions over latent
variables in the E-step, this update is performed conditioned on
model parameters; it is this coupling (made more explicit in
collapsed samplers) that makes EM susceptible to local optima.
580
References
P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and
R. L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467?
479.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing
compact but accurate tree-substitution grammars. In
North American Association for Computational Lin-
guistics (NAACL), pages 548?556.
D. B. Dahl. 2003. An improved merge-split sampler for
conjugate Dirichlet process mixture models. Techni-
cal report, Department of Statistics, University of Wis-
consin.
J. DeNero, M. Bansal, A. Pauls, and D. Klein. 2009.
Efficient parsing for transducer grammars. In North
American Association for Computational Linguistics
(NAACL), pages 227?235.
J. Gao and M. Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In Empirical Methods in Natural
Language Processing (EMNLP), pages 344?352.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Explor-
ing the effects of context. Cognition, 112:21?54.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In North American Associ-
ation for Computational Linguistics (NAACL), pages
320?327.
P. Liang, M. I. Jordan, and B. Taskar. 2007. A
permutation-augmented sampler for Dirichlet process
mixture models. In International Conference on Ma-
chine Learning (ICML).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP).
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
International Colloquium on Grammatical Inference
and Applications, pages 106?118.
R. H. Swendsen and J. S. Wang. 1987. Nonuniversal
critical dynamics in MC simulations. Physics Review
Letters, 58:86?88.
J. G. Wolff. 1988. Learning syntax and meanings
through optimization and distributional analysis. In
Categories and processes in language acquisition,
pages 179?215.
581
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582?590,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Painless Unsupervised Learning with Features
Taylor Berg-Kirkpatrick Alexandre Bouchard-Co?te? John DeNero Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, bouchard, denero, klein}@cs.berkeley.edu
Abstract
We show how features can easily be added
to standard generative models for unsuper-
vised learning, without requiring complex
new training methods. In particular, each
component multinomial of a generative model
can be turned into a miniature logistic regres-
sion model if feature locality permits. The in-
tuitive EM algorithm still applies, but with a
gradient-based M-step familiar from discrim-
inative training of logistic regression mod-
els. We apply this technique to part-of-speech
induction, grammar induction, word align-
ment, and word segmentation, incorporating
a few linguistically-motivated features into
the standard generative model for each task.
These feature-enhanced models each outper-
form their basic counterparts by a substantial
margin, and even compete with and surpass
more complex state-of-the-art models.
1 Introduction
Unsupervised learning methods have been increas-
ingly successful in recent NLP research. The rea-
sons are varied: increased supplies of unlabeled
data, improved understanding of modeling methods,
additional choices of optimization algorithms, and,
perhaps most importantly for the present work, in-
corporation of richer domain knowledge into struc-
tured models. Unfortunately, that knowledge has
generally been encoded in the form of conditional
independence structure, which means that injecting
it is both tricky (because the connection between
independence and knowledge is subtle) and time-
consuming (because new structure often necessitates
new inference algorithms).
In this paper, we present a range of experiments
wherein we improve existing unsupervised models
by declaratively adding richer features. In particu-
lar, we parameterize the local multinomials of exist-
ing generative models using features, in a way which
does not require complex new machinery but which
still provides substantial flexibility. In the feature-
engineering paradigm, one can worry less about the
backbone structure and instead use hand-designed
features to declaratively inject domain knowledge
into a model. While feature engineering has his-
torically been associated with discriminative, super-
vised learning settings, we argue that it can and
should be applied more broadly to the unsupervised
setting.
The idea of using features in unsupervised learn-
ing is neither new nor even controversial. Many
top unsupervised results use feature-based mod-
els (Smith and Eisner, 2005; Haghighi and Klein,
2006). However, such approaches have presented
their own barriers, from challenging normalization
problems, to neighborhood design, to the need for
complex optimization procedures. As a result, most
work still focuses on the stable and intuitive ap-
proach of using the EM algorithm to optimize data
likelihood in locally normalized, generative models.
The primary contribution of this paper is to
demonstrate the clear empirical success of a sim-
ple and accessible approach to unsupervised learn-
ing with features, which can be optimized by us-
ing standard NLP building blocks. We consider
the same generative, locally-normalized models that
dominate past work on a range of tasks. However,
we follow Chen (2003), Bisani and Ney (2008), and
Bouchard-Co?te? et al (2008), and allow each com-
ponent multinomial of the model to be a miniature
multi-class logistic regression model. In this case,
the EM algorithm still applies with the E-step un-
changed. The M-step involves gradient-based train-
ing familiar from standard supervised logistic re-
gression (i.e., maximum entropy models). By inte-
grating these two familiar learning techniques, we
add features to unsupervised models without any
582
specialized learning or inference.
A second contribution of this work is to show that
further gains can be achieved by directly optimiz-
ing data likelihood with LBFGS (Liu et al, 1989).
This alternative optimization procedure requires no
additional machinery beyond what EM uses. This
approach is still very simple to implement, and we
found that it empirically outperforms EM.
This paper is largely empirical; the underlying op-
timization techniques are known, even if the overall
approach will be novel to many readers. As an em-
pirical demonstration, our results span an array of
unsupervised learning tasks: part-of-speech induc-
tion, grammar induction, word alignment, and word
segmentation. In each task, we show that declaring a
few linguistically motivated feature templates yields
state-of-the-art results.
2 Models
We start by explaining our feature-enhanced model
for part-of-speech (POS) induction. This particular
example illustrates our approach to adding features
to unsupervised models in a well-known NLP task.
We then explain how the technique applies more
generally.
2.1 Example: Part-of-Speech Induction
POS induction consists of labeling words in text
with POS tags. A hidden Markov model (HMM) is a
standard model for this task, used in both a frequen-
tist setting (Merialdo, 1994; Elworthy, 1994) and in
a Bayesian setting (Goldwater and Griffiths, 2007;
Johnson, 2007).
A POS HMM generates a sequence of words in
order. In each generation step, an observed word
emission yi and a hidden successor POS tag zi+1 are
generated independently, conditioned on the current
POS tag zi . This process continues until an absorb-
ing stop state is generated by the transition model.
There are two types of conditional distributions in
the model?emission and transition probabilities?
that are both multinomial probability distributions.
The joint likelihood factors into these distributions:
P?(Y = y,Z = z) = P?(Z1 = z1) ?
|z|
?
i=1
P?(Yi = yi|Zi = zi) ? P?(Zi+1 = zi+1|Zi = zi)
The emission distribution P?(Yi = yi|Zi = zi) is
parameterized by conditional probabilities ?y,z,EMIT
for each word y given tag z. Alternatively, we can
express this emission distribution as the output of a
logistic regression model, replacing the explicit con-
ditional probability table by a logistic function pa-
rameterized by weights and features:
?y,z,EMIT(w) =
exp ?w, f(y, z, EMIT)?
?
y? exp ?w, f(y?, z, EMIT)?
This feature-based logistic expression is equivalent
to the flat multinomial in the case that the feature
function f(y, z, EMIT) consists of all indicator fea-
tures on tuples (y, z, EMIT), which we call BASIC
features. The equivalence follows by setting weight
wy,z,EMIT = log(?y,z,EMIT).1 This formulation is
known as the natural parameterization of the multi-
nomial distribution.
In order to enhance this emission distribution, we
include coarse features in f(y, z, EMIT), in addi-
tion to the BASIC features. Crucially, these features
can be active across multiple (y, z) values. In this
way, the model can abstract general patterns, such
as a POS tag co-occurring with an inflectional mor-
pheme. We discuss specific POS features in Sec-
tion 4.
2.2 General Directed Models
Like the HMM, all of the models we propose are
based on locally normalized generative decisions
that condition on some context. In general, let X =
(Z,Y) denote the sequence of generation steps (ran-
dom variables) where Z contains all hidden random
variables and Y contains all observed random vari-
ables. The joint probability of this directed model
factors as:
Pw(X = x) =
?
i?I
Pw
(
Xi = xi
?
?Xpi(i) = xpi(i)
)
,
where Xpi(i) denotes the parents of Xi and I is the
index set of the variables in X.
In the models that we use, each factor in the above
expression is the output of a local logistic regression
1As long as no transition or emission probabilities are equal
to zero. When zeros are present, for instance to model that an
absorbing stop state can only transition to itself, it is often possi-
ble to absorb these zeros into a base measure. All the arguments
in this paper carry with a structured base measure; we drop it for
simplicity.
583
model parameterized by w:
Pw
`
Xi = d
?
?Xpi(i) = c
?
=
exp?w, f(d, c, t)?
P
d? exp?w, f(d?, c, t)?
Above, d is the generative decision value for Xi
picked by the model, c is the conditioning context
tuple of values for the parents of Xi, and t is the
type of decision being made. For instance, the POS
HMM has two types of decisions: transitions and
emissions. In the emission model, the type t is EMIT,
the decision d is a word and the context c is a tag.
The denominator normalizes the factor to be a prob-
ability distribution over decisions.
The objective function we derive from this model
is the marginal likelihood of the observations y,
along with a regularization term:
L(w) = log Pw(Y = y)? ?||w||22 (1)
This model has two advantages over the more preva-
lent form of a feature-rich unsupervised model, the
globally normalized Markov random field.2 First,
as we explain in Section 3, optimizing our objec-
tive does not require computing expectations over
the joint distribution. In the case of the POS HMM,
for example, we do not need to enumerate an in-
finite sum of products of potentials when optimiz-
ing, in contrast to Haghighi and Klein (2006). Sec-
ond, we found that locally normalized models em-
pirically outperform their globally normalized coun-
terparts, despite their efficiency and simplicity.
3 Optimization
3.1 Optimizing with Expectation Maximization
In this section, we describe the EM algorithm ap-
plied to our feature-rich, locally normalized models.
For models parameterized by standard multinomi-
als, EM optimizes L(?) = log P?(Y = y) (Demp-
ster et al, 1977). The E-step computes expected
counts for each tuple of decision d, context c, and
multinomial type t:
ed,c,t?E?
[
?
i?I
 (Xi =d, Xpi(i) =c, t)
?
?
?
?
Y = y
]
(2)
2The locally normalized model class is actually equivalent
to its globally normalized counterpart when the former meets
the following three conditions: (1) The graphical model is a
directed tree. (2) The BASIC features are included in f . (3) We
do not include regularization in the model (? = 0). This follows
from Smith and Johnson (2007).
These expected counts are then normalized in the
M-step to re-estimate ?:
?d,c,t ?
ed,c,t
?
d? ed?,c,t
Normalizing expected counts in this way maximizes
the expected complete log likelihood with respect to
the current model parameters.
EM can likewise optimize L(w) for our locally
normalized models with logistic parameterizations.
The E-step first precomputes multinomial parame-
ters from w for each decision, context, and type:
?d,c,t(w)?
exp?w, f(d, c, t)?
?
d? exp?w, f(d?, c, t)?
Then, expected counts e are computed accord-
ing to Equation 2. In the case of POS induction,
expected counts are computed with the forward-
backward algorithm in both the standard and logistic
parameterizations. The only change is that the con-
ditional probabilities ? are now functions of w.
The M-step changes more substantially, but still
relies on canonical NLP learning methods. We wish
to choose w to optimize the regularized expected
complete log likelihood:
?(w, e) =
?
d,c,t
ed,c,t log ?d,c,t(w)? ?||w||22 (3)
We optimize this objective via a gradient-based
search algorithm like LBFGS. The gradient with re-
spect to w takes the form
??(w, e) =
?
d,c,t
ed,c,t ??d,c,t(w)? 2? ?w (4)
?d,c,t(w) = f(d, c, t)?
?
d?
?d?,c,t(w)f(d?, c, t)
This gradient matches that of regularized logis-
tic regression in a supervised model: the differ-
ence ? between the observed and expected features,
summed over every decision and context. In the su-
pervised case, we would observe the count of occur-
rences of (d, c, t), but in the unsupervised M-step,
we instead substitute expected counts ed,c,t.
This gradient-based M-step is an iterative proce-
dure. For each different value of w considered dur-
ing the search, we must recompute ?(w), which re-
quires computation in proportion to the size of the
584
parameter space. However, e stays fixed throughout
the M-step. Algorithm 1 outlines EM in its entirety.
The subroutine climb(?, ?, ?) represents a generic op-
timization step such as an LBFGS iteration.
Algorithm 1 Feature-enhanced EM
repeat
Compute expected counts e  Eq. 2
repeat
Compute ?(w, e)  Eq. 3
Compute??(w, e)  Eq. 4
w ? climb(w, ?(w, e),??(w, e))
until convergence
until convergence
3.2 Direct Marginal Likelihood Optimization
Another approach to optimizing Equation 1 is to
compute the gradient of the log marginal likelihood
directly (Salakhutdinov et al, 2003). The gradient
turns out to have the same form as Equation 4, with
the key difference that ed,c,t is recomputed for every
different value of w. Algorithm 2 outlines the proce-
dure. Justification for this algorithm appears in the
Appendix.
Algorithm 2 Feature-enhanced direct gradient
repeat
Compute expected counts e  Eq. 2
Compute L(w)  Eq. 1
Compute ??(w, e)  Eq. 4
w ? climb(w, L(w),??(w, e))
until convergence
In practice, we find that this optimization ap-
proach leads to higher task accuracy for several
models. However, in cases where computing ed,c,t
is expensive, EM can be a more efficient alternative.
4 Part-of-Speech Induction
We now describe experiments that demonstrate the
effectiveness of locally normalized logistic models.
We first use the bigram HMM described in Sec-
tion 2.1 for POS induction, which has two types of
multinomials. For type EMIT, the decisions d are
words and contexts c are tags. For type TRANS, the
decisions and contexts are both tags.
4.1 POS Induction Features
We use the same set of features used by Haghighi
and Klein (2006) in their baseline globally normal-
ized Markov random field (MRF) model. These are
all coarse features on emission contexts that activate
for words with certain orthographic properties. We
use only the BASIC features for transitions. For
an emission with word y and tag z, we use the
following feature templates:
BASIC:  (y = ?, z = ?)
CONTAINS-DIGIT: Check if y contains digit and conjoin
with z:
 (containsDigit(y) = ?, z = ?)
CONTAINS-HYPHEN:  (containsHyphen(x) = ?, z = ?)
INITIAL-CAP: Check if the first letter of y is
capitalized:  (isCap(y) = ?, z = ?)
N-GRAM: Indicator functions for character n-
grams of up to length 3 present in y.
4.2 POS Induction Data and Evaluation
We train and test on the entire WSJ tag corpus (Mar-
cus et al, 1993). We attempt the most difficult ver-
sion of this task where the only information our sys-
tem can make use of is the unlabeled text itself. In
particular, we do not make use of a tagging dictio-
nary. We use 45 tag clusters, the number of POS tags
that appear in the WSJ corpus. There is an identifi-
ability issue when evaluating inferred tags. In or-
der to measure accuracy on the hand-labeled corpus,
we map each cluster to the tag that gives the highest
accuracy, the many-1 evaluation approach (Johnson,
2007). We run all POS induction models for 1000
iterations, with 10 random initializations. The mean
and standard deviation of many-1 accuracy appears
in Table 1.
4.3 POS Induction Results
We compare our model to the basic HMM and a bi-
gram version of the feature-enhanced MRF model of
Haghighi and Klein (2006). Using EM, we achieve
a many-1 accuracy of 68.1. This outperforms the
basic HMM baseline by a 5.0 margin. The same
model, trained using the direct gradient approach,
achieves a many-1 accuracy of 75.5, outperforming
the basic HMM baseline by a margin of 12.4. These
results show that the direct gradient approach can of-
fer additional boosts in performance when used with
a feature-enhanced model. We also outperform the
585
globally normalized MRF, which uses the same set
of features and which we train using a direct gradi-
ent approach.
To the best of our knowledge, our system achieves
the best performance to date on the WSJ corpus for
totally unsupervised POS tagging.3
5 Grammar Induction
We next apply our technique to a grammar induction
task: the unsupervised learning of dependency parse
trees via the dependency model with valence (DMV)
(Klein and Manning, 2004). A dependency parse is
a directed tree over tokens in a sentence. Each edge
of the tree specifies a directed dependency from a
head token to a dependent, or argument token. Thus,
the number of dependencies in a parse is exactly the
number of tokens in the sentence, not counting the
artificial root token.
5.1 Dependency Model with Valence
The DMV defines a probability distribution over de-
pendency parse trees. In this head-outward attach-
ment model, a parse and the word tokens are derived
together through a recursive generative process. For
each token generated so far, starting with the root, a
set of left dependents is generated, followed by a set
of right dependents.
There are two types of multinomial distributions
in this model. The Bernoulli STOP probabilities
?d,c,STOP capture the valence of a particular head. For
this type, the decision d is whether or not to stop
generating arguments, and the context c contains the
current head h, direction ? and adjacency adj. If
a head?s stop probability is high, it will be encour-
aged to accept few arguments. The ATTACH multi-
nomial probability distributions ?d,c,ATTACH capture
attachment preferences of heads. For this type, a de-
cision d is an argument token a, and the context c
consists of a head h and a direction ?.
We take the same approach as previous work
(Klein and Manning, 2004; Cohen and Smith, 2009)
and use gold POS tags in place of words.
3Haghighi and Klein (2006) achieve higher accuracies by
making use of labeled prototypes. We do not use any external
information.
5.2 Grammar Induction Features
One way to inject knowledge into a dependency
model is to encode the similarity between the vari-
ous morphological variants of nouns and verbs. We
encode this similarity by incorporating features into
both the STOP and the ATTACH probabilities. The
attachment features appear below; the stop feature
templates are similar and are therefore omitted.
BASIC:  (a = ?, h = ?, ? = ?)
NOUN: Generalize the morphological variants of
nouns by using isNoun(?):
 (a = ?, isNoun(h) = ?, ? = ?)
 (isNoun(a) = ?, h = ?, ? = ?)
 (isNoun(a) = ?, isNoun(h) = ?, ? = ?)
VERB: Same as above, generalizing verbs instead
of nouns by using isVerb(?)
NOUN-VERB: Same as above, generalizing with
isVerbOrNoun(?) = isVerb(?)? isNoun(?)
BACK-OFF: We add versions of all other features that
ignore direction or adjacency.
While the model has the expressive power to al-
low specific morphological variants to have their
own behaviors, the existence of coarse features en-
courages uniform analyses, which in turn gives bet-
ter accuracies.
Cohen and Smith?s (2009) method has similar
characteristics. They add a shared logistic-normal
prior (SLN) to the DMV in order to tie multinomial
parameters across related derivation events. They
achieve their best results by only tying parame-
ters between different multinomials when the cor-
responding contexts are headed by nouns and verbs.
This observation motivates the features we choose to
incorporate into the DMV.
5.3 Grammar Induction Data and Evaluation
For our English experiments we train and report di-
rected attachment accuracy on portions of the WSJ
corpus. We work with a standard, reduced version of
WSJ, WSJ10, that contains only sentences of length
10 or less after punctuation has been removed. We
train on sections 2-21, and use section 22 as a de-
velopment set. We report accuracy on section 23.
These are the same training, development, and test
sets used by Cohen and Smith (2009). The regular-
ization parameter (?) is tuned on the development
set to maximize accuracy.
For our Chinese experiments, we use the same
corpus and training/test split as Cohen and Smith
586
(2009). We train on sections 1-270 of the Penn Chi-
nese Treebank (Xue et al, 2002), similarly reduced
(CTB10). We test on sections 271-300 of CTB10,
and use sections 400-454 as a development set.
The DMV is known to be sensitive to initializa-
tion. We use the deterministic harmonic initializer
from Klein and Manning (2004). We ran each op-
timization procedure for 100 iterations. The results
are reported in Table 1.
5.4 Grammar Induction Results
We are able to outperform Cohen and Smith?s (2009)
best system, which requires a more complicated
variational inference method, on both English and
Chinese data sets. Their system achieves an accu-
racy of 61.3 for English and an accuracy of 51.9 for
Chinese.4 Our feature-enhanced model, trained us-
ing the direct gradient approach, achieves an accu-
racy of 63.0 for English, and an accuracy of 53.6 for
Chinese. To our knowledge, our method for feature-
based dependency parse induction outperforms all
existing methods that make the same set of condi-
tional independence assumptions as the DMV.
6 Word Alignment
Word alignment is a core machine learning com-
ponent of statistical machine translation systems,
and one of the few NLP tasks that is dominantly
solved using unsupervised techniques. The pur-
pose of word alignment models is to induce a cor-
respondence between the words of a sentence and
the words of its translation.
6.1 Word Alignment Models
We consider two classic generative alignment mod-
els that are both used heavily today, IBM Model 1
(Brown et al, 1994) and the HMM alignment model
(Ney and Vogel, 1996). These models generate a
hidden alignment vector z and an observed foreign
sentence y, all conditioned on an observed English
sentence e. The likelihood of both models takes the
form:
P (y, z|e) =
?
j
p(zj = i|zj?1) ? ?yj ,ei,ALIGN
4Using additional bilingual data, Cohen and Smith (2009)
achieve an accuracy of 62.0 for English, and an accuracy of
52.0 for Chinese, still below our results.
Model Inference Reg Eval
POS Induction ? Many-1
W
SJ
Basic-HMM EM ? 63.1 (1.3)
Feature-MRF LBFGS 0.1 59.6 (6.9)
Feature-HMM EM 1.0 68.1 (1.7)
LBFGS 1.0 75.5 (1.1)
Grammar Induction ? Dir
W
SJ
10
Basic-DMV EM ? 47.8
Feature-DMV EM 0.05 48.3
LBFGS 10.0 63.0
(Cohen and Smith, 2009) 61.3
C
T
B
10
Basic-DMV EM ? 42.5
Feature-DMV EM 1.0 49.9
LBFGS 5.0 53.6
(Cohen and Smith, 2009) 51.9
Word Alignment ? AER
N
IS
T
C
hE
n Basic-Model 1 EM ? 38.0
Feature-Model 1 EM ? 35.6
Basic-HMM EM ? 33.8
Feature-HMM EM ? 30.0
Word Segmentation ? F1
B
R
Basic-Unigram EM ? 76.9 (0.1)
Feature-Unigram EM 0.2 84.5 (0.5)
LBFGS 0.2 88.0 (0.1)
(Johnson and Goldwater, 2009) 87
Table 1: Locally normalized feature-based models outperform
all proposed baselines for all four tasks. LBFGS outperformed
EM in all cases where the algorithm was sufficiently fast to run.
Details of each experiment appear in the main text.
The distortion term p(zj = i|zj?1) is uniform in
Model 1, and Markovian in the HMM. See Liang et
al. (2006) for details on the specific variant of the
distortion model of the HMM that we used. We use
these standard distortion models in both the baseline
and feature-enhanced word alignment systems.
The bilexical emission model ?y,e,ALIGN differen-
tiates our feature-enhanced system from the base-
line system. In the former, the emission model is a
standard conditional multinomial that represents the
probability that decision word y is generated from
context word e, while in our system, the emission
model is re-parameterized as a logistic regression
model and feature-enhanced.
Many supervised feature-based alignment models
have been developed. In fact, this logistic parame-
terization of the HMM has been proposed before and
yielded alignment improvements, but was trained
using supervised estimation techniques (Varea et al,
2002).5 However, most full translation systems to-
5Varea et al (2002) describes unsupervised EM optimiza-
tion with logistic regression models at a high level?their dy-
namic training approach?but provides no experiments.
587
day rely on unsupervised learning so that the models
may be applied easily to many language pairs. Our
approach provides efficient and consistent unsuper-
vised estimation for feature-rich alignment models.
6.2 Word Alignment Features
The BASIC features on pairs of lexical items
provide strong baseline performance. We add
coarse features to the model in order to inject
prior knowledge and tie together lexical items with
similar characteristics.
BASIC:  (e = ?, y = ?)
EDIT-DISTANCE:  (dist(y, e) = ?)
DICTIONARY:  ((y, e) ? D) for dictionary D.
STEM:  (stem(e) = ?, y = ?) for Porter stemmer.
PREFIX:  (prefix(e) = ?, y = ?) for prefixes of
length 4.
CHARACTER:  (e = ?, charAt(y, i) = ?) for index i in
the Chinese word.
These features correspond to several common
augmentations of word alignment models, such as
adding dictionary priors and truncating long words,
but here we integrate them all coherently into a sin-
gle model.
6.3 Word Alignment Data and Evaluation
We evaluate on the standard hand-aligned portion
of the NIST 2002 Chinese-English development set
(Ayan et al, 2005). The set is annotated with sure S
and possible P alignments. We measure alignment
quality using alignment error rate (AER) (Och and
Ney, 2000).
We train the models on 10,000 sentences of FBIS
Chinese-English newswire. This is not a large-scale
experiment, but large enough to be relevant for low-
resource languages. LBFGS experiments are not
provided because computing expectations in these
models is too computationally intensive to run for
many iterations. Hence, EM training is a more ap-
propriate optimization approach: computing the M-
step gradient requires only summing over word type
pairs, while the marginal likelihood gradient needed
for LBFGS requires summing over training sentence
alignments. The final alignments, in both the base-
line and the feature-enhanced models, are computed
by training the generative models in both directions,
combining the result with hard union competitive
thresholding (DeNero and Klein, 2007), and us-
ing agreement training for the HMM (Liang et al,
2006). The combination of these techniques yields
a state-of-the-art unsupervised baseline for Chinese-
English.
6.4 Word Alignment Results
For both IBM Model 1 and the HMM alignment
model, EM training with feature-enhanced models
outperforms the standard multinomial models, by
2.4 and 3.8 AER respectively.6 As expected, large
positive weights are assigned to both the dictionary
and edit distance features. Stem and character fea-
tures also contribute to the performance gain.
7 Word Segmentation
Finally, we show that it is possible to improve upon
the simple and effective word segmentation model
presented in Liang and Klein (2009) by adding
phonological features. Unsupervised word segmen-
tation is the task of identifying word boundaries in
sentences where spaces have been removed. For a
sequence of characters y = (y1, ..., yn), a segmen-
tation is a sequence of segments z = (z1, ..., z|z|)
such that z is a partition of y and each zi is a con-
tiguous subsequence of y. Unsupervised models for
this task infer word boundaries from corpora of sen-
tences of characters without ever seeing examples of
well-formed words.
7.1 Unigram Double-Exponential Model
Liang and Klein?s (2009) unigram double-
exponential model corresponds to a simple
derivational process where sentences of characters
x are generated a word at a time, drawn from a
multinomial over all possible strings ?z,SEGMENT.
For this type, there is no context and the decision is
the particular string generated. In order to avoid the
degenerate MLE that assigns mass only to single
segment sentences it is helpful to independently
generate a length for each segment from a fixed
distribution. Liang and Klein (2009) constrain in-
dividual segments to have maximum length 10 and
generate lengths from the following distribution:
?l,LENGTH = exp(?l1.6) when 1 ? l ? 10. Their
model is deficient since it is possible to generate
6The best published results for this dataset are supervised,
and trained on 17 times more data (Haghighi et al, 2009).
588
lengths that are inconsistent with the actual lengths
of the generated segments. The likelihood equation
is given by:
P (Y = y,Z = z) =
?STOP
|z|
?
i=1
[
(1? ?STOP) ?zi,SEGMENT exp(?|zi|1.6)
]
7.2 Segmentation Data and Evaluation
We train and test on the phonetic version of the
Bernstein-Ratner corpus (1987). This is the same
set-up used by Liang and Klein (2009), Goldwater
et al (2006), and Johnson and Goldwater (2009).
This corpus consists of 9790 child-directed utter-
ances transcribed using a phonetic representation.
We measure segment F1 score on the entire corpus.
We run all word segmentation models for 300 iter-
ations with 10 random initializations and report the
mean and standard deviation of F1 in Table 1.
7.3 Segmentation Features
The SEGMENT multinomial is the important distri-
bution in this model. We use the following features:
BASIC:  (z = ?)
LENGTH:  (length(z) = ?)
NUMBER-VOWELS:  (numVowels(z) = ?)
PHONO-CLASS-PREF:  (prefix(coarsePhonemes(z)) = ?)
PHONO-CLASS-PREF:  (suffix(coarsePhonemes(z)) = ?)
The phonological class prefix and suffix features
project each phoneme of a string to a coarser class
and then take prefix and suffix indicators on the
string of projected characters. We include two ver-
sions of these features that use projections with dif-
ferent levels of coarseness. The goal of these fea-
tures is to help the model learn general phonetic
shapes that correspond to well-formed word bound-
aries.
As is the case in general for our method, the
feature-enhanced unigram model still respects the
conditional independence assumptions that the stan-
dard unigram model makes, and inference is still
performed using a simple dynamic program to com-
pute expected sufficient statistics, which are just seg-
ment counts.
7.4 Segmentation Results
To our knowledge our system achieves the best per-
formance to date on the Bernstein-Ratner corpus,
with an F1 of 88.0. It is substantially simpler than
the non-parametric Bayesian models proposed by
Johnson et al (2007), which require sampling pro-
cedures to perform inference and achieve an F1 of
87 (Johnson and Goldwater, 2009). Similar to our
other results, the direct gradient approach outper-
forms EM for feature-enhanced models, and both
approaches outperform the baseline, which achieves
an F1 of 76.9.
8 Conclusion
We have shown that simple, locally normalized
models can effectively incorporate features into un-
supervised models. These enriched models can
be easily optimized using standard NLP build-
ing blocks. Beyond the four tasks explored in
this paper?POS tagging, DMV grammar induc-
tion, word alignment, and word segmentation?the
method can be applied to many other tasks, for ex-
ample grounded semantics, unsupervised PCFG in-
duction, document clustering, and anaphora resolu-
tion.
Acknowledgements
We thank Percy Liang for making his word segmen-
tation code available to us, and the anonymous re-
viewers for their comments.
Appendix: Optimization
In this section, we derive the gradient of the log marginal likeli-
hood needed for the direct gradient approach. Let w0 be the cur-
rent weights in Algorithm 2 and e = e(w0) be the expectations
under these weights as computed in Equation 2. In order to jus-
tify Algorithm 2, we need to prove that ?L(w0) = ??(w0, e).
We use the following simple lemma: if ?, ? are real-valued
functions such that: (1) ?(w0) = ?(w0) for some w0; (2)
?(w) ? ?(w) on an open set containing w0; and (3), ? and ?
are differentiable at w0; then ??(w0) = ??(w0).
We set ?(w) = L(w) and ?(w) = ?(w, e)?P
z
Pw0(Z =
z|Y = y) log Pw0(Z = z|Y = y). If we can show that ?, ?
satisfy the conditions of the lemma we are done since the second
term of ? depends on w0, but not on w.
Property (3) can be easily checked, and property (2) follows
from Jensen?s inequality. Finally, property (1) follows from
Lemma 2 of Neal and Hinton (1998).
589
References
N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining
word alignments using neural networks. In Empirical
Methods in Natural Language Processing.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. K. Nelson and A. van Kleeck.
M. Bisani and H. Ney. 2008. Joint-sequence models for
grapheme-to-phoneme conversion.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change.
In Neural Information Processing Systems.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In North American Chapter
of the Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological).
J. DeNero and D. Klein. 2007. Tailoring word align-
ments to syntactic machine translation. In Association
for Computational Linguistics.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Association for Computational Lin-
guistics.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Association for Computa-
tional Linguistics.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Association for Computational Linguistics.
M. Johnson and S. Goldwater. 2009. Improving non-
parametric Bayesian inference: Experiments on unsu-
pervised word segmentation with adaptor grammars.
In North American Chapter of the Association for
Computational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: a framework for specifying com-
positional nonparametric Bayesian models. In Neural
Information Processing Systems.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing/Computational Natural Language
Learning.
D. Klein and C. D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Association for Computational
Linguistics.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In North American Chapter of the As-
sociation for Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Chapter of the Associ-
ation for Computational Linguistics.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics.
R. Neal and G. E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models. Kluwer
Academic Publishers.
H. Ney and S. Vogel. 1996. HMM-based word alignment
in statistical translation. In International Conference
on Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Association for Computational Lin-
guistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with EM and expectation-conjugate-
gradient. In International Conference on Machine
Learning.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In As-
sociation for Computational Linguistics.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Linguistics.
I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002.
Refined lexicon models for statistical machine transla-
tion using a maximum entropy approach. In Associa-
tion for Computational Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In International
Conference on Computational Linguistics.
590
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29?38,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Fast Inference in Phrase Extraction Models with Belief Propagation
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Abstract
Modeling overlapping phrases in an align-
ment model can improve alignment quality
but comes with a high inference cost. For
example, the model of DeNero and Klein
(2010) uses an ITG constraint and beam-based
Viterbi decoding for tractability, but is still
slow. We first show that their model can be
approximated using structured belief propaga-
tion, with a gain in alignment quality stem-
ming from the use of marginals in decoding.
We then consider a more flexible, non-ITG
matching constraint which is less efficient for
exact inference but more efficient for BP. With
this new constraint, we achieve a relative error
reduction of 40% in F5 and a 5.5x speed-up.
1 Introduction
Modern statistical machine translation (MT) sys-
tems most commonly infer their transfer rules from
word-level alignments (Koehn et al, 2007; Li and
Khudanpur, 2008; Galley et al, 2004), typically
using a deterministic heuristic to convert these to
phrase alignments (Koehn et al, 2003). There have
been many attempts over the last decade to develop
model-based approaches to the phrase alignment
problem (Marcu and Wong, 2002; Birch et al, 2006;
DeNero et al, 2008; Blunsom et al, 2009). How-
ever, most of these have met with limited success
compared to the simpler heuristic method. One key
problem with typical models of phrase alignment
is that they choose a single (latent) segmentation,
giving rise to undesirable modeling biases (DeNero
et al, 2006) and reducing coverage, which in turn
reduces translation quality (DeNeefe et al, 2007;
DeNero et al, 2008). On the other hand, the extrac-
tion heuristic identifies many overlapping options,
and achieves high coverage.
In response to these effects, the recent phrase
alignment work of DeNero and Klein (2010) mod-
els extraction sets: collections of overlapping phrase
pairs that are consistent with an underlying word
alignment. Their extraction set model is empirically
very accurate. However, the ability to model over-
lapping ? and therefore non-local ? features comes
at a high computational cost. DeNero and Klein
(2010) handle this in part by imposing a structural
ITG constraint (Wu, 1997) on the underlying word
alignments. This permits a polynomial-time algo-
rithm, but it is still O(n6), with a large constant
factor once the state space is appropriately enriched
to capture overlap. Therefore, they use a heavily
beamed Viterbi search procedure to find a reason-
able alignment within an acceptable time frame. In
this paper, we show how to use belief propagation
(BP) to improve on the model?s ITG-based struc-
tural formulation, resulting in a new model that is
simultaneously faster and more accurate.
First, given the model of DeNero and Klein
(2010), we decompose it into factors that admit
an efficient BP approximation. BP is an inference
technique that can be used to efficiently approxi-
mate posterior marginals on variables in a graphical
model; here the marginals of interest are the phrase
pair posteriors. BP has only recently come into use
in the NLP community, but it has been shown to be
effective in other complex structured classification
tasks, such as dependency parsing (Smith and Eis-
ner, 2008). There has also been some prior success
in using BP for both discriminative (Niehues and
Vogel, 2008) and generative (Cromie`res and Kuro-
hashi, 2009) word alignment models.
By aligning all phrase pairs whose posterior under
BP exceeds some fixed threshold, our BP approxi-
mation of the model of DeNero and Klein (2010) can
29
achieve a comparable phrase pair F1. Furthermore,
because we have posterior marginals rather than a
single Viterbi derivation, we can explicitly force the
aligner to choose denser extraction sets simply by
lowering the marginal threshold. Therefore, we also
show substantial improvements over DeNero and
Klein (2010) in recall-heavy objectives, such as F5.
More importantly, we also show how the BP fac-
torization allows us to relax the ITG constraint, re-
placing it with a new set of constraints that per-
mit a wider family of alignments. Compared to
ITG, the resulting model is less efficient for exact
inference (where it is exponential), but more effi-
cient for our BP approximation (where it is only
quadratic). Our new model performs even better
than the ITG-constrained model on phrase align-
ment metrics while being faster by a factor of 5.5x.
2 Extraction Set Models
Figure 1 shows part of an aligned sentence pair, in-
cluding the word-to-word alignments, and the ex-
tracted phrase pairs licensed by those alignments.
Formally, given a sentence pair (e, f), a word-level
alignment a is a collection of links between target
words ei and source words fj . Following past work,
we further divide word links into two categories:
sure and possible, shown in Figure 1 as solid and
hatched grey squares, respectively. We represent a
as a grid of ternary word link variables aij , each of
which can take the value sure to represent a sure link
between ei and fj , poss to represent a possible link,
or off to represent no link.
An extraction set pi is a set of aligned phrase pairs
to be extracted from (e, f), shown in Figure 1 as
green rounded rectangles. We represent pi as a set of
boolean variables pighk`, which each have the value
true when the target span [g, h] is phrase-aligned to
the source span [k, `]. Following previous work on
phrase extraction, we limit the size of pi by imposing
a phrase length limit d: pi only contains a variable
pighk` if h? g < d and `? k < d.
There is a deterministic mapping pi(a) from a
word alignment to the extraction set licensed by that
word alignment. We will briefly describe it here, and
then present our factorized model.
e3 e4 e5 e6 e7
f5
f6
f7
f8
f9
?f5 = [7, 7]
?f6 = [5, 6]
?f7 = [5, 6]
?f8 = [4, 4]
?f9 = [?1,?]
Figure 1: A schematic representation of part of a sen-
tence pair. Solid grey squares indicate sure links (e.g.
a48 = sure), and hatched squares possible links (e.g.
a67 = poss). Rounded green rectangles are extracted
phrase pairs (e.g. pi5667 = true). Target spans are shown
as blue vertical lines and source spans as red horizontal
lines. Because there is a sure link at a48, ?f8 = [4, 4] does
not include the possible link at a38. However, f7 only
has possible links, so ?f7 = [5, 6] is the span containing
those. f9 is null-aligned, so ?f9 = [?1,?], which blocks
all phrase pairs containing f9 from being extracted.
2.1 Extraction Sets from Word Alignments
The mapping from a word alignment to the set of
licensed phrase pairs pi(a) is based on the standard
rule extraction procedures used in most modern sta-
tistical systems (Koehn et al, 2003; Galley et al,
2006; Chiang, 2007), but extended to handle pos-
sible links (DeNero and Klein, 2010). We start by
using a to find a projection from each target word ei
onto a source span, represented as blue vertical lines
in Figure 1. Similarly, source words project onto
target spans (red horizontal lines in Figure 1). pi(a)
contains a phrase pair iff every word in the target
span projects within the source span and vice versa.
Figure 1 contains an example for d = 2.
Formally, the mapping introduces a set of spans
?. We represent the spans as variables whose values
are intervals, where ?ei = [k, `] means that the tar-
get word ei projects to the source span [k, `]. The
set of legal values for ?ei includes any interval with
0 ? k ? ` < |f| and ` ? k < d, plus the special in-
terval [?1,?] that indicates ei is null-aligned. The
span variables for source words ?fj have target spans
[g, h] as values and are defined analogously.
For a set I of positions, we define the range func-
30
tion:
range(I) =
{
[?1,?] I = ?
[mini?I i,maxi?I i] else
(1)
For a fixed word alignment a we set the target
span variable ?ei :
?ei,s = range({j : aij = sure}) (2)
?ei,p = range({j : aij 6= off}) (3)
?ei = ?ei,s ? ?ei,p (4)
As illustrated in Figure 1, this sets ?ei to the min-
imal span containing all the source words with a
sure link to ei if there are any. Otherwise, because
of the special case for range(I) when I is empty,
?ei,s = [?1,?], so ?ei is the minimal span containing
all poss-aligned words. If all word links to ei are off,
indicating that ei is null-aligned, then ?ei is [?1,?],
preventing the alignment of any phrase pairs con-
taining ei.
Finally, we specify which phrase pairs should be
included in the extraction set pi. Given the spans ?
based on a, pi(a) sets pighk` = true iff every word in
each phrasal span projects within the other:
?ei ? [k, `] ?i ? [g, h] (5)
?fj ? [g, h] ?j ? [k, `]
2.2 Formulation as a Graphical Model
We score triples (a, pi, ?) as the dot product of a
weight vector w that parameterizes our model and a
feature vector ?(a, pi, ?). The feature vector decom-
poses into word alignment features ?a, phrase pair
features ?pi and target and source null word features
?e? and ?
f
? :
1
?(a, pi, ?) =
?
i,j
?a(aij) +
?
g,h,k,`
?pi(pighk`)+
?
i
?e?(?ei ) +
?
j
?f?(?
f
j ) (6)
This feature function is exactly the same as that
used by DeNero and Klein (2010).2 However, while
1In addition to the arguments we write out explicitly, all fea-
ture functions have access to the observed sentence pair (e, f).
2Although the null word features are not described in DeN-
ero and Klein (2010), all of their reported results include these
features (DeNero, 2010).
they formulated their inference problem as a search
for the highest scoring triple (a, pi, ?) for an ob-
served sentence pair (e, f), we wish to derive a con-
ditional probability distribution p(a, pi, ?|e, f). We
do this with the standard transformation for linear
models: p(a, pi, ?|e, f) ? exp(w??(a, pi, ?)). Due to
the factorization in Eq. (6), this exponentiated form
becomes a product of local multiplicative factors,
and hence our model forms an undirected graphical
model, or Markov random field.
In addition to the scoring function, our model
also includes constraints on which triples (a, pi, ?)
have nonzero probability. DeNero and Klein (2010)
implicitly included these constraints in their repre-
sentation: instead of sets of variables, they used a
structured representation that only encodes triples
(a, pi, ?) satisfying both the mapping pi = pi(a) and
the structural constraint that a can be generated by
a block ITG grammar. However, our inference pro-
cedure, BP, requires that we represent (a, pi, ?) as an
assignment of values to a set of variables. Therefore,
we must explicitly encode all constraints into the
multiplicative factors that define the model. To ac-
complish this, in addition to the soft scoring factors
we have already mentioned, our model also includes
a set of hard constraint factors. Hard constraint fac-
tors enforce the relationships between the variables
of the model by taking a value of 0 when the con-
straints they encode are violated and a value of 1
when they are satisfied. The full factor graph rep-
resentation of our model, including both soft scor-
ing factors and hard constraint factors, is drawn
schematically in Figure 2.
2.2.1 Soft Scoring Factors
The scoring factors all take the form exp(w ? ?),
and so can be described in terms of their respective
local feature vectors, ?. Depending on the values of
the variables each factor depends on, the factor can
be active or inactive. Features are only extracted for
active factors; otherwise ? is empty and the factor
produces a value of 1.
SURELINK. Each word alignment variable aij
has a corresponding SURELINK factor Lij to incor-
porate scores from the features ?a(aij). Lij is ac-
tive whenever aij = sure. ?a(aij) includes poste-
riors from unsupervised jointly trained HMM word
alignment models (Liang et al, 2006), dictionary
31
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
A
(a) ITG factor
agk ahk
ag? ah?
ag|f| ah|f|
a|e|k
a|e|?
Pghk?
Rghk?
?ghk?
Seg Seh
NehNeg
?eg ?eh
Sfk
Sf?
Nf?
Nfk
?fk
?f?
(b) SPAN and EXTRACT factors
Figure 2: A factor graph representation of the ITG-based extraction set model. For visual clarity, we draw the graph
separated into two components: one containing the factors that only neighbor word link variables, and one containing
the remaining factors.
and identical word features, a position distortion fea-
ture, and features for numbers and punctuation.
PHRASEPAIR. For each phrase pair variable
pighk`, scores from ?pi(pighk`) come from the factor
Rghk`, which is active if pighk` = true. Most of the
model?s features are on these factors, and include
relative frequency statistics, lexical template indica-
tor features, and indicators for numbers of words and
Chinese characters. See DeNero and Klein (2010)
for a more comprehensive list.
NULLWORD. We can determine if a word is
null-aligned by looking at its corresponding span
variable. Thus, we include features from ?e?(?ei ) in
a factor N ei that is active if ?ei = [?1,?]. The
features are mostly indicators for common words.
There are also factors Nfj for source words, which
are defined analogously.
2.2.2 Hard Constraint Factors
We encode the hard constraints on relationships
between variables in our model using three fami-
lies of factors, shown graphically in Figure 2. The
SPAN and EXTRACT factors together ensure that
pi = pi(a). The ITG factor encodes the structural
constraint on a.
SPAN. First, for each target word ei we include
a factor Sei to ensure that the span variable ?ei has
a value that agrees with the projection of the word
alignment a. As shown in Figure 2b, Sei depends
on ?ei and all the word alignment variables aij in
column i of the word alignment grid. Sei has value
1 iff the equality in Eq. (4) holds. Our model also
includes a factor Sfj to enforce the analogous rela-
tionship between each ?fj and corresponding row j
of a.
EXTRACT. For each phrase pair variable pighk`
we have a factor Pghk` to ensure that pighk` = true
iff it is licensed by the span projections ?. As shown
in Figure 2b, in addition to pighk`, Pghk` depends on
the range of span variables ?ei for i ? [g, h] and ?fj
for j ? [k, `]. Pghk` is satisfied when pighk` = true
and the relations in Eq. (5) all hold, or when pighk` =
false and at least one of those relations does not hold.
ITG. Finally, to enforce the structural constraint
on a, we include a single global factor A that de-
pends on all the word link variables in a (see Fig-
ure 2a). A is satisfied iff a is in the family of
block inverse transduction grammar (ITG) align-
ments. The block ITG family permits multiple links
to be on (aij 6= off) for a particular word ei via termi-
nal block productions, but ensures that every word is
32
in at most one such terminal production, and that the
full set of terminal block productions is consistent
with ITG reordering patterns (Zhang et al, 2008).
3 Relaxing the ITG Constraint
The ITG factor can be viewed as imposing two dif-
ferent types of constraints on allowable word align-
ments a. First, it requires that each word is aligned
to at most one relatively short subspan of the other
sentence. This is a linguistically plausible con-
straint, as it is rarely the case that a single word will
translate to an extremely long phrase, or to multiple
widely separated phrases.3
The other constraint imposed by the ITG factor
is the ITG reordering constraint. This constraint
is imposed primarily for reasons of computational
tractability: the standard dynamic program for bi-
text parsing depends on ITG reordering (Wu, 1997).
While this constraint is not dramatically restric-
tive (Haghighi et al, 2009), it is plausible that re-
moving it would permit the model to produce better
alignments. We tested this hypothesis by develop-
ing a new model that enforces only the constraint
that each word align to one limited-length subspan,
which can be viewed as a generalization of the at-
most-one-to-one constraint frequently considered in
the word-alignment literature (Taskar et al, 2005;
Cromie`res and Kurohashi, 2009).
Our new model has almost exactly the same form
as the previous one. The only difference is that A is
replaced with a new family of simpler factors:
ONESPAN. For each target word ei (and each
source word fj) we include a hard constraint factor
U ei (respectively U
f
j ). U ei is satisfied iff |?ei,p| < d
(length limit) and either ?ei,p = [?1,?] or ?j ?
?ei,p, aij 6= off (no gaps), with ?ei,p as in Eq. (3). Fig-
ure 3 shows the portion of the factor graph from Fig-
ure 2a redrawn with the ONESPAN factors replacing
the ITG factor. As Figure 3 shows, there is no longer
a global factor; each U ei depends only on the word
link variables from column i.
3Short gaps can be accomodated within block ITG (and in
our model are represented as possible links) as long as the total
aligned span does not exceed the block size.
a11 a21
Lij
L11 L21
a12
ai1
Li1
L12 L22
a22
a1j aij
L1j
Uf1
Uf2
Ufj
Ue1 Ue2 Uei
Figure 3: ONESPAN factors
4 Belief Propagation
Belief propagation is a generalization of the well
known sum-product algorithm for undirected graph-
ical models. We will provide only a procedural
sketch here, but a good introduction to BP for in-
ference in structured NLP models can be found
in Smith and Eisner (2008), and Chapters 16 and 23
of MacKay (2003) contain a general introduction to
BP in the more general context of message-passing
algorithms.
At a high level, each variable maintains a local
distribution over its possible values. These local dis-
tribution are updated via messages passed between
variables and factors. For a variable V , N (V ) de-
notes the set of factors neighboring V in the fac-
tor graph. Similarly, N (F ) is the set of variables
neighboring the factor F . During each round of BP,
messages are sent from each variable to each of its
neighboring factors:
q(k+1)V?F (v) ?
?
G?N (V ),G 6=F
r(k)G?V (v) (7)
and from each factor to each of its neighboring vari-
ables:
r(k+1)F?V (v) ?
?
XF ,XF [V ]=v
F (XF )
?
U?N (F ),U 6=V
q(k)U?F (v) (8)
where XF is a partial assignment of values to just
the variables in N (F ).
33
Marginal beliefs at time k can be computed by
simply multiplying together all received messages
and normalizing:
b(k)V (v) ?
?
G?N (V )
r(k)G?V (v) (9)
Although messages can be updated according to
any schedule, generally one iteration of BP updates
each message once. The process iterates until some
stopping criterion has been met: either a fixed num-
ber of iterations or some convergence metric.
For our models, we say that BP has converged
whenever
?
V,v
(
b(k)V (v)? b
(k?1)
V (v)
)2
< ? for
some small ? > 0.4 While we have no theoretical
convergence guarantees, it usually converges within
10 iterations in practice.
5 Efficient BP for Extraction Set Models
In general, the efficiency of BP depends directly on
the arity of the factors in the model. Performed
na??vely, the sum in Eq. (8) will take time that grows
exponentially with the size of N (F ). For the soft-
scoring factors, which each depend only on a single
variable, this isn?t a problem. However, our model
also includes factors whose arity grows with the in-
put size: for example, explicitly enumerating all as-
signments to the word link variables that the ITG
factor depends on would take O(3n2) time.5
To run BP in a reasonable time frame, we need
efficient factor-specific propagators that can exploit
the structure of the factor functions to compute out-
going messages in polynomial time (Duchi et al,
2007; Smith and Eisner, 2008). Fortunately, all of
our hard constraints permit dynamic programs that
accomplish this propagation. Space does not permit
a full description of these dynamic programs, but we
will briefly sketch the intuitions behind them.
SPAN and ONESPAN. Marginal beliefs for Sei or
U ei can be computed inO(nd2) time. The key obser-
vation is that for any legal value ?ei = [k, `], Sei and
U ei require that aij = off for all j /? [k, `].6 Thus, we
start by computing the product of all the off beliefs:
4We set ? = 0.001.
5For all asymptotic analysis, we define n = max(|e|, |f|).
6For ease of exposition, we assume that all alignments are
either sure or off ; the modifications to account for the general
case are straightforward.
Factor Runtime Count Total
SURELINK O(1) O(n2) O(n2)
PHRASEPAIR O(1) O(n2d2) O(n2d2)
NULLWORD O(nd) O(n) O(n2d)
SPAN O(nd2) O(n) O(n2d2)
EXTRACT O(d3) O(n2d2) O(n2d5)
ITG O(n6) 1 O(n6)
ONESPAN O(nd2) O(n) O(n2d2)
Table 1: Asymptotic complexity for all factors.
b? =
?
j qaij (off). Then, for each of the O(nd) legal
source spans [k, `] we can efficiently find a joint be-
lief by summing over consistent assignments to the
O(d) link variables in that span.
EXTRACT. Marginal beliefs for Pghk` can be
computed inO(d3) time. For each of theO(d) target
words, we can find the total incoming belief that ?ei
is within [k, `] by summing over the O(d2) values
[k?, `?] where [k?, `?] ? [k, `]. Likewise for source
words. Multiplying together these per-word beliefs
and the belief that pighk` = true yields the joint be-
lief of a consistent assignment with pighk` = true,
which can be used to efficiently compute outgoing
messages.
ITG. To build outgoing messages, the ITG fac-
torA needs to compute marginal beliefs for all of the
word link variables aij . These can all be computed
in O(n6) time by using a standard bitext parser to
run the inside-outside algorithm. By using a normal
form grammar for block ITG with nulls (Haghighi
et al, 2009), we ensure that there is a 1-1 correspon-
dence between the ITG derivations the parser sums
over and word alignments a that satisfy A.
The asymptotic complexity for all the factors is
shown in Table 1. The total complexity for inference
in each model is simply the sum of the complexities
of its factors, so the complexity of the ITG model is
O(n2d5 + n6), while the complexity of the relaxed
model is just O(n2d5). The complexity of exact in-
ference, on the other hand, is exponential in d for the
ITG model and exponential in both d and n for the
relaxed model.
34
6 Training and Decoding
We use BP to compute marginal posteriors, which
we use at training time to get expected feature counts
and at test time for posterior decoding. For each sen-
tence pair, we continue to pass messages until either
the posteriors converge, or some maximum number
of iterations has been reached.7 After running BP,
the marginals we are interested in can all be com-
puted with Eq. (9).
6.1 Training
We train the model to maximize the log likelihood of
manually word-aligned gold training sentence pairs
(with L2 regularization). Because pi and ? are deter-
mined when a is observed, the model has no latent
variables. Therefore, the gradient takes the standard
form for loglinear models:
OLL = ?(a, pi, ?) ? (10)
?
a?,pi?,??
p(a?, pi?, ??|e, f)?(a?, pi?, ??)? ?w
The feature vector ? contains features on sure
word links, extracted phrase pairs, and null-aligned
words. Approximate expectations of these features
can be efficiently computed using the marginal be-
liefs baij (sure), bpighk`(true), and b?ei ([?1,?]) and
b?fj ([?1,?]), respectively. We learned our final
weight vectorw using AdaGrad (Duchi et al, 2010),
an adaptive subgradient version of standard stochas-
tic gradient ascent.
6.2 Testing
We evaluate our model by measuring precision and
recall on extracted phrase pairs. Thus, the decod-
ing problem takes a sentence pair (e, f) as input, and
must produce an extraction set pi as output. Our ap-
proach, posterior thresholding, is extremely simple:
we set pighk` = true iff bpighk`(true) ? ? for some
fixed threshold ? . Note that this decoding method
does not require that there be any underlying word
alignment a licensing the resulting extraction set pi,8
7See Section 7.2 for an empirical investigation of this maxi-
mum.
8This would be true even if we computed posteriors ex-
actly, but is especially true with approximate marginals from
BP, which are not necessarily consistent.
but the structure of the model is such that two con-
flicting phrase pairs are unlikely to simultaneously
have high posterior probability.
Most publicly available translation systems ex-
pect word-level alignments as input. These can
also be generated by applying posterior threshold-
ing, aligning target word i to source word j when-
ever baij (sure) ? t.9
7 Experiments
Our experiments are performed on Chinese-to-
English alignment. We trained and evaluated all
models on the NIST MT02 test set, which consists
of 150 training and 191 test sentences and has been
used previously in alignment experiments (Ayan and
Dorr, 2006; Haghighi et al, 2009; DeNero and
Klein, 2010). The unsupervised HMM word aligner
used to generate features for the model was trained
on 11.3 million words of FBIS newswire data. We
test three models: the Viterbi ITG model of DeNero
and Klein (2010), our BP ITG model that uses the
ITG factor, and our BP Relaxed model that replaces
the ITG factor with the ONESPAN factors. In all of
our experiments, the phrase length d was set to 3.10
7.1 Phrase Alignment
We tested the models by computing precision and
recall on extracted phrase pairs, relative to the gold
phrase pairs of up to length 3 induced by the gold
word alignments. For the BP models, we trade
off precision and recall by adjusting the decoding
threshold ? . The Viterbi ITG model was trained to
optimize F5, a recall-biased measure, so in addition
to F1, we also report the recall-biased F2 and F5
measures. The maximum number of BP iterations
was set to 5 for the BP ITG model and to 10 for the
BP Relaxed model.
The phrase alignment results are shown in Fig-
ure 4. The BP ITG model performs comparably to
the Viterbi ITG model. However, because posterior
decoding permits explicit tradeoffs between preci-
sion and recall, it can do much better in the recall-
biased measures, even though the Viterbi ITG model
was explicitly trained to maximize F5 (DeNero and
9For our experiments, we set t = 0.2.
10Because the runtime of the Viterbi ITG model grows expo-
nentially with d, it was not feasible to perform comparisons for
higher phrase lengths.
35
beta p r f2 0.69 0.742 0.7309823
60 
65 
70 
75 
80 
60 65 70 75 80 85 
Re
ca
ll 
Precision 
Viterbi ITG BP ITG BP Relaxed 
Model
Best Scores Sentences
F1 F2 F5 per Second
Viterbi ITG 71.6 73.1 74.0 0.21
BP ITG 71.8 74.8 83.5 0.11
BP Relaxed 72.6 75.2 84.5 1.15
Figure 4: Phrase alignment results. A portion of the Pre-
cision/Recall curve is plotted for the BP models, with the
result from the Viterbi ITG model provided for reference.
Klein, 2010). The BP Relaxed model performs the
best of all, consistently achieving higher recall for
fixed precision than either of the other models. Be-
cause of its lower asymptotic runtime, it is also much
faster: over 5 times as fast as the Viterbi ITG model
and over 10 times as fast as the BP ITG model.11
7.2 Timing
BP approximates marginal posteriors by iteratively
updating beliefs for each variable based on cur-
rent beliefs about other variables. The iterative na-
ture of the algorithm permits us to make an explicit
speed/accuracy tradeoff by limiting the number of
iterations. We tested this tradeoff by limiting both
of the BP models to run for 2, 3, 5, 10, and 20 iter-
ations. The results are shown in Figure 5. Neither
model benefits from running more iterations than
used to obtain the results in Figure 4, but each can
be sped up by a factor of almost 1.5x in exchange
for a modest (< 1 F1) drop in accuracy.
11The speed advantage of Viterbi ITG over BP ITG comes
from Viterbi ITG?s aggressive beaming.
Speed F12.08333333 61.32 67.61.58730159 71.91.14942529 72.60.96153846 72.6
67 
68 
69 
70 
71 
72 
73 
0.5 1 2 4 8 16 
Be
st 
F1
 
Time (seconds per sentence) 
Viterbi ITG BP ITG BP Relaxed 
67 
68 
69 
70 
71 
72 
73 
0.0625 0.125 0.25 0.5 1 2 
Be
st 
F1
 
Speed (sentences per second) 
Viterbi ITG BP ITG BP Relaxed 
Figure 5: Speed/accuracy tradeoff. The speed axis is on
a logarithmic scale. From fastest to slowest, data points
correspond to maximums of 2, 5, 10, and 20 BP itera-
tions. F1 for the BP Relaxed model was very low when
limited to 2 iterations, so that data point is outside the
visible area of the graph.
Model BLEU
Relative Hours to
Improve. Train/Align
Baseline 32.8 +0.0 5
Viterbi ITG 33.5 +0.7 831
BP Relaxed 33.6 +0.8 39
Table 2: Machine translation results.
7.3 Translation
We ran translation experiments using Moses (Koehn
et al, 2007), which we trained on a 22.1 mil-
lion word parallel corpus from the GALE program.
We compared alignments generated by the baseline
HMM model, the Viterbi ITG model and the Re-
laxed BP model.12 The systems were tuned and
evaluated on sentences up to length 40 from the
NIST MT04 and MT05 test sets. The results, shown
in Table 2, show that the BP Relaxed model achives
a 0.8 BLEU improvement over the HMM baseline,
comparable to that of the Viterbi ITG model, but tak-
ing a fraction of the time,13 making the BP Relaxed
model a practical alternative for real translation ap-
plications.
12Following a simplified version of the procedure described
by DeNero and Klein (2010), we added rule counts from the
HMM alignments to the extraction set algners? counts.
13Some of the speed difference between the BP Relaxed and
Viterbi ITG models comes from better parallelizability due to
drastically reduced memory overhead of the BP Relaxed model.
36
8 Conclusion
For performing inference in a state-of-the-art, but in-
efficient, alignment model, belief propagation is a
viable alternative to greedy search methods, such as
beaming. BP also results in models that are much
more scalable, by reducing the asymptotic complex-
ity of inference. Perhaps most importantly, BP per-
mits the relaxation of artificial constraints that are
generally taken for granted as being necessary for
efficient inference. In particular, a relatively mod-
est relaxation of the ITG constraint can directly be
applied to any model that uses ITG-based inference
(e.g. Zhang and Gildea, 2005; Cherry and Lin, 2007;
Haghighi et al, 2009).
Acknowledgements
This project is funded by an NSF graduate research
fellowship to the first author and by BBN under
DARPA contract HR0011-06-C-0022.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In ACL.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In AMTA.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL-IJCNLP.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
NAACL Workshop on Syntax and Structure in Statisti-
cal Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In EMNLP-CoNLL.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
ACL.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In EMNLP.
John DeNero. 2010. Personal Communication.
John Duchi, Danny Tarlow, Gal Elidan, and Daphne
Koller. 2007. Using combinatorial optimization
within max-product belief propagation. In NIPS 2006.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learning and
stochastic optimization. In COLT.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-
NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
COLING-ACL.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In ACL-IJCNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
SSST.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL.
David J.C. MacKay. 2003. Information theory, infer-
ence, and learning algorithms. Cambridge Univ Press.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Jan Niehues and Stephan Vogel. 2008. Discriminative
word alignment via alignment matrix modeling. In
ACL Workshop on Statistical Machine Translation.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005.
A discriminative matching approach to word align-
ment. In EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL.
37
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL:HLT.
38
T5: Variational Inference for Structured NLP 
Models 
David Burkett, Dan Klein 
ABSTRACT
Historically, key breakthroughs in structured NLP models, such as chain CRFs or 
PCFGs, have relied on imposing careful constraints on the locality of features in order to 
permit efficient dynamic programming for computing expectations or finding the highest-
scoring structures. However, as modern structured models become more complex and 
seek to incorporate longer-range features, it is more and more often the case that 
performing exact inference is impossible (or at least impractical) and it is necessary to 
resort to some sort of approximation technique, such as beam search, pruning, or 
sampling. In the NLP community, one increasingly popular approach is the use of 
variational methods for computing approximate distributions. 
The goal of the tutorial is to provide an introduction to variational methods for 
approximate inference, particularly mean field approximation and belief propagation. 
The intuition behind the mathematical derivation of variational methods is fairly simple: 
instead of trying to directly compute the distribution of interest, first consider some 
efficiently computable approximation of the original inference problem, then find the 
solution of the approximate inference problem that minimizes the distance to the true 
distribution. Though the full derivations can be somewhat tedious, the resulting 
procedures are quite straightforward, and typically consist of an iterative process of 
individually updating specific components of the model, conditioned on the rest. 
Although we will provide some theoretical background, the main goal of the tutorial is to 
provide a concrete procedural guide to using these approximate inference techniques, 
illustrated with detailed walkthroughs of examples from recent NLP literature. 
Once both variational inference procedures have been described in detail, we'll provide 
a summary comparison of the two, along with some intuition about which approach is 
appropriate when. We'll also provide a guide to further exploration of the topic, briefly 
discussing other variational techniques, such as expectation propagation and convex 
relaxations, but concentrating mainly on providing pointers to additional resources for 
those who wish to learn more. 
OUTLINE
1. Introduction 
1. Approximate inference background 
2. Definition of variational inference 
3. Structured NLP problem setting, loglinear models 
4. Graphical model notation, feature locality 
2. Mean Field Approximation 
1. General description and theoretical background 
2. Derivation of updates for simple two-variable model 
3. Structured mean field: extension to joint CRFs 
4. Joint parsing and word alignment 
5. High level description of other models (Coref, Nonparametric Bayes) 
3. Belief Propagation 
1. General description and theoretical background 
2. Factor graph notation 
3. Formulas for messages and beliefs, with joint CRF example 
4. Dependency parsing 
5. Word alignment 
4. Wrap-up 
1. Mean Field vs Belief Propagation (i.e. what to use when) 
2. Other variational methods 
3. Additional resources 
BIOS
David Burkett
University of California, Berkeley 
dburkett--AT--cs.berkeley.edu 
David Burkett is a Ph.D. candidate in the Computer Science Division at the University of 
California, Berkeley. The main focus of his research is on modeling syntactic agreement 
in bilingual corpora. His interests are diverse, though, and he has worked on parsing, 
phrase alignment, language evolution, coreference resolution, and even video game AI. 
He has worked as an instructional assistant for multiple AI courses at Berkeley and won 
the Outstanding Graduate Student Instructor award in 2009. 
Dan Klein
University of California, Berkeley 
klein--AT--cs.berkeley.edu 
Dan Klein is an Associate Professor of Computer Science at the University of California, 
Berkeley. His research includes many areas of statistical natural language processing, 
including grammar induction, parsing, machine translation, information extraction, 
document summarization, historical linguistics, and speech recognition. His academic 
awards include a Sloan Fellowship, a Microsoft Faculty Fellowship, an NSF CAREER 
Award, the ACM Grace Murray Hopper Award, Best Paper Awards at ACL, EMNLP and 
NAACL, and the UC Berkeley Distinguished Teaching Award. 
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1030?1039,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Finding Cognate Groups using Phylogenies
David Hall and Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,klein}@cs.berkeley.edu
Abstract
A central problem in historical linguistics
is the identification of historically related
cognate words. We present a generative
phylogenetic model for automatically in-
ducing cognate group structure from un-
aligned word lists. Our model represents
the process of transformation and trans-
mission from ancestor word to daughter
word, as well as the alignment between
the words lists of the observed languages.
We also present a novel method for sim-
plifying complex weighted automata cre-
ated during inference to counteract the
otherwise exponential growth of message
sizes. On the task of identifying cognates
in a dataset of Romance words, our model
significantly outperforms a baseline ap-
proach, increasing accuracy by as much as
80%. Finally, we demonstrate that our au-
tomatically induced groups can be used to
successfully reconstruct ancestral words.
1 Introduction
A crowning achievement of historical linguistics
is the comparative method (Ohala, 1993), wherein
linguists use word similarity to elucidate the hid-
den phonological and morphological processes
which govern historical descent. The comparative
method requires reasoning about three important
hidden variables: the overall phylogenetic guide
tree among languages, the evolutionary parame-
ters of the ambient changes at each branch, and
the cognate group structure that specifies which
words share common ancestors.
All three of these variables interact and inform
each other, and so historical linguists often con-
sider them jointly. However, linguists are cur-
rently required to make qualitative judgments re-
garding the relative likelihood of certain sound
changes, cognate groups, and so on. Several re-
cent statistical methods have been introduced to
provide increased quantitative backing to the com-
parative method (Oakes, 2000; Bouchard-Co?te? et
al., 2007; Bouchard-Co?te? et al, 2009); others have
modeled the spread of language changes and spe-
ciation (Ringe et al, 2002; Daume? III and Camp-
bell, 2007; Daume? III, 2009; Nerbonne, 2010).
These automated methods, while providing ro-
bustness and scale in the induction of ancestral
word forms and evolutionary parameters, assume
that cognate groups are already known. In this
work, we address this limitation, presenting a
model in which cognate groups can be discovered
automatically.
Finding cognate groups is not an easy task,
because underlying morphological and phonolog-
ical changes can obscure relationships between
words, especially for distant cognates, where sim-
ple string overlap is an inadequate measure of sim-
ilarity. Indeed, a standard string similarity met-
ric like Levenshtein distance can lead to false
positives. Consider the often cited example of
Greek /ma:ti/ and Malay /mata/, both meaning
?eye? (Bloomfield, 1938). If we were to rely on
Levenshtein distance, these words would seem to
be a highly attractive match as cognates: they are
nearly identical, essentially differing in only a sin-
gle character. However, no linguist would posit
that these two words are related. To correctly learn
that they are not related, linguists typically rely
on two kinds of evidence. First, because sound
change is largely regular, we would need to com-
monly see /i/ in Greek wherever we see /a/ in
Malay (Ross, 1950). Second, we should look at
languages closely related to Greek and Malay, to
see if similar patterns hold there, too.
Some authors have attempted to automatically
detect cognate words (Mann and Yarowsky, 2001;
Lowe and Mazaudon, 1994; Oakes, 2000; Kon-
drak, 2001; Mulloni, 2007), but these methods
1030
typically work on language pairs rather than on
larger language families. To fully automate the
comparative method, it is necessary to consider
multiple languages, and to do so in a model which
couples cognate detection with similarity learning.
In this paper, we present a new generative model
for the automatic induction of cognate groups
given only (1) a known family tree of languages
and (2) word lists from those languages. A prior
on word survival generates a number of cognate
groups and decides which groups are attested in
each modern language. An evolutionary model
captures how each word is generated from its par-
ent word. Finally, an alignment model maps the
flat word lists to cognate groups. Inference re-
quires a combination of message-passing in the
evolutionary model and iterative bipartite graph
matching in the alignment model.
In the message-passing phase, our model en-
codes distributions over strings as weighted finite
state automata (Mohri, 2009). Weighted automata
have been successfully applied to speech process-
ing (Mohri et al, 1996) and more recently to mor-
phology (Dreyer and Eisner, 2009). Here, we
present a new method for automatically compress-
ing our message automata in a way that can take
into account prior information about the expected
outcome of inference.
In this paper, we focus on a transcribed word
list of 583 cognate sets from three Romance lan-
guages (Portuguese, Italian and Spanish), as well
as their common ancestor Latin (Bouchard-Co?te?
et al, 2007). We consider both the case where
we know that all cognate groups have a surface
form in all languages, and where we do not know
that. On the former, easier task we achieve iden-
tification accuracies of 90.6%. On the latter task,
we achieve F1 scores of 73.6%. Both substantially
beat baseline performance.
2 Model
In this section, we describe a new generative
model for vocabulary lists in multiple related lan-
guages given the phylogenetic relationship be-
tween the languages (their family tree). The gener-
ative process factors into three subprocesses: sur-
vival, evolution, and alignment, as shown in Fig-
ure 1(a). Survival dictates, for each cognate group,
which languages have words in that group. Evo-
lution describes the process by which daughter
words are transformed from their parent word. Fi-
nally, alignment describes the ?scrambling? of the
word lists into a flat order that hides their lineage.
We present each subprocess in detail in the follow-
ing subsections.
2.1 Survival
First, we choose a number G of ancestral cognate
groups from a geometric distribution. For each
cognate group g, our generative process walks
down the tree. At each branch, the word may ei-
ther survive or die. This process is modeled in a
?death tree? with a Bernoulli random variable S`g
for each language ` and cognate group g specify-
ing whether or not the word died before reaching
that language. Death at any node in the tree causes
all of that node?s descendants to also be dead. This
process captures the intuition that cognate words
are more likely to be found clustered in sibling lan-
guages than scattered across unrelated languages.
2.2 Evolution
Once we know which languages will have an at-
tested word and which will not, we generate the
actual word forms. The evolution component of
the model generates words according to a branch-
specific transformation from a node?s immediate
ancestor. Figure 1(a) graphically describes our
generative model for three Romance languages:
Italian, Portuguese, and Spanish.1 In each cog-
nate group, each word W` is generated from its
parent according to a conditional distribution with
parameter ?`, which is specific to that edge in the
tree, but shared between all cognate groups.
In this paper, each ?` takes the form of a pa-
rameterized edit distance similar to the standard
Levenshtein distance. Richer models ? such as the
ones in Bouchard-Co?te? et al (2007) ? could in-
stead be used, although with an increased infer-
ential cost. The edit transducers are represented
schematically in Figure 1(b). Characters x and
y are arbitrary phonemes, and ?(x, y) represents
the cost of substituting x with y. ? represents the
empty phoneme and is used as shorthand for inser-
tion and deletion, which have parameters ? and ?,
respectively.
As an example, see the illustration in Fig-
ure 1(c). Here, the Italian word /fwOko/ (?fire?) is
generated from its parent form /fokus/ (?hearth?)
1Though we have data for Latin, we treat it as unobserved
to represent the more common case where the ancestral lan-
guage is unattested; we also evaluate our system using the
Latin data.
1031
GW
VL
W
PI
?
?
? ?
?
W
LA
?
S
LA
S
VL
S
PI
S
IT
S
ES
S
PT
L 
L 
w
pt
w
es
L 
?
w
IT
w
IT
w
IT
w
IT
w
IT
w
IT
W
IT
W
IT
S
u
r
v
i
v
a
l
E
v
o
l
u
t
i
o
n
f u sk
f w
? 
ok
A
l
i
g
n
m
e
n
t
(a)
(b)
(c)
x
:
y
 
/
 
?
(
x
,
y
)
x
:
?
/
?
x
 
 
 
 
?
:
y
/
?
y
o
Figure 1: (a) The process by which cognate words are generated. Here, we show the derivation of Romance language words
W` from their respective Latin ancestor, parameterized by transformations ?` and survival variables S`. Languages shown
are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern
language words are observed (shaded). (b) The class of parameterized edit distances used in this paper. Each pair of phonemes
has a weight ? for deletion, and each phoneme has weights ? and ? for insertion and deletion respectively. (c) A possible
alignment produced by an edit distance between the Latin word focus (?hearth?) and the Italian word fuoco (?fire?).
by a series of edits: two matches, two substitu-
tions (/u/? /o/, and /o/?/O/), one insertion (w)
and one deletion (/s/). The probability of each
individual edit is determined by ?. Note that the
marginal probability of a specific Italian word con-
ditioned on its Vulgar Latin parent is the sum over
all possible derivations that generate it.
2.3 Alignment
Finally, at the leaves of the trees are the observed
words. (We take non-leaf nodes to be unobserved.)
Here, we make the simplifying assumption that in
any language there is at most one word per lan-
guage per cognate group. Because the assign-
ments of words to cognates is unknown, we spec-
ify an unknown alignment parameter pi` for each
modern language which is an alignment of cognate
groups to entries in the word list. In the case that
every cognate group has a word in each language,
each pi` is a permutation. In the more general case
that some cognate groups do not have words from
all languages, this mapping is injective from words
to cognate groups. From a generative perspective,
pi` generates observed positions of the words in
some vocabulary list.
In this paper, our task is primarily to learn the
alignment variables pi`. All other hidden variables
are auxiliary and are to be marginalized to the
greatest extent possible.
3 Inference of Cognate Assignments
In this section, we discuss the inference method
for determining cognate assignments under fixed
parameters ?. We are given a set of languages and
a list of words in each language, and our objec-
tive is to determine which words are cognate with
each other. Because the parameters pi` are either
permutations or injections, the inference task is re-
duced to finding an alignment pi of the respective
word lists to maximize the log probability of the
observed words.
pi? = arg max
pi
?
g
log p(w(`,pi`(g))|?, pi,w?`)
w(`,pi`(g)) is the word in language ` that pi` has
assigned to cognate group g. Maximizing this
quantity directly is intractable, and so instead we
use a coordinate ascent algorithm to iteratively
1032
maximize the alignment corresponding to a
single language ` while holding the others fixed:
pi?` = arg max
pi`
?
g
log p(w(`,pi`(g))|?, pi?`, pi`,w?`)
Each iteration is then actually an instance of
bipartite graph matching, with the words in one
language one set of nodes, and the current cognate
groups in the other languages the other set of
nodes. The edge affinities aff between these
nodes are the conditional probabilities of each
word w` belonging to each cognate group g:
aff(w`, g) = p(w`|w?`,pi?`(g), ?, pi?`)
To compute these affinities, we perform in-
ference in each tree to calculate the marginal
distribution of the words from the language `.
For the marginals, we use an analog of the for-
ward/backward algorithm. In the upward pass, we
send messages from the leaves of the tree toward
the root. For observed leaf nodes Wd, we have:
?d?a(wa) = p(Wd = wd|wa, ?d)
and for interior nodes Wi:
?i?a(wa) =
?
wi
p(wi|wa, ?i)
?
d?child(wi)
?d?i(wi)
(1)
In the downward pass (toward the lan-
guage `), we sum over ancestral words Wa:
?a?d(wd)
=
?
wa
p(wd|wa, ?d)?a??a(wa)
?
d??child(wa)
d? 6=d
?d??a(wa)
where a? is the ancestor of a. Computing these
messages gives a posterior marginal distribution
?`(w`) = p(w`|w?`,pi?`(g), ?, pi?`), which is pre-
cisely the affinity score we need for the bipartite
matching. We then use the Hungarian algorithm
(Kuhn, 1955) to find the optimal assignment for
the bipartite matching problem.
One important final note is initialization. In our
early experiments we found that choosing a ran-
dom starting configuration unsurprisingly led to
rather poor local optima. Instead, we started with
empty trees, and added in one language per itera-
tion until all languages were added, and then con-
tinued iterations on the full tree.
4 Learning
So far we have only addressed searching for
Viterbi alignments pi under fixed parameters. In
practice, it is important to estimate better para-
metric edit distances ?` and survival variables
S`. To motivate the need for good transducers,
consider the example of English ?day? /deI/ and
Latin ?die?s? /dIe:s/, both with the same mean-
ing. Surprisingly, these words are in no way re-
lated, with English ?day? probably coming from a
verb meaning ?to burn? (OED, 1989). However,
a naively constructed edit distance, which for ex-
ample might penalize vowel substitutions lightly,
would fail to learn that Latin words that are bor-
rowed into English would not undergo the sound
change /I/?/eI/. Therefore, our model must learn
not only which sound changes are plausible (e.g.
vowels turning into other vowels is more common
than vowels turning into consonants), but which
changes are appropriate for a given language.2
At a high level, our learning algorithm is much
like Expectation Maximization with hard assign-
ments: after we update the alignment variables pi
and thus form new potential cognate sets, we re-
estimate our model?s parameters to maximize the
likelihood of those assignments.3 The parameters
can be learned through standard maximum likeli-
hood estimation, which we detail in this section.
Because we enforce that a word in language d
must be dead if its parent word in language a is
dead, we just need to learn the conditional prob-
abilities p(Sd = dead|Sa = alive). Given fixed
assignments pi, the maximum likelihood estimate
can be found by counting the number of ?deaths?
that occurred between a child and a live parent,
applying smoothing ? we found adding 0.5 to be
reasonable ? and dividing by the total number of
live parents.
For the transducers ?, we learn parameterized
edit distances that model the probabilities of dif-
ferent sound changes. For each ?` we fit a non-
uniform substitution, insertion, and deletion ma-
trix ?(x, y). These edit distances define a condi-
2We note two further difficulties: our model does not han-
dle ?borrowings,? which would be necessary to capture a
significant portion of English vocabulary; nor can it seam-
lessly handle words that are inherited later in the evolution of
language than others. For instance, French borrowed words
from its parent language Latin during the Renaissance and
the Enlightenment that have not undergone the same changes
as words that evolved ?naturally? from Latin. See Bloom-
field (1938). Handling these cases is a direction for future
research.
3Strictly, we can cast this problem in a variational frame-
work similar to mean field where we iteratively maximize pa-
rameters to minimize a KL-divergence. We omit details for
clarity.
1033
tional exponential family distribution when condi-
tioned on an ancestral word. That is, for any fixed
wa:
?
wd
p(wd|wa, ?) =
?
wd
?
z?
align(wa,wd)
score(z;?)
=
?
wd
?
z?
align(wa,wd)
?
(x,y)?z
?(x, y) = 1
where align(wa, wd) is the set of possible align-
ments between the phonemes in words wa and wd.
We are seeking the maximum likelihood esti-
mate of each ?, given fixed alignments pi:
??` = arg max
?`
p(w|?, pi)
To find this maximizer for any given pi`, we
need to find a marginal distribution over the
edges connecting any two languages a and
d. With this distribution, we calculate the
expected ?alignment unigrams.? That is, for
each pair of phonemes x and y (or empty
phoneme ?), we need to find the quantity:
Ep(wa,wd)[#(x, y; z)] =
?
wa,wd
?
z?
align(wa,wd)
#(x,y; z)p(z|wa, wd)p(wa, wd)
where we denote #(x, y; z) to be the num-
ber of times the pair of phonemes (x, y) are
aligned in alignment z. The exact method for
computing these counts is to use an expectation
semiring (Eisner, 2001).
Given the expected counts, we now need to nor-
malize them to ensure that the transducer repre-
sents a conditional probability distribution (Eis-
ner, 2002; Oncina and Sebban, 2006). We have
that, for each phoneme x in the ancestor language:
?y =
E[#(?, y; z)]
E[#(?, ?; z)]
?(x, y) = (1?
?
y?
?y?)
E[#(x, y; z)]
E[#(x, ?; z)]
?x = (1?
?
y?
?y?)
E[#(x, ?; z)]
E[#(x, ?; z)]
Here, we have #(?, ?; z) =
?
x,y #(x, y; z) and
#(x, ?; z) =
?
y #(x, y; z). The (1 ?
?
y? ?y?)
term ensure that for any ancestral phoneme x,
?
y ?y+
?
y ?(x, y)+?x = 1. These equations en-
sure that the three transition types (insertion, sub-
stitution/match, deletion) are normalized for each
ancestral phoneme.
5 Transducers and Automata
In our model, it is not just the edit distances
that are finite state machines. Indeed, the words
themselves are string-valued random variables that
have, in principle, an infinite domain. To represent
distributions and messages over these variables,
we chose weighted finite state automata, which
can compactly represent functions over strings.
Unfortunately, while initially compact, these au-
tomata become unwieldy during inference, and so
approximations must be used (Dreyer and Eisner,
2009). In this section, we summarize the standard
algorithms and representations used for weighted
finite state transducers. For more detailed treat-
ment of the general transducer operations, we di-
rect readers to Mohri (2009).
A weighted automaton (resp. transducer) en-
codes a function over strings (resp. pairs of
strings) as weighted paths through a directed
graph. Each edge in the graph has a real-valued
weight4 and a label, which is a single phoneme
in some alphabet ? or the empty phoneme ? (resp.
pair of labels in some alphabet ???). The weight
of a string is then the sum of all paths through the
graph that accept that string.
For our purposes, we are concerned with three
fundamental operations on weighted transducers.
The first is computing the sum of all paths through
a transducer, which corresponds to computing the
partition function of a distribution over strings.
This operation can be performed in worst-case
cubic time (using a generalization of the Floyd-
Warshall algorithm). For acyclic or feed-forward
transducers, this time can be improved dramati-
cally by using a generalization of Djisktra?s algo-
rithm or other related algorithms (Mohri, 2009).
The second operation is the composition of two
transducers. Intuitively, composition creates a new
transducer that takes the output from the first trans-
ducer, processes it through the second transducer,
and then returns the output of the second trans-
ducer. That is, consider two transducers T1 and
T2. T1 has input alphabet ? and output alpha-
bet ?, while T2 has input alphabet ? and out-
put alphabet ?. The composition T1 ? T2 returns
a new transducer over ? and ? such that (T1 ?
T2)(x, y) =
?
u T1(x, u) ? T2(u, y). In this paper,
we use composition for marginalization and fac-
tor products. Given a factor f1(x, u;T1) and an-
4The weights can be anything that form a semiring, but for
the sake of exposition we specialize to real-valued weights.
1034
other factor f2(u, y;T2), composition corresponds
to the operation ?(x, y) =
?
u f1(x, u)f2(u, y).
For two messages ?1(w) and ?2(w), the same al-
gorithm can be used to find the product ?(w) =
?1(w)?2(w).
The third operation is transducer minimization.
Transducer composition produces O(nm) states,
where n and m are the number of states in each
transducer. Repeated compositions compound the
problem: iterated composition of k transducers
produces O(nk) states. Minimization alleviates
this problem by collapsing indistinguishable states
into a single state. Unfortunately, minimization
does not always collapse enough states. In the next
section we discuss approaches to ?lossy? mini-
mization that produce automata that are not ex-
actly the same but are much smaller.
6 Message Approximation
Recall that in inference, when summing out in-
terior nodes wi we calculated the product over
incoming messages ?d?i(wi) (Equation 1), and
that these products are calculated using transducer
composition. Unfortunately, the maximal number
of states in a message is exponential in the num-
ber of words in the cognate group. Minimization
can only help so much: in order for two states to
be collapsed, the distribution over transitions from
those states must be indistinguishable. In practice,
for the automata generated in our model, mini-
mization removes at most half the states, which is
not sufficient to counteract the exponential growth.
Thus, we need to find a way to approximate a mes-
sage ?(w) using a simpler automata ??(w; ?) taken
from a restricted class parameterized by ?.
In the context of transducers, previous authors
have focused on a combination of n-best lists
and unigram back-off models (Dreyer and Eis-
ner, 2009), a schematic diagram of which is in
Figure 2(d). For their problem, n-best lists are
sensible: their nodes? local potentials already fo-
cus messages on a small number of hypotheses.
In our setting, however, n-best lists are problem-
atic; early experiments showed that a 10,000-best
list for a typical message only accounts for 50%
of message log perplexity. That is, the posterior
marginals in our model are (at least initially) fairly
flat.
An alternative approach might be to simply
treat messages as unnormalized probability distri-
butions, and to minimize the KL divergence be-
e
 
g
 
u
 
f
 
e
 
o
 
 
 
f
u
u
  u
e
u
g
u
o
u
  
    f
f
f
f
e
e
e
e
e
g
g
g
    g
g
o
o
o
o
o
 f
2 3
e
u
g
o
f
0 1
f
e
o
4
g
o
e
u
f
u
e
o
f
g
5
o
g
u
f
f
u e g o
f
eu g o
f
e u g
f
e e
f
u
e
g
  g
(a)
(b)
(c)
(d)
u
g
o
  e
  u
  f
  o
Figure 2: Various topologies for approximating topologies:
(a) a unigram model, (b) a bigram model, (c) the anchored
unigram model, and (d) the n-best plus backoff model used in
Dreyer and Eisner (2009). In (c) and (d), the relative height
of arcs is meant to convey approximate probabilities.
tween some approximating message ??(w) and the
true message ?(w). However, messages are not
always probability distributions and ? because the
number of possible strings is in principle infinite ?
they need not sum to a finite number.5 Instead, we
propose to minimize the KL divergence between
the ?expected? marginal distribution and the ap-
proximated ?expected? marginal distribution:
?? = arg min
?
DKL(?(w)?(w)||?(w)??(w; ?))
= arg min
?
?
w
?(w)?(w) log
?(w)?(w)
?(w)??(w; ?)
= arg min
?
?
w
?(w)?(w) log
?(w)
??(w; ?)
(2)
where ? is a term acting as a surrogate for the pos-
terior distribution over w without the information
from ?. That is, we seek to approximate ? not on
its own, but as it functions in an environment rep-
resenting its final context. For example, if ?(w) is
a backward message, ? could be a stand-in for a
forward probability.6
In this paper, ?(w) is a complex automaton with
potentially many states, ??(w; ?) is a simple para-
metric automaton with forms that we discuss be-
low, and ?(w) is an arbitrary (but hopefully fairly
simple) automaton. The actual method we use is
5As an extreme example, suppose we have observed that
Wd = wd and that p(Wd = wd|wa) = 1 for all ancestral
words wa. Then, clearly
P
wd
?(wd) =
P
wd
P
p(Wd =
wd|wa) = ? whenever there are an infinite number of pos-
sible ancestral strings wa.
6This approach is reminiscent of Expectation Propaga-
tion (Minka, 2001).
1035
as follows. Given a deterministic prior automa-
ton ? , and a deterministic automaton topology ???,
we create the composed unweighted automaton
? ????, and calculate arc transitions weights to min-
imize the KL divergence between that composed
transducer and ? ? ?. The procedure for calcu-
lating these statistics is described in Li and Eis-
ner (2009), which amounts to using an expectation
semiring (Eisner, 2001) to compute expected tran-
sitions in ? ? ??? under the probability distribution
? ? ?.
From there, we need to create the automaton
??1 ? ? ? ??. That is, we need to divide out the
influence of ?(w). Since we know the topology
and arc weights for ? ahead of time, this is often
as simple as dividing arc weights in ? ? ?? by the
corresponding arc weight in ?(w). For example,
if ? encodes a geometric distribution over word
lengths and a uniform distribution over phonemes
(that is, ?(w) ? p|w|), then computing ?? is as sim-
ple as dividing each arc in ? ? ?? by p.7
There are a number of choices for ? . One is a
hard maximum on the length of words. Another is
to choose ?(w) to be a unigram language model
over the language in question with a geometric
probability over lengths. In our experiments, we
find that ?(w) can be a geometric distribution over
lengths with a uniform distribution over phonemes
and still give reasonable results. This distribution
captures the importance of shorter strings while
still maintaining a relatively weak prior.
What remains is the selection of the topologies
for the approximating message ??. We consider
three possible approximations, illustrated in Fig-
ure 2. The first is a plain unigram model, the
second is a bigram model, and the third is an an-
chored unigram topology: a position-specific un-
igram model for each position up to some maxi-
mum length.
The first we consider is a standard unigram
model, which is illustrated in Figure 2(a). It
has |?| + 2 parameters: one weight ?a for each
phoneme a ? ?, a starting weight ?, and a stop-
ping probability ?. ?? then has the form:
??(w) = ??
?
i?|w|
?wi
Estimating this model involves only computing
the expected count of each phoneme, along with
7Also, we must be sure to divide each final weight in the
transducer by (1 ? |?|p), which is the stopping probability
for a geometric transducer.
the expected length of a word, E[|w|]. We then
normalize the counts according to the maximum
likelihood estimate, with arc weights set as:
?a ? E[#(a)]
Recall that these expectations can be computed us-
ing an expectation semiring.
Finally, ? can be computed by ensuring that the
approximate and exact expected marginals have
the same partition function. That is, with the other
parameters fixed, solve:
?
w
?(w)??(w) =
?
w
?(w)?(w)
which amounts to rescaling ?? by some constant.
The second topology we consider is the bigram
topology, illustrated in Figure 2(b). It is similar
to the unigram topology except that, instead of
a single state, we have a state for each phoneme
in ?, along with a special start state. Each state
a has transitions with weights ?b|a = p(b|a) ?
E[#(b|a)]. Normalization is similar to the un-
igram case, except that we normalize the transi-
tions from each state.
The final topology we consider is the positional
unigram model in Figure 2(c). This topology takes
positional information into account. Namely, for
each position (up to some maximum position), we
have a unigram model over phonemes emitted at
that position, along with the probability of stop-
ping at that position (i.e. a ?sausage lattice?). Es-
timating the parameters of this model is similar,
except that the expected counts for the phonemes
in the alphabet are conditioned on their position in
the string. With the expected counts for each posi-
tion, we normalize each state?s final and outgoing
weights. In our experiments, we set the maximum
length to seven more than the length of the longest
observed string.
7 Experiments
We conduct three experiments. The first is a ?com-
plete data? experiment, in which we reconstitute
the cognate groups from the Romance data set,
where all cognate groups have words in all three
languages. This task highlights the evolution and
alignment models. The second is a much harder
?partial data? experiment, in which we randomly
prune 20% of the branches from the dataset ac-
cording to the survival process described in Sec-
tion 2.1. Here, only a fraction of words appear
1036
in any cognate group, so this task crucially in-
volves the survival model. The ultimate purpose
of the induced cognate groups is to feed richer
evolutionary models, such as full reconstruction
models. Therefore, we also consider a proto-word
reconstruction experiment. For this experiment,
using the system of Bouchard-Co?te? et al (2009),
we compare the reconstructions produced from
our automatic groups to those produced from gold
cognate groups.
7.1 Baseline
As a novel but heuristic baseline for cognate group
detection, we use an iterative bipartite matching
algorithm where instead of conditional likelihoods
for affinities we use Dice?s coefficient, defined for
sets X and Y as:
Dice(X,Y ) =
2|X ? Y |
|X|+ |Y |
(3)
Dice?s coefficients are commonly used in bilingual
detection of cognates (Kondrak, 2001; Kondrak et
al., 2003). We follow prior work and use sets of
bigrams within words. In our case, during bipar-
tite matching the set X is the set of bigrams in the
language being re-permuted, and Y is the union of
bigrams in the other languages.
7.2 Experiment 1: Complete Data
In this experiment, we know precisely how many
cognate groups there are and that every cognate
group has a word in each language. While this
scenario does not include all of the features of the
real-world task, it represents a good test case of
how well these models can perform without the
non-parametric task of deciding how many clus-
ters to use.
We scrambled the 583 cognate groups in the
Romance dataset and ran each method to conver-
gence. Besides the heuristic baseline, we tried our
model-based approach using Unigrams, Bigrams
and Anchored Unigrams, with and without learn-
ing the parametric edit distances. When we did not
use learning, we set the parameters of the edit dis-
tance to (0, -3, -4) for matches, substitutions, and
deletions/insertions, respectively. With learning
enabled, transducers were initialized with those
parameters.
For evaluation, we report two metrics. The first
is pairwise accuracy for each pair of languages,
averaged across pairs of words. The other is accu-
Pairwise Exact
Acc. Match
Heuristic
Baseline 48.1 35.4
Model
Transducers Messages
Levenshtein Unigrams 37.2 26.2
Levenshtein Bigrams 43.0 26.5
Levenshtein Anch. Unigrams 68.6 56.8
Learned Unigrams 0.1 0.0
Learned Bigrams 38.7 11.3
Learned Anch. Unigrams 90.3 86.6
Table 1: Accuracies for reconstructing cognate groups. Lev-
enshtein refers to fixed parameter edit distance transducer.
Learned refers to automatically learned edit distances. Pair-
wise Accuracy means averaged on each word pair; Exact
Match refers to percentage of completely and accurately re-
constructed groups. For a description of the baseline, see Sec-
tion 7.1.
Prec. Recall F1
Heuristic
Baseline 49.0 43.5 46.1
Model
Transducers Messages
Levenshtein Anch. Unigrams 86.5 36.1 50.9
Learned Anch. Unigrams 66.9 82.0 73.6
Table 2: Accuracies for reconstructing incomplete groups.
Scores reported are precision, recall, and F1, averaged over
all word pairs.
racy measured in terms of the number of correctly,
completely reconstructed cognate groups.
Table 1 shows the results under various config-
urations. As can be seen, the kind of approxima-
tion used matters immensely. In this application,
positional information is important, more so than
the context of the previous phoneme. Both Un-
igrams and Bigrams significantly under-perform
the baseline, while Anchored Unigrams easily out-
performs it both with and without learning.
An initially surprising result is that learning ac-
tually harms performance under the unanchored
approximations. The explanation is that these
topologies are not sensitive enough to context, and
that the learning procedure ends up flattening the
distributions. In the case of unigrams ? which have
the least context ? learning degrades performance
to chance. However, in the case of positional uni-
grams, learning reduces the error rate by more than
two-thirds.
7.3 Experiment 2: Incomplete Data
As a more realistic scenario, we consider the case
where we do not know that all cognate groups have
words in all languages. To test our model, we ran-
1037
domly pruned 20% of the branches according the
survival process of our model.8
Because only Anchored Unigrams performed
well in Experiment 1, we consider only it and the
Dice?s coefficient baseline. The baseline needs to
be augmented to support the fact that some words
may not appear in all cognate groups. To do this,
we thresholded the bipartite matching process so
that if the coefficient fell below some value, we
started a new group for that word. We experi-
mented on 10 values in the range (0,1) for the
baseline?s threshold and report on the one (0.2)
that gives the best pairwise F1.
The results are in Table 2. Here again, we see
that the positional unigrams perform much better
than the baseline system. The learned transduc-
ers seem to sacrifice precision for the sake of in-
creased recall. This makes sense because the de-
fault edit distance parameter settings strongly fa-
vor exact matches, while the learned transducers
learn more realistic substitution and deletion ma-
trices, at the expense of making more mistakes.
For example, the learned transducers enable
our model to correctly infer that Portuguese
/d1femdu/, Spanish /defiendo/, and Italian
/difEndo/ are all derived from Latin /de:fendo:/
?defend.? Using the simple Levenshtein transduc-
ers, on the other hand, our model keeps all three
separated, because the transducers cannot know ?
among other things ? that Portuguese /1/, Span-
ish /e/, and Italian /i/ are commonly substituted
for one another. Unfortunately, because the trans-
ducers used cannot learn contextual rules, cer-
tain transformations can be over-applied. For in-
stance, Spanish /nombRar/ ?name? is grouped to-
gether with Portuguese /num1RaR/ ?number? and
Italian /numerare/ ?number,? largely because the
rule Portuguese /u/? Spanish /o/ is applied out-
side of its normal context. This sound change oc-
curs primarily with final vowels, and does not usu-
ally occur word medially. Thus, more sophisti-
cated transducers could learn better sound laws,
which could translate into improved accuracy.
7.4 Experiment 3: Reconstructions
As a final trial, we wanted to see how each au-
tomatically found cognate group faired as com-
pared to the ?true groups? for actual reconstruc-
tion of proto-words. Our model is not optimized
8This dataset will be made available at
http://nlp.cs.berkeley.edu/Main.html#Historical
for faithful reconstruction, and so we used the An-
cestry Resampling system of Bouchard-Co?te? et al
(2009). To evaluate, we matched each Latin word
with the best possible cognate group for that word.
The process for the matching was as follows. If
two or three of the words in an constructed cognate
group agreed, we assigned the Latin word associ-
ated with the true group to it. With the remainder,
we executed a bipartite matching based on bigram
overlap.
For evaluation, we examined the Levenshtein
distance between the reconstructed word and the
chosen Latin word. As a kind of ?skyline,?
we compare to the edit distances reported in
Bouchard-Co?te? et al (2009), which was based on
complete knowledge of the cognate groups. On
this task, our reconstructed cognate groups had
an average edit distance of 3.8 from the assigned
Latin word. This compares favorably to the edit
distances reported in Bouchard-Co?te? et al (2009),
who using oracle cognate assignments achieved an
average Levenshtein distance of 3.0.9
8 Conclusion
We presented a new generative model of word
lists that automatically finds cognate groups from
scrambled vocabulary lists. This model jointly
models the origin, propagation, and evolution of
cognate groups from a common root word. We
also introduced a novel technique for approximat-
ing automata. Using these approximations, our
model can reduce the error rate by 80% over a
baseline approach. Finally, we demonstrate that
these automatically generated cognate groups can
be used to automatically reconstruct proto-words
faithfully, with a small increase in error.
Acknowledgments
Thanks to Alexandre Bouchard-Co?te? for the many
insights. This project is funded in part by the NSF
under grant 0915265 and an NSF graduate fellow-
ship to the first author.
References
Leonard Bloomfield. 1938. Language. Holt, New
York.
9Morphological noise and transcription errors contribute
to the absolute error rate for this data set.
1038
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In EMNLP.
Alexandre Bouchard-Co?te?, Thomas L. Griffiths, and
Dan Klein. 2009. Improved reconstruction of pro-
tolanguage word forms. In NAACL, pages 65?73.
Hal Daume? III and Lyle Campbell. 2007. A Bayesian
model for discovering typological implications. In
Conference of the Association for Computational
Linguistics (ACL).
Hal Daume? III. 2009. Non-parametric Bayesian model
areal linguistics. In NAACL.
Markus Dreyer and Jason Eisner. 2009. Graphical
models over multiple strings. In EMNLP, Singa-
pore, August.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, FSMNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Grzegorz Kondrak, Daniel Marcu, and Keven Knight.
2003. Cognates can improve statistical translation
models. In NAACL.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In NAACL.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP.
John B. Lowe and Martine Mazaudon. 1994. The re-
construction engine: a computer implementation of
the comparative method. Computational Linguis-
tics, 20(3):381?417.
Gideon S. Mann and David Yarowsky. 2001. Mul-
tipath translation lexicon induction via bridge lan-
guages. In NAACL, pages 1?8. Association for
Computational Linguistics.
Thomas P. Minka. 2001. Expectation propagation for
approximate bayesian inference. In UAI, pages 362?
369.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech pro-
cessing. In ECAI-96 Workshop. John Wiley and
Sons.
Mehryar Mohri, 2009. Handbook of Weighted Au-
tomata, chapter Weighted Automata Algorithms.
Springer.
Andrea Mulloni. 2007. Automatic prediction of cog-
nate orthography using support vector machines. In
ACL, pages 25?30.
John Nerbonne. 2010. Measuring the diffusion of lin-
guistic change. Philosophical Transactions of the
Royal Society B: Biological Sciences.
Michael P. Oakes. 2000. Computer estimation of
vocabulary in a protolanguage from word lists in
four daughter languages. Quantitative Linguistics,
7(3):233?243.
OED. 1989. ?day, n.?. In The Oxford English Dictio-
nary online. Oxford University Press.
John Ohala, 1993. Historical linguistics: Problems
and perspectives, chapter The phonetics of sound
change, pages 237?238. Longman.
Jose Oncina and Marc Sebban. 2006. Learning
stochastic edit distance: Application in handwritten
character recognition. Pattern Recognition, 39(9).
Don Ringe, Tandy Warnow, and Ann Taylor. 2002.
Indo-european and computational cladistics. Trans-
actions of the Philological Society, 100(1):59?129.
Alan S.C. Ross. 1950. Philological probability prob-
lems. Journal of the Royal Statistical Society Series
B.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In NAACL.
1039
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1098?1107,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Simple, Accurate Parsing with an All-Fragments Grammar
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
We present a simple but accurate parser
which exploits both large tree fragments
and symbol refinement. We parse with
all fragments of the training set, in con-
trast to much recent work on tree se-
lection in data-oriented parsing and tree-
substitution grammar learning. We re-
quire only simple, deterministic grammar
symbol refinement, in contrast to recent
work on latent symbol refinement. More-
over, our parser requires no explicit lexi-
con machinery, instead parsing input sen-
tences as character streams. Despite its
simplicity, our parser achieves accuracies
of over 88% F1 on the standard English
WSJ task, which is competitive with sub-
stantially more complicated state-of-the-
art lexicalized and latent-variable parsers.
Additional specific contributions center on
making implicit all-fragments parsing effi-
cient, including a coarse-to-fine inference
scheme and a new graph encoding.
1 Introduction
Modern NLP systems have increasingly used data-
intensive models that capture many or even all
substructures from the training data. In the do-
main of syntactic parsing, the idea that all train-
ing fragments1 might be relevant to parsing has a
long history, including tree-substitution grammar
(data-oriented parsing) approaches (Scha, 1990;
Bod, 1993; Goodman, 1996a; Chiang, 2003) and
tree kernel approaches (Collins and Duffy, 2002).
For machine translation, the key modern advance-
ment has been the ability to represent and memo-
rize large training substructures, be it in contigu-
ous phrases (Koehn et al, 2003) or syntactic trees
1In this paper, a fragment means an elementary tree in a
tree-substitution grammar, while a subtree means a fragment
that bottoms out in terminals.
(Galley et al, 2004; Chiang, 2005; Deneefe and
Knight, 2009). In all such systems, a central chal-
lenge is efficiency: there are generally a combina-
torial number of substructures in the training data,
and it is impractical to explicitly extract them all.
On both efficiency and statistical grounds, much
recent TSG work has focused on fragment selec-
tion (Zuidema, 2007; Cohn et al, 2009; Post and
Gildea, 2009).
At the same time, many high-performance
parsers have focused on symbol refinement ap-
proaches, wherein PCFG independence assump-
tions are weakened not by increasing rule sizes
but by subdividing coarse treebank symbols into
many subcategories either using structural anno-
tation (Johnson, 1998; Klein and Manning, 2003)
or lexicalization (Collins, 1999; Charniak, 2000).
Indeed, a recent trend has shown high accura-
cies from models which are dedicated to inducing
such subcategories (Henderson, 2004; Matsuzaki
et al, 2005; Petrov et al, 2006). In this paper,
we present a simplified parser which combines the
two basic ideas, using both large fragments and
symbol refinement, to provide non-local and lo-
cal context respectively. The two approaches turn
out to be highly complementary; even the simplest
(deterministic) symbol refinement and a basic use
of an all-fragments grammar combine to give ac-
curacies substantially above recent work on tree-
substitution grammar based parsers and approach-
ing top refinement-based parsers. For example,
our best result on the English WSJ task is an F1
of over 88%, where recent TSG parsers2 achieve
82-84% and top refinement-based parsers3 achieve
88-90% (e.g., Table 5).
Rather than select fragments, we use a simplifi-
cation of the PCFG-reduction of DOP (Goodman,
2Zuidema (2007), Cohn et al (2009), Post and Gildea
(2009). Zuidema (2007) incorporates deterministic refine-
ments inspired by Klein and Manning (2003).
3Including Collins (1999), Charniak and Johnson (2005),
Petrov and Klein (2007).
1098
1996a) to work with all fragments. This reduction
is a flexible, implicit representation of the frag-
ments that, rather than extracting an intractably
large grammar over fragment types, indexes all
nodes in the training treebank and uses a com-
pact grammar over indexed node tokens. This in-
dexed grammar, when appropriately marginalized,
is equivalent to one in which all fragments are ex-
plicitly extracted. Our work is the first to apply
this reduction to full-scale parsing. In this direc-
tion, we present a coarse-to-fine inference scheme
and a compact graph encoding of the training set,
which, together, make parsing manageable. This
tractability allows us to avoid selection of frag-
ments, and work with all fragments.
Of course, having a grammar that includes all
training substructures is only desirable to the ex-
tent that those structures can be appropriately
weighted. Implicit representations like those
used here do not allow arbitrary weightings of
fragments. However, we use a simple weight-
ing scheme which does decompose appropriately
over the implicit encoding, and which is flexible
enough to allow weights to depend not only on fre-
quency but also on fragment size, node patterns,
and certain lexical properties. Similar ideas have
been explored in Bod (2001), Collins and Duffy
(2002), and Goodman (2003). Our model empir-
ically affirms the effectiveness of such a flexible
weighting scheme in full-scale experiments.
We also investigate parsing without an explicit
lexicon. The all-fragments approach has the ad-
vantage that parsing down to the character level
requires no special treatment; we show that an ex-
plicit lexicon is not needed when sentences are
considered as strings of characters rather than
words. This avoids the need for complex un-
known word models and other specialized lexical
resources.
The main contribution of this work is to show
practical, tractable methods for working with an
all-fragments model, without an explicit lexicon.
In the parsing case, the central result is that ac-
curacies in the range of state-of-the-art parsers
(i.e., over 88% F1 on English WSJ) can be ob-
tained with no sampling, no latent-variable mod-
eling, no smoothing, and even no explicit lexicon
(hence negligible training overall). These tech-
niques, however, are not limited to the case of
monolingual parsing, offering extensions to mod-
els of machine translation, semantic interpretation,
and other areas in which a similar tension exists
between the desire to extract many large structures
and the computational cost of doing so.
2 Representation of Implicit Grammars
2.1 All-Fragments Grammars
We consider an all-fragments grammar G (see
Figure 1(a)) derived from a binarized treebank
B. G is formally a tree-substitution grammar
(Resnik, 1992; Bod, 1993) wherein each subgraph
of each training tree in B is an elementary tree,
or fragment f , in G. In G, each derivation d is
a tree (multiset) of fragments (Figure 1(c)), and
the weight of the derivation is the product of the
weights of the fragments: ?(d) =
?
f?d ?(f). In
the following, the derivation weights, when nor-
malized over a given sentence s, are interpretable
as conditional probabilities, soG induces distribu-
tions of the form P (d|s).
In models like G, many derivations will gen-
erally correspond to the same unsegmented tree,
and the parsing task is to find the tree whose
sum of derivation weights is highest: tmax =
arg maxt
?
d?t ?(d). This final optimization is in-
tractable in a way that is orthogonal to this pa-
per (Sima?an, 1996); we describe minimum Bayes
risk approximations in Section 4.
2.2 Implicit Representation of G
Explicitly extracting all fragment-rules of a gram-
marG is memory and space intensive, and imprac-
tical for full-size treebanks. As a tractable alter-
native, we consider an implicit grammar GI (see
Figure 1(b)) that has the same posterior probabil-
ities as G. To construct GI , we use a simplifi-
cation of the PCFG-reduction of DOP by Good-
man (1996a).4 GI has base symbols, which are
the symbol types from the original treebank, as
well as indexed symbols, which are obtained by
assigning a unique index to each node token in
the training treebank. The vast majority of sym-
bols in GI are therefore indexed symbols. While
it may seem that such grammars will be overly
large, they are in fact reasonably compact, being
linear in the treebank size B, while G is exponen-
tial in the length of a sentence. In particular, we
found that GI was smaller than explicit extraction
of all depth 1 and 2 unbinarized fragments for our
4The difference is that Goodman (1996a) collapses our
BEGIN and END rules into the binary productions, giving a
larger grammar which is less convenient for weighting.
1099
!
SYMBOLS: X, for all types in treebank "
RULES: X?#, for all fragments in "
! $
SYMBOLS:
?Base: X   for all types in treebank "
?Indexed: Xi for all tokens of X in "
RULES:
?Begin: ;?;i for all Xi in "
?Continue: Xi?<j Zk for all rule-tokens in "
?End: Xi?;IRUDOO;i in " %$
FRAGMENTSDERIVATIONS
(a)
(b)
GRAMMAR
%
#$
A
X
Al
CONTINUE
END
Xi
ZkYj
BEGIN
B
Bm
C
Cn
#
A
X
ZY
B C
CBA
X
words 
X
CBA
words 
EX
PL
IC
IT
IM
PL
IC
IT
MAP  ?
Figure 1: Grammar definition and sample derivations and fragments in the grammar for (a) the explicitly extracted all-fragments
grammar G, and (b) its implicit representation GI .
treebanks ? in practice, even just the raw treebank
grammar grows almost linearly in the size of B.5
There are 3 kinds of rules inGI , which are illus-
trated in Figure 1(d). The BEGIN rules transition
from a base symbol to an indexed symbol and rep-
resent the beginning of a fragment from G. The
CONTINUE rules use only indexed symbols and
correspond to specific depth-1 binary fragment to-
kens from training trees, representing the internal
continuation of a fragment in G. Finally, END
rules transition from an indexed symbol to a base
symbol, representing the frontier of a fragment.
By construction, all derivations in GI will seg-
ment, as shown in Figure 1(d), into regions corre-
sponding to tokens of fragments from the training
treebank B. Let pi be the map which takes appro-
priate fragments in GI (those that begin and end
with base symbols and otherwise contain only in-
dexed symbols), and maps them to the correspond-
ing f in G. We can consider any derivation dI in
GI to be a tree of fragments f I , each fragment a
token of a fragment type f = pi(f I) in the orig-
inal grammar G. By extension, we can therefore
map any derivation dI in GI to the corresponding
derivation d = pi(dI) in G.
The mapping pi is an onto mapping from GI to
5Just half the training set (19916 trees) itself had 1.7 mil-
lion depth 1 and 2 unbinarized rules compared to the 0.9 mil-
lion indexed symbols in GI (after graph packing). Even ex-
tracting binarized fragments (depth 1 and 2, with one order
of parent annotation) gives us 0.75 million rules, and, practi-
cally, we would need fragments of greater depth.
G. In particular, each derivation d in G has a non-
empty set of corresponding derivations {dI} =
pi?1(d) in GI , because fragments f in d corre-
spond to multiple fragments f I in GI that differ
only in their indexed symbols (one f I per occur-
rence of f in B). Therefore, the set of derivations
in G is preserved in GI . We now discuss how
weights can be preserved under pi.
2.3 Equivalence for Weighted Grammars
In general, arbitrary weight functions ? on frag-
ments in G do not decompose along the increased
locality of GI . However, we now consider a use-
fully broad class of weighting schemes for which
the posterior probabilities under G of derivations
d are preserved in GI . In particular, assume that
we have a weighting ? on rules in GI which does
not depend on the specific indices used. There-
fore, any fragment f I will have a weight in GI of
the form:
?I(f
I) = ?BEGIN(b)
?
r?C
?CONT(r)
?
e?E
?END(e)
where b is the BEGIN rule, r are CONTINUE rules,
and e are END rules in the fragment f I (see Fig-
ure 1(d)). Because ? is assumed to not depend on
the specific indices, all f I which correspond to the
same f under pi will have the same weight ?I(f)
in GI .
In this case, we can define an induced weight
1100
Xi
BEGIN
A
X
Al
CONTINUE
END
ZkYj
Bm
word
DOP1MIN-FRAGMENTS OUR MODEL
!!
" #$ !"%#$%!
!!CONTINUE
RULE TYPES WEIGHTS
Figure 2: Rules defined for grammar GI and weight schema
for the DOP1 model, the Min-Fragments model (Goodman
(2003)) and our model. Here s(X) denotes the total number
of fragments rooted at base symbol X .
for fragments f in G by
?G(f) =
?
fI?pi?1(f)
?I(f
I) = n(f)?I(f)
= n(f)?BEGIN(b
?)
?
r??C
?CONT(r
?)
?
e??E
?END(e
?)
where now b?, r? and e? are non-indexed type ab-
stractions of f ?s member productions in GI and
n(f) = |pi?1(f)| is the number of tokens of f in
B.
Under the weight function ?G(f), any deriva-
tion d in G will have weight which obeys
?G(d) =
?
f?d
?G(f) =
?
f?d
n(f)?I(f)
=
?
dI?d
?I(d
I)
and so the posterior P (d|s) of a derivation d for
a sentence s will be the same whether computed
in G or GI . Therefore, provided our weighting
function on fragments f in G decomposes over
the derivational representation of f in GI , we can
equivalently compute the quantities we need for
inference (see Section 4) using GI instead.
3 Parameterization of Implicit
Grammars
3.1 Classical DOP1
The original data-oriented parsing model ?DOP1?
(Bod, 1993) is a particular instance of the general
weighting scheme which decomposes appropri-
ately over the implicit encoding, described in Sec-
tion 2.3. Figure 2 shows rule weights for DOP1
in the parameter schema we have defined. The
END rule weight is 0 or 1 depending on whether
A is an intermediate symbol or not.6 The local
fragments in DOP1 were flat (non-binary) so this
weight choice simulates that property by not al-
lowing switching between fragments at intermedi-
ate symbols.
The original DOP1 model weights a fragment f
in G as ?G(f) = n(f)/s(X), i.e., the frequency
of fragment f divided by the number of fragments
rooted at base symbol X . This is simulated by our
weight choices (Figure 2) where each fragment f I
inGI has weight ?I(f I) = 1/s(X) and therefore,
?G(f) =
?
fI?pi?1(f) ?I(f
I) = n(f)/s(X).
Given the weights used for DOP1, the recursive
formula for the number of fragments s(Xi) rooted
at indexed symbol Xi (and for the CONTINUE rule
Xi ? Yj Zk) is
s(Xi) = (1 + s(Yj))(1 + s(Zk)), (1)
where s(Yj) and s(Zk) are the number of frag-
ments rooted at indexed symbols Yj and Zk (non-
intermediate) respectively. The number of frag-
ments s(X) rooted at base symbol X is then
s(X) =
?
Xi
s(Xi).
Implicitly parsing with the full DOP1 model (no
sampling of fragments) using the weights in Fig-
ure 2 gives a 68% parsing accuracy on the WSJ
dev-set.7 This result indicates that the weight of a
fragment should depend on more than just its fre-
quency.
3.2 Better Parameterization
As has been pointed out in the literature, large-
fragment grammars can benefit from weights of
fragments depending not only on their frequency
but also on other properties. For example, Bod
(2001) restricts the size and number of words
in the frontier of the fragments, and Collins and
Duffy (2002) and Goodman (2003) both give
larger fragments smaller weights. Our model can
incorporate both size and lexical properties. In
particular, we set ?CONT(r) for each binary CON-
TINUE rule r to a learned constant ?BODY, and we
set the weight for each rule with a POS parent to a
6Intermediate symbols are those created during binariza-
tion.
7For DOP1 experiments, we use no symbol refinement.
We annotate with full left binarization history to imitate the
flat nature of fragments in DOP1. We use mild coarse-pass
pruning (Section 4.1) without which the basic all-fragments
chart does not fit in memory. Standard WSJ treebank splits
used: sec 2-21 training, 22 dev, 23 test.
1101
Rule score: r(A? B C, i, k, j) =
?
x
?
y
?
z
O(Ax, i, j)?(Ax ? By Cz)I(By, i, k)I(Cz, k, j)
Max-Constituent: q(A, i, j) =
?
x O(Ax,i,j)I(Ax,i,j)?
r I(rootr,0,n)
tmax = argmax
t
?
c?t
q(c)
Max-Rule-Sum: q(A? B C, i, k, j) = r(A?B C,i,k,j)?
r I(rootr,0,n)
tmax = argmax
t
?
e?t
q(e)
Max-Variational: q(A? B C, i, k, j) = r(A?B C,i,k,j)?
x O(Ax,i,j)I(Ax,i,j)
tmax = argmax
t
?
e?t
q(e)
Figure 3: Inference: Different objectives for parsing with posteriors. A, B, C are base symbols, Ax, By , Cz are indexed
symbols and i,j,k are between-word indices. Hence, (Ax, i, j) represents a constituent labeled with Ax spanning words i
to j. I(Ax, i, j) and O(Ax, i, j) denote the inside and outside scores of this constituent, respectively. For brevity, we write
c ? (A, i, j) and e ? (A? B C, i, k, j). Also, tmax is the highest scoring parse. Adapted from Petrov and Klein (2007).
constant ?LEX (see Figure 2). Fractional values of
these parameters allow the weight of a fragment to
depend on its size and lexical properties.
Another parameter we introduce is a
?switching-penalty? csp for the END rules
(Figure 2). The DOP1 model uses binary values
(0 if symbol is intermediate, 1 otherwise) as
the END rule weight, which is equivalent to
prohibiting fragment switching at intermediate
symbols. We learn a fractional constant asp
that allows (but penalizes) switching between
fragments at annotated symbols through the
formulation csp(Xintermediate) = 1 ? asp and
csp(Xnon?intermediate) = 1 + asp. This feature
allows fragments to be assigned weights based on
the binarization status of their nodes.
With the above weights, the recursive formula
for s(Xi), the total weighted number of fragments
rooted at indexed symbol Xi, is different from
DOP1 (Equation 1). For rule Xi ? Yj Zk, it is
s(Xi) = ?BODY.(csp(Yj)+s(Yj))(csp(Zk)+s(Zk)).
The formula uses ?LEX in place of ?BODY if r is a
lexical rule (Figure 2).
The resulting grammar is primarily parameter-
ized by the training treebank B. However, each
setting of the hyperparameters (?BODY, ?LEX, asp)
defines a different conditional distribution on
trees. We choose amongst these distributions by
directly optimizing parsing F1 on our develop-
ment set. Because this objective is not easily dif-
ferentiated, we simply perform a grid search on
the three hyperparameters. The tuned values are
?BODY = 0.35, ?LEX = 0.25 and asp = 0.018.
For generalization to a larger parameter space, we
would of course need to switch to a learning ap-
proach that scales more gracefully in the number
of tunable hyperparameters.8
8Note that there has been a long history of DOP estima-
tors. The generative DOP1 model was shown to be inconsis-
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
Constituent 88.4 33.7 88.5 33.0 87.6 30.8
Rule-Sum 88.2 34.6 88.3 33.8 87.4 31.6
Variational 87.7 34.4 87.7 33.9 86.9 31.6
Table 1: All-fragments WSJ results (accuracy F1 and exact
match EX) for the constituent, rule-sum and variational ob-
jectives, using parent annotation and one level of markoviza-
tion.
4 Efficient Inference
The previously described implicit grammarGI de-
fines a posterior distribution P (dI |s) over a sen-
tence s via a large, indexed PCFG. This distri-
bution has the property that, when marginalized,
it is equivalent to a posterior distribution P (d|s)
over derivations in the correspondingly-weighted
all-fragments grammar G. However, even with
an explicit representation of G, we would not be
able to tractably compute the parse that maxi-
mizes P (t|s) =
?
d?t P (d|s) =
?
dI?t P (d
I |s)
(Sima?an, 1996). We therefore approximately
maximize over trees by computing various exist-
ing approximations to P (t|s) (Figure 3). Good-
man (1996b), Petrov and Klein (2007), and Mat-
suzaki et al (2005) describe the details of con-
stituent, rule-sum and variational objectives re-
spectively. Note that all inference methods depend
on the posterior P (t|s) only through marginal ex-
pectations of labeled constituent counts and an-
chored local binary tree counts, which are easily
computed from P (dI |s) and equivalent to those
from P (d|s). Therefore, no additional approxima-
tions are made in GI over G.
As shown in Table 1, our model (an all-
fragments grammar with the weighting scheme
tent by Johnson (2002). Later, Zollmann and Sima?an (2005)
presented a statistically consistent estimator, with the basic
insight of optimizing on a held-out set. Our estimator is not
intended to be viewed as a generative model of trees at all,
but simply a loss-minimizing conditional distribution within
our parametric family.
1102
shown in Figure 2) achieves an accuracy of
88.5% (using simple parent annotation) which is
4-5% (absolute) better than the recent TSG work
(Zuidema, 2007; Cohn et al, 2009; Post and
Gildea, 2009) and also approaches state-of-the-
art refinement-based parsers (e.g., Charniak and
Johnson (2005), Petrov and Klein (2007)).9
4.1 Coarse-to-Fine Inference
Coarse-to-fine inference is a well-established way
to accelerate parsing. Charniak et al (2006) in-
troduced multi-level coarse-to-fine parsing, which
extends the basic pre-parsing idea by adding more
rounds of pruning. Their pruning grammars
were coarse versions of the raw treebank gram-
mar. Petrov and Klein (2007) propose a multi-
stage coarse-to-fine method in which they con-
struct a sequence of increasingly refined gram-
mars, reparsing with each refinement. In par-
ticular, in their approach, which we adopt here,
coarse-to-fine pruning is used to quickly com-
pute approximate marginals, which are then used
to prune subsequent search. The key challenge
in coarse-to-fine inference is the construction of
coarse models which are much smaller than the
target model, yet whose posterior marginals are
close enough to prune with safely.
Our grammar GI has a very large number of in-
dexed symbols, so we use a coarse pass to prune
away their unindexed abstractions. The simple,
intuitive, and effective choice for such a coarse
grammar GC is a minimal PCFG grammar com-
posed of the base treebank symbols X and the
minimal depth-1 binary rules X ? Y Z (and
with the same level of annotation as in the full
grammar). If a particular base symbolX is pruned
by the coarse pass for a particular span (i, j) (i.e.,
the posterior marginal P (X, i, j|s) is less than a
certain threshold), then in the full grammar GI ,
we do not allow building any indexed symbol
Xl of type X for that span. Hence, the pro-
jection map for the coarse-to-fine model is piC :
Xl (indexed symbol)? X (base symbol).
We achieve a substantial improvement in speed
and memory-usage from the coarse-pass pruning.
Speed increases by a factor of 40 and memory-
usage decreases by a factor of 10 when we go
9All our experiments use the constituent objective ex-
cept when we report results for max-rule-sum and max-
variational parsing (where we use the parameters tuned for
max-constituent, therefore they unsurprisingly do not per-
form as well as max-constituent). Evaluations use EVALB,
see http://nlp.cs.nyu.edu/evalb/.
87.8
88.0
88.2
88.4
-4.0 -4.5 -5.0 -5.5 -6.0 -6.5 -7.0 -7.5Coarse-pass Log Posterior Threshold (PT)
F1
-6.2
Figure 4: Effect of coarse-pass pruning on parsing accuracy
(for WSJ dev-set, ? 40 words). Pruning increases to the left
as log posterior threshold (PT) increases.
86.086.5
87.087.5
88.088.5
89.089.5
90.0
-1 -3 -5 -7 -9 -11 -13Coarse-pass Log Posterior Threshold (PT)
-6
F1
89.6 No Pruning (PT = -inf)
89.8
Figure 5: Effect of coarse-pass pruning on parsing accuracy
(WSJ, training ? 20 words, tested on dev-set ? 20 words).
This graph shows that the fortuitous improvement due to
pruning is very small and that the peak accuracy is almost
equal to the accuracy without pruning (the dotted line).
from no pruning to pruning with a ?6.2 log pos-
terior threshold.10 Figure 4 depicts the variation
in parsing accuracies in response to the amount
of pruning done by the coarse-pass. Higher pos-
terior pruning thresholds induce more aggressive
pruning. Here, we observe an effect seen in previ-
ous work (Charniak et al (1998), Petrov and Klein
(2007), Petrov et al (2008)), that a certain amount
of pruning helps accuracy, perhaps by promoting
agreement between the coarse and full grammars
(model intersection). However, these ?fortuitous?
search errors give only a small improvement and
the peak accuracy is almost equal to the pars-
ing accuracy without any pruning (as seen in Fig-
ure 5).11 This outcome suggests that the coarse-
pass pruning is critical for tractability but not for
performance.
10Unpruned experiments could not be run for 40-word test
sentences even with 50GB of memory, therefore we calcu-
lated the improvement factors using a smaller experiment
with full training and sixty 30-word test sentences.
11To run experiments without pruning, we used training
and dev sentences of length ? 20 for the graph in Figure 5.
1103
tree-to-graph encoding
Figure 6: Collapsing the duplicate training subtrees converts
them to a graph and reduces the number of indexed symbols
significantly.
4.2 Packed Graph Encoding
The implicit all-fragments approach (Section 2.2)
avoids explicit extraction of all rule fragments.
However, the number of indexed symbols in our
implicit grammar GI is still large, because ev-
ery node in each training tree (i.e., every symbol
token) has a unique indexed symbol. We have
around 1.9 million indexed symbol tokens in the
word-level parsing model (this number increases
further to almost 12.3 million when we parse char-
acter strings in Section 5.1). This large symbol
space makes parsing slow and memory-intensive.
We reduce the number of symbols in our im-
plicit grammar GI by applying a compact, packed
graph encoding to the treebank training trees. We
collapse the duplicate subtrees (fragments that
bottom out in terminals) over all training trees.
This keeps the grammar unchanged because in an
tree-substitution grammar, a node is defined (iden-
tified) by the subtree below it. We maintain a
hashmap on the subtrees which allows us to eas-
ily discover the duplicates and bin them together.
The collapsing converts all the training trees in the
treebank to a graph with multiple parents for some
nodes as shown in Figure 6. This technique re-
duces the number of indexed symbols significantly
as shown in Table 2 (1.9 million goes down to 0.9
million, reduction by a factor of 2.1). This reduc-
tion increases parsing speed by a factor of 1.4 (and
by a factor of 20 for character-level parsing, see
Section 5.1) and reduces memory usage to under
4GB.
We store the duplicate-subtree counts for each
indexed symbol of the collapsed graph (using a
hashmap). When calculating the number of frag-
Parsing Model No. of Indexed Symbols
Word-level Trees 1,900,056
Word-level Graph 903,056
Character-level Trees 12,280,848
Character-level Graph 1,109,399
Table 2: Number of indexed symbols for word-level and
character-level parsing and their graph versions (for all-
fragments grammar with parent annotation and one level of
markovization).
Figure 7: Character-level parsing: treating the sentence as a
string of characters instead of words.
ments s(Xi) parented by an indexed symbol Xi
(see Section 3.2), and when calculating the inside
and outside scores during inference, we account
for the collapsed subtree tokens by expanding the
counts and scores using the corresponding multi-
plicities. Therefore, we achieve the compaction
with negligible overhead in computation.
5 Improved Treebank Representations
5.1 Character-Level Parsing
The all-fragments approach to parsing has the
added advantage that parsing below the word level
requires no special treatment, i.e., we do not need
an explicit lexicon when sentences are considered
as strings of characters rather than words.
Unknown words in test sentences (unseen in
training) are a major issue in parsing systems for
which we need to train a complex lexicon, with
various unknown classes or suffix tries. Smooth-
ing factors need to be accounted for and tuned.
With our implicit approach, we can avoid training
a lexicon by building up the parse tree from char-
acters instead of words. As depicted in Figure 7,
each word in the training trees is split into its cor-
responding characters with start and stop bound-
ary tags (and then binarized in a standard right-
branching style). A test sentence?s words are split
up similarly and the test-parse is built from train-
ing fragments using the same model and inference
procedure as defined for word-level parsing (see
Sections 2, 3 and 4). The lexical items (alphabets,
digits etc.) are now all known, so unlike word-level
parsing, no sophisticated lexicon is needed.
We choose a slightly richer weighting scheme
1104
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
Constituent 88.2 33.6 88.0 31.9 87.1 29.8
Rule-Sum 88.0 33.9 87.8 33.1 87.0 30.9
Variational 87.6 34.4 87.2 32.3 86.4 30.2
Table 3: All-fragments WSJ results for the character-level
parsing model, using parent annotation and one level of
markovization.
for this representation by extending the two-
weight schema for CONTINUE rules (?LEX and
?BODY) to a three-weight one: ?LEX, ?WORD, and
?SENT for CONTINUE rules in the lexical layer, in
the portion of the parse that builds words from
characters, and in the portion of the parse that
builds the sentence from words, respectively. The
tuned values are ?SENT = 0.35, ?WORD = 0.15,
?LEX = 0.95 and asp = 0. The character-level
model achieves a parsing accuracy of 88.0% (see
Table 3), despite lacking an explicit lexicon.12
Character-level parsing expands the training
trees (see Figure 7) and the already large indexed
symbol space size explodes (1.9 million increases
to 12.3 million, see Table 2). Fortunately, this
is where the packed graph encoding (Section 4.2)
is most effective because duplication of character
strings is high (e.g., suffixes). The packing shrinks
the symbol space size from 12.3 million to 1.1 mil-
lion, a reduction by a factor of 11. This reduction
increases parsing speed by almost a factor of 20
and brings down memory-usage to under 8GB.13
5.2 Basic Refinement: Parent Annotation
and Horizontal Markovization
In a pure all-fragments approach, compositions
of units which would have been independent in
a basic PCFG are given joint scores, allowing
the representation of certain non-local phenom-
ena, such as lexical selection or agreement, which
in fully local models require rich state-splitting
or lexicalization. However, at substitution sites,
the coarseness of raw unrefined treebank sym-
bols still creates unrealistic factorization assump-
tions. A standard solution is symbol refinement;
Johnson (1998) presents the particularly simple
case of parent annotation, in which each node is
12Note that the word-level model yields a higher accuracy
of 88.5%, but uses 50 complex unknown word categories
based on lexical, morphological and position features (Petrov
et al, 2006). Cohn et al (2009) also uses this lexicon.
13Full char-level experiments (w/o packed graph encoding)
could not be run even with 50GB of memory. We calcu-
late the improvement factors using a smaller experiment with
70% training and fifty 20-word test sentences.
Parsing Model F1
No Refinement (P=0, H=0)? 71.3
Basic Refinement (P=1, H=1)? 80.0
All-Fragments + No Refinement (P=0, H=0) 85.7
All-Fragments + Basic Refinement (P=1, H=1) 88.4
Table 4: F1 for a basic PCFG, and incorporation of basic
refinement, all-fragments and both, for WSJ dev-set (? 40
words). P = 1 means parent annotation of all non-terminals,
including the preterminal tags. H = 1 means one level of
markovization. ?Results from Klein and Manning (2003).
marked with its parent in the underlying treebank.
It is reasonable to hope that the gains from us-
ing large fragments and the gains from symbol re-
finement will be complementary. Indeed, previous
work has shown or suggested this complementar-
ity. Sima?an (2000) showed modest gains from en-
riching structural relations with semi-lexical (pre-
head) information. Charniak and Johnson (2005)
showed accuracy improvements from composed
local tree features on top of a lexicalized base
parser. Zuidema (2007) showed a slight improve-
ment in parsing accuracy when enough fragments
were added to learn enrichments beyond manual
refinements. Our work reinforces this intuition by
demonstrating how complementary they are in our
model (?20% error reduction on adding refine-
ment to an all-fragments grammar, as shown in the
last two rows of Table 4).
Table 4 shows results for a basic PCFG, and its
augmentation with either basic refinement (parent
annotation and one level of markovization), with
all-fragments rules (as in previous sections), or
both. The basic incorporation of large fragments
alone does not yield particularly strong perfor-
mance, nor does basic symbol refinement. How-
ever, the two approaches are quite additive in our
model and combine to give nearly state-of-the-art
parsing accuracies.
5.3 Additional Deterministic Refinement
Basic symbol refinement (parent annotation), in
combination with all-fragments, gives test-set ac-
curacies of 88.5% (? 40 words) and 87.6% (all),
shown as the Basic Refinement model in Table 5.
Klein and Manning (2003) describe a broad set
of simple, deterministic symbol refinements be-
yond parent annotation. We included ten of their
simplest annotation features, namely: UNARY-DT,
UNARY-RB, SPLIT-IN, SPLIT-AUX, SPLIT-CC, SPLIT-%,
GAPPED-S, POSS-NP, BASE-NP and DOMINATES-V.
None of these annotation schemes use any head
information. This additional annotation (see Ad-
1105
83
84
85
86
87
88
89
0 20 40 60 80 100
F1
Percentage of WSJ sections 2-21 used for training
Figure 8: Parsing accuracy F1 on the WSJ dev-set (? 40
words) increases with increasing percentage of training data.
ditional Refinement, Table 5) improves the test-
set accuracies to 88.7% (? 40 words) and 88.1%
(all), which is equal to a strong lexicalized parser
(Collins, 1999), even though our model does not
use lexicalization or latent symbol-split induc-
tion.
6 Other Results
6.1 Parsing Speed and Memory Usage
The word-level parsing model using the whole
training set (39832 trees, all-fragments) takes ap-
proximately 3 hours on the WSJ test set (2245
trees of ?40 words), which is equivalent to
roughly 5 seconds of parsing time per sen-
tence; and runs in under 4GB of memory. The
character-level version takes about twice the time
and memory. This novel tractability of an all-
fragments grammar is achieved using both coarse-
pass pruning and packed graph encoding. Micro-
optimization may further improve speed and mem-
ory usage.
6.2 Training Size Variation
Figure 8 shows how WSJ parsing accuracy in-
creases with increasing amount of training data
(i.e., percentage of WSJ sections 2-21). Even if we
train on only 10% of the WSJ training data (3983
sentences), we still achieve a reasonable parsing
accuracy of nearly 84% (on the development set,
? 40 words), which is comparable to the full-
system results obtained by Zuidema (2007), Cohn
et al (2009) and Post and Gildea (2009).
6.3 Other Language Treebanks
On the French and German treebanks (using the
standard dataset splits mentioned in Petrov and
test (? 40) test (all)
Parsing Model F1 EX F1 EX
FRAGMENT-BASED PARSERS
Zuidema (2007) ? ? 83.8? 26.9?
Cohn et al (2009) ? ? 84.0 ?
Post and Gildea (2009) 82.6 ? ? ?
THIS PAPER
All-Fragments
+ Basic Refinement 88.5 33.0 87.6 30.8
+ Additional Refinement 88.7 33.8 88.1 31.7
REFINEMENT-BASED PARSERS
Collins (1999) 88.6 ? 88.2 ?
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
Table 5: Our WSJ test set parsing accuracies, compared
to recent fragment-based parsers and top refinement-based
parsers. Basic Refinement is our all-fragments grammar with
parent annotation. Additional Refinement adds determinis-
tic refinement of Klein and Manning (2003) (Section 5.3).
?Results on the dev-set (? 100).
Klein (2008)), our simple all-fragments parser
achieves accuracies in the range of top refinement-
based parsers, even though the model parameters
were tuned out of domain on WSJ. For German,
our parser achieves an F1 of 79.8% compared
to 81.5% by the state-of-the-art and substantially
more complex Petrov and Klein (2008) work. For
French, our approach yields an F1 of 78.0% vs.
80.1% by Petrov and Klein (2008).14
7 Conclusion
Our approach of using all fragments, in combi-
nation with basic symbol refinement, and even
without an explicit lexicon, achieves results in the
range of state-of-the-art parsers on full scale tree-
banks, across multiple languages. The main take-
away is that we can achieve such results in a very
knowledge-light way with (1) no latent-variable
training, (2) no sampling, (3) no smoothing be-
yond the existence of small fragments, and (4) no
explicit unknown word model at all. While these
methods offer a simple new way to construct an
accurate parser, we believe that this general ap-
proach can also extend to other large-fragment
tasks, such as machine translation.
Acknowledgments
This project is funded in part by BBN under
DARPA contract HR0011-06-C-0022 and the NSF
under grant 0643742.
14All results on the test set (? 40 words).
1106
References
Rens Bod. 1993. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings of EACL.
Rens Bod. 2001. What is the Minimal Set of Frag-
ments that Achieves Maximum Parse Accuracy? In
Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of ACL.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing.
In Proceedings of the 6th Workshop on Very Large
Corpora.
Eugene Charniak, Mark Johnson, et al 2006. Multi-
level Coarse-to-fine PCFG Parsing. In Proceedings
of HLT-NAACL.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL.
David Chiang. 2003. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Data-Oriented Parsing.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing Compact but Accurate Tree-
Substitution Grammars. In Proceedings of NAACL.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
Proceedings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia.
Steve Deneefe and Kevin Knight. 2009. Synchronous
Tree Adjoining Machine Translation. In Proceed-
ings of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
Joshua Goodman. 1996a. Efficient Algorithms for
Parsing the DOP Model. In Proceedings of EMNLP.
Joshua Goodman. 1996b. Parsing Algorithms and
Metrics. In Proceedings of ACL.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod R, Scha R, Sima?an K
(eds.) Data-Oriented Parsing. University of Chicago
Press, Chicago, IL.
James Henderson. 2004. Discriminative Training of
a Neural Network Statistical Parser. In Proceedings
of ACL.
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24:613?632.
Mark Johnson. 2002. The DOP Estimation Method Is
Biased and Inconsistent. In Computational Linguis-
tics 28(1).
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-
ings of HLT-NAACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of
NAACL-HLT.
Slav Petrov and Dan Klein. 2008. Sparse Multi-Scale
Grammars for Discriminative Latent Variable Pars-
ing. In Proceedings of EMNLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-Fine Syntactic Machine Translation using
Language Projections. In Proceedings of EMNLP.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of
ACL-IJCNLP.
Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural
Language Processing. In Proceedings of COLING.
Remko Scha. 1990. Taaltheorie en taaltechnologie;
competence en performance. In R. de Kort and
G.L.J. Leerdam (eds.): Computertoepassingen in de
Neerlandistiek.
Khalil Sima?an. 1996. Computational Complexity
of Probabilistic Disambiguation by means of Tree-
Grammars. In Proceedings of COLING.
Khalil Sima?an. 2000. Tree-gram Parsing: Lexical De-
pendencies and Structural Relations. In Proceedings
of ACL.
Andreas Zollmann and Khalil Sima?an. 2005. A
Consistent and Efficient Estimator for Data-Oriented
Parsing. Journal of Automata, Languages and Com-
binatorics (JALC), 10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proceedings of EMNLP-CoNLL.
1107
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288?1297,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Phylogenetic Grammar Induction
Taylor Berg-Kirkpatrick and Dan Klein
Computer Science Division
University of California, Berkeley
{tberg, klein}@cs.berkeley.edu
Abstract
We present an approach to multilin-
gual grammar induction that exploits a
phylogeny-structured model of parameter
drift. Our method does not require any
translated texts or token-level alignments.
Instead, the phylogenetic prior couples
languages at a parameter level. Joint in-
duction in the multilingual model substan-
tially outperforms independent learning,
with larger gains both from more articu-
lated phylogenies and as well as from in-
creasing numbers of languages. Across
eight languages, the multilingual approach
gives error reductions over the standard
monolingual DMV averaging 21.1% and
reaching as high as 39%.
1 Introduction
Learning multiple languages together should be
easier than learning them separately. For exam-
ple, in the domain of syntactic parsing, a range
of recent work has exploited the mutual constraint
between two languages? parses of the same bi-
text (Kuhn, 2004; Burkett and Klein, 2008; Kuz-
man et al, 2009; Smith and Eisner, 2009; Sny-
der et al, 2009a). Moreover, Snyder et al (2009b)
in the context of unsupervised part-of-speech in-
duction (and Bouchard-Co?te? et al (2007) in the
context of phonology) show that extending be-
yond two languages can provide increasing ben-
efit. However, multitexts are only available for
limited languages and domains. In this work, we
consider unsupervised grammar induction without
bitexts or multitexts. Without translation exam-
ples, multilingual constraints cannot be exploited
at the sentence token level. Rather, we capture
multilingual constraints at a parameter level, us-
ing a phylogeny-structured prior to tie together the
various individual languages? learning problems.
Our joint, hierarchical prior couples model param-
eters for different languages in a way that respects
knowledge about how the languages evolved.
Aspects of this work are closely related to Co-
hen and Smith (2009) and Bouchard-Co?te? et al
(2007). Cohen and Smith (2009) present a model
for jointly learning English and Chinese depen-
dency grammars without bitexts. In their work,
structurally constrained covariance in a logistic
normal prior is used to couple parameters between
the two languages. Our work, though also differ-
ent in technical approach, differs most centrally in
the extension to multiple languages and the use of
a phylogeny. Bouchard-Co?te? et al (2007) consid-
ers an entirely different problem, phonological re-
construction, but shares with this work both the
use of a phylogenetic structure as well as the use
of log-linear parameterization of local model com-
ponents. Our work differs from theirs primarily
in the task (syntax vs. phonology) and the vari-
ables governed by the phylogeny: in our model it
is the grammar parameters that drift (in the prior)
rather than individual word forms (in the likeli-
hood model).
Specifically, we consider dependency induction
in the DMV model of Klein and Manning (2004).
Our data is a collection of standard dependency
data sets in eight languages: English, Dutch, Dan-
ish, Swedish, Spanish, Portuguese, Slovene, and
Chinese. Our focus is not the DMV model itself,
which is well-studied, but rather the prior which
couples the various languages? parameters. While
some choices of prior structure can greatly com-
plicate inference (Cohen and Smith, 2009), we
choose a hierarchical Gaussian form for the drift
term, which allows the gradient of the observed
data likelihood to be easily computed using stan-
dard dynamic programming methods.
In our experiments, joint multilingual learning
substantially outperforms independent monolin-
gual learning. Using a limited phylogeny that
1288
only couples languages within linguistic families
reduces error by 5.6% over the monolingual base-
line. Using a flat, global phylogeny gives a greater
reduction, almost 10%. Finally, a more articu-
lated phylogeny that captures both inter- and intra-
family effects gives an even larger average relative
error reduction of 21.1%.
2 Model
We define our model over two kinds of random
variables: dependency trees and parameters. For
each language ? in a set L, our model will generate
a collection t? of dependency trees ti?. We assume
that these dependency trees are generated by the
DMV model of Klein and Manning (2004), which
we write as ti? ? DMV(??). Here, ?? is a vector
of the various model parameters for language ?.
The prior is what couples the ?? parameter vectors
across languages; it is the focus of this work. We
first consider the likelihood model before moving
on to the prior.
2.1 Dependency Model with Valence
A dependency parse is a directed tree t over tokens
in a sentence s. Each edge of the tree specifies a
directed dependency from a head token to a de-
pendent, or argument token. The DMV is a gen-
erative model for trees t, which has been widely
used for dependency parse induction. The ob-
served data likelihood, used for parameter estima-
tion, is the marginal probability of generating the
observed sentences s, which are simply the leaves
of the trees t. Generation in the DMV model in-
volves two types of local conditional probabilities:
CONTINUE distributions that capture valence and
ATTACH distributions that capture argument selec-
tion.
First, the Bernoulli CONTINUE probability dis-
tributions P CONTINUE(c|h, dir, adj; ??) model the
fertility of a particular head type h. The outcome
c ? {stop, continue} is conditioned on the head
type h, direction dir, and adjacency adj. If a head
type?s continue probability is low, tokens of this
type will tend to generate few arguments.
Second, the ATTACH multinomial probability
distributions P ATTACH(a|h, dir; ??) capture attach-
ment preferences of heads, where a and h are both
token types. We take the same approach as pre-
vious work (Klein and Manning, 2004; Cohen and
Smith, 2009) and use gold part-of-speech labels as
tokens. Thus, the basic observed ?word? types are
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
Global
Indo-
European
Germanic
West
Germanic
North
Germanic
Ibero-
Romance
Italic Balto-Slavic
Slavic
Sino-
Tibetan
Sinitic
Figure 1: An example of a linguistically-plausible phylo-
genetic tree over the languages in our training data. Leaves
correspond to (observed) modern languages, while internal
nodes represent (unobserved) ancestral languages.
actually word classes.
2.1.1 Log-Linear Parameterization
The DMV?s local conditional distributions were
originally given as simple multinomial distribu-
tions with one parameter per outcome. However,
they can be re-parameterized to give the following
log-linear form (Eisner, 2002; Bouchard-Co?te? et
al., 2007; Berg-Kirkpatrick et al, 2010):
P CONTINUE(c|h, dir, adj; ??) =
exp
?
??T f CONTINUE(c, h, dir, adj)
?
P
c? exp
?
??T f CONTINUE(c?, h, dir, adj)
?
P ATTACH(a|h, dir; ??) =
exp
?
??T f ATTACH(a, h, dir)
?
P
a? exp
?
??T f ATTACH(a?, h, dir)
?
The parameters are weights ?? with one weight
vector per language. In the case where the vec-
tor of feature functions f has an indicator for each
possible conjunction of outcome and conditions,
the original multinomial distributions are recov-
ered. We refer to these full indicator features as
the set of SPECIFIC features.
2.2 Phylogenetic Prior
The focus of this work is coupling each of the pa-
rameters ?? in a phylogeny-structured prior. Con-
sider a phylogeny like the one shown in Fig-
ure 1, where each modern language ? in L is a
leaf. We would like to say that the leaves? pa-
rameter vectors arise from a process which slowly
1289
drifts along each branch. A convenient choice is
to posit additional parameter variables ??+ at in-
ternal nodes ?+ ? L+, a set of ancestral lan-
guages, and to assume that the conditional dis-
tribution P (??|?par(?)) at each branch in the phy-
logeny is a Gaussian centered on ?par(?), where
par(?) is the parent of ? in the phylogeny and
? ranges over L ? L+. The variance structure
of the Gaussian would then determine how much
drift (and in what directions) is expected. Con-
cretely, we assume that each drift distribution is
an isotropic Gaussian with mean ?par(?) and scalar
variance ?2. The root is centered at zero. We have
thus defined a joint distribution P (?|?2) where
? = (?? : ? ? L?L+). ?2 is a hyperparameter for
this prior which could itself be re-parameterized to
depend on branch length or be learned; we simply
set it to a plausible constant value.
Two primary challenges remain. First, infer-
ence under arbitrary priors can become complex.
However, in the simple case of our diagonal co-
variance Gaussians, the gradient of the observed
data likelihood can be computed directly using the
DMV?s expected counts and maximum-likelihood
estimation can be accomplished by applying stan-
dard gradient optimization methods. Second,
while the choice of diagonal covariance is effi-
cient, it causes components of ? that correspond
to features occurring in only one language to be
marginally independent of the parameters of all
other languages. In other words, only features
which fire in more than one language are coupled
by the prior. In the next section, we therefore in-
crease the overlap between languages? features by
using coarse projections of parts-of-speech.
2.3 Projected Features
With diagonal covariance in the Gaussian drift
terms, each parameter evolves independently of
the others. Therefore, our prior will be most
informative when features activate in multiple
languages. In phonology, it is useful to map
phonemes to the International Phonetic Alphabet
(IPA) in order to have a language-independent
parameterization. We introduce a similarly neu-
tral representation here by projecting language-
specific parts-of-speech to a coarse, shared inven-
tory.
Indeed, we assume that each language has a dis-
tinct tagset, and so the basic configurational fea-
tures will be language specific. For example, when
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
1(c = ?, h = ?, dir = ?, adj = ?)
SHARED: Activate for heads from multiple languages
using cross-lingual POS projection pi(?):
1(c = ?, pi(h) = ?, dir = ?, adj = ?)
CONTINUE distribution feature templates.
SPECIFIC: Activate for only one conjunction of out-
come and conditions:
1(a = ?, h = ?, dir = ?)
SHARED: Activate for heads and arguments from
multiple languages using cross-lingual
POS projection pi(?):
1(pi(a) = ?, pi(h) = ?, dir = ?)
1(pi(a) = ?, h = ?, dir = ?)
1(a = ?, pi(h) = ?, dir = ?)
ATTACH distribution feature templates.
Table 1: Feature templates for CONTINUE and ATTACH con-
ditional distributions.
an English VBZ takes a left argument headed by a
NNS, a feature will activate specific to VBZ-NNS-
LEFT. That feature will be used in the log-linear
attachment probability for English. However, be-
cause that feature does not show up in any other
language, it is not usefully controlled by the prior.
Therefore, we also include coarser features which
activate on more abstract, cross-linguistic config-
urations. In the same example, a feature will fire
indicating a coarse, direction-free NOUN-VERB at-
tachment. This feature will now occur in multiple
languages and will contribute to each of those lan-
guages? attachment models. Although such cross-
lingual features will have different weight param-
eters in each language, those weights will covary,
being correlated by the prior.
The coarse features are defined via a projec-
tion ? from language-specific part-of-speech la-
bels to coarser, cross-lingual word classes, and
hence we refer to them as SHARED features. For
each corpus used in this paper, we use the tagging
annotation guidelines to manually define a fixed
mapping from the corpus tagset to the following
coarse tagset: noun, verb, adjective, adverb, con-
junction, preposition, determiner, interjection, nu-
meral, and pronoun. Parts-of-speech for which
this coarse mapping is ambiguous or impossible
are not mapped, and do not have corresponding
SHARED features.
We summarize the feature templates for the
CONTINUE and ATTACH conditional distributions
in Table 1. Variants of all feature templates that
ignore direction and/or adjacency are included. In
practice, we found it beneficial for all language-
1290
independent features to ignore direction.
Again, only the coarse features occur in mul-
tiple languages, so all phylogenetic influence is
through those. Nonetheless, the effect of the phy-
logeny turns out to be quite strong.
2.4 Learning
We now turn to learning with the phylogenetic
prior. Since the prior couples parameters across
languages, this learning problem requires param-
eters for all languages be estimated jointly. We
seek to find ? = (?? : ? ? L ? L+) which
optimizes log P (?|s), where s aggregates the ob-
served leaves of all the dependency trees in all the
languages. This can be written as
log P (?) + logP (s|?) ? log P (s)
The third term is a constant and can be ignored.
The first term can be written as
logP (?) =
?
??L?L+
1
2?2 ??? ? ?par(?)?
2
2 + C
where C is a constant. The form of logP (?) im-
mediately shows how parameters are penalized for
being different across languages, more so for lan-
guages that are near each other in the phylogeny.
The second term
log P (s|?) =
?
??L
log P (s?|??)
is a sum of observed data likelihoods under
the standard DMV models for each language,
computable by dynamic programming (Klein
and Manning, 2004). Together, this yields the
following objective function:
l(?) =
?
??L?L+
1
2?2 ??? ? ?par(?)?22 +
?
??L logP (s?|??)
which can be optimized using gradient methods
or (MAP) EM. Here we used L-BFGS (Liu et al,
1989). This requires computation of the gradient
of the observed data likelihood log P (s?|??)
which is given by:
? logP (s?|??) = Et?|s?
[
? log P (s?, t?|??)
]
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
c,h,dir,adj ec,h,dir,adj(s?; ??) ?
[
f CONTINUE(c, h, dir, adj) ?
?
c? P CONTINUE(c?|h, dir, adj; ??)f CONTINUE(c?, h, dir, adj)
]
?
a,h,dir ea,h,dir(s?; ??) ?
[
f ATTACH(a, h, dir) ?
?
a? P ATTACH(a?|h, dir; ??)f ATTACH(a?, h, dir)
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The expected gradient of the log joint likelihood
of sentences and parses is equal to the gradient of
the log marginal likelihood of just sentences, or
the observed data likelihood (Salakhutdinov et al,
2003). ea,h,dir(s?; ??) is the expected count of the
number of times head h is attached to a in direc-
tion dir given the observed sentences s? and DMV
parameters ??. ec,h,dir,adj(s?; ??) is defined simi-
larly. Note that these are the same expected counts
required to perform EM on the DMV, and are com-
putable by dynamic programming.
The computation time is dominated by the com-
putation of each sentence?s posterior expected
counts, which are independent given the parame-
ters, so the time required per iteration is essentially
the same whether training all languages jointly or
independently. In practice, the total number of it-
erations was also similar.
3 Experimental Setup
3.1 Data
We ran experiments with the following languages:
English, Dutch, Danish, Swedish, Spanish, Por-
tuguese, Slovene, and Chinese. For all languages
but English and Chinese, we used corpora from the
2006 CoNLL-X Shared Task dependency parsing
data set (Buchholz and Marsi, 2006). We used the
shared task training set to both train and test our
models. These corpora provide hand-labeled part-
of-speech tags (except for Dutch, which is auto-
matically tagged) and provide dependency parses,
which are either themselves hand-labeled or have
been converted from hand-labeled parses of other
kinds. For English and Chinese we use sections
2-21 of the Penn Treebank (PTB) (Marcus et al,
1993) and sections 1-270 of the Chinese Tree-
bank (CTB) (Xue et al, 2002) respectively. Sim-
ilarly, these sections were used for both training
and testing. The English and Chinese data sets
have hand-labeled constituency parses and part-of-
speech tags, but no dependency parses. We used
the Bikel Chinese head finder (Bikel and Chiang,
2000) and the Collins English head finder (Collins,
1999) to transform the gold constituency parses
into gold dependency parses. None of the corpora
are bitexts. For all languages, we ran experiments
on all sentences of length 10 or less after punctua-
tion has been removed.
When constructing phylogenies over the lan-
guages we made use of their linguistic classifica-
tions. English and Dutch are part of the West Ger-
1291
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
West
Germanic
North
Germanic
Ibero-
Romance Slavic Sinitic
Global
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
Global
(a)
(b)
(c)
English Dutch SwedishDanish Spanish Portuguese Slovene Chinese
West
Germanic
North
Germanic
Ibero-
Romance Slavic Sinitic
Figure 2: (a) Phylogeny for FAMILIES model. (b) Phylogeny
for GLOBAL model. (c) Phylogeny for LINGUISTIC model.
manic family of languages, whereas Danish and
Swedish are part of the North Germanic family.
Spanish and Portuguese are both part of the Ibero-
Romance family. Slovene is part of the Slavic
family. Finally, Chinese is in the Sinitic family,
and is not an Indo-European language like the oth-
ers. We interchangeably speak of a language fam-
ily and the ancestral node corresponding to that
family?s root language in a phylogeny.
3.2 Models Compared
We evaluated three phylogenetic priors, each with
a different phylogenetic structure. We compare
with two monolingual baselines, as well as an all-
pairs multilingual model that does not have a phy-
logenetic interpretation, but which provides very
similar capacity for parameter coupling.
3.2.1 Phylogenetic Models
The first phylogenetic model uses the shallow phy-
logeny shown in Figure 2(a), in which only lan-
guages within the same family have a shared par-
ent node. We refer to this structure as FAMILIES.
Under this prior, the learning task decouples into
independent subtasks for each family, but no reg-
ularities across families can be captured.
The family-level model misses the constraints
between distant languages. Figure 2(b) shows an-
other simple configuration, wherein all languages
share a common parent node in the prior, meaning
that global regularities that are consistent across
all languages can be captured. We refer to this
structure as GLOBAL.
While the global model couples the parameters
for all eight languages, it does so without sensi-
tivity to the articulated structure of their descent.
Figure 2(c) shows a more nuanced prior struc-
ture, LINGUISTIC, which groups languages first
by family and then under a global node. This
structure allows global regularities as well as reg-
ularities within families to be learned.
3.2.2 Parameterization and ALLPAIRS Model
Daume? III (2007) and Finkel and Manning (2009)
consider a formally similar Gaussian hierarchy for
domain adaptation. As pointed out in Finkel and
Manning (2009), there is a simple equivalence be-
tween hierarchical regularization as described here
and the addition of new tied features in a ?flat?
model with zero-meaned Gaussian regularization
on all parameters. In particular, instead of param-
eterizing the objective in Section 2.4 in terms of
multiple sets of weights, one at each node in the
phylogeny (the hierarchical parameterization, de-
scribed in Section 2.4), it is equivalent to param-
eterize this same objective in terms of a single set
of weights on a larger of group features (the flat
parameterization). This larger group of features
contains a duplicate set of the features discussed in
Section 2.3 for each node in the phylogeny, each
of which is active only on the languages that are its
descendants. A linear transformation between pa-
rameterizations gives equivalence. See Finkel and
Manning (2009) for details.
In the flat parameterization, it seems equally
reasonable to simply tie all pairs of languages by
adding duplicate sets of features for each pair.
This gives the ALLPAIRS setting, which we also
compare to the tree-structured phylogenetic mod-
els above.
3.3 Baselines
To evaluate the impact of multilingual constraint,
we compared against two monolingual baselines.
The first baseline is the standard DMV with
only SPECIFIC features, which yields the standard
multinomial DMV (weak baseline). To facilitate
comparison to past work, we used no prior for this
monolingual model. The second baseline is the
DMV with added SHARED features. This model
includes a simple isotropic Gaussian prior on pa-
1292
Monolingual Multilingual
Phylogenetic
Corpus Size Ba
se
lin
e
B
as
el
in
e
w
/S
H
A
R
ED
A
LL
PA
IR
S
FA
M
IL
IE
S
B
ES
TP
A
IR
G
LO
B
A
L
LI
N
G
U
IS
TI
C
West Germanic English 6008 47.1 51.3 48.5 51.3 51.3 (Ch) 51.2 62.3Dutch 6678 36.3 36.0 44.0 36.1 36.2 (Sw) 44.0 45.1
North Germanic Danish 1870 33.5 33.6 40.5 31.4 34.2 (Du) 39.6 41.6Swedish 3571 45.3 44.8 56.3 44.8 44.8 (Ch) 44.5 58.3
Ibero-Romance Spanish 712 28.0 40.5 58.7 63.4 63.8 (Da) 59.4 58.4Portuguese 2515 38.5 38.5 63.1 37.4 38.4 (Sw) 37.4 63.0
Slavic Slovene 627 38.5 39.7 49.0 ? 49.6 (En) 49.4 48.4
Sinitic Chinese 959 36.3 43.3 50.7 ? 49.7 (Sw) 50.1 49.6
Macro-Avg. Relative Error Reduction 17.1 5.6 8.5 9.9 21.1
Table 2: Directed dependency accuracy of monolingual and multilingual models, and relative error reduction over the monolin-
gual baseline with SHARED features macro-averaged over languages. Multilingual models outperformed monolingual models
in general, with larger gains from increasing numbers of languages. Additionally, more nuanced phylogenetic structures out-
performed cruder ones.
rameters. This second baseline is the more direct
comparison to the multilingual experiments here
(strong baseline).
3.4 Evaluation
For each setting, we evaluated the directed de-
pendency accuracy of the minimum Bayes risk
(MBR) dependency parses produced by our mod-
els under maximum (posterior) likelihood parame-
ter estimates. We computed accuracies separately
for each language in each condition. In addition,
for multilingual models, we computed the relative
error reduction over the strong monolingual base-
line, macro-averaged over languages.
3.5 Training
Our implementation used the flat parameteriza-
tion described in Section 3.2.2 for both the phy-
logenetic and ALLPAIRS models. We originally
did this in order to facilitate comparison with the
non-phylogenetic ALLPAIRS model, which has no
equivalent hierarchical parameterization. In prac-
tice, optimizing with the hierarchical parameteri-
zation also seemed to underperform.1
1We noticed that the weights of features shared across lan-
guages had larger magnitude early in the optimization proce-
dure when using the flat parameterization compared to us-
ing the hierarchical parameterization, perhaps indicating that
cross-lingual influences had a larger effect on learning in its
initial stages.
All models were trained by directly optimizing
the observed data likelihood using L-BFGS (Liu et
al., 1989). Berg-Kirkpatrick et al (2010) suggest
that directly optimizing the observed data likeli-
hood may offer improvements over the more stan-
dard expectation-maximization (EM) optimization
procedure for models such as the DMV, espe-
cially when the model is parameterized using fea-
tures. We stopped training after 200 iterations in
all cases. This fixed stopping criterion seemed to
be adequate in all experiments, but presumably
there is a potential gain to be had in fine tuning.
To initialize, we used the harmonic initializer pre-
sented in Klein and Manning (2004). This type of
initialization is deterministic, and thus we did not
perform random restarts.
We found that for all models ?2 = 0.2 gave rea-
sonable results, and we used this setting in all ex-
periments. For most models, we found that vary-
ing ?2 in a reasonable range did not substantially
affect accuracy. For some models, the directed ac-
curacy was less flat with respect to ?2. In these
less-stable cases, there seemed to be an interac-
tion between the variance and the choice between
head conventions. For example, for some settings
of ?2, but not others, the model would learn that
determiners head noun phrases. In particular, we
observed that even when direct accuracy did fluc-
tuate, undirected accuracy remained more stable.
1293
4 Results
Table 2 shows the overall results. In all cases,
methods which coupled the languages in some
way outperformed the independent baselines that
considered each language independently.
4.1 Bilingual Models
The weakest of the coupled models was FAMI-
LIES, which had an average relative error reduc-
tion of 5.6% over the strong baseline. In this case,
most of the average improvement came from a sin-
gle family: Spanish and Portuguese. The limited
improvement of the family-level prior compared
to other phylogenies suggests that there are impor-
tant multilingual interactions that do not happen
within families. Table 2 also reports the maximum
accuracy achieved for each language when it was
paired with another language (same family or oth-
erwise) and trained together with a single common
parent. These results appear in the column headed
by BESTPAIR, and show the best accuracy for the
language on that row over all possible pairings
with other languages. When pairs of languages
were trained together in isolation, the largest bene-
fit was seen for languages with small training cor-
pora, not necessarily languages with common an-
cestry. In our setup, Spanish, Slovene, and Chi-
nese have substantially smaller training corpora
than the rest of the languages considered. Other-
wise, the patterns are not particularly clear; com-
bined with subsequent results, it seems that pair-
wise constraint is fairly limited.
4.2 Multilingual Models
Models that coupled multiple languages per-
formed better in general than models that only
considered pairs of languages. The GLOBAL
model, which couples all languages, if crudely,
yielded an average relative error reduction of
9.9%. This improvement comes as the number
of languages able to exert mutual constraint in-
creases. For example, Dutch and Danish had large
improvements, over and above any improvements
these two languages gained when trained with a
single additional language. Beyond the simplistic
GLOBAL phylogeny, the more nuanced LINGUIS-
TIC model gave large improvements for English,
Swedish, and Portuguese. Indeed, the LINGUIS-
TIC model is the only model we evaluated that
gave improvements for all the languages we con-
sidered.
It is reasonable to worry that the improvements
from these multilingual models might be partially
due to having more total training data in the mul-
tilingual setting. However, we found that halv-
ing the amount of data used to train the English,
Dutch, and Swedish (the languages with the most
training data) monolingual models did not sub-
stantially affect their performance, suggesting that
for languages with several thousand sentences or
more, the increase in statistical support due to ad-
ditional monolingual data was not an important ef-
fect (the DMV is a relatively low-capacity model
in any case).
4.3 Comparison of Phylogenies
Recall the structures of the three phylogenies
presented in Figure 2. These phylogenies dif-
fer in the correlations they can represent. The
GLOBAL phylogeny captures only ?universals,?
while FAMILIES captures only correlations be-
tween languages that are known to be similar. The
LINGUISTIC model captures both of these effects
simultaneously by using a two layer hierarchy.
Notably, the improvement due to the LINGUISTIC
model is more than the sum of the improvements
due to the GLOBAL and FAMILIES models.
4.4 Phylogenetic vs. ALLPAIRS
The phylogeny is capable of allowing appropri-
ate influence to pass between languages at mul-
tiple levels. We compare these results to the
ALLPAIRS model in order to see whether limi-
tation to a tree structure is helpful. The ALL-
PAIRS model achieved an average relative error
reduction of 17.1%, certainly outperforming both
the simple phylogenetic models. However, the
rich phylogeny of the LINGUISTIC model, which
incorporates linguistic constraints, outperformed
the freer ALLPAIRS model. A large portion of
this improvement came from English, a language
for which the LINGUISTIC model greatly outper-
formed all other models evaluated. We found that
the improved English analyses produced by the
LINGUISTIC model were more consistent with this
model?s analyses of other languages. This consis-
tency was not present for the English analyses pro-
duced by other models. We explore consistency in
more detail in Section 5.
4.5 Comparison to Related Work
The likelihood models for both the strong mono-
lingual baseline and the various multilingual mod-
1294
els are the same, both expanding upon the standard
DMV by adding coarse SHARED features. These
coarse features, even in a monolingual setting, im-
proved performance slightly over the weak base-
line, perhaps by encouraging consistent treatment
of the different finer-grained variants of parts-
of-speech (Berg-Kirkpatrick et al, 2010).2 The
only difference between the multilingual systems
and the strong baseline is whether or not cross-
language influence is allowed through the prior.
While this progression of model structure is
similar to that explored in Cohen and Smith
(2009), Cohen and Smith saw their largest im-
provements from tying together parameters for the
varieties of coarse parts-of-speech monolinugally,
and then only moderate improvements from allow-
ing cross-linguistic influence on top of monolin-
gual sharing. When Cohen and Smith compared
their best shared logistic-normal bilingual mod-
els to monolingual counter-parts for the languages
they investigate (Chinese and English), they re-
ported a relative error reduction of 5.3%. In com-
parison, with the LINGUISTIC model, we saw a
much larger 16.9% relative error reduction over
our strong baseline for these languages. Evaluat-
ing our LINGUISTIC model on the same test sets
as (Cohen and Smith, 2009), sentences of length
10 or less in section 23 of PTB and sections 271-
300 of CTB, we achieved an accuracy of 56.6 for
Chinese and 60.3 for English. The best models
of Cohen and Smith (2009) achieved accuracies of
52.0 and 62.0 respectively on these same test sets.
Our results indicate that the majority of our
model?s power beyond that of the standard DMV
is derived from multilingual, and in particular,
more-than-bilingual, interaction. These are, to the
best of our knowledge, the first results of this kind
for grammar induction without bitext.
5 Analysis
By examining the proposed parses we found that
the LINGUISTIC and ALLPAIRS models produced
analyses that were more consistent across lan-
guages than those of the other models. We
also observed that the most common errors can
be summarized succinctly by looking at attach-
ment counts between coarse parts-of-speech. Fig-
ure 3 shows matrix representations of dependency
2Coarse features that only tie nouns and verbs are ex-
plored in Berg-Kirkpatrick et al (2010). We found that these
were very effective for English and Chinese, but gave worse
performance for other languages.
counts. The area of a square is proportional to the
number of order-collapsed dependencies where
the column label is the head and the row label is
the argument in the parses from each system. For
ease of comprehension, we use the cross-lingual
projections and only show counts for selected in-
teresting classes.
Comparing Figure 3(c), which shows depen-
dency counts proposed by the LINGUISTIC model,
to Figure 3(a), which shows the same for the
strong monolingual baseline, suggests that the
analyses proposed by the LINGUISTIC model are
more consistent across languages than are the
analyses proposed by the monolingual model. For
example, the monolingual learners are divided
as to whether determiners or nouns head noun
phrases. There is also confusion about which la-
bels head whole sentences. Dutch has the problem
that verbs modify pronouns more often than pro-
nouns modify verbs, and pronouns are predicted
to head sentences as often as verbs are. Span-
ish has some confusion about conjunctions, hy-
pothesizing that verbs often attach to conjunctions,
and conjunctions frequently head sentences. More
subtly, the monolingual analyses are inconsistent
in the way they head prepositional phrases. In
the monolingual Portuguese hypotheses, preposi-
tions modify nouns more often than nouns mod-
ify prepositions. In English, nouns modify prepo-
sitions, and prepositions modify verbs. Both the
Dutch and Spanish models are ambivalent about
the attachment of prepositions.
As has often been observed in other contexts
(Liang et al, 2008), promoting agreement can
improve accuracy in unsupervised learning. Not
only are the analyses proposed by the LINGUISTIC
model more consistent, they are also more in ac-
cordance with the gold analyses. Under the LIN-
GUISTIC model, Dutch now attaches pronouns to
verbs, and thus looks more like English, its sister
in the phylogenetic tree. The LINGUISTIC model
has also chosen consistent analyses for preposi-
tional phrases and noun phrases, calling preposi-
tions and nouns the heads of each, respectively.
The problem of conjunctions heading Spanish sen-
tences has also been corrected.
Figure 3(b) shows dependency counts for the
GLOBAL multilingual model. Unsurprisingly, the
analyses proposed under global constraint appear
somewhat more consistent than those proposed
under no multi-lingual constraint (now three lan-
1295
Figure 3: Dependency counts in proposed parses. Row label modifies column label. (a) Monolingual baseline with SHARED
features. (b) GLOBAL model. (c) LINGUISTIC model. (d) Dependency counts in hand-labeled parses. Analyses proposed by
monolingual baseline show significant inconsistencies across languages. Analyses proposed by LINGUISTIC model are more
consistent across languages than those proposed by either the monolingual baseline or the GLOBAL model.
guages agree that prepositional phrases are headed
by prepositions), but not as consistent as those pro-
posed by the LINGUISTIC model.
Finally, Figure 3(d) shows dependency counts
in the hand-labeled dependency parses. It appears
that even the very consistent LINGUISTIC parses
do not capture the non-determinism of preposi-
tional phrase attachment to both nouns and verbs.
6 Conclusion
Even without translated texts, multilingual con-
straints expressed in the form of a phylogenetic
prior on parameters can give substantial gains
in grammar induction accuracy over treating lan-
guages in isolation. Additionally, articulated phy-
logenies that are sensitive to evolutionary structure
can outperform not only limited flatter priors but
also unconstrained all-pairs interactions.
7 Acknowledgements
This project is funded in part by the NSF un-
der grant 0915265 and DARPA under grant
N10AP20007.
1296
References
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In North American Chapter of the
Association for Computational Linguistics.
D. M. Bikel and D. Chiang. 2000. Two statistical pars-
ing models applied to the Chinese treebank. In Sec-
ond Chinese Language Processing Workshop.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Grif-
fiths. 2007. A probabilistic approach to diachronic
phonology. In Empirical Methods in Natural Lan-
guage Processing.
S. Buchholz and E. Marsi. 2006. Computational Nat-
ural Language Learning-X shared task on multilin-
gual dependency parsing. In Conference on Compu-
tational Natural Language Learning.
D. Burkett and D. Klein. 2008. Two languages are
better than one (for syntactic parsing). In Empirical
Methods in Natural Language Processing.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in un-
supervised grammar induction. In North American
Chapter of the Association for Computational Lin-
guistics.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. In Ph.D. thesis, University
of Pennsylvania, Philadelphia.
H. Daume? III. 2007. Frustratingly easy domain adap-
tation. In Association for Computational Linguis-
tics.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Association for Compu-
tational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Hierarchi-
cal bayesian domain adaptation. In North American
Chapter of the Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Association for Compu-
tational Linguistics.
J. Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Association for Computa-
tional Linguistics.
G. Kuzman, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Association for Computational Lin-
guistics/International Joint Conference on Natural
Language Processing.
P. Liang, D. Klein, and M. I. Jordan. 2008.
Agreement-based learning. In Advances in Neural
Information Processing Systems.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the
limited memory BFGS method for large scale opti-
mization. Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani.
2003. Optimization with EM and expectation-
conjugate-gradient. In International Conference on
Machine Learning.
D. A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Empirical Methods in Natural Lan-
guage Processing.
B. Snyder, T. Naseem, and R. Barzilay. 2009a. Unsu-
pervised multilingual grammar induction. In Asso-
ciation for Computational Linguistics/International
Joint Conference on Natural Language Processing.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzi-
lay. 2009b. Adding more languages improves un-
supervised multilingual part-of-speech tagging: A
Bayesian non-parametric approach. In North Amer-
ican Chapter of the Association for Computational
Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building
a large-scale annotated Chinese corpus. In Interna-
tional Conference on Computational Linguistics.
1297
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453?1463,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Discriminative Modeling of Extraction Sets for Machine Translation
John DeNero and Dan Klein
Computer Science Division
University of California, Berkeley
{denero,klein}@cs.berkeley.edu
Abstract
We present a discriminative model that di-
rectly predicts which set of phrasal transla-
tion rules should be extracted from a sen-
tence pair. Our model scores extraction
sets: nested collections of all the overlap-
ping phrase pairs consistent with an under-
lying word alignment. Extraction set mod-
els provide two principle advantages over
word-factored alignment models. First,
we can incorporate features on phrase
pairs, in addition to word links. Second,
we can optimize for an extraction-based
loss function that relates directly to the
end task of generating translations. Our
model gives improvements in alignment
quality relative to state-of-the-art unsuper-
vised and supervised baselines, as well
as providing up to a 1.4 improvement in
BLEU score in Chinese-to-English trans-
lation experiments.
1 Introduction
In the last decade, the field of statistical machine
translation has shifted from generating sentences
word by word to systems that recycle whole frag-
ments of training examples, expressed as transla-
tion rules. This general paradigm was first pur-
sued using contiguous phrases (Och et al, 1999;
Koehn et al, 2003), and has since been general-
ized to a wide variety of hierarchical and syntactic
formalisms. The training stage of statistical sys-
tems focuses primarily on discovering translation
rules in parallel corpora.
Most systems discover translation rules via a
two-stage pipeline: a parallel corpus is aligned at
the word level, and then a second procedure ex-
tracts fragment-level rules from word-aligned sen-
tence pairs. This paper offers a model-based alter-
native to phrasal rule extraction, which merges this
two-stage pipeline into a single step. We present a
discriminative model that directly predicts which
set of phrasal translation rules should be extracted
from a sentence pair. Our model predicts extrac-
tion sets: combinatorial objects that include the
set of all overlapping phrasal translation rules con-
sistent with an underlying word-level alignment.
This approach provides additional discriminative
power relative to word aligners because extraction
sets are scored based on the phrasal rules they con-
tain in addition to word-to-word alignment links.
Moreover, the structure of our model directly re-
flects the purpose of alignment models in general,
which is to discover translation rules.
We address several challenges to training and
applying an extraction set model. First, we would
like to leverage existing word-level alignment re-
sources. To do so, we define a deterministic map-
ping from word alignments to extraction sets, in-
spired by existing extraction procedures. In our
mapping, possible alignment links have a precise
interpretation that dictates what phrasal translation
rules can be extracted from a sentence pair. This
mapping allows us to train with existing annotated
data sets and use the predictions from word-level
aligners as features in our extraction set model.
Second, our model solves a structured predic-
tion problem, and the choice of loss function dur-
ing training affects model performance. We opti-
mize for a phrase-level F-measure in order to fo-
cus learning on the task of predicting phrasal rules
rather than word alignment links.
Third, our discriminative approach requires that
we perform inference in the space of extraction
sets. Our model does not factor over disjoint word-
to-word links or minimal phrase pairs, and so ex-
isting inference procedures do not directly apply.
However, we show that the dynamic program for
a block ITG aligner can be augmented to score ex-
traction sets that are indexed by underlying ITG
word alignments (Wu, 1997). We also describe a
1453
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
(a)
(b)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
(a)
(b)
Figure 1: A word alignment A (shaded grid cells)
defines projections ?(ei) and ?(fj), shown as dot-
ted lines for each word in each sentence. The ex-
traction set R3(A) includes all bispans licensed by
these projections, shown as rounded rectangles.
coarse-to-fine inference approach that allows us to
scale our method to long sentences.
Our extraction set model outperforms both un-
supervised and supervised word aligners at pre-
dicting word alignments and extraction sets. We
also demonstrate that extraction sets are useful for
end-to-end machine translation. Our model im-
proves translation quality relative to state-of-the-
art Chinese-to-English baselines across two pub-
licly available systems, providing total BLEU im-
provements of 1.2 in Moses, a phrase-based sys-
tem, and 1.4 in a Joshua, a hierarchical system
(Koehn et al, 2007; Li et al, 2009)
2 Extraction Set Models
The input to our model is an unaligned sentence
pair, and the output is an extraction set of phrasal
translation rules. Word-level alignments are gen-
erated as a byproduct of inference. We first spec-
ify the relationship between word alignments and
extraction sets, then define our model.
2.1 Extraction Sets from Word Alignments
Rule extraction is a standard concept in machine
translation: word alignment constellations license
particular sets of overlapping rules, from which
subsets are selected according to limits on phrase
length (Koehn et al, 2003), number of gaps (Chi-
ang, 2007), count of internal tree nodes (Galley et
al., 2006), etc. In this paper, we focus on phrasal
rule extraction (i.e., phrase pair extraction), upon
which most other extraction procedures are based.
Given a sentence pair (e, f), phrasal rule extrac-
tion defines a mapping from a set of word-to-word
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
Figure 2: Examples of two types of possible align-
ment links (striped). These types account for 96%
of the possible alignment links in our data set.
alignment links A = {(i, j)} to an extraction set
of bispans Rn(A) = {[g, h) ? [k, `)}, where
each bispan links target span [g, h) to source span
[k, `).1 The maximum phrase length n ensures that
max(h? g, `? k) ? n.
We can describe this mapping via word-to-
phrase projections, as illustrated in Figure 1. Let
word ei project to the phrasal span ?(ei), where
?(ei) =
[
min
j?Ji
j , max
j?Ji
j + 1
)
(1)
Ji = {j : (i, j) ? A}
and likewise each word fj projects to a span of e.
Then, Rn(A) includes a bispan [g, h)? [k, `) iff
?(ei) ? [k, `) ?i ? [g, h)
?(fj) ? [g, h) ?j ? [k, `)
That is, every word in one of the phrasal spans
must project within the other. This mapping is de-
terministic, and so we can interpret a word-level
alignment A as also specifying the phrasal rules
that should be extracted from a sentence pair.
2.2 Possible and Null Alignment Links
We have not yet accounted for two special cases
in annotated corpora: possible alignments and null
alignments. To analyze these annotations, we con-
sider a particular data set: a hand-aligned portion
1We use the fencepost indexing scheme used commonly
for parsing. Words are 0-indexed. Spans are inclusive on the
lower bound and exclusive on the upper bound. For example,
the span [0, 2) includes the first two words of a sentence.
1454
of the NIST MT02 Chinese-to-English test set,
which has been used in previous alignment experi-
ments (Ayan et al, 2005; DeNero and Klein, 2007;
Haghighi et al, 2009).
Possible links account for 22% of all alignment
links in these data, and we found that most of
these links fall into two categories. First, possible
links are used to align function words that have no
equivalent in the other language, but colocate with
aligned content words, such as English determin-
ers. Second, they are used to mark pairs of words
or short phrases that are not lexical equivalents,
but which play equivalent roles in each sentence.
Figure 2 shows examples of these two use cases,
along with their corpus frequencies.2
On the other hand, null alignments are used
sparingly in our annotated data. More than 90%
of words participate in some alignment link. The
unaligned words typically express content in one
sentence that is absent in its translation.
Figure 3 illustrates how we interpret possible
and null links in our projection. Possible links are
typically not included in extraction procedures be-
cause most aligners predict only sure links. How-
ever, we see a natural interpretation for possible
links in rule extraction: they license phrasal rules
that both include and exclude them. We exclude
null alignments from extracted phrases because
they often indicate a mismatch in content.
We achieve these effects by redefining the pro-
jection operator ?. Let A(s) be the subset of A
that are sure links, then let the index set Ji used
for projection ? in Equation 1 be
Ji =
?
??
??
{
j : (i, j) ? A(s)
}
if ?j : (i, j) ? A(s)
{?1, |f|} if @j : (i, j) ? A
{j : (i, j) ? A} otherwise
Here, Ji is a set of integers, and ?(ei) for null
aligned ei will be [?1, |f|+ 1) by Equation 1.
Of course, the characteristics of our aligned cor-
pus may not hold for other annotated corpora or
other language pairs. However, we hope that the
overall effectiveness of our modeling approach
will influence future annotation efforts to build
corpora that are consistent with this interpretation.
2.3 A Linear Model of Extraction Sets
We now define a linear model that scores extrac-
tion sets. We restrict our model to score only co-
2We collected corpus frequencies of possible alignment
link types ourselves on a sample of the hand-aligned data set.
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
(past)
k =2
l =4
g =1
h =3
Figure 3: Possible links constrain the word-to-
phrase projection of otherwise unaligned words,
which in turn license overlapping phrases. In this
example, ?(f2) = [1, 2) does not include the
possible link at (1, 0) because of the sure link at
(1, 1), but ?(e1) = [1, 2) does use the possible
link because it would otherwise be unaligned. The
word ?PDT? is null aligned, and so its projection
?(e4) = [?1, 4) extends beyond the bounds of the
sentence, excluding ?PDT? from all phrase pairs.
herent extraction sets Rn(A), those that are li-
censed by an underlying word alignment A with
sure alignments A(s) ? A. Conditioned on a
sentence pair (e, f) and maximum phrase length
n, we score extraction sets via a feature vec-
tor ?(A(s), Rn(A)) that includes features on sure
links (i, j) ? A(s) and features on the bispans in
Rn(A) that link [g, h) in e to [k, `) in f :
?(A(s), Rn(A)) =
?
(i,j)?A(s)
?a(i, j) +
?
[g,h)?[k,`)?Rn(A)
?b(g, h, k, `)
Because the projection operator Rn(?) is a
deterministic function, we can abbreviate
?(A(s), Rn(A)) as ?(A) without loss of infor-
mation, although we emphasize that A is a set
of sure and possible alignments, and ?(A) does
not decompose as a sum of vectors on individual
word-level alignment links. Our model is param-
eterized by a weight vector ?, which scores an
extraction set Rn(A) as ? ? ?(A).
To further limit the space of extraction sets we
are willing to consider, we restrict A to block
inverse transduction grammar (ITG) alignments,
a space that allows many-to-many alignments
through phrasal terminal productions, but other-
wise enforces at-most-one-to-one phrase match-
ings with ITG reordering patterns (Cherry and Lin,
2007; Zhang et al, 2008). The ITG constraint
1455
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
Figure 4: Above, we show a representative sub-
set of the block alignment patterns that serve as
terminal productions of the ITG that restricts the
output space of our model. These terminal pro-
ductions cover up to n = 3 words in each sentence
and include a mixture of sure (filled) and possible
(striped) word-level alignment links.
is more computationally convenient than arbitrar-
ily ordered phrase matchings (Wu, 1997; DeNero
and Klein, 2008). However, the space of block
ITG alignments is expressive enough to include
the vast majority of patterns observed in hand-
annotated parallel corpora (Haghighi et al, 2009).
In summary, our model scores all Rn(A) for
A ? ITG(e, f) where A can include block termi-
nals of size up to n. In our experiments, n = 3.
Unlike previous work, we allow possible align-
ment links to appear in the block terminals, as de-
picted in Figure 4.
3 Model Estimation
We estimate the weights ? of our extraction set
model discriminatively using the margin-infused
relaxed algorithm (MIRA) of Crammer and Singer
(2003)?a large-margin, perceptron-style, online
learning algorithm. MIRA has been used suc-
cessfully in MT to estimate both alignment mod-
els (Haghighi et al, 2009) and translation models
(Chiang et al, 2008).
For each training example, MIRA requires that
we find the alignment Am corresponding to the
highest scoring extraction set Rn(Am) under the
current model,
Am = arg maxA?ITG(e,f)? ? ?(A) (2)
Section 4 describes our approach to solving this
search problem for model inference.
MIRA updates away from Rn(Am) and to-
ward a gold extraction set Rn(Ag). Some hand-
annotated alignments are outside of the block ITG
model class. Hence, we update toward the ex-
traction set for a pseudo-gold alignment Ag ?
ITG(e, f) with minimal distance from the true ref-
erence alignment At.
Ag = arg minA?ITG(e,f)|A ? At ?A ?At| (3)
Inference details appear in Section 4.3.
GivenAg andAm, we update the model param-
eters away from Am and toward Ag.
? ? ? + ? ? (?(Ag)? ?(Am))
where ? is the minimal step size that will ensure
we prefer Ag to Am by a margin greater than
the loss L(Am;Ag), capped at some maximum
update size C to provide regularization. We use
C = 0.01 in experiments. The step size is a closed
form function of the loss and feature vectors: ? =
min
(
C,
L(Am;Ag)? ? ? (?(Ag)? ?(Am))
||?(Ag)? ?(Am)||22
)
We train the model for 30 iterations over the
training set, shuffling the order each time, and we
average the weight vectors observed after each it-
eration to estimate our final model.
3.1 Extraction Set Loss Function
In order to focus learning on predicting the
right bispans, we use an extraction-level loss
L(Am;Ag): an F-measure of the overlap between
bispans in Rn(Am) and Rn(Ag). This measure
has been proposed previously to evaluate align-
ment systems (Ayan and Dorr, 2006). Based
on preliminary translation results during develop-
ment, we chose bispan F5 as our loss:
Pr(Am) = |Rn(Am) ?Rn(Ag)|/|Rn(Am)|
Rc(Am) = |Rn(Am) ?Rn(Ag)|/|Rn(Ag)|
F5(Am;Ag) =
(1 + 52) ? Pr(Am) ? Rc(Am)
52 ? Pr(Am) + Rc(Am)
L(Am;Ag) = 1? F5(Am;Ag)
F5 favors recall over precision. Previous align-
ment work has shown improvements from adjust-
ing the F-measure parameter (Fraser and Marcu,
2006). In particular, Lacoste-Julien et al (2006)
also chose a recall-biased objective.
Optimizing for a bispan F-measure penalizes
alignment mistakes in proportion to their rule ex-
traction consequences. That is, adding a word
link that prevents the extraction of many correct
phrasal rules, or which licenses many incorrect
rules, is strongly discouraged by this loss.
1456
3.2 Features on Extraction Sets
The discriminative power of our model is driven
by the features on sure word alignment links
?a(i, j) and bispans ?b(g, h, k, `). In both cases,
the most important features come from the pre-
dictions of unsupervised models trained on large
parallel corpora, which provide frequency and co-
occurrence information.
To score word-to-word links, we use the poste-
rior predictions of a jointly trained HMM align-
ment model (Liang et al, 2006). The remaining
features include a dictionary feature, an identical
word feature, an absolute position distortion fea-
ture, and features for numbers and punctuation.
To score phrasal translation rules in an extrac-
tion set, we use a mixture of feature types. Ex-
traction set models allow us to incorporate the
same phrasal relative frequency statistics that drive
phrase-based translation performance (Koehn et
al., 2003). To implement these frequency features,
we extract a phrase table from the alignment pre-
dictions of a jointly trained unsupervised HMM
model using Moses (Koehn et al, 2007), and score
bispans using the resulting features. We also in-
clude indicator features on lexical templates for
the 50 most common words in each language, as
in Haghighi et al (2009). We include indicators
for the number of words and Chinese characters
in rules. One useful indicator feature exploits the
fact that capitalized terms in English tend to align
to Chinese words with three or more characters.
On 1-by-n or n-by-1 phrasal rules, we include in-
dicator features of fertility for common words.3
We also include monolingual phrase features
that expose useful information to the model. For
instance, English bigrams beginning with ?the?
are often extractable phrases. English trigrams
with a hyphen as the second word are typically ex-
tractable, meaning that the first and third words
align to consecutive Chinese words. When any
conjugation of the word ?to be? is followed by a
verb, indicating passive voice or progressive tense,
the two words tend to align together.
Our feature set alo includes bias features on
phrasal rules and links, which control the num-
ber of null-aligned words and number of rules li-
censed. In total, our final model includes 4,249
individual features, dominated by various instanti-
ations of lexical templates.
3Limiting lexicalized features to common words helps
prevent overfitting.
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
or
Figure 5: Both possible ITG decompositions of
this example alignment will split one of the two
highlighted bispans across constituents.
4 Model Inference
Equation 2 asks for the highest scoring extraction
set under our model, Rn(Am), which we also re-
quire at test time. Although we have restricted
Am ? ITG(e, f), our extraction set model does not
factor over ITG productions, and so the dynamic
program for a vanilla block ITG will not suffice to
find Rn(Am). To see this, consider the extraction
set in Figure 5. An ITG decomposition of the un-
derlying alignment imposes a hierarchical brack-
eting on each sentence, and some bispan in the ex-
traction set for this alignment will cross any such
bracketing. Hence, the score of some licensed bis-
pan will be non-local to the ITG decomposition.
4.1 A Dynamic Program for Extraction Sets
If we treat the maximum phrase length n as a fixed
constant, then we can define a dynamic program to
search the space of extraction sets. An ITG deriva-
tion for some alignment A decomposes into two
sub-derivations forAL andAR.4 The model score
of A, which scores extraction set Rn(A), decom-
poses over AL and AR, along with any phrasal
bispans licensed by adjoining AL and AR.
? ? ?(A) = ? ? ?(AL) + ? ? ?(AR) + I(AL,AR)
where I(AL,AR) is ? ?
?
?(g, h, k, l) summed
over licensed bispans [g, h) ? [k, `) that overlap
the boundary between AL and AR.5
4We abuse notation in conflating an alignment A with its
derivation. All derivations of the same alignment receive the
same score, and we only compute the max, not the sum.
5We focus on the case of adjoining two aligned bispans.
Our algorithm easily extends to include null alignments, but
we focus on the non-null setting for simplicity.
1457
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
(past)
k =2
l =4
g =1
h =3
Figure 6: Augmenting the ITG grammar states
with the alignment configuration in an n? 1 deep
perimeter of the bispan allows us to score all over-
lapping phrasal rules introduced by adjoining two
bispans. The state must encode whether a sure link
appears in each edge column or row, but the spe-
cific location of edge links is not required.
In order to compute I(AL,AR), we need cer-
tain information about the alignment configura-
tions of AL and AR where they adjoin at a corner.
The state must represent (a) the specific alignment
links in the n ? 1 deep corner of each A, and (b)
whether any sure alignments appear in the rows or
columns extending from those corners.6 With this
information, we can infer the bispans licensed by
adjoining AL and AR, as in Figure 6.
Applying our score recurrence yields a
polynomial-time dynamic program. This dynamic
program is an instance of ITG bitext parsing,
where the grammar uses symbols to encode
the alignment contexts described above. This
context-as-symbol augmentation of the grammar
is similar in character to augmenting symbols with
lexical items to score language models during
hierarchical decoding (Chiang, 2007).
4.2 Coarse-to-Fine Inference and Pruning
Exhaustive inference under an ITG requiresO(k6)
time in sentence length k, and is prohibitively slow
when there is no sparsity in the grammar. Main-
taining the context necessary to score non-local
bispans further increases running time. That is,
ITG inference is organized around search states
associated with a grammar symbol and a bispan;
augmenting grammar symbols also augments this
state space.
To parse quickly, we prune away search states
using predictions from the more efficient HMM
6The number of configuration states does not depend on
the size ofA because corners have fixed size, and because the
position of links within rows or columns is not needed.
alignment model (Ney and Vogel, 1996). We dis-
card all states corresponding to bispans that are
incompatible with 3 or more alignment links un-
der an intersected HMM?a proven approach to
pruning the space of ITG alignments (Zhang and
Gildea, 2006; Haghighi et al, 2009). Pruning in
this way reduces the search space dramatically, but
only rarely prohibits correct alignments. The ora-
cle alignment error rate for the block ITG model
class is 1.4%; the oracle alignment error rate for
this pruned subset of ITG is 2.0%.
To take advantage of the sparsity that results
from pruning, we use an agenda-based parser that
orders search states from small to large, where we
define the size of a bispan as the total number of
words contained within it. For each size, we main-
tain a separate agenda. Only when the agenda for
size k is exhausted does the parser proceed to pro-
cess the agenda for size k + 1.
We also employ coarse-to-fine search to speed
up inference (Charniak and Caraballo, 1998). In
the coarse pass, we search over the space of ITG
alignments, but score only features on alignment
links and bispans that are local to terminal blocks.
This simplification eliminates the need to augment
grammar symbols, and so we can exhaustively ex-
plore the (pruned) space. We then compute out-
side scores for bispans under a max-sum semir-
ing (Goodman, 1996). In the fine pass with the
full extraction set model, we impose a maximum
size of 10,000 for each agenda. We order states on
agendas by the sum of their inside score under the
full model and the outside score computed in the
coarse pass, pruning all states not within the fixed
agenda beam size.
Search states that are popped off agendas are
indexed by their corner locations for fast look-
up when constructing new states. For each cor-
ner and size combination, built states are main-
tained in sorted order according to their inside
score. This ordering allows us to stop combin-
ing states early when the results are falling off the
agenda beams. Similar search and beaming strate-
gies appear in many decoders for machine trans-
lation (Huang and Chiang, 2007; Koehn and Had-
dow, 2009; Moore and Quirk, 2007).
4.3 Finding Pseudo-Gold ITG Alignments
Equation 3 asks for the block ITG alignment
Ag that is closest to a reference alignment At,
which may not lie in ITG(e,f). We search for
1458
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =1
l =4
g =0
h =3
or
Figure 7: A* search for pseudo-gold ITG align-
ments uses an admissible heuristic for bispans that
counts the number of gold links outside of [k, `)
but within [g, h). Above, the heuristic is 1, which
is also the minimal number of alignment errors
that an ITG alignment will incur using this bispan.
Ag using A* bitext parsing (Klein and Manning,
2003). Search states, which correspond to bispans
[g, h)? [k, `), are scored by the number of errors
within the bispan plus the number of (i, j) ? At
such that j ? [k, `) but i /? [g, h) (recall errors).
As an admissible heuristic for the future cost of
a bispan [g, h) ? [k, `), we count the number of
(i, j) ? At such that i ? [g, h) but j /? [k, `), as
depicted in Figure 7. These links will become re-
call errors eventually. A* search with this heuristic
makes no errors, and the time required to compute
pseudo-gold alignments is negligible.
5 Relationship to Previous Work
Our model is certainly not the first alignment ap-
proach to include structures larger than words.
Model-based phrase-to-phrase alignment was pro-
posed early in the history of phrase-based trans-
lation as a method for training translation models
(Marcu and Wong, 2002). A variety of unsuper-
vised models refined this initial work with priors
(DeNero et al, 2008; Blunsom et al, 2009) and
inference constraints (DeNero et al, 2006; Birch
et al, 2006; Cherry and Lin, 2007; Zhang et al,
2008). These models fundamentally differ from
ours in that they stipulate a segmentation of the
sentence pair into phrases, and only align the min-
imal phrases in that segmentation. Our model
scores the larger overlapping phrases that result
from composing these minimal phrases.
Discriminative alignment is also a well-
explored area. Most work has focused on pre-
dicting word alignments via partial matching in-
ference algorithms (Melamed, 2000; Taskar et al,
2005; Moore, 2005; Lacoste-Julien et al, 2006).
Work in semi-supervised estimation has also con-
tributed evidence that hand-annotations are useful
for training alignment models (Fraser and Marcu,
2006; Fraser and Marcu, 2007). The ITG gram-
mar formalism, the corresponding word alignment
class, and inference procedures for the class have
also been explored extensively (Wu, 1997; Zhang
and Gildea, 2005; Cherry and Lin, 2007; Zhang
et al, 2008). At the intersection of these lines of
work, discriminative ITG models have also been
proposed, including one-to-one alignment mod-
els (Cherry and Lin, 2006) and block models
(Haghighi et al, 2009). Our model directly ex-
tends this research agenda with first-class possi-
ble links, overlapping phrasal rule features, and an
extraction-level loss function.
Ka?a?ria?inen (2009) trains a translation model
discriminatively using features on overlapping
phrase pairs. That work differs from ours in
that it uses fixed word alignments and focuses on
translation model estimation, while we focus on
alignment and translate using standard relative fre-
quency estimators.
Deng and Zhou (2009) present an alignment
combination technique that uses phrasal features.
Our approach differs in two ways. First, their ap-
proach is tightly coupled to the input alignments,
while we perform a full search over the space of
ITG alignments. Also, their approach uses greedy
search, while our search is optimal aside from
pruning and beaming. Despite these differences,
their strong results reinforce our claim that phrase-
level information is useful for alignment.
6 Experiments
We evaluate our extraction set model by the bis-
pans it predicts, the word alignments it generates,
and the translations generated by two end-to-end
systems. Table 1 compares the five systems de-
scribed below, including three baselines. All su-
pervised aligners were optimized for bispan F5.
Unsupervised Baseline: GIZA++. We trained
GIZA++ (Och and Ney, 2003) using the default
parameters included with the Moses training script
(Koehn et al, 2007). The designated regimen con-
cludes by Viterbi aligning under Model 4 in both
directions. We combined these alignments with
1459
the grow-diag heuristic (Koehn et al, 2003).
Unsupervised Baseline: Joint HMM. We
trained and combined two HMM alignment mod-
els (Ney and Vogel, 1996) using the Berkeley
Aligner.7 We initialized the HMM model pa-
rameters with jointly trained Model 1 param-
eters (Liang et al, 2006), combined word-to-
word posteriors by averaging (soft union), and de-
coded with the competitive thresholding heuristic
of DeNero and Klein (2007), yielding a state-of-
the-art unsupervised baseline.
Supervised Baseline: Block ITG. We discrimi-
natively trained a block ITG aligner with only sure
links, using block terminal productions up to 3
words by 3 words in size. This supervised base-
line is a reimplementation of the MIRA-trained
model of Haghighi et al (2009). We use the same
features and parser implementation for this model
as we do for our extraction set model to ensure a
clean comparison. To remain within the alignment
class, MIRA updates this model toward a pseudo-
gold alignment with only sure links. This model
does not score any overlapping bispans.
Extraction Set Coarse Pass. We add possible
links to the output of the block ITG model by
adding the mixed terminal block productions de-
scribed in Section 2.3. This model scores over-
lapping phrasal rules contained within terminal
blocks that result from including or excluding pos-
sible links. However, this model does not score
bispans that cross bracketing of ITG derivations.
Full Extraction Set Model. Our full model in-
cludes possible links and features on extraction
sets for phrasal bispans with a maximum size of
3. Model inference is performed using the coarse-
to-fine scheme described in Section 4.2.
6.1 Data
In this paper, we focus exclusively on Chinese-to-
English translation. We performed our discrimi-
native training and alignment evaluations using a
hand-aligned portion of the NIST MT02 test set,
which consists of 150 training and 191 test sen-
tences (Ayan and Dorr, 2006). We trained the
baseline HMM on 11.3 million words of FBIS
newswire data, a comparable dataset to those used
in previous alignment evaluations on our test set
(DeNero and Klein, 2007; Haghighi et al, 2009).
7http://code.google.com/p/berkeleyaligner
Our end-to-end translation experiments were
tuned and evaluated on sentences up to length 40
from the NIST MT04 and MT05 test sets. For
these experiments, we trained on a 22.1 million
word parallel corpus consisting of sentences up to
length 40 of newswire data from the GALE pro-
gram, subsampled from a larger data set to pro-
mote overlap with the tune and test sets. This cor-
pus also includes a bilingual dictionary. To im-
prove performance, we retrained our aligner on a
retokenized version of the hand-annotated data to
match the tokenization of our corpus.8 We trained
a language model with Kneser-Ney smoothing
on 262 million words of newswire using SRILM
(Stolcke, 2002).
6.2 Word and Phrase Alignment
The first panel of Table 1 gives a word-level eval-
uation of all five aligners. We use the alignment
error rate (AER) measure: precision is the frac-
tion of sure links in the system output that are sure
or possible in the reference, and recall is the frac-
tion of sure links in the reference that the system
outputs as sure. For this evaluation, possible links
produced by our extraction set models are ignored.
The full extraction set model performs the best by
a small margin, although it was not tuned for word
alignment.
The second panel gives a phrasal rule-level
evaluation, which measures the degree to which
these aligners matched the extraction sets of hand-
annotated alignments, R3(At).9 To compete
fairly, all models were evaluated on the full ex-
traction sets induced by the word alignments they
predicted. Again, the extraction set model outper-
formed the baselines, particularly on the F5 mea-
sure for which these systems were trained.
Our coarse pass extraction set model performed
nearly as well as the full model. We believe
these models perform similarly for two reasons.
First, most of the information needed to predict
an extraction set can be inferred from word links
and phrasal rules contained within ITG terminal
productions. Second, the coarse-to-fine inference
may be constraining the full phrasal model to pre-
dict similar output to the coarse model. This simi-
larity persists in translation experiments.
8All alignment results are reported under the annotated
data set?s original tokenization.
9While pseudo-gold approximations to the annotation
were used for training, the evaluation is always performed
relative to the original human annotation.
1460
Word Bispan BLEU
Pr Rc AER Pr Rc F1 F5 Joshua Moses
Baseline GIZA++ 72.5 71.8 27.8 69.4 45.4 54.9 46.0 33.8 32.6
models Joint HMM 84.0 76.9 19.6 69.5 59.5 64.1 59.9 34.5 33.2
Block ITG 83.4 83.8 16.4 75.8 62.3 68.4 62.8 34.7 33.6
Extraction Coarse Pass 82.2 84.2 16.9 70.0 72.9 71.4 72.8 35.7 34.2
set models Full Model 84.7 84.0 15.6 69.0 74.2 71.6 74.0 35.9 34.4
Table 1: Experimental results demonstrate that the full extraction set model outperforms supervised and
unsupervised baselines in evaluations of word alignment quality, extraction set quality, and translation.
In word and bispan evaluations, GIZA++ did not have access to a dictionary while all other methods
did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The
BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate
for parse failures.
6.3 Translation Experiments
We evaluate the alignments predicted by our
model using two publicly available, open-source,
state-of-the-art translation systems. Moses is a
phrase-based system with lexicalized reordering
(Koehn et al, 2007). Joshua (Li et al, 2009) is
an implementation of Hiero (Chiang, 2007) using
a suffix-array-based grammar extraction approach
(Lopez, 2007).
Both of these systems take word alignments as
input, and neither of these systems accepts possi-
ble links in the alignments they consume. To inter-
face with our extraction set models, we produced
three sets of sure-only alignments from our model
predictions: one that omitted possible links, one
that converted all possible links to sure links, and
one that includes each possible link with 0.5 prob-
ability. These three sets were aggregated and rules
were extracted from all three.
The training set we used for MT experiments
is quite heterogenous and noisy compared to our
alignment test sets, and the supervised aligners
did not handle certain sentence pairs in our par-
allel corpus well. In some cases, pruning based
on consistency with the HMM caused parse fail-
ures, which in turn caused training sentences to be
skipped. To account for these issues, we added
counts of phrasal rules extracted from the baseline
HMM to the counts produced by supervised align-
ers.
In Moses, our extraction set model predicts the
set of phrases extracted by the system, and so the
estimation techniques for the alignment model and
translation model both share a common underly-
ing representation: extraction sets. Empirically,
we observe a BLEU score improvement of 1.2
over the best unsupervised baseline and 0.8 over
the block ITG supervised baseline (Papineni et al,
2002).
In Joshua, hierarchical rule extraction is based
upon phrasal rule extraction, but abstracts away
sub-phrases to create a grammar. Hence, the ex-
traction sets we predict are closely linked to the
representation that this system uses to translate.
The extraction model again outperformed both un-
supervised and supervised baselines, by 1.4 BLEU
and 1.2 BLEU respectively.
7 Conclusion
Our extraction set model serves to coordinate the
alignment and translation model components of a
statistical translation system by unifying their rep-
resentations. Moreover, our model provides an ef-
fective alternative to phrase alignment models that
choose a particular phrase segmentation; instead,
we predict many overlapping phrases, both large
and small, that are mutually consistent. In future
work, we look forward to developing extraction
set models for richer formalisms, including hier-
archical grammars.
Acknowledgments
This project is funded in part by BBN under
DARPA contract HR0011-06-C-0022 and by the
NSF under grant 0643742. We thank the anony-
mous reviewers for their helpful comments.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
1461
the Annual Conference of the Association for Com-
putational Linguistics.
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Neuralign: combining word alignments us-
ing neural networks. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings of the Conference for the Association for Ma-
chine Translation in the Americas.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics.
Eugene Charniak and Sharon Caraballo. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. In Computational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of the Annual Confer-
ence of the Association for Computational Linguis-
tics.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proceedings of the Annual Conference of
the North American Chapter of the Association for
Computational Linguistics Workshop on Syntax and
Structure in Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the Annual Conference of the Associa-
tion for Computational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics: Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models un-
derperform surface heuristics. In Proceedings of the
NAACL Workshop on Statistical Machine Transla-
tion.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table train-
ing. In Proceedings of the Annual Conference of the
Association for Computational Linguistics: Short
Paper Track.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the Annual Conference of the Asso-
ciation for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Getting
the structure right for word alignment: Leaf. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the Annual Conference of the Associa-
tion for Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Annual
Conference of the Association for Computational
Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the Annual Conference of the
Association for Computational Linguistics.
Matti Ka?a?ria?inen. 2009. Sinuhe?statistical machine
translation using a globally trained conditional ex-
ponential family translation model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Dan Klein and Chris Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburghs
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In Proceedings of the Workshop on Statis-
tical Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
1462
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Annual Conference of the Associ-
ation for Computational Linguistics: Demonstration
track.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Daniel Marcu and Daniel Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Robert Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In Proceedings of MT Summit XI.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Hermann Ney and Stephan Vogel. 1996. HMM-based
word alignment in statistical translation. In Pro-
ceedings of the Conference on Computational lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19?51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statisti-
cal machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the Annual Conference of the Association for Com-
putational Linguistics.
Andreas Stolcke. 2002. Srilm an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the Annual Conference of
the Association for Computational Linguistics.
Hao Zhang and Daniel Gildea. 2006. Efficient search
for inversion transduction grammar. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the Annual Conference of the Asso-
ciation for Computational Linguistics.
1463
Proceedings of the ACL 2010 Conference Short Papers, pages 200?204,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Top-Down K-Best A? Parsing
Adam Pauls and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
Chris Quirk
Microsoft Research
Redmond, WA, 98052
chrisq@microsoft.com
Abstract
We propose a top-down algorithm for ex-
tracting k-best lists from a parser. Our
algorithm, TKA? is a variant of the k-
best A? (KA?) algorithm of Pauls and
Klein (2009). In contrast to KA?, which
performs an inside and outside pass be-
fore performing k-best extraction bottom
up, TKA? performs only the inside pass
before extracting k-best lists top down.
TKA? maintains the same optimality and
efficiency guarantees of KA?, but is sim-
pler to both specify and implement.
1 Introduction
Many situations call for a parser to return a k-
best list of parses instead of a single best hypothe-
sis.1 Currently, there are two efficient approaches
known in the literature. The k-best algorithm of
Jime?nez and Marzal (2000) and Huang and Chi-
ang (2005), referred to hereafter as LAZY, oper-
ates by first performing an exhaustive Viterbi in-
side pass and then lazily extracting k-best lists in
top-down manner. The k-best A? algorithm of
Pauls and Klein (2009), hereafter KA?, computes
Viterbi inside and outside scores before extracting
k-best lists bottom up.
Because these additional passes are only partial,
KA? can be significantly faster than LAZY, espe-
cially when a heuristic is used (Pauls and Klein,
2009). In this paper, we propose TKA?, a top-
down variant of KA? that, like LAZY, performs
only an inside pass before extracting k-best lists
top-down, but maintains the same optimality and
efficiency guarantees as KA?. This algorithm can
be seen as a generalization of the lattice k-best al-
gorithm of Soong and Huang (1991) to parsing.
Because TKA? eliminates the outside pass from
KA?, TKA? is simpler both in implementation and
specification.
1See Huang and Chiang (2005) for a review.
2 Review
Because our algorithm is very similar to KA?,
which is in turn an extension of the (1-best) A?
parsing algorithm of Klein and Manning (2003),
we first introduce notation and review those two
algorithms before presenting our new algorithm.
2.1 Notation
Assume we have a PCFG2 G and an input sen-
tence s0 . . . sn?1 of length n. The grammar G has
a set of symbols denoted by capital letters, includ-
ing a distinguished goal (root) symbol G. With-
out loss of generality, we assume Chomsky nor-
mal form: each non-terminal rule r in G has the
form r = A ? B C with weight wr. Edges
are labeled spans e = (A, i, j). Inside deriva-
tions of an edge (A, i, j) are trees with root non-
terminalA, spanning si . . . sj?1. The weight (neg-
ative log-probability) of the best (minimum) inside
derivation for an edge e is called the Viterbi in-
side score ?(e), and the weight of the best deriva-
tion of G ? s0 . . . si?1 A sj . . . sn?1 is called
the Viterbi outside score ?(e). The goal of a k-
best parsing algorithm is to compute the k best
(minimum weight) inside derivations of the edge
(G, 0, n).
We formulate the algorithms in this paper
in terms of prioritized weighted deduction rules
(Shieber et al, 1995; Nederhof, 2003). A prior-
itized weighted deduction rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
2While we present the algorithm specialized to parsing
with a PCFG, this algorithm generalizes to a wide range of
200
VP
s
2
s
3
s
4
s
0
s
2
... s
5
s
n-1
...
VP
VBZ NP
DT NN
s
2
s
3
s
4
VP
G
(a) (b)
(c)
VP
VP NP
s
1
s
2
s
n-1
(d) G
s
0
NN
NP
Figure 1: Representations of the different types of items
used in parsing. (a) An inside edge item I(VP, 2, 5). (b)
An outside edge item O(VP, 2, 5). (c) An inside deriva-
tion item: D(TVP, 2, 5). (d) An outside derivation item:
Q(TGVP, 1, 2, {(NP, 2, n)}. The edges in boldface are fron-
tier edges.
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item ? from the
agenda, put it into the chart with its current weight,
and apply deduction rules to form any items which
can be built by combining ? with items already
in the chart. When the resulting items are either
new or have a weight smaller than an item?s best
score so far, they are put on the agenda with pri-
ority given by p(?). Because all antecedents must
be constructed before a deduction rule is executed,
we sometimes refer to particular conclusion item
as ?waiting? on another item before it can be built.
2.2 A?
A? parsing (Klein and Manning, 2003) is an al-
gorithm for computing the 1-best parse of a sen-
tence. A? operates on items called inside edge
items I(A, i, j), which represent the many pos-
sible inside derivations of an edge (A, i, j). In-
side edge items are constructed according to the
IN deduction rule of Table 1. This deduction rule
constructs inside edge items in a bottom-up fash-
ion, combining items representing smaller edges
I(B, i, k) and I(C, k, j) with a grammar rule r =
A ? B C to form a larger item I(A, i, j). The
weight of a newly constructed item is given by the
sum of the weights of the antecedent items and
the grammar rule r, and its priority is given by
hypergraph search problems as shown in Klein and Manning
(2001).
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP NN
(a)
(b)
Figure 2: (a) An outside derivation item before expansion at
the edge (VP, 1, 4). (b) A possible expansion of the item in
(a) using the rule VP? VP NN. Frontier edges are marked in
boldface.
its weight plus a heuristic h(A, i, j). For consis-
tent and admissible heuristics h(?), this deduction
rule guarantees that when an inside edge item is
removed from the agenda, its current weight is its
true Viterbi inside score.
The heuristic h controls the speed of the algo-
rithm. It can be shown that an edge e satisfying
?(e) + h(A, i, j) > ?(G, 0, n) will never be re-
moved from the agenda, allowing some edges to
be safely pruned during parsing. The more closely
h(e) approximates the Viterbi outside cost ?(e),
the more items are pruned.
2.3 KA?
The use of inside edge items in A? exploits the op-
timal substructure property of derivations ? since
a best derivation of a larger edge is always com-
posed of best derivations of smaller edges, it is
only necessary to compute the best way of build-
ing a particular inside edge item. When finding
k-best lists, this is no longer possible, since we are
interested in suboptimal derivations.
Thus, KA?, the k-best extension of A?, must
search not in the space of inside edge items,
but rather in the space of inside derivation items
D(TA, i, j), which represent specific derivations
of the edge (A, i, j) using tree TA. However, the
number of inside derivation items is exponential
in the length of the input sentence, and even with
a very accurate heuristic, running A? directly in
this space is not feasible.
Fortunately, Pauls and Klein (2009) show that
with a perfect heuristic, that is, h(e) = ?(e) ?e,
A? search on inside derivation items will only
remove items from the agenda that participate
in the true k-best lists (up to ties). In order
to compute this perfect heuristic, KA? makes
use of outside edge items O(A, i, j) which rep-
resent the many possible derivations of G ?
201
IN??: I(B, i, l) : w1 I(C, l, j) : w2
w1+w2+wr+h(A,i,j)??????????????? I(A, i, j) : w1 + w2 + wr
IN-D?: O(A, i, j) : w1 D(TB , i, l) : w2 D(TC , l, j) : w3
w2+w3+wr+w1??????????? D(TA, i, j) : w2 + w3 + wr
OUT-L?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w3+wr+w2??????????? O(B, i, l) : w1 + w3 + wr
OUT-R?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w2+wr+w3??????????? O(C, l, j) : w1 + w2 + wr
OUT-D?: Q(TGA , i, j,F) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+wr+w2+w3+?(F)???????????????? Q(TGB , i, l,FC) : w1 + wr
Table 1: The deduction rules used in this paper. Here, r is the rule A ? B C. A superscript * indicates that the rule is used
in TKA?, and a superscript ? indicates that the rule is used in KA?. In IN-D, the tree TA is rooted at (A, i, j) and has children
TB and TC . In OUT-D, the tree TGB is the tree T
G
A extended at (A, i, j) with rule r, FC is the list F with (C, l, j) prepended,
and ?(F) is
P
e?F ?(e). Whenever the left child I(B, i, l) of an application of OUT-D represents a terminal, the next edge is
removed from F and is used as the new point of expansion.
s1 . . . si A sj+1 . . . sn (see Figure 1(b)).
Outside items are built using the OUT-L and
OUT-R deduction rules shown in Table 1. OUT-
L and OUT-R combine, in a top-down fashion, an
outside edge over a larger span and inside edge
over a smaller span to form a new outside edge
over a smaller span. Because these rules make ref-
erence to inside edge items I(A, i, j), these items
must also be built using the IN deduction rules
from 1-best A?. Outside edge items must thus wait
until the necessary inside edge items have been
built. The outside pass is initialized with the item
O(G, 0, n) when the inside edge item I(G, 0, n) is
popped from the agenda.
Once we have started populating outside scores
using the outside deductions, we can initiate a
search on inside derivation items.3 These items
are built bottom-up using the IN-D deduction rule.
The crucial element of this rule is that derivation
items for a particular edge wait until the exact out-
side score of that edge has been computed. The al-
gorithm terminates when k derivation items rooted
at (G, 0, n) have been popped from the agenda.
3 TKA?
KA? efficiently explores the space of inside
derivation items because it waits for the exact
Viterbi outside cost before building each deriva-
tion item. However, these outside costs and asso-
ciated deduction items are only auxiliary quanti-
ties used to guide the exploration of inside deriva-
tions: they allow KA? to prioritize currently con-
structed inside derivation items (i.e., constructed
derivations of the goal) by their optimal comple-
tion costs. Outside costs are thus only necessary
because we construct partial derivations bottom-
up; if we constructed partial derivations in a top-
down fashion, all we would need to compute opti-
3We stress that the order of computation is entirely speci-
fied by the deduction rules ? we only speak about e.g. ?initi-
ating a search? as an appeal to intuition.
mal completion costs are Viterbi inside scores, and
we could forget the outside pass.
TKA? does exactly that. Inside edge items are
constructed in the same way as KA?, but once the
inside edge item I(G, 0, n) has been discovered,
TKA? begins building partial derivations from the
goal outwards. We replace the inside derivation
items of KA? with outside derivation items, which
represent trees rooted at the goal and expanding
downwards. These items bottom out in a list of
edges called the frontier edges. See Figure 1(d)
for a graphical representation. When a frontier
edge represents a single word in the input, i.e. is
of the form (si, i, i+ 1), we say that edge is com-
plete. An outside derivation can be expanded by
applying a rule to one of its incomplete frontier
edges; see Figure 2. In the same way that inside
derivation items wait on exact outside scores be-
fore being built, outside derivation items wait on
the inside edge items of all frontier edges before
they can be constructed.
Although building derivations top-down obvi-
ates the need for a 1-best outside pass, it raises a
new issue. When building derivations bottom-up,
the only way to expand a particular partial inside
derivation is to combine it with another partial in-
side derivation to build a bigger tree. In contrast,
an outside derivation item can be expanded any-
where along its frontier. Naively building deriva-
tions top-down would lead to a prohibitively large
number of expansion choices.
We solve this issue by always expanding the
left-most incomplete frontier edge of an outside
derivation item. We show the deduction rule
OUT-D which performs this deduction in Fig-
ure 1(d). We denote an outside derivation item as
Q(TGA , i, j,F), where T
G
A is a tree rooted at the
goal with left-most incomplete edge (A, i, j), and
F is the list of incomplete frontier edges exclud-
ing (A, i, j), ordered from left to right. Whenever
the application of this rule ?completes? the left-
202
most edge, the next edge is removed from F and
is used as the new point of expansion. Once all
frontier edges are complete, the item represents a
correctly scored derivation of the goal, explored in
a pre-order traversal.
3.1 Correctness
It should be clear that expanding the left-most in-
complete frontier edge first eventually explores the
same set of derivations as expanding all frontier
edges simultaneously. The only worry in fixing
this canonical order is that we will somehow ex-
plore the Q items in an incorrect order, possibly
building some complete derivation Q?C before a
more optimal complete derivation QC . However,
note that all items Q along the left-most construc-
tion ofQC have priority equal to or better than any
less optimal complete derivation Q?C . Therefore,
when Q?C is enqueued, it will have lower priority
than all Q; Q?C will therefore not be dequeued un-
til all Q ? and hence QC ? have been built.
Furthermore, it can be shown that the top-down
expansion strategy maintains the same efficiency
and optimality guarantees as KA? for all item
types: for consistent heuristics h, the first k en-
tirely complete outside derivation items are the
true k-best derivations (modulo ties), and that only
derivation items which participate in those k-best
derivations will be removed from the queue (up to
ties).
3.2 Implementation Details
Building derivations bottom-up is convenient from
an indexing point of view: since larger derivations
are built from smaller ones, it is not necessary to
construct the larger derivation from scratch. In-
stead, one can simply construct a new tree whose
children point to the old trees, saving both mem-
ory and CPU time.
In order keep the same efficiency when build-
ing trees top-down, a slightly different data struc-
ture is necessary. We represent top-down deriva-
tions as a lazy list of expansions. The top node
TGG is an empty list, and whenever we expand an
outside derivation item Q(TGA , i, j,F) with a rule
r = A ? B C and split point l, the resulting
derivation TGB is a new list item with (r, l) as the
head data, and TGA as its tail. The tree can be re-
constructed later by recursively reconstructing the
parent, and adding the edges (B, i, l) and (C, l, j)
as children of (A, i, j).
3.3 Advantages
Although our algorithm eliminates the 1-best out-
side pass of KA?, in practice, even for k = 104,
the 1-best inside pass remains the overwhelming
bottleneck (Pauls and Klein, 2009), and our modi-
fications leave that pass unchanged.
However, we argue that our implementation is
simpler to specify and implement. In terms of de-
duction rules, our algorithm eliminates the 2 out-
side deduction rules and replaces the IN-D rule
with the OUT-D rule, bringing the total number
of rules from four to two.
The ease of specification translates directly into
ease of implementation. In particular, if high-
quality heuristics are not available, it is often more
efficient to implement the 1-best inside pass as
an exhaustive dynamic program, as in Huang and
Chiang (2005). In this case, one would only need
to implement a single, agenda-based k-best extrac-
tion phase, instead of the 2 needed for KA?.
3.4 Performance
The contribution of this paper is theoretical, not
empirical. We have argued that TKA? is simpler
than TKA?, but we do not expect it to do any more
or less work than KA?, modulo grammar specific
optimizations. Therefore, we simply verify, like
KA?, that the additional work of extracting k-best
lists with TKA? is negligible compared to the time
spent building 1-best inside edges.
We examined the time spent building 100-best
lists for the same experimental setup as Pauls and
Klein (2009).4 On 100 sentences, our implemen-
tation of TKA? constructed 3.46 billion items, of
which about 2% were outside derivation items.
Our implementation of KA? constructed 3.41 bil-
lion edges, of which about 0.1% were outside edge
items or inside derivation items. In other words,
the cost of k-best extraction is dwarfed by the
the 1-best inside edge computation in both cases.
The reason for the slight performance advantage
of KA? is that our implementation of KA? uses
lazy optimizations discussed in Pauls and Klein
(2009), and while such optimizations could easily
be incorporated in TKA?, we have not yet done so
in our implementation.
4This setup used 3- and 6-round state-split grammars from
Petrov et al (2006), the former used to compute a heuristic
for the latter, tested on sentences of length up to 25.
203
4 Conclusion
We have presented TKA?, a simplification to the
KA? algorithm. Our algorithm collapses the 1-
best outside and bottom-up derivation passes of
KA? into a single, top-down pass without sacri-
ficing efficiency or optimality. This reduces the
number of non base-case deduction rules, making
TKA? easier both to specify and implement.
Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
References
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53?64.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of the Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 123?134.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computationl Linguis-
tics, 29(1):135?143.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Proccedings of the Association for Computational
Linguistics (ACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Frank K. Soong and Eng-Fong Huang. 1991. A tree-
trellis based fast search for finding the n best sen-
tence hypotheses in continuous speech recognition.
In Proceedings of the Workshop on Speech and Nat-
ural Language.
204
Proceedings of the ACL 2010 Conference Short Papers, pages 291?295,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
An Entity-Level Approach to Information Extraction
Aria Haghighi
UC Berkeley, CS Division
aria42@cs.berkeley.edu
Dan Klein
UC Berkeley, CS Division
klein@cs.berkeley.edu
Abstract
We present a generative model of
template-filling in which coreference
resolution and role assignment are jointly
determined. Underlying template roles
first generate abstract entities, which in
turn generate concrete textual mentions.
On the standard corporate acquisitions
dataset, joint resolution in our entity-level
model reduces error over a mention-level
discriminative approach by up to 20%.
1 Introduction
Template-filling information extraction (IE) sys-
tems must merge information across multiple sen-
tences to identify all role fillers of interest. For
instance, in the MUC4 terrorism event extrac-
tion task, the entity filling the individual perpetra-
tor role often occurs multiple times, variously as
proper, nominal, or pronominal mentions. How-
ever, most template-filling systems (Freitag and
McCallum, 2000; Patwardhan and Riloff, 2007)
assign roles to individual textual mentions using
only local context as evidence, leaving aggrega-
tion for post-processing. While prior work has
acknowledged that coreference resolution and dis-
course analysis are integral to accurate role identi-
fication, to our knowledge no model has been pro-
posed which jointly models these phenomena.
In this work, we describe an entity-centered ap-
proach to template-filling IE problems. Our model
jointly merges surface mentions into underlying
entities (coreference resolution) and assigns roles
to those discovered entities. In the generative pro-
cess proposed here, document entities are gener-
ated for each template role, along with a set of
non-template entities. These entities then generate
mentions in a process sensitive to both lexical and
structural properties of the mention. Our model
outperforms a discriminative mention-level base-
line. Moreover, since our model is generative, it
[S CSR] has said that [S it] has sold [S its]  [B oil 
interests] held in  [A Delhi Fund].  [P Esso Inc.] did not 
disclose how much [P they] paid for [A Dehli].
(a)
(b)
Document
Esso Inc.
PURCHASERACQUIRED
Delhi FundOil and Gas
BUSINESS
CSR Limited
SELLER
Template
Figure 1: Example of the corporate acquisitions role-filling
task. In (a), an example template specifying the entities play-
ing each domain role. In (b), an example document with
coreferent mentions sharing the same role label. Note that
pronoun mentions provide direct clues to entity roles.
can naturally incorporate unannotated data, which
further increases accuracy.
2 Problem Setting
Figure 1(a) shows an example template-filling
task from the corporate acquisitions domain (Fre-
itag, 1998).1 We have a template of K roles
(PURCHASER, AMOUNT, etc.) and we must iden-
tify which entity (if any) fills each role (CSR Lim-
ited, etc.). Often such problems are modeled at the
mention level, directly labeling individual men-
tions as in Figure 1(b). Indeed, in this data set,
the mention-level perspective is evident in the gold
annotations, which ignore pronominal references.
However, roles in this domain appear in several lo-
cations throughout the document, with pronominal
mentions often carrying the critical information
for template filling. Therefore, Section 3 presents
a model in which entities are explicitly modeled,
naturally merging information across all mention
types and explicitly representing latent structure
very much like the entity-level template structure
from Figure 1(a).
1In Freitag (1998), some of these fields are split in two to
distinguish a full versus abbreviated name, but we ignore this
distinction. Also we ignore the status field as it doesn?t apply
to entities and its meaning is not consistent.
291
R1 R2 RK
Z1 Z2 Zn
M1 M2 Mn...........
Document
Role Entity Parameters
Mentions
?
Role 
Priors
E1 E2
M3
Z3 ...........
EK
Other 
Entities
.... ....
Other Entity Parameters
.... ....
Entity Indicators
1
[1: 0.02, 
  0:0.015,
     2: 0.01,...]
MOD-APPOS
[company: 0.02, 
 firm:0.015,
  group: 0.01,...]
[1: 0.19, 
2:0.14,
     0: 0.08,...]
HEAD-NAM
[Inc.: 0.02, 
 Corp.:0.015,
  Ltd.: 0.01,...]
[2: 0.18, 
3:0.12,
     1: 0.09,...]
GOV-NSUBJ
fr?rr
[bought: 0.02, 
 obtained:0.015,
acquired: 0.01,...]
Purchaser Role
Role
Entites
CaliforniaMOD-PREP
MOD-NN search, giant
companyHEAD-NOM
HEAD-NAM
Lrr
Google, GOOG
Purchaser Entity
GOV-NSUBJ bought
HEAD-NAM Google
wr
Purchaser Mention
Figure 2: Graphical model depiction of our generative model described in Section 3. Sample values are illustrated for key
parameters and latent variables.
3 Model
We describe our generative model for a document,
which has many similarities to the coreference-
only model of Haghighi and Klein (2010), but
which integrally models template role-fillers. We
briefly describe the key abstractions of our model.
Mentions: A mention is an observed textual
reference to a latent real-world entity. Mentions
are associated with nodes in a parse tree and are
typically realized as NPs. There are three ba-
sic forms of mentions: proper (NAM), nominal
(NOM), and pronominal (PRO). Each mention M
is represented as collection of key-value pairs.
The keys are called properties and the values are
words. The set of properties utilized here, de-
noted R, are the same as in Haghighi and Klein
(2010) and consist of the mention head, its depen-
dencies, and its governor. See Figure 2 for a con-
crete example. Mention types are trivially deter-
mined from mention head POS tag. All mention
properties and their values are observed.
Entities: An entity is a specific individual or
object in the world. Entities are always latent in
text. Where a mention has a single word for each
property, an entity has a list of signature words.
Formally, entities are mappings from properties
r ? R to lists Lr of ?canonical? words which that
entity uses for that property.
Roles: The elements we have described so far
are standard in many coreference systems. Our
model performs role-filling by assuming that each
entity is drawn from an underlying role. These
roles include theK template roles as well as ?junk?
roles to represent entities which do not fill a tem-
plate role (see Section 5.2). Each role R is rep-
resented as a mapping between properties r and
pairs of multinomials (?r, fr). ?r is a unigram dis-
tribution of words for property r that are seman-
tically licensed for the role (e.g., being the sub-
ject of ?acquired? for the ACQUIRED role). fr is a
?fertility? distribution over the integers that char-
acterizes entity list lengths. Together, these distri-
butions control the lists Lr for entities which in-
stantiate the role.
We first present a broad sketch of our model?s
components and then detail each in a subsequent
section. We temporarily assume that all men-
tions belong to a template role-filling entity; we
lift this restriction in Section 5.2. First, a se-
mantic component generates a sequence of enti-
ties E = (E1, . . . , EK), where each Ei is gen-
erated from a corresponding role Ri. We use
R = (R1, . . . , RK) to denote the vector of tem-
plate role parameters. Note that this work assumes
that there is a one-to-one mapping between entities
and roles; in particular, at most one entity can fill
each role. This assumption is appropriate for the
domain considered here.
Once entities have been generated, a dis-
course component generates which entities will be
evoked in each of the n mention positions. We
represent these choices using entity indicators de-
noted by Z = (Z1, . . . , Zn). This component uti-
lizes a learned global prior ? over roles. The Zi in-
292
dicators take values in 1, . . . ,K indicating the en-
tity number (and thereby the role) underlying the
ith mention position. Finally, a mention genera-
tion component renders each mention conditioned
on the underlying entity and role. Formally:
P (E,Z,M|R, ?) =
(
K?
i=1
P (Ei|Ri)
)
[Semantic, Sec. 3.1]
?
?
n?
j=1
P (Zj |Z<j , ?)
?
? [Discourse, Sec. 3.2]
?
?
n?
j=1
P (Mj |EZj , RZj )
?
? [Mention, Sec. 3.3]
3.1 Semantic Component
Each role R generates an entity E as follows: for
each mention property r, a word list, Lr, is drawn
by first generating a list length from the corre-
sponding fr distribution in R.2 This list is then
populated by an independent draw from R?s uni-
gram distribution ?r. Formally, for each r ? R, an
entity word list is drawn according to,3
P (Lr|R) = P (len(Lr)|fr)
?
w?Lr
P (w|?r)
3.2 Discourse Component
The discourse component draws the entity indica-
tor Zj for the jth mention according to,
P (Zj |Z<j , ?) =
{
P (Zj |?), if non-pronominal
?
j? 1[Zj = Zj? ]P (j?|j), o.w.
When the jth mention is non-pronominal, we draw
Zj from ?, a global prior over the K roles. When
Mj is a pronoun, we first draw an antecedent men-
tion position j?, such that j? < j, and then we set
Zj = Zj? . The antecedent position is selected ac-
cording to the distribution,
P (j?|j) ? exp{??TREEDIST(j?, j)}
where TREEDIST(j?,j) represents the tree distance
between the parse nodes forMj andMj? .4 Mass is
2There is one exception: the sizes of the proper and nom-
inal head property lists are jointly generated, but their word
lists are still independently populated.
3While, in principle, this process can yield word lists with
duplicate words, we constrain the model during inference to
not allow that to occur.
4Sentence parse trees are merged into a right-branching
document parse tree. This allows us to extend tree distance to
inter-sentence nodes.
restricted to antecedent mention positions j? which
occur earlier in the same sentence or in the previ-
ous sentence.5
3.3 Mention Generation
Once the entity indicator has been drawn, we gen-
erate words associated with mention conditioned
on the underlying entity E and role R. For each
mention property r associated with the mention,
a word w is drawn utilizing E?s word list Lr as
well as the multinomials (fr, ?r) from roleR. The
word w is drawn according to,
P (w|E,R)=(1? ?r)
1 [w ? Lr]
len(Lr)
+ ?rP (w|?r)
For each property r, there is a hyper-parameter ?r
which interpolates between selecting a word uni-
formly from the entity list Lr and drawing from
the underlying role distribution ?r. Intuitively, a
small ?r indicates that an entity prefers to re-use a
small number of words for property r. This is typi-
cally the case for proper and nominal heads as well
as modifiers. At the other extreme, setting ?r to 1
indicates the property isn?t particular to the entity
itself, but rather always drawn from the underly-
ing role distribution. We set ?r to 1 for pronoun
heads as well as for the governor properties.
4 Learning and Inference
Since we will make use of unannotated data (see
Section 5), we utilize a variational EM algorithm
to learn parameters R and ?. The E-Step re-
quires the posterior P (E,Z|R,M, ?), which is
intractable to compute exactly. We approximate
it using a surrogate variational distribution of the
following factored form:
Q(E,Z) =
(
K?
i=1
qi(Ei)
)?
?
n?
j=1
rj(Zj)
?
?
Each rj(Zj) is a distribution over the entity in-
dicator for mention Mj , which approximates the
true posterior of Zj . Similarly, qi(Ei) approxi-
mates the posterior over entity Ei which is asso-
ciated with role Ri. As is standard, we iteratively
update each component distribution to minimize
KL-divergence, fixing all other distributions:
qi ? argmin
qi
KL(Q(E,Z)|P (E,Z|M,R, ?)
? exp{EQ/qi lnP (E,Z|M,R, ?))}
5The sole parameter ? is fixed at 0.1.
293
Ment Acc. Ent. Acc.
INDEP 60.0 43.7
JOINT 64.6 54.2
JOINT+PRO 68.2 57.8
Table 1: Results on corporate acquisition tasks with given
role mention boundaries. We report mention role accuracy
and entity role accuracy (correctly labeling all entity men-
tions).
For example, the update for a non-pronominal
entity indicator component rj(?) is given by:6
ln rj(z) ? EQ/rj lnP (E,Z,M|R, ?)
? Eqz ln (P (z|?)P (Mj |Ez, Rz))
= lnP (z|?) + Eqz lnP (Mj |Ez, Rz)
A similar update is performed on pronominal en-
tity indicator distributions, which we omit here for
space. The update for variational entity distribu-
tion is given by:
ln qi(ei) ? EQ/qi lnP (E,Z,M|R, ?)
? E{rj} ln
?
?P (ei|Ri)
?
j:Zj=i
P (Mj |ei, Ri)
?
?
= lnP (ei|Ri) +
?
j
rj(i) lnP (Mj |ei, Ri)
It is intractable to enumerate all possible entities
ei (each consisting of several sets of words). We
instead limit the support of qi(ei) to several sam-
pled entities. We obtain entity samples by sam-
pling mention entity indicators according to rj .
For a given sample, we assume that Ei consists
of the non-pronominal head words and modifiers
of mentions such that Zj has sampled value i.
During the E-Step, we perform 5 iterations of
updating each variational factor, which results in
an approximate posterior distribution. Using ex-
pectations from this approximate posterior, our M-
Step is relatively straightforward. The role param-
eters Ri are computed from the qi(ei) and rj(z)
distributions, and the global role prior ? from the
non-pronominal components of rj(z).
5 Experiments
We present results on the corporate acquisitions
task, which consists of 600 annotated documents
split into a 300/300 train/test split. We use 50
training documents as a development set. In all
6For simplicity of exposition, we omit terms where Mj is
an antecedent to a pronoun.
documents, proper and (usually) nominal men-
tions are annotated with roles, while pronouns are
not. We preprocess each document identically to
Haghighi and Klein (2010): we sentence-segment
using the OpenNLP toolkit, parse sentences with
the Berkeley Parser (Petrov et al, 2006), and ex-
tract mention properties from parse trees and the
Stanford Dependency Extractor (de Marneffe et
al., 2006).
5.1 Gold Role Boundaries
We first consider the simplified task where role
mention boundaries are given. We map each la-
beled token span in training and test data to a parse
tree node that shares the same head. In this set-
ting, the role-filling task is a collective classifica-
tion problem, since we know each mention is fill-
ing some role.
As our baseline, INDEP, we built a maxi-
mum entropy model which independently classi-
fies each mention?s role. It uses features as similar
as possible to the generative model (and more), in-
cluding the head word, typed dependencies of the
head, various tree features, governing word, and
several conjunctions of these features as well as
coarser versions of lexicalized features. This sys-
tem yields 60.0 mention labeling accuracy (see Ta-
ble 1). The primary difficulty in classification is
the disambiguation amongst the acquired, seller,
and purchaser roles, which have similar internal
structure, and differ primarily in their semantic
contexts. Our entity-centered model, JOINT in Ta-
ble 1, has no latent variables at training time in this
setting, since each role maps to a unique entity.
This model yields 64.6, outperforming INDEP.7
During development, we noted that often the
most direct evidence of the role of an entity was
associated with pronoun usage (see the first ?it?
in Figure 1). Training our model with pronominal
mentions, whose roles are latent variables at train-
ing time, improves accuracy to 68.2.8
5.2 Full Task
We now consider the more difficult setting where
role mention boundaries are not provided at test
time. In this setting, we automatically extract
mentions from a parse tree using a heuristic ap-
7We use the mode of the variational posteriors rj(Zj) to
make predictions (see Section 4).
8While this approach incorrectly assumes that all pro-
nouns have antecedents amongst our given mentions, this did
not appear to degrade performance.
294
ROLE ID OVERALL
P R F1 P R F1
INDEP 79.0 65.5 71.6 48.6 40.3 44.0
JOINT+PRO 80.3 69.2 74.3 53.4 46.4 49.7
BEST 80.1 70.1 74.8 57.3 49.2 52.9
Table 2: Results on corporate acquisitions data where men-
tion boundaries are not provided. Systems must determine
which mentions are template role-fillers as well as label them.
ROLE ID only evaluates the binary decision of whether a
mention is a template role-filler or not. OVERALL includes
correctly labeling mentions. Our BEST system, see Sec-
tion 5, adds extra unannotated data to our JOINT+PRO sys-
tem.
proach. Our mention extraction procedure yields
95% recall over annotated role mentions and 45%
precision.9 Using extracted mentions as input, our
task is to label some subset of the mentions with
template roles. Since systems can label mentions
as non-role bearing, only recall is critical to men-
tion extraction. To adapt INDEP to this setting, we
first use a binary classifier trained to distinguish
role-bearing mentions. The baseline then classi-
fies mentions which pass this first phase as before.
We add ?junk? roles to our model to flexibly model
entities that do not correspond to annotated tem-
plate roles. During training, extracted mentions
which are not matched in the labeled data have
posteriors which are constrained to be amongst the
?junk? roles.
We first evaluate role identification (ROLE ID in
Table 2), the task of identifying mentions which
play some role in the template. The binary clas-
sifier for INDEP yields 71.6 F1. Our JOINT+PRO
system yields 74.3. On the task of identifying and
correctly labeling role mentions, our model out-
performs INDEP as well (OVERALL in Table 2). As
our model is generative, it is straightforward to uti-
lize totally unannotated data. We added 700 fully
unannotated documents from the mergers and ac-
quisitions portion of the Reuters 21857 corpus.
Training JOINT+PRO on this data as well as our
original training data yields the best performance
(BEST in Table 2).10
To our knowledge, the best previously pub-
lished results on this dataset are from Siefkes
(2008), who report 45.9 weighted F1. Our BEST
system evaluated in their slightly stricter way
yields 51.1.
9Following Patwardhan and Riloff (2009), we match ex-
tracted mentions to labeled spans if the head of the mention
matches the labeled span.
10We scaled expected counts from the unlabeled data so
that they did not overwhelm those from our (partially) labeled
data.
6 Conclusion
We have presented a joint generative model of
coreference resolution and role-filling information
extraction. This model makes role decisions at
the entity, rather than at the mention level. This
approach naturally aggregates information across
multiple mentions, incorporates unannotated data,
and yields strong performance.
Acknowledgements: This project is funded in
part by the Office of Naval Research under MURI
Grant No. N000140911081.
References
M. C. de Marneffe, B. Maccartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC.
Dayne Freitag and Andrew McCallum. 2000. Infor-
mation extraction with hmm structures learned by
stochastic optimization. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).
Dayne Freitag. 1998. Machine learning for informa-
tion extraction in informal domains.
A. Haghighi and D. Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In North
American Association of Computational Linguistics
(NAACL).
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tuto-
rial). In Association for Computational Linguistics
(ACL).
S. Patwardhan and E. Riloff. 2007. Effective infor-
mation extraction with semantic affinity patterns and
relevant regions. In Joint Conference on Empirical
Methods in Natural Language Processing.
S. Patwardhan and E Riloff. 2009. A unified model of
phrasal and sentential evidence for information ex-
traction. In Empirical Methods in Natural Language
Processing (EMNLP).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Christian Siefkes. 2008. An Incrementally Train-
able Statistical Approach to Information Extraction:
Based on Token Classification and Rich Context
Model. VDM Verlag, Saarbru?cken, Germany, Ger-
many.
295
Proceedings of the ACL 2010 Conference Short Papers, pages 348?352,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical A? Parsing with Bridge Outside Scores
Adam Pauls and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
Hierarchical A? (HA?) uses of a hierarchy
of coarse grammars to speed up parsing
without sacrificing optimality. HA? pri-
oritizes search in refined grammars using
Viterbi outside costs computed in coarser
grammars. We present Bridge Hierarchi-
cal A? (BHA?), a modified Hierarchial A?
algorithm which computes a novel outside
cost called a bridge outside cost. These
bridge costs mix finer outside scores with
coarser inside scores, and thus consti-
tute tighter heuristics than entirely coarse
scores. We show that BHA? substan-
tially outperforms HA? when the hierar-
chy contains only very coarse grammars,
while achieving comparable performance
on more refined hierarchies.
1 Introduction
The Hierarchical A? (HA?) algorithm of Felzen-
szwalb and McAllester (2007) allows the use of a
hierarchy of coarse grammars to speed up pars-
ing without sacrificing optimality. Pauls and
Klein (2009) showed that a hierarchy of coarse
grammars outperforms standard A? parsing for a
range of grammars. HA? operates by computing
Viterbi inside and outside scores in an agenda-
based way, using outside scores computed under
coarse grammars as heuristics which guide the
search in finer grammars. The outside scores com-
puted by HA? are auxiliary quantities, useful only
because they form admissible heuristics for search
in finer grammars.
We show that a modification of the HA? algo-
rithm can compute modified bridge outside scores
which are tighter bounds on the true outside costs
in finer grammars. These bridge outside scores
mix inside and outside costs from finer grammars
with inside costs from coarser grammars. Because
the bridge costs represent tighter estimates of the
true outside costs, we expect them to reduce the
work of computing inside costs in finer grammars.
At the same time, because bridge costs mix com-
putation from coarser and finer levels of the hier-
archy, they are more expensive to compute than
purely coarse outside costs. Whether the work
saved by using tighter estimates outweighs the ex-
tra computation needed to compute them is an em-
pirical question.
In this paper, we show that the use of bridge out-
side costs substantially outperforms the HA? al-
gorithm when the coarsest levels of the hierarchy
are very loose approximations of the target gram-
mar. For hierarchies with tighter estimates, we
show that BHA? obtains comparable performance
to HA?. In other words, BHA? is more robust to
poorly constructed hierarchies.
2 Previous Work
In this section, we introduce notation and review
HA?. Our presentation closely follows Pauls and
Klein (2009), and we refer the reader to that work
for a more detailed presentation.
2.1 Notation
Assume we have input sentence s0 . . . sn?1 of
length n, and a hierarchy of m weighted context-
free grammars G1 . . .Gm. We call the most refined
grammar Gm the target grammar, and all other
(coarser) grammars auxiliary grammars. Each
grammar Gt has a set of symbols denoted with cap-
ital letters and a subscript indicating the level in
the hierarchy, including a distinguished goal (root)
symbol Gt. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in Gt has the form r = At ? Bt Ct with weight
wr.
Edges are labeled spans e = (At, i, j). The
weight of a derivation is the sum of rule weights
in the derivation. The weight of the best (mini-
mum) inside derivation for an edge e is called the
Viterbi inside score ?(e), and the weight of the
348
(a) (b)
G
t
s
0
s
2
s
n-1
VP
t
G
t
s
3
s
4
s
5
..
s
0
s
2
s
n-1
s
3
s
4
s
5
..
VP
t
.. ..
Figure 1: Representations of the different types of items
used in parsing and how they depend on each other. (a)
In HA?, the inside item I(VPt, 3, 5) relies on the coarse
outside item O(pit(VPt), 3, 5) for outside estimates. (b) In
BHA?, the same inside item relies on the bridge outside item
O?(VPt, 3, 5), which mixes coarse and refined outside costs.
The coarseness of an item is indicated with dotted lines.
best derivation of G ? s0 . . . si?1 At sj . . . sn?1
is called the Viterbi outside score ?(e). The goal
of a 1-best parsing algorithm is to compute the
Viterbi inside score of the edge (Gm, 0, n); the
actual best parse can be reconstructed from back-
pointers in the standard way.
We assume that each auxiliary grammar Gt?1
forms a relaxed projection of Gt. A grammar Gt?1
is a projection of Gt if there exists some many-
to-one onto function pit which maps each symbol
in Gt to a symbol in Gt?1; hereafter, we will use
A?t to represent pit(At). A projection is relaxed
if, for every rule r = At ? Bt Ct with weight
wr the projection r? = A?t ? B
?
t C
?
t has weight
wr? ? wr in Gt?1. In other words, the weight of r?
is a lower bound on the weight of all rules r in Gt
which project to r?.
2.2 Deduction Rules
HA? and our modification BHA? can be formu-
lated in terms of prioritized weighted deduction
rules (Shieber et al, 1995; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item ? from
the agenda, put it into the chart with its current
weight, and form using deduction rules any items
which can be built by combining ? with items al-
ready in the chart. If new or improved, resulting
items are put on the agenda with priority given by
p(?). Because all antecedents must be constructed
before a deduction rule is executed, we sometimes
refer to particular conclusion item as ?waiting? on
an other item(s) before it can be built.
2.3 HA?
HA? can be formulated in terms of two types of
items. Inside items I(At, i, j) represent possible
derivations of the edge (At, i, j), while outside
items O(At, i, j) represent derivations of G ?
s1 . . . si?1 At sj . . . sn rooted at (Gt, 0, n). See
Figure 1(a) for a graphical depiction of these
edges. Inside items are used to compute Viterbi in-
side scores under grammar Gt, while outside items
are used to compute Viterbi outside scores.
The deduction rules which construct inside and
outside items are given in Table 1. The IN deduc-
tion rule combines two inside items over smaller
spans with a grammar rule to form an inside item
over larger spans. The weight of the resulting item
is the sum of the weights of the smaller inside
items and the grammar rule. However, the IN rule
also requires that an outside score in the coarse
grammar1 be computed before an inside item is
built. Once constructed, this coarse outside score
is added to the weight of the conclusion item to
form the priority of the resulting item. In other
words, the coarse outside score computed by the
algorithm plays the same role as a heuristic in stan-
dard A? parsing (Klein and Manning, 2003).
Outside scores are computed by the OUT-L and
OUT-R deduction rules. These rules combine an
outside item over a large span and inside items
over smaller spans to form outside items over
smaller spans. Unlike the IN deduction, the OUT
deductions only involve items from the same level
of the hierarchy. That is, whereas inside scores
wait on coarse outside scores to be constructed,
outside scores wait on inside scores at the same
level in the hierarchy.
Conceptually, these deduction rules operate by
1For the coarsest grammar G1, the IN rule builds rules
using 0 as an outside score.
349
HA?
IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O(A?t, i, j) : w3
w1+w2+wr+w3
??????????? I(At, i, j) : w1 + w2 + wr
OUT-L: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3
w1+w3+wr+w2??????????? O(Bt, i, l) : w1 + w3 + wr
OUT-R: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3
w1+w2+wr+w3??????????? O(Ct, l, j) : w1 + w2 + wr
Table 1: HA? deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
BHA?
B-IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O?(At, i, j) : w3
w1+w2+wr+w3??????????? I(At, i, j) : w1 + w2 + wr
B-OUT-L: O?(At, i, j) : w1 I(B?t, i, l) : w2 I(C?t, l, j) : w3
w1+wr+w2+w3
??????????? O?(Bt, i, l) : w1 + wr + w3
B-OUT-R: O?(At, i, j) : w1 I(Bt, i, l) : w2 I(C?t, l, j) : w3
w1+w2+wr+w3
??????????? O?(Ct, l, j) : w1 + w2 + wr
Table 2: BHA? deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
first computing inside scores bottom-up in the
coarsest grammar, then outside scores top-down
in the same grammar, then inside scores in the
next finest grammar, and so on. However, the cru-
cial aspect of HA? is that items from all levels
of the hierarchy compete on the same queue, in-
terleaving the computation of inside and outside
scores at all levels. The HA? deduction rules come
with three important guarantees. The first is a
monotonicity guarantee: each item is popped off
the agenda in order of its intrinsic priority p?(?).
For inside items I(e) over edge e, this priority
p?(I(e)) = ?(e) + ?(e?) where e? is the projec-
tion of e. For outside items O(?) over edge e, this
priority is p?(O(e)) = ?(e) + ?(e).
The second is a correctness guarantee: when
an inside/outside item is popped of the agenda, its
weight is its true Viterbi inside/outside cost. Taken
together, these two imply an efficiency guarantee,
which states that only items x whose intrinsic pri-
ority p?(x) is less than or equal to the Viterbi inside
score of the goal are removed from the agenda.
2.4 HA? with Bridge Costs
The outside scores computed by HA? are use-
ful for prioritizing computation in more refined
grammars. The key property of these scores is
that they form consistent and admissible heuristic
costs for more refined grammars, but coarse out-
side costs are not the only quantity which satisfy
this requirement. As an alternative, we propose
a novel ?bridge? outside cost ??(e). Intuitively,
this cost represents the cost of the best deriva-
tion where rules ?above? and ?left? of an edge e
come from Gt, and rules ?below? and ?right? of
the e come from Gt?1; see Figure 2 for a graph-
ical depiction. More formally, let the spine of
an edge e = (At, i, j) for some derivation d be
VP
t
NP
t
Xt-1
s
1
s
2
s
3
G
t
s
0
NN
t
NP
t
s
4
s
5
VP
t
VP
t
S
t
Xt-1Xt-1 Xt-1
NP
t
Xt-1
NP
t
Xt-1
s
n-1
Figure 2: A concrete example of a possible bridge outside
derivation for the bridge item O?(VPt, 1, 4). This edge is
boxed for emphasis. The spine of the derivation is shown
in bold and colored in blue. Rules from a coarser grammar
are shown with dotted lines, and colored in red. Here we have
the simple projection pit(A) = X , ?A.
the sequence of rules between e and the root edge
(Gt, 0, n). A bridge outside derivation of e is a
derivation d of G ? s1 . . . si At sj+1 . . . sn such
that every rule on or left of the spine comes from
Gt, and all other rules come from Gt?1. The score
of the best such derivation for e is the bridge out-
side cost ??(e).
Like ordinary outside costs, bridge outside costs
form consistent and admissible estimates of the
true Viterbi outside score ?(e) of an edge e. Be-
cause bridge costs mix rules from the finer and
coarser grammar, bridge costs are at least as good
an estimate of the true outside score as entirely
coarse outside costs, and will in general be much
tighter. That is, we have
?(e?) ? ??(e) ? ?(e)
In particular, note that the bridge costs become
better approximations farther right in the sentence,
and the bridge cost of the last word in the sentence
is equal to the Viterbi outside cost of that word.
To compute bridge outside costs, we introduce
350
bridge outside items O?(At, i, j), shown graphi-
cally in Figure 1(b). The deduction rules which
build both inside items and bridge outside items
are shown in Table 2. The rules are very simi-
lar to those which define HA?, but there are two
important differences. First, inside items wait for
bridge outside items at the same level, while out-
side items wait for inside items from the previous
level. Second, the left and right outside deductions
are no longer symmetric ? bridge outside items
can extended to the left given two coarse inside
items, but can only be extended to the right given
an exact inside item on the left and coarse inside
item on the right.
2.5 Guarantees
These deduction rules come with guarantees anal-
ogous to those of HA?. The monotonicity guaran-
tee ensures that inside and (bridge) outside items
are processed in order of:
p?(I(e)) = ?(e) + ??(e)
p?(O?(e)) = ??(e) + ?(e?)
The correctness guarantee ensures that when an
item is removed from the agenda, its weight will
be equal to ?(e) for inside items and ??(e) for
bridge items. The efficiency guarantee remains the
same, though because the intrinsic priorities are
different, the set of items processed will be differ-
ent from those processed by HA?.
A proof of these guarantees is not possible
due to space restrictions. The proof for BHA?
follows the proof for HA? in Felzenszwalb and
McAllester (2007) with minor modifications. The
key property of HA? needed for these proofs is
that coarse outside costs form consistent and ad-
missible heuristics for inside items, and exact in-
side costs form consistent and admissible heuris-
tics for outside items. BHA? also has this prop-
erty, with bridge outside costs forming admissi-
ble and consistent heuristics for inside items, and
coarse inside costs forming admissible and consis-
tent heuristics for outside items.
3 Experiments
The performance of BHA? is determined by the
efficiency guarantee given in the previous sec-
tion. However, we cannot determine in advance
whether BHA? will be faster than HA?. In fact,
BHA? has the potential to be slower ? BHA?
0
10
20
30
40
0-split 1-split 2-split 3-split 4-split 5-split
I
t
e
m
s
 
P
u
s
h
e
d
 
(
B
i
l
l
i
o
n
s
)
BHA*
HA*
Figure 3: Performance of HA? and BHA? as a function of
increasing refinement of the coarse grammar. Lower is faster.
0
2.5
5
7.5
10
3 3-5 0-5
E
d
g
e
s
 
P
u
s
h
e
d
 
(
b
i
l
l
i
o
n
s
)
Figure 4: Performance of BHA? on hierarchies of varying
size. Lower is faster. Along the x-axis, we show which coarse
grammars were used in the hierarchy. For example, 3-5 in-
dicates the 3-,4-, and 5-split grammars were used as coarse
grammars.
builds both inside and bridge outside items under
the target grammar, where HA? only builds inside
items. It is an empirical, grammar- and hierarchy-
dependent question whether the increased tight-
ness of the outside estimates outweighs the addi-
tional cost needed to compute them. We demon-
strate empirically in this section that for hier-
archies with very loosely approximating coarse
grammars, BHA? can outperform HA?, while
for hierarchies with good approximations, perfor-
mance of the two algorithms is comparable.
We performed experiments with the grammars
of Petrov et al (2006). The training procedure for
these grammars produces a hierarchy of increas-
ingly refined grammars through state-splitting, so
a natural projection function pit is given. We used
the Berkeley Parser2 to learn such grammars from
Sections 2-21 of the Penn Treebank (Marcus et al,
1993). We trained with 6 split-merge cycles, pro-
ducing 7 grammars. We tested these grammars on
300 sentences of length ? 25 of Section 23 of the
Treebank. Our ?target grammar? was in all cases
the most split grammar.
2http://berkeleyparser.googlecode.com
351
In our first experiment, we construct 2-level hi-
erarchies consisting of one coarse grammar and
the target grammar. By varying the coarse gram-
mar from the 0-split (X-bar) through 5-split gram-
mars, we can investigate the performance of each
algorithm as a function of the coarseness of the
coarse grammar. We follow Pauls and Klein
(2009) in using the number of items pushed as
a machine- and implementation-independent mea-
sure of speed. In Figure 3, we show the perfor-
mance of HA? and BHA? as a function of the
total number of items pushed onto the agenda.
We see that for very coarse approximating gram-
mars, BHA? substantially outperforms HA?, but
for more refined approximating grammars the per-
formance is comparable, with HA? slightly out-
performing BHA? on the 3-split grammar.
Finally, we verify that BHA? can benefit from
multi-level hierarchies as HA? can. We con-
structed two multi-level hierarchies: a 4-level hier-
archy consisting of the 3-,4-,5-, and 6- split gram-
mars, and 7-level hierarchy consisting of all gram-
mars. In Figure 4, we show the performance of
BHA? on these multi-level hierarchies, as well as
the best 2-level hierarchy from the previous exper-
iment. Our results echo the results of Pauls and
Klein (2009): although the addition of the rea-
sonably refined 4- and 5-split grammars produces
modest performance gains, the addition of coarser
grammars can actually hurt overall performance.
Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
References
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
352
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 258?267,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Faster and Smaller N -Gram Language Models
Adam Pauls Dan Klein
Computer Science Division
University of California, Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
N -gram language models are a major resource
bottleneck in machine translation. In this pa-
per, we present several language model imple-
mentations that are both highly compact and
fast to query. Our fastest implementation is
as fast as the widely used SRILM while re-
quiring only 25% of the storage. Our most
compact representation can store all 4 billion
n-grams and associated counts for the Google
n-gram corpus in 23 bits per n-gram, the most
compact lossless representation to date, and
even more compact than recent lossy compres-
sion techniques. We also discuss techniques
for improving query speed during decoding,
including a simple but novel language model
caching technique that improves the query
speed of our language models (and SRILM)
by up to 300%.
1 Introduction
For modern statistical machine translation systems,
language models must be both fast and compact.
The largest language models (LMs) can contain as
many as several hundred billion n-grams (Brants
et al, 2007), so storage is a challenge. At the
same time, decoding a single sentence can trig-
ger hundreds of thousands of queries to the lan-
guage model, so speed is also critical. As al-
ways, trade-offs exist between time, space, and ac-
curacy, with many recent papers considering small-
but-approximate noisy LMs (Chazelle et al, 2004;
Guthrie and Hepple, 2010) or small-but-slow com-
pressed LMs (Germann et al, 2009).
In this paper, we present several lossless meth-
ods for compactly but efficiently storing large LMs
in memory. As in much previous work (Whittaker
and Raj, 2001; Hsu and Glass, 2008), our meth-
ods are conceptually based on tabular trie encodings
wherein each n-gram key is stored as the concatena-
tion of one word (here, the last) and an offset encod-
ing the remaining words (here, the context). After
presenting a bit-conscious basic system that typifies
such approaches, we improve on it in several ways.
First, we show how the last word of each entry can
be implicitly encoded, almost entirely eliminating
its storage requirements. Second, we show that the
deltas between adjacent entries can be efficiently en-
coded with simple variable-length encodings. Third,
we investigate block-based schemes that minimize
the amount of compressed-stream scanning during
lookup.
To speed up our language models, we present two
approaches. The first is a front-end cache. Caching
itself is certainly not new to language modeling, but
because well-tuned LMs are essentially lookup ta-
bles to begin with, naive cache designs only speed
up slower systems. We present a direct-addressing
cache with a fast key identity check that speeds up
our systems (or existing fast systems like the widely-
used, speed-focused SRILM) by up to 300%.
Our second speed-up comes from a more funda-
mental change to the language modeling interface.
Where classic LMs take word tuples and produce
counts or probabilities, we propose an LM that takes
a word-and-context encoding (so the context need
not be re-looked up) and returns both the probabil-
ity and also the context encoding for the suffix of the
original query. This setup substantially accelerates
the scrolling queries issued by decoders, and also
exploits language model state equivalence (Li and
Khudanpur, 2008).
Overall, we are able to store the 4 billion n-grams
of the Google Web1T (Brants and Franz, 2006) cor-
258
pus, with associated counts, in 10 GB of memory,
which is smaller than state-of-the-art lossy language
model implementations (Guthrie and Hepple, 2010),
and significantly smaller than the best published
lossless implementation (Germann et al, 2009). We
are also able to simultaneously outperform SRILM
in both total size and speed. Our LM toolkit, which
is implemented in Java and compatible with the stan-
dard ARPA file formats, is available on the web.1
2 Preliminaries
Our goal in this paper is to provide data structures
that map n-gram keys to values, i.e. probabilities
or counts. Maps are fundamental data structures
and generic implementations of mapping data struc-
tures are readily available. However, because of the
sheer number of keys and values needed for n-gram
language modeling, generic implementations do not
work efficiently ?out of the box.? In this section,
we will review existing techniques for encoding the
keys and values of an n-gram language model, tak-
ing care to account for every bit of memory required
by each implementation.
To provide absolute numbers for the storage re-
quirements of different implementations, we will
use the Google Web1T corpus as a benchmark. This
corpus, which is on the large end of corpora typically
employed in language modeling, is a collection of
nearly 4 billion n-grams extracted from over a tril-
lion tokens of English text, and has a vocabulary of
about 13.5 million words.
2.1 Encoding Values
In the Web1T corpus, the most frequent n-gram
occurs about 95 billion times. Storing this count
explicitly would require 37 bits, but, as noted by
Guthrie and Hepple (2010), the corpus contains only
about 770 000 unique counts, so we can enumerate
all counts using only 20 bits, and separately store an
array called the value rank array which converts the
rank encoding of a count back to its raw count. The
additional array is small, requiring only about 3MB,
but we save 17 bits per n-gram, reducing value stor-
age from around 16GB to about 9GB for Web1T.
We can rank encode probabilities and back-offs in
the same way, allowing us to be agnostic to whether
1http://code.google.com/p/berkeleylm/
we encode counts, probabilities and/or back-off
weights in our model. In general, the number of bits
per value required to encode all value ranks for a
given language model will vary ? we will refer to
this variable as v .
2.2 Trie-Based Language Models
The data structure of choice for the majority of
modern language model implementations is a trie
(Fredkin, 1960). Tries or variants thereof are
implemented in many LM tool kits, including
SRILM (Stolcke, 2002), IRSTLM (Federico and
Cettolo, 2007), CMU SLM (Whittaker and Raj,
2001), and MIT LM (Hsu and Glass, 2008). Tries
represent collections of n-grams using a tree. Each
node in the tree encodes a word, and paths in the
tree correspond to n-grams in the collection. Tries
ensure that each n-gram prefix is represented only
once, and are very efficient when n-grams share
common prefixes. Values can also be stored in a trie
by placing them in the appropriate nodes.
Conceptually, trie nodes can be implemented as
records that contain two entries: one for the word
in the node, and one for either a pointer to the par-
ent of the node or a list of pointers to children. At
a low level, however, naive implementations of tries
can waste significant amounts of space. For exam-
ple, the implementation used in SRILM represents a
trie node as a C struct containing a 32-bit integer
representing the word, a 64-bit memory2 pointer to
the list of children, and a 32-bit floating point num-
ber representing the value stored at a node. The total
storage for a node alone is 16 bytes, with additional
overhead required to store the list of children. In
total, the most compact implementation in SRILM
uses 33 bytes per n-gram of storage, which would
require around 116 GB of memory to store Web1T.
While it is simple to implement a trie node in this
(already wasteful) way in programming languages
that offer low-level access to memory allocation like
C/C++, the situation is even worse in higher level
programming languages. In Java, for example, C-
style structs are not available, and records are
most naturally implemented as objects that carry an
additional 64 bits of overhead.
2While 32-bit architectures are still in use today, their lim-
ited address space is insufficient for modern language models
and we will assume all machines use a 64-bit architecture.
259
Despite its relatively large storage requirements,
the implementation employed by SRILM is still
widely in use today, largely because of its speed ? to
our knowledge, SRILM is the fastest freely available
language model implementation. We will show that
we can achieve access speeds comparable to SRILM
but using only 25% of the storage.
2.3 Implicit Tries
A more compact implementation of a trie is de-
scribed in Whittaker and Raj (2001). In their imple-
mentation, nodes in a trie are represented implicitly
as entries in an array. Each entry encodes a word
with enough bits to index all words in the language
model (24 bits for Web1T), a quantized value, and
a 32-bit3 offset that encodes the contiguous block
of the array containing the children of the node.
Note that 32 bits is sufficient to index all n-grams in
Web1T; for larger corpora, we can always increase
the size of the offset.
Effectively, this representation replaces system-
level memory pointers with offsets that act as logical
pointers that can reference other entries in the array,
rather than arbitrary bytes in RAM. This represen-
tation saves space because offsets require fewer bits
than memory pointers, but more importantly, it per-
mits straightforward implementation in any higher-
level language that provides access to arrays of inte-
gers.4
2.4 Encoding n-grams
Hsu and Glass (2008) describe a variant of the im-
plicit tries of Whittaker and Raj (2001) in which
each node in the trie stores the prefix (i.e. parent).
This representation has the property that we can re-
fer to each n-gram wn1 by its last word wn and the
offset c(wn?11 ) of its prefix w
n?1
1 , often called the
context. At a low-level, we can efficiently encode
this pair (wn, c(w
n?1
1 )) as a single 64-bit integer,
where the first 24 bits refer to wn and the last 40 bits
3The implementation described in the paper represents each
32-bit integer compactly using only 16 bits, but this represen-
tation is quite inefficient, because determining the full 32-bit
offset requires a binary search in a look up table.
4Typically, programming languages only provide support
for arrays of bytes, not bits, but it is of course possible to simu-
late arrays with arbitrary numbers of bits using byte arrays and
bit manipulation.
encode c(wn?11 ). We will refer to this encoding as a
context encoding.
Note that typically, n-grams are encoded in tries
in the reverse direction (first-rest instead of last-
rest), which enables a more efficient computation of
back-offs. In our implementations, we found that the
speed improvement from switching to a first-rest en-
coding and implementing more efficient queries was
modest. However, as we will see in Section 4.2, the
last-rest encoding allows us to exploit the scrolling
nature of queries issued by decoders, which results
in speedups that far outweigh those achieved by re-
versing the trie.
3 Language Model Implementations
In the previous section, we reviewed well-known
techniques in language model implementation. In
this section, we combine these techniques to build
simple data structures in ways that are to our knowl-
edge novel, producing language models with state-
of-the-art memory requirements and speed. We will
also show that our data structures can be very effec-
tively compressed by implicitly encoding the word
wn, and further compressed by applying a variable-
length encoding on context deltas.
3.1 Sorted Array
A standard way to implement a map is to store an
array of key/value pairs, sorted according to the key.
Lookup is carried out by performing binary search
on a key. For an n-gram language model, we can ap-
ply this implementation with a slight modification:
we need n sorted arrays, one for each n-gram order.
We construct keys (wn, c(w
n?1
1 )) using the context
encoding described in the previous section, where
the context offsets c refer to entries in the sorted ar-
ray of (n ? 1)-grams. This data structure is shown
graphically in Figure 1.
Because our keys are sorted according to their
context-encoded representation, we cannot straight-
forwardly answer queries about an n-gram w with-
out first determining its context encoding. We can
do this efficiently by building up the encoding in-
crementally: we start with the context offset of the
unigram w1, which is simply its integer representa-
tion, and use that to form the context encoding of the
bigram w21 = (w2, c(w1)). We can find the offset of
260
?ran?
w c
15180053
w c
24
bits
40
bits
64 bits
?cat?
15176585
?dog?
6879
6879
6879
6879
6879
6880
6880
6880
6879
00004598
?slept?
00004588
00004568
00004530
00004502
00004668
00004669
00004568
00004577
6879 0000449815176583
15176585
15176593
15176613
15179801
15180051
15176589
15176591
?had?
?the?
?left?
1933
.
.
.
.
.
.
.
.
.
.
.
.
?slept?
1933
1933
1933
1933
1933
1933
1935
1935
1935
.
.
val val val
v
bits
.
.
?dog?
3-grams
2-grams 1-grams
w
Figure 1: Our SORTED implementation of a trie. The dotted paths correspond to ?the cat slept?, ?the cat ran?, and ?the
dog ran?. Each node in the trie is an entry in an array with 3 parts: w represents the word at the node; val represents
the (rank encoded) value; and c is an offset in the array of n ? 1 grams that represents the parent (prefix) of a node.
Words are represented as offsets in the unigram array.
the bigram using binary search, and form the context
encoding of the trigram, and so on. Note, however,
that if our queries arrive in context-encoded form,
queries are faster since they involve only one binary
search in the appropriate array. We will return to this
later in Section 4.2
This implementation, SORTED, uses 64 bits for
the integer-encoded keys and v bits for the values.
Lookup is linear in the length of the key and log-
arithmic in the number of n-grams. For Web1T
(v = 20), the total storage is 10.5 bytes/n-gram or
about 37GB.
3.2 Hash Table
Hash tables are another standard way to implement
associative arrays. To enable the use of our context
encoding, we require an implementation in which
we can refer to entries in the hash table via array
offsets. For this reason, we use an open address hash
map that uses linear probing for collision resolution.
As in the sorted array implementation, in order to
insert an n-gram wn1 into the hash table, we must
form its context encoding incrementally from the
offset of w1. However, unlike the sorted array im-
plementation, at query time, we only need to be
able to check equality between the query key wn1 =
(wn, c(w
n?1
1 )) and a key w
?n
1 = (w
?
n, c(w
?n?1
1 )) in
the table. Equality can easily be checked by first
checking if wn = w?n, then recursively checking
equality between wn?11 and w
?n?1
1 , though again,
equality is even faster if the query is already context-
encoded.
This HASH data structure also uses 64 bits for
integer-encoded keys and v bits for values. How-
ever, to avoid excessive hash collisions, we also al-
locate additional empty space according to a user-
defined parameter that trades off speed and time ?
we used about 40% extra space in our experiments.
For Web1T, the total storage for this implementation
is 15 bytes/n-gram or about 53 GB total.
Look up in a hash map is linear in the length of
an n-gram and constant with respect to the number
261
of n-grams. Unlike the sorted array implementa-
tion, the hash table implementation also permits ef-
ficient insertion and deletion, making it suitable for
stream-based language models (Levenberg and Os-
borne, 2009).
3.3 Implicitly Encoding wn
The context encoding we have used thus far still
wastes space. This is perhaps most evident in the
sorted array representation (see Figure 1): all n-
grams ending with a particular word wi are stored
contiguously. We can exploit this redundancy by
storing only the context offsets in the main array,
using as many bits as needed to encode all context
offsets (32 bits for Web1T). In auxiliary arrays, one
for each n-gram order, we store the beginning and
end of the range of the trie array in which all (wi, c)
keys are stored for each wi. These auxiliary arrays
are negligibly small ? we only need to store 2n off-
sets for each word.
The same trick can be applied in the hash table
implementation. We allocate contiguous blocks of
the main array for n-grams which all share the same
last word wi, and distribute keys within those ranges
using the hashing function.
This representation reduces memory usage for
keys from 64 bits to 32 bits, reducing overall storage
for Web1T to 6.5 bytes/n-gram for the sorted imple-
mentation and 9.1 bytes for the hashed implementa-
tion, or about 23GB and 32GB in total. It also in-
creases query speed in the sorted array case, since to
find (wi, c), we only need to search the range of the
array over which wi applies. Because this implicit
encoding reduces memory usage without a perfor-
mance cost, we will assume its use for the rest of
this paper.
3.4 A Compressed Implementation
3.4.1 Variable-Length Coding
The distribution of value ranks in language mod-
eling is Zipfian, with far more n-grams having low
counts than high counts. If we ensure that the value
rank array sorts raw values by descending order of
frequency, then we expect that small ranks will oc-
cur much more frequently than large ones, which we
can exploit with a variable-length encoding.
To compress n-grams, we can exploit the context
encoding of our keys. In Figure 2, we show a portion
w c val
1933 15176585 3
1933 15176587 2
1933 15176593 1
1933 15176613 8
1933 15179801 1
1935 15176585 298
1935 15176589 1
1933 15176585 563097887 956 3 0 +0 +2 2 +0 +5 1 +0 +40 8
!w !c
val
1933 15176585 3
+0 +2 1
+0 +5 1
+0 +40 8
+0 +188 1
+2 15176585 298
+0 +4 1
|!w| |!c|
|val|
24 40 3
2 3 3
2 3 3
2 9 6
2 12 3
4 36 15
2 6 3
.  .  . 
(a) Context-Encoding (b) Context Deltas (c) Bits Required
(d) Compressed Array
Number 
of bits 
in this 
block
Value rank 
for header 
key
Header 
key
Logical 
offset of 
this block
True if 
all !w in 
block are 
0 
Figure 2: Compression using variable-length encoding.
(a) A snippet of an (uncompressed) context-encoded ar-
ray. (b) The context and word deltas. (c) The number
of bits required to encode the context and word deltas as
well as the value ranks. Word deltas use variable-length
block coding with k = 1, while context deltas and value
ranks use k = 2. (d) A snippet of the compressed encod-
ing array. The header is outlined in bold.
of the key array used in our sorted array implemen-
tation. While we have already exploited the fact that
the 24 word bits repeat in the previous section, we
note here that consecutive context offsets tend to be
quite close together. We found that for 5-grams, the
median difference between consecutive offsets was
about 50, and 90% of offset deltas were smaller than
10000. By using a variable-length encoding to rep-
resent these deltas, we should require far fewer than
32 bits to encode context offsets.
We used a very simple variable-length coding to
encode offset deltas, word deltas, and value ranks.
Our encoding, which is referred to as ?variable-
length block coding? in Boldi and Vigna (2005),
works as follows: we pick a (configurable) radix
r = 2k. To encode a number m, we determine the
number of digits d required to express m in base r.
We write d in unary, i.e. d ? 1 zeroes followed by
a one. We then write the d digits of m in base r,
each of which requires k bits. For example, using
k = 2, we would encode the decimal number 7 as
010111. We can choose k separately for deltas and
value indices, and also tune these parameters to a
given language model.
We found this encoding outperformed other
standard prefix codes, including Golomb
codes (Golomb, 1966; Church et al, 2007)
262
and Elias ? and ? codes. We also experimented
with the ? codes of Boldi and Vigna (2005), which
modify variable-length block codes so that they
are optimal for certain power law distributions.
We found that ? codes performed no better than
variable-length block codes and were slightly more
complex. Finally, we found that Huffman codes
outperformed our encoding slightly, but came at a
much higher computational cost.
3.4.2 Block Compression
We could in principle compress the entire array of
key/value pairs with the encoding described above,
but this would render binary search in the array im-
possible: we cannot jump to the mid-point of the ar-
ray since in order to determine what key lies at a par-
ticular point in the compressed bit stream, we would
need to know the entire history of offset deltas.
Instead, we employ block compression, a tech-
nique also used by Harb et al (2009) for smaller
language models. In particular, we compress the
key/value array in blocks of 128 bytes. At the be-
ginning of the block, we write out a header consist-
ing of: an explicit 64-bit key that begins the block;
a 32-bit integer representing the offset of the header
key in the uncompressed array;5 the number of bits
of compressed data in the block; and the variable-
length encoding of the value rank of the header key.
The remainder of the block is filled with as many
compressed key/value pairs as possible. Once the
block is full, we start a new block. See Figure 2 for
a depiction.
When we encode an offset delta, we store the delta
of the word portion of the key separately from the
delta of the context offset. When an entire block
shares the same word portion of the key, we set a
single bit in the header that indicates that we do not
encode any word deltas.
To find a key in this compressed array, we first
perform binary search over the header blocks (which
are predictably located every 128 bytes), followed
by a linear search within a compressed block.
Using k = 6 for encoding offset deltas and k = 5
for encoding value ranks, this COMPRESSED im-
plementation stores Web1T in less than 3 bytes per
n-gram, or about 10.2GB in total. This is about
5We need this because n-grams refer to their contexts using
array offsets.
6GB less than the storage required by Germann et
al. (2009), which is the best published lossless com-
pression to date.
4 Speeding up Decoding
In the previous section, we provided compact and
efficient implementations of associative arrays that
allow us to query a value for an arbitrary n-gram.
However, decoders do not issue language model re-
quests at random. In this section, we show that lan-
guage model requests issued by a standard decoder
exhibit two patterns we can exploit: they are highly
repetitive, and also exhibit a scrolling effect.
4.1 Exploiting Repetitive Queries
In a simple experiment, we recorded all of the
language model queries issued by the Joshua de-
coder (Li et al, 2009) on a 100 sentence test set.
Of the 31 million queries, only about 1 million were
unique. Therefore, we expect that keeping the re-
sults of language model queries in a cache should be
effective at reducing overall language model latency.
To this end, we added a very simple cache to
our language model. Our cache uses an array of
key/value pairs with size fixed to 2b ? 1 for some
integer b (we used 24). We use a b-bit hash func-
tion to compute the address in an array where we
will always place a given n-gram and its fully com-
puted language model score. Querying the cache is
straightforward: we check the address of a key given
by its b-bit hash. If the key located in the cache ar-
ray matches the query key, then we return the value
stored in the cache. Otherwise, we fetch the lan-
guage model probability from the language model
and place the new key and value in the cache, evict-
ing the old key in the process. This scheme is often
called a direct-mapped cache because each key has
exactly one possible address.
Caching n-grams in this way reduces overall la-
tency for two reasons: first, lookup in the cache is
extremely fast, requiring only a single evaluation of
the hash function, one memory lookup to find the
cache key, and one equality check on the key. In
contrast, even our fastest (HASH) implementation
may have to perform multiple memory lookups and
equality checks in order to resolve collisions. Sec-
ond, when calculating the probability for an n-gram
263
the cat + fell down
the cat fell
cat fell down
18569876 fell
35764106 down
LM
 
0.76
LM
 
0.12
LM
 
LM
 
0.76
0.12
?the cat?
?cat fell?
3576410
E
x
p
l
i
c
i
t
 
R
e
p
r
e
s
e
n
t
a
t
i
o
n
C
o
n
t
e
x
t
 
E
n
c
o
d
i
n
g
Figure 3: Queries issued when scoring trigrams that are
created when a state with LM context ?the cat? combines
with ?fell down?. In the standard explicit representation
of an n-gram as list of words, queries are issued atom-
ically to the language model. When using a context-
encoding, a query from the n-gram ?the cat fell? returns
the context offset of ?cat fell?, which speeds up the query
of ?cat fell down?.
not in the language model, language models with
back-off schemes must in general perform multiple
queries to fetch the necessary back-off information.
Our cache retains the full result of these calculations
and thus saves additional computation.
Federico and Cettolo (2007) also employ a cache
in their language model implementation, though
based on traditional hash table cache with linear
probing. Unlike our cache, which is of fixed size,
their cache must be cleared after decoding a sen-
tence. We would not expect a large performance in-
crease from such a cache for our faster models since
our HASH implementation is already a hash table
with linear probing. We found in our experiments
that a cache using linear probing provided marginal
performance increases of about 40%, largely be-
cause of cached back-off computation, while our
simpler cache increases performance by about 300%
even over our HASH LM implementation. More tim-
ing results are presented in Section 5.
4.2 Exploiting Scrolling Queries
Decoders with integrated language models (Och and
Ney, 2004; Chiang, 2005) score partial translation
hypotheses in an incremental way. Each partial hy-
pothesis maintains a language model context con-
sisting of at most n ? 1 target-side words. When
we combine two language model contexts, we create
several new n-grams of length of n, each of which
generate a query to the language model. These new
WMT2010
Order #n-grams
1gm 4,366,395
2gm 61,865,588
3gm 123,158,761
4gm 217,869,981
5gm 269,614,330
Total 676,875,055
WEB1T
Order #n-grams
1gm 13,588,391
2gm 314,843,401
3gm 977,069,902
4gm 1,313,818,354
5gm 1,176,470,663
Total 3,795,790,711
Table 1: Sizes of the two language models used in our
experiments.
n-grams exhibit a scrolling effect, shown in Fig-
ure 3: the n ? 1 suffix words of one n-gram form
the n? 1 prefix words of the next.
As discussed in Section 3, our LM implementa-
tions can answer queries about context-encoded n-
grams faster than explicitly encoded n-grams. With
this in mind, we augment the values stored in our
language model so that for a key (wn, c(w
n?1
1 )),
we store the offset of the suffix c(wn2 ) as well as
the normal counts/probabilities. Then, rather than
represent the LM context in the decoder as an ex-
plicit list of words, we can simply store context off-
sets. When we query the language model, we get
back both a language model score and context offset
c(w?n?11 ), where w?
n?1
1 is the the longest suffix of
wn?11 contained in the language model. We can then
quickly form the context encoding of the next query
by simply concatenating the new word with the off-
set c(w?n?11 ) returned from the previous query.
In addition to speeding up language model
queries, this approach also automatically supports an
equivalence of LM states (Li and Khudanpur, 2008):
in standard back-off schemes, whenever we compute
the probability for an n-gram (wn, c(wn?11 )) when
wn?11 is not in the language model, the result will be
the same as the result of the query (wn, c(w?
n?1
1 ). It
is therefore only necessary to store as much of the
context as the language model contains instead of
all n ? 1 words in the context. If a decoder main-
tains LM states using the context offsets returned
by our language model, then the decoder will au-
tomatically exploit this equivalence and the size of
the search space will be reduced. This same effect is
exploited explicitly by some decoders (Li and Khu-
danpur, 2008).
264
WMT2010
LM Type bytes/ bytes/ bytes/ Total
key value n-gram Size
SRILM-H ? ? 42.2 26.6G
SRILM-S ? ? 33.5 21.1G
HASH 5.6 6.0 11.6 7.5G
SORTED 4.0 4.5 8.5 5.5G
TPT ? ? 7.5?? 4.7G??
COMPRESSED 2.1 3.8 5.9 3.7G
Table 2: Memory usages of several language model im-
plementations on the WMT2010 language model. A
?? indicates that the storage in bytes per n-gram is re-
ported for a different language model of comparable size,
and the total size is thus a rough projection.
5 Experiments
5.1 Data
To test our LM implementations, we performed
experiments with two different language models.
Our first language model, WMT2010, was a 5-
gram Kneser-Ney language model which stores
probability/back-off pairs as values. We trained this
language model on the English side of all French-
English corpora provided6 for use in the WMT 2010
workshop, about 2 billion tokens in total. This data
was tokenized using the tokenizer.perl script
provided with the data. We trained the language
model using SRILM. We also extracted a count-
based language model, WEB1T, from the Web1T
corpus (Brants and Franz, 2006). Since this data is
provided as a collection of 1- to 5-grams and asso-
ciated counts, we used this data without further pre-
processing. The make up of these language models
is shown in Table 1.
5.2 Compression Experiments
We tested our three implementations (HASH,
SORTED, and COMPRESSED) on the WMT2010
language model. For this language model, there are
about 80 million unique probability/back-off pairs,
so v ? 36. Note that here v includes both the
cost per key of storing the value rank as well as the
(amortized) cost of storing two 32 bit floating point
numbers (probability and back-off) for each unique
value. The results are shown in Table 2.
6www.statmt.org/wmt10/translation-task.html
WEB1T
LM Type bytes/ bytes/ bytes/ Total
key value n-gram Size
Gzip ? ? 7.0 24.7G
T-MPHR? ? ? 3.0 10.5G
COMPRESSED 1.3 1.6 2.9 10.2G
Table 3: Memory usages of several language model im-
plementations on the WEB1T. A ? indicates lossy com-
pression.
We compare against three baselines. The first two,
SRILM-H and SRILM-S, refer to the hash table-
and sorted array-based trie implementations pro-
vided by SRILM. The third baseline is the Tightly-
Packed Trie (TPT) implementation of Germann et
al. (2009). Because this implementation is not freely
available, we use their published memory usage in
bytes per n-gram on a language model of similar
size and project total usage.
The memory usage of all of our models is con-
siderably smaller than SRILM ? our HASH imple-
mentation is about 25% the size of SRILM-H, and
our SORTED implementation is about 25% the size
of SRILM-S. Our COMPRESSED implementation
is also smaller than the state-of-the-art compressed
TPT implementation.
In Table 3, we show the results of our COM-
PRESSED implementation on WEB1T and against
two baselines. The first is compression of the ASCII
text count files using gzip, and the second is the
Tiered Minimal Perfect Hash (T-MPHR) of Guthrie
and Hepple (2010). The latter is a lossy compres-
sion technique based on Bloomier filters (Chazelle
et al, 2004) and additional variable-length encod-
ing that achieves the best published compression of
WEB1T to date. Our COMPRESSED implementa-
tion is even smaller than T-MPHR, despite using a
lossless compression technique. Note that since T-
MPHR uses a lossy encoding, it is possible to re-
duce the storage requirements arbitrarily at the cost
of additional errors in the model. We quote here the
storage required when keys7 are encoded using 12-
bit hash codes, which gives a false positive rate of
about 2?12 =0.02%.
7Guthrie and Hepple (2010) also report additional savings
by quantizing values, though we could perform the same quan-
tization in our storage scheme.
265
LM Type No Cache Cache Size
COMPRESSED 9264?73ns 565?7ns 3.7G
SORTED 1405?50ns 243?4ns 5.5G
HASH 495?10ns 179?6ns 7.5G
SRILM-H 428?5ns 159?4ns 26.6G
HASH+SCROLL 323?5ns 139?6ns 10.5G
Table 4: Raw query speeds of various language model
implementations. Times were averaged over 3 runs on
the same machine. For HASH+SCROLL, all queries were
issued to the decoder in context-encoded form, which
speeds up queries that exhibit scrolling behaviour. Note
that memory usage is higher than for HASH because we
store suffix offsets along with the values for an n-gram.
LM Type No Cache Cache Size
COMPRESSED 9880?82s 1547?7s 3.7G
SRILM-H 1120?26s 938?11s 26.6G
HASH 1146?8s 943?16s 7.5G
Table 5: Full decoding times for various language model
implementations. Our HASH LM is as fast as SRILM
while using 25% of the memory. Our caching also re-
duces total decoding time by about 20% for our fastest
models and speeds up COMPRESSED by a factor of 6.
Times were averaged over 3 runs on the same machine.
5.3 Timing Experiments
We first measured pure query speed by logging all
LM queries issued by a decoder and measuring
the time required to query those n-grams in isola-
tion. We used the the Joshua decoder8 with the
WMT2010 model to generate queries for the first
100 sentences of the French 2008 News test set. This
produced about 30 million queries. We measured the
time9 required to perform each query in order with
and without our direct-mapped caching, not includ-
ing any time spent on file I/O.
The results are shown in Table 4. As expected,
HASH is the fastest of our implementations, and
comparable10 in speed to SRILM-H, but using sig-
8We used a grammar trained on all French-English data
provided for WMT 2010 using the make scripts provided
at http://sourceforge.net/projects/joshua/files
/joshua/1.3/wmt2010-experiment.tgz/download
9All experiments were performed on an Amazon EC2 High-
Memory Quadruple Extra Large instance, with an Intel Xeon
X5550 CPU running at 2.67GHz and 8 MB of cache.
10Because we implemented our LMs in Java, we issued
queries to SRILM via Java Native Interface (JNI) calls, which
introduces a performance overhead. When called natively, we
found that SRILM was about 200 ns/query faster. Unfortu-
nificantly less space. SORTED is slower but of
course more memory efficient, and COMPRESSED
is the slowest but also the most compact repre-
sentation. In HASH+SCROLL, we issued queries
to the language model using the context encoding,
which speeds up queries substantially. Finally, we
note that our direct-mapped cache is very effective.
The query speed of all models is boosted substan-
tially. In particular, our COMPRESSED implementa-
tion with caching is nearly as fast as SRILM-H with-
out caching, and even the already fast HASH imple-
mentation is 300% faster in raw query speed with
caching enabled.
We also measured the effect of LM performance
on overall decoder performance. We modified
Joshua to optionally use our LM implementations
during decoding, and measured the time required
to decode all 2051 sentences of the 2008 News
test set. The results are shown in Table 5. With-
out caching, SRILM-H and HASH were comparable
in speed, while COMPRESSED introduces a perfor-
mance penalty. With caching enabled, overall de-
coder speed is improved for both HASH and SRILM-
H, while the COMPRESSED implementation is only
about 50% slower that the others.
6 Conclusion
We have presented several language model imple-
mentations which are state-of-the-art in both size
and speed. Our experiments have demonstrated im-
provements in query speed over SRILM and com-
pression rates against state-of-the-art lossy compres-
sion. We have also described a simple caching tech-
nique which leads to performance increases in over-
all decoding time.
Acknowledgements
This work was supported by a Google Fellowship for the
first author and by BBN under DARPA contract HR0011-
06-C-0022. We would like to thank David Chiang, Zhifei
Li, and the anonymous reviewers for their helpful com-
ments.
nately, it is not completely fair to compare our LMs against ei-
ther of these numbers: although the JNI overhead slows down
SRILM, implementing our LMs in Java instead of C++ slows
down our LMs. In the tables, we quote times which include
the JNI overhead, since this reflects the true cost to a decoder
written in Java (e.g. Joshua).
266
References
Paolo Boldi and Sebastiano Vigna. 2005. Codes for the
world wide web. Internet Mathematics, 2.
Thorsten Brants and Alex Franz. 2006. Google web1t
5-gram corpus, version 1. In Linguistic Data Consor-
tium, Philadelphia, Catalog Number LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The Bloomier filter: an efficient
data structure for static support lookup tables. In Pro-
ceedings of the fifteenth annual ACM-SIAM sympo-
sium on Discrete algorithms.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation.
Edward Fredkin. 1960. Trie memory. Communications
of the ACM, 3:490?499, September.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly packed tries: how to fit large models into mem-
ory, and make them load fast, too. In Proceedings of
the Workshop on Software Engineering, Testing, and
Quality Assurance for Natural Language Processing.
S. W. Golomb. 1966. Run-length encodings. IEEE
Transactions on Information Theory, 12.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: space efficient language models with con-
stant time retrieval. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Boulos Harb, Ciprian Chelba, Jeffrey Dean, and Sanjay
Ghemawat. 2009. Back-off language model compres-
sion. In Proceedings of Interspeech.
Bo-June Hsu and James Glass. 2008. Iterative language
model estimation: Efficient data structure and algo-
rithms. In Proceedings of Interspeech.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second Workshop on Syntax and Struc-
ture in Statistical Translation.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computationl Linguistics, 30:417?449, Decem-
ber.
Andreas Stolcke. 2002. SRILM: An extensible language
modeling toolkit. In Proceedings of Interspeech.
E. W. D. Whittaker and B. Raj. 2001. Quantization-
based language model compression. In Proceedings
of Eurospeech.
267
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Jointly Learning to Extract and Compress
Taylor Berg-Kirkpatrick Dan Gillick Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, dgillick, klein}@cs.berkeley.edu
Abstract
We learn a joint model of sentence extraction
and compression for multi-document summa-
rization. Our model scores candidate sum-
maries according to a combined linear model
whose features factor over (1) the n-gram
types in the summary and (2) the compres-
sions used. We train the model using a margin-
based objective whose loss captures end sum-
mary quality. Because of the exponentially
large set of candidate summaries, we use a
cutting-plane algorithm to incrementally de-
tect and add active constraints efficiently. In-
ference in our model can be cast as an ILP
and thereby solved in reasonable time; we also
present a fast approximation scheme which
achieves similar performance. Our jointly
extracted and compressed summaries outper-
form both unlearned baselines and our learned
extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguis-
tic quality. We achieve the highest published
ROUGE results to date on the TAC 2008 data
set.
1 Introduction
Applications of machine learning to automatic sum-
marization have met with limited success, and, as a
result, many top-performing systems remain largely
ad-hoc. One reason learning may have provided lim-
ited gains is that typical models do not learn to opti-
mize end summary quality directly, but rather learn
intermediate quantities in isolation. For example,
many models learn to score each input sentence in-
dependently (Teufel and Moens, 1997; Shen et al,
2007; Schilder and Kondadadi, 2008), and then as-
semble extractive summaries from the top-ranked
sentences in a way not incorporated into the learn-
ing process. This extraction is often done in the
presence of a heuristic that limits redundancy. As
another example, Yih et al (2007) learn predictors
of individual words? appearance in the references,
but in isolation from the sentence selection proce-
dure. Exceptions are Li et al (2009) who take a
max-margin approach to learning sentence values
jointly, but still have ad hoc constraints to handle
redundancy. One main contribution of the current
paper is the direct optimization of summary quality
in a single model; we find that our learned systems
substantially outperform unlearned counterparts on
both automatic and manual metrics.
While pure extraction is certainly simple and does
guarantee some minimal readability, Lin (2003)
showed that sentence compression (Knight and
Marcu, 2001; McDonald, 2006; Clarke and Lap-
ata, 2008) has the potential to improve the resulting
summaries. However, attempts to incorporate com-
pression into a summarization system have largely
failed to realize large gains. For example, Zajic et
al (2006) use a pipeline approach, pre-processing
to yield additional candidates for extraction by ap-
plying heuristic sentence compressions, but their
system does not outperform state-of-the-art purely
extractive systems. Similarly, Gillick and Favre
(2009), though not learning weights, do a limited
form of compression jointly with extraction. They
report a marginal increase in the automatic word-
overlap metric ROUGE (Lin, 2004), but a decline in
manual Pyramid (Nenkova and Passonneau, 2004).
A second contribution of the current work is to
show a system for jointly learning to jointly com-
press and extract that exhibits gains in both ROUGE
and content metrics over purely extractive systems.
Both Martins and Smith (2009) and Woodsend and
Lapata (2010) build models that jointly extract and
compress, but learn scores for sentences (or phrases)
using independent classifiers. Daume? III (2006)
481
learns parameters for compression and extraction
jointly using an approximate training procedure, but
his results are not competitive with state-of-the-art
extractive systems, and he does not report improve-
ments on manual content or quality metrics.
In our approach, we define a linear model that
scores candidate summaries according to features
that factor over the n-gram types that appear in the
summary and the structural compressions used to
create the sentences in the summary. We train these
parameters jointly using a margin-based objective
whose loss captures end summary quality through
the ROUGE metric. Because of the exponentially
large set of candidate summaries, we use a cutting
plane algorithm to incrementally detect and add ac-
tive constraints efficiently. To make joint learning
possible we introduce a new, manually-annotated
data set of extracted, compressed sentences. Infer-
ence in our model can be cast as an integer linear
program (ILP) and solved in reasonable time using
a generic ILP solver; we also introduce a fast ap-
proximation scheme which achieves similar perfor-
mance. Our jointly extracted and compressed sum-
maries outperform both unlearned baselines and our
learned extraction-only system on both ROUGE and
Pyramid, without a drop in judged linguistic quality.
We achieve the highest published comparable results
(ROUGE) to date on our test set.
2 Joint Model
We focus on the task of multi-document summariza-
tion. The input is a collection of documents, each
consisting of multiple sentences. The output is a
summary of length no greater than Lmax. Let x be
the input document set, and let y be a representation
of a summary as a vector. For an extractive sum-
mary, y is as a vector of indicators y = (ys : s ? x),
one indicator ys for each sentence s in x. A sentence
s is present in the summary if and only if its indica-
tor ys = 1 (see Figure 1a). Let Y (x) be the set of
valid summaries of document set x with length no
greater than Lmax.
While past extractive methods have assigned
value to individual sentences and then explicitly rep-
resented the notion of redundancy (Carbonell and
Goldstein, 1998), recent methods show greater suc-
cess by using a simpler notion of coverage: bigrams
Figure 1: Diagram of (a) extractive and (b) joint extrac-
tive and compressive summarization models. Variables
ys indicate the presence of sentences in the summary.
Variables yn indicate the presence of parse tree nodes.
Note that there is intentionally a bigram missing from (a).
contribute content, and redundancy is implicitly en-
coded in the fact that redundant sentences cover
fewer bigrams (Nenkova and Vanderwende, 2005;
Gillick and Favre, 2009). This later approach is as-
sociated with the following objective function:
max
y?Y (x)
?
b?B(y)
vb (1)
Here, vb is the value of bigram b, andB(y) is the set
of bigrams present in the summary encoded by y.
Gillick and Favre (2009) produced a state-of-the-art
system1 by directly optimizing this objective. They
let the value vb of each bigram be given by the num-
ber of input documents the bigram appears in. Our
implementation of their system will serve as a base-
line, referred to as EXTRACTIVE BASELINE.
We extend objective 1 so that it assigns value not
just to the bigrams that appear in the summary, but
also to the choices made in the creation of the sum-
mary. In our complete model, which jointly extracts
and compresses sentences, we choose whether or not
to cut individual subtrees in the constituency parses
1See Text Analysis Conference results in 2008 and 2009.
482
of each sentence. This is in contrast to the extractive
case where choices are made on full sentences.
max
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc (2)
C(y) is the set of cut choices made in y, and vc
assigns value to each.
Next, we present details of our representation of
compressive summaries. Assume a constituency
parse ts for every sentence s. We represent a com-
pressive summary as a vector y = (yn : n ? ts, s ?
x) of indicators, one for each non-terminal node in
each parse tree of the sentences in the document set
x. A word is present in the output summary if and
only if its parent parse tree node n has yn = 1 (see
Figure 1b). In addition to the length constraint on
the members of Y (x), we require that each node
n may have yn = 1 only if its parent pi(n) has
ypi(n) = 1. This ensures that only subtrees may
be deleted. While we use constituency parses rather
than dependency parses, this model has similarities
with the vine-growth model of Daume? III (2006).
For the compressive model we define the set of
cut choices C(y) for a summary y to be the set of
edges in each parse that are broken in order to delete
a subtree (see Figure 1b). We require that each sub-
tree has a non-terminal node for a root, and say that
an edge (n, pi(n)) between a node and its parent is
broken if the parent has ypi(n) = 1 but the child has
yn = 0. Notice that breaking a single edge deletes
an entire subtree.
2.1 Parameterization
Before learning weights in Section 3, we parameter-
ize objectives 1 and 2 using features. This entails to
parameterizing each bigram score vb and each sub-
tree deletion score vc. For weights w ? Rd and
feature functions g(b, x) ? Rd and h(c, x) ? Rd we
let:
vb = w
Tg(b, x)
vc = wTh(c, x)
For example, g(b, x) might include a feature the
counts the number of documents in x that b appears
in, and h(c, x) might include a feature that indicates
whether the deleted subtree is an SBAR modifying
a noun.
This parameterization allows us to cast summa-
rization as structured prediction. We can define a
feature function f(y, x) ? Rd which factors over
summaries y through B(y) and C(y):
f(y, x) =
?
b?B(y)
g(b, x) +
?
c?C(y)
h(c, x)
Using this characterization of summaries as feature
vectors we can define a linear predictor for summa-
rization:
d(x;w) = argmax
y?Y (x)
wTf(y, x) (3)
= argmax
y?Y (x)
?
b?B(y)
vb +
?
c?C(y)
vc
The arg max in Equation 3 optimizes Objective 2.
Learning weights for Objective 1 where Y (x) is
the set of extractive summaries gives our LEARNED
EXTRACTIVE system. Learning weights for Objec-
tive 2 where Y (x) is the set of compressive sum-
maries, and C(y) the set of broken edges that pro-
duce subtree deletions, gives our LEARNED COM-
PRESSIVE system, which is our joint model of ex-
traction and compression.
3 Structured Learning
Discriminative training attempts to minimize the
loss incurred during prediction by optimizing an ob-
jective on the training set. We will perform discrim-
inative training using a loss function that directly
measures end-to-end summarization quality.
In Section 4 we show that finding summaries that
optimize Objective 2, Viterbi prediction, is efficient.
Online learning algorithms like perceptron or the
margin-infused relaxed algorithm (MIRA) (Cram-
mer and Singer, 2003) are frequently used for struc-
tured problems where Viterbi inference is available.
However, we find that such methods are unstable on
our problem. We instead turn to an approach that
optimizes a batch objective which is sensitive to all
constraints on all instances, but is efficient by adding
these constraints incrementally.
3.1 Max-margin objective
For our problem the data set consists of pairs of doc-
ument sets and label summaries, D = {(xi,y?i ) :
i ? 1, . . . , N}. Note that the label summaries
483
can be expressed as vectors y? because our training
summaries are variously extractive or extractive and
compressive (see Section 5). We use a soft-margin
support vector machine (SVM) (Vapnik, 1998) ob-
jective over the full structured output space (Taskar
et al, 2003; Tsochantaridis et al, 2004) of extractive
and compressive summaries:
min
w
1
2
?w?2 +
C
N
N?
i=1
?i (4)
s.t. ?i,?y ? Y (xi) (5)
wT
(
f(y?i , xi)? f(y, xi)
)
? `(y,y?i )? ?i
The constraints in Equation 5 require that the differ-
ence in model score between each possible summary
y and the gold summary y?i be no smaller than the
loss `(y,y?i ), padded by a per-instance slack of ?i.
We use bigram recall as our loss function (see Sec-
tion 3.3). C is the regularization constant. When the
output space Y (xi) is small these constraints can be
explicitly enumerated. In this case it is standard to
solve the dual, which is a quadratic program. Un-
fortunately, the size of the output space of extractive
summaries is exponential in the number of sentences
in the input document set.
3.2 Cutting-plane algorithm
The cutting-plane algorithm deals with the expo-
nential number of constraints in Equation 5 by per-
forming constraint induction (Tsochantaridis et al,
2004). It alternates between solving Objective 4
with a reduced set of currently active constraints,
and adding newly active constraints to the set. In
our application, this approach efficiently solves the
structured SVM training problem up to some speci-
fied tolerance .
Suppose w? and ?? optimize Objective 4 under the
currently active constraints on a given iteration. No-
tice that the y?i satisfying
y?i = argmax
y?Y (xi)
[
w?Tf(y, xi) + `(y,y?i )
]
(6)
corresponds to the constraint in the fully constrained
problem, for training instance (xi,y?i ), most vio-
lated by w? and ??. On each round of constraint induc-
tion the cutting-plane algorithm computes the arg
max in Equation 6 for a training instance, which is
referred to as loss-augmented prediction, and adds
the corresponding constraint to the active set.
The constraints from Equation (5) are equivalent
to: ?i wTf(y?i , xi) ? maxy?Y (xi)
[
wTf(y, xi) +
`(y,y?i )
]
? ?i. Thus, if loss-augmented prediction
turns up no new constraints on a given iteration, the
current solution to the reduced problem, w? and ??,
is the solution to the full SVM training problem. In
practice, constraints are only added if the right hand
side of Equation (5) exceeds the left hand side by at
least . Tsochantaridis et al (2004) prove that only
O(N ) constraints are added before constraint induc-
tion finds a C-optimal solution.
Loss-augmented prediction is not always
tractable. Luckily, our choice of loss function,
bigram recall, factors over bigrams. Thus, we can
easily perform loss-augmented prediction using
the same procedure we use to perform Viterbi
prediction (described in Section 4). We simply
modify each bigram value vb to include bigram
b?s contribution to the total loss. We solve the
intermediate partially-constrained max-margin
problems using the factored sequential minimal
optimization (SMO) algorithm (Platt, 1999; Taskar
et al, 2004). In practice, for  = 10?4, the
cutting-plane algorithm converges after only three
passes through the training set when applied to our
summarization task.
3.3 Loss function
In the simplest case, 0-1 loss, the system only re-
ceives credit for exactly identifying the label sum-
mary. Since there are many reasonable summaries
we are less interested in exactly matching any spe-
cific training instance, and more interested in the de-
gree to which a predicted summary deviates from a
label.
The standard method for automatically evaluating
a summary against a reference is ROUGE, which we
simplify slightly to bigram recall. With an extractive
reference denoted by y?, our loss function is:
`(y,y?) =
|B(y)
?
B(y?)|
|B(y?)|
We verified that bigram recall correlates well with
ROUGE and with manual metrics.
484
4 Efficient Prediction
We show how to perform prediction with the extrac-
tive and compressive models by solving ILPs. For
many instances, a generic ILP solver can find exact
solutions to the prediction problems in a matter of
seconds. For difficult instances, we present a fast
approximate algorithm.
4.1 ILP for extraction
Gillick and Favre (2009) express the optimization of
Objective 1 for extractive summarization as an ILP.
We begin here with their algorithm. Let each input
sentence s have length ls. Let the presence of each
bigram b inB(y) be indicated by the binary variable
zb. LetQsb be an indicator of the presence of bigram
b in sentence s. They specify the following ILP over
binary variables y and z:
max
y,z
?
b
vbzb
s.t.
?
s
lsys ? Lmax
?b
?
s
Qsb ? zb (7)
?s, b ysQsb ? zb (8)
Constraints 7 and 8 ensure consistency between sen-
tences and bigrams. Notice that the Constraint 7 re-
quires that selecting a sentence entails selecting all
its bigrams, and Constraint 8 requires that selecting
a bigram entails selecting at least one sentence that
contains it. Solving the ILP is fast in practice. Us-
ing the GNU Linear Programming Kit (GLPK) on
a 3.2GHz Intel machine, decoding took less than a
second on most instances.
4.2 ILP for joint compression and extraction
We can extend the ILP formulation of extraction
to solve the compressive problem. Let ln be the
number of words node n has as children. With
this notation we can write the length restriction as
?
n lnyn ? Lmax. Let the presence of each cut c in
C(y) be indicated by the binary variable zc, which
is active if and only if yn = 0 but ypi(n) = 1, where
node pi(n) is the parent of node n. The constraints
on zc are diagrammed in Figure 2.
While it is possible to let B(y) contain all bi-
grams present in the compressive summary, the re-
Figure 2: Diagram of ILP for joint extraction and com-
pression. Variables zb indicate the presence of bigrams
in the summary. Variables zc indicate edges in the parse
tree that have been cut in order to remove subtrees. The
figure suppresses bigram variables zstopped,in and zfrance,he
to reduce clutter. Note that the edit shown is intentionally
bad. It demonstrates a loss of bigram coverage.
duction of B(y) makes the ILP formulation effi-
cient. We omit fromB(y) bigrams that are the result
of deleted intermediate words. As a result the re-
quired number of variables zb is linear in the length
of a sentence. The constraints on zb are given in
Figure 2. They can be expressed in terms of the vari-
ables yn.
By solving the following ILP we can compute the
arg max required for prediction in the joint model:
max
y,z
?
b
vbzb +
?
c
vczc
s.t.
?
n
lnyn ? Lmax
?n yn ? ypi(n) (9)
?b zb = 1
[
b ? B(y)
]
(10)
?c zc = 1
[
c ? C(y)
]
(11)
485
Constraint 9 encodes the requirement that only full
subtrees may be deleted. For simplicity, we have
written Constraints 10 and 11 in implicit form.
These constraints can be encoded explicitly using
O(N) linear constraints, where N is the number
of words in the document set x. The reduction of
B(y) to include only bigrams not resulting from
deleted intermediate words avoids O(N2) required
constraints.
In practice, solving this ILP for joint extraction
and compression is, on average, an order of magni-
tude slower than solving the ILP for pure extraction,
and for certain instances finding the exact solution is
prohibitively slow.
4.3 Fast approximate prediction
One common way to quickly approximate an ILP
is to solve its LP relaxation (and round the results).
We found that, while very fast, the LP relaxation of
the joint ILP gave poor results, finding unacceptably
suboptimal solutions. This appears possibly to have
been problematic for Martins and Smith (2009) as
well. We developed an alternative fast approximate
joint extractive and compressive solver that gives
better results in terms of both objective value and
bigram recall of resulting solutions.
The approximate joint solver first extracts a sub-
set of the sentences in the document set that total no
more than M words. In a second step, we apply the
exact joint extractive and compressive summarizer
(see Section 4.2) to the resulting extraction. The ob-
jective we maximize in performing the initial extrac-
tion is different from the one used in extractive sum-
marization. Specifically, we pick an extraction that
maximizes
?
s?y
?
b?s vb. This objective rewards
redundant bigrams, and thus is likely to give the joint
solver multiple options for including the same piece
of relevant content.
M is a parameter that trades-off between approx-
imation quality and problem difficulty. When M
is the size of the document set x, the approximate
solver solves the exact joint problem. In Figure 3
we plot the trade-off between approximation quality
and computation time, comparing to the exact joint
solver, an exact solver that is limited to extractive
solutions, and the LP relaxation solver. The results
show that the approximate joint solver yields sub-
stantial improvements over the LP relaxation, and
Figure 3: Plot of objective value, bigram recall, and
elapsed time for the approximate joint extractive and
compressive solver against size of intermediate extraction
set. Also shown are values for an LP relaxation approx-
imate solver, a solver that is restricted to extractive so-
lutions, and finally the exact compressive solver. These
solvers do not use an intermediate extraction. Results are
for 44 document sets, averaging about 5000 words per
document set.
can achieve results comparable to those produced by
the exact solver with a 5-fold reduction in compu-
tation time. On particularly difficult instances the
parameter M can be decreased, ensuring that all in-
stances are solved in a reasonable time period.
5 Data
We use the data from the Text Analysis Conference
(TAC) evaluations from 2008 and 2009, a total of
92 multi-document summarization problems. Each
problem asks for a 100-word-limited summary of
10 related input documents and provides a set of
four abstracts written by experts. These are the non-
update portions of the TAC 2008 and 2009 tasks.
To train the extractive system described in Sec-
tion 2, we use as our labels y? the extractions with
the largest bigram recall values relative to the sets
of references. While these extractions are inferior
to the abstracts, they are attainable by our model, a
quality found to be advantageous in discriminative
training for machine translation (Liang et al, 2006;
486
COUNT: 1(docCount(b) = ?) where docCount(b) is the
number of documents containing b.
STOP: 1(isStop(b1) = ?, isStop(b2) = ?) where
isStop(w) indicates a stop word.
POSITION: 1(docPosition(b) = ?) where docPosition(b) is
the earliest position in a document of any sen-
tence containing b, buckets earliest positions? 4.
CONJ: All two- and three-way conjunctions of COUNT,
STOP, and POSITION features.
BIAS: Bias feature, active on all bigrams.
Table 1: Bigram features: component feature functions
in g(b, x) that we use to characterize the bigram b in both
the extractive and compressive models.
Chiang et al, 2008).
Previous work has referred to the lack of ex-
tracted, compressed data sets as an obstacle to joint
learning for summarizaiton (Daume? III, 2006; Mar-
tins and Smith, 2009). We collected joint data via
a Mechanical Turk task. To make the joint anno-
tation task more feasible, we adopted an approx-
imate approach that closely matches our fast ap-
proximate prediction procedure. Annotators were
shown a 150-word maximum bigram recall extrac-
tions from the full document set and instructed to
form a compressed summary by deleting words un-
til 100 or fewer words remained. Each task was per-
formed by two annotators. We chose the summary
we judged to be of highest quality from each pair
to add to our corpus. This gave one gold compres-
sive summary y? for each of the 44 problems in the
TAC 2009 set. We used these labels to train our joint
extractive and compressive system described in Sec-
tion 2. Of the 288 total sentences presented to anno-
tators, 38 were unedited, 45 were deleted, and 205
were compressed by an average of 7.5 words.
6 Features
Here we describe the features used to parameterize
our model. Relative to some NLP tasks, our fea-
ture sets are small: roughly two hundred features
on bigrams and thirteen features on subtree dele-
tions. This is because our data set is small; with
only 48 training documents we do not have the sta-
tistical support to learn weights for more features.
For larger training sets one could imagine lexical-
ized versions of the features we describe.
COORD: Indicates phrase involved in coordination. Four
versions of this feature: NP, VP, S, SBAR.
S-ADJUNCT: Indicates a child of an S, adjunct to and left of
the matrix verb. Four version of this feature:
CC, PP, ADVP, SBAR.
REL-C: Indicates a relative clause, SBAR modifying a
noun.
ATTR-C: Indicates a sentence-final attribution clause,
e.g. ?the senator announced Friday.?
ATTR-PP: Indicates a PP attribution, e.g. ?according to the
senator.?
TEMP-PP: Indicates a temporal PP, e.g. ?on Friday.?
TEMP-NP: Indicates a temporal NP, e.g. ?Friday.?
BIAS: Bias feature, active on all subtree deletions.
Table 2: Subtree deletion features: component feature
functions in h(c, x) that we use to characterize the sub-
tree deleted by cutting edge c = (n, pi(n)) in the joint
extractive and compressive model.
6.1 Bigram features
Our bigram features include document counts, the
earliest position in a document of a sentence that
contains the bigram, and membership of each word
in a standard set of stopwords. We also include all
possible two- and three-way conjuctions of these
features. Table 1 describes the features in detail.
We use stemmed bigrams and prune bigrams that
appear in fewer than three input documents.
6.2 Subtree deletion features
Table 2 gives a description of our subtree tree dele-
tion features. Of course, by training to optimize a
metric like ROUGE, the system benefits from re-
strictions on the syntactic variety of edits; the learn-
ing is therefore more about deciding when an edit
is worth the coverage trade-offs rather than fine-
grained decisions about grammaticality.
We constrain the model to only allow subtree
deletions where one of the features in Table 2 (aside
from BIAS) is active. The root, and thus the entire
sentence, may always be cut. We choose this par-
ticular set of allowed deletions by looking at human
annotated data and taking note of the most common
types of edits. Edits which are made rarely by hu-
mans should be avoided in most scenarios, and we
simply don?t have enough data to learn when to do
them safely.
487
System BR R-2 R-SU4 Pyr LQ
LAST DOCUMENT 4.00 5.85 9.39 23.5 7.2
EXT. BASELINE 6.85 10.05 13.00 35.0 6.2
LEARNED EXT. 7.43 11.05 13.86 38.4 6.6
LEARNED COMP. 7.75 11.70 14.38 41.3 6.5
Table 3: Bigram Recall (BR), ROUGE (R-2 and R-SU4)
and Pyramid (Pyr) scores are multiplied by 100; Linguis-
tic Quality (LQ) is scored on a 1 (very poor) to 10 (very
good) scale.
7 Experiments
7.1 Experimental setup
We set aside the TAC 2008 data set (48 problems)
for testing and use the TAC 2009 data set (44 prob-
lems) for training, with hyper-parameters set to max-
imize six-fold cross-validation bigram recall on the
training set. We run the factored SMO algorithm
until convergence, and run the cutting-plane algo-
rithm until convergence for  = 10?4. We used
GLPK to solve all ILPs. We solved extractive ILPs
exactly, and joint extractive and compressive ILPs
approximately using an intermediate extraction size
of 1000. Constituency parses were produced using
the Berkeley parser (Petrov and Klein, 2007). We
show results for three systems, EXTRACTIVE BASE-
LINE, LEARNED EXTRACTIVE, LEARNED COM-
PRESSIVE, and the standard baseline that extracts
the first 100 words in the the most recent document,
LAST DOCUMENT.
7.2 Results
Our evaluation results are shown in Table 3.
ROUGE-2 (based on bigrams) and ROUGE-SU4
(based on both unigrams and skip-bigrams, sepa-
rated by up to four words) are given by the offi-
cial ROUGE toolkit with the standard options (Lin,
2004).
Pyramid (Nenkova and Passonneau, 2004) is a
manually evaluated measure of recall on facts or
Semantic Content Units appearing in the reference
summaries. It is designed to help annotators dis-
tinguish information content from linguistic qual-
ity. Two annotators performed the entire evaluation
without overlap by splitting the set of problems in
half.
To evaluate linguistic quality, we sent all the sum-
maries to Mechanical Turk (with two times redun-
System Sents Words/Sent Word Types
LAST DOCUMENT 4.0 25.0 36.5
EXT. BASELINE 5.0 20.8 36.3
LEARNED EXT. 4.8 21.8 37.1
LEARNED COMP. 4.5 22.9 38.8
Table 4: Summary statistics for the summaries gener-
ated by each system: Average number of sentences per
summary, average number of words per summary sen-
tence, and average number of non-stopword word types
per summary.
dancy), using the template and instructions designed
by Gillick and Liu (2010). They report that Turk-
ers can faithfully reproduce experts? rankings of av-
erage system linguistic quality (though their judge-
ments of content are poorer). The table shows aver-
age linguistic quality.
All the content-based metrics show substantial
improvement for learned systems over unlearned
ones, and we see an extremely large improvement
for the learned joint extractive and compressive sys-
tem over the previous state-of-the-art EXTRACTIVE
BASELINE. The ROUGE scores for the learned
joint system, LEARNED COMPRESSIVE, are, to our
knowledge, the highest reported on this task. We
cannot compare Pyramid scores to other reported
scores because of annotator difference. As expected,
the LAST DOCUMENT baseline outperforms other
systems in terms of linguistic quality. But, impor-
tantly, the gains achieved by the joint extractive and
compressive system in content-based metrics do not
come at the cost of linguistic quality when compared
to purely extractive systems.
Table 4 shows statistics on the outputs of the sys-
tems we evaluated. The joint extractive and com-
pressive system fits more word types into a sum-
mary than the extractive systems, but also produces
longer sentences on average. Reading the output
summaries more carefully suggests that by learning
to extract and compress jointly, our joint system has
the flexibility to use or create reasonable, medium-
length sentences, whereas the extractive systems are
stuck with a few valuable long sentences, but several
less productive shorter sentences. Example sum-
maries produced by the joint system are given in Fig-
ure 4 along with reference summaries produced by
humans.
488
LEARNED COMPRESSIVE: The country?s work safety authority will
release the list of the first batch of coal mines to be closed down said
Wang Xianzheng, deputy director of the National Bureau of Produc-
tion Safety Supervision and Administration. With its coal mining
safety a hot issue, attracting wide attention from both home and over-
seas, China is seeking solutions from the world to improve its coal
mining safety system. Despite government promises to stem the car-
nage the death toll in China?s disaster-plagued coal mine industry is
rising according to the latest statistics released by the government Fri-
day. Fatal coal mine accidents in China rose 8.5 percent in the first
eight months of this year with thousands dying despite stepped-up ef-
forts to make the industry safer state media said Wednesday.
REFERENCE: China?s accident-plagued coal mines cause thousands
of deaths and injuries annually. 2004 saw over 6,000 mine deaths.
January through August 2005, deaths rose 8.5% over the same period
in 2004. Most accidents are gas explosions, but fires, floods, and cave-
ins also occur. Ignored safety procedures, outdated equipment, and
corrupted officials exacerbate the problem. Official responses include
shutting down thousands of ill-managed and illegally-run mines, pun-
ishing errant owners, issuing new safety regulations and measures,
and outlawing local officials from investing in mines. China also
sought solutions at the Conference on South African Coal Mining
Safety Technology and Equipment held in Beijing.
LEARNED COMPRESSIVE: Karl Rove the White House deputy chief
of staff told President George W. Bush and others that he never en-
gaged in an effort to disclose a CIA operative?s identity to discredit
her husband?s criticism of the administration?s Iraq policy according
to people with knowledge of Rove?s account in the investigation. In a
potentially damaging sign for the Bush administration special counsel
Patrick Fitzgerald said that although his investigation is nearly com-
plete it?s not over. Lewis Scooter Libby Vice President Dick Cheney?s
chief of staff and a key architect of the Iraq war was indicted Friday on
felony charges of perjury making false statements to FBI agents and
obstruction of justice for impeding the federal grand jury investigating
the CIA leak case.
REFERENCE: Special Prosecutor Patrick Fitzgerald is investigating
who leaked to the press that Valerie Plame, wife of former Ambas-
sador Joseph Wilson, was an undercover CIA agent. Wilson was a
critic of the Bush administration. Administration staffers Karl Rove
and I. Lewis Libby are the focus of the investigation. NY Times cor-
respondent Judith Miller was jailed for 85 days for refusing to testify
about Libby. Libby was eventually indicted on five counts: 2 false
statements, 1 obstruction of justice, 2 perjury. Libby resigned imme-
diately. He faces 30 years in prison and a fine of $1.25 million if
convicted. Libby pleaded not guilty.
Figure 4: Example summaries produced by our learned
joint model of extraction and compression. These are
each 100-word-limited summaries of a collection of ten
documents from the TAC 2008 data set. Constituents that
have been removed via subtree deletion are grayed out.
References summaries produced by humans are provided
for comparison.
8 Conclusion
Jointly learning to extract and compress within a
unified model outperforms learning pure extraction,
which in turn outperforms a state-of-the-art extrac-
tive baseline. Our system gives substantial increases
in both automatic and manual content metrics, while
maintaining high linguistic quality scores.
Acknowledgements
We thank the anonymous reviewers for their com-
ments. This project is supported by DARPA under
grant N10AP20007.
References
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression: An Integer Linear Programming
Approach. Journal of Artificial Intelligence Research,
31:399?429.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research, 3:951?991.
H.C. Daume? III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proc. of ACL Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing.
D. Gillick and Y. Liu. 2010. Non-Expert Evaluation of
Summarization Systems is Risky. In Proc. of NAACL
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
K. Knight and D. Marcu. 2001. Statistics-based
summarization-step one: Sentence compression. In
Proc. of AAAI.
L. Li, K. Zhou, G.R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proc. of the 18th
International Conference on World Wide Web.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of the ACL.
489
C.Y. Lin. 2003. Improving summarization performance
by sentence compression: a pilot study. In Proc. of
ACL Workshop on Information Retrieval with Asian
Languages.
C.Y. Lin. 2004. Rouge: A package for automatic evalua-
tion of summaries. In Proc. of ACL Workshop on Text
Summarization Branches Out.
A.F.T. Martins and N.A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In Proc. of NAACL Workshop on Integer Lin-
ear Programming for Natural Language Processing.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
A. Nenkova and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: The pyramid method.
In Proc. of NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical report, MSR-
TR-2005-101. Redmond, Washington: Microsoft Re-
search.
S. Petrov and D. Klein. 2007. Learning and inference for
hierarchically split PCFGs. In AAAI.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
Kernel Methods. MIT press.
F. Schilder and R. Kondadadi. 2008. Fastsum: Fast and
accurate query-based multi-document summarization.
In Proc. of ACL.
D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen. 2007.
Document summarization using conditional random
fields. In Proc. of IJCAI.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. of NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
S. Teufel and M. Moens. 1997. Sentence extraction as
a classification task. In Proc. of ACL Workshop on
Intelligent and Scalable Text Summarization.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
V.N. Vapnik. 1998. Statistical learning theory. John
Wiley and Sons, New York.
K. Woodsend and M. Lapata. 2010. Automatic genera-
tion of story highlights. In Proc. of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content-words. In Proc. of IJCAI.
D.M. Zajic, B.J. Dorr, R. Schwartz, and J. Lin. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the 2006
Document Understanding Workshop.
490
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 590?599,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Dependency-Based Compositional Semantics
Percy Liang
UC Berkeley
pliang@cs.berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
Compositional question answering begins by
mapping questions to logical forms, but train-
ing a semantic parser to perform this mapping
typically requires the costly annotation of the
target logical forms. In this paper, we learn
to map questions to answers via latent log-
ical forms, which are induced automatically
from question-answer pairs. In tackling this
challenging learning problem, we introduce a
new semantic representation which highlights
a parallel between dependency syntax and effi-
cient evaluation of logical forms. On two stan-
dard semantic parsing benchmarks (GEO and
JOBS), our system obtains the highest pub-
lished accuracies, despite requiring no anno-
tated logical forms.
1 Introduction
What is the total population of the ten largest cap-
itals in the US? Answering these types of complex
questions compositionally involves first mapping the
questions into logical forms (semantic parsing). Su-
pervised semantic parsers (Zelle and Mooney, 1996;
Tang and Mooney, 2001; Ge and Mooney, 2005;
Zettlemoyer and Collins, 2005; Kate and Mooney,
2007; Zettlemoyer and Collins, 2007; Wong and
Mooney, 2007; Kwiatkowski et al, 2010) rely on
manual annotation of logical forms, which is expen-
sive. On the other hand, existing unsupervised se-
mantic parsers (Poon and Domingos, 2009) do not
handle deeper linguistic phenomena such as quan-
tification, negation, and superlatives.
As in Clarke et al (2010), we obviate the need
for annotated logical forms by considering the end-
to-end problem of mapping questions to answers.
However, we still model the logical form (now as a
latent variable) to capture the complexities of lan-
guage. Figure 1 shows our probabilistic model:
(parameters) (world)
? w
x z y
(question) (logical form) (answer)state with thelargest area x1
11
c
argmax
area
state
?? Alaska
z ? p?(z | x)
y = JzKw
Semantic Parsing Evaluation
Figure 1: Our probabilistic model: a question x is
mapped to a latent logical form z, which is then evaluated
with respect to a world w (database of facts), producing
an answer y. We represent logical forms z as labeled
trees, induced automatically from (x, y) pairs.
We want to induce latent logical forms z (and pa-
rameters ?) given only question-answer pairs (x, y),
which is much cheaper to obtain than (x, z) pairs.
The core problem that arises in this setting is pro-
gram induction: finding a logical form z (over an
exponentially large space of possibilities) that pro-
duces the target answer y. Unlike standard semantic
parsing, our end goal is only to generate the correct
y, so we are free to choose the representation for z.
Which one should we use?
The dominant paradigm in compositional se-
mantics is Montague semantics, which constructs
lambda calculus forms in a bottom-up manner. CCG
is one instantiation (Steedman, 2000), which is used
by many semantic parsers, e.g., Zettlemoyer and
Collins (2005). However, the logical forms there
can become quite complex, and in the context of
program induction, this would lead to an unwieldy
search space. At the same time, representations such
as FunQL (Kate et al, 2005), which was used in
590
Clarke et al (2010), are simpler but lack the full ex-
pressive power of lambda calculus.
The main technical contribution of this work is
a new semantic representation, dependency-based
compositional semantics (DCS), which is both sim-
ple and expressive (Section 2). The logical forms in
this framework are trees, which is desirable for two
reasons: (i) they parallel syntactic dependency trees,
which facilitates parsing and learning; and (ii) eval-
uating them to obtain the answer is computationally
efficient.
We trained our model using an EM-like algorithm
(Section 3) on two benchmarks, GEO and JOBS
(Section 4). Our system outperforms all existing
systems despite using no annotated logical forms.
2 Semantic Representation
We first present a basic version (Section 2.1) of
dependency-based compositional semantics (DCS),
which captures the core idea of using trees to rep-
resent formal semantics. We then introduce the full
version (Section 2.2), which handles linguistic phe-
nomena such as quantification, where syntactic and
semantic scope diverge.
We start with some definitions, using US geogra-
phy as an example domain. Let V be the set of all
values, which includes primitives (e.g., 3, CA ? V)
as well as sets and tuples formed from other values
(e.g., 3, {3, 4, 7}, (CA, {5}) ? V). Let P be a set
of predicates (e.g., state, count ? P), which are
just symbols.
A world w is mapping from each predicate p ?
P to a set of tuples; for example, w(state) =
{(CA), (OR), . . . }. Conceptually, a world is a rela-
tional database where each predicate is a relation
(possibly infinite). Define a special predicate ? with
w(?) = V . We represent functions by a set of input-
output pairs, e.g., w(count) = {(S, n) : n = |S|}.
As another example, w(average) = {(S, x?) :
x? = |S1|?1
?
x?S1 S(x)}, where a set of pairs S
is treated as a set-valued function S(x) = {y :
(x, y) ? S} with domain S1 = {x : (x, y) ? S}.
The logical forms in DCS are called DCS trees,
where nodes are labeled with predicates, and edges
are labeled with relations. Formally:
Definition 1 (DCS trees) Let Z be the set of DCS
trees, where each z ? Z consists of (i) a predicate
RelationsR
j
j? (join) E (extract)
? (aggregate) Q (quantify)
Xi (execute) C (compare)
Table 1: Possible relations appearing on the edges of a
DCS tree. Here, j, j? ? {1, 2, . . . } and i ? {1, 2, . . . }?.
z.p ? P and (ii) a sequence of edges z.e1, . . . , z.em,
each edge e consisting of a relation e.r ? R (see
Table 1) and a child tree e.c ? Z .
We write a DCS tree z as ?p; r1 : c1; . . . ; rm : cm?.
Figure 2(a) shows an example of a DCS tree. Al-
though a DCS tree is a logical form, note that it looks
like a syntactic dependency tree with predicates in
place of words. It is this transparency between syn-
tax and semantics provided by DCS which leads to
a simple and streamlined compositional semantics
suitable for program induction.
2.1 Basic Version
The basic version of DCS restrictsR to join and ag-
gregate relations (see Table 1). Let us start by con-
sidering a DCS tree z with only join relations. Such
a z defines a constraint satisfaction problem (CSP)
with nodes as variables. The CSP has two types of
constraints: (i) x ? w(p) for each node x labeled
with predicate p ? P; and (ii) xj = yj? (the j-th
component of x must equal the j?-th component of
y) for each edge (x, y) labeled with jj? ? R.
A solution to the CSP is an assignment of nodes
to values that satisfies all the constraints. We say a
value v is consistent for a node x if there exists a
solution that assigns v to x. The denotation JzKw (z
evaluated on w) is the set of consistent values of the
root node (see Figure 2 for an example).
Computation We can compute the denotation
JzKw of a DCS tree z by exploiting dynamic pro-
gramming on trees (Dechter, 2003). The recurrence
is as follows:
J
?
p; j1j?1 :c1; ? ? ? ;
jm
j?m
:cm
?
K
w
(1)
= w(p) ?
m?
i=1
{v : vji = tj?i , t ? JciKw}.
At each node, we compute the set of tuples v consis-
tent with the predicate at that node (v ? w(p)), and
591
Example: major city in California
z = ?city; 11 :?major? ; 11 :?loc; 21 :?CA???
1
1
1
1
major
2
1
CA
loc
city ?c?m?`?s .
city(c) ? major(m)?
loc(`) ? CA(s)?c1 = m1 ? c1 = `1 ? `2 = s1
(a) DCS tree (b) Lambda calculus formula
(c) Denotation: JzKw = {SF, LA, . . . }
Figure 2: (a) An example of a DCS tree (written in both
the mathematical and graphical notation). Each node is
labeled with a predicate, and each edge is labeled with a
relation. (b) A DCS tree z with only join relations en-
codes a constraint satisfaction problem. (c) The denota-
tion of z is the set of consistent values for the root node.
for each child i, the ji-th component of v must equal
the j?i-th component of some t in the child?s deno-
tation (t ? JciKw). This algorithm is linear in the
number of nodes times the size of the denotations.1
Now the dual importance of trees in DCS is clear:
We have seen that trees parallel syntactic depen-
dency structure, which will facilitate parsing. In
addition, trees enable efficient computation, thereby
establishing a new connection between dependency
syntax and efficient semantic evaluation.
Aggregate relation DCS trees that only use join
relations can represent arbitrarily complex compo-
sitional structures, but they cannot capture higher-
order phenomena in language. For example, con-
sider the phrase number of major cities, and suppose
that number corresponds to the count predicate.
It is impossible to represent the semantics of this
phrase with just a CSP, so we introduce a new ag-
gregate relation, notated ?. Consider a tree ??:c?,
whose root is connected to a child c via ?. If the de-
notation of c is a set of values s, the parent?s denota-
tion is then a singleton set containing s. Formally:
J??:c?Kw = {JcKw}. (2)
Figure 3(a) shows the DCS tree for our running
example. The denotation of the middle node is {s},
1Infinite denotations (such as J<Kw) are represented as im-
plicit sets on which we can perform membership queries. The
intersection of two sets can be performed as long as at least one
of the sets is finite.
number ofmajor cities
12
11
?
11major
city
??
count
??
average population ofmajor cities
12
11
?
11
11major
city
population
??
average
??
(a) Counting (b) Averaging
Figure 3: Examples of DCS trees that use the aggregate
relation (?) to (a) compute the cardinality of a set and (b)
take the average over a set.
where s is all major cities. Having instantiated s as
a value, everything above this node is an ordinary
CSP: s constrains the count node, which in turns
constrains the root node to |s|.
A DCS tree that contains only join and aggre-
gate relations can be viewed as a collection of tree-
structured CSPs connected via aggregate relations.
The tree structure still enables us to compute deno-
tations efficiently based on (1) and (2).
2.2 Full Version
The basic version of DCS described thus far han-
dles a core subset of language. But consider Fig-
ure 4: (a) is headed by borders, but states needs
to be extracted; in (b), the quantifier no is syntacti-
cally dominated by the head verb borders but needs
to take wider scope. We now present the full ver-
sion of DCS which handles this type of divergence
between syntactic and semantic scope.
The key idea that allows us to give semantically-
scoped denotations to syntactically-scoped trees is
as follows: We mark a node low in the tree with a
mark relation (one of E, Q, or C). Then higher up in
the tree, we invoke it with an execute relation Xi to
create the desired semantic scope.2
This mark-execute construct acts non-locally, so
to maintain compositionality, we must augment the
2Our mark-execute construct is analogous to Montague?s
quantifying in, Cooper storage, and Carpenter?s scoping con-
structor (Carpenter, 1998).
592
California borders which states?
x1
2 111CA
e
??
state
border
?? Alaska borders no states.
x1
2 111AK
q
no
state
border
?? Some river traverses every city.
x12
2 111
q
some
river
q
every
city
traverse
??
x21
2 111
q
some
river
q
every
city
traverse
??
(narrow) (wide)
city traversed by no rivers
x12
1 2e?? 11
q
no
river
traverse
city
??
(a) Extraction (e) (b) Quantification (q) (c) Quantifier ambiguity (q,q) (d) Quantification (q,e)
state borderingthe most states
x12
1 1e?? 21
c
argmax
state
border
state
??
state borderingmore states than Texas
x12
1 1e?? 21
c
31TX
more
state
border
state
??
state borderingthe largest state
11
21
x12
1 1e??
c
argmax
size
state
??
border
state
x12
1 1e?? 21
11
c
argmax
size
state
border
state
??
(absolute) (relative)
Every state?slargest city is major.
x1
x2
1 111
21
q
every
state
loc
c
argmax
size
city
major
??
(e) Superlative (c) (f) Comparative (c) (g) Superlative ambiguity (c) (h) Quantification+Superlative (q,c)
Figure 4: Example DCS trees for utterances in which syntactic and semantic scope diverge. These trees reflect the
syntactic structure, which facilitates parsing, but importantly, these trees also precisely encode the correct semantic
scope. The main mechanism is using a mark relation (E, Q, or C) low in the tree paired with an execute relation (Xi)
higher up at the desired semantic point.
denotation d = JzKw to include any information
about the marked nodes in z that can be accessed
by an execute relation later on. In the basic ver-
sion, d was simply the consistent assignments to the
root. Now d contains the consistent joint assign-
ments to the active nodes (which include the root
and all marked nodes), as well as information stored
about each marked node. Think of d as consisting
of n columns, one for each active node according to
a pre-order traversal of z. Column 1 always corre-
sponds to the root node. Formally, a denotation is
defined as follows (see Figure 5 for an example):
Definition 2 (Denotations) Let D be the set of de-
notations, where each d ? D consists of
? a set of arrays d.A, where each array a =
[a1, . . . , an] ? d.A is a sequence of n tuples
(ai ? V?); and
? a list of n stores d.? = (d.?1, . . . , d.?n),
where each store ? contains a mark relation
?.r ? {E, Q, C, ?}, a base denotation ?.b ?
D?{?}, and a child denotation ?.c ? D?{?}.
We write d as ??A; (r1, b1, c1); . . . ; (rn, bn, cn)??. We
use d{ri = x} to mean d with d.ri = d.?i.r = x
(similar definitions apply for d{?i = x}, d{bi = x},
and d{ci = x}).
The denotation of a DCS tree can now be defined
recursively:
J?p?Kw = ??{[v] : v ? w(p)}; ???, (3)
J
?
p; e; jj? :c
?
K
w
= Jp; eKw ./j,j? JcKw, (4)
J?p; e; ?:c?Kw = Jp; eKw ./?,? ? (JcKw) , (5)
J?p; e; Xi :c?Kw = Jp; eKw ./?,? Xi(JcKw), (6)
J?p; e; E :c?Kw = M(Jp; eKw, E, c), (7)
J?p; e; C :c?Kw = M(Jp; eKw, C, c), (8)
J?p; Q :c; e?Kw = M(Jp; eKw, Q, c). (9)
593
11
21
11
c
argmax
size
state
border
state
J?Kw
column 1 column 2
A:
(OK)(NM)(NV)? ? ?
(TX,2.7e5)(TX,2.7e5)(CA,1.6e5)? ? ?r: ? cb: ? J?size?Kwc: ? J?argmax?KwDCS tree Denotation
Figure 5: Example of the denotation for a DCS tree with
a compare relation C. This denotation has two columns,
one for each active node?the root node state and the
marked node size.
The base case is defined in (3): if z is a sin-
gle node with predicate p, then the denotation of z
has one column with the tuples w(p) and an empty
store. The other six cases handle different edge re-
lations. These definitions depend on several opera-
tions (./j,j? ,?,Xi,M) which we will define shortly,
but let us first get some intuition.
Let z be a DCS tree. If the last child c of z?s
root is a join ( jj?), aggregate (?), or execute (Xi) re-
lation ((4)?(6)), then we simply recurse on z with c
removed and join it with some transformation (iden-
tity, ?, or Xi) of c?s denotation. If the last (or first)
child is connected via a mark relation E, C (or Q),
then we strip off that child and put the appropriate
information in the store by invoking M.
We now define the operations ./j,j? ,?,Xi,M.
Some helpful notation: For a sequence v =
(v1, . . . , vn) and indices i = (i1, . . . , ik), let vi =
(vi1 , . . . , vik) be the projection of v onto i; we write
v?i to mean v[1,...,n]\i. Extending this notation to
denotations, let ??A;???[i] = ??{ai : a ? A};?i??.
Let d[??] = d[?i], where i are the columns with
empty stores. For example, for d in Figure 5, d[1]
keeps column 1, d[??] keeps column 2, and d[2,?2]
swaps the two columns.
Join The join of two denotations d and d? with re-
spect to components j and j? (? means all compo-
nents) is formed by concatenating all arrays a of d
with all compatible arrays a? of d?, where compat-
ibility means a1j = a?1j? . The stores are also con-
catenated (?+??). Non-initial columns with empty
stores are projected away by applying ?[1,??]. The
full definition of join is as follows:
??A;??? ./j,j? ??A
?;???? = ??A??;? + ????[1,??],
A?? = {a + a? : a ? A,a? ? A?, a1j = a
?
1j?}. (10)
Aggregate The aggregate operation takes a deno-
tation and forms a set out of the tuples in the first
column for each setting of the rest of the columns:
? (??A;???) = ??A? ?A??;??? (11)
A? = {[S(a), a2, . . . , an] : a ? A}
S(a) = {a?1 : [a
?
1, a2, . . . , an] ? A}
A?? = {[?, a2, . . . , an] : ??a1,a ? A,
?2 ? i ? n, [ai] ? d.bi[1].A}.
2.2.1 Mark and Execute
Now we turn to the mark (M) and execute (Xi)
operations, which handles the divergence between
syntactic and semantic scope. In some sense, this is
the technical core of DCS. Marking is simple: When
a node (e.g., size in Figure 5) is marked (e.g., with
relation C), we simply put the relation r, current de-
notation d and child c?s denotation into the store of
column 1:
M(d, r, c) = d{r1 = r, b1 = d, c1 = JcKw}. (12)
The execute operation Xi(d) processes columns
i in reverse order. It suffices to define Xi(d) for a
single column i. There are three cases:
Extraction (d.ri = E) In the basic version, the
denotation of a tree was always the set of con-
sistent values of the root node. Extraction al-
lows us to return the set of consistent values of a
marked non-root node. Formally, extraction sim-
ply moves the i-th column to the front: Xi(d) =
d[i,?(i, ?)]{?1 = ?}. For example, in Figure 4(a),
before execution, the denotation of the DCS tree
is ??{[(CA, OR), (OR)], . . . }; ?; (E, J?state?Kw, ?)??;
after applying X1, we have ??{[(OR)], . . . }; ???.
Generalized Quantification (d.ri = Q) Gener-
alized quantifiers are predicates on two sets, a re-
strictor A and a nuclear scope B. For example,
w(no) = {(A,B) : A ? B = ?} and w(most) =
{(A,B) : |A ?B| > 12 |A|}.
In a DCS tree, the quantifier appears as the
child of a Q relation, and the restrictor is the par-
ent (see Figure 4(b) for an example). This in-
formation is retrieved from the store when the
594
quantifier in column i is executed. In particu-
lar, the restrictor is A = ? (d.bi) and the nu-
clear scope is B = ? (d[i,?(i, ?)]). We then
apply d.ci to these two sets (technically, denota-
tions) and project away the first column: Xi(d) =
((d.ci ./1,1 A) ./2,1 B) [?1].
For the example in Figure 4(b), the de-
notation of the DCS tree before execution is
???; ?; (Q, J?state?Kw, J?no?Kw)??. The restrictor
set (A) is the set of all states, and the nuclear scope
(B) is the empty set. Since (A,B) exists in no, the
final denotation, which projects away the actual pair,
is ??{[ ]}?? (our representation of true).
Figure 4(c) shows an example with two interact-
ing quantifiers. The quantifier scope ambiguity is
resolved by the choice of execute relation; X12 gives
the narrow reading and X21 gives the wide reading.
Figure 4(d) shows how extraction and quantification
work together.
Comparatives and Superlatives (d.ri = C) To
compare entities, we use a set S of (x, y) pairs,
where x is an entity and y is a number. For su-
perlatives, the argmax predicate denotes pairs of
sets and the set?s largest element(s): w(argmax) =
{(S, x?) : x? ? argmaxx?S1 maxS(x)}. For com-
paratives, w(more) contains triples (S, x, y), where
x is ?more than? y as measured by S; formally:
w(more) = {(S, x, y) : maxS(x) > maxS(y)}.
In a superlative/comparative construction, the
root x of the DCS tree is the entity to be compared,
the child c of a C relation is the comparative or su-
perlative, and its parent p contains the information
used for comparison (see Figure 4(e) for an exam-
ple). If d is the denotation of the root, its i-th column
contains this information. There are two cases: (i) if
the i-th column of d contains pairs (e.g., size in
Figure 5), then let d? = J???Kw ./1,2 d[i,?i], which
reads out the second components of these pairs; (ii)
otherwise (e.g., state in Figure 4(e)), let d? =
J???Kw ./1,2 J?count?Kw ./1,1 ? (d[i,?i]), which
counts the number of things (e.g., states) that occur
with each value of the root x. Given d?, we construct
a denotation S by concatenating (+i) the second and
first columns of d? (S = ? (+2,1 (d?{?2 = ?})))
and apply the superlative/comparative: Xi(d) =
(J???Kw ./1,2 (d.ci ./1,1 S)){?1 = d.?1}.
Figure 4(f) shows that comparatives are handled
using the exact same machinery as superlatives. Fig-
ure 4(g) shows that we can naturally account for
superlative ambiguity based on where the scope-
determining execute relation is placed.
3 Semantic Parsing
We now turn to the task of mapping natural language
utterances to DCS trees. Our first question is: given
an utterance x, what trees z ? Z are permissible? To
define the search space, we first assume a fixed set
of lexical triggers L. Each trigger is a pair (x, p),
where x is a sequence of words (usually one) and p
is a predicate (e.g., x = California and p = CA).
We use L(x) to denote the set of predicates p trig-
gered by x ((x, p) ? L). Let L() be the set of
trace predicates, which can be introduced without
an overt lexical trigger.
Given an utterance x = (x1, . . . , xn), we define
ZL(x) ? Z , the set of permissible DCS trees for
x. The basic approach is reminiscent of projective
labeled dependency parsing: For each span i..j, we
build a set of trees Ci,j and set ZL(x) = C0,n. Each
set Ci,j is constructed recursively by combining the
trees of its subspans Ci,k and Ck?,j for each pair of
split points k, k? (words between k and k? are ig-
nored). These combinations are then augmented via
a functionA and filtered via a functionF , to be spec-
ified later. Formally, Ci,j is defined recursively as
follows:
Ci,j = F
(
A
(
L(xi+1..j) ?
?
i?k?k?<j
a?Ci,k
b?Ck?,j
T1(a, b))
))
.
(13)
In (13), L(xi+1..j) is the set of predicates triggered
by the phrase under span i..j (the base case), and
Td(a, b) = ~Td(a, b) ? ~T d(b, a), which returns all
ways of combining trees a and b where b is a de-
scendant of a (~Td) or vice-versa ( ~T d). The former is
defined recursively as follows: ~T0(a, b) = ?, and
~Td(a, b) =
?
r?R
p?L()
{?a; r :b?} ? ~Td?1(a, ?p; r :b?).
The latter ( ~T k) is defined similarly. Essentially,
~Td(a, b) allows us to insert up to d trace predi-
cates between the roots of a and b. This is use-
ful for modeling relations in noun compounds (e.g.,
595
California cities), and it also allows us to underspec-
ify L. In particular, our L will not include verbs or
prepositions; rather, we rely on the predicates corre-
sponding to those words to be triggered by traces.
The augmentation function A takes a set of trees
and optionally attaches E and Xi relations to the
root (e.g., A(?city?) = {?city? , ?city; E :??}).
The filtering function F rules out improperly-typed
trees such as ?city; 00 :?state??. To further reduce
the search space, F imposes a few additional con-
straints, e.g., limiting the number of marked nodes
to 2 and only allowing trace predicates between ar-
ity 1 predicates.
Model We now present our discriminative se-
mantic parsing model, which places a log-linear
distribution over z ? ZL(x) given an utter-
ance x. Formally, p?(z | x) ? e?(x,z)
>?,
where ? and ?(x, z) are parameter and feature vec-
tors, respectively. As a running example, con-
sider x = city that is in California and z =
?city; 11 :?loc;
2
1 :?CA???, where city triggers city
and California triggers CA.
To define the features, we technically need to
augment each tree z ? ZL(x) with alignment
information?namely, for each predicate in z, the
span in x (if any) that triggered it. This extra infor-
mation is already generated from the recursive defi-
nition in (13).
The feature vector ?(x, z) is defined by sums of
five simple indicator feature templates: (F1) a word
triggers a predicate (e.g., [city, city]); (F2) a word
is under a relation (e.g., [that, 11]); (F3) a word is un-
der a trace predicate (e.g., [in, loc]); (F4) two pred-
icates are linked via a relation in the left or right
direction (e.g., [city, 11, loc, RIGHT]); and (F5) a
predicate has a child relation (e.g., [city, 11]).
Learning Given a training dataset D con-
taining (x, y) pairs, we define the regu-
larized marginal log-likelihood objective
O(?) =
?
(x,y)?D log p?(JzKw = y | x, z ?
ZL(x)) ? ????22, which sums over all DCS trees z
that evaluate to the target answer y.
Our model is arc-factored, so we can sum over all
DCS trees in ZL(x) using dynamic programming.
However, in order to learn, we need to sum over
{z ? ZL(x) : JzKw = y}, and unfortunately, the
additional constraint JzKw = y does not factorize.
We therefore resort to beam search. Specifically, we
truncate each Ci,j to a maximum of K candidates
sorted by decreasing score based on parameters ?.
Let Z?L,?(x) be this approximation of ZL(x).
Our learning algorithm alternates between (i) us-
ing the current parameters ? to generate the K-best
set Z?L,?(x) for each training example x, and (ii)
optimizing the parameters to put probability mass
on the correct trees in these sets; sets contain-
ing no correct answers are skipped. Formally, let
O?(?, ??) be the objective function O(?) with ZL(x)
replaced with Z?L,??(x). We optimize O?(?, ??) by
setting ?(0) = ~0 and iteratively solving ?(t+1) =
argmax? O?(?, ?
(t)) using L-BFGS until t = T . In all
experiments, we set ? = 0.01, T = 5, andK = 100.
After training, given a new utterance x, our system
outputs the most likely y, summing out the latent
logical form z: argmaxy p?(T )(y | x, z ? Z?L,?(T )).
4 Experiments
We tested our system on two standard datasets, GEO
and JOBS. In each dataset, each sentence x is an-
notated with a Prolog logical form, which we use
only to evaluate and get an answer y. This evalua-
tion is done with respect to a world w. Recall that
a world w maps each predicate p ? P to a set of
tuples w(p). There are three types of predicates in
P: generic (e.g., argmax), data (e.g., city), and
value (e.g., CA). GEO has 48 non-value predicates
and JOBS has 26. For GEO, w is the standard US
geography database that comes with the dataset. For
JOBS, if we use the standard Jobs database, close to
half the y?s are empty, which makes it uninteresting.
We therefore generated a random Jobs database in-
stead as follows: we created 100 job IDs. For each
data predicate p (e.g., language), we add each pos-
sible tuple (e.g., (job37, Java)) to w(p) indepen-
dently with probability 0.8.
We used the same training-test splits as Zettle-
moyer and Collins (2005) (600+280 for GEO and
500+140 for JOBS). During development, we fur-
ther held out a random 30% of the training sets for
validation.
Our lexical triggers L include the following: (i)
predicates for a small set of ? 20 function words
(e.g., (most, argmax)), (ii) (x, x) for each value
596
System Accuracy
Clarke et al (2010) w/answers 73.2
Clarke et al (2010) w/logical forms 80.4
Our system (DCS with L) 78.9
Our system (DCS with L+) 87.2
Table 2: Results on GEO with 250 training and 250
test examples. Our results are averaged over 10 random
250+250 splits taken from our 600 training examples. Of
the three systems that do not use logical forms, our two
systems yield significant improvements. Our better sys-
tem even outperforms the system that uses logical forms.
predicate x in w (e.g., (Boston, Boston)), and
(iii) predicates for each POS tag in {JJ, NN, NNS}
(e.g., (JJ, size), (JJ, area), etc.).3 Predicates
corresponding to verbs and prepositions (e.g.,
traverse) are not included as overt lexical trig-
gers, but rather in the trace predicates L().
We also define an augmented lexicon L+ which
includes a prototype word x for each predicate ap-
pearing in (iii) above (e.g., (large, size)), which
cancels the predicates triggered by x?s POS tag. For
GEO, there are 22 prototype words; for JOBS, there
are 5. Specifying these triggers requires minimal
domain-specific supervision.
Results We first compare our system with Clarke
et al (2010) (henceforth, SEMRESP), which also
learns a semantic parser from question-answer pairs.
Table 2 shows that our system using lexical triggers
L (henceforth, DCS) outperforms SEMRESP (78.9%
over 73.2%). In fact, although neither DCS nor
SEMRESP uses logical forms, DCS uses even less su-
pervision than SEMRESP. SEMRESP requires a lex-
icon of 1.42 words per non-value predicate, Word-
Net features, and syntactic parse trees; DCS requires
only words for the domain-independent predicates
(overall, around 0.5 words per non-value predicate),
POS tags, and very simple indicator features. In
fact, DCS performs comparably to even the version
of SEMRESP trained using logical forms. If we add
prototype triggers (use L+), the resulting system
(DCS+) outperforms both versions of SEMRESP by
a significant margin (87.2% over 73.2% and 80.4%).
3We used the Berkeley Parser (Petrov et al, 2006) to per-
form POS tagging. The triggers L(x) for a word x thus include
L(t) where t is the POS tag of x.
System GEO JOBS
Tang and Mooney (2001) 79.4 79.8
Wong and Mooney (2007) 86.6 ?
Zettlemoyer and Collins (2005) 79.3 79.3
Zettlemoyer and Collins (2007) 81.6 ?
Kwiatkowski et al (2010) 88.2 ?
Kwiatkowski et al (2010) 88.9 ?
Our system (DCS with L) 88.6 91.4
Our system (DCS with L+) 91.1 95.0
Table 3: Accuracy (recall) of systems on the two bench-
marks. The systems are divided into three groups. Group
1 uses 10-fold cross-validation; groups 2 and 3 use the in-
dependent test set. Groups 1 and 2 measure accuracy of
logical form; group 3 measures accuracy of the answer;
but there is very small difference between the two as seen
from the Kwiatkowski et al (2010) numbers. Our best
system improves substantially over past work, despite us-
ing no logical forms as training data.
Next, we compared our systems (DCS and DCS+)
with the state-of-the-art semantic parsers on the full
dataset for both GEO and JOBS (see Table 3). All
other systems require logical forms as training data,
whereas ours does not. Table 3 shows that even DCS,
which does not use prototypes, is comparable to the
best previous system (Kwiatkowski et al, 2010), and
by adding a few prototypes, DCS+ offers a decisive
edge (91.1% over 88.9% on GEO). Rather than us-
ing lexical triggers, several of the other systems use
IBM word alignment models to produce an initial
word-predicate mapping. This option is not avail-
able to us since we do not have annotated logical
forms, so we must instead rely on lexical triggers
to define the search space. Note that having lexical
triggers is a much weaker requirement than having
a CCG lexicon, and far easier to obtain than logical
forms.
Intuitions How is our system learning? Initially,
the weights are zero, so the beam search is essen-
tially unguided. We find that only for a small frac-
tion of training examples do the K-best sets contain
any trees yielding the correct answer (29% for DCS
on GEO). However, training on just these exam-
ples is enough to improve the parameters, and this
29% increases to 66% and then to 95% over the next
few iterations. This bootstrapping behavior occurs
naturally: The ?easy? examples are processed first,
where easy is defined by the ability of the current
597
model to generate the correct answer using any tree.
Our system learns lexical associations between
words and predicates. For example, area (by virtue
of being a noun) triggers many predicates: city,
state, area, etc. Inspecting the final parameters
(DCS on GEO), we find that the feature [area, area]
has a much higher weight than [area, city]. Trace
predicates can be inserted anywhere, but the fea-
tures favor some insertions depending on the words
present (for example, [in, loc] has high weight).
The errors that the system makes stem from mul-
tiple sources, including errors in the POS tags (e.g.,
states is sometimes tagged as a verb, which triggers
no predicates), confusion of Washington state with
Washington D.C., learning the wrong lexical asso-
ciations due to data sparsity, and having an insuffi-
ciently large K.
5 Discussion
A major focus of this work is on our semantic rep-
resentation, DCS, which offers a new perspective
on compositional semantics. To contrast, consider
CCG (Steedman, 2000), in which semantic pars-
ing is driven from the lexicon. The lexicon en-
codes information about how each word can used in
context; for example, the lexical entry for borders
is S\NP/NP : ?y.?x.border(x, y), which means
borders looks right for the first argument and left
for the second. These rules are often too stringent,
and for complex utterances, especially in free word-
order languages, either disharmonic combinators are
employed (Zettlemoyer and Collins, 2007) or words
are given multiple lexical entries (Kwiatkowski et
al., 2010).
In DCS, we start with lexical triggers, which are
more basic than CCG lexical entries. A trigger for
borders specifies only that border can be used, but
not how. The combination rules are encoded in the
features as soft preferences. This yields a more
factorized and flexible representation that is easier
to search through and parametrize using features.
It also allows us to easily add new lexical triggers
without becoming mired in the semantic formalism.
Quantifiers and superlatives significantly compli-
cate scoping in lambda calculus, and often type rais-
ing needs to be employed. In DCS, the mark-execute
construct provides a flexible framework for dealing
with scope variation. Think of DCS as a higher-level
programming language tailored to natural language,
which results in programs (DCS trees) which are
much simpler than the logically-equivalent lambda
calculus formulae.
The idea of using CSPs to represent semantics is
inspired by Discourse Representation Theory (DRT)
(Kamp and Reyle, 1993; Kamp et al, 2005), where
variables are discourse referents. The restriction to
trees is similar to economical DRT (Bos, 2009).
The other major focus of this work is program
induction?inferring logical forms from their deno-
tations. There has been a fair amount of past work on
this topic: Liang et al (2010) induces combinatory
logic programs in a non-linguistic setting. Eisen-
stein et al (2009) induces conjunctive formulae and
uses them as features in another learning problem.
Piantadosi et al (2008) induces first-order formu-
lae using CCG in a small domain assuming observed
lexical semantics. The closest work to ours is Clarke
et al (2010), which we discussed earlier.
The integration of natural language with denota-
tions computed against a world (grounding) is be-
coming increasingly popular. Feedback from the
world has been used to guide both syntactic parsing
(Schuler, 2003) and semantic parsing (Popescu et
al., 2003; Clarke et al, 2010). Past work has also fo-
cused on aligning text to a world (Liang et al, 2009),
using text in reinforcement learning (Branavan et al,
2009; Branavan et al, 2010), and many others. Our
work pushes the grounded language agenda towards
deeper representations of language?think grounded
compositional semantics.
6 Conclusion
We built a system that interprets natural language
utterances much more accurately than existing sys-
tems, despite using no annotated logical forms. Our
system is based on a new semantic representation,
DCS, which offers a simple and expressive alter-
native to lambda calculus. Free from the burden
of annotating logical forms, we hope to use our
techniques in developing even more accurate and
broader-coverage language understanding systems.
Acknowledgments We thank Luke Zettlemoyer
and Tom Kwiatkowski for providing us with data
and answering questions.
598
References
J. Bos. 2009. A controlled fragment of DRT. In Work-
shop on Controlled Natural Language, pages 1?5.
S. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay.
2009. Reinforcement learning for mapping instruc-
tions to actions. In Association for Computational Lin-
guistics and International Joint Conference on Natural
Language Processing (ACL-IJCNLP), Singapore. As-
sociation for Computational Linguistics.
S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL). Association for Computa-
tional Linguistics.
B. Carpenter. 1998. Type-Logical Semantics. MIT Press.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
R. Dechter. 2003. Constraint Processing. Morgan Kauf-
mann.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Empirical Methods in Natural
Language Processing (EMNLP), Singapore.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Compu-
tational Natural Language Learning (CoNLL), pages
9?16, Ann Arbor, Michigan.
H. Kamp and U. Reyle. 1993. From Discourse to Logic:
An Introduction to the Model-theoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Kluwer, Dordrecht.
H. Kamp, J. v. Genabith, and U. Reyle. 2005. Discourse
representation theory. In Handbook of Philosophical
Logic.
R. J. Kate and R. J. Mooney. 2007. Learning lan-
guage semantics from ambiguous supervision. In As-
sociation for the Advancement of Artificial Intelligence
(AAAI), pages 895?900, Cambridge, MA. MIT Press.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages. In
Association for the Advancement of Artificial Intel-
ligence (AAAI), pages 1062?1068, Cambridge, MA.
MIT Press.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning se-
mantic correspondences with less supervision. In As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), Singapore. Association for Com-
putational Linguistics.
P. Liang, M. I. Jordan, and D. Klein. 2010. Learning
programs: A hierarchical Bayesian approach. In In-
ternational Conference on Machine Learning (ICML).
Omnipress.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL), pages 433?440. Associa-
tion for Computational Linguistics.
S. T. Piantadosi, N. D. Goodman, B. A. Ellis, and J. B.
Tenenbaum. 2008. A Bayesian model of the acquisi-
tion of compositional semantics. In Proceedings of the
Thirtieth Annual Conference of the Cognitive Science
Society.
H. Poon and P. Domingos. 2009. Unsupervised semantic
parsing. In Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI).
W. Schuler. 2003. Using model-theoretic semantic inter-
pretation to guide statistical parsing and word recog-
nition in a spoken language interface. In Association
for Computational Linguistics (ACL). Association for
Computational Linguistics.
M. Steedman. 2000. The Syntactic Process. MIT Press.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In European Conference on Ma-
chine Learning, pages 466?477.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960?967, Prague, Czech Republic.
Association for Computational Linguistics.
M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), Cambridge, MA. MIT Press.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658?666.
L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP/CoNLL), pages 678?687.
599
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 693?702,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Web-Scale Features for Full-Scale Parsing
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
Counts from large corpora (like the web) can
be powerful syntactic cues. Past work has
used web counts to help resolve isolated am-
biguities, such as binary noun-verb PP attach-
ments and noun compound bracketings. In
this work, we first present a method for gener-
ating web count features that address the full
range of syntactic attachments. These fea-
tures encode both surface evidence of lexi-
cal affinities as well as paraphrase-based cues
to syntactic structure. We then integrate our
features into full-scale dependency and con-
stituent parsers. We show relative error re-
ductions of 7.0% over the second-order depen-
dency parser of McDonald and Pereira (2006),
9.2% over the constituent parser of Petrov et
al. (2006), and 3.4% over a non-local con-
stituent reranker.
1 Introduction
Current state-of-the art syntactic parsers have
achieved accuracies in the range of 90% F1 on the
Penn Treebank, but a range of errors remain. From
a dependency viewpoint, structural errors can be
cast as incorrect attachments, even for constituent
(phrase-structure) parsers. For example, in the
Berkeley parser (Petrov et al, 2006), about 20%
of the errors are prepositional phrase attachment er-
rors as in Figure 1, where a preposition-headed (IN)
phrase was assigned an incorrect parent in the im-
plied dependency tree. Here, the Berkeley parser
(solid blue edges) incorrectly attaches from debt to
the noun phrase $ 30 billion whereas the correct at-
tachment (dashed gold edges) is to the verb rais-
ing. However, there are a range of error types, as
shown in Figure 2. Here, (a) is a non-canonical PP
VBG 
VP 
NP 
NP ? raising 
$ 30 billion 
PP 
from debt ? 
Figure 1: A PP attachment error in the parse output of the
Berkeley parser (on Penn Treebank). Guess edges are in solid
blue, gold edges are in dashed gold and edges common in guess
and gold parses are in black.
attachment ambiguity where by yesterday afternoon
should attach to had already, (b) is an NP-internal
ambiguity where half a should attach to dozen and
not to newspapers, and (c) is an adverb attachment
ambiguity, where just should modify fine and not the
verb ?s.
Resolving many of these errors requires informa-
tion that is simply not present in the approximately
1M words on which the parser was trained. One
way to access more information is to exploit sur-
face counts from large corpora like the web (Volk,
2001; Lapata and Keller, 2004). For example, the
phrase raising from is much more frequent on the
Web than $ x billion from. While this ?affinity? is
only a surface correlation, Volk (2001) showed that
comparing such counts can often correctly resolve
tricky PP attachments. This basic idea has led to a
good deal of successful work on disambiguating iso-
lated, binary PP attachments. For example, Nakov
and Hearst (2005b) showed that looking for para-
phrase counts can further improve PP resolution.
In this case, the existence of reworded phrases like
raising it from on the Web also imply a verbal at-
693
S 
NP 
NP PP 
?/eKman +Xtton ,nFby yesterday afternoon 
VP 
Kad aOread\ ? PDT 
NP 
? KaOI 
DT 
a 
PDT 
dozen 
PDT 
newspapers 
QP VBZ 
VP 
? ?s 
ADVP 
RB 
just 
ADJP 
JJ 
fine 
ADJP 
(a) (b) (c) 
Figure 2: Different kinds of attachment errors in the parse output of the Berkeley parser (on Penn Treebank). Guess edges are in
solid blue, gold edges are in dashed gold and edges common in guess and gold parses are in black.
tachment. Still other work has exploited Web counts
for other isolated ambiguities, such as NP coordina-
tion (Nakov and Hearst, 2005b) and noun-sequence
bracketing (Nakov and Hearst, 2005a; Pitler et al,
2010). For example, in (b), half dozen is more fre-
quent than half newspapers.
In this paper, we show how to apply these ideas
to all attachments in full-scale parsing. Doing so
requires three main issues to be addressed. First,
we show how features can be generated for arbitrary
head-argument configurations. Affinity features are
relatively straightforward, but paraphrase features,
which have been hand-developed in the past, are
more complex. Second, we integrate our features
into full-scale parsing systems. For dependency
parsing, we augment the features in the second-order
parser of McDonald and Pereira (2006). For con-
stituent parsing, we rerank the output of the Berke-
ley parser (Petrov et al, 2006). Third, past systems
have usually gotten their counts from web search
APIs, which does not scale to quadratically-many
attachments in each sentence. Instead, we consider
how to efficiently mine the Google n-grams corpus.
Given the success of Web counts for isolated am-
biguities, there is relatively little previous research
in this direction. The most similar work is Pitler
et al (2010), which use Web-scale n-gram counts
for multi-way noun bracketing decisions, though
that work considers only sequences of nouns and
uses only affinity-based web features. Yates et al
(2006) use Web counts to filter out certain ?seman-
tically bad? parses from extraction candidate sets
but are not concerned with distinguishing amongst
top parses. In an important contrast, Koo et al
(2008) smooth the sparseness of lexical features in a
discriminative dependency parser by using cluster-
based word-senses as intermediate abstractions in
addition to POS tags (also see Finkel et al (2008)).
Their work also gives a way to tap into corpora be-
yond the training data, through cluster membership
rather than explicit corpus counts and paraphrases.
This work uses a large web-scale corpus (Google
n-grams) to compute features for the full parsing
task. To show end-to-end effectiveness, we incor-
porate our features into state-of-the-art dependency
and constituent parsers. For the dependency case,
we can integrate them into the dynamic program-
ming of a base parser; we use the discriminatively-
trained MST dependency parser (McDonald et al,
2005; McDonald and Pereira, 2006). Our first-order
web-features give 7.0% relative error reduction over
the second-order dependency baseline of McDon-
ald and Pereira (2006). For constituent parsing, we
use a reranking framework (Charniak and Johnson,
2005; Collins and Koo, 2005; Collins, 2000) and
show 9.2% relative error reduction over the Berke-
ley parser baseline. In the same framework, we
also achieve 3.4% error reduction over the non-local
syntactic features used in Huang (2008). Our web-
scale features reduce errors for a range of attachment
types. Finally, we present an analysis of influential
features. We not only reproduce features suggested
in previous work but also discover a range of new
ones.
2 Web-count Features
Structural errors in the output of state-of-the-art
parsers, constituent or dependency, can be viewed
as attachment errors, examples of which are Figure 1
and Figure 2.1 One way to address attachment errors
is through features which factor over head-argument
1For constituent parsers, there can be minor tree variations
which can result in the same set of induced dependencies, but
these are rare in comparison.
694
raising          $           from    debt 
?(raising     from) ?($     from) 
?(head     arg) 
Figure 3: Features factored over head-argument pairs.
pairs, as is standard in the dependency parsing liter-
ature (see Figure 3). Here, we discuss which web-
count based features ?(h, a) should fire over a given
head-argument pair (we consider the words h and
a to be indexed, and so features can be sensitive to
their order and distance, as is also standard).
2.1 Affinity Features
Affinity statistics, such as lexical co-occurrence
counts from large corpora, have been used previ-
ously for resolving individual attachments at least as
far back as Lauer (1995) for noun-compound brack-
eting, and later for PP attachment (Volk, 2001; La-
pata and Keller, 2004) and coordination ambigu-
ity (Nakov and Hearst, 2005b). The approach of
Lauer (1995), for example, would be to take an am-
biguous noun sequence like hydrogen ion exchange
and compare the various counts (or associated con-
ditional probabilities) of n-grams like hydrogen ion
and hydrogen exchange. The attachment with the
greater score is chosen. More recently, Pitler et al
(2010) use web-scale n-grams to compute similar
association statistics for longer sequences of nouns.
Our affinity features closely follow this basic idea
of association statistics. However, because a real
parser will not have access to gold-standard knowl-
edge of the competing attachment sites (see Atterer
and Schutze (2007)?s criticism of previous work),
we must instead compute features for all possible
head-argument pairs from our web corpus. More-
over, when there are only two competing attachment
options, one can do things like directly compare two
count-based heuristics and choose the larger. Inte-
gration into a parser requires features to be functions
of single attachments, not pairwise comparisons be-
tween alternatives. A learning algorithm can then
weight features so that they compare appropriately
across parses.
We employ a collection of affinity features of
varying specificity. The basic feature is the core ad-
jacency count feature ADJ, which fires for all (h, a)
pairs. What is specific to a particular (h, a) is the
value of the feature, not its identity. For example, in
a naive approach, the value of the ADJ feature might
be the count of the query issued to the web-corpus ?
the 2-gram q = ha or q = ah depending on the or-
der of h and a in the sentence. However, it turns out
that there are several problems with this approach.
First, rather than a single all-purpose feature like
ADJ, the utility of such query counts will vary ac-
cording to aspects like the parts-of-speech of h and
a (because a high adjacency count is not equally in-
formative for all kinds of attachments). Hence, we
add more refined affinity features that are specific
to each pair of POS tags, i.e. ADJ ? POS(h) ?
POS(a). The values of these POS-specific features,
however, are still derived from the same queries as
before. Second, using real-valued features did not
work as well as binning the query-counts (we used
b = floor(logr(count)/5) ? 5) and then firing in-
dicator features ADJ ? POS(h) ? POS(a) ? b for
values of b defined by the query count. Adding still
more complex features, we conjoin to the preceding
features the order of the words h and a as they occur
in the sentence, and the (binned) distance between
them. For features which mark distances, wildcards
(?) are used in the query q = h ? a, where the num-
ber of wildcards allowed in the query is proportional
to the binned distance between h and a in the sen-
tence. Finally, we also include unigram variants of
the above features, which are sensitive to only one of
the head or argument. For all features used, we add
cumulative variants where indicators are fired for all
count bins b? up to query count bin b.
2.2 Paraphrase Features
In addition to measuring counts of the words present
in the sentence, there exist clever ways in which
paraphrases and other accidental indicators can help
resolve specific ambiguities, some of which are dis-
cussed in Nakov and Hearst (2005a), Nakov and
Hearst (2005b). For example, finding attestations of
eat : spaghetti with sauce suggests a nominal attach-
ment in Jean ate spaghetti with sauce. As another
example, one clue that the example in Figure 1 is
695
a verbal attachment is that the proform paraphrase
raising it from is commonly attested. Similarly, the
attestation of be noun prep suggests nominal attach-
ment.
These paraphrase features hint at the correct at-
tachment decision by looking for web n-grams
with special contexts that reveal syntax superficially.
Again, while effective in their isolated disambigua-
tion tasks, past work has been limited by both the
range of attachments considered and the need to in-
tuit these special contexts. For instance, frequency
of the pattern The noun prep suggests noun attach-
ment and of the pattern verb adverb prep suggests
verb attachment for the preposition in the phrase
verb noun prep, but these features were not in the
manually brainstormed list.
In this work, we automatically generate a large
number of paraphrase-style features for arbitrary at-
tachment ambiguities. To induce our list of fea-
tures, we first mine useful context words. We take
each (correct) training dependency relation (h, a)
and consider web n-grams of the form cha, hca,
and hac. Aggregating over all h and a (of a given
POS pair), we determine which context words c are
most frequent in each position. For example, for h =
raising and a = from (see Figure 1), we look at web
n-grams of the form raising c from and see that one
of the most frequent values of c on the web turns out
to be the word it.
Once we have collected context words (for each
position p in {BEFORE, MIDDLE, AFTER}), we
turn each context word c into a collection of features
of the form PARA ? POS(h) ? POS(a) ? c ? p ?
dir, where dir is the linear order of the attachment
in the sentence. Note that h and a are head and ar-
gument words and so actually occur in the sentence,
but c is a context word that generally does not. For
such features, the queries that determine their val-
ues are then of the form cha, hca, and so on. Con-
tinuing the previous example, if the test set has a
possible attachment of two words like h = lower-
ing and a = with, we will fire a feature PARA ?
VBG ? IN ? it ? MIDDLE ? ? with value (indi-
cator bins) set according to the results of the query
lowering it with. The idea is that if frequent oc-
currences of raising it from indicated a correct at-
tachment between raising and from, frequent occur-
rences of lowering it with will indicate the correct-
ness of an attachment between lowering and with.
Finally, to handle the cases where no induced con-
text word is helpful, we also construct abstracted
versions of these paraphrase features where the con-
text words c are collapsed to their parts-of-speech
POS(c), obtained using a unigram-tagger trained on
the parser training set. As discussed in Section 5, the
top features learned by our learning algorithm dupli-
cate the hand-crafted configurations used in previous
work (Nakov and Hearst, 2005b) but also add nu-
merous others, and, of course, apply to many more
attachment types.
3 Working with Web n-Grams
Previous approaches have generally used search en-
gines to collect count statistics (Lapata and Keller,
2004; Nakov and Hearst, 2005b; Nakov and Hearst,
2008). Lapata and Keller (2004) uses the number
of page hits as the web-count of the queried n-
gram (which is problematic according to Kilgarriff
(2007)). Nakov and Hearst (2008) post-processes
the first 1000 result snippets. One challenge with
this approach is that an external search API is now
embedded into the parser, raising issues of both
speed and daily query limits, especially if all pos-
sible attachments trigger queries. Such methods
also create a dependence on the quality and post-
processing of the search results, limitations of the
query process (for instance, search engines can ig-
nore punctuation (Nakov and Hearst, 2005b)).
Rather than working through a search API (or
scraper), we use an offline web corpus ? the Google
n-gram corpus (Brants and Franz, 2006) ? which
contains English n-grams (n = 1 to 5) and their ob-
served frequency counts, generated from nearly 1
trillion word tokens and 95 billion sentences. This
corpus allows us to efficiently access huge amounts
of web-derived information in a compressed way,
though in the process it limits us to local queries.
In particular, we only use counts of n-grams of the
form x ? y where the gap length is ? 3.
Our system requires the counts from a large col-
lection of these n-gram queries (around 4.5 million).
The most basic queries are counts of head-argument
pairs in contiguous h a and gapped h ? a configura-
tions.2 Here, we describe how we process queries
2Paraphrase features give situations where we query ? h a
696
of the form (q1, q2) with some number of wildcards
in between. We first collect all such queries over
all trees in preprocessing (so a new test set requires
a new query-extraction phase). Next, we exploit a
simple but efficient trie-based hashing algorithm to
efficiently answer all of them in one pass over the
n-grams corpus.
Consider Figure 4, which illustrates the data
structure which holds our queries. We first create
a trie of the queries in the form of a nested hashmap.
The key of the outer hashmap is the first word q1
of the query. The entry for q1 points to an inner
hashmap whose key is the final word q2 of the query
bigram. The values of the inner map is an array of
4 counts, to accumulate each of (q1q2), (q1 ? q2),
(q1 ? ?q2), and (q1 ? ? ? q2), respectively. We use k-
grams to collect counts of (q1...q2) with gap length
= k? 2, i.e. 2-grams to get count(q1q2), 3-grams to
get count(q1 ? q2) and so on.
With this representation of our collection of
queries, we go through the web n-grams (n = 2 to
5) one by one. For an n-gram w1...wn, if the first n-
gram word w1 doesn?t occur in the outer hashmap,
we move on. If it does match (say q?1 = w1), then
we look into the inner map for q?1 and check for the
final word wn. If we have a match, we increment the
appropriate query?s result value.
In similar ways, we also mine the most frequent
words that occur before, in between and after the
head and argument query pairs. For example, to col-
lect mid words, we go through the 3-gramsw1w2w3;
if w1 matches q?1 in the outer hashmap and w3 oc-
curs in the inner hashmap for q?1, then we store w2
and the count of the 3-gram. After the sweep, we
sort the context words in decreasing order of count.
We also collect unigram counts of the head and ar-
gument words by sweeping over the unigrams once.
In this way, our work is linear in the size of the
n-gram corpus, but essentially constant in the num-
ber of queries. Of course, if the number of queries is
expected to be small, such as for a one-off parse of
a single sentence, other solutions might be more ap-
propriate; in our case, a large-batch setting, the num-
ber of queries was such that this formulation was
chosen. Our main experiments (with no paralleliza-
tion) took 115 minutes to sweep over the 3.8 billion
and h a ?; these are handled similarly.
????? 
????? 
Web N-grams Query Count-Trie 
counts 
???? 
????? 
?????? 
??????? 
??????? 
SCA
N 
???? hash 
???? hash 
Figure 4: Trie-based nested hashmap for collecting ngram web-
counts of queries.
n-grams (n = 1 to 5) to compute the answers to 4.5
million queries, much less than the time required to
train the baseline parsers.
4 Parsing Experiments
Our features are designed to be used in full-sentence
parsing rather than for limited decisions about iso-
lated ambiguities. We first integrate our features into
a dependency parser, where the integration is more
natural and pushes all the way into the underlying
dynamic program. We then add them to a constituent
parser in a reranking approach. We also verify that
our features contribute on top of standard reranking
features.3
4.1 Dependency Parsing
For dependency parsing, we use the
discriminatively-trained MSTParser4, an im-
plementation of first and second order MST parsing
models of McDonald et al (2005) and McDonald
and Pereira (2006). We use the standard splits of
Penn Treebank into training (sections 2-21), devel-
opment (section 22) and test (section 23). We used
the ?pennconverter?5 tool to convert Penn trees from
constituent format to dependency format. Following
Koo et al (2008), we used the MXPOST tagger
(Ratnaparkhi, 1996) trained on the full training data
to provide part-of-speech tags for the development
3All reported experiments are run on all sentences, i.e. with-
out any length limit.
4http://sourceforge.net/projects/mstparser
5This supersedes ?Penn2Malt? and is available at
http://nlp.cs.lth.se/software/treebank converter. We follow
its recommendation to patch WSJ data with NP bracketing by
Vadas and Curran (2007).
697
Order 2 + Web features % Error Redn.
Dev (sec 22) 92.1 92.7 7.6%
Test (sec 23) 91.4 92.0 7.0%
Table 1: UAS results for English WSJ dependency parsing. Dev
is WSJ section 22 (all sentences) and Test is WSJ section 23
(all sentences). The order 2 baseline represents McDonald and
Pereira (2006).
and the test set, and we used 10-way jackknifing to
generate tags for the training set.
We added our first-order Web-scale features to
the MSTParser system to evaluate improvement over
the results of McDonald and Pereira (2006).6 Ta-
ble 1 shows unlabeled attachments scores (UAS)
for their second-order projective parser and the im-
proved numbers resulting from the addition of our
Web-scale features. Our first-order web-scale fea-
tures show significant improvement even over their
non-local second-order features.7 Additionally, our
web-scale features are at least an order of magnitude
fewer in number than even their first-order base fea-
tures.
4.2 Constituent Parsing
We also evaluate the utility of web-scale features
on top of a state-of-the-art constituent parser ? the
Berkeley parser (Petrov et al, 2006), an unlexical-
ized phrase-structure parser. Because the underly-
ing parser does not factor along lexical attachments,
we instead adopt the discriminative reranking frame-
work, where we generate the top-k candidates from
the baseline system and then rerank this k-best list
using (generally non-local) features.
Our baseline system is the Berkeley parser, from
which we obtain k-best lists for the development set
(WSJ section 22) and test set (WSJ section 23) using
a grammar trained on all the training data (WSJ sec-
tions 2-21).8 To get k-best lists for the training set,
we use 3-fold jackknifing where we train a grammar
6Their README specifies ?training-k:5 iters:10 loss-
type:nopunc decode-type:proj?, which we used for all final ex-
periments; we used the faster ?training-k:1 iters:5? setting for
most development experiments.
7Work such as Smith and Eisner (2008), Martins et al
(2009), Koo and Collins (2010) has been exploring more non-
local features for dependency parsing. It will be interesting to
see how these features interact with our web features.
8Settings: 6 iterations of split and merge with smoothing.
k = 1 k = 2 k = 10 k = 25 k = 50 k = 100
Dev 90.6 92.3 95.1 95.8 96.2 96.5
Test 90.2 91.8 94.7 95.6 96.1 96.4
Table 2: Oracle F1-scores for k-best lists output by Berkeley
parser for English WSJ parsing (Dev is section 22 and Test is
section 23, all lengths).
on 2 folds to get parses for the third fold.9 The ora-
cle scores of the k-best lists (for different values of
k) for the development and test sets are shown in Ta-
ble 2. Based on these results, we used 50-best lists
in our experiments. For discriminative learning, we
used the averaged perceptron (Collins, 2002; Huang,
2008).
Our core feature is the log conditional likelihood
of the underlying parser.10 All other features are in-
dicator features. First, we add all the Web-scale fea-
tures as defined above. These features alone achieve
a 9.2% relative error reduction. The affinity and
paraphrase features contribute about two-fifths and
three-fifths of this improvement, respectively. Next,
we rerank with only the features (both local and
non-local) from Huang (2008), a simplified merge
of Charniak and Johnson (2005) and Collins (2000)
(here configurational). These features alone achieve
around the same improvements over the baseline as
our web-scale features, even though they are highly
non-local and extensive. Finally, we rerank with
both our Web-scale features and the configurational
features. When combined, our web-scale features
give a further error reduction of 3.4% over the con-
figurational reranker (and a combined error reduc-
tion of 12.2%). All results are shown in Table 3.11
5 Analysis
Table 4 shows error counts and relative reductions
that our web features provide over the 2nd-order
dependency baseline. While we do see substantial
gains for classic PP (IN) attachment cases, we see
equal or greater error reductions for a range of at-
tachment types. Further, Table 5 shows how the to-
9Default: we ran the Berkeley parser in its default ?fast?
mode; the output k-best lists are ordered by max-rule-score.
10This is output by the flag -confidence. Note that baseline
results with just this feature are slightly worse than 1-best re-
sults because the k-best lists are generated by max-rule-score.
We report both numbers in Table 3.
11We follow Collins (1999) for head rules.
698
Dev (sec 22) Test (sec 23)
Parsing Model F1 EX F1 EX
Baseline (1-best) 90.6 39.4 90.2 37.3
log p(t|w) 90.4 38.9 89.9 37.3
+ Web features 91.6 42.5 91.1 40.6
+ Configurational features 91.8 43.8 91.1 40.6
+ Web + Configurational 92.1 44.0 91.4 41.4
Table 3: Parsing results for reranking 50-best lists of Berkeley
parser (Dev is WSJ section 22 and Test is WSJ section 23, all
lengths).
Arg Tag # Attach Baseline This Work % ER
NN 5725 5387 5429 12.4
NNP 4043 3780 3804 9.1
IN 4026 3416 3490 12.1
DT 3511 3424 3429 5.8
NNS 2504 2319 2348 15.7
JJ 2472 2310 2329 11.7
CD 1845 1739 1738 -0.9
VBD 1705 1571 1580 6.7
RB 1308 1097 1100 1.4
CC 1000 855 854 -0.7
VB 983 940 945 11.6
TO 868 761 776 14.0
VBN 850 776 786 13.5
VBZ 705 633 629 -5.6
PRP 612 603 606 33.3
Table 4: Error reduction for attachments of various child (argu-
ment) categories. The columns depict the tag, its total attach-
ments as argument, number of correct ones in baseline (Mc-
Donald and Pereira, 2006) and this work, and the relative error
reduction. Results are for dependency parsing on the dev set for
iters:5,training-k:1.
tal errors break down by gold head. For example,
the 12.1% total error reduction for attachments of an
IN argument (which includes PPs as well as comple-
mentized SBARs) includes many errors where the
gold attachments are to both noun and verb heads.
Similarly, for an NN-headed argument, the major
corrections are for attachments to noun and verb
heads, which includes both object-attachment am-
biguities and coordination ambiguities.
We next investigate the features that were given
high weight by our learning algorithm (in the con-
stituent parsing case). We first threshold features
by a minimum training count of 400 to focus on
frequently-firing ones (recall that our features are
not bilexical indicators and so are quite a bit more
Arg Tag % Error Redn for Various Parent Tags
NN IN: 18, NN: 23, VB: 30, NNP:20, VBN: 33
IN NN: 11, VBD: 11, NNS: 20, VB:18, VBG: 23
NNS IN: 9, VBD: 29, VBP: 21, VB:15, CC: 33
Table 5: Error reduction for each type of parent attachment for
a given child in Table 4.
POShead POSarg Example (head, arg)
RB IN back? into
NN IN review? of
NN DT The? rate
NNP IN Regulation? of
VB NN limit? access
VBD NN government? cleared
NNP NNP Dean? Inc
NN TO ability? to
JJ IN active? for
NNS TO reasons? to
IN NN under? pressure
NNS IN reports? on
NN NNP Warner? studio
NNS JJ few? plants
Table 6: The highest-weight features (thresholded at a count of
400) of the affinity schema. We list only the head and argu-
ment POS and the direction (arrow from head to arg). We omit
features involving punctuation.
frequent). We then sort them by descending (signed)
weight.
Table 6 shows which affinity features received the
highest weights, as well as examples of training set
attachments for which the feature fired (for concrete-
ness), suppressing both features involving punctua-
tion and the features? count and distance bins. With
the standard caveats that interpreting feature weights
in isolation is always to be taken for what it is,
the first feature (RB?IN) indicates that high counts
for an adverb occurring adjacent to a preposition
(like back into the spotlight) is a useful indicator
that the adverb actually modifies that preposition.
The second row (NN?IN) indicates that whether a
preposition is appropriate to attach to a noun is well
captured by how often that preposition follows that
noun. The fifth row (VB?NN) indicates that when
considering an NP as the object of a verb, it is a good
sign if that NP?s head frequently occurs immediately
following that verb. All of these features essentially
state cases where local surface counts are good indi-
699
POShead mid-word POSarg Example (head, arg)
VBN this IN leaned, from
VB this IN publish, in
VBG him IN using, as
VBG them IN joining, in
VBD directly IN converted, into
VBD held IN was, in
VBN jointly IN offered, by
VBZ it IN passes, in
VBG only IN consisting, of
VBN primarily IN developed, for
VB us IN exempt, from
VBG this IN using, as
VBD more IN looked, like
VB here IN stay, for
VBN themselves IN launched, into
VBG down IN lying, on
Table 7: The highest-weight features (thresholded at a count of
400) of the mid-word schema for a verb head and preposition
argument (with head on left of argument).
cators of (possibly non-adjacent) attachments.
A subset of paraphrase features, which in the
automatically-extracted case don?t really correspond
to paraphrases at all, are shown in Table 7. Here
we show features for verbal heads and IN argu-
ments. The mid-words m which rank highly are
those where the occurrence of hma as an n-gram
is a good indicator that a attaches to h (m of course
does not have to actually occur in the sentence). In-
terestingly, the top such features capture exactly the
intuition from Nakov and Hearst (2005b), namely
that if the verb h and the preposition a occur with
a pronoun in between, we have evidence that a at-
taches to h (it certainly can?t attach to the pronoun).
However, we also see other indicators that the prepo-
sition is selected for by the verb, such as adverbs like
directly.
As another example of known useful features
being learned automatically, Table 8 shows the
previous-context-word paraphrase features for a
noun head and preposition argument (N ? IN).
Nakov and Hearst (2005b) suggested that the attes-
tation of be N IN is a good indicator of attachment to
the noun (the IN cannot generally attach to forms of
auxiliaries). One such feature occurs on this top list
? for the context word have ? and others occur far-
ther down. We also find their surface marker / punc-
bfr-word POShead POSarg Example (head, arg)
second NN IN season, in
The NN IN role, of
strong NN IN background, in
our NNS IN representatives, in
any NNS IN rights, against
A NN IN review, of
: NNS IN Results, in
three NNS IN years, in
In NN IN return, for
no NN IN argument, about
current NN IN head, of
no NNS IN plans, for
public NN IN appearance, at
from NNS IN sales, of
net NN IN revenue, of
, NNS IN names, of
you NN IN leave, in
have NN IN time, for
some NN IN money, for
annual NNS IN reports, on
Table 8: The highest-weight features (thresholded at a count of
400) of the before-word schema for a noun head and preposition
argument (with head on left of argument).
tuation cues of : and , preceding the noun. However,
we additionally find other cues, most notably that if
the N IN sequence occurs following a capitalized de-
terminer, it tends to indicate a nominal attachment
(in the n-gram, the preposition cannot attach left-
ward to anything else because of the beginning of
the sentence).
In Table 9, we see the top-weight paraphrase fea-
tures that had a conjunction as a middle-word cue.
These features essentially say that if two heads w1
and w2 occur in the direct coordination n-gram w1
and w2, then they are good heads to coordinate (co-
ordination unfortunately looks the same as comple-
mentation or modification to a basic dependency
model). These features are relevant to a range of
coordination ambiguities.
Finally, Table 10 depicts the high-weight, high-
count general paraphrase-cue features for arbitrary
head and argument categories, with those shown
in previous tables suppressed. Again, many inter-
pretable features appear. For example, the top entry
(the JJ NNS) shows that when considering attaching
an adjective a to a noun h, it is a good sign if the
700
POShead mid-CC POSarg Example (head, arg)
NNS and NNS purchases, sales
VB and VB buy, sell
NN and NN president, officer
NN and NNS public, media
VBD and VBD said, added
VBZ and VBZ makes, distributes
JJ and JJ deep, lasting
IN and IN before, during
VBD and RB named, now
VBP and VBP offer, need
Table 9: The highest-weight features (thresholded at a count
of 400) of the mid-word schema where the mid-word was a
conjunction. For variety, for a given head-argument POS pair,
we only list features corresponding to the and conjunction and
h? a direction.
trigram the a h is frequent ? in that trigram, the ad-
jective attaches to the noun. The second entry (NN
- NN) shows that one noun is a good modifier of
another if they frequently appear together hyphen-
ated (another punctuation-based cue mentioned in
previous work on noun bracketing, see Nakov and
Hearst (2005a)). While they were motivated on sep-
arate grounds, these features can also compensate
for inapplicability of the affinity features. For exam-
ple, the third entry (VBD this NN) is a case where
even if the head (a VBD like adopted) actually se-
lects strongly for the argument (a NN like plan), the
bigram adopted plan may not be as frequent as ex-
pected, because it requires a determiner in its mini-
mal analogous form adopted the plan.
6 Conclusion
Web features are a way to bring evidence from a
large unlabeled corpus to bear on hard disambigua-
tion decisions that are not easily resolvable based on
limited parser training data. Our approach allows re-
vealing features to be mined for the entire range of
attachment types and then aggregated and balanced
in a full parsing setting. Our results show that these
web features resolve ambiguities not correctly han-
dled by current state-of-the-art systems.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
POSh POSa mid/bfr-word Example (h, a)
NNS JJ b = the other? things
NN NN m = - auto? maker
VBD NN m = this adopted? plan
NNS NN b = of computer? products
NN DT m = current the? proposal
VBG IN b = of going? into
NNS IN m = ? clusters? of
IN NN m = your In? review
TO VB b = used to? ease
VBZ NN m = that issue? has
IN NNS m = two than? minutes
IN NN b = used as? tool
IN VBD m = they since? were
VB TO b = will fail? to
Table 10: The high-weight high-count (thresholded at a count of
2000) general features of the mid and before paraphrase schema
(examples show head and arg in linear order with arrow from
head to arg).
ported by BBN under DARPA contract HR0011-06-
C-0022.
References
M. Atterer and H. Schutze. 2007. Prepositional phrase
attachment without oracles. Computational Linguis-
tics, 33(4):469476.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
701
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Pro-
ceedings of HLT-NAACL.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of ACL-
IJCNLP.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL.
Preslav Nakov and Marti Hearst. 2005b. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of EMNLP.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, , and Kenneth
Church. 2010. Using web-scale n-grams to improve
base NP parsing performance. In Proceedings of COL-
ING.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
David Vadas and James R. Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceedings
of ACL.
Martin Volk. 2001. Exploiting the WWW as a corpus to
resolve PP attachment ambiguities. In Proceedings of
Corpus Linguistics.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-based
semantic filters. In Proceedings of EMNLP.
702
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 24?29,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Empirical Investigation of Discounting
in Cross-Domain Language Models
Greg Durrett and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,klein}@cs.berkeley.edu
Abstract
We investigate the empirical behavior of n-
gram discounts within and across domains.
When a language model is trained and evalu-
ated on two corpora from exactly the same do-
main, discounts are roughly constant, match-
ing the assumptions of modified Kneser-Ney
LMs. However, when training and test corpora
diverge, the empirical discount grows essen-
tially as a linear function of the n-gram count.
We adapt a Kneser-Ney language model to
incorporate such growing discounts, result-
ing in perplexity improvements over modified
Kneser-Ney and Jelinek-Mercer baselines.
1 Introduction
Discounting, or subtracting from the count of each
n-gram, is one of the core aspects of Kneser-Ney
language modeling (Kneser and Ney, 1995). For all
but the smallest n-gram counts, Kneser-Ney uses a
single discount, one that does not grow with the n-
gram count, because such constant-discounting was
seen in early experiments on held-out data (Church
and Gale, 1991). However, due to increasing com-
putational power and corpus sizes, language model-
ing today presents a different set of challenges than
it did 20 years ago. In particular, modeling cross-
domain effects has become increasingly more im-
portant (Klakow, 2000; Moore and Lewis, 2010),
and deployed systems must frequently process data
that is out-of-domain from the standpoint of the lan-
guage model.
In this work, we perform experiments on held-
out data to evaluate how discounting behaves in the
cross-domain setting. We find that, when training
and testing on corpora that are as similar as possi-
ble, empirical discounts indeed do not grow with n-
gram count, which validates the parametric assump-
tion of Kneser-Ney smoothing. However, when the
train and evaluation corpora differ, even slightly, dis-
counts generally exhibit linear growth in the count of
the n-gram, with the amount of growth being closely
correlated with the corpus divergence. Finally, we
build a language model exploiting a parametric form
of the growing discount and show perplexity gains of
up to 5.4% over modified Kneser-Ney.
2 Discount Analysis
Underlying discounting is the idea that n-grams will
occur fewer times in test data than they do in training
data. We investigate this quantitatively by conduct-
ing experiments similar in spirit to those of Church
and Gale (1991). Suppose that we have collected
counts on two corpora of the same size, which we
will call our train and test corpora. For an n-gram
w = (w1, ..., wn), let ktrain(w) denote the number of
occurrences of w in the training corpus, and ktest(w)
denote the number of occurrences of w in the test
corpus. We define the empirical discount of w to be
d(w) = ktrain(w) ? ktest(w); this will be negative
when the n-gram occurs more in the test data than
in the training data. Let Wi = {w : ktrain(w) = i}
be the set of n-grams with count i in the training
corpus. We define the average empirical discount
function as
d?(i) =
1
|Wi|
?
w?Wi
d(w)
24
Kneser-Ney implicitly makes two assumptions:
first, that discounts do not depend on n-gram count,
i.e. that d?(i) is constant in i. Modified Kneser-Ney
relaxes this assumption slightly by having indepen-
dent parameters for 1-count, 2-count, and many-
count n-grams, but still assumes that d?(i) is constant
for i greater than two. Second, by using the same
discount for all n-grams with a given count, Kneser-
Ney assumes that the distribution of d(w) for w in a
particular Wi is well-approximated by its mean. In
this section, we analyze whether or not the behavior
of the average empirical discount function supports
these two assumptions. We perform experiments on
various subsets of the documents in the English Gi-
gaword corpus, chiefly drawn from New York Times
(NYT) and Agence France Presse (AFP).1
2.1 Are Discounts Constant?
Similar corpora To begin, we consider the NYT
documents from Gigaword for the year 1995. In
order to create two corpora that are maximally
domain-similar, we randomly assign half of these
documents to train and half of them to test, yielding
train and test corpora of approximately 50M words
each, which we denote by NYT95 and NYT95?. Fig-
ure 1 shows the average empirical discounts d?(i)
for trigrams on this pair of corpora. In this setting,
we recover the results of Church and Gale (1991)
in that discounts are approximately constant for n-
gram counts of two or greater.
Divergent corpora In addition to these two cor-
pora, which were produced from a single contigu-
ous batch of documents, we consider testing on cor-
pus pairs with varying degrees of domain difference.
We construct additional corpora NYT96, NYT06,
AFP95, AFP96, and AFP06, by taking 50M words
from documents in the indicated years of NYT
and AFP data. We then collect training counts on
NYT95 and alternately take each of our five new cor-
pora as the test data. Figure 1 also shows the average
empirical discount curves for these train/test pairs.
Even within NYT newswire data, we see growing
discounts when the train and test corpora are drawn
1Gigaword is drawn from six newswire sources and contains
both miscellaneous text and complete, contiguous documents,
sorted chronologically. Our experiments deal exclusively with
the document text, which constitutes the majority of Gigaword
and is of higher quality than the miscellaneous text.
 0 1 2 3 4 5 6
 0
 5
 1
0
 1
5
 2
0
Average empirical discount
Tr
ig
ra
m
 c
ou
nt
 in
 tr
ai
n
A
FP
06
A
FP
96
A
FP
95
N
Y
T0
6
N
Y
T9
6
N
Y
T9
5?
Figure 1: Average empirical trigram discounts d?(i) for
six configurations, training on NYT95 and testing on the
indicated corpora. For each n-gram count k, we compute
the average number of occurrences in test for all n-grams
occurring k times in training data, then report k minus
this quantity as the discount. Bigrams and bigram types
exhibit similar discount relationships.
from different years, and between the NYT and AFP
newswire, discounts grow even more quickly. We
observed these trends continuing steadily up into n-
gram counts in the hundreds, beyond which point it
becomes difficult to robustly estimate discounts due
to fewer n-gram types in this count range.
This result is surprising in light of the constant
discounts observed for the NYT95/NYT95? pair.
Goodman (2001) proposes that discounts arise from
document-level ?burstiness? in a corpus, because
language often repeats itself locally within a doc-
ument, and Moore and Quirk (2009) suggest that
discounting also corrects for quantization error due
to estimating a continuous distribution using a dis-
crete maximum likelihood estimator (MLE). Both
of these factors are at play in the NYT95/NYT95?
experiment, and yet only a small, constant discount
is observed. Our growing discounts must therefore
be caused by other, larger-scale phenomena, such as
shifts in the subjects of news articles over time or in
the style of the writing between newswire sources.
The increasing rate of discount growth as the source
changes and temporal divergence increases lends
credence to this hypothesis.
2.2 Nonuniformity of Discounts
Figure 1 considers discounting in terms of averaged
discounts for each count, which tests one assump-
tion of modified Kneser-Ney, that discounts are a
25
 0
 0
.1
 0
.2
 0
.3
 0
.4
0
5
10
15
20
Fraction of train-count-10 trigrams
Tr
ig
ra
m
 c
ou
nt
 in
 te
stN
Y
T9
5/
N
Y
T9
5?
N
Y
T9
5/
A
FP
95
Figure 2: Empirical probability mass functions of occur-
rences in the test data for trigrams that appeared 10 times
in training data. Discounting by a single value is plau-
sible in the case of similar train and test corpora, where
the mean of the distribution (8.50) is close to the median
(8.0), but not in the case of divergent corpora, where the
mean (6.04) and median (1.0) are very different.
constant function of n-gram counts. In Figure 2, we
investigate the second assumption, namely that the
distribution over discounts for a given n-gram count
is well-approximated by its mean. For similar cor-
pora, this seems to be true, with a histogram of test
counts for trigrams of count 10 that is nearly sym-
metric. For divergent corpora, the data exhibit high
skew: almost 40% of the trigrams simply never ap-
pear in the test data, and the distribution has very
high standard deviation (17.0) due to a heavy tail
(not shown). Using a discount that depends only on
the n-gram count is less appropriate in this case.
In combination with the growing discounts of sec-
tion 2.1, these results point to the fact that modified
Kneser-Ney does not faithfully model the discount-
ing in even a mildly cross-domain setting.
2.3 Correlation of Divergence and Discounts
Intuitively, corpora that are more temporally distant
within a particular newswire source should perhaps
be slightly more distinct, and still a higher degree of
divergence should exist between corpora from dif-
ferent newswire sources. From Figure 1, we see that
this notion agrees with the relative sizes of the ob-
served discounts. We now ask whether growth in
discounts is correlated with train/test dissimilarity in
a more quantitative way. For a given pair of cor-
pora, we canonicalize the degree of discounting by
selecting the point d?(30), the average empirical dis-
 0 5 1
0
 1
5
-5
00
-4
00
-3
00
Discount for count-30 trigrams
Lo
g 
lik
el
ih
oo
d 
di
ffe
re
nc
e 
(in
 m
ill
io
ns
)
Figure 3: Log likelihood difference versus average empir-
ical discount of trigrams with training count 30 (d?(30))
for the train/test pairs. More negative values of the log
likelihood indicate more dissimilar corpora, as the trained
model is doing less well relative to the jackknife model.
count for n-grams occurring 30 times in training.2
To measure divergence between the corpus pair, we
compute the difference between the log likelihood
of the test corpus under the train corpus language
model (using basic Kneser-Ney) and the likelihood
of the test corpus under a jackknife language model
from the test itself, which holds out and scores each
test n-gram in turn. This dissimilarity metric resem-
bles the cross-entropy difference used by Moore and
Lewis (2010) to subsample for domain adaptation.
We compute this canonicalization for each of
twenty pairs of corpora, with each corpus contain-
ing 240M trigram tokens between train and test. The
corpus pairs were chosen to span varying numbers
of newswire sources and lengths of time in order to
capture a wide range of corpus divergences. Our re-
sults are plotted in Figure 3. The log likelihood dif-
ference and d?(30) are negatively correlated with a
correlation coefficient value of r = ?0.88, which
strongly supports our hypothesis that higher diver-
gence yields higher discounting. One explanation
for the remaining variance is that the trigram dis-
count curve depends on the difference between the
number of bigram types in the train and test corpora,
which can be as large as 10%: observing more bi-
gram contexts in training fragments the token counts
2One could also imagine instead canonicalizing the curves
by using either the exponent or slope parameters from a fitted
power law as in section 3. However, there was sufficient non-
linearity in the average empirical discount curves that neither of
these parameters was an accurate proxy for d?(i).
26
and leads to smaller observed discounts.
2.4 Related Work
The results of section 2.1 point to a remarkably per-
vasive phenomenon of growing empirical discounts,
except in the case of extremely similar corpora.
Growing discounts of this sort were previously sug-
gested by the model of Teh (2006). However, we
claim that the discounting phenomenon in our data is
fundamentally different from his model?s prediction.
In the held-out experiments of section 2.1, growing
discounts only emerge when one evaluates against a
dissimilar held-out corpus, whereas his model would
predict discount growth even in NYT95/NYT95?,
where we do not observe it.
Adaptation across corpora has also been ad-
dressed before. Bellegarda (2004) describes a range
of techniques, from interpolation at either the count
level or the model level (Bacchiani and Roark, 2003;
Bacchiani et al, 2006) to using explicit models of
syntax or semantics. Hsu and Glass (2008) employ
a log-linear model for multiplicatively discounting
n-grams in Kneser-Ney; when they include the log-
count of an n-gram as the only feature, they achieve
75% of their overall word error rate reduction, sug-
gesting that predicting discounts based on n-gram
count can substantially improve the model. Their
work also improves on the second assumption of
Kneser-Ney, that of the inadequacy of the average
empirical discount as a discount constant, by em-
ploying various other features in order to provide
other criteria on which to discount n-grams.
Taking a different approach, both Klakow (2000)
and Moore and Lewis (2010) use subsampling to
select the domain-relevant portion of a large, gen-
eral corpus given a small in-domain corpus. This
can be interpreted as a form of hard discounting,
and implicitly models both growing discounts, since
frequent n-grams will appear in more of the re-
jected sentences, and nonuniform discounting over
n-grams of each count, since the sentences are cho-
sen according to a likelihood criterion. Although
we do not consider this second point in constructing
our language model, an advantage of our approach
over subsampling is that we use our entire training
corpus, and in so doing compromise between min-
imizing errors from data sparsity and accommodat-
ing domain shifts to the extent possible.
3 A Growing Discount Language Model
We now implement and evaluate a language model
that incorporates growing discounts.
3.1 Methods
Instead of using a fixed discount for most n-gram
counts, as prescribed by modified Kneser-Ney, we
discount by an increasing parametric function of the
n-gram count. We use a tune set to compute an av-
erage empirical discount curve d?(i), and fit a func-
tion of the form f(x) = a + bxc to this curve using
weighted least-L1-loss regression, with the weight
for each point proportional to i|Wi|, the total to-
ken counts of n-grams occurring that many times
in training. To improve the fit of the model, we
use dedicated parameters for count-1 and count-2 n-
grams as in modified Kneser-Ney, yielding a model
with five parameters per n-gram order. We call this
model GDLM. We also instantiate this model with
c fixed to one, so that the model is strictly linear
(GDLM-LIN).
As baselines for comparison, we use basic inter-
polated Kneser-Ney (KNLM), with one discount pa-
rameter per n-gram order, and modified interpolated
Kneser-Ney (MKNLM), with three parameters per
n-gram order, as described in (Chen and Goodman,
1998). We also compare against Jelinek-Mercer
smoothing (JMLM), which interpolates the undis-
counted MLEs from every order. According to Chen
and Goodman (1998), it is common to use different
interpolation weights depending on the history count
of an n-gram, since MLEs based on many samples
are presumed to be more accurate than those with
few samples. We used five history count buckets so
that JMLM would have the same number of param-
eters as GDLM.
All five models are trigram models with type
counts at the lower orders and independent discount
or interpolation parameters for each order. Param-
eters for GDLM, MKNLM, and KNLM are initial-
ized based on estimates from d?(i): the regression
thereof for GDLM, and raw discounts for MKNLM
and KNLM. The parameters of JMLM are initialized
to constants independent of the data. These initial-
izations are all heuristic and not guaranteed to be
optimal, so we then iterate through the parameters
of each model several times and perform line search
27
Train NYT00+01 Train AFP02+05+06
Voc. 157K 50K 157K 50K
GDLM(*) 151 131 258 209
GDLM-LIN(*) 151 132 259 210
JMLM 165 143 274 221
MKNLM 152 132 273 221
KNLM 159 138 300 241
Table 1: Perplexities of the growing discounts language
model (GDLM) and its purely linear variant (GDLM-
LIN), which are contributions of this work, versus
the modified Kneser-Ney (MKNLM), basic Kneser-Ney
(KNLM), and Jelinek-Mercer (JMLM) baselines. We
report results for in-domain (NYT00+01) and out-of-
domain (AFP02+05+06) training corpora, for two meth-
ods of closing the vocabulary.
in each to optimize tune-set perplexity.
For evaluation, we train, tune, and test on three
disjoint corpora. We consider two different train-
ing sets: one of 110M words of NYT from 2000
and 2001 (NYT00+01), and one of 110M words of
AFP from 2002, 2005, and 2006 (AFP02+05+06).
In both cases, we compute d?(i) and tune parameters
on 110M words of NYT from 2002 and 2003, and
do our final perplexity evaluation on 4M words of
NYT from 2004. This gives us both in-domain and
out-of-domain results for our new language model.
Our tune set is chosen to be large so that we can
initialize parameters based on the average empirical
discount curve; in practice, one could compute em-
pirical discounts based on a smaller tune set with the
counts scaled up proportionately, or simply initialize
to constant values.
We use two different methods to handle out-of-
vocabulary (OOV) words: one scheme replaces any
unigram token occurring fewer than five times in
training with an UNK token, yielding a vocabulary
of approximately 157K words, and the other scheme
only keeps the top 50K words in the vocabulary.
The count truncation method has OOV rates of 0.9%
and 1.9% in the NYT/NYT and NYT/AFP settings,
respectively, and the constant-size vocabulary has
OOV rates of 2% and 3.6%.
3.2 Results
Perplexity results are given in Table 1. As expected,
for in-domain data, GDLM performs comparably to
MKNLM, since the discounts do not grow and so
there is little to be gained by choosing a param-
eterization that permits this. Out-of-domain, our
model outperforms MKNLM and JMLM by approx-
imately 5% for both vocabulary sizes. The out-
of-domain perplexity values are competitive with
those of Rosenfeld (1996), who trained on New York
Times data and tested on AP News data under simi-
lar conditions, and even more aggressive closing of
the vocabulary. Moore and Lewis (2010) achieve
lower perplexities, but they use in-domain training
data that we do not include in our setting.
We briefly highlight some interesting features of
these results. In the small vocabulary cross-domain
setting, for GDLM-LIN, we find
dtri(i) = 1.31 + 0.27i, dbi(i) = 1.34 + 0.05i
as the trigram and bigram discount functions that
minimize tune set perplexity. For GDLM,
dtri(i) = 1.19 + 0.32i
0.45, dbi(i) = 0.86 + 0.56i
0.86
In both cases, a growing discount is indeed learned
from the tuning procedure, demonstrating the im-
portance of this in our model. Modeling nonlin-
ear discount growth in GDLM yields only a small
marginal improvement over the linear discounting
model GDLM-LIN, so we prefer GDLM-LIN for its
simplicity.
A somewhat surprising result is the strong per-
formance of JMLM relative to MKNLM on the di-
vergent corpus pair. We conjecture that this is be-
cause the bucketed parameterization of JMLM gives
it the freedom to change interpolation weights with
n-gram count, whereas MKNLM has essentially a
fixed discount. This suggests that modified Kneser-
Ney as it is usually parameterized may be a particu-
larly poor choice in cross-domain settings.
Overall, these results show that the growing dis-
count phenomenon detailed in section 2, beyond
simply being present in out-of-domain held-out data,
provides the basis for a new discounting scheme that
allows us to improve perplexity relative to modified
Kneser-Ney and Jelinek-Mercer baselines.
Acknowledgments
The authors gratefully acknowledge partial support
from the GALE program via BBN under DARPA
contract HR0011-06-C-0022, and from an NSF fel-
lowship for the first author. Thanks to the anony-
mous reviewers for their insightful comments.
28
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
Langauge Model Adaptation. In Proceedings of Inter-
national Conference on Acoustics, Speech, and Signal
Processing.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1):41 ?
68.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42:93?108.
Stanley Chen and Joshua Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Model-
ing. Technical report, Harvard University, August.
Kenneth Church and William Gale. 1991. A Compari-
son of the Enhanced Good-Turing and Deleted Estima-
tion Methods for Estimating Probabilities of English
Bigrams. Computer Speech & Language, 5(1):19?54.
Joshua Goodman. 2001. A Bit of Progress in Language
Modeling. Computer Speech & Language, 15(4):403?
434.
Bo-June (Paul) Hsu and James Glass. 2008. N-
gram Weighting: Reducing Training Data Mismatch in
Cross-Domain Language Model Estimation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 829?838.
Dietrich Klakow. 2000. Selecting articles from the lan-
guage model training corpus. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing, volume 3, pages 1695?1698.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-Gram Language Modeling. In Pro-
ceedings of International Conference on Acoustics,
Speech, and Signal Processing.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers, pages
220?224, July.
Robert C. Moore and Chris Quirk. 2009. Improved
Smoothing for N-gram Language Models Based on
Ordinary Counts. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 349?352.
Ronald Rosenfeld. 1996. A Maximum Entropy Ap-
proach to Adaptive Statistical Language Modeling.
Computer, Speech & Language, 10:187?228.
Yee Whye Teh. 2006. A Hierarchical Bayesian Lan-
guage Model Based On Pitman-Yor Processes. In Pro-
ceedings of ACL, pages 985?992, Sydney, Australia,
July. Association for Computational Linguistics.
29
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720?725,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The Surprising Variance in Shortest-Derivation Parsing
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
We investigate full-scale shortest-derivation
parsing (SDP), wherein the parser selects an
analysis built from the fewest number of train-
ing fragments. Shortest derivation parsing
exhibits an unusual range of behaviors. At
one extreme, in the fully unpruned case, it
is neither fast nor accurate. At the other ex-
treme, when pruned with a coarse unlexical-
ized PCFG, the shortest derivation criterion
becomes both fast and surprisingly effective,
rivaling more complex weighted-fragment ap-
proaches. Our analysis includes an investi-
gation of tie-breaking and associated dynamic
programs. At its best, our parser achieves an
accuracy of 87% F1 on the English WSJ task
with minimal annotation, and 90% F1 with
richer annotation.
1 Introduction
One guiding intuition in parsing, and data-driven
NLP more generally, is that, all else equal, it is ad-
vantageous to memorize large fragments of training
examples. Taken to the extreme, this intuition sug-
gests shortest derivation parsing (SDP), wherein a
test sentence is analyzed in a way which uses as few
training fragments as possible (Bod, 2000; Good-
man, 2003). SDP certainly has appealing properties:
it is simple and parameter free ? there need not even
be an explicit lexicon. However, SDP may be too
simple to be competitive.
In this paper, we consider SDP in both its pure
form and with several direct modifications, finding a
range of behaviors. In its pure form, with no prun-
ing or approximation, SDP is neither fast nor accu-
rate, achieving less than 70% F1 on the English WSJ
task. Moreover, basic tie-breaking variants and lexi-
cal augmentation are insufficient to achieve compet-
itive accuracies.1 On the other hand, SDP is dramat-
ically improved in both speed and accuracy when
a simple, unlexicalized PCFG is used for coarse-
to-fine pruning (and tie-breaking). On the English
WSJ, the coarse PCFG and the fine SDP together
achieve 87% F1 with basic treebank annotation (see
Table 2) and up to 90% F1 with richer treebank an-
notation (see Table 4).
The main contribution of this work is to analyze
the behavior of shortest derivation parsing, showing
both when it fails and when it succeeds. Our final
parser, which combines a simple PCFG coarse pass
with an otherwise pure SPD fine pass, can be quite
accurate while being straightforward to implement.
2 Implicit Grammar for SDP
The all-fragments grammar (AFG) for a (binarized)
treebank is formally the tree-substitution grammar
(TSG) (Resnik, 1992; Bod, 1993) that consists of
all fragments (elementary trees) of all training trees
in the treebank, with some weighting on each frag-
ment. AFGs are too large to fully extract explicitly;
researchers therefore either work with a tractable
subset of the fragments (Sima?an, 2000; Bod, 2001;
Post and Gildea, 2009; Cohn and Blunsom, 2010) or
use a PCFG reduction like that of Goodman (1996a),
in which each treebank node token Xi is given its
own unique grammar symbol.
We follow Bansal and Klein (2010) in choosing
the latter, both to permit comparison to their results
and because SDP is easily phrased as a PCFG re-
duction. Bansal and Klein (2010) use a carefully pa-
1Bod (2000) presented another SDP parser, but with a sam-
pled subset of the training fragments.
720
rameterized weighting of the substructures in their
grammar in an effort to extend the original DOP1
model (Bod, 1993; Goodman, 1996a). However, for
SDP, the grammar is even simpler (Goodman, 2003).
In principle, the implicit SDP grammar needs just
two rule schemas: CONTINUE (Xp ? Yq Zr) and
SWITCH (Xp ? Xq), with additive costs 0 and 1,
respectively. CONTINUE rules walk along training
trees, while SWITCH rules change between trees for
a unit cost.2 Assuming that the SWITCH rules are in
practice broken down into BEGIN and END sub-rules
as in Bansal and Klein (2010), the grammar is linear
in the size of the treebank.3 Note that no lexicon
is needed in this grammar: lexical switches are like
any other.
A derivation in our grammar has weight (cost) w
where w is the number of switches (or the num-
ber of training fragments minus one) used to build
the derivation (see Figure 1). The Viterbi dy-
namic program for finding the shortest derivation is
quite simple: it requires CKY to store only byte-
valued switch-counts s(Xp, i, j) (i.e., the number
of switches) for each chart item and compute the
derivation with the least switch-count. Specifically,
in the dynamic program, if we use a SWITCH rule
Xp ? Xq, then we update
s(Xp, i, j) := s(Xq, i, j) + 1.
If we use a continue rule Xp ? Yq Zr, then the up-
date is
s(Xp, i, j) := s(Yq, i, k) + s(Zr, k, j),
where k is a split point in the chart. Using this
dynamic program, we compute the exact shortest
derivation parse in the full all-fragments grammar
(which is reduced to a PCFG with 2 rules schemas
as described above).
3 Basic SDP: Inaccurate and Slow
SDP in its most basic form is appealingly simple,
but has two serious issues: it is both slow and in-
accurate. Because there are millions of grammar
2This grammar is a very minor variant of the reduction of
SDP suggested by Goodman (2003).
3For a compact WSJ training set with graph packing (see
Bansal and Klein (2010)) and one level of parent annotation
and markovization, our grammar has 0.9 million indexed sym-
bols compared to 7.5 million unbinarized (and 0.75 million bi-
narized) explicitly-extracted fragments of just depth 1 and 2.
Test Sentence 
Test Parse 
The  girl 
Training Data 
DT-2 
The girl 
NP-4 
DT-5 NN-6 
girl The 
NP-1 
DT-2 NN-3 
Derivation 2 Derivation 1 
NP 
DT NN 
The girl 
NP-1 
DT-2 NN-3 
The girl 
NP-4 
DT-5 
A girl 
NN-6 
SWITCH 
Figure 1: SDP - the best parse corresponds to the shortest
derivation (fewest switches).
symbols, exact SDP parsing takes more than 45 sec-
onds per sentence in our implementation (in addition
to being highly memory-intensive). Many methods
exist for speeding up parsing through approxima-
tion, but basic SDP is too inaccurate to merit them.
When implemented as described in Section 2, SDP
achieves only 66% F1 on the WSJ task (dev set, ?
40 words).
Why does SDP perform so poorly? One reason
for low accuracy may be that there are many short-
est derivations, i.e. derivations that are all built with
the fewest number of fragments, and that tie break-
ing could be at fault. To investigate this, we tried
various methods for tie-breaking: FIRST/LAST (pro-
cedurally break ties), UNIFORM (sample derivations
equally), FREQ (use the frequency of local rules).
However, none of these methods help much, giv-
ing results within a percentage of F1. In fact, even
oracle tie-breaking, where ties are broken to favor
the number of gold constituents in the derivation
achieves only 80% F1, indicating that correct deriva-
tions are often not the shortest ones. Another rea-
son for the poor performance of SDP may be that
the parameter-free treatment of the lexical layer is
particularly pathological. Indeed, this hypothesis is
partially verified by the result that using a lexicon
(similar to that in Petrov et al (2006)) at the termi-
nal layer brings the uniform tie-breaking result up to
80% F1. However, combining a lexicon with oracle
tie-breaking yields only 81.8% F1.
These results at first seem quite discouraging, but
we will show that they can be easily improved with
information from even a simple PCFG.
721
4 Improvements from a Coarse PCFG
The additional information that makes shortest
derivation parsing work comes from a coarse un-
lexicalized PCFG. In the standard way, our PCFG
consists of the local (depth-1) rules X ? Y Z with
probability P (Y Z|X) computed using the count
of the rule and the count of the nonterminal X in
the given treebank (no smoothing was used). Our
coarse grammar uses a lexicon with unknown word
classes, similar to that in Petrov et al (2006). When
taken from a binarized treebank with one level of
parent annotation (Johnson, 1998) and horizontal
markovization, the PCFG is quite small, with around
3500 symbols and 25000 rules; it achieves an accu-
racy of 84% on its own (see Table 2), so the PCFG
on its own is better than the basic SDP, but still rela-
tively weak.
When filtered by a coarse PCFG pass, how-
ever, SDP becomes both fast and accurate, even for
the basic, lexicon-free SDP formulation. Summed
marginals (posteriors) are computed in the coarse
PCFG and used for pruning and tie-breaking in the
SDP chart, as described next. Pruning works in the
standard coarse-to-fine (CTF) way (see Charniak et
al. (2006)). If a particular base symbol X is pruned
by the PCFG coarse pass for a particular span (i, j)
(i.e., the posterior marginal P (X, i, j|s) is less than
a certain threshold), then in the full SDP pass we do
not allow building any indexed symbol Xl of type X
for span (i, j). In all our pruning-based experiments,
we use a log posterior threshold of ?3.8, tuned on
the WSJ development set.
We also use the PCFG coarse pass for tie-
breaking. During Viterbi shortest-derivation pars-
ing (after coarse-pruning), if two derivations have
the same cost (i.e., the number of switches), then we
break the tie between them by choosing the deriva-
tion which has a higher sum of coarse posteriors
(i.e., the sum of the coarse PCFG chart-cell pos-
teriors P (X, i, j|s) used to build the derivation).4
The coarse PCFG has an extremely beneficial in-
teraction with the fine all-fragments SDP grammar,
wherein the accuracy of the combined grammars
is significantly higher than either individually (see
4This is similar to the maximum recall objective for approx-
imate inference (Goodman, 1996b). The product of posteriors
also works equally well.
dev (? 40) test (? 40)
Model F1 EX F1 EX
B&K2010 pruned 88.4 33.7 88.5 33.0
B&K2010 unpruned 87.9 32.4 88.1 31.9
Table 1: Accuracy (F1) and exact match (EX) for Bansal and
Klein (2010). The pruned row shows their original results with
coarse-to-fine pruning. The unpruned row shows new results
for an unpruned version of their parser; these accuracies are
very similar to their pruned counterparts.
Table 2). In addition, the speed of parsing and
memory-requirements improve by more than an or-
der of magnitude over the exact SDP pass alone.
It is perhaps surprising that coarse-pass pruning
improves accuracy by such a large amount for SDP.
Indeed, given that past all-fragments work has used
a coarse pass for speed, and that we are the first (to
our knowledge) to actually parse at scale with an
implicit grammar without such a coarse pass, it is
a worry that previous results could be crucially de-
pendent on fortuitous coarse-pass pruning. To check
one such result, we ran the full, weighted AFG con-
struction of Bansal and Klein (2010) without any
pruning (using the maximum recall objective as they
did). Their results hold up without pruning: the re-
sults of the unpruned version are only around 0.5%
less (in parsing F1) than the results achieved with
pruning (see Table 1). However, in the case of our
shortest-derivation parser, the coarse-pass is essen-
tial for high accuracies (and for speed and memory,
as always).
5 Results
We have seen that basic, unpruned SDP is both slow
and inaccurate, but improves greatly when comple-
mented by a coarse PCFG pass; these results are
shown in Table 2. Shortest derivation parsing with a
PCFG coarse-pass (PCFG+SDP) achieves an accu-
racy of nearly 87% F1 (on the WSJ test set, ? 40
word sentences), which is significantly higher than
the accuracy of the PCFG or SDP alone.5 When
the coarse PCFG is combined with basic SDP, the
majority of the improvement comes from pruning
with the coarse-posteriors; tie-breaking with coarse-
posteriors contributes around 0.5% F1 over pruning.
5PCFG+SDP accuracies are around 3% higher in F1 and
10% higher in EX than the PCFG-only accuracies.
722
dev (? 40) test (? 40) test (all)
Model F1 EX F1 EX F1 EX
SDP 66.2 18.0 66.9 18.4 64.9 17.3
PCFG 83.8 20.0 84.0 21.6 83.2 20.1
PCFG+SDP 86.4 30.6 86.9 31.5 86.0 29.4
Table 2: Our primary results on the WSJ task. SDP is the
basic unpruned shortest derivation parser. PCFG results are
with one level of parent annotation and horizontal markoviza-
tion. PCFG+SDP incorporates the coarse PCFG posteriors into
SDP. See end of Section 5 for a comparison to other parsing
approaches.
Figure 2 shows the number of fragments for short-
est derivation parsing (averaged for each sentence
length). Note that the number of fragments is of
course greater for the combined PCFG+SDP model
than the exact basic SDP model (which is guaranteed
to be minimal). This result provides some analysis
of how coarse-pruning helps SDP: it illustrates that
the coarse-pass filters out certain short but inaccu-
rate derivations (that the minimal SDP on its own is
forced to choose) to improve performance.
Figure 3 shows the parsing accuracy of the
PCFG+SDP model for various pruning thresholds
in coarse-to-fine pruning. Note how this is differ-
ent from the standard coarse-pass pruning graphs
(see Charniak et al (1998), Petrov and Klein (2007),
Bansal and Klein (2010)) where only a small im-
provement is achieved from pruning. In contrast,
coarse-pass pruning provides large accuracy benefits
here, perhaps because of the unusual complementar-
ity of the two grammars (typical coarse passes are
designed to be as similar as possible to their fine
counterparts, even explicitly so in Petrov and Klein
(2007)).
Our PCFG+SDP parser is more accurate than re-
cent sampling-based TSG?s (Post and Gildea, 2009;
Cohn and Blunsom, 2010), who achieve 83-85% F1,
and it is competitive with more complex weighted-
fragment approaches.6 See Bansal and Klein (2010)
for a more thorough comparison to other parsing
work. In addition to being accurate, the PCFG+SDP
parser is simple and fast, requiring negligible train-
ing and tuning. It takes 2 sec/sentence, less than 2
GB of memory and is written in less than 2000 lines
6Bansal and Klein (2010) achieve around 1.0% higher F1
than our results without a lexicon (character-level parsing) and
1.5% higher F1 with a lexicon.
0
5
10
15
20
25
0 4 8 12 16 20 24 28 32 36 40
# o
f fra
gmen
ts
 
sentence length  
PCFG + SDP
SDP
Figure 2: The average number of fragments in shortest deriva-
tion parses, computed using the basic version (SDP) and the
pruned version (PCFG+SDP), for WSJ dev-set (? 40 words).
65.0
7 0.0
75.0
8 0.0
85.0
9 0.0
-3 -5 -7 -9 -11 -13 -15 -17Coarse-pass Log Posterior Threshold (PT)  
F1 
No Pruning (PT = -inf)  
Figure 3: Parsing accuracy for various coarse-pass pruning
thresholds (on WSJ dev-set ? 40 words). A larger threshold
means more pruning. These are results without the coarse-
posterior tie-breaking to illustrate the sole effect of pruning.
of Java code, including I/O.7
5.1 Other Treebanks
One nice property of the parameter-free, all-
fragments SDP approach is that we can easily trans-
fer it to any new domain with a treebank, or any
new annotation of an existing treebank. Table 3
shows domain adaptation performance by the re-
sults for training and testing on the Brown and
German datasets.8 On Brown, we perform better
than the relatively complex lexicalized Model 1 of
Collins (1999). For German, our parser outperforms
Dubey (2005) and we are not far behind latent-
variable parsers, for which parsing is substantially
7These statistics can be further improved with standard pars-
ing micro-optimization.
8See Gildea (2001) and Petrov and Klein (2007) for the ex-
act experimental setup that we followed here.
723
test (? 40) test (all)
Model F1 EX F1 EX
BROWN
Gildea (2001) 84.1 ? ? ?
This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6
GERMAN
Dubey (2005) 76.3 ? ? ?
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2
Table 3: Results for training and testing on the Brown and
German treebanks. Gildea (2001) uses the lexicalized Collins?
Model 1 (Collins, 1999).
test (? 40) test (all)
Annotation F1 EX F1 EX
STAN-ANNOTATION 88.1 34.3 87.4 32.2
BERK-ANNOTATION 90.0 38.9 89.5 36.8
Table 4: Results with richer WSJ-annotations from Stanford
and Berkeley parsers.
more complex.
5.2 Treebank Annotations
PCFG+SDP achieves 87% F1 on the English WSJ
task using basic annotation only (i.e., one level
of parent annotation and horizontal markoviza-
tion). Table 4 shows that by pre-transforming the
WSJ treebank with richer annotation from previ-
ous work, we can obtain state-of-the-art accuracies
of up to 90% F1 with no change to our simple
parser. In STAN-ANNOTATION, we annotate the
treebank symbols with annotations from the Stan-
ford parser (Klein and Manning, 2003). In BERK-
ANNOTATION, we annotate with the splits learned
via hard-EM and 5 split-merge rounds of the Berke-
ley parser (Petrov et al, 2006).
6 Conclusion
Our investigation of shortest-derivation parsing
showed that, in the exact case, SDP performs poorly.
When pruned (and, to a much lesser extent, tie-
broken) by a coarse PCFG, however, it is competi-
tive with a range of other, more complex techniques.
An advantage of this approach is that the fine SDP
pass is actually quite simple compared to typical fine
passes, while still retaining enough complementarity
to the coarse PCFG to increase final accuracies. One
aspect of our findings that may apply more broadly
is the caution that coarse-to-fine methods may some-
times be more critical to end system quality than
generally thought.
Acknowledgments
We would like to thank Adam Pauls, Slav Petrov
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by BBN un-
der DARPA contract HR0011-06-C-0022 and by the
Office of Naval Research under MURI Grant No.
N000140911081.
References
Mohit Bansal and Dan Klein. 2010. Simple, Accurate
Parsing with an All-Fragments Grammar. In Proceed-
ings of ACL.
Rens Bod. 1993. Using an Annotated Corpus as a
Stochastic Grammar. In Proceedings of EACL.
Rens Bod. 2000. Parsing with the Shortest Derivation.
In Proceedings of COLING.
Rens Bod. 2001. What is the Minimal Set of Fragments
that Achieves Maximum Parse Accuracy? In Proceed-
ings of ACL.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-Based Best-First Chart Parsing. In Pro-
ceedings of the 6th Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, et al 2006. Multi-
level Coarse-to-fine PCFG Parsing. In Proceedings of
HLT-NAACL.
Trevor Cohn and Phil Blunsom. 2010. Blocked Inference
in Bayesian Tree Substitution Grammars. In Proceed-
ings of NAACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ?05.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP.
Joshua Goodman. 1996a. Efficient Algorithms for Pars-
ing the DOP Model. In Proceedings of EMNLP.
Joshua Goodman. 1996b. Parsing Algorithms and Met-
rics. In Proceedings of ACL.
Joshua Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. In Bod R, Scha R, Sima?an K (eds.)
Data-Oriented Parsing. University of Chicago Press,
Chicago, IL.
724
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of ACL.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of NAACL-
HLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of
ACL-IJCNLP.
Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural Lan-
guage Processing. In Proceedings of COLING.
Khalil Sima?an. 2000. Tree-gram Parsing: Lexical De-
pendencies and Structural Relations. In Proceedings
of ACL.
725
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 389?398,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Coreference Semantics from Web Features
Mohit Bansal and Dan Klein
Computer Science Division
University of California, Berkeley
{mbansal,klein}@cs.berkeley.edu
Abstract
To address semantic ambiguities in corefer-
ence resolution, we use Web n-gram features
that capture a range of world knowledge in a
diffuse but robust way. Specifically, we ex-
ploit short-distance cues to hypernymy, se-
mantic compatibility, and semantic context, as
well as general lexical co-occurrence. When
added to a state-of-the-art coreference base-
line, our Web features give significant gains on
multiple datasets (ACE 2004 and ACE 2005)
and metrics (MUC and B3), resulting in the
best results reported to date for the end-to-end
task of coreference resolution.
1 Introduction
Many of the most difficult ambiguities in corefer-
ence resolution are semantic in nature. For instance,
consider the following example:
When Obama met Jobs, the president dis-
cussed the economy, technology, and educa-
tion. His election campaign is expected to [...]
For resolving coreference in this example, a sys-
tem would benefit from the world knowledge that
Obama is the president. Also, to resolve the pro-
noun his to the correct antecedent Obama, we can
use the knowledge that Obama has an election cam-
paign while Jobs does not. Such ambiguities are
difficult to resolve on purely syntactic or configu-
rational grounds.
There have been multiple previous systems that
incorporate some form of world knowledge in coref-
erence resolution tasks. Most work (Poesio et
al., 2004; Markert and Nissim, 2005; Yang et
al., 2005; Bergsma and Lin, 2006) addresses spe-
cial cases and subtasks such as bridging anaphora,
other anaphora, definite NP reference, and pronoun
resolution, computing semantic compatibility via
Web-hits and counts from large corpora. There
is also work on end-to-end coreference resolution
that uses large noun-similarity lists (Daume? III and
Marcu, 2005) or structured knowledge bases such as
Wikipedia (Yang and Su, 2007; Haghighi and Klein,
2009; Kobdani et al, 2011) and YAGO (Rahman
and Ng, 2011). However, such structured knowledge
bases are of limited scope, and, while Haghighi and
Klein (2010) self-acquires knowledge about corefer-
ence, it does so only via reference constructions and
on a limited scale.
In this paper, we look to the Web for broader if
shallower sources of semantics. In order to harness
the information on the Web without presupposing
a deep understanding of all Web text, we instead
turn to a diverse collection of Web n-gram counts
(Brants and Franz, 2006) which, in aggregate, con-
tain diffuse and indirect, but often robust, cues to
reference. For example, we can collect the co-
occurrence statistics of an anaphor with various can-
didate antecedents to judge relative surface affinities
(i.e., (Obama, president) versus (Jobs, president)).
We can also count co-occurrence statistics of com-
peting antecedents when placed in the context of an
anaphoric pronoun (i.e., Obama?s election campaign
versus Jobs? election campaign).
All of our features begin with a pair of head-
words from candidate mention pairs and compute
statistics derived from various potentially informa-
tive queries? counts. We explore five major cat-
egories of semantically informative Web features,
based on (1) general lexical affinities (via generic
co-occurrence statistics), (2) lexical relations (via
Hearst-style hypernymy patterns), (3) similarity of
entity-based context (e.g., common values of y for
389
which h is a y is attested), (4) matches of distribu-
tional soft cluster ids, and (5) attested substitutions
of candidate antecedents in the context of a pronom-
inal anaphor.
We first describe a strong baseline consisting of
the mention-pair model of the Reconcile system
(Stoyanov et al, 2009; Stoyanov et al, 2010) us-
ing a decision tree (DT) as its pairwise classifier. To
this baseline system, we add our suite of features
in turn, each class of features providing substantial
gains. Altogether, our final system produces the best
numbers reported to date on end-to-end coreference
resolution (with automatically detected system men-
tions) on multiple data sets (ACE 2004 and ACE
2005) and metrics (MUC and B3), achieving signif-
icant improvements over the Reconcile DT baseline
and over the state-of-the-art results of Haghighi and
Klein (2010).
2 Baseline System
Before describing our semantic Web features, we
first describe our baseline. The core inference and
features come from the Reconcile package (Stoy-
anov et al, 2009; Stoyanov et al, 2010), with modi-
fications described below. Our baseline differs most
substantially from Stoyanov et al (2009) in using a
decision tree classifier rather than an averaged linear
perceptron.
2.1 Reconcile
Reconcile is one of the best implementations of the
mention-pair model (Soon et al, 2001) of coref-
erence resolution. The mention-pair model relies
on a pairwise function to determine whether or not
two mentions are coreferent. Pairwise predictions
are then consolidated by transitive closure (or some
other clustering method) to form the final set of
coreference clusters (chains). While our Web fea-
tures could be adapted to entity-mention systems,
their current form was most directly applicable to
the mention-pair approach, making Reconcile a par-
ticularly well-suited platform for this investigation.
The Reconcile system provides baseline features,
learning mechanisms, and resolution procedures that
already achieve near state-of-the-art results on mul-
tiple popular datasets using multiple standard met-
rics. It includes over 80 core features that exploit
various automatically generated annotations such as
named entity tags, syntactic parses, and WordNet
classes, inspired by Soon et al (2001), Ng and
Cardie (2002), and Bengtson and Roth (2008). The
Reconcile system also facilitates standardized em-
pirical evaluation to past work.1
In this paper, we develop a suite of simple seman-
tic Web features based on pairs of mention head-
words which stack with the default Reconcile fea-
tures to surpass past state-of-the-art results.
2.2 Decision Tree Classifier
Among the various learning algorithms that Recon-
cile supports, we chose the decision tree classifier,
available in Weka (Hall et al, 2009) as J48, an open
source Java implementation of the C4.5 algorithm of
Quinlan (1993).
The C4.5 algorithm builds decision trees by incre-
mentally maximizing information gain. The train-
ing data is a set of already classified samples, where
each sample is a vector of attributes or features. At
each node of the tree, C4.5 splits the data on an
attribute that most effectively splits its set of sam-
ples into more ordered subsets, and then recurses on
these smaller subsets. The decision tree can then be
used to classify a new sample by following a path
from the root downward based on the attribute val-
ues of the sample.
We find the decision tree classifier to work better
than the default averaged perceptron (used by Stoy-
anov et al (2009)), on multiple datasets using multi-
ple metrics (see Section 4.3). Many advantages have
been claimed for decision tree classifiers, including
interpretability and robustness. However, we sus-
pect that the aspect most relevant to our case is that
decision trees can capture non-linear interactions be-
tween features. For example, recency is very im-
portant for pronoun reference but much less so for
nominal reference.
3 Semantics via Web Features
Our Web features for coreference resolution are sim-
ple and capture a range of diffuse world knowledge.
Given a mention pair, we use the head finder in Rec-
oncile to find the lexical heads of both mentions (for
1We use the default configuration settings of Reconcile
(Stoyanov et al, 2010) unless mentioned otherwise.
390
example, the head of the Palestinian territories is the
word territories). Next, we take each headword pair
(h1, h2) and compute various Web-count functions
on it that can signal whether or not this mention pair
is coreferent.
As the source of Web information, we use the
Google n-grams corpus (Brants and Franz, 2006)
which contains English n-grams (n = 1 to 5) and
their Web frequency counts, derived from nearly 1
trillion word tokens and 95 billion sentences. Be-
cause we have many queries that must be run against
this corpus, we apply the trie-based hashing algo-
rithm of Bansal and Klein (2011) to efficiently an-
swer all of them in one pass over it. The features
that require word clusters (Section 3.4) use the out-
put of Lin et al (2010).2
We describe our five types of features in turn. The
first four types are most intuitive for mention pairs
where both members are non-pronominal, but, aside
from the general co-occurrence group, helped for all
mention pair types. The fifth feature group applies
only to pairs in which the anaphor is a pronoun but
the antecedent is a non-pronoun. Related work for
each feature category is discussed inline.
3.1 General co-occurrence
These features capture co-occurrence statistics of
the two headwords, i.e., how often h1 and h2 are
seen adjacent or nearly adjacent on the Web. This
count can be a useful coreference signal because,
in general, mentions referring to the same entity
will co-occur more frequently (in large corpora) than
those that do not. Using the n-grams corpus (for n
= 1 to 5), we collect co-occurrence Web-counts by
allowing a varying number of wildcards between h1
and h2 in the query. The co-occurrence value is:
bin
(
log10
(
c12
c1 ? c2
))
2These clusters are derived form the V2 Google n-grams
corpus. The V2 corpus itself is not publicly available, but
the clusters are available at http://www.clsp.jhu.edu/
?sbergsma/PhrasalClusters
where
c12 = count(?h1 ? h2?)
+ count(?h1 ? ? h2?)
+ count(?h1 ? ? ? h2?),
c1 = count(?h1?), and
c2 = count(?h2?).
We normalize the overall co-occurrence count of the
headword pair c12 by the unigram counts of the indi-
vidual headwords c1 and c2, so that high-frequency
headwords do not unfairly get a high feature value
(this is similar to computing scaled mutual infor-
mation MI (Church and Hanks, 1989)).3 This nor-
malized value is quantized by taking its log10 and
binning. The actual feature that fires is an indica-
tor of which quantized bin the query produced. As
a real example from our development set, the co-
occurrence count c12 for the headword pair (leader,
president) is 11383, while it is only 95 for the head-
word pair (voter, president); after normalization and
log10, the values are -10.9 and -12.0, respectively.
These kinds of general Web co-occurrence statis-
tics have been used previously for other supervised
NLP tasks such as spelling correction and syntac-
tic parsing (Bergsma et al, 2010; Bansal and Klein,
2011). In coreference, similar word-association
scores were used by Kobdani et al (2011), but from
Wikipedia and for self-training.
3.2 Hearst co-occurrence
These features capture templated co-occurrence of
the two headwords h1 and h2 in the Web-corpus.
Here, we only collect statistics of the headwords co-
occurring with a generalized Hearst pattern (Hearst,
1992) in between. Hearst patterns capture various
lexical semantic relations between items. For ex-
ample, seeing X is a Y or X and other Y indicates
hypernymy and also tends to cue coreference. The
specific patterns we use are:
? h1 {is | are | was | were} {a | an | the}? h2
? h1 {and | or} {other | the other | another} h2
? h1 other than {a | an | the}? h2
3We also tried adding count(?h1 h2?) to c12 but this
decreases performance, perhaps because truly adjacent occur-
rences are often not grammatical.
391
? h1 such as {a | an | the}? h2
? h1 , including {a | an | the}? h2
? h1 , especially {a | an | the}? h2
? h1 of {the| all}? h2
For this feature, we again use a quantized nor-
malized count as in Section 3.1, but c12 here is re-
stricted to n-grams where one of the above patterns
occurs in between the headwords. We did not al-
low wildcards in between the headwords and the
Hearst-patterns because this introduced a significant
amount of noise. Also, we do not constrain the or-
der of h1 and h2 because these patterns can hold
for either direction of coreference.4 As a real ex-
ample from our development set, the c12 count for
the headword pair (leader, president) is 752, while
for (voter, president), it is 0.
Hypernymic semantic compatibility for corefer-
ence is intuitive and has been explored in varying
forms by previous work. Poesio et al (2004) and
Markert and Nissim (2005) employ a subset of our
Hearst patterns and Web-hits for the subtasks of
bridging anaphora, other-anaphora, and definite NP
resolution. Others (Haghighi and Klein, 2009; Rah-
man and Ng, 2011; Daume? III and Marcu, 2005)
use similar relations to extract compatibility statis-
tics from Wikipedia, YAGO, and noun-similarity
lists. Yang and Su (2007) use Wikipedia to auto-
matically extract semantic patterns, which are then
used as features in a learning setup. Instead of ex-
tracting patterns from the training data, we use all
the above patterns, which helps us generalize to new
datasets for end-to-end coreference resolution (see
Section 4.3).
3.3 Entity-based context
For each headword h, we first collect context seeds
y using the pattern
h {is | are | was | were} {a | an | the}? y
taking seeds y in order of decreasing Web count.
The corresponding ordered seed list Y = {y} gives
us useful information about the headword?s entity
type. For example, for h = president, the top
4Two minor variants not listed above are h1 including h2
and h1 especially h2.
30 seeds (and their parts of speech) include impor-
tant cues such as president is elected (verb), pres-
ident is authorized (verb), president is responsible
(adjective), president is the chief (adjective), presi-
dent is above (preposition), and president is the head
(noun).
Matches in the seed lists of two headwords can
be a strong signal that they are coreferent. For ex-
ample, in the top 30 seed lists for the headword
pair (leader, president), we get matches including
elected, responsible, and expected. To capture this
effect, we create a feature that indicates whether
there is a match in the top k seeds of the two head-
words (where k is a hyperparameter to tune).
We create another feature that indicates whether
the dominant parts of speech in the seed lists
matches for the headword pair. We first collect the
POS tags (using length 2 character prefixes to indi-
cate coarse parts of speech) of the seeds matched in
the top k? seed lists of the two headwords, where
k? is another hyperparameter to tune. If the domi-
nant tags match and are in a small list of important
tags ({JJ, NN, RB, VB}), we fire an indicator feature
specifying the matched tag, otherwise we fire a no-
match indicator. To obtain POS tags for the seeds,
we use a unigram-based POS tagger trained on the
WSJ treebank training set.
3.4 Cluster information
The distributional hypothesis of Harris (1954) says
that words that occur in similar contexts tend to have
a similar linguistic behavior. Here, we design fea-
tures with the idea that this hypothesis extends to
reference: mentions occurring in similar contexts
in large document sets such as the Web tend to be
compatible for coreference. Instead of collecting the
contexts of each mention and creating sparse fea-
tures from them, we use Web-scale distributional
clustering to summarize compatibility.
Specifically, we begin with the phrase-based clus-
ters from Lin et al (2010), which were created us-
ing the Google n-grams V2 corpus. These clusters
come from distributional K-Means clustering (with
K = 1000) on phrases, using the n-gram context as
features. The cluster data contains almost 10 mil-
lion phrases and their soft cluster memberships. Up
to twenty cluster ids with the highest centroid sim-
ilarities are included for each phrase in this dataset
392
(Lin et al, 2010).
Our cluster-based features assume that if the
headwords of the two mentions have matches in
their cluster id lists, then they are more compatible
for coreference. We check the match of not just the
top 1 cluster ids, but also farther down in the 20 sized
lists because, as discussed in Lin and Wu (2009),
the soft cluster assignments often reveal different
senses of a word. However, we also assume that
higher-ranked matches tend to imply closer mean-
ings. To this end, we fire a feature indicating the
value bin(i+j), where i and j are the earliest match
positions in the cluster id lists of h1 and h2. Binning
here means that match positions in a close range
generally trigger the same feature.
Recent previous work has used clustering infor-
mation to improve the performance of supervised
NLP tasks such as NER and dependency parsing
(Koo et al, 2008; Lin and Wu, 2009). However, in
coreference, the only related work to our knowledge
is from Daume? III and Marcu (2005), who use word
class features derived from a Web-scale corpus via a
process described in Ravichandran et al (2005).
3.5 Pronoun context
Our last feature category specifically addresses pro-
noun reference, for cases when the anaphoric men-
tion NP2 (and hence its headword h2) is a pronoun,
while the candidate antecedent mention NP1 (and
hence its headword h1) is not. For such a head-
word pair (h1, h2), the idea is to substitute the non-
pronoun h1 into h2?s position and see whether the
result is attested on the Web.
If the anaphoric pronominal mention is h2 and its
sentential context is l? l h2 r r?, then the substituted
phrase will be l? l h1 r r?.5 High Web counts of sub-
stituted phrases tend to indicate semantic compati-
bility. Perhaps unsurprisingly for English, only the
right context was useful in this capacity. We chose
the following three context types, based on perfor-
mance on a development set:
5Possessive pronouns are replaced with an additional apos-
trophe, i.e., h1 ?s. We also use features (see R1Gap) that allow
wildcards (?) in between the headword and the context when
collecting Web-counts, in order to allow for determiners and
other filler words.
? h1 r (R1)
? h1 r r? (R2)
? h1 ? r (R1Gap)
As an example of the R1Gap feature, if the
anaphor h2 + context is his victory and one candidate
antecedent h1 is Bush, then we compute the normal-
ized value
count(?Bush ?s ? victory?)
count(? ? ?s ? victory?)count(?Bush?)
In general, we compute
count(?h1 ?s ? r?)
count(? ? ?s ? r?)count(?h1?)
The final feature value is again a normalized count
converted to log10 and then binned.
6 We have three
separate features for the R1, R2, and R1Gap context
types. We tune a separate bin-size hyperparameter
for each of these three features.
These pronoun resolution features are similar to
selectional preference work by Yang et al (2005)
and Bergsma and Lin (2006), who compute seman-
tic compatibility for pronouns in specific syntactic
relationships such as possessive-noun, subject-verb,
etc. In our case, we directly use the general context
of any pronominal anaphor to find its most compat-
ible antecedent.
Note that all our above features are designed to be
non-sparse by firing indicators of the quantized Web
statistics and not the lexical- or class-based identities
of the mention pair. This keeps the total number of
features small, which is important for the relatively
small datasets used for coreference resolution. We
go from around 100 features in the Reconcile base-
line to around 165 features after adding all our Web
features.
6Normalization helps us with two kinds of balancing. First,
we divide by the count of the antecedent so that when choos-
ing the best antecedent for a fixed anaphor, we are not biased
towards more frequently occurring antecedents. Second, we di-
vide by the count of the context so that across anaphora, an
anaphor with rarer context does not get smaller values (for all its
candidate antecedents) than another anaphor with a more com-
mon context.
393
Dataset docs dev test ment chn
ACE04 128 63/27 90/38 3037 1332
ACE05 81 40/17 57/24 1991 775
ACE05-ALL 599 337/145 482/117 9217 3050
Table 1: Dataset characteristics ? docs: the total number of doc-
uments; dev: the train/test split during development; test: the
train/test split during testing; ment: the number of gold men-
tions in the test split; chn: the number of coreference chains in
the test split.
4 Experiments
4.1 Data
We show results on three popular and comparatively
larger coreference resolution data sets ? the ACE04,
ACE05, and ACE05-ALL datasets from the ACE
Program (NIST, 2004). In ACE04 and ACE05, we
have only the newswire portion (of the original ACE
2004 and 2005 training sets) and use the standard
train/test splits reported in Stoyanov et al (2009)
and Haghighi and Klein (2010). In ACE05-ALL,
we have the full ACE 2005 training set and use the
standard train/test splits reported in Rahman and Ng
(2009) and Haghighi and Klein (2010). Note that
most previous work does not report (or need) a stan-
dard development set; hence, for tuning our fea-
tures and its hyper-parameters, we randomly split
the original training data into a training and devel-
opment set with a 70/30 ratio (and then use the full
original training set during testing). Details of the
corpora are shown in Table 1.7
Details of the Web-scale corpora used for extract-
ing features are discussed in Section 3.
4.2 Evaluation Metrics
We evaluated our work on both MUC (Vilain et al,
1995) and B3 (Bagga and Baldwin, 1998). Both
scorers are available in the Reconcile infrastruc-
ture.8 MUC measures how many predicted clusters
need to be merged to cover the true gold clusters.
B3 computes precision and recall for each mention
by computing the intersection of its predicted and
gold cluster and dividing by the size of the predicted
7Note that the development set is used only for ACE04, be-
cause for ACE05, and ACE05-ALL, we directly test using the
features tuned on ACE04.
8Note that B3 has two versions which handle twinless (spu-
rious) mentions in different ways (see Stoyanov et al (2009) for
details). We use the B3All version, unless mentioned otherwise.
MUC B3
Feature P R F1 P R F1
AvgPerc 69.0 63.1 65.9 82.2 69.9 75.5
DecTree 80.9 61.0 69.5 89.5 69.0 77.9
+ Co-occ 79.8 62.1 69.8 88.7 69.8 78.1
+ Hearst 80.0 62.3 70.0 89.1 70.1 78.5
+ Entity 79.4 63.2 70.4 88.1 70.9 78.6
+ Cluster 79.5 63.6 70.7 87.9 71.2 78.6
+ Pronoun 79.9 64.3 71.3 88.0 71.6 79.0
Table 2: Incremental results for the Web features on the ACE04
development set. AvgPerc is the averaged perceptron baseline,
DecTree is the decision tree baseline, and the +Feature rows
show the effect of adding a particular feature incrementally (not
in isolation) to the DecTree baseline. The feature categories
correspond to those described in Section 3.
and gold cluster, respectively. It is well known
(Recasens and Hovy, 2010; Ng, 2010; Kobdani et
al., 2011) that MUC is biased towards large clus-
ters (chains) whereas B3 is biased towards singleton
clusters. Therefore, for a more balanced evaluation,
we show improvements on both metrics simultane-
ously.
4.3 Results
We start with the Reconcile baseline but employ the
decision tree (DT) classifier, because it has signifi-
cantly better performance than the default averaged
perceptron classifier used in Stoyanov et al (2009).9
Table 2 compares the baseline perceptron results to
the DT results and then shows the incremental addi-
tion of the Web features to the DT baseline (on the
ACE04 development set).
The DT classifier, in general, is precision-biased.
The Web features somewhat balance this by increas-
ing the recall and decreasing precision to a lesser ex-
tent, improving overall F1. Each feature type incre-
mentally increases both MUC and B3 F1-measures,
showing that they are not taking advantage of any
bias of either metric. The incremental improve-
ments also show that each Web feature type brings
in some additional benefit over the information al-
ready present in the Reconcile baseline, which in-
cludes alias, animacy, named entity, and WordNet
9Moreover, a DT classifier takes roughly the same amount of
time and memory as a perceptron on our ACE04 development
experiments. It is, however, slower and more memory-intensive
(?3x) on the bigger ACE05-ALL dataset.
394
MUC B3
System P R F1 P R F1
ACE04-TEST-RESULTS
Stoyanov et al (2009) - - 62.0 - - 76.5
Haghighi and Klein (2009) 67.5 61.6 64.4 77.4 69.4 73.2
Haghighi and Klein (2010) 67.4 66.6 67.0 81.2 73.3 77.0
This Work: Perceptron Baseline 65.5 61.9 63.7 84.1 70.9 77.0
This Work: DT Baseline 76.0 60.7 67.5 89.6 70.3 78.8
This Work: DT + Web Features 74.8 64.2 69.1 87.5 73.7 80.0
This Work: ? of DT+Web over DT (p < 0.05) 1.7 (p < 0.005) 1.3
ACE05-TEST-RESULTS
Stoyanov et al (2009) - - 67.4 - - 73.7
Haghighi and Klein (2009) 73.1 58.8 65.2 82.1 63.9 71.8
Haghighi and Klein (2010) 74.6 62.7 68.1 83.2 68.4 75.1
This Work: Perceptron Baseline 72.2 61.6 66.5 85.0 65.5 73.9
This Work: DT Baseline 79.6 59.7 68.2 89.4 64.2 74.7
This Work: DT + Web Features 75.0 64.7 69.5 81.1 70.8 75.6
This Work: ? of DT+Web over DT (p < 0.12) 1.3 (p < 0.1) 0.9
ACE05-ALL-TEST-RESULTS
Rahman and Ng (2009) 75.4 64.1 69.3 54.4 70.5 61.4
Haghighi and Klein (2009) 72.9 60.2 67.0 53.2 73.1 61.6
Haghighi and Klein (2010) 77.0 66.9 71.6 55.4 74.8 63.8
This Work: Perceptron Baseline 68.9 60.4 64.4 80.6 60.5 69.1
This Work: DT Baseline 78.0 60.4 68.1 85.1 60.4 70.6
This Work: DT + Web Features 77.6 64.0 70.2 80.7 65.9 72.5
This Work: ? of DT+Web over DT (p < 0.001) 2.1 (p < 0.001) 1.9
Table 3: Primary test results on the ACE04, ACE05, and ACE05-ALL datasets. All systems reported here use automatically
extracted system mentions. B3 here is the B3All version of Stoyanov et al (2009). We also report statistical significance of the
improvements from the Web features on the DT baseline, using the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1993). The
perceptron baseline in this work (Reconcile settings: 15 iterations, threshold = 0.45, SIG for ACE04 and AP for ACE05, ACE05-
ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in
their paper (p.c.). Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and
only the singleton twinless system mentions, so it is neither B3All nor B3None). For completeness, our (untuned) B3None results
(DT + Web) on the ACE05-ALL dataset are P=69.9|R=65.9|F1=67.8.
class / sense information.10
Table 3 shows our primary test results on the
ACE04, ACE05, and ACE05-ALL datasets, for the
MUC and B3 metrics. All systems reported use au-
tomatically detected mentions. We report our re-
sults (the 3 rows marked ?This Work?) on the percep-
tron baseline, the DT baseline, and the Web features
added to the DT baseline. We also report statistical
significance of the improvements from the Web fea-
10We also initially experimented with smaller datasets
(MUC6 and MUC7) and an averaged perceptron baseline, and
we did see similar improvements, arguing that these features are
useful independently of the learning algorithm and dataset.
tures on the DT baseline.11 For significance testing,
we use the bootstrap test (Noreen, 1989; Efron and
Tibshirani, 1993).
Our main comparison is against Haghighi and
Klein (2010), a mostly-unsupervised generative ap-
proach that models latent entity types, which gen-
erate specific entities that in turn render individual
mentions. They learn on large datasets including
11All improvements are significant, except on the small
ACE05 dataset with the MUC metric (where it is weak, at
p < 0.12). However, on the larger version of this dataset,
ACE05-ALL, we get improvements which are both larger and
more significant (at p < 0.001).
395
Wikipedia, and their results are state-of-the-art in
coreference resolution. We outperform their system
on most datasets and metrics (except on ACE05-
ALL for the MUC metric). The other systems we
compare to and outperform are the perceptron-based
Reconcile system of Stoyanov et al (2009), the
strong deterministic system of Haghighi and Klein
(2009), and the cluster-ranking model of Rahman
and Ng (2009).
We develop our features and tune their hyper-
parameter values on the ACE04 development set and
then use these on the ACE04 test set.12 On the
ACE05 and ACE05-ALL datasets, we directly trans-
fer our Web features and their hyper-parameter val-
ues from the ACE04 dev-set, without any retuning.
The test improvements we get on all the datasets (see
Table 3) suggest that our features are generally use-
ful across datasets and metrics.13
5 Analysis
In this section, we briefly discuss errors (in the DT
baseline) corrected by our Web features, and ana-
lyze the decision tree classifier built during training
(based on the ACE04 development experiments).
To study error correction, we begin with the men-
tion pairs that are coreferent according to the gold-
standard annotation (after matching the system men-
tions to the gold ones). We consider the pairs that are
wrongly predicted to be non-coreferent by the base-
line DT system but correctly predicted to be corefer-
ent when we add our Web features. Some examples
of such pairs include:
Iran ; the country
the EPA ; the agency
athletic director ; Mulcahy
Democrat Al Gore ; the vice president
12Note that for the ACE04 dataset only, we use the ?SmartIn-
stanceGenerator? (SIG) filter of Reconcile that uses only a fil-
tered set of mention-pairs (based on distance and other proper-
ties of the pair) instead of the ?AllPairs? (AP) setting that uses
all pairs of mentions, and makes training and tuning very slow.
13For the ACE05 and ACE05-ALL datasets, we revert to the
?AllPairs? (AP) setting of Reconcile because this gives us base-
lines competitive with Haghighi and Klein (2010). Since we did
not need to retune on these datasets, training and tuning speed
were not a bottleneck. Moreover, the improvements from our
Web features are similar even when tried over the SIG baseline;
hence, the filter choice doesn?t affect the performance gain from
the Web features.
Barry Bonds ; the best baseball player
Vojislav Kostunica ; the pro-democracy leader
its closest rival ; the German magazine Das Motorrad
One of those difficult-to-dislodge judges ; John Marshall
These pairs are cases where our features
on Hearst-style co-occurrence and entity-based
context-match are informative and help discriminate
in favor of the correct antecedents. One advan-
tage of using Web-based features is that the Web
has a surprising amount of information on even rare
entities such as proper names. Our features also
correct coreference for various cases of pronominal
anaphora, but these corrections are harder to convey
out of context.
Next, we analyze the decision tree built after
training the classifier (with all our Web features in-
cluded). Around 30% of the decision nodes (both
non-terminals and leaves) correspond to Web fea-
tures, and the average error in classification at the
Web-feature leaves is only around 2.5%, suggest-
ing that our features are strongly discriminative for
pairwise coreference decisions. Some of the most
discriminative nodes correspond to the general co-
occurrence feature for most (binned) log-count val-
ues, the Hearst-style co-occurrence feature for its
zero-count value, the cluster-match feature for its
zero-match value, and the R2 pronoun context fea-
ture for certain (binned) log-count values.
6 Conclusion
We have presented a collection of simple Web-count
features for coreference resolution that capture a
range of world knowledge via statistics of general
lexical co-occurrence, hypernymy, semantic com-
patibility, and semantic context. When added to a
strong decision tree baseline, these features give sig-
nificant improvements and achieve the best results
reported to date, across multiple datasets and met-
rics.
Acknowledgments
We would like to thank Nathan Gilbert, Adam Pauls,
and the anonymous reviewers for their helpful sug-
gestions. This research is supported by Qualcomm
via an Innovation Fellowship to the first author
and by BBN under DARPA contract HR0011-12-C-
0014.
396
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of MUC-7
and LREC Workshop.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of EMNLP.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In Proceedings
of COLING-ACL.
Shane Bergsma, Emily Pitler, and Dekang Lin. 2010.
Creating robust supervised classifiers via web-scale n-
gram data. In Proceedings of ACL.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of ACL.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
EMNLP.
B. Efron and R. Tibshirani. 1993. An introduction to the
bootstrap. Chapman & Hall CRC.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Proceedings of EMNLP.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of NAACL-HLT.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Hamidreza Kobdani, Hinrich Schutze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?402.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of ACL.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of
ACL.
NIST. 2004. The ACE evaluation plan. In NIST.
E.W. Noreen. 1989. Computer intensive methods for
hypothesis testing: An introduction. Wiley, New York.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of ACL.
J. R. Quinlan. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized algorithms and NLP: Using local-
ity sensitive hash functions for high speed noun clus-
tering. In Proceedings of ACL.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL/IJCNLP.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Rec-
oncile: A coreference resolution research platform. In
Technical report, Cornell University.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL.
397
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
ACL.
398
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959?968,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Large-Scale Syntactic Language Modeling with Treelets
Adam Pauls Dan Klein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720, USA
{adpauls,klein}@cs.berkeley.edu
Abstract
We propose a simple generative, syntactic
language model that conditions on overlap-
ping windows of tree context (or treelets) in
the same way that n-gram language models
condition on overlapping windows of linear
context. We estimate the parameters of our
model by collecting counts from automati-
cally parsed text using standard n-gram lan-
guage model estimation techniques, allowing
us to train a model on over one billion tokens
of data using a single machine in a matter of
hours. We evaluate on perplexity and a range
of grammaticality tasks, and find that we per-
form as well or better than n-gram models and
other generative baselines. Our model even
competes with state-of-the-art discriminative
models hand-designed for the grammaticality
tasks, despite training on positive data alone.
We also show fluency improvements in a pre-
liminary machine translation experiment.
1 Introduction
N -gram language models are a central component
of all speech recognition and machine translation
systems, and a great deal of research centers around
refining models (Chen and Goodman, 1998), ef-
ficient storage (Pauls and Klein, 2011; Heafield,
2011), and integration into decoders (Koehn, 2004;
Chiang, 2005). At the same time, because n-gram
language models only condition on a local window
of linear word-level context, they are poor models of
long-range syntactic dependencies. Although sev-
eral lines of work have proposed generative syntac-
tic language models that improve on n-gram mod-
els for moderate amounts of data (Chelba, 1997; Xu
et al, 2002; Charniak, 2001; Hall, 2004; Roark,
2004), these models have only recently been scaled
to the impressive amounts of data routinely used by
n-gram language models (Tan et al, 2011).
In this paper, we describe a generative, syntac-
tic language model that conditions on local con-
text treelets1 in a parse tree, backing off to smaller
treelets as necessary. Our model can be trained sim-
ply by collecting counts and using the same smooth-
ing techniques normally applied to n-gram mod-
els (Kneser and Ney, 1995), enabling us to apply
techniques developed for scaling n-gram models out
of the box (Brants et al, 2007; Pauls and Klein,
2011). The simplicity of our training procedure al-
lows us to train a model on a billion tokens of data in
a matter of hours on a single machine, which com-
pares favorably to the more involved training algo-
rithm of Tan et al (2011), who use a two-pass EM
training algorithm that takes several days on several
hundred CPUs using similar amounts of data.
The simplicity of our approach also contrasts with
recent work on language modeling with tree sub-
stitution grammars (Post and Gildea, 2009), where
larger treelet contexts are incorporated by using so-
phisticated priors to learn a segmentation of parse
trees. Such an approach implicitly assumes that a
?correct? segmentation exists, but it is not clear that
this is true in practice. Instead, we build upon the
success of n-gram language models, which do not
assume a segmentation and instead score all over-
lapping contexts.
We evaluate our model in terms of perplexity, and
show that we achieve the same performance as a
state-of-the-art n-gram model. We also evaluate our
model on several grammaticality tasks proposed in
1We borrow the term treelet from Quirk et al (2005), who
use it to refer to an arbitrary connected subgraph of a tree.
959
(a) The index fell 109.85 Monday . (b) ROOT
S-VBD?ROOT
NP-NN
DT-the
The
NN
index
VP-VBD?S
VBD
fell
CD-DC
109.85
NNTP
Monday
.
.
(c) ROOT
S-VBD?ROOT
NP-NN
DT-the
The
NN
index
VP-VBD?S
VBD
fell
CD-DC
109.85
NNTP
Monday
.
.
5-GRAM
The board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .
They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :
Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :
But 8.32 % stake in and Rep. any money for you got from several months , ? he says .
TREELET
Why a $ 1.2 million investment in various types of the bulk of TVS E. August ?
? One operating price position has a system that has Quartet for the first time , ? he said .
He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations . ?
The centers are losses of meals , and the runs are willing to like them .
Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.
rule context would need its own state in the gram-
mar), and extensive pruning would be in order.
In practice, however, language models are nor-
mally integrated into a decoder, a non-trivial task
that is highly problem-dependent and beyond the
scope of this paper. However, we note that for
machine translation, a model that builds target-side
constituency parses, such as that of Galley et al
(2006), combined with an efficient pruning strategy
like cube pruning (Chiang, 2005), should be able to
integrate our model without much difficulty.
That said, for evaluation purposes, whenever we
need to query our model, we use the simple strategy
of parsing a sentence using a black box parser, and
summing over our model?s probabilities of the 1000-
best parses.4 Note that the bottleneck in this case
is the parser, so our model can essentially score a
sentence at the speed of a parser.
5 Experiments
We evaluate our model along several dimensions.
We first show some sample sentences generated by
our model in Section 5.1. We report perplexity re-
4We found that using the 1-best worked just as well as the
1000-best on our grammaticality tasks, but significantly overes-
timated our model?s perplexities.
sults in Section 5.2. In Section 5.3, we measure
its ability to distinguish between grammatical En-
glish and various types of automatically generated,
or pseudo-negative,5 English. We report machine
translation reranking results in Section 5.4.
5.1 Generating Samples
Because our model is generative, we can qualita-
tively assess it by generating samples and verifying
that they are more syntactically coherent than other
approaches. In Table 1, we show the first four sam-
ples of length between 15 and 20 generated from
both model and a 5-gram model trained on the Penn
Treebank.
5.2 Perplexity
Perplexity is the standard intrinsic evaluation metric
for language models. It measures the inverse of the
per-word probability a model assigns to some held-
out set of grammatical English (so lower is better).
For training data, we constructed a large treebank by
concatenating the Penn Treebank, the Brown Cor-
pus, the 50K BLLIP training sentences from Post
(2011), and the AFP and APW portions of English
5We follow Okanohara and Tsujii (2007) in using the term
pseudo-negative to highlight the fact that automatically gener-
ated negative examples might not actually be ungrammatical.
Figure 1: Conditioning contexts and back-off strategies for Markov models. The bolded symbol indicates the part of the
tree/sentence being generated, and the dotted lines represent the conditioning contexts; back-off proceeds from the largest to the
smallest context. (a) A trigram model. (b) The context used for non-terminal productions in our treelet model. For this context,
P=VP-VBD?S, P ?=S-VBD?ROOT, and r?=S-VBD?ROOT?NP-NN VP-VBD?S . (c) The context used for terminal productions
in our treelet model. Here, P=VBD, R=CD-DC, r?=VP-VBD?S?VBD CD-DC NNTP, w?1=index, and w?2=The. Note that the
tree is a modified version of a standard Penn Treebank parse ? see Section 3 for details.
the literature (Okanohara and Tsujii, 2007; Foster et
al., 2008; Cherry and Quirk, 2008) and show that
it consistently outperforms an n-gram model as well
as other head-driven and tree-driven generative base-
lines. Our model even competes with state-of-the-art
discriminative classifiers specifically designed for
each task, despite being estimated on positive data
alone. We also show fluency improvements in a pre-
liminary machine translation reranking experiment.
2 Treelet Language Modeling
The common denominator of most n-gram language
models is that they assign probabilities roughly ac-
cording to empirical frequencies for observed n-
grams, but fall back to distributions conditioned on
smaller contexts for unobserved n-grams, as shown
in Figure 1(a). This type of smoothing is both highly
robust and easy to implement, requiring only the col-
lection of counts from data.
We would like to apply the same smoothing tech-
niques to distributions over rule yields in a con-
stituency tree, conditioned on contexts consisting
of previously generated treelets (rules, nodes, etc.).
Formally, let T be a constituency tree consisting of
context-free rules of the form r = P ? C1 ? ? ?Cd,
where P is the parent symbol of rule r and Cd1 =
C1 . . . Cd are its children. We wish to assign proba-
bilities to trees2
2A distribution over trees also induces a distribution over
sentences w`1 given by p(w`1) = PT :s(T )=w`1 p(T ), where
p(T ) =
?
r?T
p(Cd1 |h)
where the conditioning context h is some portion of
the already-generated parts of the tree. In this paper,
we assume that the children of a rule are expanded
from left to right, so that when generatin th yi ld
Cd1 , all treelets above and left of th parent P are
available. Note that a raw PCFG would condition
only on P , i.e. h = P .
As in the n-gram case, we would like to pick h
to be large enough to capture relevant dependencies,
but small enough that we can obtain meaningful es-
timates from data. We start with a straightforward
choice of context: we condition on P , as well as the
rule r? that generated P , as shown in in Figure 1(b).
Conditioning on the parent rule r? allows us to
capture several important dependencies. First, it
captures both P and its parent P ?, which predicts
the distribution over child symbols far better than
just P (Johnson, 1998). Second, it captures posi-
tional effects. For example, subject and object noun
phrases (NPs) have different distributions (Klein and
Manning, 2003), and the position of an NP relative
to a verb is a good indicator of this distinction. Fi-
nally, the generation of words at preterminals can
condition on siblings, allowing the model to capture,
for example, verb subcategorization frames.
We should be clear that we are not the first
s(T ) is the terminal yield of T .
960
to use back-off-based smoothing for syntactic lan-
guage modeling ? such techniques have been ap-
plied to models that condition on head-word con-
texts (Charniak, 2001; Roark, 2004; Zhang, 2009).
Parent rule context has also been employed in trans-
lation (Vaswani et al, 2011). However, to our
knowledge, we are the first to apply these techniques
for language modeling on large amounts of data.
2.1 Lexical context
Although it is tempting to think that we can replace
the left-to-right generation of n-gram models with
the purely top-down generation of typical PCFGs,
in practice, words are often highly predictive of the
words that follow them ? indeed, n-gram models
would be terrible language models if this were not
the case. To capture linear effects, we extend the
context for terminal (lexical) productions to include
the previous two wordsw?2 andw?1 in the sentence
in addition to r?; see Figure 1(c) for a depiction. This
allows us to capture collocations and other lexical
correlations.
2.2 Backing off
As with n-gram models, counts for rule yields con-
ditioned on r? are sparse, and we must choose an ap-
propriate back-off strategy. We handle terminal and
non-terminal productions slightly differently.
For non-terminal productions, we back off from
r? to P and its parent P ?, and then to just P .
That is, we back off from a rule-annotated gram-
mar p(Cd1 |P, P ?, r?) to a parent-annotated gram-
mar (Johnson, 1998) p(Cd1 |P, P ?), then to a raw
PCFG p(Cd1 |P ). In order to generalize to unseen
rule yields Cd1 , we further back off from the ba-
sic PCFG probability p(Cd1 |P ) to p(Ci|Ci?1i?3 , P ), a
4-gram model over symbols C conditioned on P ,
interpolated with an unconditional 4-gram model
p(Ci|C
i?1
i?3 ). In other words, we back off from a raw
PCFG to
?
d?
i=1
p(Ci|C
i?1
i?3 , P ) + (1? ?)
d?
i=1
p(Ci|C
i?1
i?3 )
where ? = 0.9 is an interpolation constant.
For terminal (i.e lexical) productions, we
first remove lexical context, backing off from
p(w|P,R, r?, w?1, w?2) to p(w|P,R, r?, w?1) and
then p(w|P,R, r?). From there, we back off to
p(w|P,R) whereR is the sibling immediately to the
right of P , then to a raw PCFG p(w|P ), and finally
to a unigram distribution. We chose this scheme be-
cause p(w|P,R) allows, for example, a verb to be
generated conditioned on the non-terminal category
of the argument it takes (since arguments usually im-
mediately follow verbs). We depict these two back-
off schemes pictorially in Figure 1(b) and (c).
2.3 Estimation
Estimating the probabilities in our model can be
done very simply using the same techniques (in fact,
the same code) used to estimate n-gram language
models. Our model requires estimates of four distri-
butions: p(Cd1 |P, P ?, r?), p(w|P,R, r?, w?1, w?2),
p(Ci|C
i?1
i?n+1, P ), and p(Ci|Ci?1i?n+1). In each case,
we require empirical counts of treelet tuples in the
same way that we require counts of word tuples for
estimating n-gram language models.
There is one additional hurdle in the estimation of
our model: while there exist corpora with human-
annotated constituency parses like the Penn Tree-
bank (Marcus et al, 1993), these corpora are quite
small ? on the order of millions of tokens ? and we
cannot gather nearly as many counts as we can for n-
grams, for which billions or even trillions (Brants et
al., 2007) of tokens are available on the Web. How-
ever, we can use one of several high-quality con-
stituency parsers (Collins, 1997; Charniak, 2000;
Petrov et al, 2006) to automatically generate parses.
These parses may contain errors, but not all parsing
errors are problematic for our model, since we only
care about the sentences generated by our model and
not the parses themselves. We show in our experi-
ments that the addition of data with automatic parses
does improve the performance of our language mod-
els across a range of tasks.
3 Tree Transformations
In the previous section, we described how to condi-
tion on rich parse context to better capture the dis-
tribution of English trees. While such context al-
lows our model to capture many interesting depen-
dencies, several important dependencies require ad-
ditional attention. In this section, we describe a
961
ROOT
S-VB?ROOT
PRP-he
He
VP-VB?S
VB
reset
NP-NNS
JJ
opening
NNS
arguments
PP-for
IN-for
for
NNT
today
.
.
Figure 2: A sample parse from the Penn Treebank after
the tree transformations described in Section 3. Note that
we have not shown head tag annotations on preterminals
because in that case, the head tag is the preterminal itself.
number of transformations of Treebank constituency
parses that allow us to capture such dependencies.
We list the annotations and deletions in the order in
which they are performed. A sample transformed
tree is shown in Figure 2.
Temporal NPs Following Klein and Manning (2003),
we attempt to annotate temporal noun phrases. Although
the Penn Treebank annotates temporal NPs, most off-the-
shelf parsers do not retain these tags, and we do not as-
sume their presence. Instead, we mark any noun that is
the head of a NP-TMP constituent at least once in the
Treebank as a temporal noun, so for example today would
be tagged as NNT and months would be tagged as NNTS.
Head Annotations We annotate every non-terminal or
preterminal with its head word if the head is a closed-
class word3 and with its head tag otherwise. Klein and
Manning (2003) used head tag annotation extensively,
though they applied their splits much more selectively.
NP Flattening We delete NPs dominated by
other NPs, unless the child NPs are in coordi-
nation or apposition. These NPs typically oc-
cur when nouns are modified by PPs, as in
(NP (NP (NN stock) (NNS sales)) (PP (IN by) (NNS traders))). By
removing the dominated NP, we allow the production
NNS?sales to condition on the presence of a modifying
PP (here a PP head-annotated with by).
Number Annotations Numbers are divided into five
classes: CD-YR for numbers that consist of four digits
(which are usually years); CD-NM for entirely numeric
numbers; CD-DC for numbers that have a decimal; CD-
3We define the following to be closed class words: any punc-
tuation; all inflections of the verbs do, be, and have; and any
word tagged with IN, WDT, PDT, WP, WP$, TO, WRB, RP,
DT, SYM, EX, POS, PRP, AUX, or CC.
MX for numbers that mix letters and digits; and CD-AL
for numbers that are entirely alphabetic.
SBAR Flattening We remove any sentential (S) nodes
immediately dominated by an SBAR. S nodes under
SBAR have very distinct distributions from other senten-
tial nodes, mostly due to empty subjects and/or objects.
VP Flattening We remove any VPs immediately domi-
nating a VP, unless it is conjoined with another VP. In the
Treebank, chains of verbs (e.g. will be going) have a sep-
arate VP for each verb. By flattening such structures, we
allow the main verb and its arguments to condition on the
whole chain of verbs. This effect is particularly important
for passive constructions.
Gapped Sentence Annotation Collins (1999) and
Klein and Manning (2003) annotate nodes which have
empty subjects. Because we only assume the presence
of automatically derived parses, which do not produce
the empty elements in the original Treebank, we must
identify such elements on our own. We use a very simple
procedure: we annotate all S or SBAR nodes that have a
VP before any NPs.
Parent Annotation We annotate all VPs with their par-
ent symbol. Because our treelet model already conditions
on the parent, this has the effect of allowing verbs to con-
dition on their grandparents. This was important for VPs
under SBAR nodes, which often have empty objects. We
also parent-annotated any child of the ROOT.
Unary Deletion We remove all unary productions ex-
cept the root and preterminal productions, keeping only
the bottom-most symbol. Because we are not interested
in the internal labels of the trees, unaries are largely a
nuisance, and their removal brings many symbols into the
context of others.
4 Scoring a Sentence
Computing the probability of a sentence w`1 under
our model requires summing over all possible parses
of w`1. Although our model can be formulated as a
straightforward PCFG, allowing O(`3) computation
of this sum, the grammar constant for this PCFG
would be unmanageably large (since every parent
rule context would need its own state in the gram-
mar), and extensive pruning would be in order.
In practice, however, language models are nor-
mally integrated into a decoder, a non-trivial task
that is highly problem-dependent and beyond the
scope of this paper. For machine translation, a model
that builds target-side constituency parses, such as
that of Galley et al (2006), combined with an ef-
ficient pruning strategy like cube pruning (Chiang,
962
5-GRAM
The board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .
They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :
Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :
But 8.32 % stake in and Rep. any money for you got from several months , ? he says .
TREELET
Why a $ 1.2 million investment in various types of the bulk of TVS E. August ?
? One operating price position has a system that has Quartet for the first time , ? he said .
He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations . ?
The centers are losses of meals , and the runs are willing to like them .
Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.
2005), should be able to integrate our model without
much difficulty.
That said, for evaluation purposes, whenever we
need to query our model, we use the simple strategy
of parsing a sentence using a black box parser, and
summing over our model?s probabilities of the 1000-
best parses.4 Note that the bottleneck in this case
is the parser, so our model can essentially score a
sentence at the speed of a parser.
5 Experiments
We evaluate our model along several dimensions.
We first show some sample generated sentences in
Section 5.1. We report perplexity results in Sec-
tion 5.2. In Section 5.3, we measure its ability to
distinguish between grammatical English and var-
ious types of automatically generated, or pseudo-
negative,5 English. We report machine translation
reranking results in Section 5.4.
5.1 Generating Samples
Because our model is generative, we can qualita-
tively assess it by generating samples and verifying
that they are more syntactically coherent than other
approaches. In Table 1, we show the first four sam-
ples of length between 15 and 20 generated from our
model and a 5-gram model trained on the Penn Tree-
bank.
4We found that using the 1-best worked just as well as the
1000-best on our grammaticality tasks, but significantly overes-
timated our model?s perplexities.
5We follow Okanohara and Tsujii (2007) in using the term
pseudo-negative to highlight the fact that automatically gener-
ated negative examples might not actually be ungrammatical.
5.2 Perplexity
Perplexity is the standard intrinsic evaluation metric
for language models. It measures the inverse of the
per-word probability a model assigns to some held-
out set of grammatical English (so lower is better).
For training data, we constructed a large treebank by
concatenating the WSJ and Brown portions of the
Penn Treebank, the 50K BLLIP training sentences
from Post (2011), and the AFP and APW portions
of English Gigaword version 3 (Graff, 2003), total-
ing about 1.3 billion tokens. We used the human-
annotated parses for the sentences in the Penn Tree-
bank, but parsed the Gigaword and BLLIP sentences
with the Berkeley Parser. Hereafter, we refer to this
training data as our 1B corpus. We used Section 0
of the WSJ as our test corpus. Results are shown in
Table 2. In addition to our TREELET model, we also
show results for the following baselines:
5-GRAM A 5-gram interpolated Kneser-Ney model.
PCFG-LA The Berkeley Parser in language model mode.
HEADLEX A head-lexicalized model similar to, but
more powerful6 than, Collins Model 1 (Collins, 1999).
PCFG A raw PCFG.
TREELET-TRANS A PCFG estimated on the trees after
the transformations of Section 3.
TREELET-RULE The TREELET-TRANS model with the
parent rule context described in Section 2. This is equiv-
alent to the full TREELET model without the lexical con-
text described in Section 2.1.
6Specifically, like Collins Model 1, we generate a rule yield
conditioned on parent symbol P and head word h by first gen-
erating its head symbol Ch, then generating the head words and
symbols for left and right modifiers outwards from Ch. Unlike
Model 1, which generates each modifier head and symbol con-
ditioned only on Ch, h, and P , we additionally condition on the
previously generated modifier?s head and symbol and back off
to Model 1.
963
Model Perplexity
PCFG 1772
TREELET-TRANS 722
TREELET-RULE 329
TREELET 198?
PCFG-LA 330**
HEADLEX 299
5-GRAM 207?
Table 2: Perplexity of several generative models on Sec-
tion 0 of the WSJ. The differences between scores marked
with ? are not statistically significant. PCFG-LA (marked
with **) was only trained on the WSJ and Brown corpora
because it does not scale to large amounts of data.
We used the Berkeley LM toolkit (Pauls
and Klein, 2011), which implements Kneser-Ney
smoothing, to estimate all back-off models for both
n-gram and treelet models. To deal with unknown
words, we use the following strategy: after the first
10000 sentences, whenever we see a new word in
our training data, we replace it with a signature7
10% of the time.
Our model outperforms all other generative mod-
els, though the improvement over the n-gram model
is not statistically significant. Note that because we
use a k-best approximation for the sum over trees,
all perplexities (except for PCFG-LA and 5-GRAM)
are pessimistic bounds.
5.3 Classification of Pseudo-Negative Sentences
We make use of three kinds of automatically gener-
ated pseudo-negative sentences previously proposed
in the literature: Okanohara and Tsujii (2007) pro-
posed generating pseudo-negative examples from a
trigram language model; Foster et al (2008) create
?noisy? sentences by automatically inserting a sin-
gle error into grammatical sentences with a script
that randomly deletes, inserts, or misspells a word;
and Och et al (2004) and Cherry and Quirk (2008)
both use the 1-best output of a machine translation
system. Examples of these three types of pseudo-
negative data are shown in Table 3. We evaluate our
model?s ability to distinguish positive from pseudo-
negative data, and compare against generative base-
lines and state-of-the-art discriminative methods.
7We use signatures generated by the Berkeley Parser.
These signatures capture surface features such as capitalization,
presents of digits, and common suffixes. For example, the word
vexing would be replaced with the signature UNK-ing.
Noisy There was were many contributors.
Trigram For years in dealer immediately .
MT we must further steps .
Table 3: Sample pseudo-negative sentences.
We would like to use our model to make grammat-
icality judgements, but as a generative model it can
only provide us with probabilities. Simply thresh-
olding generative probabilities, even with a separate
threshold for each length, has been shown to be very
ineffective for grammaticality judgements, both for
n-gram and syntactic language models (Cherry and
Quirk, 2008; Post, 2011). We used a simple measure
for isolating the syntactic likelihood of a sentence:
we take the log-probability under our model and
subtract the log-probability under a unigram model,
then normalize by the length of the sentence.8 This
measure, which we call the syntactic log-odds ratio
(SLR), is a crude way of ?subtracting out? the se-
mantic component of the generative probability, so
that sentences that use rare words are not penalized
for doing so.
5.3.1 Trigram Classification
To facilitate comparison with previous work, we
used the same negative corpora as Post (2011) for
trigram classification. They randomly selected 50K
train, 3K development, and 3K positive test sen-
tences from the BLLIP corpus, then trained a tri-
gram model on 450K BLLIP sentences and gener-
ated 50K train, 3K development, and 3K negative
sentences. We parsed the 50K positive training ex-
amples of Post (2011) with the Berkeley Parser and
used the resulting treebank to train a treelet language
model. We set an SLR threshold for each model on
the 6K positive and negative development sentences.
Results are shown in Table 4. In addition to our
generative baselines, we show results for the dis-
criminative models reported in Cherry and Quirk
(2008) and Post (2011). The former train a latent
PCFG support vector machine for binary classifica-
tion (LSVM). The latter report results for two bi-
nary classifiers: RERANK uses the reranking fea-
tures of Charniak and Johnson (2005), and TSG uses
8Och et al (2004) also report using a parser probability nor-
malized by the unigram probability (but not length), and did not
find it effective. We assume this is either because the length-
normalization is important, or because their choice of syntactic
language model was poor.
964
Generative
BLLIP 1B
PCFG 81.5 81.8
TREELET-TRANS 87.7 90.1
TREELET-RULE 89.8 94.1
TREELET 88.9 93.3
PCFG-LA 87.1* ?
HEADLEX 87.6 92.0
5-GRAM 67.9 87.5
Discriminative
BLLIP 1B
LSVM 81.42** ?
TSG 89.9 ?
RERANK 93.0 ?
Table 4: Classification accuracy for trigram pseudo-negative
sentences on the BLLIP corpus. The number reported for
PCFG-LA is marked with a * to indicate that this model was
trained on the training section of the WSJ, not the BLLIP cor-
pus. The number reported for LSVM (marked with **) was eval-
uated on a different random split of the BLLIP corpus, and so is
not directly comparable.
indicator features extracted from a tree substitution
grammar derivation of each sentence.
Our TREELET model performs nearly as well as
the TSG method, and substantially outperforms the
LSVM method, though the latter was not tested on
the same random split. Interestingly, the TREELET-
RULE baseline, which removes lexical context from
our model, outperforms the full model. This is likely
because the negative data is largely coherent at the
trigram level (because it was generated from a tri-
gram model), and the full model is much more sen-
sitive to trigram coherence than the TREELET-RULE
model. This also explains the poor performance of
the 5-GRAM model.
We emphasize that the discriminative baselines
are specifically trained to separate trigram text from
natural English, while our model is trained on pos-
itive examples alone. Indeed, the methods in Post
(2011) are simple binary classifiers, and it is not
clear that these models would be properly calibrated
for any other task, such as integration in a decoder.
One of the design goals of our system was that
it be scalable. Unlike some of the discriminative
baselines, which require expensive operations9 on
9It is true that in order train our system, one must parse large
amounts of training data, which can be costly, though it only
needs to be done once. In contrast, even with observed train-
ing trees, the discriminative algorithms must still iteratively per-
form expensive operations (like parsing) for each sentence, and
a new model must be trained for new types of negative data.
Model Pairwise Independent
WSJ 1B WSJ 1B
PCFG 79.1 77.0 58.9 58.6
TREELET-RULE 90.3 94.4 63.8 66.2
TREELET 90.7 94.5 63.4 65.5
5-GRAM 86.3 93.5 55.7 60.1
HEADLEX 90.7 94.0 59.5 62.0
PCFG-LA 91.3 ? 59.7 ?
Foster et al (2008) ? ? 65.9 ?
Table 5: Classification accuracies on the noisy WSJ for mod-
els trained on WSJ Sections 2-21 and our 1B token corpus.
?Pairwise? accuracy is the fraction of correct sentences whose
SLR score was higher than its noisy version, and ?independent?
refers to standard binary classification accuracy.
each training sentence, we can very easily scale
our model to much larger amounts of data. In Ta-
ble 4, we also show the performance of the gener-
ative models trained on our 1B corpus. All gener-
ative models improve, but TREELET-RULE remains
the best, now outperforming the RERANK system,
though of course it is likely that RERANK would im-
prove if it could be scaled up to more training data.
5.3.2 ?Noisy? Classification
We also evaluate the performance of our model
on the task of distinguishing the noisy WSJ sen-
tences of Foster et al (2008) from their original
versions. We use the noisy versions of Section 0
and 23 produced by their error-generating proce-
dure. Because they only report classification re-
sults on Section 0, we used Section 23 to tune an
SLR threshold, and tested our model on Section 0.
We show the results of both independent and pair-
wise classification for the WSJ and 1B training sets
in Table 5. Note that independent classification is
much more difficult than for the trigram data, be-
cause sentences contain at most one change, which
may not even result in an ungrammaticality. Again,
our model outperforms the n-gram model for both
types of classification, and achieves the same per-
formance as the discriminative system of Foster et
al. (2008), which is state-of-the-art for this data set.
The TREELET-RULE system again slightly outper-
forms the full TREELET model at independent clas-
sification, though not at pairwise classification. This
probably reflects the fact that semantic coherence
can still influence the SLR score, despite our efforts
to subtract it out. Because the TREELET model in-
cludes lexical context, it is more sensitive to seman-
965
French German Chinese
5-GRAM 44.8 37.8 60.0
TREELET 57.9 66.0 83.8
Table 6: Pairwise comparison accuracy of MT output
against a reference translation for French, German, and
Chinese. The BLEU scores for these outputs are 32.7,
27.8, and 20.8. This task becomes easier, at least for our
TREELET model, as translation quality drops. Cherry and
Quirk (2008) report an accuracy of 71.9% on a similar
experiment with German a source language, though the
translation system and training data were different so the
numbers are not comparable. In particular, their transla-
tions had a lower BLEU score, making their task easier.
tic coherence and thus more likely to misclassify
semantically coherent but ungrammatical sentences.
For pairwise comparisons, where semantic coher-
ence is effectively held constant, such sentences are
not problematic.
5.3.3 Machine Translation Classification
We follow Och et al (2004) and Cherry and Quirk
(2008) in evaluating our language models on their
ability to distinguish the 1-best output of a machine
translation system from a reference translation in a
pairwise fashion. Unfortunately, we do not have
access to the data used in those papers, so a di-
rect comparison is not possible. Instead, we col-
lected the English output of Moses (Hoang et al,
2007), using both French and German as source lan-
guage, trained on the Europarl corpus used by WMT
2009.10 We also collected the output of Joshua (Li
et al, 2009) trained on 500K sentences of GALE
Chinese-English parallel newswire. We trained both
our TREELET model and a 5-GRAM model on the
union of our 1B corpus and the English sides of our
parallel corpora.
In Table 6, we show the pairwise comparison ac-
curacy (using SLR) on these three corpora. We see
that our system prefers the reference much more of-
ten than the 5-GRAM language model.11 However,
we also note that the easiness of the task is corre-
lated with the quality of translations (as measured in
BLEU score). This is not surprising ? high-quality
translations are often grammatical and even a per-
10http://www.statmt.org/wmt09
11We note that the n-gram language model used by the MT
system was much smaller than the 5-GRAM model, as they were
only trained on the English sides of their parallel data.
fect language model might not be able to differenti-
ate such translations from their references.
5.4 Machine Translation Fluency
We also carried out reranking experiments on 1000-
best lists from Moses using our syntactic language
model as a feature. We did not find that the use
of our syntactic language model made any statis-
tically significant increases in BLEU score. How-
ever, we noticed in general that the translations fa-
vored by our model were more fluent, a useful im-
provement to which BLEU is often insensitive. To
confirm this, we carried out an Amazon Mechan-
ical Turk experiment where users from the United
States were asked to compare translations using our
TREELET language model as the language model
feature to those using the 5-GRAM model.12 We had
1000 such translation pairs rated by 4 separate Turk-
ers each. Although these two hypothesis sets had
the same BLEU score (up to statistical significance),
the Turkers preferred the output obtained using our
syntactic language model 59% of the time, indicat-
ing that our model had managed to pick out more
fluent hypotheses that nonetheless were of the same
BLEU score. This result was statistically significant
with p < 0.001 using bootstrap resampling.
6 Conclusion
We have presented a simple syntactic language
model that can be estimated using standard n-gram
smoothing techniques on large amounts of data. Our
model outperforms generative baselines on several
evaluation metrics and achieves the same perfor-
mance as state-of-the-art discriminative classifiers
specifically trained on several types of negative data.
Acknowledgments
We would like to thank David Hall for some modeling
suggestions and the anonymous reviewers for their com-
ments. We thank both Matt Post and Jennifer Foster for
providing us with their corpora. This work was partially
supported by a Google Fellowship to the first author and
by BBN under DARPA contract HR0011-12-C-0014.
12We used translations from the baseline Moses system of
Section 5.3.3 with German as the input language. For each lan-
guage model, we took k-best lists from the baseline system and
replaced the baseline LM score with the new model?s score. We
then retrained all feature weights with MERT on the tune set,
and selected the 1-best output on the test set.
966
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, Jeffrey Dean, and Google Inc. 2007. Large lan-
guage models in machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American chapter
of the Association for Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the Association
for Computational Linguistics.
Ciprian Chelba. 1997. A structured language model. In
Proceedings of the Association for Computational Lin-
guistics.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the Association for Computa-
tional Linguistics.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proceedings of The Association for Machine Transla-
tion in the Americas.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of As-
sociation for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jennifer Foster, Joachim Wagner, and Josef van Genabith.
2008. Adapting a wsj-trained parser to grammatically
noisy text. In Proceedings of the Association for Com-
putational Linguistics: Short Paper Track.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics (ACL).
David Graff. 2003. English gigaword, version 3. In Lin-
guistic Data Consortium, Philadelphia, Catalog Num-
ber LDC2003T05.
Keith Hall. 2004. Best-first Word-lattice Parsing: Tech-
niques for Integrated Syntactic Language Modeling.
Ph.D. thesis, Brown University.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
dej Bojar. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics: Demonstra-
tion Session,.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In IEEE
International Conference on Acoustics, Speech and
Signal Processing.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of The Association for Machine
Translation in the Americas.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
In Proceedings of the North American Association for
Computational Linguistic.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the Association for Com-
putational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the Asso-
ciation for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL 2006.
Matt Post and Daniel Gildea. 2009. Language model-
ing with tree substitution grammars. In Proceedings
967
of the Conference on Neural Information Processing
Systems.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar. In Proceedings of the Association
for Computational Linguistics: Short Paper Track.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the Association of
Computational Linguistics.
Brian Roark. 2004. Probabilistic top-down parsing and
language modeling. Computational Linguistics.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2011. A large scale distributed syntactic, semantic
and lexical language model for machine translation.
In Proceedings of the Association for Computational
Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the Association
for Computations Linguistics.
Peng Xu, Ciprian Chelba, and Fred Jelinek. 2002. A
study on richer syntactic dependencies for structured
language modeling. In Proceedings of the Association
for Computational Linguistics. Association for Com-
putational Linguistics.
Ying Zhang. 2009. Structured language models for sta-
tistical machine translation. Ph.D. thesis, Johns Hop-
kins University.
968
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105?109,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Robust Conversion of CCG Derivations to Phrase Structure Trees
Jonathan K. Kummerfeld? Dan Klein? James R. Curran?
?Computer Science Division ? e-lab, School of IT
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,klein}@cs.berkeley.edu james@it.usyd.edu.au
Abstract
We propose an improved, bottom-up method
for converting CCG derivations into PTB-style
phrase structure trees. In contrast with past
work (Clark and Curran, 2009), which used
simple transductions on category pairs, our ap-
proach uses richer transductions attached to
single categories. Our conversion preserves
more sentences under round-trip conversion
(51.1% vs. 39.6%) and is more robust. In par-
ticular, unlike past methods, ours does not re-
quire ad-hoc rules over non-local features, and
so can be easily integrated into a parser.
1 Introduction
Converting the Penn Treebank (PTB, Marcus et al,
1993) to other formalisms, such as HPSG (Miyao
et al, 2004), LFG (Cahill et al, 2008), LTAG (Xia,
1999), and CCG (Hockenmaier, 2003), is a com-
plex process that renders linguistic phenomena in
formalism-specific ways. Tools for reversing these
conversions are desirable for downstream parser use
and parser comparison. However, reversing conver-
sions is difficult, as corpus conversions may lose in-
formation or smooth over PTB inconsistencies.
Clark and Curran (2009) developed a CCG to PTB
conversion that treats the CCG derivation as a phrase
structure tree and applies hand-crafted rules to ev-
ery pair of categories that combine in the derivation.
Because their approach does not exploit the gener-
alisations inherent in the CCG formalism, they must
resort to ad-hoc rules over non-local features of the
CCG constituents being combined (when a fixed pair
of CCG categories correspond to multiple PTB struc-
tures). Even with such rules, they correctly convert
only 39.6% of gold CCGbank derivations.
Our conversion assigns a set of bracket instruc-
tions to each word based on its CCG category, then
follows the CCG derivation, applying and combin-
ing instructions at each combinatory step to build a
phrase structure tree. This requires specific instruc-
tions for each category (not all pairs), and generic
operations for each combinator. We cover all cate-
gories in the development set and correctly convert
51.1% of sentences. Unlike Clark and Curran?s ap-
proach, we require no rules that consider non-local
features of constituents, which enables the possibil-
ity of simple integration with a CKY-based parser.
The most common errors our approach makes in-
volve nodes for clauses and rare spans such as QPs,
NXs, and NACs. Many of these errors are inconsis-
tencies in the original PTB annotations that are not
recoverable. These issues make evaluating parser
output difficult, but our method does enable an im-
proved comparison of CCG and PTB parsers.
2 Background
There has been extensive work on converting parser
output for evaluation, e.g. Lin (1998) and Briscoe et
al. (2002) proposed using underlying dependencies
for evaluation. There has also been work on conver-
sion to phrase structure, from dependencies (Xia and
Palmer, 2001; Xia et al, 2009) and from lexicalised
formalisms, e.g. HPSG (Matsuzaki and Tsujii, 2008)
and TAG (Chiang, 2000; Sarkar, 2001). Our focus is
on CCG to PTB conversion (Clark and Curran, 2009).
2.1 Combinatory Categorial Grammar (CCG)
The lower half of Figure 1 shows a CCG derivation
(Steedman, 2000) in which each word is assigned a
category, and combinatory rules are applied to ad-
jacent categories until only one remains. Categories
105
JJ NNS
PRP$ NN DT NN
NP NP
VBD S
NP VP
S
Italian magistrates labeled his death a suicide
N /N N ((S [dcl ]\NP)/NP)/NP NP [nb]/N N NP [nb]/N N
> > >
N NP NP
>
NP (S [dcl ]\NP)/NP
>
S [dcl ]\NP
<
S [dcl ]
Figure 1: A crossing constituents example: his . . . suicide
(PTB) crosses labeled . . . death (CCGbank).
Categories Schema
N create an NP
((S [dcl ]\NP)/NP)/NP create a VP
N /N + N place left under right
NP [nb]/N + N place left under right
((S [dcl ]\NP)/NP)/NP + NP place right under left
(S [dcl ]\NP)/NP + NP place right under left
NP + S [dcl ]\NP place both under S
Table 1: Example C&C-CONV lexical and rule schemas.
can be atomic, e.g. the N assigned to magistrates,
or complex functions of the form result / arg, where
result and arg are categories and the slash indicates
the argument?s directionality. Combinators define
how adjacent categories can combine. Figure 1 uses
function application, where a complex category con-
sumes an adjacent argument to form its result, e.g.
S [dcl ]\NP combines with the NP to its left to form
an S [dcl ]. More powerful combinators allow cate-
gories to combine with greater flexibility.
We cannot form a PTB tree by simply relabeling
the categories in a CCG derivation because the map-
ping to phrase labels is many-to-many, CCG deriva-
tions contain extra brackets due to binarisation, and
there are cases where the constituents in the PTB tree
and the CCG derivation cross (e.g. in Figure 1).
2.2 Clark and Curran (2009)
Clark and Curran (2009), hereafter C&C-CONV, as-
sign a schema to each leaf (lexical category) and rule
(pair of combining categories) in the CCG derivation.
The PTB tree is constructed from the CCG bottom-
up, creating leaves with lexical schemas, then merg-
ing/adding sub-trees using rule schemas at each step.
The schemas for Figure 1 are shown in Table 1.
These apply to create NPs over magistrates, death,
and suicide, and a VP over labeled, and then com-
bine the trees by placing one under the other at each
step, and finally create an S node at the root.
C&C-CONV has sparsity problems, requiring
schemas for all valid pairs of categories ? at a
minimum, the 2853 unique category combinations
found in CCGbank. Clark and Curran (2009) create
schemas for only 776 of these, handling the remain-
der with approximate catch-all rules.
C&C-CONV only specifies one simple schema for
each rule (pair of categories). This appears reason-
able at first, but frequently causes problems, e.g.:
(N /N )/(N /N ) + N /N
?more than? + ?30? (1)
?relatively? + ?small? (2)
Here either a QP bracket (1) or an ADJP bracket
(2) should be created. Since both examples involve
the same rule schema, C&C-CONV would incorrectly
process them in the same way. To combat the most
glaring errors, C&C-CONV manipulates the PTB tree
with ad-hoc rules based on non-local features over
the CCG nodes being combined ? an approach that
cannot be easily integrated into a parser.
These disadvantages are a consequence of failing
to exploit the generalisations that CCG combinators
define. We return to this example below to show how
our approach handles both cases correctly.
3 Our Approach
Our conversion assigns a set of instructions to each
lexical category and defines generic operations for
each combinator that combine instructions. Figure 2
shows a typical instruction, which specifies the node
to create and where to place the PTB trees associated
with the two categories combining. More complex
operations are shown in Table 2. Categories with
multiple arguments are assigned one instruction per
argument, e.g. labeled has three. These are applied
one at a time, as each combinatory step occurs.
For the example from the previous section we be-
gin by assigning the instructions shown in Table 3.
Some of these can apply immediately as they do not
involve an argument, e.g. magistrates has (NP f).
One of the more complex cases in the example is
Italian, which is assigned (NP f {a}). This creates
a new bracket, inserts the functor?s tree, and flattens
and inserts the argument?s tree, producing:
(NP (JJ Italian) (NNS magistrates))
106
((S\NP)/NP)/NP NP
f a
(S\NP)/NP
f a
VP
Figure 2: An example function application. Top row:
CCG rule. Bottom row: applying instruction (VP f a).
Symbol Meaning Example
(X f a) Add an X bracket around (VP f a)
functor and argument
{ } Flatten enclosed node (N f {a})
X* Use same label as arg. (S* f {a})
or default to X
fi Place subtrees (PP f0 (S f1..k a))
Table 2: Types of operations in instructions.
For the complete example the final tree is almost
correct but omits the S bracket around the final two
NPs. To fix our example we could have modified our
instructions to use the final symbol in Table 2. The
subscripts indicate which subtrees to place where.
However, for this particular construction the PTB an-
notations are inconsistent, and so we cannot recover
without introducing more errors elsewhere.
For combinators other than function application,
we combine the instructions in various ways. Ad-
ditionally, we vary the instructions assigned based
on the POS tag in 32 cases, and for the word not,
to recover distinctions not captured by CCGbank
categories alone. In 52 cases the later instruc-
tions depend on the structure of the argument being
picked up. We have sixteen special cases for non-
combinatory binary rules and twelve special cases
for non-combinatory unary rules.
Our approach naturally handles our QP vs. ADJP
example because the two cases have different lexical
categories: ((N /N )/(N /N ))\(S [adj ]\NP) on than
and (N /N )/(N /N ) on relatively. This lexical dif-
ference means we can assign different instructions to
correctly recover the QP and ADJP nodes, whereas
C&C-CONV applies the same schema in both cases
as the categories combining are the same.
4 Evaluation
Using sections 00-21 of the treebanks, we hand-
crafted instructions for 527 lexical categories, a pro-
cess that took under 100 hours, and includes all the
categories used by the C&C parser. There are 647
further categories and 35 non-combinatory binary
rules in sections 00-21 that we did not annotate. For
Category Instruction set
N (NP f)
N /N1 (NP f {a})
NP [nb]/N1 (NP f {a})
((S [dcl ]\NP3 )/NP2 )/NP1 (VP f a)
(VP {f} a)
(S a f)
Table 3: Instruction sets for the categories in Figure 1.
System Data P R F Sent.
00 (all) 95.37 93.67 94.51 39.6
C&C 00 (len ? 40) 95.85 94.39 95.12 42.1
CONV 23 (all) 95.33 93.95 94.64 39.7
23 (len ? 40) 95.44 94.04 94.73 41.9
00 (all) 96.69 96.58 96.63 51.1
This 00 (len ? 40) 96.98 96.77 96.87 53.6
Work 23 (all) 96.49 96.11 96.30 51.4
23 (len ? 40) 96.57 96.21 96.39 53.8
Table 4: PARSEVAL Precision, Recall, F-Score, and exact
sentence match for converted gold CCG derivations.
unannotated categories, we use the instructions of
the result category with an added instruction.
Table 4 compares our approach with C&C-CONV
on gold CCG derivations. The results shown are as
reported by EVALB (Abney et al, 1991) using the
Collins (1997) parameters. Our approach leads to in-
creases on all metrics of at least 1.1%, and increases
exact sentence match by over 11% (both absolute).
Many of the remaining errors relate to missing
and extra clause nodes and a range of rare structures,
such as QPs, NACs, and NXs. The only other promi-
nent errors are single word spans, e.g. extra or miss-
ing ADVPs. Many of these errors are unrecover-
able from CCGbank, either because inconsistencies
in the PTB have been smoothed over or because they
are genuine but rare constructions that were lost.
4.1 Parser Comparison
When we convert the output of a CCG parser, the PTB
trees that are produced will contain errors created by
our conversion as well as by the parser. In this sec-
tion we are interested in comparing parsers, so we
need to factor out errors created by our conversion.
One way to do this is to calculate a projected score
(PROJ), as the parser result over the oracle result, but
this is a very rough approximation. Another way is
to evaluate only on the 51% of sentences for which
our conversion from gold CCG derivations is perfect
(CLEAN). However, even on this set our conversion
107
0
20
40
60
80
100
0 20 40 60 80 100C
o
nv
er
te
d
C&
C,
EV
A
LB
Converted Gold, EVALB
0
20
40
60
80
100
0 20 40 60 80 100
N
at
iv
e
C&
C,
ld
ep
s
Converted Gold, EVALB
Figure 3: For each sentence in the treebank, we plot
the converted parser output against gold conversion (left),
and the original parser evaluation against gold conversion
(right). Left: Most points lie below the diagonal, indicat-
ing that the quality of converted parser output (y) is upper
bounded by the quality of conversion on gold parses (x).
Right: No clear correlation is present, indicating that the
set of sentences that are converted best (on the far right),
are not necessarily easy to parse.
introduces errors, as the parser output may contain
categories that are harder to convert.
Parser F-scores are generally higher on CLEAN,
which could mean that this set is easier to parse, or it
could mean that these sentences don?t contain anno-
tation inconsistencies, and so the parsers aren?t in-
correct for returning the true parse (as opposed to
the one in the PTB). To test this distinction we look
for correlation between conversion quality and parse
difficulty on another metric. In particular, Figure 3
(right) shows CCG labeled dependency performance
for the C&C parser vs. CCGbank conversion PARSE-
VAL scores. The lack of a strong correlation, and the
spread on the line x = 100, supports the theory that
these sentences are not necessarily easier to parse,
but rather have fewer annotation inconsistencies.
In the left plot, the y-axis is PARSEVAL on con-
verted C&C parser output. Conversion quality essen-
tially bounds the performance of the parser. The few
points above the diagonal are mostly short sentences
on which the C&C parser uses categories that lead
to one extra correct node. The main constructions
on which parse errors occur, e.g. PP attachment, are
rarely converted incorrectly, and so we expect the
number of errors to be cumulative. Some sentences
are higher in the right plot than the left because there
are distinctions in CCG that are not always present in
the PTB, e.g. the argument-adjunct distinction.
Table 5 presents F-scores for three PTB parsers
and three CCG parsers (with their output converted
by our method). One interesting comparison is be-
tween the PTB parser of Petrov and Klein (2007) and
Sentences CLEAN ALL PROJ
Converted gold CCG
CCGbank 100.0 96.3 ?
Converted CCG
Clark and Curran (2007) 90.9 85.5 88.8
Fowler and Penn (2010) 90.9 86.0 89.3
Auli and Lopez (2011) 91.7 86.2 89.5
Native PTB
Klein and Manning (2003) 89.8 85.8 ?
Petrov and Klein (2007) 93.6 90.1 ?
Charniak and Johnson (2005) 94.8 91.5 ?
Table 5: F-scores on section 23 for PTB parsers and
CCG parsers with their output converted by our method.
CLEAN is only on sentences that are converted perfectly
from gold CCG (51%). ALL is over all sentences. PROJ is
a projected F-score (ALL result / CCGbank ALL result).
the CCG parser of Fowler and Penn (2010), which
use the same underlying parser. The performance
gap is partly due to structures in the PTB that are not
recoverable from CCGbank, but probably also indi-
cates that the split-merge model is less effective in
CCG, which has far more symbols than the PTB.
It is difficult to make conclusive claims about
the performance of the parsers. As shown earlier,
CLEAN does not completely factor out the errors in-
troduced by our conversion, as the parser output may
be more difficult to convert, and the calculation of
PROJ only roughly factors out the errors. However,
the results do suggest that the performance of the
CCG parsers is approaching that of the Petrov parser.
5 Conclusion
By exploiting the generalised combinators of the
CCG formalism, we have developed a new method
of converting CCG derivations into PTB-style trees.
Our system, which is publicly available1 , is more
effective than previous work, increasing exact sen-
tence match by more than 11% (absolute), and can
be directly integrated with a CCG parser.
Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research was
supported by a General Sir John Monash Fellow-
ship, the Office of Naval Research under MURI
Grant No. N000140911081, ARC Discovery grant
DP1097291, and the Capital Markets CRC.
1http://code.google.com/p/berkeley-ccg2pst/
108
References
S. Abney, S. Flickenger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, M. Marcus, S. Roukos, B. San-
torini, and T. Strzalkowski. 1991. Procedure for quan-
titatively comparing the syntactic coverage of english
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306?311.
Michael Auli and Adam Lopez. 2011. A comparison of
loopy belief propagation and dual decomposition for
integrated ccg supertagging and parsing. In Proceed-
ings of ACL, pages 470?480.
Ted Briscoe, John Carroll, Jonathan Graham, and Ann
Copestake. 2002. Relational evaluation schemes. In
Proceedings of the Beyond PARSEVAL Workshop at
LREC, pages 4?8.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-coverage deep statistical parsing using auto-
matic dependency structure annotation. Computa-
tional Linguistics, 34(1):81?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL, pages 456?463.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of CCG and penn treebank parsers. In
Proceedings of ACL, pages 53?56.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL,
pages 16?23.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proceedings of ACL, pages 335?344.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, School of Informatics, The University of
Edinburgh.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Dekang Lin. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 4(2):97?114.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313?330.
Takuya Matsuzaki and Jun?ichi Tsujii. 2008. Com-
parative parser performance analysis across grammar
frameworks through automatic tree conversion using
synchronous grammars. In Proceedings of Coling,
pages 545?552.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a head-driven phrase structure grammar from
the penn treebank. In Proceedings of IJCNLP, pages
684?693.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL,
pages 404?411.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL, pages
1?8.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
of HLT, pages 1?5.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories, pages 159?170.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the Natural Lan-
guage Processing Pacific Rim Symposium, pages 398?
403.
109
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 114?124,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decentralized Entity-Level Modeling for Coreference Resolution
Greg Durrett, David Hall, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,dlwh,klein}@cs.berkeley.edu
Abstract
Efficiently incorporating entity-level in-
formation is a challenge for coreference
resolution systems due to the difficulty of
exact inference over partitions. We de-
scribe an end-to-end discriminative prob-
abilistic model for coreference that, along
with standard pairwise features, enforces
structural agreement constraints between
specified properties of coreferent men-
tions. This model can be represented as
a factor graph for each document that ad-
mits efficient inference via belief propaga-
tion. We show that our method can use
entity-level information to outperform a
basic pairwise system.
1 Introduction
The inclusion of entity-level features has been a
driving force behind the development of many
coreference resolution systems (Luo et al, 2004;
Rahman and Ng, 2009; Haghighi and Klein, 2010;
Lee et al, 2011). There is no polynomial-time dy-
namic program for inference in a model with ar-
bitrary entity-level features, so systems that use
such features typically rely on making decisions
in a pipelined manner and sticking with them, op-
erating greedily in a left-to-right fashion (Rahman
and Ng, 2009) or in a multi-pass, sieve-like man-
ner (Raghunathan et al, 2010). However, such
systems may be locked into bad coreference deci-
sions and are difficult to directly optimize for stan-
dard evaluation metrics.
In this work, we present a new structured model
of entity-level information designed to allow effi-
cient inference. We use a log-linear model that can
be expressed as a factor graph. Pairwise features
appear in the model as unary factors, adjacent
to nodes representing a choice of antecedent (or
none) for each mention. Additional nodes model
entity-level properties on a per-mention basis, and
structural agreement factors softly drive properties
of coreferent mentions to agree with one another.
This is a key feature of our model: mentions man-
age their partial membership in various corefer-
ence chains, so that information about entity-level
properties is decentralized and propagated across
individual mentions, and we never need to explic-
itly instantiate entities.
Exact inference in this factor graph is in-
tractable, but efficient approximate inference can
be carried out with belief propagation. Our model
is the first discriminatively-trained model that both
makes joint decisions over an entire document and
models specific entity-level properties, rather than
simply enforcing transitivity of pairwise decisions
(Finkel and Manning, 2008; Song et al, 2012).
We evaluate our system on the dataset from
the CoNLL 2011 shared task using three differ-
ent types of properties: synthetic oracle proper-
ties, entity phi features (number, gender, animacy,
and NER type), and properties derived from un-
supervised clusters targeting semantic type infor-
mation. In all cases, our transitive model of en-
tity properties equals or outperforms our pairwise
system and our reimplementation of a previous
entity-level system (Rahman and Ng, 2009). Our
final system is competitive with the winner of the
CoNLL 2011 shared task (Lee et al, 2011).
2 Example
We begin with an example motivating our use of
entity-level features. Consider the following ex-
cerpt concerning two famous auction houses:
When looking for [art items], [people] go
to [Sotheby?s and Christie?s] because [they]A
believe [they]B can get the best price for
[them].
The first three mentions are all distinct entities,
theyA and theyB refer to people, and them refers to
art items. The three pronouns are tricky to resolve
114
automatically because they could at first glance re-
solve to any of the preceding mentions. We focus
in particular on the resolution of theyA and them.
In order to correctly resolve theyA to people rather
than Sotheby?s and Christie?s, we must take ad-
vantage of the fact that theyA appears as the sub-
ject of the verb believe, which is much more likely
to be attributed to people than to auction houses.
Binding principles prevent them from attaching
to theyB. But how do we prevent it from choos-
ing as its antecedent the next closest agreeing pro-
noun, theyA? One way is to exploit the correct
coreference decision we have already made, theyA
referring to people, since people are not as likely
to have a price as art items are. This observa-
tion argues for enforcing agreement of entity-level
semantic properties during inference, specifically
properties relating to permitted semantic roles.
Because even these six mentions have hundreds
of potential partitions into coreference chains, we
cannot search over partitions exhaustively, and
therefore we must design our model to be able to
use this information while still admitting an effi-
cient inference scheme.
3 Models
We will first present our BASIC model (Sec-
tion 3.1) and describe the features it incorporates
(Section 3.2), then explain how to extend it to use
transitive features (Sections 3.3 and 3.4).
Throughout this section, let x be a variable con-
taining the words in a document along with any
relevant precomputed annotation (such as parse in-
formation, semantic roles, etc.), and let n denote
the number of mentions in a given document.
3.1 BASIC Model
Our BASIC model is depicted in Figure 1 in stan-
dard factor graph notation. Each mention i has
an associated random variable ai taking values in
the set {1, . . . , i?1, <new>}; this variable spec-
ifies mention i?s selected antecedent or indicates
that it begins a new coreference chain. Let a =
(a1, ..., an) be the vector of the ai. Note that a set
of coreference chains C (the final desired output)
can be uniquely determined from a, but a is not
uniquely determined by C.
We use a log linear model of the conditional dis-
tribution P (a|x) as follows:
P (a|x) ? exp
( n?
i=1
wT fA(i, ai, x)
)
When looking for [art items], [people] go to [Sotheby's 
and Christie's] because [they]
A
 believe [they]
B
 can get 
the best price for [them].
art items 0.15
people 0.4
Sotheby?s and 
Christie?s
0.4
<new> 0.05
a
2
a
3
a
4
a
1
A
1
A
2
A
3
A
4
art items 0.05
<new> 0.95
antecedent 
choices
antecedent 
factors
}
}
Figure 1: Our BASIC coreference model. A de-
cision ai is made independently for each men-
tion about what its antecedent mention should
be or whether it should start a new coreference
chain. Each unary factor Ai has a log-linear form
with features examining mention i, its selected an-
tecedent ai, and the document context x.
where fA(i, ai, x) is a feature function that exam-
ines the coreference decision ai for mention i with
document context x; note that this feature function
can include pairwise features based on mention i
and the chosen antecedent ai, since information
about each mention is contained in x.
Because the model factors completely over the
individual ai, these feature functions fA can be ex-
pressed as unary factors Ai (see Figure 1), with
Ai(j) ? exp
(
wT fA(i, j, x)
). Given a setting of
w, we can determine a? = argmaxa P (a|x) and
then deterministically compute C(a), the final set
of coreference chains.
While the features of this model factor over
coreference links, this approach differs from clas-
sical pairwise systems such as Bengtson and Roth
(2008) or Stoyanov et al (2010). Because poten-
tial antecedents compete with each other and with
the non-anaphoric hypothesis, the choice of ai ac-
tually represents a joint decision about i?1 pair-
wise links, as opposed to systems that use a pair-
wise binary classifier and a separate agglomera-
tion step, which consider one link at a time during
learning. This approach is similar to the mention-
ranking model of Rahman and Ng (2009).
3.2 Pairwise Features
We now present the set of features fA used by our
unary factors Ai. Each feature examines the an-
115
tecedent choice ai of the current mention as well
as the observed information x in the document.
For each of the features we present, two conjoined
versions are included: one with an indicator of the
type of the current mention being resolved, and
one with an indicator of the types of the current
and antecedent mentions. Mention types are either
NOMINAL, PROPER, or, if the mention is pronom-
inal, a canonicalized version of the pronoun ab-
stracting away case.1
Several features, especially those based on the
precise constructs (apposition, etc.) and those in-
corporating phi feature information, are computed
using the machinery in Lee et al (2011). Other
features were inspired by Song et al (2012) and
Rahman and Ng (2009).
Anaphoricity features: Indicator of anaphoric-
ity, indicator on definiteness.
Configurational features: Indicator on distance
in mentions (capped at 10), indicator on dis-
tance in sentences (capped at 10), does the an-
tecedent c-command the current mention, are the
two mentions in a subject/object construction, are
the mentions nested, are the mentions in determin-
istic appositive/role appositive/predicate nomina-
tive/relative pronoun constructions.
Match features: Is one mention an acronym of
the other, head match, head contained (each way),
string match, string contained (each way), relaxed
head match features from Lee et al (2011).
Agreement features: Gender, number, ani-
macy, and NER type of the current mention and
the antecedent (separately and conjoined).
Discourse features: Speaker match conjoined
with an indicator of whether the document is an
article or conversation.
Because we use conjunctions of these base fea-
tures together with the antecedent and mention
type, our system can capture many relationships
that previous systems hand-coded, especially re-
garding pronouns. For example, our system has
access to features such as ?it is non-anaphoric?,
?it has as its antecedent a geopolitical entity?, or
?I has as its antecedent I with the same speaker.?
1While this canonicalization could theoretically impair
our ability to resolve, for example, reflexive pronouns, con-
joining features with raw pronoun strings does not improve
performance.
We experimented with synonymy and hyper-
nymy features from WordNet (Miller, 1995), but
these did not empirically improve performance.
3.3 TRANSITIVE Model
The BASIC model can capture many relationships
between pairs of mentions, but cannot necessarily
capture entity-level properties like those discussed
in Section 2. We could of course model entities
directly (Luo et al, 2004; Rahman and Ng, 2009),
saying that each mention refers to some prior en-
tity rather than to some prior mention. However,
inference in this model would require reasoning
about all possible partitions of mentions, which is
computationally infeasible without resorting to se-
vere approximations like a left-to-right inference
method (Rahman and Ng, 2009).
Instead, we would like to try to preserve the
tractability of the BASIC model while still being
able to exploit entity-level information. To do so,
we will allow each mention to maintain its own
distributions over values for a number of proper-
ties; these properties could include gender, named-
entity type, or semantic class. Then, we will re-
quire each anaphoric mention to agree with its an-
tecedent on the value of each of these properties.
Our TRANSITIVE model which implements this
scheme is shown in Figure 2. Each mention i
has been augmented with a single property node
pi ? {1, ..., k}. The unary Pi factors encode prior
knowledge about the setting of each pi; these fac-
tors may be hard (I will not refer to a plural entity),
soft (such as a distribution over named entity types
output by an NER tagger), or practically uniform
(e.g. the last name Smith does not specify a partic-
ular gender).
To enforce agreement of a particular property,
we require a mention to have the same property
value as its antecedent. That is, for mentions i and
j, if ai = j, we want to ensure that pi and pj
agree. We can achieve this with the following set
of structural equality factors:
Ei?j(ai, pi, pj) = 1? I[ai = j ? pi 6= pj ]
In words, this factor is zero if both ai = j and
pi disagrees with pj . These equality factors es-
sentially provide a mechanism by which these pri-
ors Pi can influence the coreference decisions: if,
for example, the factors Pi and Pj disagree very
strongly, choosing ai 6= j will be preferred in or-
der to avoid forcing one of pi or pj to take an un-
desirable value. Moreover, note that although ai
116
E4-3
a
2
a
4
p
4
p
3
p
2
E
4-2
A
2
A
3
A
4
P
2
P
3
P
4
antecedent 
choices
antecedent 
factors
property 
factors
properties
equality 
factors
a
3
}
}
}
}
}
people
Sotheby's
and Christie's
they
Figure 2: The factor graph for our TRANSI-
TIVE coreference model. Each node ai now has
a property pi, which is informed by its own unary
factor Pi. In our example, a4 strongly indicates
that mentions 2 and 4 are coreferent; the factor
E4?2 then enforces equality between p2 and p4,
while the factor E4?3 has no effect.
only indicates a single antecedent, the transitive
nature of the E factors forces pi to agree with the
p nodes of all other mentions likely to be in the
same entity.
3.4 Property Projection
So far, our model as specified ensures agreement
of our entity-level properties, but strictly enforc-
ing agreement may not always be correct. Suppose
that we are using named entity type as an entity-
level property. Organizations and geo-political en-
tities are two frequently confused and ambiguous
tags, and in the gold-standard coreference chains
it may be the case that a single chain contains in-
stances of both. We might wish to learn that or-
ganizations and geo-political entities are ?compat-
ible? in the sense that we should forgive entities
for containing both, but without losing the ability
to reject a chain containing both organizations and
people, for example.
To address these effects, we expand our model
as indicated in Figure 3. As before, we have a
set of properties pi and agreement factors Eij . On
top of that, we introduce the notion of raw prop-
erty values ri ? {1, ..., k} together with priors in
the form of the Ri factors. The ri and pi could in
principle have different domains, but for this work
we take them to have the same domain. The Pi
factors now have a new structure: they now rep-
resent a featurized projection of the ri onto the
pi, which can now be thought of as ?coreference-
p
4p
3
p
2
r
4
r
3
r
2
P
2
P
3
P
4
R
2
R
3
R
4
raw property 
factors
raw properties
projection 
factors
projected 
properties
}
}
}
}
a
2
a
4
A
2
A
3
A
4
a
3
E
3-1
E
4-1
Figure 3: The complete factor graph for our
TRANSITIVE coreference model. Compared to
Figure 2, the Ri contain the raw cluster posteriors,
and the Pi factors now project raw cluster values ri
into a set of ?coreference-adapted? clusters pi that
are used as before. This projection allows men-
tions with different but compatible raw property
values to coexist in the same coreference chain.
adapted? properties. The Pi factors are defined by
Pi(pi, ri) ? exp(wT fP (pi, ri)), where fP is a fea-
ture vector over the projection of ri onto pi. While
there are many possible choices of fP , we choose
it to be an indicator of the values of pi and ri, so
that we learn a fully-parameterized projection ma-
trix.2 The Ri are constant factors, and may come
from an upstream model or some other source de-
pending on the property being modeled.
Our description thus far has assumed that we
are modeling only one type of property. In fact,
we can use multiple properties for each mention
by duplicating the r and p nodes and the R, P ,
and E factors across each desired property. We
index each of these by l ? {1, . . . ,m} for each of
m properties.
The final log-linear model is given by the fol-
lowing formula:
P (a|x) ?
?
p,r
?
?
?
??
i,j,l
El,i?j(ai, pli, plj)
?
?
?
??
i,l
Rli(rli)
?
?
exp
(
wT
?
i
(
fA(i, ai, x) +
?
l
fP (pli, rli)
))]
where i and j range over mentions, l ranges over
2Initialized to zero (or small values), this matrix actually
causes the transitive machinery to have no effect, since all
posteriors over the pi are flat and completely uninformative.
Therefore, we regularize the weights of the indicators of pi =
ri towards 1 and all other features towards 0 to give each raw
cluster a preference for a distinct projected cluster.
117
each of m properties, and the outer sum indicates
marginalization over all p and r variables.
4 Learning
Now that we have defined our model, we must
decide how to train its weights w. The first
issue to address is one of the supervision pro-
vided. Our model traffics in sets of labels a
which are more specified than gold coreference
chains C, which give cluster membership for each
mention but not antecedence. Let A(C) be the
set of labelings a that are consistent with a set
of coreference chains C. For example, if C =
{{1, 2, 3}, {4}}, then (<new>, 1, 2, <new>) ?
A(C) and (<new>, 1, 1, <new>) ? A(C) but
(<new>, 1, <new>, 3) /? A(C), since this im-
plies the chains C = {{1, 2}, {3, 4}}
The most natural objective is a variant of
standard conditional log-likelihood that treats the
choice of a for the specified C as a latent variable
to be marginalized out:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P (a|xi)
?
? (1)
where (xi, Ci) is the ith labeled training example.
This optimizes for the 0-1 loss; however, we are
much more interested in optimizing with respect
to a coreference-specific loss function.
To this end, we will use softmax-margin (Gim-
pel and Smith, 2010), which augments the proba-
bility of each example with a term proportional to
its loss, pushing the model to assign less mass to
highly incorrect examples. We modify Equation 1
to use a new probability distribution P ? instead
of P , where P ?(a|xi) ? P (a|xi) exp (l(a,C))
and l(a,C) is a loss function. In order to
perform inference efficiently, l(a,C) must de-
compose linearly across mentions: l(a,C) =?n
i=1 l(ai, C). Commonly-used coreference met-
rics such as MUC (Vilain et al, 1995) and B3
(Bagga and Baldwin, 1998) do not have this prop-
erty, so we instead make use of a parameterized
loss function that does and fit the parameters to
give good performance. Specifically, we take
l(a,C) =
n?
i=1
[c1I(K1(ai, C)) + c2I(K2(ai, C))
+ c3I(K3(ai, C))]
where c1, c2, and c3 are real-valued weights, K1
denotes the event that ai is falsely anaphoric when
it should be non-anaphoric, K2 denotes the event
that ai is falsely non-anaphoric when it should be
anaphoric, and K3 denotes the event that ai is cor-
rectly determined to be anaphoric but . These can
be computed based on only ai and C. By setting
c1 low and c2 high relative to c3, we can force
the system to be less conservative about making
anaphoricity decisions and achieve a better bal-
ance with the final coreference metrics.
Finally, we incorporate L1 regularization, giv-
ing us our final objective:
`(w) =
t?
i=1
log
?
? ?
a?A(Ci)
P ?(a|xi)
?
?+ ??w?1
We optimize this objective using AdaGrad
(Duchi et al, 2011); we found this to be faster and
give higher performance than L-BFGS using L2
regularization (Liu and Nocedal, 1989). Note that
because of the marginalization over A(Ci), even
the objective for the BASIC model is not convex.
5 Inference
Inference in the BASIC model is straightforward.
Given a set of weights w, we can predict
a? = argmax
a
P (a|x)
We then report the corresponding chains C(a)
as the system output.3 For learning, the gradi-
ent takes the standard form of the gradient of a
log-linear model, a difference of expected feature
counts under the gold annotation and under no
annotation. This requires computing marginals
P ?(ai|x) for each mention i, but because the
model already factors this way, this step is easy.
The TRANSITIVE model is more complex. Ex-
act inference is intractable due to theE factors that
couple all of the ai by way of the pi nodes. How-
ever, we can compute approximate marginals for
the ai, pi, and ri using belief propagation. BP has
been effectively used on other NLP tasks (Smith
and Eisner, 2008; Burkett and Klein, 2012), and is
effective in cases such as this where the model is
largely driven by non-loopy factors (here, the Ai).
From marginals over each node, we can com-
pute the necessary gradient and decode as before:
a? = argmax
a
P? (a|x)
3One could use ILP-based decoding in the style of Finkel
and Manning (2008) and Song et al (2012) to attempt to ex-
plicitly find the optimal C with choice of a marginalized out,
but we did not explore this option.
118
This corresponds to minimum-risk decoding with
respect to the Hamming loss over antecedence pre-
dictions.
Pruning. The TRANSITIVE model requires in-
stantiating a factor for each potential setting of
each ai. This factor graph grows quadratically in
the size of the document, and even approximate in-
ference becomes slow when a document contains
over 200 mentions. Therefore, we use our BA-
SIC model to prune antecedent choices for each
ai in order to reduce the size of the factor graph
that we must instantiate. Specifically, we prune
links between pairs of mentions that are of men-
tion distance more than 100, as well as values for
ai that fall below a particular odds ratio threshold
with respect to the best setting of that ai in the
BASIC model; that is, those for which
log
( PBASIC (ai|x)
maxj PBASIC (ai = j|x)
)
is below a cutoff ?.
6 Related Work
Our BASIC model is a mention-ranking approach
resembling models used by Denis and Baldridge
(2008) and Rahman and Ng (2009), though it is
trained using a novel parameterized loss function.
It is also similar to the MLN-JOINT(BF) model
of Song et al (2012), but we enforce the single-
parent constraint at a deeper structural level, al-
lowing us to treat non-anaphoricity symmetrically
with coreference as in Denis and Baldridge (2007)
and Stoyanov and Eisner (2012). The model of
Fernandes et al (2012) also uses the single-parent
constraint structurally, but with learning via la-
tent perceptron and ILP-based one-best decod-
ing rather than logistic regression and BP-based
marginal computation.
Our TRANSITIVE model is novel; while Mc-
Callum and Wellner (2004) proposed the idea of
using attributes for mentions, they do not actu-
ally implement a model that does so. Other sys-
tems include entity-level information via hand-
written rules (Raghunathan et al, 2010), induced
rules (Yang et al, 2008), or features with learned
weights (Luo et al, 2004; Rahman and Ng, 2011),
but all of these systems freeze past coreference de-
cisions in order to compute their entities.
Most similar to our entity-level approach is
the system of Haghighi and Klein (2010), which
also uses approximate global inference; however,
theirs is an unsupervised, generative system and
they attempt to directly model multinomials over
words in each mention. Their system could be ex-
tended to handle property information like we do,
but our system has many other advantages, such as
freedom from a pre-specified list of entity types,
the ability to use multiple input clusterings, and
discriminative projection of clusters.
7 Experiments
We use the datasets, experimental setup, and scor-
ing program from the CoNLL 2011 shared task
(Pradhan et al, 2011), based on the OntoNotes
corpus (Hovy et al, 2006). We use the standard
automatic parses and NER tags for each docu-
ment. Our mentions are those output by the sys-
tem of Lee et al (2011); we also use their postpro-
cessing to remove appositives, predicate nomina-
tives, and singletons before evaluation. For each
experiment, we report MUC (Vilain et al, 1995),
B3 (Bagga and Baldwin, 1998), and CEAFe (Luo,
2005), as well as their average.
Parameter settings. We take the regularization
constant ? = 0.001 and the parameters of our
surrogate loss (c1, c2, c3) = (0.15, 2.5, 1) for all
models.4 All models are trained for 20 iterations.
We take the pruning threshold ? = ?2.
7.1 Systems
Besides our BASIC and TRANSITIVE systems, we
evaluate a strictly pairwise system that incorpo-
rates property information by way of indicator fea-
tures on the current mention?s most likely property
value and the proposed antecedent?s most likely
property value. We call this system PAIRPROP-
ERTY; it is simply the BASIC system with an ex-
panded feature set.
Furthermore, we compare against a LEFT-
TORIGHT entity-level system like that of Rahman
and Ng (2009).5 Decoding now operates in a se-
quential fashion, with BASIC features computed
as before and entity features computed for each
mention based on the coreference decisions made
thus far. Following Rahman and Ng (2009), fea-
tures for each property indicate whether the cur-
4Additional tuning of these hyper parameters did not sig-
nificantly improve any of the models under any of the exper-
imental conditions.
5Unfortunately, their publicly-available system is closed-
source and performs poorly on the CoNLL shared task
dataset, so direct comparison is difficult.
119
rent mention agrees with no mentions in the an-
tecedent cluster, at least one mention, over half of
the mentions, or all of the mentions; antecedent
clusters of size 1 or 2 fire special-cased features.
These additional features beyond those in Rah-
man and Ng (2009) were helpful, but more in-
volved conjunction schemes and fine-grained fea-
tures were not. During training, entity features of
both the gold and the prediction are computed us-
ing the Viterbi clustering of preceding mentions
under the current model parameters.6
All systems are run in a two-pass manner:
first, the BASIC model is run, then antecedent
choices are pruned, then our second-round model
is trained from scratch on the pruned data.7
7.2 Noisy Oracle Features
We first evaluate our model?s ability to exploit syn-
thetic entity-level properties. For this experiment,
mention properties are derived from corrupted or-
acle information about the true underlying corefer-
ence cluster. Each coreference cluster is assumed
to have one underlying value for each of m coref-
erence properties, each taking values over a do-
main D. Mentions then sample distributions over
D from a Dirichlet distribution peaked around the
true underlying value.8 These posteriors are taken
as the Ri for the TRANSITIVE model.
We choose this setup to reflect two important
properties of entity-level information: first, that it
may come from a variety of disparate sources, and
second, that it may be based on the determinations
of upstream models which produce posteriors nat-
urally. A strength of our model is that it can accept
such posteriors as input, naturally making use of
this information in a model-based way.
Table 1 shows development results averaged
across ten train-test splits with m = 3 proper-
ties, each taking one of |D| = 5 values. We em-
phasize that these parameter settings give fairly
weak oracle information: a document may have
hundreds of clusters, so even in the absence of
noise these oracle properties do not have high dis-
6Using gold entities for training as in Rahman and Ng
(2009) resulted in a lower-performing system.
7We even do this for the BASIC model, since we found
that performance of the pruned and retrained model was gen-
erally higher.
8Specifically, the distribution used is a Dirichlet with
? = 3.5 for the true underlying cluster and ? = 1 for other
values, chosen so that 25% of samples from the distribution
did not have the correct mode. Though these parameters af-
fect the quality of the oracle information, varying them did
not change the relative performance of the different models.
NOISY ORACLE
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 66.31 72.68 49.08 62.69
LEFTTORIGHT 66.49 73.14 49.46 63.03
TRANSITIVE 67.37 74.05 49.68 63.70
Table 1: CoNLL metric scores for our four dif-
ferent systems incorporating noisy oracle data.
This information helps substantially in all cases.
Both entity-level models outperform the PAIR-
PROPERTY model, but we observe that the TRAN-
SITIVE model is more effective than the LEFT-
TORIGHT model at using this information.
criminating power. Still, we see that all mod-
els are able to benefit from incorporating this in-
formation; however, our TRANSITIVE model out-
performs both the PAIRPROPERTY model and the
LEFTTORIGHT model. There are a few reasons
for this: first, our model is able to directly use soft
posteriors, so it is able to exploit the fact that more
peaked samples from the Dirichlet are more likely
to be correct. Moreover, our model can propagate
information backwards in a document as well as
forwards, so the effects of noise can be more eas-
ily mitigated. By contrast, in the LEFTTORIGHT
model, if the first or second mention in a cluster
has the wrong property value, features indicating
high levels of property agreement will not fire on
the next few mentions in those clusters.
7.3 Phi Features
As we have seen, our TRANSITIVE model can ex-
ploit high-quality entity-level features. How does
it perform using real features that have been pro-
posed for entity-level coreference?
Here, we use hard phi feature determinations
extracted from the system of Lee et al (2011).
Named-entity type and animacy are both com-
puted based on the output of a named-entity tag-
ger, while number and gender use the dataset of
Bergsma and Lin (2006). Once this informa-
tion is determined, the PAIRPROPERTY and LEFT-
TORIGHT systems can compute features over it di-
rectly. In the TRANSITIVE model, each of the Ri
factors places 34 of its mass on the determined la-bel and distributes the remainder uniformly among
the possible options.
Table 2 shows results when adding entity-level
phi features on top of our BASIC pairwise system
(which already contains pairwise features) and on
top of an ablated BASIC system without pairwise
120
PHI FEATURES
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
LEFTTORIGHT 61.34 70.41 47.64 59.80
TRANSITIVE 62.66 70.92 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 59.45 69.21 46.02 58.23
PAIRPROPERTY 61.88 70.66 47.14 59.90
LEFTTORIGHT 61.42 70.53 47.49 59.81
TRANSITIVE 62.23 70.78 46.74 59.92
Table 2: CoNLL metric scores for our systems in-
corporating phi features. Our standard BASIC sys-
tem already includes phi features, so no results are
reported for PAIRPROPERTY. Here, our TRAN-
SITIVE system does not give substantial improve-
ment on the averaged metric. Over a baseline
which does not include phi features, all systems
are able to incorporate them comparably.
phi features. Our entity-level systems successfully
captures phi features when they are not present in
the baseline, but there is only slight benefit over
pairwise incorporation, a result which has been
noted previously (Luo et al, 2004).
7.4 Clustering Features
Finally, we consider mention properties derived
from unsupervised clusterings; these properties
are designed to target semantic properties of nom-
inals that should behave more like the oracle fea-
tures than the phi features do.
We consider clusterings that take as input pairs
(n, r) of a noun head n and a string r which con-
tains the semantic role of n (or some approxima-
tion thereof) conjoined with its governor. Two dif-
ferent algorithms are used to cluster these pairs: a
NAIVEBAYES model, where c generates n and r,
and a CONDITIONAL model, where c is generated
conditioned on r and then n is generated from c.
Parameters for each can be learned with the ex-
pectation maximization (EM) algorithm (Demp-
ster et al, 1977), with symmetry broken by a small
amount of random noise at initialization.
Similar models have been used to learn sub-
categorization information (Rooth et al, 1999)
or properties of verb argument slots (Yao et al,
2011). We choose this kind of clustering for its rel-
ative simplicity and because it allows pronouns to
have more informed properties (from their verbal
context) than would be possible using a model that
makes type-level decisions about nominals only.
Though these specific cluster features are novel
to coreference, previous work has used similar
CLUSTERS
MUC B3 CEAFe Avg.
BASIC 61.96 70.66 47.30 59.97
PAIRPROPERTY 62.88 70.71 47.45 60.35
LEFTTORIGHT 61.98 70.19 45.77 59.31
TRANSITIVE 63.34 70.89 46.88 60.37
Table 3: CoNLL metric scores for our systems
incorporating clustering features. These features
are equally effectively incorporated by our PAIR-
PROPERTY system and our TRANSITIVE system.
government
officials
court
authorities
ARG0:said
ARG0:say
ARG0:found
ARG0:announced
prices
shares
index
rates
ARG1:rose
ARG1:fell
ARG1:cut
ARG1:closed
way
law
agreement
plan
ARG1:signed
ARG1:announced
ARG1:set
ARG1:approved
attack
problems
attacks
charges
ARG1:cause
ARG2:following
ARG1:reported
ARG1:filed
... ...
... ...
... ...
... ...
...
Figure 4: Examples of clusters produced by the
NAIVEBAYES model on SRL-tagged data with
pronouns discarded.
types of fine-grained semantic class information
(Hendrickx and Daelemans, 2007; Ng, 2007; Rah-
man and Ng, 2010). Other approaches incorpo-
rate information from other sources (Ponzetto and
Strube, 2006) or compute heuristic scores for real-
valued features based on a large corpus or the web
(Dagan and Itai, 1990; Yang et al, 2005; Bansal
and Klein, 2012).
We use four different clusterings in our
experiments, each with twenty clusters:
dependency-parse-derived NAIVEBAYES clusters,
semantic-role-derived CONDITIONAL clusters,
SRL-derived NAIVEBAYES clusters generating
a NOVERB token when r cannot be determined,
and SRL-derived NAIVEBAYES clusters with all
pronoun tuples discarded. Examples of the latter
clusters are shown in Figure 4. Each clustering
is learned for 30 iterations of EM over English
Gigaword (Graff et al, 2007), parsed with the
Berkeley Parser (Petrov et al, 2006) and with
SRL determined by Senna (Collobert et al, 2011).
Table 3 shows results of modeling these cluster
properties. As in the case of oracle features, the
PAIRPROPERTY and LEFTTORIGHT systems use
the modes of the cluster posteriors, and the TRAN-
SITIVE system uses the posteriors directly as the
Ri. We see comparable performance from incor-
porating features in both an entity-level framework
and a pairwise framework, though the TRANSI-
121
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 69.99 55.59 61.96 80.96 62.69 70.66 41.37 55.21 47.30 59.97
STANFORD 61.49 59.59 60.49 74.60 68.25 71.28 47.57 49.45 48.49 60.10
NOISY ORACLE
PAIRPROPERTY 76.49 58.53 66.31 84.98 63.48 72.68 41.84 59.36 49.08 62.69
LEFTTORIGHT 76.92 58.55 66.49 85.68 63.81 73.14 42.07 60.01 49.46 63.03
TRANSITIVE 76.48 60.20 *67.37 84.84 65.69 *74.05 42.89 59.01 *49.68 63.70
PHI FEATURES
LEFTTORIGHT 69.77 54.73 61.34 81.40 62.04 70.41 41.49 55.92 47.64 59.80
TRANSITIVE 70.27 56.54 *62.66 79.81 63.82 *70.92 41.17 54.44 46.88 60.16
PHI FEATURES (ABLATED BASIC)
BASIC-PHI 67.04 53.41 59.45 78.93 61.63 69.21 40.40 53.46 46.02 58.23
PAIRPROPERTY 70.24 55.31 61.88 81.10 62.60 70.66 41.04 55.38 47.14 59.90
LEFTTORIGHT 69.94 54.75 61.42 81.38 62.23 70.53 41.29 55.87 47.49 59.81
TRANSITIVE 70.06 55.98 *62.23 79.92 63.52 70.78 40.90 54.52 46.74 59.92
CLUSTERS
PAIRPROPERTY 71.77 55.95 62.88 81.76 62.30 70.71 40.98 56.35 47.45 60.35
LEFTTORIGHT 69.75 54.82 61.39 81.48 62.29 70.60 41.62 55.89 47.71 59.90
TRANSITIVE 71.54 56.83 *63.34 80.55 63.31 *70.89 40.77 55.14 46.88 60.37
Table 4: CoNLL metric scores averaged across ten different splits of the training set for each experiment.
We include precision, recall, and F1 for each metric for completeness. Starred F1 values on the individual
metrics for the TRANSITIVE system are significantly better than all other results in the same block at the
p = 0.01 level according to a bootstrap resampling test.
MUC B3 CEAFe Avg.
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 F1
BASIC 68.84 56.08 61.81 77.60 61.40 68.56 38.25 50.57 43.55 57.97
PAIRPROPERTY 70.90 56.26 62.73 78.95 60.79 68.69 37.69 51.92 43.67 58.37
LEFTTORIGHT 68.84 55.56 61.49 78.64 61.03 68.72 38.97 51.74 44.46 58.22
TRANSITIVE 70.62 58.06 *63.73 76.93 62.24 68.81 38.00 50.40 43.33 58.62
STANFORD 60.91 62.13 61.51 70.61 67.75 69.15 45.79 44.55 45.16 58.61
Table 5: CoNLL metric scores for our best systems (including clustering features) on the CoNLL blind
test set, reported in the same manner as Table 4.
TIVE system appears to be more effective than the
LEFTTORIGHT system.
7.5 Final Results
Table 4 shows expanded results on our develop-
ment sets for the different types of entity-level
information we considered. We also show in in
Table 5 the results of our system on the CoNLL
test set, and see that it performs comparably to
the Stanford coreference system (Lee et al, 2011).
Here, our TRANSITIVE system provides modest
improvements over all our other systems.
Based on Table 4, our TRANSITIVE system ap-
pears to do better on MUC andB3 than on CEAFe.
However, we found no simple way to change the
relative performance characteristics of our various
systems; notably, modifying the parameters of the
loss function mentioned in Section 4 or changing
it entirely did not trade off these three metrics but
merely increased or decreased them in lockstep.
Therefore, the TRANSITIVE system actually sub-
stantially improves over our baselines and is not
merely trading off metrics in a way that could be
easily reproduced through other means.
8 Conclusion
In this work, we presented a novel coreference ar-
chitecture that can both take advantage of standard
pairwise features as well as use transitivity to en-
force coherence of decentralized entity-level prop-
erties within coreference clusters. Our transitive
system is more effective at using properties than
a pairwise system and a previous entity-level sys-
tem, and it achieves performance comparable to
that of the Stanford coreference resolution system,
the winner of the CoNLL 2011 shared task.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014, by an NSF
fellowship for the first author, and by a Google fel-
lowship for the second. Thanks to the anonymous
reviewers for their insightful comments.
122
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference.
Mohit Bansal and Dan Klein. 2012. Coreference Se-
mantics from Web Features. In Proceedings of the
Association for Computational Linguistics.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping Path-Based Pronoun Resolution. In Proceed-
ings of the Conference on Computational Linguistics
and the Association for Computational Linguistics.
David Burkett and Dan Klein. 2012. Fast Inference in
Phrase Extraction Models with Belief Propagation.
In Proceedings of the North American Chapter of
the Association for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537, November.
Ido Dagan and Alon Itai. 1990. Automatic Process-
ing of Large Corpora for the Resolution of Anaphora
References. In Proceedings of the Conference on
Computational Linguistics - Volume 3.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society, Series B, 39(1):1?38.
Pascal Denis and Jason Baldridge. 2007. Joint Deter-
mination of Anaphoricity and Coreference Resolu-
tion using Integer Programming. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Eraldo Rezende Fernandes, C??cero Nogueira dos San-
tos, and Ruy Luiz Milidiu?. 2012. Latent Structure
Perceptron with Feature Induction for Unrestricted
Coreference Resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Proceedings and Conference on Computa-
tional Natural Language Learning - Shared Task.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing Transitivity in Coreference Resolution.
In Proceedings of the Association for Computational
Linguistics: Short Papers.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin CRFs: Training Log-Linear Models with
Cost Functions. In Proceedings of the North Amer-
ican Chapter for the Association for Computational
Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Aria Haghighi and Dan Klein. 2010. Coreference Res-
olution in a Modular, Entity-Centered Model. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Iris Hendrickx and Walter Daelemans, 2007. Adding
Semantic Information: Unsupervised Clusters for
Coreference Resolution.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics: Short Papers.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s Multi-Pass Sieve Corefer-
ence Resolution System at the CoNLL-2011 Shared
Task. In Proceedings of the Conference on Compu-
tational Natural Language Learning: Shared Task.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Mathematical Programming, 45(3):503?528,
December.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings of
the Association for Computational Linguistics.
Xiaoqiang Luo. 2005. On Coreference Resolution
Performance Metrics. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional Models of Identity Uncertainty with Applica-
tion to Noun Coreference. In Proceedings of Ad-
vances in Neural Information Processing Systems.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39?41.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the Association
for Computational Linguistics.
123
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Conference on Computational Linguistics and the
Association for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution. In Proceed-
ings of the North American Chapter of the Associa-
tion of Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In Proceed-
ings of the Conference on Computational Natural
Language Learning: Shared Task.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A Multi-
Pass Sieve for Coreference Resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Altaf Rahman and Vincent Ng. 2010. Inducing Fine-
Grained Semantic Classes via Hierarchical and Col-
lective Classification. In Proceedings of the Interna-
tional Conference on Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intel-
ligence Research, 40(1):469?521, January.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a Semanti-
cally Annotated Lexicon via EM-Based Clustering.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2008. Dependency
Parsing by Belief Propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint Learning for Corefer-
ence Resolution with Markov Logic. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference Resolution with Reconcile. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Papers.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Pro-
ceedings of the Conference on Message Understand-
ing.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information. In Proceed-
ings of the Association for Computational Linguis-
tics.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan, Ting
Liu, and Sheng Li. 2008. An Entity-Mention Model
for Coreference Resolution with Inductive Logic
Programming. In Proceedings of the Association for
Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured Relation Discov-
ery Using Generative Models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
124
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 207?217,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Unsupervised Transcription of Historical Documents
Taylor Berg-Kirkpatrick Greg Durrett Dan Klein
Computer Science Division
University of California at Berkeley
{tberg,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a generative probabilistic
model, inspired by historical printing pro-
cesses, for transcribing images of docu-
ments from the printing press era. By
jointly modeling the text of the docu-
ment and the noisy (but regular) process
of rendering glyphs, our unsupervised sys-
tem is able to decipher font structure and
more accurately transcribe images into
text. Overall, our system substantially out-
performs state-of-the-art solutions for this
task, achieving a 31% relative reduction
in word error rate over the leading com-
mercial system for historical transcription,
and a 47% relative reduction over Tesser-
act, Google?s open source OCR system.
1 Introduction
Standard techniques for transcribing modern doc-
uments do not work well on historical ones. For
example, even state-of-the-art OCR systems pro-
duce word error rates of over 50% on the docu-
ments shown in Figure 1. Unsurprisingly, such er-
ror rates are too high for many research projects
(Arlitsch and Herbert, 2004; Shoemaker, 2005;
Holley, 2010). We present a new, generative
model specialized to transcribing printing-press
era documents. Our model is inspired by the un-
derlying printing processes and is designed to cap-
ture the primary sources of variation and noise.
One key challenge is that the fonts used in his-
torical documents are not standard (Shoemaker,
2005). For example, consider Figure 1a. The fonts
are not irregular like handwriting ? each occur-
rence of a given character type, e.g. a, will use the
same underlying glyph. However, the exact glyphs
are unknown. Some differences between fonts are
minor, reflecting small variations in font design.
Others are more severe, like the presence of the
archaic long s character before 1804. To address
the general problem of unknown fonts, our model
(a)
(b)
(c)
Figure 1: Portions of historical documents with (a) unknown
font, (b) uneven baseline, and (c) over-inking.
learns the font in an unsupervised fashion. Font
shape and character segmentation are tightly cou-
pled, and so they are modeled jointly.
A second challenge with historical data is that
the early typesetting process was noisy. Hand-
carved blocks were somewhat uneven and often
failed to sit evenly on the mechanical baseline.
Figure 1b shows an example of the text?s baseline
moving up and down, with varying gaps between
characters. To deal with these phenomena, our
model incorporates random variables that specifi-
cally describe variations in vertical offset and hor-
izontal spacing.
A third challenge is that the actual inking was
also noisy. For example, in Figure 1c some charac-
ters are thick from over-inking while others are ob-
scured by ink bleeds. To be robust to such render-
ing irregularities, our model captures both inking
levels and pixel-level noise. Because the model
is generative, we can also treat areas that are ob-
scured by larger ink blotches as unobserved, and
let the model predict the obscured text based on
visual and linguistic context.
Our system, which we call Ocular, operates by
fitting the model to each document in an unsuper-
vised fashion. The system outperforms state-of-
the-art baselines, giving a 47% relative error re-
duction over Google?s open source Tesseract sys-
tem, and giving a 31% relative error reduction over
ABBYY?s commercial FineReader system, which
has been used in large-scale historical transcrip-
tion projects (Holley, 2010).
207
Over-inked
It appeared that the Prisoner was veryE :
X :
Wandering baseline Historical font
Figure 2: An example image from a historical document (X)
and its transcription (E).
2 Related Work
Relatively little prior work has built models specif-
ically for transcribing historical documents. Some
of the challenges involved have been addressed
(Ho and Nagy, 2000; Huang et al, 2006; Kae and
Learned-Miller, 2009), but not in a way targeted
to documents from the printing press era. For ex-
ample, some approaches have learned fonts in an
unsupervised fashion but require pre-segmentation
of the image into character or word regions (Ho
and Nagy, 2000; Huang et al, 2006), which is not
feasible for noisy historical documents. Kae and
Learned-Miller (2009) jointly learn the font and
image segmentation but do not outperform mod-
ern baselines.
Work that has directly addressed historical doc-
uments has done so using a pipelined approach,
and without fully integrating a strong language
model (Vamvakas et al, 2008; Kluzner et al,
2009; Kae et al, 2010; Kluzner et al, 2011).
The most comparable work is that of Kopec and
Lomelin (1996) and Kopec et al (2001). They
integrated typesetting models with language mod-
els, but did not model noise. In the NLP com-
munity, generative models have been developed
specifically for correcting outputs of OCR systems
(Kolak et al, 2003), but these do not deal directly
with images.
A closely related area of work is automatic de-
cipherment (Ravi and Knight, 2008; Snyder et al,
2010; Ravi and Knight, 2011; Berg-Kirkpatrick
and Klein, 2011). The fundamental problem is
similar to our own: we are presented with a se-
quence of symbols, and we need to learn a corre-
spondence between symbols and letters. Our ap-
proach is also similar in that we use a strong lan-
guage model (in conjunction with the constraint
that the correspondence be regular) to learn the
correct mapping. However, the symbols are not
noisy in decipherment problems and in our prob-
lem we face a grid of pixels for which the segmen-
tation into symbols is unknown. In contrast, deci-
pherment typically deals only with discrete sym-
bols.
3 Model
Most historical documents have unknown fonts,
noisy typesetting layouts, and inconsistent ink lev-
els, usually simultaneously. For example, the por-
tion of the document shown in Figure 2 has all
three of these problems. Our model must handle
them jointly.
We take a generative modeling approach in-
spired by the overall structure of the historical
printing process. Our model generates images of
documents line by line; we present the generative
process for the image of a single line. Our pri-
mary random variables are E (the text) andX (the
pixels in an image of the line). Additionally, we
have a random variable T that specifies the layout
of the bounding boxes of the glyphs in the image,
and a random variable R that specifies aspects of
the inking and rendering process. The joint distri-
bution is:
P (E, T,R,X) =
P (E) [Language model]
? P (T |E) [Typesetting model]
? P (R) [Inking model]
? P (X|E, T,R) [Noise model]
We let capital letters denote vectors of concate-
nated random variables, and we denote the indi-
vidual random variables with lower-case letters.
For example, E represents the entire sequence of
text, while ei represents ith character in the se-
quence.
3.1 Language Model P (E)
Our language model, P (E), is a Kneser-Ney
smoothed character n-gram model (Kneser and
Ney, 1995). We generate printed lines of text
(rather than sentences) independently, without
generating an explicit stop character. This means
that, formally, the model must separately generate
the character length of each line. We choose not to
bias the model towards longer or shorter character
sequences and let the line length m be drawn uni-
formly at random from the positive integers less
than some large constant M.1 When i < 1, let ei
denote a line-initial null character. We can now
write:
P (E) = P (m) ?
m?
i=1
P (ei|ei?1, . . . , ei?n)
1In particular, we do not use the kind of ?word bonus?
common to statistical machine translation models.
208
ei 1 ei+1ei
li gi ri
XRPADiX
LPAD
i X
GLYPH
i
P ( ? | th)P ( ? | th)
a b c . . . z
Offset: ?VERT
LM params
cb
b
1 30
15 1 5
a
Glyph weights:  c
Bounding box probs:
Left padwidth: ?LPADc
Right padwidth: ?RPADc
Glyph width: ?GLYPHc
Font params
a a a
a a a
P ( ? | pe)
Inking: ?INK
Inking params
Figure 3: Character tokens ei are generated by the language model. For each token index i, a glyph bounding box width gi,
left padding width li, and a right padding width ri, are generated. Finally, the pixels in each glyph bounding box XGLYPHi aregenerated conditioned on the corresponding character, while the pixels in left and right padding bounding boxes, XLPADi and
XRPADi , are generated from a background distribution.
3.2 Typesetting Model P (T |E)
Generally speaking, the process of typesetting
produces a line of text by first tiling bounding
boxes of various widths and then filling in the
boxes with glyphs. Our generative model, which
is depicted in Figure 3, reflects this process. As
a first step, our model generates the dimensions
of character bounding boxes; for each character
token index i we generate three bounding box
widths: a glyph box width gi, a left padding box
width li, and a right padding box width ri, as
shown in Figure 3. We let the pixel height of all
lines be fixed to h. Let Ti = (li, gi, ri) so that Ti
specifies the dimensions of the character box for
token index i; T is then the concatenation of all
Ti, denoting the full layout.
Because the width of a glyph depends on its
shape, and because of effects resulting from kern-
ing and the use of ligatures, the components of
each Ti are drawn conditioned on the character
token ei. This means that, as part of our param-
eterization of the font, for each character type c
we have vectors of multinomial parameters ?LPADc ,
?GLYPHc , and ?RPADc governing the distribution of the
dimensions of character boxes of type c. These
parameters are depicted on the right-hand side of
Figure 3. We can now express the typesetting lay-
out portion of the model as:
P (T |E) =
m?
i=1
P (Ti|ei)
=
m?
i=1
[
P (li; ?LPADei ) ? P (gi; ?
GLYPH
ei ) ? P (ri; ?
RPAD
ei )
]
Each character type c in our font has another set
of parameters, a matrix ?c. These are weights that
specify the shape of the character type?s glyph,
and are depicted in Figure 3 as part of the font pa-
rameters. ?c will come into play when we begin
generating pixels in Section 3.3.
3.2.1 Inking Model P (R)
Before we start filling the character boxes with
pixels, we need to specify some properties of
the inking and rendering process, including the
amount of ink used and vertical variation along
the text baseline. Our model does this by gener-
ating, for each character token index i, a discrete
value di that specifies the overall inking level in
the character?s bounding box, and a discrete value
vi that specifies the glyph?s vertical offset. These
variations in the inking and typesetting process are
mostly independent of character type. Thus, in
209
our model, their distributions are not character-
specific. There is one global set of multinomial
parameters governing inking level (?INK), and an-
other governing offset (?VERT); both are depicted
on the left-hand side of Figure 3. LetRi = (di, vi)
and let R be the concatenation of all Ri so that we
can express the inking model as:
P (R) =
m?
i=1
P (Ri)
=
m?
i=1
[
P (di; ?INK) ? P (vi; ?VERT)
]
The di and vi variables are suppressed in Figure 3
to reduce clutter but are expressed in Figure 4,
which depicts the process of rendering a glyph
box.
3.3 Noise Model P (X|E, T,R)
Now that we have generated a typesetting layout
T and an inking context R, we have to actually
generate each of the pixels in each of the charac-
ter boxes, left padding boxes, and right padding
boxes; the matrices that these groups of pixels
comprise are denoted XGLYPHi , XLPADi , and XRPADi ,
respectively, and are depicted at the bottom of Fig-
ure 3.
We assume that pixels are binary valued and
sample their values independently from Bernoulli
distributions.2 The probability of black (the
Bernoulli parameter) depends on the type of pixel
generated. All the pixels in a padding box have
the same probability of black that depends only on
the inking level of the box, di. Since we have al-
ready generated this value and the widths li and ri
of each padding box, we have enough information
to generate left and right padding pixel matrices
XLPADi and XRPADi .
The Bernoulli parameter of a pixel inside a
glyph bounding box depends on the pixel?s loca-
tion inside the box (as well as on di and vi, but
for simplicity of exposition, we temporarily sup-
press this dependence) and on the model param-
eters governing glyph shape (for each character
type c, the parameter matrix ?c specifies the shape
of the character?s glyph.) The process by which
glyph pixels are generated is depicted in Figure 4.
The dependence of glyph pixels on location
complicates generation of the glyph pixel matrix
XGLYPHi since the corresponding parameter matrix
2We could generate real-valued pixels with a different
choice of noise distribution.
}
}
}
}
}
aa a
a a a
a a
a
}
Interpolate, apply logistic
Sample pixels
Choosewidth
Chooseoffset
Glyph weights
gi
di
vi
 ei
?PIXEL(j, k, gi, di, vi; ei)
?XGLYPHi
?
jk ? Bernoulli
Bernoulli parameters
Pixel values
Chooseinking
Figure 4: We generate the pixels for the character token ei
by first sampling a glyph width gi, an inking level di, and
a vertical offset vi. Then we interpolate the glyph weights
?ei and apply the logistic function to produce a matrix ofBernoulli parameters of width gi, inking di, and offset vi.
?PIXEL(j, k, gi, di, vi;?ei) is the Bernoulli parameter at row jand column k. Finally, we sample from each Bernoulli distri-
bution to generate a matrix of pixel values, XGLYPHi .
?ei has some type-level width w which may dif-
fer from the current token-level width gi. Intro-
ducing distinct parameters for each possible width
would yield a model that can learn completely dif-
ferent glyph shapes for slightly different widths of
the same character. We, instead, need a parame-
terization that ties the shapes for different widths
together, and at the same time allows mobility in
the parameter space during learning.
Our solution is to horizontally interpolate the
weights of the shape parameter matrix ?ei down
to a smaller set of columns matching the token-
level choice of glyph width gi. Thus, the type-
level matrix ?ei specifies the canonical shape of
the glyph for character ei when it takes its max-
imum width w. After interpolating, we apply
the logistic function to produce the individual
Bernoulli parameters. If we let [XGLYPHi ]jk denote
the value of the pixel at the jth row and kth col-
umn of the glyph pixel matrix XGLYPHi for token i,
and let ?PIXEL(j, k, gi;?ei) denote the token-level
210
?PIXEL :
Interpolate, apply logistic
 c :
Glyph weights
Bernoulli params
?
Figure 5: In order to produce Bernoulli parameter matrices
?PIXEL of variable width, we interpolate over columns of ?c
with vectors ?, and apply the logistic function to each result.
Bernoulli parameter for this pixel, we can write:
[XGLYPHi ]jk ? Bernoulli
(
?PIXEL(j, k, gi;?ei)
)
The interpolation process for a single row is de-
picted in Figure 5. We define a constant interpola-
tion vector ?(gi, k) that is specific to the glyph box
width gi and glyph box column k. Each ?(gi, k)
is shaped according to a Gaussian centered at the
relative column position in ?ei . The glyph pixel
Bernoulli parameters are defined as follows:
?PIXEL(j, k,gi;?ei) =
logistic
( w?
k?=1
[
?(gi, k)k? ? [?ei ]jk?
])
The fact that the parameterization is log-linear will
ensure that, during the unsupervised learning pro-
cess, updating the shape parameters ?c is simple
and feasible.
By varying the magnitude of ? we can change
the level of smoothing in the logistic model and
cause it to permit areas that are over-inked. This is
the effect that di controls. By offsetting the rows
of ?c that we interpolate weights from, we change
the vertical offset of the glyph, which is controlled
by vi. The full pixel generation process is dia-
grammed in Figure 4, where the dependence of
?PIXEL on di and vi is also represented.
4 Learning
We use the EM algorithm (Dempster et al, 1977)
to find the maximum-likelihood font parameters:
?c, ?LPADc , ?GLYPHc , and ?RPADc . The image X is the
only observed random variable in our model. The
identities of the characters E the typesetting lay-
out T and the inking R will all be unobserved. We
do not learn ?INK and ?VERT, which are set to the
uniform distribution.
4.1 Expectation Maximization
During the E-step we compute expected counts
for E and T , but maximize over R, for which
we compute hard counts. Our model is an in-
stance of a hidden semi-Markov model (HSMM),
and therefore the computation of marginals is
tractable with the semi-Markov forward-backward
algorithm (Levinson, 1986).
During the M-step, we update the parame-
ters ?LPADc , ?RPADc using the standard closed-form
multinomial updates and use a specialized closed-
form update for ?GLYPHc that enforces unimodal-
ity of the glyph width distribution.3 The glyph
weights, ?c, do not have a closed-form update.
The noise model that ?c parameterizes is a lo-
cal log-linear model, so we follow the approach
of Berg-Kirkpatrick et al (2010) and use L-BFGS
(Liu and Nocedal, 1989) to optimize the expected
likelihood with respect to ?c.
4.2 Coarse-to-Fine Learning and Inference
The number of states in the dynamic programming
lattice grows exponentially with the order of the
language model (Jelinek, 1998; Koehn, 2004). As
a result, inference can become slow when the lan-
guage model order n is large. To remedy this, we
take a coarse-to-fine approach to both learning and
inference. On each iteration of EM, we perform
two passes: a coarse pass using a low-order lan-
guage model, and a fine pass using a high-order
language model (Petrov et al, 2008; Zhang and
Gildea, 2008). We use the marginals4 from the
coarse pass to prune states from the dynamic pro-
gram of the fine pass.
In the early iterations of EM, our font parame-
ters are still inaccurate, and to prune heavily based
on such parameters would rule out correct anal-
yses. Therefore, we gradually increase the ag-
gressiveness of pruning over the course of EM. To
ensure that each iteration takes approximately the
same amount of computation, we also gradually
increase the order of the fine pass, only reaching
the full order n on the last iteration. To produce a
decoding of the image into text, on the final iter-
ation we run a Viterbi pass using the pruned fine
model.
3We compute the weighted mean and weighted variance
of the glyph width expected counts. We set ?GLYPHc to be pro-
portional to a discretized Gaussian with the computed mean
and variance. This update is approximate in the sense that it
does not necessarily find the unimodal multinomial that max-
imizes expected log-likelihood, but it works well in practice.
4In practice, we use max-marginals for pruning to ensure
that there is still a valid path in the pruned lattice.
211
Old Bailey, 1725:
Old Bailey, 1875:
Trove, 1883:
Trove, 1823:
(a)
(b)
(c)
(d)
Figure 6: Portions of several documents from our test set rep-
resenting a range of difficulties are displayed. On document
(a), which exhibits noisy typesetting, our system achieves a
word error rate (WER) of 25.2. Document (b) is cleaner in
comparison, and on it we achieve a WER of 15.4. On doc-
ument (c), which is also relatively clean, we achieve a WER
of 12.5. On document (d), which is severely degraded, we
achieve a WER of 70.0.
5 Data
We perform experiments on two historical datasets
consisting of images of documents printed be-
tween 1700 and 1900 in England and Australia.
Examples from both datasets are displayed in Fig-
ure 6.
5.1 Old Bailey
The first dataset comes from a large set of im-
ages of the proceedings of the Old Bailey, a crimi-
nal court in London, England (Shoemaker, 2005).
The Old Bailey curatorial effort, after deciding
that current OCR systems do not adequately han-
dle 18th century fonts, manually transcribed the
documents into text. We will use these manual
transcriptions to evaluate the output of our system.
From the Old Bailey proceedings, we extracted a
set of 20 images, each consisting of 30 lines of
text to use as our first test set. We picked 20 doc-
uments, printed in consecutive decades. The first
document is from 1715 and the last is from 1905.
We choose the first document in each of the corre-
sponding years, choose a random page in the doc-
ument, and extracted an image of the first 30 con-
secutive lines of text consisting of full sentences.5
The ten documents in the Old Bailey dataset that
were printed before 1810 use the long s glyph,
while the remaining ten do not.
5.2 Trove
Our second dataset is taken from a collection of
digitized Australian newspapers that were printed
between the years of 1803 and 1954. This col-
lection is called Trove, and is maintained by the
the National Library of Australia (Holley, 2010).
We extracted ten images from this collection in the
same way that we extracted images from Old Bai-
ley, but starting from the year 1803. We manually
produced our own gold annotations for these ten
images. Only the first document of Trove uses the
long s glyph.
5.3 Pre-processing
Many of the images in historical collections are
bitonal (binary) as a result of how they were cap-
tured on microfilm for storage in the 1980s (Arl-
itsch and Herbert, 2004). This is part of the reason
our model is designed to work directly with bi-
narized images. For consistency, we binarized the
images in our test sets that were not already binary
by thresholding pixel values.
Our model requires that the image be pre-
segmented into lines of text. We automatically
segment lines by training an HSMM over rows of
pixels. After the lines are segmented, each line
is resampled so that its vertical resolution is 30
pixels. The line extraction process also identifies
pixels that are not located in central text regions,
and are part of large connected components of ink,
spanning multiple lines. The values of such pixels
are treated as unobserved in the model since, more
often than not, they are part of ink blotches.
5This ruled out portions of the document with extreme
structural abnormalities, like title pages and lists. These
might be interesting to model, but are not within the scope
of this paper.
212
6 Experiments
We evaluate our system by comparing our text
recognition accuracy to that of two state-of-the-art
systems.
6.1 Baselines
Our first baseline is Google?s open source OCR
system, Tesseract (Smith, 2007). Tesseract takes
a pipelined approach to recognition. Before rec-
ognizing the text, the document is broken into
lines, and each line is segmented into words.
Then, Tesseract uses a classifier, aided by a word-
unigram language model, to recognize whole
words.
Our second baseline, ABBYY FineReader 11
Professional Edition,6 is a state-of-the-art com-
mercial OCR system. It is the OCR system that
the National Library of Australia used to recognize
the historical documents in Trove (Holley, 2010).
6.2 Evaluation
We evaluate the output of our system and the base-
line systems using two metrics: character error
rate (CER) and word error rate (WER). Both these
metrics are based on edit distance. CER is the edit
distance between the predicted and gold transcrip-
tions of the document, divided by the number of
characters in the gold transcription. WER is the
word-level edit distance (words, instead of char-
acters, are treated as tokens) between predicted
and gold transcriptions, divided by the number of
words in the gold transcription. When computing
WER, text is tokenized into words by splitting on
whitespace.
6.3 Language Model
We ran experiments using two different language
models. The first language model was trained
on the initial one million sentences of the New
York Times (NYT) portion of the Gigaword cor-
pus (Graff et al, 2007), which contains about 36
million words. This language model is out of do-
main for our experimental documents. To inves-
tigate the effects of using an in domain language
model, we created a corpus composed of the man-
ual annotations of all the documents in the Old
Bailey proceedings, excluding those used in our
test set. This corpus consists of approximately 32
million words. In all experiments we used a char-
acter n-gram order of six for the final Viterbi de-
6http://www.abbyy.com
System CER WER
Old Bailey
Google Tesseract 29.6 54.8
ABBYY FineReader 15.1 40.0
Ocular w/ NYT (this work) 12.6 28.1
Ocular w/ OB (this work) 9.7 24.1
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
Ocular w/ NYT (this work) 14.9 33.0
Table 1: We evaluate the predicted transcriptions in terms of
both character error rate (CER) and word error rate (WER),
and report macro-averages across documents. We compare
with two baseline systems: Google?s open source OCR sys-
tem, Tessearact, and a state-of-the-art commercial system,
ABBYY FineReader. We refer to our system as Ocular w/
NYT and Ocular w/ OB, depending on whether NYT or Old
Bailey is used to train the language model.
coding pass and an order of three for all coarse
passes.
6.4 Initialization and Tuning
We used as a development set ten additional docu-
ments from the Old Bailey proceedings and five
additional documents from Trove that were not
part of our test set. On this data, we tuned the
model?s hyperparameters7 and the parameters of
the pruning schedule for our coarse-to-fine ap-
proach.
In experiments we initialized ?RPADc and ?LPADc to
be uniform, and initialized ?GLYPHc and ?c based
on the standard modern fonts included with the
Ubuntu Linux 12.04 distribution.8 For documents
that use the long s glyph, we introduce a special
character type for the non-word-final s, and ini-
tialize its parameters from a mixture of the modern
f and | glyphs.9
7 Results and Analysis
The results of our experiments are summarized in
Table 1. We refer to our system as Ocular w/
NYT or Ocular w/ OB, depending on whether the
language model was trained using NYT or Old
Bailey, respectively. We compute macro-averages
7One of the hyperparameters we tune is the exponent of
the language model. This balances the contributions of the
language model and the typesetting model to the posterior
(Och and Ney, 2004).
8http://www.ubuntu.com/
9Following Berg-Kirkpatrick et al (2010), we use a reg-
ularization term in the optimization of the log-linear model
parameters ?c during the M-step. Instead of regularizing to-
wards zero, we regularize towards the initializer. This slightly
improves performance on our development set and can be
thought of as placing a prior on the glyph shape parameters.
213
(c) Trove, 1883:
(b) Old Bailey, 1885:
(a) Old Bailey, 1775: the prisoner at the bar. Jacob Lazarus and his
taken ill and taken away ? I remember
how the murderers came to learn the nation in
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Predicted text:
Predicted typesetting:
Image:
Figure 7: For each of these portions of test documents, the first line shows the transcription predicted by our model and the
second line shows a representation of the learned typesetting layout. The grayscale glyphs show the Bernoulli pixel distributions
learned by our model, while the padding regions are depicted in blue. The third line shows the input image.
across documents from all years. Our system, us-
ing the NYT language model, achieves an average
WER of 28.1 on Old Bailey and an average WER
of 33.0 on Trove. This represents a substantial er-
ror reduction compared to both baseline systems.
If we average over the documents in both Old
Bailey and Trove, we find that Tesseract achieved
an average WER of 56.3, ABBYY FineReader
achieved an average WER of 43.1, and our system,
using the NYT language model, achieved an aver-
age WER of 29.7. This means that while Tesseract
incorrectly predicts more than half of the words in
these documents, our system gets more than three-
quarters of them right. Overall, we achieve a rela-
tive reduction in WER of 47% compared to Tesser-
act and 31% compared to ABBYY FineReader.
The baseline systems do not have special pro-
visions for the long s glyph. In order to make
sure the comparison is fair, we separately com-
puted average WER on only the documents from
after 1810 (which do no use the long s glyph). We
found that using this evaluation our system actu-
ally acheives a larger relative reduction in WER:
50% compared to Tesseract and 35% compared to
ABBYY FineReader.
Finally, if we train the language model using
the Old Bailey corpus instead of the NYT corpus,
we see an average improvement of 4 WER on the
Old Bailey test set. This means that the domain of
the language model is important, but, the results
are not affected drastically even when using a lan-
guage model based on modern corpora (NYT).
7.1 Learned Typesetting Layout
Figure 7 shows a representation of the typesetting
layout learned by our model for portions of several
Initializer
1700
1740
1780 1820
1860
1900
Figure 8: The central glyph is a representation of the initial
model parameters for the glyph shape for g, and surrounding
this are the learned parameters for documents from various
years.
test documents. For each portion of a test doc-
ument, the first line shows the transcription pre-
dicted by our model, and the second line shows
padding and glyph regions predicted by the model,
where the grayscale glyphs represent the learned
Bernoulli parameters for each pixel. The third line
shows the input image.
Figure 7a demonstrates a case where our model
has effectively explained both the uneven baseline
and over-inked glyphs by using the vertical offsets
vi and inking variables di. In Figure 7b the model
has used glyph widths gi and vertical offsets to ex-
plain the thinning of glyphs and falling baseline
that occurred near the binding of the book. In sep-
arate experiments on the Old Bailey test set, using
the NYT language model, we found that remov-
ing the vertical offset variables from the model in-
creased WER by 22, and removing the inking vari-
ables increased WER by 16. This indicates that it
is very important to model both these aspects of
printing press rendering.
214
Figure 9: This Old Bailey document from 1719 has severe ink bleeding from the facing page. We annotated these blotches (in
red) and treated the corresponding pixels as unobserved in the model. The layout shown is predicted by the model.
Figure 7c shows the output of our system on
a difficult document. Here, missing characters
and ink blotches confuse the model, which picks
something that is reasonable according to the lan-
guage model, but incorrect.
7.2 Learned Fonts
It is interesting to look at the fonts learned by our
system, and track how historical fonts changed
over time. Figure 8 shows several grayscale im-
ages representing the Bernoulli pixel probabilities
for the most likely width of the glyph for g under
various conditions. At the center is the representa-
tion of the initial parameter values, and surround-
ing this are the learned parameters for documents
from various years. The learned shapes are visibly
different from the initializer, which is essentially
an average of modern fonts, and also vary across
decades.
We can ask to what extent learning the font
structure actually improved our performance. If
we turn off learning and just use the initial pa-
rameters to decode, WER increases by 8 on the
Old Bailey test set when using the NYT language
model.
7.3 Unobserved Ink Blotches
As noted earlier, one strength of our generative
model is that we can make the values of certain
pixels unobserved in the model, and let inference
fill them in. We conducted an additional experi-
ment on a document from the Old Bailey proceed-
ings that was printed in 1719. This document, a
fragment of which is shown in Figure 9, has se-
vere ink bleeding from the facing page. We manu-
ally annotated the ink blotches (shown in red), and
made them unobserved in the model. The result-
ing typesetting layout learned by the model is also
shown in Figure 9. The model correctly predicted
most of the obscured words. Running the model
with the manually specified unobserved pixels re-
duced the WER on this document from 58 to 19
when using the NYT language model.
7.4 Remaining Errors
We performed error analysis on our development
set by randomly choosing 100 word errors from
the WER alignment and manually annotating them
with relevant features. Specifically, for each word
error we recorded whether or not the error con-
tained punctuation (either in the predicted word or
the gold word), whether the text in the correspond-
ing portion of the original image was italicized,
and whether the corresponding portion of the im-
age exhibited over-inking, missing ink, or signif-
icant ink blotches. These last three feature types
are subjective in nature but may still be informa-
tive. We found that 56% of errors were accompa-
nied by over-inking, 50% of errors were accom-
panied by ink blotches, 42% of errors contained
punctuation, 21% of errors showed missing ink,
and 12% of errors contained text that was itali-
cized in the original image.
Our own subjective assessment indicates that
many of these error features are in fact causal.
More often than not, italicized text is incorrectly
transcribed. In cases of extreme ink blotching,
or large areas of missing ink, the system usually
makes an error.
8 Conclusion
We have demonstrated a model, based on the his-
torical typesetting process, that effectively learns
font structure in an unsupervised fashion to im-
prove transcription of historical documents into
text. The parameters of the learned fonts are inter-
pretable, as are the predicted typesetting layouts.
Our system achieves state-of-the-art results, sig-
nificantly outperforming two state-of-the-art base-
line systems.
215
References
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &
Imaging Review.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings
of the 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies:.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Tin Kam Ho and George Nagy. 2000. OCR with no
shape training. In Proceedings of the 15th Interna-
tional Conference on Pattern Recognition.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Gary Huang, Erik G Learned-Miller, and Andrew Mc-
Callum. 2006. Cryptogram decoding for optical
character recognition. University of Massachusetts-
Amherst Technical Report.
Fred Jelinek. 1998. Statistical methods for speech
recognition. MIT press.
Andrew Kae and Erik Learned-Miller. 2009. Learn-
ing on the fly: font-free approaches to difficult OCR
problems. In Proceedings of the 2009 International
Conference on Document Analysis and Recognition.
Andrew Kae, Gary Huang, Carl Doersch, and Erik
Learned-Miller. 2010. Improving state-of-the-
art OCR through high-precision document-specific
modeling. In Proceedings of the 2010 IEEE Confer-
ence on Computer Vision and Pattern Recognition.
Vladimir Kluzner, Asaf Tzadok, Yuval Shimony, Eu-
gene Walach, and Apostolos Antonacopoulos. 2009.
Word-based adaptive OCR for historical books. In
Proceedings of the 2009 International Conference
on on Document Analysis and Recognition.
Vladimir Kluzner, Asaf Tzadok, Dan Chevion, and Eu-
gene Walach. 2011. Hybrid approach to adaptive
OCR for historical books. In Proceedings of the
2011 International Conference on Document Anal-
ysis and Recognition.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. Machine translation: From real users
to research.
Okan Kolak, William Byrne, and Philip Resnik. 2003.
A generative probabilistic OCR model for NLP ap-
plications. In Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Gary Kopec and Mauricio Lomelin. 1996. Document-
specific character template estimation. In Proceed-
ings of the International Society for Optics and Pho-
tonics.
Gary Kopec, Maya Said, and Kris Popat. 2001. N-
gram language models for document image decod-
ing. In Proceedings of Society of Photographic In-
strumentation Engineers.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech & Language.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming.
Franz Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the tesseract ocr
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
216
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics.
Georgios Vamvakas, Basilios Gatos, Nikolaos Stam-
atopoulos, and Stavros Perantonis. 2008. A com-
plete optical character recognition methodology for
historical documents. In The Eighth IAPR Interna-
tional Workshop on Document Analysis Systems.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
217
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98?103,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Examination of Challenges in Chinese Parsing
Jonathan K. Kummerfeld
?
Daniel Tse
?
James R. Curran
?
Dan Klein
?
?
Computer Science Division
?
School of Information Technology
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
{jkk,klein}@cs.berkeley.edu {dtse6695,james}@it.usyd.edu.au
Abstract
Aspects of Chinese syntax result in a dis-
tinctive mix of parsing challenges. How-
ever, the contribution of individual sources
of error to overall difficulty is not well un-
derstood. We conduct  a  comprehensive
automatic analysis of error types made by
Chinese parsers, covering a broad range of
error types for large sets of sentences, en-
abling the first empirical ranking of Chi-
nese error types by their performance im-
pact. We also investigate which error types
are resolved by using gold part-of-speech
tags, showing that improving Chinese tag-
ging  only  addresses  certain  error  types,
leaving substantial outstanding challenges.
1 Introduction
A decade of Chinese parsing research, enabled
by the Penn Chinese Treebank (PCTB; Xue et al,
2005), has seen Chinese parsing performance im-
prove from 76.7 F1 (Bikel and Chiang, 2000) to
84.1 F1 (Qian and Liu, 2012). While recent ad-
vances have focused on understanding and reduc-
ing the errors that occur in segmentation and part-
of-speech tagging (Qian and Liu, 2012; Jiang et al,
2009; Forst and Fang, 2009), a range of substantial
issues remain that are purely syntactic.
Early work by Levy and Manning (2003) pre-
sented modifications to a parser motivated by a
manual investigation of parsing errors. They noted
substantial differences between Chinese and En-
glish parsing, attributing some of the differences to
treebank annotation decisions and others to mean-
ingful differences in syntax. Based on this analysis
they considered how to modify their parser to cap-
ture the information necessary to model the syn-
tax within the PCTB. However, their manual ana-
lysis was limited in scope, covering only part of
the parser output, and was unable to characterize
the relative impact of the issues they uncovered.
This paper presents a more comprehensive ana-
lysis of errors in Chinese parsing, building on the
technique presented in Kummerfeld et al (2012),
which characterized the error behavior of English
parsers by quantifying how often they make er-
rors such as PP attachment and coordination scope.
To accommodate error classes that are absent in
English, we  augment  the  system  to  recognize
Chinese-specific parse errors.
1
We use the modi-
fied system to show the relative impact of different
error types across a range of Chinese parsers.
To understand the impact of tagging errors on
different  error  types, we  performed  a  part-of-
speech ablation experiment, in  which particular
confusions are introduced in isolation. By analyz-
ing the distribution of errors in the system output
with and without gold part-of-speech tags, we are
able to isolate and quantify the error types that can
be resolved by improvements in tagging accuracy.
Our analysis shows that improvements in tag-
ging accuracy can only address a subset of the chal-
lenges of Chinese syntax. Further improvement in
Chinese parsing performance will require research
addressing other challenges, in particular, deter-
mining coordination scope.
2 Background
The closest previous work is the detailed manual
analysis performed by Levy and Manning (2003).
While their focus was on issues faced by their fac-
tored PCFG parser (Klein and Manning, 2003b),
the error types they identified are general issues
presented by Chinese syntax in the PCTB. They
presented several Chinese error types that are rare
or absent in English, including noun/verb ambigu-
ity, NP-internal structure and coordination ambi-
guity due to pro-drop, suggesting that closing the
English-Chinese parsing gap demands techniques
1
The system described  in  this  paper  is  available  from
http://code.google.com/p/berkeley-parser-analyser/
98
beyond those currently used for English. How-
ever, as noted in their final section, their manual
analysis of parse errors in 100 sentences only cov-
ered a portion of a single parser?s output, limiting
the conclusions they could reach regarding the dis-
tribution of errors in Chinese parsing.
2.1 Automatic Error Analysis
Our  analysis  builds  on  Kummerfeld  et al
(2012), which presented a system that automati-
cally classifies English parse errors using a two
stage process. First, the system finds the shortest
path from the system output to the gold annota-
tions, where each step in the path is a tree transfor-
mation, fixing at least one bracket error. Second,
each transformation step is classified into one of
several error types.
When directly applied to Chinese parser output,
the system placed over 27% of the errors in the
catch-all ?Other? type. Many of these errors clearly
fall into one of a small set of error types, motivat-
ing an adaptation to Chinese syntax.
3 Adapting error analysis to Chinese
To adapt the Kummerfeld et al (2012) system to
Chinese, we developed a new version of the second
stage of the system, which assigns an error cate-
gory to each tree transformation step.
To characterize the errors the original system
placed in the ?Other? category, we looked through
one  hundred  sentences, identifying  error  types
generated by Chinese syntax that the existing sys-
tem did not account for. With these observations
we were able to implement new rules to catch the
previously missed cases, leading to the set shown
in Table 1. To ensure the accuracy of our classifica-
tions, we alternated between refining the classifica-
tion code and looking at affected classifications to
identify issues. We also periodically changed the
sentences from the development set we manually
checked, to avoid over-fitting.
Where necessary, we also expanded the infor-
mation available during classification. For exam-
ple, we use the structure of the final gold standard
tree when classifying errors that are a byproduct of
sense disambiguation errors.
4 Chinese parsing errors
Table 1 presents the errors made by the Berkeley
parser. Below we describe the error types that are
Error Type Brackets % of total
NP-internal* 6019 22.70%
Coordination 2781 10.49%
Verb taking wrong args* 2310 8.71%
Unary 2262 8.53%
Modifier Attachment 1900 7.17%
One Word Span 1560 5.88%
Different label 1418 5.35%
Unary A-over-A 1208 4.56%
Wrong sense/bad attach* 1018 3.84%
Noun boundary error* 685 2.58%
VP Attachment 626 2.36%
Clause Attachment 542 2.04%
PP Attachment 514 1.94%
Split Verb Compound* 232 0.88%
Scope error* 143 0.54%
NP Attachment 109 0.41%
Other 3186 12.02%
Table 1: Errors made when parsing Chinese. Values are the
number of bracket errors attributed to that error type. The
values shown are for the Berkeley parser, evaluated on the
development set. * indicates error types that were added or
substantially changed as part of this work.
either new in this analysis, have had their definition
altered, or have an interesting distribution.
2
In all of our results we follow Kummerfeld et al
(2012), presenting the number of bracket errors
(missing or extra)  attributed to each error type.
Bracket counts are more informative than a direct
count of each error type, because the impact on
EVALB F-score varies between errors, e.g. a sin-
gle attachment error can cause 20 bracket errors,
while a unary error causes only one.
NP-internal. (Figure 1a). Unlike  the  Penn
Treebank (Marcus et al, 1993), the PCTB anno-
tates some NP-internal structure. We assign this
error type when a transformation involves words
whose parts of speech in the gold tree are one of:
CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.
We investigated the errors that fall into the NP-
internal category and found that 49% of the errors
involved the creation or deletion of a single pre-
termianl phrasal bracket. These errors arise when
a parser proposes a tree in which POS tags (for in-
stance, JJ or NN) occur as siblings of phrasal tags
(such as NP), a configuration used by the PCTB
bracketing guidelines to indicate complementation
as opposed to adjunction (Xue et al, 2005).
2
For an explanation of the English error types, see Kum-
merfeld et al (2012).
99
Verb taking wrong args. (Figure 1b). This
error type arises when a verb (e.g.?? reverse)
is  hypothesized  to  take  an  incorrect  argument
(?? Bush instead of ?? position). Note that
this also covers some of the errors that Kummer-
feld  et al (2012) classified  as  NP Attachment,
changing the distribution for that type.
Unary. For mis-application of unary rules we
separate out instances in which the two brackets in
the production have the the same label (A-over-A).
This cases is created when traces are eliminated, a
standard step in evaluation. More than a third of
unary errors made by the Berkeley parser are of the
A-over-A type. This can be attributed to two fac-
tors: (i) the PCTB annotates non-local dependen-
cies using traces, and (ii) Chinese syntax generates
more traces than English syntax (Guo et al, 2007).
However, for parsers that do not return traces they
are a benign error.
Modifier attachment. (Figure 1c). Incorrect
modifier scope caused by modifier phrase attach-
ment level. This is less frequent in Chinese than
in English: while English VP modifiers occur in
pre- and post-verbal positions, Chinese only al-
lows pre-verbal modification.
Wrong sense/bad attach. (Figure 1d). This ap-
plies when the head word of a phrase receives the
wrong POS, leading to an attachment error. This
error type is common in Chinese because of POS
fluidity, e.g. the well-known Chinese verb/noun
ambiguity often causes mis-attachments that are
classified as this error type.
In  Figure 1d, the  word ?? invest has
both  noun  and  verb  senses. While  the  gold
standard  interpretation  is  the  relative  clause
firms that Macau invests in, the parser returned an
NP interpretation Macau investment firms.
Noun boundary error. In this error type, a span
is moved to a position where the POS tags of its
new siblings all belong to the list of NP-internal
structure tags which we identified above, reflecting
the inclusion of additional material into an NP.
Split  verb  compound. The  PCTB annota-
tions recognize several Chinese verb compound-
ing strategies, such as  the serial  verb construc-
tion (???? plan [and] build) and the resulta-
tive construction (?? cook [until] done), which
join a bare verb to another lexical item. We in-
troduce an error type specific to Chinese, in which
such verb compounds are split, with the two halves
of the compound placed in different phrases.
..NP
.
.NN .
??
coach
.
.NN .
??
soccer
.
.NN .
??
nat'l
.NP
.
.NP
.
.NP.NN
.
.NP.NN
.
.NP.NN
(a) NP-internal structure errors
..VP
.
.NP
.
.NP .
??
position
.
.DNP
.
.DEG .?
.
.NP .
??
Bush
.
.VV .
??
reverse
.CP
.
.IP
.
.VP
.
.VV
.
.NP
.
.DEC
.
.NP
(b) Verb taking wrong arguments
..VP
.
.VP .
????
win gold
.
.QP
.
.QP .
???
3rd time
.
.ADVP .
??
in a row
.VP
.
.ADVP
.
.QP
.
.QP
.VP
(c) Modifier attachment ambiguity
..CP
.
.NP .
??
firm
.
.IP
.
.VP .
??
invest
.
.NP .
??
Macau
.NP
.
.NP
.
.NP
.
.NP
.NP
(d) Sense confusion
Figure 1: Prominent error types in Chinese parsing. The left
tree is the gold structure; the right is the parser hypothesis.
Scope error. These are cases in which a new
span must be added to more closely bind a modifier
phrase (ADVP, ADJP, and PP).
PP attachment. This error type is rare in Chi-
nese, as adjunct PPs are pre-verbal. It does oc-
cur near coordinated VPs, where ambiguity arises
about  which of  the conjuncts  the PP has scope
over. Whether this particular case is PP attachment
or coordination is debatable; we follow Kummer-
feld et al (2012) and label it PP attachment.
4.1 Chinese-English comparison
It is difficult to directly compare error analysis
results for Chinese and English parsing because
of substantial changes in the classification method,
and differences in treebank annotations.
As described in the previous section, the set of
error categories considered for Chinese is very dif-
ferent to the set of categories for English. Even
for some of the categories that were not substan-
tially changed, errors may be classified differently
because of cross-over between categories between
100
NP Verb Mod. 1-Word Diff Wrong Noun VP Clause PP
System F1 Int. Coord Args Unary Attach Span Label Sense Edge Attach Attach Attach Other
Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87
Berk-G 86.8
Berk-2 81.8
Berk-1 81.1
ZPAR 78.1
Bikel 76.1
Stan-F 76.0
Stan-P 70.0
Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11
Table 2: Error breakdown for the development set of PCTB 6. The area filled in for each bar indicates the average number of
bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in
the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with
two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel),
the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P).
two categories (e.g. between Verb taking wrong
args and NP Attachment).
Differences in treebank annotations also present
a challenge for cross-language error comparison.
The  most  common  error  type  in  Chinese, NP-
internal structure, is rare in the results of Kummer-
feld et al (2012), but the datasets are not compara-
ble because the PTB has very limited NP-internal
structure annotated. Further characterization of the
impact of annotation differences on errors is be-
yond the scope of this paper.
Three conclusions that can be made are that (i)
coordination is a major issue in both languages,
(ii) PP attachment is a much greater problem in
English, and  (iii)  a  higher  frequency  of  trace-
generating syntax in Chinese compared to English
poses substantial challenges.
5 Cross-parser analysis
The previous section described the error types
and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by
showing that the same pattern is observed for sev-
eral  different  parsers  on  the  PCTB 6 dev  set.
3
We include results  for  a  transition-based parser
(ZPAR; Zhang  and  Clark, 2009), a  split-merge
PCFG parser (Petrov et al, 2006; Petrov and Klein,
2007; Petrov, 2010), a lexicalized parser (Bikel
and Chiang, 2000), and a factored PCFG and de-
pendency parser (Levy and Manning, 2003; Klein
and Manning, 2003a,b).
4
Comparing the two Stanford parsers in Table 2,
the factored model provides clear improvements
3
We use the standard data split suggested by the PCTB 6
file manifest. As a result, our results differ from those previ-
ously reported on other splits. All analysis is on the dev set,
to avoid revealing specific information about the test set.
4
These parsers represent a variety of parsing methods,
though exclude some recently developed parsers that are not
publicly available (Qian and Liu, 2012; Xiong et al, 2005).
on  sense  disambiguation, but  performs  slightly
worse on coordination.
The Berkeley product parser we include uses
only two grammars because we found, in contrast
to the English results (Petrov, 2010), that further
grammars provided limited benefits. Comparing
the performance with the standard Berkeley parser
it seems that the diversity in the grammars only as-
sists certain error types, with most of the improve-
ment  occurring in  four  of  the categories, while
there is no improvement, or a slight decrease, in
five categories.
6 Tagging Error Impact
The challenge of accurate POS tagging in Chi-
nese has been a major part of several recent papers
(Qian and Liu, 2012; Jiang et al, 2009; Forst and
Fang, 2009). The Berk-G row of Table 2 shows
the performance of the Berkeley parser when given
gold POS tags.
5
While the F1 improvement is un-
surprising, for the first time we can clearly show
that the gains are only in a subset of the error types.
In particular, tagging improvement will not help
for two of the most significant challenges: coordi-
nation scope errors, and verb argument selection.
To see which tagging confusions contribute to
which error reductions, we adapt the POS ablation
approach of Tse and Curran (2012). We consider
the POS tag pairs shown in Table 3. To isolate the
effects of each confusion we start from the gold
tags and introduce the output of the Stanford tag-
ger whenever it returns one of the two tags being
considered.
6
We then feed these ?semi-gold? tags
5
We used the Berkeley parser as it was the best of the
parsers we considered. Note that the Berkeley parser occa-
sionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative. This leads to a
POS accuracy of 99.35%, which is still well above the parser?s
standard POS accuracy of 93.66%.
6
We introduce errors to gold tags, rather than removing er-
101
Confused tags Errors ? F1
VV NN 1055 -2.72
DEC DEG 526 -1.72
JJ NN 297 -0.57
NR NN 320 -0.05
Table 3: The most frequently confused POS tag pairs. Each
? F1 is relative to Berk-G.
to the Berkeley parser, and run the fine-grained er-
ror analysis on its output.
VV/NN. This confusion has been consistently
shown to be a major contributor to parsing errors
(Levy and Manning, 2003; Tse and Curran, 2012;
Qian and Liu, 2012), and we find a drop of over 2.7
F1 when the output of the tagger is introduced. We
found that while most error types have contribu-
tions from a range of POS confusions, verb/noun
confusion was responsible for virtually all of the
noun boundary errors corrected by using gold tags.
DEG/DEC. This confusion between the rela-
tivizer and subordinator senses of the particle ?
de is the primary source of improvements on mod-
ifier attachment when using gold tags.
NR/NN and JJ/NN. Despite  their  frequency,
these confusions have little effect on parsing per-
formance. Even within the NP-internal error type
their impact is limited, and almost all of the errors
do not change the logical form.
7 Conclusion
We have  quantified  the  relative  impacts  of  a
comprehensive set of error types in Chinese pars-
ing. Our analysis has also shown that while im-
provements in Chinese POS tagging can make a
substantial difference for some error types, it will
not address two high-frequency error types: in-
correct verb argument attachment and coordina-
tion scope. The frequency of these two error types
is also unimproved by the use of products of la-
tent variable grammars. These observations sug-
gest that resolving the core challenges of Chinese
parsing will require new developments that suit the
distinctive properties of Chinese syntax.
Acknowledgments
We extend our thanks to Yue Zhang for helping
us train new ZPAR models. We would also like
to thank the anonymous reviewers for their help-
ful suggestions. This research was supported by
a General Sir John Monash Fellowship to the first
rors from automatic tags, isolating the effect of a single con-
fusion by eliminating interaction between tagging decisions.
author, the Capital Markets CRC under ARC Dis-
covery grant DP1097291, and the NSF under grant
0643742.
References
Daniel M. Bikel and David Chiang. 2000. Two
Statistical Parsing Models Applied to the Chi-
nese Treebank. In Proceedings of the Second
Chinese Language Processing Workshop, pages
1?6. Hong Kong, China.
Martin Forst and Ji Fang. 2009. TBL-improved
non-deterministic  segmentation  and  POS tag-
ging for a Chinese parser. In Proceedings of the
12th Conference of the European Chapter of the
ACL, pages 264?272. Athens, Greece.
Yuqing Guo, Haifeng Wang, and Josef van Gen-
abith. 2007. Recovering Non-Local Dependen-
cies for Chinese. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 257?266. Prague, Czech Republic.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging
? A Case Study. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
volume 1, pages 522?530. Suntec, Singapore.
Dan Klein and Christopher D. Manning. 2003a.
Accurate Unlexicalized Parsing. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 423?430.
Sapporo, Japan.
Dan Klein and Christopher D. Manning. 2003b.
Fast Exact Inference with a Factored Model for
Natural Language Parsing. In Advances in Neu-
ral Information Processing Systems 15, pages
3?10. MIT Press, Cambridge, MA.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser Show-
down at the Wall Street Corral: An Empirical
Investigation of Error Types in Parser Output.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048?1059. Jeju Island, South
Korea.
102
Roger Levy and Christopher Manning. 2003. Is
it harder to parse Chinese, or the Chinese Tree-
bank? In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguis-
tics, pages 439?446. Sapporo, Japan.
Mitchell P.  Marcus, Beatrice  Santorini, and
Mary Ann  Marcinkiewicz.  1993. Building
a  Large  Annotated  Corpus  of  English: The
Penn  Treebank. Computational  Linguistics,
19(2):313?330.
Slav Petrov. 2010. Products of Random Latent
Variable Grammars. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational  Linguistics, pages  19?27.  Los
Angeles, California.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein.  2006. Learning  Accurate, Com-
pact, and Interpretable Tree Annotation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual
Meeting of the Association for Computational
Linguistics, pages 433?440. Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved In-
ference for Unlexicalized Parsing. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics; Proceed-
ings of the Main Conference, pages 404?411.
Rochester, New York, USA.
Xian Qian and Yang Liu.  2012. Joint Chinese
word segmentation, POS tagging and parsing.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 501?511. Jeju Island, Korea.
Daniel  Tse  and  James R.  Curran.  2012. The
Challenges of Parsing Chinese with Combina-
tory Categorial Grammar. In Proceedings of the
2012 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
295?304. Montre?al, Canada.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun
Lin, and Yueliang Qian. 2005. Parsing the Penn
Chinese Treebank with semantic knowledge. In
Proceedings of  the Second international  joint
conference  on  Natural  Language  Processing,
pages 70?81. Jeju Island, Korea.
Nianwen  Xue, Fei  Xia, Fu-Dong  Chiou, and
Martha  Palmer.  2005. The  Penn  Chinese
TreeBank: Phrase  structure  annotation  of  a
large corpus. Natural Language Engineering,
11(2):207?238.
Yue Zhang and Stephen Clark. 2009. Transition-
Based Parsing of the Chinese Treebank using a
Global Discriminative Model. In Proceedings
of the 11th International Conference on Parsing
Technologies (IWPT?09), pages 162?171. Paris,
France.
103
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 9?10,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Variational Inference for Structured NLP Models
David Burkett and Dan Klein
Computer Science Division
University of California, Berkeley
{dburkett,klein}@cs.berkeley.edu
Description
Historically, key breakthroughs in structured NLP
models, such as chain CRFs or PCFGs, have re-
lied on imposing careful constraints on the local-
ity of features in order to permit efficient dynamic
programming for computing expectations or find-
ing the highest-scoring structures. However, as
modern structured models become more complex
and seek to incorporate longer-range features, it is
more and more often the case that performing ex-
act inference is impossible (or at least impractical)
and it is necessary to resort to some sort of approx-
imation technique, such as beam search, pruning,
or sampling. In the NLP community, one increas-
ingly popular approach is the use of variational
methods for computing approximate distributions.
The goal of the tutorial is to provide an intro-
duction to variational methods for approximate in-
ference, particularly mean field approximation and
belief propagation. The intuition behind the math-
ematical derivation of variational methods is fairly
simple: instead of trying to directly compute the
distribution of interest, first consider some effi-
ciently computable approximation of the original
inference problem, then find the solution of the ap-
proximate inference problem that minimizes the
distance to the true distribution. Though the full
derivations can be somewhat tedious, the resulting
procedures are quite straightforward, and typically
consist of an iterative process of individually up-
dating specific components of the model, condi-
tioned on the rest. Although we will provide some
theoretical background, the main goal of the tu-
torial is to provide a concrete procedural guide to
using these approximate inference techniques, il-
lustrated with detailed walkthroughs of examples
from recent NLP literature.
Once both variational inference procedures
have been described in detail, we?ll provide a sum-
mary comparison of the two, along with some in-
tuition about which approach is appropriate when.
We?ll also provide a guide to further exploration of
the topic, briefly discussing other variational tech-
niques, such as expectation propagation and con-
vex relaxations, but concentrating mainly on pro-
viding pointers to additional resources for those
who wish to learn more.
Outline
1. Structured Models and Factor Graphs
? Factor graph notation
? Example structured NLP models
? Inference
2. Mean Field
? Warmup (iterated conditional modes)
? Mean field procedure
? Derivation of mean field update
? Example
3. Structured Mean Field
? Structured approximation
? Computing structured updates
? Example: Joint parsing and alignment
4. Belief Propagation
? Intro
? Messages and beliefs
? Loopy BP
5. Structured Belief Propagation
? Warmup (efficient products for mes-
sages)
? Example: Word alignment
? Example: Dependency parsing
6. Wrap-Up
? Mean field vs BP
? Other approximation techniques
9
Presenter Bios
David Burkett is a postdoctoral researcher in the
Computer Science Division at the University of
California, Berkeley. The main focus of his re-
search is on modeling syntactic agreement in bilin-
gual corpora. His interests are diverse, though, and
he has worked on parsing, phrase alignment, lan-
guage evolution, coreference resolution, and even
video game AI. He has worked as an instructional
assistant for multiple AI courses at Berkeley and
won multiple Outstanding Graduate Student In-
structor awards.
Dan Klein is an Associate Professor of Com-
puter Science at the University of California,
Berkeley. His research includes many areas of
statistical natural language processing, includ-
ing grammar induction, parsing, machine trans-
lation, information extraction, document summa-
rization, historical linguistics, and speech recog-
nition. His academic awards include a Sloan Fel-
lowship, a Microsoft Faculty Fellowship, an NSF
CAREER Award, the ACM Grace Murray Hop-
per Award, Best Paper Awards at ACL, EMNLP
and NAACL, and the UC Berkeley Distinguished
Teaching Award.
10
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 208?217,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Sparser, Better, Faster GPU Parsing
David Hall Taylor Berg-Kirkpatrick John Canny Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,tberg,jfc,klein}@cs.berkeley.edu
Abstract
Due to their origin in computer graph-
ics, graphics processing units (GPUs)
are highly optimized for dense problems,
where the exact same operation is applied
repeatedly to all data points. Natural lan-
guage processing algorithms, on the other
hand, are traditionally constructed in ways
that exploit structural sparsity. Recently,
Canny et al (2013) presented an approach
to GPU parsing that sacrifices traditional
sparsity in exchange for raw computa-
tional power, obtaining a system that can
compute Viterbi parses for a high-quality
grammar at about 164 sentences per sec-
ond on a mid-range GPU. In this work,
we reintroduce sparsity to GPU parsing
by adapting a coarse-to-fine pruning ap-
proach to the constraints of a GPU. The
resulting system is capable of computing
over 404 Viterbi parses per second?more
than a 2x speedup?on the same hard-
ware. Moreover, our approach allows us
to efficiently implement less GPU-friendly
minimum Bayes risk inference, improv-
ing throughput for this more accurate algo-
rithm from only 32 sentences per second
unpruned to over 190 sentences per second
using pruning?nearly a 6x speedup.
1 Introduction
Because NLP models typically treat sentences in-
dependently, NLP problems have long been seen
as ?embarrassingly parallel? ? large corpora can
be processed arbitrarily fast by simply sending dif-
ferent sentences to different machines. However,
recent trends in computer architecture, particularly
the development of powerful ?general purpose?
GPUs, have changed the landscape even for prob-
lems that parallelize at the sentence level. First,
classic single-core processors and main memory
architectures are no longer getting substantially
faster over time, so speed gains must now come
from parallelism within a single machine. Second,
compared to CPUs, GPUs devote a much larger
fraction of their computational power to actual
arithmetic. Since tasks like parsing boil down to
repeated read-multiply-write loops, GPUs should
be many times more efficient in time, power, or
cost. The challenge is that GPUs are not a good
fit for the kinds of sparse computations that most
current CPU-based NLP algorithms rely on.
Recently, Canny et al (2013) proposed a GPU
implementation of a constituency parser that sac-
rifices all sparsity in exchange for the sheer horse-
power that GPUs can provide. Their system uses a
grammar based on the Berkeley parser (Petrov and
Klein, 2007) (which is particularly amenable to
GPU processing), ?compiling? the grammar into a
sequence of GPU kernels that are applied densely
to every item in the parse chart. Together these
kernels implement the Viterbi inside algorithm.
On a mid-range GPU, their system can compute
Viterbi derivations at 164 sentences per second on
sentences of length 40 or less (see timing details
below).
In this paper, we develop algorithms that can
exploit sparsity on a GPU by adapting coarse-to-
fine pruning to a GPU setting. On a CPU, pruning
methods can give speedups of up to 100x. Such
extreme speedups over a dense GPU baseline cur-
rently seem unlikely because fine-grained sparsity
appears to be directly at odds with dense paral-
lelism. However, in this paper, we present a sys-
tem that finds a middle ground, where some level
of sparsity can be maintained without losing the
parallelism of the GPU. We use a coarse-to-fine
approach as in Petrov and Klein (2007), but with
only one coarse pass. Figure 1 shows an overview
of the approach: we first parse densely with a
coarse grammar and then parse sparsely with the
208
fine grammar, skipping symbols that the coarse
pass deemed sufficiently unlikely. Using this ap-
proach, we see a gain of more than 2x over the
dense GPU implementation, resulting in overall
speeds of up to 404 sentences per second. For
comparison, the publicly available CPU imple-
mentation of Petrov and Klein (2007) parses ap-
proximately 7 sentences per second per core on a
modern CPU.
A further drawback of the dense approach in
Canny et al (2013) is that it only computes
Viterbi parses. As with other grammars with
a parse/derivation distinction, the grammars of
Petrov and Klein (2007) only achieve their full
accuracy using minimum-Bayes-risk parsing, with
improvements of over 1.5 F1 over best-derivation
Viterbi parsing on the Penn Treebank (Marcus et
al., 1993). To that end, we extend our coarse-to-
fine GPU approach to computing marginals, along
the way proposing a new way to exploit the coarse
pass to avoid expensive log-domain computations
in the fine pass. We then implement minimum-
Bayes-risk parsing via the max recall algorithm of
Goodman (1996). Without the coarse pass, the
dense marginal computation is not efficient on a
GPU, processing only 32 sentences per second.
However, our approach allows us to process over
190 sentences per second, almost a 6x speedup.
2 A Note on Experiments
We build up our approach incrementally, with ex-
periments interspersed throughout the paper, and
summarized in Tables 1 and 2. In this paper, we
focus our attention on current-generation NVIDIA
GPUs. Many of the ideas described here apply to
other GPUs (such as those from AMD), but some
specifics will differ. All experiments are run with
an NVIDIA GeForce GTX 680, a mid-range GPU
that costs around $500 at time of writing. Unless
otherwise noted, all experiments are conducted on
sentences of length ? 40 words, and we estimate
times based on batches of 20K sentences.
1
We
should note that our experimental condition dif-
fers from that of Canny et al (2013): they evaluate
on sentences of length ? 30. Furthermore, they
1
The implementation of Canny et al (2013) cannot han-
dle batches so large, and so we tested it on batches of 1200
sentences. Our reimplementation is approximately the same
speed for the same batch sizes. For batches of 20K sentences,
we used sentences from the training set. We verified that there
was no significant difference in speed for sentences from the
training set and from the test set.
use two NVIDIA GeForce GTX 690s?each of
which is essentially a repackaging of two 680s?
meaning that our system and experiments would
run approximately four times faster on their hard-
ware. (This expected 4x factor is empirically con-
sistent with the result of running their system on
our hardware.)
3 Sparsity and CPUs
One successful approach for speeding up con-
stituency parsers has been to use coarse-to-fine
inference (Charniak et al, 2006). In coarse-to-
fine inference, we have a sequence of increasingly
complex grammars G
`
. Typically, each succes-
sive grammar G
`
is a refinement of the preceding
grammar G
`?1
. That is, for each symbol A
x
in
the fine grammar, there is some symbol A in the
coarse grammar. For instance, in a latent variable
parser, the coarse grammar would have symbols
like NP , V P , etc., and the fine pass would have
refined symbols NP
0
, NP
1
, V P
4
, and so on.
In coarse-to-fine inference, one applies the
grammars in sequence, computing inside and out-
side scores. Next, one computes (max) marginals
for every labeled span (A, i, j) in a sentence.
These max marginals are used to compute a prun-
ing mask for every span (i, j). This mask is the set
of symbols allowed for that span. Then, in the next
pass, one only processes rules that are licensed by
the pruning mask computed at the previous level.
This approach works because a low quality
coarse grammar can still reliably be used to prune
many symbols from the fine chart without loss of
accuracy. Petrov and Klein (2007) found that over
98% of symbols can be pruned from typical charts
using a simple X-bar grammar without any loss
of accuracy. Thus, the vast majority of rules can
be skipped, and therefore most computation can
be avoided. It is worth pointing out that although
98% of labeled spans can be skipped due to X-bar
pruning, we found that only about 79% of binary
rule applications can be skipped, because the un-
pruned symbols tend to be the ones with a larger
grammar footprint.
4 GPU Architectures
Unfortunately, the standard coarse-to-fine ap-
proach does not na??vely translate to GPU archi-
tectures. GPUs work by executing thousands of
threads at once, but impose the constraint that
large blocks of threads must be executing the same
209
RAM
CPU
GPU
RAM
Instruction Cache
Parse Charts
Work Array
Grammar
Queue
Sentences
Queue
Masks
Masks
Queue
Trees
Figure 1: Overview of the architecture of our system, which is an extension of Canny et al (2013)?s
system. The GPU and CPU communicate via a work queue, which ferries parse items from the CPU to
the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask
that is used by the CPU when deciding which items to queue during the fine pass. The original system
of Canny et al (2013) only used the fine pass, with no pruning.
instructions in lockstep, differing only in their in-
put data. Thus sparsely skipping rules and sym-
bols will not save any work. Indeed, it may ac-
tually slow the system down. In this section, we
provide an overview of GPU architectures, focus-
ing on the details that are relevant to building an
efficient parser.
The large number of threads that a GPU exe-
cutes are packaged into blocks of 32 threads called
warps. All threads in a warp must execute the
same instruction at every clock cycle: if one thread
takes a branch the others do not, then all threads in
the warp must follow both code paths. This situa-
tion is called warp divergence. Because all threads
execute all code paths that any thread takes, time
can only be saved if an entire warp agrees to skip
any particular branch.
NVIDIA GPUs have 8-15 processors called
streaming multi-processors or SMs.
2
Each SM
can process up to 48 different warps at a time:
it interleaves the execution of each warp, so that
when one warp is stalled another warp can exe-
cute. Unlike threads within a single warp, the 48
warps do not have to execute the same instruc-
tions. However, the memory architecture is such
that they will be faster if they access related mem-
ory locations.
2
Older hardware (600 series or older) has 8 SMs. Newer
hardware has more.
A further consideration is that the number of
registers available to a thread in a warp is rather
limited compared to a CPU. On the 600 series,
maximum occupancy can only be achieved if each
thread uses at most 63 registers (Nvidia, 2008).
3
Registers are many times faster than variables lo-
cated in thread-local memory, which is actually
the same speed as global memory.
5 Anatomy of a Dense GPU Parser
This architecture environment puts very different
constraints on parsing algorithms from a CPU en-
vironment. Canny et al (2013) proposed an imple-
mentation of a PCFG parser that sacrifices stan-
dard sparse methods like coarse-to-fine pruning,
focusing instead on maximizing the instruction
and memory throughput of the parser. They as-
sume that they are parsing many sentences at once,
with throughput being more important than la-
tency. In this section, we describe their dense algo-
rithm, which we take as the baseline for our work;
we present it in a way that sets up the changes to
follow.
At the top level, the CPU and GPU communi-
cate via a work queue of parse items of the form
(s, i, k, j), where s is an identifier of a sentence,
i is the start of a span, k is the split point, and j
3
A thread can use more registers than this, but the full
complement of 48 warps cannot execute if too many are used.
210
Clustering Pruning Sent/Sec Speedup
Canny et al ? 164.0 ?
Reimpl ? 192.9 1.0x
Reimpl Empty, Coarse 185.5 0.96x
Reimpl Labeled, Coarse 187.5 0.97x
Parent ? 158.6 0.82x
Parent Labeled, Coarse 278.9 1.4x
Parent Labeled, 1-split 404.7 2.1x
Parent Labeled, 2-split 343.6 1.8x
Table 1: Performance numbers for computing
Viterbi inside charts on 20,000 sentences of length
?40 from the Penn Treebank. All times are
measured on an NVIDIA GeForce GTX 680.
?Reimpl? is our reimplementation of their ap-
proach. Speedups are measured in reference to this
reimplementation. See Section 7 for discussion of
the clustering algorithms and Section 6 for a de-
scription of the pruning methods. The Canny et al
(2013) system is benchmarked on a batch size of
1200 sentences, the others on 20,000.
is the end point. The GPU takes large numbers of
parse items and applies the entire grammar to them
in parallel. These parse items are enqueued in or-
der of increasing span size, blocking until all items
of a given length are complete. This approach is
diagrammed in Figure 2.
Because all rules are applied to all parse items,
all threads are executing the same sequence of in-
structions. Thus, there is no concern of warp di-
vergence.
5.1 Grammar Compilation
One important feature of Canny et al (2013)?s sys-
tem is grammar compilation. Because registers
are so much faster than thread-local memory, it
is critical to keep as many variables in registers
as possible. One way to accomplish this is to un-
roll loops at compilation time. Therefore, they in-
lined the iteration over the grammar directly into
the GPU kernels (i.e. the code itself), which al-
lows the compiler to more effectively use all of its
registers.
However, register space is limited on GPUs.
Because the Berkeley grammar is so large, the
compiler is not able to efficiently schedule all of
the operations in the grammar, resulting in regis-
ter spills. Canny et al (2013) found they had to
partition the grammar into multiple different ker-
nels. We discuss this partitioning in more detail in
Section 7. However, in short, the entire grammar
G is broken into multiple clusters G
i
where each
rule belongs to exactly one cluster.
NP
DT NN
VB
VP
NP
NP
PP
IN
NP
S
VP
(0, 1, 3)
(0, 2, 3)
(1, 2, 4)
(1, 3, 4)
(2, 3, 5)
(2, 4, 5)
Grammar
Queue
(i, k, j)
Figure 2: Schematic representation of the work
queue used in Canny et al (2013). The Viterbi
inside loop for the grammar is inlined into a ker-
nel. The kernel is applied to all items in the queue
in a blockwise manner.
NP
DT NN
NP
DT NN
NP
DT NN
NP
NP
PP
IN
NP
PP
IN
NP
PP
IN
PP
VB
VP
NP
VB
VP
NP
VB
VP
NP
VP
(0, 1, 3)
(1, 2, 4)
(3, 5, 6)
(1, 3, 4)
(1, 2, 4)
(0, 2, 3)
(2, 4, 5)
(3, 4, 6)
Queues
(i, k, j)
Grammar Clusters
Figure 3: Schematic representation of the work
queue and grammar clusters used in the fine pass
of our work. Here, the rules of the grammar are
clustered by their coarse parent symbol. We then
have multiple work queues, with parse items only
being enqueued if the span (i, j) allows that sym-
bol in its pruning mask.
All in all, Canny et al (2013)?s system is able
to compute Viterbi charts at 164 sentences per sec-
ond, for sentences up to length 40. On larger batch
sizes, our reimplementation of their approach is
able to achieve 193 sentences per second on the
same hardware. (See Table 1.)
6 Pruning on a GPU
Now we turn to the algorithmic and architectural
changes in our approach. First, consider trying to
211
directly apply the coarse-to-fine method sketched
in Section 3 to the dense baseline described above.
The natural implementation would be for each
thread to check if each rule is licensed before
applying it. However, we would only avoid the
work of applying the rule if all threads in the warp
agreed to skip it. Since each thread in the warp is
processing a different span (perhaps even from a
different sentence), consensus from all 32 threads
on any skip would be unlikely.
Another approach would be to skip enqueu-
ing any parse item (s, i, k, j) where the pruning
mask for any of (i, j), (i, k), or (k, j) is entirely
empty (i.e. all symbols are pruned in this cell by
the coarse grammar). However, our experiments
showed that only 40% of parse items are pruned in
this manner. Because of the overhead associated
with creating pruning masks and the further over-
head of GPU communication, we found that this
method did not actually produce any time savings
at all. The result is a parsing speed of 185.5 sen-
tences per second, as shown in Table 1 on the row
labeled ?Reimpl? with ?Empty, Coarse? pruning.
Instead, we take advantage of the partitioned
structure of the grammar and organize our com-
putation around the coarse symbol set. Recall that
the baseline already partitions the grammar G into
rule clusters G
i
to improve register sharing. (See
Section 7 for more on the baseline clustering.) We
create a separate work queue for each partition.
We call each such queue a labeled work queue, and
each one only queues items to which some rule in
the corresponding partition applies. We call the set
of coarse symbols for a partition (and therefore the
corresponding labeled work queue) a signature.
During parsing, we only enqueue items
(s, i, k, j) to a labeled queue if two conditions are
met. First, the span (i, j)?s pruning mask must
have a non-empty intersection with the signature
of the queue. Second, the pruning mask for the
children (i, k) and (k, j) must be non-empty.
Once on the GPU, parse items are processed us-
ing the same style of compiled kernel as in Canny
et al (2013). Because the entire partition (though
not necessarily the entire grammar) is applied to
each item in the queue, we still do not need to
worry about warp divergence.
At the top level, our system first computes prun-
ing masks with a coarse grammar. Then it pro-
cesses the same sentences with the fine gram-
mar. However, to the extent that the signatures
are small, items can be selectively queued only to
certain queues. This approach is diagrammed in
Figure 3.
We tested our new pruning approach using an
X-bar grammar as the coarse pass. The result-
ing speed is 187.5 sentences per second, labeled
in Table 1 as row labeled ?Reimpl? with ?Labeled,
Coarse? pruning. Unfortunately, this approach
again does not produce a speedup relative to our
reimplemented baseline. To improve upon this re-
sult, we need to consider how the grammar clus-
tering interacts with the coarse pruning phase.
7 Grammar Clustering
Recall that the rules in the grammar are partitioned
into a set of clusters, and that these clusters are
further divided into subclusters. How can we best
cluster and subcluster the grammar so as to maxi-
mize performance? A good clustering will group
rules together that use the same symbols, since
this means fewer memory accesses to read and
write scores for symbols. Moreover, we would
like the time spent processing each of the subclus-
ters within a cluster to be about the same. We can-
not move on to the next cluster until all threads
from a cluster are finished, which means that the
time a cluster takes is the amount of time taken
by the longest-running subcluster. Finally, when
pruning, it is best if symbols that have the same
coarse projection are clustered together. That way,
we are more likely to be able to skip a subcluster,
since fewer distinct symbols need to be ?off? for a
parse item to be skipped in a given subcluster.
Canny et al (2013) clustered symbols of the
grammar using a sophisticated spectral clustering
algorithm to obtain a permutation of the symbols.
Then the rules of the grammar were laid out in
a (sparse) three-dimensional tensor, with one di-
mension representing the parent of the rule, one
representing the left child, and one representing
the right child. They then split the cube into 6x2x2
contiguous ?major cubes,? giving a partition of the
rules into 24 clusters. They then further subdi-
vided these cubes into 2x2x2 minor cubes, giv-
ing 8 subclusters that executed in parallel. Note
that the clusters induced by these major and minor
cubes need not be of similar sizes; indeed, they of-
ten are not. Clustering using this method is labeled
?Reimplementation? in Table 1.
The addition of pruning introduces further con-
siderations. First, we have a coarse grammar, with
212
many fewer rules and symbols. Second, we are
able to skip a parse item for an entire cluster if that
item?s pruning mask does not intersect the clus-
ter?s signature. Spreading symbols across clusters
may be inefficient: if a parse item licenses a given
symbol, we will have to enqueue that item to any
queue that has the symbol in its signature, no mat-
ter how many other symbols are in that cluster.
Thus, it makes sense to choose a clustering al-
gorithm that exploits the structure introduced by
the pruning masks. We use a very simple method:
we cluster the rules in the grammar by coarse par-
ent symbol. When coarse symbols are extremely
unlikely (and therefore have few corresponding
rules), we merge their clusters to avoid the over-
head of beginning work on clusters where little
work has to be done.
4
In order to subcluster, we
divide up rules among subclusters so that each
subcluster has the same number of active parent
symbols. We found this approach to subclustering
worked well in practice.
Clustering using this method is labeled ?Parent?
in Table 1. Now, when we use a coarse pruning
pass, we are able to parse nearly 280 sentences
per second, a 70% increase in parsing performance
relative to Canny et al (2013)?s system, and nearly
50% over our reimplemented baseline.
It turns out that this simple clustering algorithm
produces relatively efficient kernels even in the un-
pruned case. The unpruned Viterbi computations
in a fine grammar using the clustering method of
Canny et al (2013) yields a speed of 193 sen-
tences per second, whereas the same computation
using coarse parent clustering has a speed of 159
sentences per second. (See Table 1.) This is not
as efficient as Canny et al (2013)?s highly tuned
method, but it is still fairly fast, and much simpler
to implement.
8 Pruning with Finer Grammars
The coarse to fine pruning approach of Petrov and
Klein (2007) employs an X-bar grammar as its
first pruning phase, but there is no reason why
we cannot begin with a more complex grammar
for our initial pass. As Petrov and Klein (2007)
have shown, intermediate-sized Berkeley gram-
mars prune many more symbols than the X-bar
system. However, they are slower to parse with
4
Specifically, after clustering based on the coarse parent
symbol, we merge all clusters with less than 300 rules in them
into one large cluster.
in a CPU context, and so they begin with an X-bar
grammar.
Because of the overhead associated with trans-
ferring work items to GPU, using a very small
grammar may not be an efficient use of the GPU?s
computational resources. To that end, we tried
computing pruning masks with one-split and two-
split Berkeley grammars. The X-bar grammar can
compute pruning masks at just over 1000 sen-
tences per second, the 1-split grammar parses 858
sentences per second, and the 2-split grammar
parses 526 sentences per second.
Because parsing with these grammars is still
quite fast, we tried using them as the coarse pass
instead. As shown in Table 1, using a 1-split gram-
mar as a coarse pass allows us to produce over 400
sentences per second, a full 2x improvement over
our original system. Conducting a coarse pass
with a 2-split grammar is somewhat slower, at a
?mere? 343 sentences per second.
9 Minimum Bayes risk parsing
The Viterbi algorithm is a reasonably effective
method for parsing. However, many authors
have noted that parsers benefit substantially from
minimum Bayes risk decoding (Goodman, 1996;
Simaan, 2003; Matsuzaki et al, 2005; Titov and
Henderson, 2006; Petrov and Klein, 2007). MBR
algorithms for parsing do not compute the best
derivation, as in Viterbi parsing, but instead the
parse tree that maximizes the expected count of
some figure of merit. For instance, one might want
to maximize the expected number of correct con-
stituents (Goodman, 1996), or the expected rule
counts (Simaan, 2003; Petrov and Klein, 2007).
MBR parsing has proven especially useful in la-
tent variable grammars. Petrov and Klein (2007)
showed that MBR trees substantially improved
performance over Viterbi parses for latent variable
grammars, earning up to 1.5F1.
Here, we implement the Max Recall algorithm
of Goodman (1996). This algorithm maximizes
the expected number of correct coarse symbols
(A, i, j) with respect to the posterior distribution
over parses for a sentence.
This particular MBR algorithm has the advan-
tage that it is relatively straightforward to imple-
ment. In essence, we must compute the marginal
probability of each fine-labeled span ?(A
x
, i, j),
and then marginalize to obtain ?(A, i, j). Then,
for each span (i, j), we find the best possible split
213
point k that maximizes C(i, j) = ?(A, i, j) +
max
k
(C(i, k) + C(k, j)). Parse extraction is
then just a matter of following back pointers from
the root, as in the Viterbi algorithm.
9.1 Computing marginal probabilities
The easiest way to compute marginal probabilities
is to use the log space semiring rather than the
Viterbi semiring, and then to run the inside and
outside algorithms as before. We should expect
this algorithm to be at least a factor of two slower:
the outside pass performs at least as much work as
the inside pass. Moreover, it typically has worse
memory access patterns, leading to slower perfor-
mance.
Without pruning, our approach does not han-
dle these log domain computations well at all:
we are only able to compute marginals for 32.1
sentences/second, more than a factor of 5 slower
than our coarse pass. To begin, log space addition
requires significantly more operations than max,
which is a primitive operation on GPUs. Beyond
the obvious consequence that executing more op-
erations means more time taken, the sheer number
of operations becomes too much for the compiler
to handle. Because the grammars are compiled
into code, the additional operations are all inlined
into the kernels, producing much larger kernels.
Indeed, in practice the compiler will often hang if
we use the same size grammar clusters as we did
for Viterbi. In practice, we found there is an effec-
tive maximum of 2000 rules per kernel using log
sums, while we can use more than 10,000 rules
rules in a single kernel with Viterbi.
With coarse pruning, however, we can avoid
much of the increased cost associated with log
domain computations. Because so many labeled
spans are pruned, we are able to skip many of the
grammar clusters and thus avoid many of the ex-
pensive operations. Using coarse pruning and log
domain calculations, our system produces MBR
trees at a rate of 130.4 sentences per second, a
four-fold increase.
9.2 Scaling with the Coarse Pass
One way to avoid the expense of log domain com-
putations is to use scaled probabilities rather than
log probabilities. Scaling is one of the folk tech-
niques that are commonly used in the NLP com-
munity, but not generally written about. Recall
that floating point numbers are composed of a
mantissa m and an exponent e, giving a number
System Sent/Sec Speedup
Unpruned Log Sum MBR 32.1 ?
Pruned Log Sum MBR 130.4 4.1x
Pruned Scaling MBR 190.6 5.9x
Pruned Viterbi 404.7 12.6x
Table 2: Performance numbers for computing max
constituent (Goodman, 1996) trees on 20,000 sen-
tences of length 40 or less from the Penn Tree-
bank. For convenience, we have copied our pruned
Viterbi system?s result.
f = m ? 2
e
. When a float underflows, the ex-
ponent becomes too low to represent the available
number of bits. In scaling, floating point numbers
are paired with an additional number that extends
the exponent. That is, the number is represented
as f
?
= f ? exp(s). Whenever f becomes either
too big or too small, the number is rescaled back
to a less ?dangerous? range by shifting mass from
the exponent e to the scaling factor s.
In practice, one scale s is used for an entire span
(i, j), and all scores for that span are rescaled in
concert. In our GPU system, multiple scores in
any given span are being updated at the same time,
which makes this dynamic rescaling tricky and ex-
pensive, especially since inter-warp communica-
tion is fairly limited.
We propose a much simpler static solution that
exploits the coarse pass. In the coarse pass, we
compute Viterbi inside and outside scores for ev-
ery span. Because the grammar used in the coarse
pass is a projection of the grammar used in the
fine pass, these coarse scores correlate reasonably
closely with the probabilities computed in the fine
pass: If a span has a very high or very low score
in the coarse pass, it typically has a similar score
in the fine pass. Thus, we can use the coarse
pass?s inside and outside scores as the scaling val-
ues for the fine pass?s scores. That is, in addition
to computing a pruning mask, in the coarse pass
we store the maximum inside and outside score in
each span, giving two arrays of scores s
I
i,j
and s
O
i,j
.
Then, when applying rules in the fine pass, each
fine inside score over a split span (i, k, j) is scaled
to the appropriate s
I
i,j
by multiplying the score by
exp
(
s
I
i,k
+ s
I
k,j
? s
I
i,j
)
, where s
I
i,k
, s
I
k,j
, s
I
i,j
are
the scaling factors for the left child, right child,
and parent, respectively. The outside scores are
scaled analogously.
By itself, this approach works on nearly ev-
ery sentence. However, scores for approximately
214
0.5% of sentences overflow (sic). Because we are
summing instead of maxing scores in the fine pass,
the scaling factors computed using max scores are
not quite large enough, and so the rescaled inside
probabilities grow too large when multiplied to-
gether. Most of this difference arises at the leaves,
where the lexicon typically has more uncertainty
than higher up in the tree. Therefore, in the fine
pass, we normalize the inside scores at the leaves
to sum to 1.0.
5
Using this slight modification, no
sentences from the Treebank under- or overflow.
We know of no reason why this same trick can-
not be employed in more traditional parsers, but
it is especially useful here: with this static scal-
ing, we can avoid the costly log sums without in-
troducing any additional inter-thread communica-
tion, making the kernels much smaller and much
faster. Using scaling, we are able to push our
parser to 190.6 sentences/second for MBR extrac-
tion, just under half the speed of the Viterbi sys-
tem.
9.3 Parsing Accuracies
It is of course important verify the correctness of
our system; one easy way to do so is to exam-
ine parsing accuracy, as compared to the original
Berkeley parser. We measured parsing accuracy
on sentences of length? 40 from section 22 of the
Penn Treebank. Our Viterbi parser achieves 89.7
F1, while our MBR parser scores 91.0. These re-
sults are nearly identical to the Berkeley parsers
most comparable numbers: 89.8 for Viterbi, and
90.9 for their ?Max-Rule-Sum? MBR algorithm.
These slight differences arise from the usual mi-
nor variation in implementation details. In partic-
ular, we use one coarse pass instead of several, and
a different MBR algorithm. In addition, there are
some differences in unary processing.
10 Analyzing System Performance
In this section we attempt to break down how ex-
actly our system is spending its time. We do this in
an effort to give a sense of how time is spent dur-
ing computation on GPUs. These timing numbers
are computed using the built-in profiling capabil-
ities of the programming environment. As usual,
profiles exhibit an observer effect, where the act of
measuring the system changes the execution. Nev-
5
One can instead interpret this approach as changing the
scaling factors to s
I
?
i,j
= s
I
i,j
?
?
i?k<j
?
A
inside(A, k, k +
1), where inside is the array of scores for the fine pass.
System Coarse Pass Fine Pass
Unpruned Viterbi ? 6.4
Pruned Viterbi 1.2 1.5
Unpruned Logsum MBR ? 28.6
Pruned Scaling MBR 1.2 4.3
Table 3: Time spent in the passes of our differ-
ent systems, in seconds per 1000 sentences. Prun-
ing refers to using a 1-split grammar for the coarse
pass.
ertheless, the general trends should more or less be
preserved as compared to the unprofiled code.
To begin, we can compute the number of sec-
onds needed to parse 1000 sentences. (We use sec-
onds per sentence rather than sentences per second
because the former measure is additive.) The re-
sults are in Table 3. In the case of pruned Viterbi,
pruning reduces the amount of time spent in the
fine pass by more than 4x, though half of those
gains are lost to computing the pruning masks.
In Table 4, we break down the time taken by
our system into individual components. As ex-
pected, binary rules account for the vast majority
of the time in the unpruned Viterbi case, but much
less time in the pruned case, with the total time
taken for binary rules in the coarse and fine passes
taking about 1/5 of the time taken by binaries in
the unpruned version. Queueing, which involves
copying memory around within the GPU to pro-
cess the individual parse items, takes a fairly con-
sistent amount of time in all systems. Overhead,
which includes transport time between the CPU
and GPU and other processing on the CPU, is rela-
tively small for most system configurations. There
is greater overhead in the scaling system, because
scaling factors are copied to the CPU between the
coarse and fine passes.
A final question is: how many sentences per
second do we need to process to saturate the
GPU?s processing power? We computed Viterbi
parses of successive powers of 10, from 1 to
100,000 sentences.
6
In Figure 4, we then plotted
the throughput, in terms of number of sentences
per second. Throughput increases through parsing
10,000 sentences, and then levels off by the time it
reaches 100,000 sentences.
6
We replicated the Treebank for the 100,000 sentences
pass.
215
System Coarse Pass Fine Pass
Binary Unary Queueing Masks Overhead Binary Unary Queueing Overhead
Unpruned Viterbi ? ? ? ? ? 5.42 0.14 0.33 0.40
Pruned Viterbi 0.59 0.02 0.19 0.04 0.22 0.56 0.10 0.34 0.22
Pruned Scaling 0.59 0.02 0.19 0.04 0.20 1.74 0.24 0.46 0.84
Table 4: Breakdown of time spent in our different systems, in seconds per 1000 sentences. Binary and
Unary refer to spent processing binary rules. Queueing refers to the amount of time used to move memory
around within the GPU for processing. Overhead includes all other time, which includes communication
between the GPU and the CPU.
Sentenc
es/Seco
nd
0
100
200
300
400
Number of Sentences1 10 100 1K 10K 100K
Figure 4: Plot of speeds (sentences / second) for
various sizes of input corpora. The full power of
the GPU parser is only reached when run on large
numbers of sentences.
11 Related Work
Apart from the model of Canny et al (2013), there
have been a few attempts at using GPUs in NLP
contexts before. Johnson (2011) and Yi et al
(2011) both had early attempts at porting pars-
ing algorithms to the GPU. However, they did
not demonstrate significantly increased speed over
a CPU implementation. In machine translation,
He et al (2013) adapted algorithms designed for
GPUs in the computational biology literature to
speed up on-demand phrase table extraction.
12 Conclusion
GPUs represent a challenging opportunity for nat-
ural language processing. By carefully design-
ing within the constraints imposed by the architec-
ture, we have created a parser that can exploit the
same kinds of sparsity that have been developed
for more traditional architectures.
One of the key remaining challenges going
forward is confronting the kind of lexicalized
sparsity common in other NLP models. The
Berkeley parser?s grammars?by virtue of being
unlexicalized?can be applied uniformly to all
parse items. The bilexical features needed by
dependency models and lexicalized constituency
models are not directly amenable to acceleration
using the techniques we described here. Deter-
mining how to efficiently implement these kinds
of models is a promising area for new research.
Our system is available as open-source at
https://www.github.com/dlwh/puck.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
John Canny, David Hall, and Dan Klein. 2013. A
multi-teraflop constituency parser using GPUs. In
Proceedings of EMNLP, pages 1898?1907, October.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R Shrivaths, Jeremy Moore, Michael Pozar,
et al 2006. Multilevel coarse-to-fine pcfg pars-
ing. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 168?175. Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In ACL, pages 177?183.
Hua He, Jimmy Lin, and Adam Lopez. 2013. Mas-
sively parallel suffix array queries and on-demand
phrase extraction for statistical machine translation
using gpus. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 325?334, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.
Mark Johnson. 2011. Parsing in parallel on multiple
cores and gpus. In Proceedings of the Australasian
Language Technology Association Workshop.
216
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
CUDA Nvidia. 2008. Programming guide.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Khalil Simaan. 2003. On maximizing metrics for syn-
tactic disambiguation. In Proceedings of IWPT.
Ivan Titov and James Henderson. 2006. Loss min-
imization in parse reranking. In Proceedings of
EMNLP, pages 560?567. Association for Computa-
tional Linguistics.
Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efficient parallel cky parsing on
gpus. In Proceedings of the 2011 Conference on
Parsing Technologies, Dublin, Ireland, October.
217
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 228?237,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Less Grammar, More Features
David Hall Greg Durrett Dan Klein
Computer Science Division
University of California, Berkeley
{dlwh,gdurrett,klein}@cs.berkeley.edu
Abstract
We present a parser that relies primar-
ily on extracting information directly from
surface spans rather than on propagat-
ing information through enriched gram-
mar structure. For example, instead of cre-
ating separate grammar symbols to mark
the definiteness of an NP, our parser might
instead capture the same information from
the first word of the NP. Moving context
out of the grammar and onto surface fea-
tures can greatly simplify the structural
component of the parser: because so many
deep syntactic cues have surface reflexes,
our system can still parse accurately with
context-free backbones as minimal as X-
bar grammars. Keeping the structural
backbone simple and moving features to
the surface also allows easy adaptation
to new languages and even to new tasks.
On the SPMRL 2013 multilingual con-
stituency parsing shared task (Seddah et
al., 2013), our system outperforms the top
single parser system of Bj?orkelund et al
(2013) on a range of languages. In addi-
tion, despite being designed for syntactic
analysis, our system also achieves state-
of-the-art numbers on the structural senti-
ment task of Socher et al (2013). Finally,
we show that, in both syntactic parsing and
sentiment analysis, many broad linguistic
trends can be captured via surface features.
1 Introduction
Na??ve context-free grammars, such as those em-
bodied by standard treebank annotations, do not
parse well because their symbols have too little
context to constrain their syntactic behavior. For
example, to PPs usually attach to verbs and of
PPs usually attach to nouns, but a context-free PP
symbol can equally well attach to either. Much
of the last few decades of parsing research has
therefore focused on propagating contextual in-
formation from the leaves of the tree to inter-
nal nodes. For example, head lexicalization (Eis-
ner, 1996; Collins, 1997; Charniak, 1997), struc-
tural annotation (Johnson, 1998; Klein and Man-
ning, 2003), and state-splitting (Matsuzaki et al,
2005; Petrov et al, 2006) are all designed to take
coarse symbols like PP and decorate them with
additional context. The underlying reason that
such propagation is even needed is that PCFG
parsers score trees based on local configurations
only, and any information that is not threaded
through the tree becomes inaccessible to the scor-
ing function. There have been non-local ap-
proaches as well, such as tree-substitution parsers
(Bod, 1993; Sima?an, 2000), neural net parsers
(Henderson, 2003), and rerankers (Collins and
Koo, 2005; Charniak and Johnson, 2005; Huang,
2008). These non-local approaches can actually
go even further in enriching the grammar?s struc-
tural complexity by coupling larger domains in
various ways, though their non-locality generally
complicates inference.
In this work, we instead try to minimize the
structural complexity of the grammar by moving
as much context as possible onto local surface fea-
tures. We examine the position that grammars
should not propagate any information that is avail-
able from surface strings, since a discriminative
parser can access that information directly. We
therefore begin with a minimal grammar and it-
eratively augment it with rich input features that
do not enrich the context-free backbone. Previ-
ous work has also used surface features in their
parsers, but the focus has been on machine learn-
ing methods (Taskar et al, 2004), latent annota-
tions (Petrov and Klein, 2008a; Petrov and Klein,
2008b), or implementation (Finkel et al, 2008).
By contrast, we investigate the extent to which
228
we need a grammar at all. As a thought experi-
ment, consider a parser with no grammar, which
functions by independently classifying each span
(i, j) of a sentence as an NP, VP, and so on, or
null if that span is a non-constituent. For exam-
ple, spans that begin with the might tend to be
NPs, while spans that end with of might tend to
be non-constituents. An independent classification
approach is actually very viable for part-of-speech
tagging (Toutanova et al, 2003), but is problem-
atic for parsing ? if nothing else, parsing comes
with a structural requirement that the output be a
well-formed, nested tree. Our parser uses a min-
imal PCFG backbone grammar to ensure a ba-
sic level of structural well-formedness, but relies
mostly on features of surface spans to drive accu-
racy. Formally, our model is a CRF where the fea-
tures factor over anchored rules of a small back-
bone grammar, as shown in Figure 1.
Some aspects of the parsing problem, such as
the tree constraint, are clearly best captured by a
PCFG. Others, such as heaviness effects, are nat-
urally captured using surface information. The
open question is whether surface features are ade-
quate for key effects like subcategorization, which
have deep definitions but regular surface reflexes
(e.g. the preposition selected by a verb will often
linearly follow it). Empirically, the answer seems
to be yes, and our system produces strong results,
e.g. up to 90.5 F1 on English parsing. Our parser
is also able to generalize well across languages
with little tuning: it achieves state-of-the-art re-
sults on multilingual parsing, scoring higher than
the best single-parser system from the SPMRL
2013 Shared Task on a range of languages, as well
as on the competition?s average F1 metric.
One advantage of a system that relies on surface
features and a simple grammar is that it is portable
not only across languages but also across tasks
to an extent. For example, Socher et al (2013)
demonstrates that sentiment analysis, which is
usually approached as a flat classification task,
can be viewed as tree-structured. In their work,
they propagate real-valued vectors up a tree using
neural tensor nets and see gains from their recur-
sive approach. Our parser can be easily adapted
to this task by replacing the X-bar grammar over
treebank symbols with a grammar over the sen-
timent values to encode the output variables and
then adding n-gram indicators to our feature set
to capture the bulk of the lexical effects. When
applied to this task, our system generally matches
their accuracy overall and is able to outperform it
on the overall sentence-level subtask.
2 Parsing Model
In order to exploit non-independent surface fea-
tures of the input, we use a discriminative formula-
tion. Our model is a conditional random field (Laf-
ferty et al, 2001) over trees, in the same vein as
Finkel et al (2008) and Petrov and Klein (2008a).
Formally, we define the probability of a tree T
conditioned on a sentence w as
p(T |w) ? exp
(
?
?
?
r?T
f(r,w)
)
(1)
where the feature domains r range over the (an-
chored) rules used in the tree. An anchored rule
r is the conjunction of an unanchored grammar
rule rule(r) and the start, stop, and split indexes
where that rule is anchored, which we refer to as
span(r). It is important to note that the richness of
the backbone grammar is reflected in the structure
of the trees T , while the features that condition di-
rectly on the input enter the equation through the
anchoring span(r). To optimize model parame-
ters, we use the Adagrad algorithm of Duchi et al
(2010) with L2 regularization.
We start with a simple X-bar grammar whose
only symbols are NP, NP-bar, VP, and so on. Our
base model has no surface features: formally, on
each anchored rule r we have only an indicator of
the (unanchored) rule identity, rule(r). Because
the X-bar grammar is so minimal, this grammar
does not parse very accurately, scoring just 73 F1
on the standard English Penn Treebank task.
In past work that has used tree-structured CRFs
in this way, increased accuracy partially came
from decorating trees T with additional annota-
tions, giving a tree T
?
over a more complex symbol
set. These annotations introduce additional con-
text into the model, usually capturing linguistic in-
tuition about the factors that influence grammati-
cality. For instance, we might annotate every con-
stituent X in the tree with its parent Y , giving a
tree with symbolsX[?Y ]. Finkel et al (2008) used
parent annotation, head tag annotation, and hori-
zontal sibling annotation together in a single large
grammar. In Petrov and Klein (2008a) and Petrov
and Klein (2008b), these annotations were latent;
they were inferred automatically during training.
229
Hall and Klein (2012) employed both kinds of an-
notations, along with lexicalized head word anno-
tation. All of these past CRF parsers do also ex-
ploit span features, as did the structured margin
parser of Taskar et al (2004); the current work pri-
marily differs in shifting the work from the gram-
mar to the surface features.
The problem with rich annotations is that they
increase the state space of the grammar substan-
tially. For example, adding parent annotation can
square the number of symbols, and each subse-
quent annotation causes a multiplicative increase
in the size of the state space. Hall and Klein
(2012) attempted to reduce this state space by fac-
toring these annotations into individual compo-
nents. Their approach changed the multiplicative
penalty of annotation into an additive penalty, but
even so their individual grammar projections are
much larger than the base X-bar grammar.
In this work, we want to see how much of the
expressive capability of annotations can be cap-
tured using surface evidence, with little or no an-
notation of the underlying grammar. To that end,
we avoid annotating our trees at all, opting instead
to see how far simple surface features will go in
achieving a high-performance parser. We will re-
turn to the question of annotation in Section 5.
3 Surface Feature Framework
To improve the performance of our X-bar gram-
mar, we will add a number of surface feature tem-
plates derived only from the words in the sentence.
We say that an indicator is a surface property if
it can be extracted without reference to the parse
tree. These features can be implemented with-
out reference to structured linguistic notions like
headedness; however, we will argue that they still
capture a wide range of linguistic phenomena in a
data-driven way.
Throughout this and the following section, we
will draw on motivating examples from the En-
glish Penn Treebank, though similar examples
could be equally argued for other languages. For
performance on other languages, see Section 6.
Recall that our CRF factors over anchored rules
r, where each r has identity rule(r) and anchor-
ing span(r). The X-bar grammar has only indi-
cators of rule(r), ignoring the anchoring. Let a
surface property of r be an indicator function of
span(r) and the sentence itself. For example, the
first word in a constituent is a surface property, as
averted    financial    disaster
VP
NP
VBD
JJ NN
PARENT = VP
FIRSTWORD = averted
LENGTH = 3
RULE = VP ? VBD NP
PARENT = VP
Span properties
Rule backoffs
Features
...
5 6
7
8
...
LASTWORD = disaster
?
FIRSTWORD = averted
LASTWORD = disaster PARENT = VP
?
?
FIRSTWORD = averted
RULE = VP ? VBD NP
Figure 1: Features computed over the application
of the rule VP ? VBD NP over the anchored
span averted financial disaster with the shown in-
dices. Span properties are generated as described
throughout Section 4; they are then conjoined with
the rule and just the parent nonterminal to give the
features fired over the anchored production.
is the word directly preceding the constituent. As
illustrated in Figure 1, the actual features of the
model are obtained by conjoining surface proper-
ties with various abstractions of the rule identity.
For rule abstractions, we use two templates: the
parent of the rule and the identity of the rule. The
surface features are somewhat more involved, and
so we introduce them incrementally.
One immediate computational and statistical is-
sue arises from the sheer number of possible sur-
face features. There are a great number of spans
in a typical treebank; extracting features for ev-
ery possible combination of span and rule is pro-
hibitive. One simple solution is to only extract
features for rule/span pairs that are actually ob-
served in gold annotated examples during train-
ing. Because these ?positive? features correspond
to observed constituents, they are far less numer-
ous than the set of all possible features extracted
from all spans. As far as we can tell, all past CRF
parsers have used ?positive? features only.
However, negative features?features that are
not observed in any tree?are still powerful indica-
tors of (un)grammaticality: if we have never seen
a PRN that starts with ?has,? or a span that be-
gins with a quotation mark and ends with a close
bracket, then we would like the model to be able to
place negative weights on these features. Thus, we
use a simple feature hashing scheme where posi-
tive features are indexed individually, while nega-
230
Features Section F1
RULE 4 73.0
+ SPAN FIRST WORD + SPAN LAST WORD + LENGTH 4.1 85.0
+ WORD BEFORE SPAN + WORD AFTER SPAN 4.2 89.0
+ WORD BEFORE SPLIT + WORD AFTER SPLIT 4.3 89.7
+ SPAN SHAPE 4.4 89.9
Table 1: Results for the Penn Treebank development set, reported in F1 on sentences of length ? 40
on Section 22, for a number of incrementally growing feature sets. We show that each feature type
presented in Section 4 adds benefit over the previous, and in combination they produce a reasonably
good yet simple parser.
tive features are bucketed together. During train-
ing there are no collisions between positive fea-
tures, which generally receive positive weight, and
negative features, which generally receive nega-
tive weight; only negative features can collide.
Early experiments indicated that using a number
of negative buckets equal to the number of posi-
tive features was effective.
4 Features
Our goal is to use surface features to replicate
the functionality of other annotations, without in-
creasing the state space of our grammar, meaning
that the rules rule(r) remain simple, as does the
state space used during inference.
Before we present our main features, we briefly
discuss the issue of feature sparsity. While lexical
features are a powerful driver of our parser, firing
features on rare words would allow it to overfit the
training data quite heavily. To that end, for the
purposes of computing our features, a word is rep-
resented by its longest suffix that occurs 100 or
more times in the training data (which will be the
entire word, for common words).
1
Table 1 shows the results of incrementally
building up our feature set on the Penn Treebank
development set. RULE specifies that we use only
indicators on rule identity for binary production
and nonterminal unaries. For this experiment and
all others, we include a basic set of lexicon fea-
tures, i.e. features on preterminal part-of-speech
tags. A given preterminal unary at position i in
the sentence includes features on the words (suf-
fixes) at position i ? 1, i, and i + 1. Because the
lexicon is especially sensitive to morphological ef-
fects, we also fire features on all prefixes and suf-
1
Experiments with the Brown clusters (Brown et al,
1992) provided by Turian et al (2010) in lieu of suffixes were
not promising. Moreover, lowering this threshold did not im-
prove performance.
fixes of the current word up to length 5, regardless
of frequency.
Subsequent lines in Table 1 indicate additional
surface feature templates computed over the span,
which are then conjoined with the rule identity as
shown in Figure 1 to give additional features. In
the rest of the section, we describe the features of
this type that we use. Note that many of these fea-
tures have been used before (Taskar et al, 2004;
Finkel et al, 2008; Petrov and Klein, 2008b); our
goal here is not to amass as many feature tem-
plates as possible, but rather to examine the ex-
tent to which a simple set of features can replace a
complicated state space.
4.1 Basic Span Features
We start with some of the most obvious proper-
ties available to us, namely, the identity of the first
and last words of a span. Because heads of con-
stituents are often at the beginning or the end of
a span, these feature templates can (noisily) cap-
ture monolexical properties of heads without hav-
ing to incur the inferential cost of lexicalized an-
notations. For example, in English, the syntactic
head of a verb phrase is typically at the beginning
of the span, while the head of a simple noun phrase
is the last word. Other languages, like Korean or
Japanese, are more consistently head final.
Structural contexts like those captured by par-
ent annotation (Johnson, 1998) are more subtle.
Parent annotation can capture, for instance, the
difference in distribution in NPs that have S as a
parent (that is, subjects) and NPs under VPs (ob-
jects). We try to capture some of this same intu-
ition by introducing a feature on the length of a
span. For instance, VPs embedded in NPs tend
to be short, usually as embedded gerund phrases.
Because constituents in the treebank can be quite
long, we bin our length features into 8 buckets, of
231
no  read  messages  in  his  inbox
VP
VBP NNS
VP ? no VBP NNS
Figure 2: An example showing the utility of span
context. The ambiguity about whether read is an
adjective or a verb is resolved when we construct
a VP and notice that the word proceeding it is un-
likely.
has  an  impact  on  the  market
PPNP
NP
NP ? (NP ... impact) PP)
Figure 3: An example showing split point features
disambiguating a PP attachment. Because impact
is likely to take a PP, the monolexical indicator
feature that conjoins impact with the appropriate
rule will help us parse this example correctly.
lengths 1, 2, 3, 4, 5, 10, 20, and ?21 words.
Adding these simple features (first word, last
word, and lengths) as span features of the X-
bar grammar already gives us a substantial im-
provement over our baseline system, improving
the parser?s performance from 73.0 F1 to 85.0 F1
(see Table 1).
4.2 Span Context Features
Of course, there is no reason why we should con-
fine ourselves to just the words within the span:
words outside the span also provide a rich source
of context. As an example, consider disambiguat-
ing the POS tag of the word read in Figure 2. A
VP is most frequently preceded by a subject NP,
whose rightmost word is often its head. Therefore,
we fire features that (separately) look at the words
immediately preceding and immediately follow-
ing the span.
4.3 Split Point Features
Another important source of features are the words
at and around the split point of a binary rule ap-
plication. Figure 3 shows an example of one in-
(  CEO  of  Enron  )
PRN
(XxX)
     said  ,  ?  Too  bad  ,  ?
VP
x,?Xx,?
Figure 4: Computation of span shape features on
two examples. Parentheticals, quotes, and other
punctuation-heavy, short constituents benefit from
being explicitly modeled by a descriptor like this.
stance of this feature template. impact is a noun
that is more likely to take a PP than other nouns,
and so we expect this feature to have high weight
and encourage the attachment; this feature proves
generally useful in resolving such cases of right-
attachments to noun phrases, since the last word
of the noun phrase is often the head. As another
example, coordination can be represented by an
indicator of the conjunction, which comes imme-
diately after the split point. Finally, control struc-
tures with infinitival complements can be captured
with a rule S? NP VP with the word ?to? at the
split point.
4.4 Span Shape Features
We add one final feature characterizing the span,
which we call span shape. Figure 4 shows how this
feature is computed. For each word in the span,
2
we indicate whether that word begins with a cap-
ital letter, lowercase letter, digit, or punctuation
mark. If it begins with punctuation, we indicate
the punctuation mark explicitly. Figure 4 shows
that this is especially useful in characterizing con-
structions such as parentheticals and quoted ex-
pressions. Because this feature indicates capital-
ization, it can also capture properties of NP in-
ternal structure relevant to named entities, and its
sensitivity to capitalization and punctuation makes
it useful for recognizing appositive constructions.
5 Annotations
We have built up a strong set of features by this
point, but have not yet answered the question of
whether or not grammar annotation is useful on
top of them. In this section, we examine two of the
most commonly used types of additional annota-
tion, structural annotation, and lexical annotation.
2
For longer spans, we only use words sufficiently close to
the span?s beginning and end.
232
Annotation Dev, len ? 40
v = 0, h = 0 90.1
v = 1, h = 0 90.5
v = 0, h = 1 90.2
v = 1, h = 1 90.9
Lexicalized 90.3
Table 2: Results for the Penn Treebank develop-
ment set, sentences of length ? 40, for different
annotation schemes implemented on top of the X-
bar grammar.
Recall from Section 3 that every span feature is
conjoined with indicators over rules and rule par-
ents to produce features over anchored rule pro-
ductions; when we consider adding an annotation
layer to the grammar, what that does is refine the
rule indicators that are conjoined with every span
feature. While this is a powerful way of refining
features, we show that common successful anno-
tation schemes provide at best modest benefit on
top of the base parser.
5.1 Structural Annotation
The most basic, well-understood kind of annota-
tion on top of an X-bar grammar is structural an-
notation, which annotates each nonterminal with
properties of its environment (Johnson, 1998;
Klein and Manning, 2003). This includes vertical
annotation (parent, grandparent, etc.) as well as
horizontal annotation (only partially Markovizing
rules as opposed to using an X-bar grammar).
Table 2 shows the performance of our feature
set in grammars with several different levels of
structural annotation.
3
Klein and Manning (2003)
find large gains (6% absolute improvement, 20%
relative improvement) going from v = 0, h = 0 to
v = 1, h = 1; however, we do not find the same
level of benefit. To the extent that our parser needs
to make use of extra information in order to ap-
ply a rule correctly, simply inspecting the input to
determine this information appears to be almost
as effective as relying on information threaded
through the parser.
In Section 6 and Section 7, we use v = 1 and
h = 0; we find that v = 1 provides a small, reli-
able improvement across a range of languages and
tasks, whereas other annotations are less clearly
beneficial.
3
We use v = 0 to indicate no annotation, diverging from
the notation in Klein and Manning (2003).
Test ? 40 Test all
Berkeley 90.6 90.1
This work 89.9 89.2
Table 3: Final Parseval results for the v = 1, h = 0
parser on Section 23 of the Penn Treebank.
5.2 Lexical Annotation
Another commonly-used kind of structural an-
notation is lexicalization (Eisner, 1996; Collins,
1997; Charniak, 1997). By annotating grammar
nonterminals with their headwords, the idea is to
better model phenomena that depend heavily on
the semantics of the words involved, such as coor-
dination and PP attachment.
Table 2 shows results from lexicalizing the X-
bar grammar; it provides meager improvements.
One probable reason for this is that our parser al-
ready includes monolexical features that inspect
the first and last words of each span, which cap-
tures the syntactic or the semantic head in many
cases or can otherwise provide information about
what the constituent?s type may be and how it is
likely to combine. Lexicalization allows us to cap-
ture bilexical relationships along dependency arcs,
but it has been previously shown that these add
only marginal benefit to Collins?s model anyway
(Gildea, 2001).
5.3 English Evaluation
Finally, Table 3 shows our final evaluation on Sec-
tion 23 of the Penn Treebank. We use the v =
1, h = 0 grammar. While we do not do as well as
the Berkeley parser, we will see in Section 6 that
our parser does a substantially better job of gener-
alizing to other languages.
6 Other Languages
Historically, many annotation schemes for parsers
have required language-specific engineering: for
example, lexicalized parsers require a set of head
rules and manually-annotated grammars require
detailed analysis of the treebank itself (Klein and
Manning, 2003). A key strength of a parser that
does not rely heavily on an annotated grammar is
that it may be more portable to other languages.
We show that this is indeed the case: on nine lan-
guages, our system is competitive with or better
than the Berkeley parser, which is the best single
233
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Avg
Dev, all lengths
Berkeley 78.24 69.17 79.74 81.74 87.83 83.90 70.97 84.11 74.50 78.91
Berkeley-Rep 78.70 84.33 79.68 82.74 89.55 89.08 82.84 87.12 75.52 83.28
Our work 78.89 83.74 79.40 83.28 88.06 87.44 81.85 91.10 75.95 83.30
Test, all lengths
Berkeley 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
Berkeley-Tags 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
Our work 78.75 83.39 79.70 78.43 87.18 88.25 80.18 90.66 82.00 83.17
Table 4: Results for the nine treebanks in the SPMRL 2013 Shared Task; all values are F-scores for
sentences of all lengths using the version of evalb distributed with the shared task. Berkeley-Rep is
the best single parser from (Bj?orkelund et al, 2013); we only compare to this parser on the development
set because neither the system nor test set values are publicly available. Berkeley-Tags is a version of
the Berkeley parser run by the task organizers where tags are provided to the model, and is the best
single parser submitted to the official task. In both cases, we match or outperform the baseline parsers in
aggregate and on the majority of individual languages.
parser
4
for the majority of cases we consider.
We evaluate on the constituency treebanks from
the Statistical Parsing of Morphologically Rich
Languages Shared Task (Seddah et al, 2013).
We compare to the Berkeley parser (Petrov and
Klein, 2007) as well as two variants. First,
we use the ?Replaced? system of Bj?orkelund et
al. (2013) (Berkeley-Rep), which is their best
single parser.
5
The ?Replaced? system modi-
fies the Berkeley parser by replacing rare words
with morphological descriptors of those words
computed using language-specific modules, which
have been hand-crafted for individual languages
or are trained with additional annotation layers
in the treebanks that we do not exploit. Unfor-
tunately, Bj?orkelund et al (2013) only report re-
sults on the development set for the Berkeley-Rep
model; however, the task organizers also use a ver-
sion of the Berkeley parser provided with parts
of speech from high-quality POS taggers for each
language (Berkeley-Tags). These part-of-speech
taggers often incorporate substantial knowledge
of each language?s morphology. Both Berkeley-
Rep and Berkeley-Tags make up for some short-
comings of the Berkeley parser?s unknown word
model, which is tuned to English.
In Table 4, we see that our performance is over-
all substantially higher than that of the Berkeley
parser. On the development set, we outperform the
Berkeley parser and match the performance of the
Berkeley-Rep parser. On the test set, we outper-
4
I.e. it does not use a reranking step or post-hoc combina-
tion of parser results.
5
Their best parser, and the best overall parser from the
shared task, is a reranked product of ?Replaced? Berkeley
parsers.
form both the Berkeley parser and the Berkeley-
Tags parser on seven of nine languages, losing
only on Arabic and French.
These results suggest that the Berkeley parser
may be heavily fit to English, particularly in its
lexicon. However, even when language-specific
unknown word handling is added to the parser, our
model still outperforms the Berkeley parser over-
all, showing that our model generalizes even bet-
ter across languages than a parser for which this
is touted as a strength (Petrov and Klein, 2007).
Our span features appear to work well on both
head-initial and head-final languages (see Basque
and Korean in the table), and the fact that our
parser performs well on such morphologically-
rich languages as Hungarian indicates that our suf-
fix model is sufficient to capture most of the mor-
phological effects relevant to parsing. Of course,
a language that was heavily prefixing would likely
require this feature to be modified. Likewise, our
parser does not perform as well on Arabic and He-
brew. These closely related languages use tem-
platic morphology, for which suffixing is not ap-
propriate; however, using additional surface fea-
tures based on the output of a morphological ana-
lyzer did not lead to increased performance.
Finally, our high performance on languages
such as Polish and Swedish, whose training tree-
banks consist of 6578 and 5000 sentences, respec-
tively, show that our feature-rich model performs
robustly even on treebanks much smaller than the
Penn Treebank.
6
6
The especially strong performance on Polish relative to
other systems is partially a result of our model being able to
produce unary chains of length two, which occur frequently
in the Polish treebank (Bj?orkelund et al, 2013).
234
While ? Gangs ? is never lethargic    , it is hindered by its plot .
4
1
2
2 ? (4 While...) 1
Figure 5: An example of a sentence from the Stan-
ford Sentiment Treebank which shows the utility
of our span features for this task. The presence
of ?While? under this kind of rule tells us that the
sentiment of the constituent to the right dominates
the sentiment to the left.
7 Sentiment Analysis
Finally, because the system is, at its core, a classi-
fier of spans, it can be used equally well for tasks
that do not normally use parsing algorithms. One
example is sentiment analysis. While approaches
to sentiment analysis often simply classify the sen-
tence monolithically, treating it as a bag of n-
grams (Pang et al, 2002; Pang and Lee, 2005;
Wang and Manning, 2012), the recent dataset of
Socher et al (2013) imposes a layer of structure
on the problem that we can exploit. They annotate
every constituent in a number of training trees with
an integer sentiment value from 1 (very negative)
to 5 (very positive), opening the door for models
such as ours to learn how syntax can structurally
affect sentiment.
7
Figure 5 shows an example that requires some
analysis of sentence structure to correctly under-
stand. The first constituent conveys positive senti-
ment with never lethargic and the second conveys
negative sentiment with hindered, but to determine
the overall sentiment of the sentence, we need to
exploit the fact that while signals a discounting of
the information that follows it. The grammar rule
2 ? 4 1 already encodes the notion of the senti-
ment of the right child being dominant, so when
this is conjoined with our span feature on the first
word (While), we end up with a feature that cap-
tures this effect. Our features can also lexicalize
on other discourse connectives such as but or how-
ever, which often occur at the split point between
two spans.
7
Note that the tree structure is assumed to be given; the
problem is one of labeling a fixed parse backbone.
7.1 Adapting to Sentiment
Our parser is almost entirely unchanged from the
parser that we used for syntactic analysis. Though
the treebank grammar is substantially different,
with the nonterminals consisting of five integers
with very different semantics from syntactic non-
terminals, we still find that parent annotation is ef-
fective and otherwise additional annotation layers
are not useful.
One structural difference between sentiment
analysis and syntactic parsing lies in where the rel-
evant information is present in a span. Syntax is
often driven by heads of constituents, which tend
to be located at the beginning or the end, whereas
sentiment is more likely to depend on modifiers
such as adjectives, which are typically present
in the middle of spans. Therefore, we augment
our existing model with standard sentiment anal-
ysis features that look at unigrams and bigrams
in the span (Wang and Manning, 2012). More-
over, the Stanford Sentiment Treebank is unique
in that each constituent was annotated in isolation,
meaning that context never affects sentiment and
that every word always has the same tag. We ex-
ploit this by adding an additional feature template
similar to our span shape feature from Section 4.4
which uses the (deterministic) tag for each word
as its descriptor.
7.2 Results
We evaluated our model on the fine-grained sen-
timent analysis task presented in Socher et al
(2013) and compare to their released system. The
task is to predict the root sentiment label of each
parse tree; however, because the data is annotated
with sentiment at each span of each parse tree, we
can also evaluate how well our model does at these
intermediate computations. Following their exper-
imental conditions, we filter the test set so that it
only contains trees with non-neutral sentiment la-
bels at the root.
Table 5 shows that our model outperforms the
model of Socher et al (2013)?both the published
numbers and latest released version?on the task
of root classification, even though the system was
not explicitly designed for this task. Their model
has high capacity to model complex interactions
of words through a combinatory tensor, but it ap-
pears that our simpler, feature-driven model is just
as effective at capturing the key effects of compo-
sitionality for sentiment analysis.
235
Root All Spans
Non-neutral Dev (872 trees)
Stanford CoreNLP current 50.7 80.8
This work 53.1 80.5
Non-neutral Test (1821 trees)
Stanford CoreNLP current 49.1 80.2
Stanford EMNLP 2013 45.7 80.7
This work 49.6 80.4
Table 5: Fine-grained sentiment analysis results
on the Stanford Sentiment Treebank of Socher et
al. (2013). We compare against the printed num-
bers in Socher et al (2013) as well as the per-
formance of the corresponding release, namely
the sentiment component in the latest version of
the Stanford CoreNLP at the time of this writ-
ing. Our model handily outperforms the results
from Socher et al (2013) at root classification and
edges out the performance of the latest version of
the Stanford system. On all spans of the tree, our
model has comparable accuracy to the others.
8 Conclusion
To date, the most successful constituency parsers
have largely been generative, and operate by refin-
ing the grammar either manually or automatically
so that relevant information is available locally to
each parsing decision. Our main contribution is
to show that there is an alternative to such anno-
tation schemes: namely, conditioning on the input
and firing features based on anchored spans. We
build up a small set of feature templates as part of a
discriminative constituency parser and outperform
the Berkeley parser on a wide range of languages.
Moreover, we show that our parser is adaptable to
other tree-structured tasks such as sentiment anal-
ysis; we outperform the recent system of Socher et
al. (2013) and obtain state of the art performance
on their dataset.
Our system is available as open-source at
https://www.github.com/dlwh/epic.
Acknowledgments
This work was partially supported by BBN un-
der DARPA contract HR0011-12-C-0014, by a
Google PhD fellowship to the first author, and
an NSF fellowship to the second. We further
gratefully acknowledge a hardware donation by
NVIDIA Corporation.
References
Anders Bj?orkelund, Ozlem Cetinoglu, Rich?ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking Meets Morphosyntax: State-of-the-art
Results from the SPMRL 2013 Shared Task. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages.
Rens Bod. 1993. Using an Annotated Corpus As a
Stochastic Grammar. In Proceedings of the Sixth
Conference on European Chapter of the Association
for Computational Linguistics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
Eugene Charniak. 1997. Statistical Techniques for
Natural Language Parsing. AI Magazine, 18:33?44.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?70, March.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In ACL, pages 16?23.
John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. COLT.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING-96).
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In ACL 2008, pages
959?967.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of Empirical Methods in
Natural Language Processing.
David Hall and Dan Klein. 2012. Training factored
PCFGs with expectation propagation. In EMNLP.
James Henderson. 2003. Inducing History Represen-
tations for Broad Coverage Statistical Parsing. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
236
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632, December.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL, pages 423?430.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of the Eighteenth
International Conference on Machine Learning.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82, Morristown, NJ, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Ex-
ploiting Class Relationships for Sentiment Catego-
rization with Respect to Rating Scales. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs Up?: Sentiment Classification Us-
ing Machine Learning Techniques. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In NAACL-HLT.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. In NIPS,
pages 1153?1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
867?876, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolf-
gang Seeker, Yannick Versley, Veronika Vincze,
Marcin Woli?nski, and Alina Wr?oblewska. 2013.
Overview of the SPMRL 2013 Shared Task: A
Cross-Framework Evaluation of Parsing Morpho-
logically Rich Languages. In Proceedings of
the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages.
Khalil Sima?an. 2000. Tree-gram Parsing Lexical De-
pendencies and Structural Relations. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of Empirical Methods in
Natural Language Processing.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
Margin Parsing. In In Proceedings of Empirical
Methods in Natural Language Processing.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich Part-
of-speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology - Volume 1.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Sida Wang and Christopher Manning. 2012. Baselines
and Bigrams: Simple, Good Sentiment and Topic
Classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers).
237
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041?1051,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Structured Learning for Taxonomy Induction with Belief Propagation
Mohit Bansal
TTI Chicago
mbansal@ttic.edu
David Burkett
Twitter Inc.
dburkett@twitter.com
Gerard de Melo
Tsinghua University
gdm@demelo.org
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
We present a structured learning approach
to inducing hypernym taxonomies using a
probabilistic graphical model formulation.
Our model incorporates heterogeneous re-
lational evidence about both hypernymy
and siblinghood, captured by semantic
features based on patterns and statistics
from Web n-grams and Wikipedia ab-
stracts. For efficient inference over tax-
onomy structures, we use loopy belief
propagation along with a directed span-
ning tree algorithm for the core hyper-
nymy factor. To train the system, we ex-
tract sub-structures of WordNet and dis-
criminatively learn to reproduce them, us-
ing adaptive subgradient stochastic opti-
mization. On the task of reproducing
sub-hierarchies of WordNet, our approach
achieves a 51% error reduction over a
chance baseline, including a 15% error re-
duction due to the non-hypernym-factored
sibling features. On a comparison setup,
we find up to 29% relative error reduction
over previous work on ancestor F1.
1 Introduction
Many tasks in natural language understanding,
such as question answering, information extrac-
tion, and textual entailment, benefit from lexical
semantic information in the form of types and hy-
pernyms. A recent example is IBM?s Jeopardy!
system Watson (Ferrucci et al, 2010), which used
type information to restrict the set of answer can-
didates. Information of this sort is present in term
taxonomies (e.g., Figure 1), ontologies, and the-
sauri. However, currently available taxonomies
such as WordNet are incomplete in coverage (Pen-
nacchiotti and Pantel, 2006; Hovy et al, 2009),
unavailable in many domains and languages, and
vertebrate
mammal
placental
cow rodent
squirrel rat
metatherian
marsupial
kangaroo
reptile
diapsid
snake crocodilian
anapsid
chelonian
turtle
1
Figure 1: An excerpt of WordNet?s vertebrates taxonomy.
time-intensive to create or extend manually. There
has thus been considerable interest in building lex-
ical taxonomies automatically.
In this work, we focus on the task of taking col-
lections of terms as input and predicting a com-
plete taxonomy structure over them as output. Our
model takes a loglinear form and is represented
using a factor graph that includes both 1st-order
scoring factors on directed hypernymy edges (a
parent and child in the taxonomy) and 2nd-order
scoring factors on sibling edge pairs (pairs of hy-
pernym edges with a shared parent), as well as in-
corporating a global (directed spanning tree) struc-
tural constraint. Inference for both learning and
decoding uses structured loopy belief propagation
(BP), incorporating standard spanning tree algo-
rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,
1984). The belief propagation approach allows us
to efficiently and effectively incorporate hetero-
geneous relational evidence via hypernymy and
siblinghood (e.g., coordination) cues, which we
capture by semantic features based on simple sur-
face patterns and statistics from Web n-grams and
Wikipedia abstracts. We train our model to max-
imize the likelihood of existing example ontolo-
gies using stochastic optimization, automatically
learning the most useful relational patterns for full
taxonomy induction.
As an example of the relational patterns that our
1041
system learns, suppose we are interested in build-
ing a taxonomy for types of mammals (see Fig-
ure 1). Frequent attestation of hypernymy patterns
like rat is a rodent in large corpora is a strong sig-
nal of the link rodent ? rat. Moreover, sibling
or coordination cues like either rats or squirrels
suggest that rat is a sibling of squirrel and adds
evidence for the links rodent ? rat and rodent
? squirrel. Our supervised model captures ex-
actly these types of intuitions by automatically dis-
covering such heterogeneous relational patterns as
features (and learning their weights) on edges and
on sibling edge pairs, respectively.
There have been several previous studies on
taxonomy induction. e.g., the incremental tax-
onomy induction system of Snow et al (2006),
the longest path approach of Kozareva and Hovy
(2010), and the maximum spanning tree (MST)
approach of Navigli et al (2011) (see Section 4 for
a more detailed overview). The main contribution
of this work is that we present the first discrimina-
tively trained, structured probabilistic model over
the full space of taxonomy trees, using a struc-
tured inference procedure through both the learn-
ing and decoding phases. Our model is also the
first to directly learn relational patterns as part of
the process of training an end-to-end taxonomic
induction system, rather than using patterns that
were hand-selected or learned via pairwise clas-
sifiers on manually annotated co-occurrence pat-
terns. Finally, it is the first end-to-end (i.e., non-
incremental) system to include sibling (e.g., coor-
dination) patterns at all.
We test our approach in two ways. First, on
the task of recreating fragments of WordNet, we
achieve a 51% error reduction on ancestor-based
F1 over a chance baseline, including a 15% error
reduction due to the non-hypernym-factored sib-
ling features. Second, we also compare to the re-
sults of Kozareva and Hovy (2010) by predicting
the large animal subtree of WordNet. Here, we
get up to 29% relative error reduction on ancestor-
based F1. We note that our approach falls at a
different point in the space of performance trade-
offs from past work ? by producing complete,
highly articulated trees, we naturally see a more
even balance between precision and recall, while
past work generally focused on precision.
1
To
1
While different applications will value precision and
recall differently, and past work was often intentionally
precision-focused, it is certainly the case that an ideal solu-
tion would maximize both.
avoid presumption of a single optimal tradeoff, we
also present results for precision-based decoding,
where we trade off recall for precision.
2 Structured Taxonomy Induction
Given an input term set x = {x
1
, x
2
, . . . , x
n
},
we wish to compute the conditional distribution
over taxonomy trees y. This distribution P (y|x)
is represented using the graphical model formu-
lation shown in Figure 2. A taxonomy tree y is
composed of a set of indicator random variables
y
ij
(circles in Figure 2), where y
ij
= ON means
that x
i
is the parent of x
j
in the taxonomy tree
(i.e. there exists a directed edge from x
i
to x
j
).
One such variable exists for each pair (i, j) with
0 ? i ? n, 1 ? j ? n, and i 6= j.
2
In a factor graph formulation, a set of factors
(squares and rectangles in Figure 2) determines the
probability of each possible variable assignment.
Each factor F has an associated scoring function
?
F
, with the probability of a total assignment de-
termined by the product of all these scores:
P (y|x) ?
?
F
?
F
(y) (1)
2.1 Factor Types
In the models we present here, there are three
types of factors: EDGE factors that score individ-
ual edges in the taxonomy tree, SIBLING factors
that score pairs of edges with a shared parent, and
a global TREE factor that imposes the structural
constraint that y form a legal taxonomy tree.
EDGE Factors. For each edge variable y
ij
in
the model, there is a corresponding factor E
ij
(small blue squares in Figure 2) that depends only
on y
ij
. We score each edge by extracting a set
of features f(x
i
, x
j
) and weighting them by the
(learned) weight vector w. So, the factor scoring
function is:
?
E
ij
(y
ij
) =
{
exp(w ? f(x
i
, x
j
)) y
ij
= ON
exp(0) = 1 y
ij
= OFF
SIBLING Factors. Our second model also in-
cludes factors that permit 2nd-order features look-
ing at terms that are siblings in the taxonomy tree.
For each triple (i, j, k) with i 6= j, i 6= k, and
j < k,
3
we have a factor S
ijk
(green rectangles in
2
We assume a special dummy root symbol x
0
.
3
The ordering of the siblings x
j
and x
k
doesn?t mat-
ter here, so having separate factors for (i, j, k) and (i, k, j)
would be redundant.
1042
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
T
(a) Edge Features Only
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
S12n
S21n
Sn12
T
(b) Full Model
Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.
Figure 2b) that depends on y
ij
and y
ik
, and thus
can be used to encode features that should be ac-
tive whenever x
j
and x
k
share the same parent, x
i
.
The scoring function is similar to the one above:
?
S
ijk
(y
ij
, y
ik
) =
{
exp(w ? f(x
i
, x
j
, x
k
)) y
ij
= y
ik
= ON
1 otherwise
TREE Factor. Of course, not all variable as-
signments y form legal taxonomy trees (i.e., di-
rected spanning trees). For example, the assign-
ment ?i, j, y
ij
= ON might get a high score, but
would not be a valid output of the model. Thus,
we need to impose a structural constraint to ensure
that such illegal variable assignments are assigned
0 probability by the model. We encode this in our
factor graph setting using a single global factor T
(shown as a large red square in Figure 2) with the
following scoring function:
?
T
(y) =
{
1 y forms a legal taxonomy tree
0 otherwise
Model. For a given global assignment y, let
f(y) =
?
i,j
y
ij
=ON
f(x
i
, x
j
) +
?
i,j,k
y
ij
=y
ik
=ON
f(x
i
, x
j
, x
k
)
Note that by substituting our model?s factor scor-
ing functions into Equation 1, we get:
P (y|x) ?
{
exp(w ? f(y)) y is a tree
0 otherwise
Thus, our model has the form of a standard loglin-
ear model with feature function f .
2.2 Inference via Belief Propagation
With the model defined, there are two main in-
ference tasks we wish to accomplish: computing
expected feature counts and selecting a particular
taxonomy tree for a given set of input terms (de-
coding). As an initial step to each of these pro-
cedures, we wish to compute the marginal prob-
abilities of particular edges (and pairs of edges)
being on. In a factor graph, the natural infer-
ence procedure for computing marginals is belief
propagation. Note that finding taxonomy trees is
a structurally identical problem to directed span-
ning trees (and thereby non-projective dependency
parsing), for which belief propagation has previ-
ously been worked out in depth (Smith and Eisner,
2008). Therefore, we will only briefly sketch the
procedure here.
Belief propagation is a general-purpose infer-
ence method that computes marginals via directed
messages passed from variables to adjacent fac-
tors (and vice versa) in the factor graph. These
messages take the form of (possibly unnormal-
ized) distributions over values of the variable. The
two types of messages (variable to factor or fac-
tor to variable) have mutually recursive defini-
tions. The message from a factor F to an adjacent
variable V involves a sum over all possible val-
ues of every other variable that F touches. While
the EDGE and SIBLING factors are simple enough
to compute this sum by brute force, performing
the sum na??vely for computing messages from the
TREE factor would take exponential time. How-
1043
ever, due to the structure of that particular factor,
all of its outgoing messages can be computed si-
multaneously in O(n
3
) time via an efficient adap-
tation of Kirchhoff?s Matrix Tree Theorem (MTT)
(Tutte, 1984) which computes partition functions
and marginals for directed spanning trees.
Once message passing is completed, marginal
beliefs are computed by merely multiplying to-
gether all the messages received by a particular
variable or factor.
2.2.1 Loopy Belief Propagation
Looking closely at Figure 2a, one can observe
that the factor graph for the first version of our
model, containing only EDGE and TREE factors,
is acyclic. In this special case, belief propagation
is exact: after one round of message passing, the
beliefs computed (as discussed in Section 2.2) will
be the true marginal probabilities under the cur-
rent model. However, in the full model, shown
in Figure 2b, the SIBLING factors introduce cy-
cles into the factor graph, and now the messages
being passed around often depend on each other
and so they will change as they are recomputed.
The process of iteratively recomputing messages
based on earlier messages is known as loopy belief
propagation. This procedure only finds approx-
imate marginal beliefs, and is not actually guar-
anteed to converge, but in practice can be quite
effective for finding workable marginals in mod-
els for which exact inference is intractable, as is
the case here. All else equal, the more rounds
of message passing that are performed, the closer
the computed marginal beliefs will be to the true
marginals, though in practice, there are usually di-
minishing returns after the first few iterations. In
our experiments, we used a fairly conservative up-
per bound of 20 iterations, but in most cases, the
messages converged much earlier than that.
2.3 Training
We used gradient-based maximum likelihood
training to learn the model parameters w. Since
our model has a loglinear form, the derivative
of w with respect to the likelihood objective is
computed by just taking the gold feature vec-
tor and subtracting the vector of expected feature
counts. For computing expected counts, we run
belief propagation until completion and then, for
each factor in the model, we simply read off the
marginal probability of that factor being active (as
computed in Section 2.2), and accumulate a par-
tial count for each feature that is fired by that fac-
tor. This method of computing the gradient can be
incorporated into any gradient-based optimizer in
order to learn the weights w. In our experiments
we used AdaGrad (Duchi et al, 2011), an adaptive
subgradient variant of standard stochastic gradient
ascent for online learning.
2.4 Decoding
Finally, once the model parameters have been
learned, we want to use the model to find taxon-
omy trees for particular sets of input terms. Note
that if we limit our scores to be edge-factored,
then finding the highest scoring taxonomy tree
becomes an instance of the MST problem (also
known as the maximum arborescence problem
for the directed case), which can be solved effi-
ciently in O(n
2
) quadratic time (Tarjan, 1977) us-
ing the greedy, recursive Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967).
4
Since the MST problem can be solved effi-
ciently, the main challenge becomes finding a way
to ensure that our scores are edge-factored. In the
first version of our model, we could simply set the
score of each edge to be w?f(x
i
, x
j
), and the MST
recovered in this way would indeed be the high-
est scoring tree: arg maxyP (y|x). However, this
straightforward approach doesn?t apply to the full
model which also uses sibling features. Hence, at
decoding time, we instead start out by once more
using belief propagation to find marginal beliefs,
and then set the score of each edge to be its belief
odds ratio:
b
Y
ij
(ON)
b
Y
ij
(OFF)
.
5
3 Features
While spanning trees are familiar from non-
projective dependency parsing, features based on
the linear order of the words or on lexical identi-
4
See Georgiadis (2003) for a detailed algorithmic proof,
and McDonald et al (2005) for an illustrative example. Also,
we constrain the Chu-Liu-Edmonds MST algorithm to out-
put only single-root MSTs, where the (dummy) root has ex-
actly one child (Koo et al, 2007), because multi-root span-
ning ?forests? are not applicable to our task.
Also, note that we currently assume one node per term. We
are following the task description from previous work where
the goal is to create a taxonomy for a specific domain (e.g.,
animals). Within a specific domain, terms typically just have
a single sense. However, our algorithms could certainly be
adapted to the case of multiple term senses (by treating the
different senses as unique nodes in the tree) in future work.
5
The MST that is found using these edge scores is actually
the minimum Bayes risk tree (Goodman, 1996) for an edge
accuracy loss function (Smith and Eisner, 2008).
1044
ties or syntactic word classes, which are primary
drivers for dependency parsing, are mostly unin-
formative for taxonomy induction. Instead, induc-
ing taxonomies requires world knowledge to cap-
ture the semantic relations between various unseen
terms. For this, we use semantic cues to hyper-
nymy and siblinghood via features on simple sur-
face patterns and statistics in large text corpora.
We fire features on both the edge and the sibling
factors. We first describe all the edge features
in detail (Section 3.1 and Section 3.2), and then
briefly describe the sibling features (Section 3.3),
which are quite similar to the edge ones.
For each edge factor E
ij
, which represents the
potential parent-child term pair (x
i
, x
j
), we add
the surface and semantic features discussed below.
Note that since edges are directed, we have sepa-
rate features for the factors E
ij
versus E
ji
.
3.1 Surface Features
Capitalization: Checks which of x
i
and x
j
are
capitalized, with one feature for each value of the
tuple (isCap(x
i
), isCap(x
j
)). The intuition is that
leaves of a taxonomy are often proper names and
hence capitalized, e.g., (bison, American bison).
Therefore, the feature for (true, false) (i.e., parent
capitalized but not the child) gets a substantially
negative weight.
Ends with: Checks if x
j
ends with x
i
, or not. This
captures pairs such as (fish, bony fish) in our data.
Contains: Checks if x
j
contains x
i
, or not. This
captures pairs such as (bird, bird of prey).
Suffix match: Checks whether the k-length suf-
fixes of x
i
and x
j
match, or not, for k =
1, 2, . . . , 7.
LCS: We compute the longest common substring
of x
i
and x
j
, and create indicator features for
rounded-off and binned values of |LCS|/((|x
i
|+
|x
j
|)/2).
Length difference: We compute the signed length
difference between x
j
and x
i
, and create indica-
tor features for rounded-off and binned values of
(|x
j
| ? |x
i
|)/((|x
i
| + |x
j
|)/2). Yang and Callan
(2009) use a similar feature.
3.2 Semantic Features
3.2.1 Web n-gram Features
Patterns and counts: Hypernymy for a term pair
(P=x
i
, C=x
j
) is often signaled by the presence
of surface patterns like C is a P, P such as C
in large text corpora, an observation going back
to Hearst (1992). For each potential parent-child
edge (P=x
i
, C=x
j
), we mine the top k strings
(based on count) in which both x
i
and x
j
occur
(we use k=200). We collect patterns in both direc-
tions, which allows us to judge the correct direc-
tion of an edge (e.g., C is a P is a positive signal
for hypernymy whereas P is a C is a negative sig-
nal).
6
Next, for each pattern in this top-k list, we
compute its normalized pattern count c, and fire
an indicator feature on the tuple (pattern, t), for
all thresholds t (in a fixed set) s.t. c ? t. Our
supervised model then automatically learns which
patterns are good indicators of hypernymy.
Pattern order: We add features on the order (di-
rection) in which the pair (x
i
, x
j
) found a pattern
(in its top-k list) ? indicator features for boolean
values of the four cases: P . . . C, C . . . P , neither
direction, and both directions. Ritter et al (2009)
used the ?both? case of this feature.
Individual counts: We also compute the indi-
vidual Web-scale term counts c
x
i
and c
x
j
, and
add a comparison feature (c
x
i
>c
x
j
), plus features
on values of the signed count difference (|c
x
i
| ?
|c
x
j
|)/((|c
x
i
| + |c
x
j
|)/2), after rounding off, and
binning at multiple granularities. The intuition is
that this feature could learn whether the relative
popularity of the terms signals their hypernymy di-
rection.
3.2.2 Wikipedia Abstract Features
The Web n-grams corpus has broad coverage but
is limited to up to 5-grams, so it may not contain
pattern-based evidence for various longer multi-
word terms and pairs. Therefore, we supplement
it with a full-sentence resource, namely Wikipedia
abstracts, which are concise descriptions (hence
useful to signal hypernymy) of a large variety of
world entities.
Presence and distance: For each potential edge
(x
i
, x
j
), we mine patterns from all abstracts in
which the two terms co-occur in either order, al-
lowing a maximum term distance of 20 (because
beyond that, co-occurrence may not imply a rela-
tion). We add a presence feature based on whether
the process above found at least one pattern for
that term pair, or not. We also fire features on
the value of the minimum distance d
min
at which
6
We also allow patterns with surrounding words, e.g., the
C is a P and C , P of.
1045
the two terms were found in some abstract (plus
thresholded versions).
Patterns: For each term pair, we take the top-k
?
patterns (based on count) of length up to l from
its full list of patterns, and add an indicator feature
on each pattern string (without the counts). We use
k
?
=5, l=10. Similar to the Web n-grams case, we
also fire Wikipedia-based pattern order features.
3.3 Sibling Features
We also incorporate similar features on sibling
factors. For each sibling factor S
ijk
which rep-
resents the potential parent-children term triple
(x
i
, x
j
, x
k
), we consider the potential sibling term
pair (x
j
, x
k
). Siblinghood for this pair would be
indicated by the presence of surface patterns such
as either C
1
or C
2
, C
1
is similar to C
2
in large cor-
pora. Hence, we fire Web n-gram pattern features
and Wikipedia presence, distance, and pattern fea-
tures, similar to those described above, on each
potential sibling term pair.
7
The main difference
here from the edge factors is that the sibling fac-
tors are symmetric (in the sense that S
ijk
is redun-
dant to S
ikj
) and hence the patterns are undirected.
Therefore, for each term pair, we first symmetrize
the collected Web n-grams and Wikipedia patterns
by accumulating the counts of symmetric patterns
like rats or squirrels and squirrels or rats.
8
4 Related Work
In our work, we assume a known term set and
do not address the problem of extracting related
terms from text. However, a great deal of past
work has considered automating this process, typ-
ically taking one of two major approaches. The
clustering-based approach (Lin, 1998; Lin and
Pantel, 2002; Davidov and Rappoport, 2006; Ya-
mada et al, 2009) discovers relations based on the
assumption that similar concepts appear in sim-
7
One can also add features on the full triple (x
i
, x
j
, x
k
)
but most such features will be sparse.
8
All the patterns and counts for our Web and Wikipedia
edge and sibling features described above are extracted after
stemming the words in the terms, the n-grams, and the ab-
stracts (using the Porter stemmer). Also, we threshold the
features (to prune away the sparse ones) by considering only
those that fire for at least t trees in the training data (t = 4 in
our experiments).
Note that one could also add various complementary types of
useful features presented by previous work, e.g., bootstrap-
ping using syntactic heuristics (Phillips and Riloff, 2002),
dependency patterns (Snow et al, 2006), doubly anchored
patterns (Kozareva et al, 2008; Hovy et al, 2009), and Web
definition classifiers (Navigli et al, 2011).
ilar contexts (Harris, 1954). The pattern-based
approach uses special lexico-syntactic patterns to
extract pairwise relation lists (Phillips and Riloff,
2002; Girju et al, 2003; Pantel and Pennacchiotti,
2006; Suchanek et al, 2007; Ritter et al, 2009;
Hovy et al, 2009; Baroni et al, 2010; Ponzetto
and Strube, 2011) and semantic classes or class-
instance pairs (Riloff and Shepherd, 1997; Katz
and Lin, 2003; Pas?ca, 2004; Etzioni et al, 2005;
Talukdar et al, 2008).
We focus on the second step of taxonomy induc-
tion, namely the structured organization of terms
into a complete and coherent tree-like hierarchy.
9
Early work on this task assumes a starting par-
tial taxonomy and inserts missing terms into it.
Widdows (2003) place unknown words into a re-
gion with the most semantically-similar neigh-
bors. Snow et al (2006) add novel terms by greed-
ily maximizing the conditional probability of a set
of relational evidence given a taxonomy. Yang and
Callan (2009) incrementally cluster terms based
on a pairwise semantic distance. Lao et al (2012)
extend a knowledge base using a random walk
model to learn binary relational inference rules.
However, the task of inducing full taxonomies
without assuming a substantial initial partial tax-
onomy is relatively less well studied. There is
some prior work on the related task of hierarchical
clustering, or grouping together of semantically
related words (Cimiano et al, 2005; Cimiano and
Staab, 2005; Poon and Domingos, 2010; Fountain
and Lapata, 2012). The task we focus on, though,
is the discovery of direct taxonomic relationships
(e.g., hypernymy) between words.
We know of two closely-related previous sys-
tems, Kozareva and Hovy (2010) and Navigli et
al. (2011), that build full taxonomies from scratch.
Both of these systems use a process that starts
by finding basic level terms (leaves of the fi-
nal taxonomy tree, typically) and then using re-
lational patterns (hand-selected ones in the case of
Kozareva and Hovy (2010), and ones learned sep-
arately by a pairwise classifier on manually anno-
tated co-occurrence patterns for Navigli and Ve-
lardi (2010), Navigli et al (2011)) to find interme-
diate terms and all the attested hypernymy links
between them.
10
To prune down the resulting tax-
9
Determining the set of input terms is orthogonal to our
work, and our method can be used in conjunction with vari-
ous term extraction approaches described above.
10
Unlike our system, which assumes a complete set of
terms and only attempts to induce the taxonomic structure,
1046
onomy graph, Kozareva and Hovy (2010) use a
procedure that iteratively retains the longest paths
between root and leaf terms, removing conflicting
graph edges as they go. The end result is acyclic,
though not necessarily a tree; Navigli et al (2011)
instead use the longest path intuition to weight
edges in the graph and then find the highest weight
taxonomic tree using a standard MST algorithm.
Our work differs from the two systems above
in that ours is the first discriminatively trained,
structured probabilistic model over the full space
of taxonomy trees that uses structured inference
via spanning tree algorithms (MST and MTT)
through both the learning and decoding phases.
Our model also automatically learns relational pat-
terns as a part of the taxonomic training phase, in-
stead of relying on hand-picked rules or pairwise
classifiers on manually annotated co-occurrence
patterns, and it is the first end-to-end (i.e., non-
incremental) system to include heterogeneous re-
lational information via sibling (e.g., coordina-
tion) patterns.
5 Experiments
5.1 Data and Experimental Regime
We considered two distinct experimental setups,
one that illustrates the general performance of
our model by reproducing various medium-sized
WordNet domains, and another that facilitates
comparison to previous work by reproducing the
much larger animal subtree provided by Kozareva
and Hovy (2010).
General setup: In order to test the accuracy
of structured prediction on medium-sized full-
domain taxonomies, we extracted from WordNet
3.0 all bottomed-out full subtrees which had a
tree-height of 3 (i.e., 4 nodes from root to leaf),
and contained (10, 50] terms.
11
This gives us
761 non-overlapping trees, which we partition into
both these systems include term discovery in the taxonomy
building process.
11
Subtrees that had a smaller or larger tree height were dis-
carded in order to avoid overlap between the training and test
divisions. This makes it a much stricter setting than other
tasks such as parsing, which usually has repeated sentences,
clauses and phrases between training and test sets.
To project WordNet synsets to terms, we used the first (most
frequent) term in each synset. A few WordNet synsets have
multiple parents so we only keep the first of each such pair of
overlapping trees. We also discard a few trees with duplicate
terms because this is mostly due to the projection of different
synsets to the same term, and theoretically makes the tree a
graph.
70/15/15% (533/114/114 trees) train/dev/test sets.
Comparison setup: We also compare our method
(as closely as possible) with related previous work
by testing on the much larger animal subtree made
available by Kozareva and Hovy (2010), who cre-
ated this dataset by selecting a set of ?harvested?
terms and retrieving all the WordNet hypernyms
between each input term and the root (i.e., an-
imal), resulting in ?700 terms and ?4,300 is-a
ancestor-child links.
12
Our training set for this an-
imal test case was generated from WordNet us-
ing the following process: First, we strictly re-
move the full animal subtree from WordNet in or-
der to avoid any possible overlap with the test data.
Next, we create random 25-sized trees by picking
random nodes as singleton trees, and repeatedly
adding child edges from WordNet to the tree. This
process gives us a total of ?1600 training trees.
13
Feature sources: The n-gram semantic features
are extracted from the Google n-grams corpus
(Brants and Franz, 2006), a large collection of
English n-grams (for n = 1 to 5) and their fre-
quencies computed from almost 1 trillion tokens
(95 billion sentences) of Web text. The Wikipedia
abstracts are obtained via the publicly available
dump, which contains almost ?4.1 million ar-
ticles.
14
Preprocessing includes standard XML
parsing and tokenization. Efficient collection of
feature statistics is important because these must
be extracted for millions of query pairs (for each
potential edge and sibling pair in each term set).
For this, we use a hash-trie on term pairs (sim-
ilar to that of Bansal and Klein (2011)), and scan
once through the n-gram (or abstract) set, skipping
many n-grams (or abstracts) based on fast checks
of missing unigrams, exceeding length, suffix mis-
matches, etc.
5.2 Evaluation Metric
Ancestor F1: Measures the precision, recall, and
F
1
= 2PR/(P +R) of correctly predicted ances-
12
This is somewhat different from our general setup where
we work with any given set of terms; they start with a large
set of leaves which have substantial Web-based relational
information based on their selected, hand-picked patterns.
Their data is available at http://www.isi.edu/
?
kozareva/
downloads.html.
13
We tried this training regimen as different from that of
the general setup (which contains only bottomed-out sub-
trees), so as to match the animal test tree, which is of depth
12 and has intermediate nodes from higher up in WordNet.
14
We used the 20130102 dump.
1047
System P R F1
Edges-Only Model
Baseline 5.9 8.3 6.9
Surface Features 17.5 41.3 24.6
Semantic Features 37.0 49.1 42.2
Surface+Semantic 41.1 54.4 46.8
Edges + Siblings Model
Surface+Semantic 53.1 56.6 54.8
Surface+Semantic (Test) 48.0 55.2 51.4
Table 1: Main results on our general setup. On the devel-
opment set, we present incremental results on the edges-only
model where we start with the chance baseline, then use sur-
face features only, semantic features only, and both. Finally,
we add sibling factors and features to get results for the full,
edges+siblings model with all features, and also report the
final test result for this setting.
tors, i.e., pairwise is-a relations:
P =
|isa
gold
? isa
predicted
|
|isa
predicted
|
, R =
|isa
gold
? isa
predicted
|
|isa
gold
|
5.3 Results
Table 1 shows our main results for ancestor-based
evaluation on the general setup. We present a de-
velopment set ablation study where we start with
the edges-only model (Figure 2a) and its random
tree baseline (which chooses any arbitrary span-
ning tree for the term set). Next, we show results
on the edges-only model with surface features
(Section 3.1), semantic features (Section 3.2), and
both. We see that both surface and semantic fea-
tures make substantial contributions, and they also
stack. Finally, we add the sibling factors and fea-
tures (Figure 2b, Section 3.3), which further im-
proves the results significantly (8% absolute and
15% relative error reduction over the edges-only
results on the ancestor F1 metric). The last row
shows the final test set results for the full model
with all features.
Table 2 shows our results for comparison to
the larger animal dataset of Kozareva and Hovy
(2010).
15
In the table, ?Kozareva2010? refers
to Kozareva and Hovy (2010) and ?Navigli2011?
refers to Navigli et al (2011).
16
For appropri-
15
These results are for the 1st order model due to the scale
of the animal taxonomy (?700 terms). For scaling the 2nd
order sibling model, one can use approximations, e.g., prun-
ing the set of sibling factors based on 1st order link marginals,
or a hierarchical coarse-to-fine approach based on taxonomy
induction on subtrees, or a greedy approach of adding a few
sibling factors at a time. This is future work.
16
The Kozareva and Hovy (2010) ancestor results are ob-
tained by using the output files provided on their webpage.
System P R F1
Previous Work
Kozareva2010 98.6 36.2 52.9
Navigli2011
??
97.0
??
43.7
??
60.3
??
This Paper
Fixed Prediction 84.2 55.1 66.6
Free Prediction 79.3 49.0 60.6
Table 2: Comparison results on the animal dataset of
Kozareva and Hovy (2010). Here, ?Kozareva2010? refers to
Kozareva and Hovy (2010) and ?Navigli2011? refers to Nav-
igli et al (2011). For appropriate comparison to each previ-
ous work, we show our results both for the ?Fixed Prediction?
setup, which assumes the true root and leaves, and for the
?Free Prediction? setup, which doesn?t assume any prior in-
formation. The ?? results of Navigli et al (2011) represent a
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details.
ate comparison to each previous work, we show
results for two different setups. The first setup
?Fixed Prediction? assumes that the model knows
the true root and leaves of the taxonomy to provide
for a somewhat fairer comparison to Kozareva and
Hovy (2010). We get substantial improvements
on ancestor-based recall and F1 (a 29% relative
error reduction). The second setup ?Free Predic-
tion? assumes no prior knowledge and predicts the
full tree (similar to the general setup case). On
this setup, we do compare as closely as possible
to Navigli et al (2011) and see a small gain in F1,
but regardless, we should note that their results are
incomparable (denoted by ?? in Table 2) because
they have a different ground-truth data condition:
their definition and hypernym extraction phase in-
volves using the Google define keyword, which
often returns WordNet glosses itself.
We note that previous work achieves higher an-
cestor precision, while our approach achieves a
more even balance between precision and recall.
Of course, precision and recall should both ide-
ally be high, even if some applications weigh one
over the other. This is why our tuning optimized
for F1, which represents a neutral combination
for comparison, but other F
?
metrics could also
be optimized. In this direction, we also tried an
experiment on precision-based decoding (for the
?Free Prediction? scenario), where we discard any
edges with score (i.e., the belief odds ratio de-
scribed in Section 2.4) less than a certain thresh-
old. This allowed us to achieve high values of pre-
cision (e.g., 90.8%) at still high enough F1 values
(e.g., 61.7%).
1048
Hypernymy features
C and other P > P > C
C , P of C is a P
C , a P P , including C
C or other P P ( C
C : a P C , american P
C - like P C , the P
Siblinghood features
C
1
and C
2
C
1
, C
2
(
C
1
or C
2
of C
1
and / or C
2
, C
1
, C
2
and either C
1
or C
2
the C
1
/ C
2
<s> C
1
and C
2
</s>
Table 3: Examples of high-weighted hypernymy and sibling-
hood features learned during development.
butterfly
copper
American copper
hairstreak
Strymon melinus
admiral
white admiral
1
Figure 3: Excerpt from the predicted butterfly tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
6 Analysis
Table 3 shows some of the hypernymy and sibling-
hood features given highest weight by our model
(in general-setup development experiments). The
training process not only rediscovers most of the
standard Hearst-style hypernymy patterns (e.g., C
and other P, C is a P), but also finds various
novel, intuitive patterns. For example, the pattern
C, american P is prominent because it captures
pairs like Lemmon, american actor and Bryon,
american politician, etc. Another pattern > P >
C captures webpage navigation breadcrumb trails
(representing category hierarchies). Similarly, the
algorithm also discovers useful siblinghood fea-
tures, e.g., either C
1
or C
2
, C
1
and / or C
2
, etc.
Finally, we look at some specific output errors
to give as concrete a sense as possible of some sys-
tem confusions, though of course any hand-chosen
examples must be taken as illustrative. In Figure
3, we attach white admiral to admiral, whereas
the gold standard makes these two terms siblings.
In reality, however, white admirals are indeed a
species of admirals, so WordNet?s ground truth
turns out to be incomplete. Another such example
is that we place logistic assessment in the evalu-
bottle
flask
vacuum flask thermos Erlenmeyer flask
wine bottle jeroboam
1
Figure 4: Excerpt from the predicted bottle tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
ation subtree of judgment, but WordNet makes it
a direct child of judgment. However, other dictio-
naries do consider logistic assessments to be eval-
uations. Hence, this illustrates that there may be
more than one right answer, and that the low re-
sults on this task should only be interpreted as
such. In Figure 4, our algorithm did not recog-
nize that thermos is a hyponym of vacuum flask,
and that jeroboam is a kind of wine bottle. Here,
our Web n-grams dataset (which only contains fre-
quent n-grams) and Wikipedia abstracts do not
suffice and we would need to add richer Web data
for such world knowledge to be reflected in the
features.
7 Conclusion
Our approach to taxonomy induction allows het-
erogeneous information sources to be combined
and balanced in an error-driven way. Direct indi-
cators of hypernymy, such as Hearst-style context
patterns, are the core feature for the model and are
discovered automatically via discriminative train-
ing. However, other indicators, such as coordina-
tion cues, can indicate that two words might be
siblings, independently of what their shared par-
ent might be. Adding second-order factors to our
model allows these two kinds of evidence to be
weighed and balanced in a discriminative, struc-
tured probabilistic framework. Empirically, we
see substantial gains (in ancestor F1) from sibling
features, and also over comparable previous work.
We also present results on the precision and recall
trade-offs inherent in this task.
Acknowledgments
We would like to thank the anonymous review-
ers for their insightful comments. This work
was supported by BBN under DARPA contract
HR0011-12-C-0014, 973 Program China Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003.
1049
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram corpus version 1.1. LDC2006T13.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14(1396-1400):270.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided agglom-
erative clustering algorithm. In Proceedings of the
ICML 2005 Workshop on Learning and Extending
Lexical Ontologies with Machine Learning Meth-
ods.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of Arti-
ficial Intelligence Research, 24(1):305?339.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proceedings of COLING-ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71:233?240.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence,
165(1):91?134.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the DeepQA
project. AI magazine, 31(3):59?79.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
induction using hierarchical random graphs. In Pro-
ceedings of NAACL.
Leonidas Georgiadis. 2003. Arborescence optimiza-
tion problems solvable by edmonds algorithm. The-
oretical Computer Science, 301(1):427?437.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of EMNLP.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In Proceedings of the Workshop on NLP for Ques-
tion Answering (EACL 2003).
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-CoNLL.
Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of EMNLP.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceedings of COLING.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In Proceedings of IJCAI.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL.
1050
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn
semantic lexicons. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively
built knowledge repository. Artificial Intelligence,
175(9):1737?1756.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of ACL.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In
Proceedings of EMNLP.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of AAAI Spring Sympo-
sium on Learning by Reading and Learning to Read.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP.
Robert E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25?35.
William T. Tutte. 1984. Graph theory. Addison-
Wesley.
Dominic Widdows. 2003. Unsupervised methods
for developing taxonomies by combining syntactic
and statistical information. In Proceedings of HLT-
NAACL.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of EMNLP.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of ACL-IJCNLP.
1051
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118?123,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improved Typesetting Models for Historical OCR
Taylor Berg-Kirkpatrick Dan Klein
Computer Science Division
University of California, Berkeley
{tberg,klein}@cs.berkeley.edu
Abstract
We present richer typesetting models
that extend the unsupervised historical
document recognition system of Berg-
Kirkpatrick et al (2013). The first
model breaks the independence assump-
tion between vertical offsets of neighbor-
ing glyphs and, in experiments, substan-
tially decreases transcription error rates.
The second model simultaneously learns
multiple font styles and, as a result, is
able to accurately track italic and non-
italic portions of documents. Richer mod-
els complicate inference so we present a
new, streamlined procedure that is over
25x faster than the method used by Berg-
Kirkpatrick et al (2013). Our final sys-
tem achieves a relative word error reduc-
tion of 22% compared to state-of-the-art
results on a dataset of historical newspa-
pers.
1 Introduction
Modern OCR systems perform poorly on histor-
ical documents from the printing-press era, often
yielding error rates that are too high for down-
stream research projects (Arlitsch and Herbert,
2004; Shoemaker, 2005; Holley, 2010). The two
primary reasons that historical documents present
difficultly for automatic systems are (1) the type-
setting process used to produce such documents
was extremely noisy and (2) the fonts used in the
documents are unknown. Berg-Kirkpatrick et al
(2013) proposed a system for historical OCR that
generatively models the noisy typesetting process
of printing-press era documents and learns the font
for each input document in an unsupervised fash-
ion. Their system achieves state-of-the-art results
on the task of historical document recognition.
We take the system of Berg-Kirkpatrick et al
(2013) as a starting point and consider extensions
of the typesetting model that address two short-
comings of their model: (1) their layout model as-
sumes that baseline offset noise is independent for
each glyph and (2) their font model assumes a sin-
gle font is used in every document. Both of these
assumptions are untrue in many historical datasets.
The baseline of the text in printing-press era
documents is not rigid as in modern documents but
rather drifts up and down noisily (see Figure 2).
In practice, the vertical offsets of character glyphs
change gradually along a line. This means the ver-
tical offsets of neighboring glyphs are correlated,
a relationship that is not captured by the original
model. In our first extension, we let the vertical
offsets of character glyphs be generated from a
Markov chain, penalizing large changes in offset.
We find that this extension decreases transcription
error rates. Our system achieves a relative word
error reduction of 22% compared to the state-of-
the-art original model on a test set of historical
newspapers (see Section 4.1), and a 11% relative
reduction on a test set of historical court proceed-
ings.
Multiple font styles are also frequently used in
printing-press era documents; the most common
scenario is for a basic font style to co-occur with
an italic variant. For example, it is common for
proper nouns and quotations to be italicized in
the Old Bailey corpus (Shoemaker, 2005). In our
second extension, we incorporate a Markov chain
over font styles, extending the original model so
that it is capable of simultaneously learning italic
and non-italic fonts within a single document. In
experiments, this model is able to detect which
words are italicized with 93% precision at 74%
recall in a test set of historical court proceedings
(see Section 4.2).
These richer models that we propose do in-
crease the state space and therefore make infer-
ence more costly. To remedy this, we stream-
line inference by replacing the coarse-to-fine in-
ference scheme of Berg-Kirkpatrick et al (2013)
118
ei 1 ei
gi pi
vi
gi 1
vi 1
pi 1
X
PAD
iX
GLYPH
iX
PAD
i 1X
GLYPH
i 1
fi 1 fi
O
r
i
g
i
n
a
l
m
o
d
e
l
S
l
o
w
-
v
a
r
y
I
t
a
l
i
c
{
{
{
Vertical offset
Glyph width
Pad width
Character
Font
Pixels
Figure 1: See Section 2 for a description of the generative process. We consider an extension of Berg-Kirkpatrick et al (2013)
that generates v
i
conditioned on the previous vertical offset v
i?1
(labeled Slow-vary) and an extension that generates a sequence
of font styles f
i
(labeled Italic).
with a forward-cost-augmented beaming scheme.
Our method is over 25x faster on a typical docu-
ment, yet actually yields improved transcriptions.
2 Model
We first describe the generative model used by
the ?Ocular? historical OCR system of Berg-
Kirkpatrick et al (2013)
1
and then describe our
extensions. The graphical model corresponding
to their basic generative process for a single line
of text is diagrammed in Figure 1. A Kneser-
Ney (Kneser and Ney, 1995) character 6-gram lan-
guage model generates a sequence of characters
E = (e
1
, e
2
, . . . , e
n
). For each character index i, a
glyph box width g
i
and a pad box width p
i
are gen-
erated, conditioned on the character e
i
. g
i
specifies
the width of the bounding box that will eventually
house the pixels of the glyph for character e
i
. p
i
specifies the width of a padding box which con-
tains the horizontal space before the next character
begins. Next, a vertical offset v
i
is generated for
the glyph corresponding to character e
i
. v
i
allows
the model to capture variance in the baseline of the
text in the document. We will later let v
i
depend
on v
i?1
, as depicted in Figure 1, but in the baseline
1
The model we describe and extend has two minor dif-
ferences from the one described by Berg-Kirkpatrick et al
(2013). While Berg-Kirkpatrick et al (2013) generate two
pad boxes for each character token, one to the left and one to
the right, we only generate one pad box, always to the right.
Additionally, Berg-Kirkpatrick et al (2013) do not carry over
the language model context between lines, while we do.
system they are independent. Finally, the pixels in
the ith glyph bounding box X
GLYPH
i
are generated
conditioned on the character e
i
, width g
i
, and ver-
tical offset v
i
, and the pixels in the ith pad bound-
ing box X
PAD
i
are generated conditioned on the
width p
i
. We refer the reader to Berg-Kirkpatrick
et al (2013) for the details of the pixel generation
process. We have omitted the token-level inking
random variables for the purpose of brevity. These
can be treated as part of the pixel generation pro-
cess.
Let X denote the matrix of pixels for the entire
line, V = (v
1
, . . . , v
n
), P = (p
1
, . . . , p
n
), and
G = (g
1
, . . . , g
n
). The joint distribution is writ-
ten:
P (X,V, P,G,E) =
P (E) [Language model]
?
n
?
i=1
P (g
i
|e
i
; ?) [Glyph widths]
?
n
?
i=1
P (p
i
|e
i
; ?) [Pad widths]
?
n
?
i=1
P (v
i
) [Vertical offsets]
?
n
?
i=1
P (X
PAD
i
|p
i
) [Pad pixels]
?
n
?
i=1
P (X
GLYPH
i
|v
i
, g
i
, e
i
; ?) [Glyph pixels]
119
Document image:
Learned typsetting
independent offsets:
slow-varying offsets:
Learned typsetting
Figure 2: The first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-SV model. The second line
depicts the same, but for the OCULAR-BEAM model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display
the Bernoulli template probabilities used to generate the observed pixels. The third line shows the corresponding portion of the
input image.
The font is parameterized by the vector ? which
governs the shapes of glyphs and the distributions
over box widths. ? is learned in an unsupervised
fashion. Document recognition is accomplished
via Viterbi decoding over the character random
variables e
i
.
2.1 Slow-varying Offsets
The original model generates the vertical offsets
v
i
independently, and therefore cannot model how
neighboring offsets are correlated. This correla-
tion is actually strong in printing-press era docu-
ments. The baseline of the text wanders in the in-
put image for two reasons: (1) the physical groove
along which character templates were set was un-
even and (2) the original document was imaged in
a way that produced distortion. Both these under-
lying causes are likely to yield baselines that wan-
der slowly up and down across a document. We
refer to this behavior of vertical offsets as slow-
varying, and extend the model to capture it.
In our first extension, we augment the model
by incorporating a Markov chain over the verti-
cal offset random variables v
i
, as depicted in Fig-
ure 1. Specifically, v
i
is generated from a dis-
cretized Gaussian centered at v
i?1
:
P (v
i
|v
i?1
) ? exp
(
(v
i
? v
i?1
)
2
2?
2
)
This means that the if v
i
differs substantially from
v
i?1
, a large penalty is incurred. As a result,
the model should prefer sequences of v
i
that vary
slowly. In experiments, we set ?
2
= 0.05.
2.2 Italic Font Styles
Many of the documents in the Old Bailey corpus
contain both italic and non-italic font styles (Shoe-
maker, 2005). The way that italic fonts are used
depends on the year the document was printed,
but generally italics are reserved for proper nouns,
quotations, and sentences that have a special role
(e.g. the final judgment made in a court case). The
switch between font styles almost always occurs
at space characters.
Our second extension of the typesetting model
deals with both italic and non-italic font styles.
We augment the model with a Markov chain
over font styles f
i
, as depicted in Figure 1.
Each font style token f
i
takes on a value in
{ITALIC, NON-ITALIC} and is generated condi-
tioned on the previous font style f
i?1
and the cur-
rent character token e
i
. Specifically, after generat-
ing a character token that is not a space, the lan-
guage model deterministically generates the last
font used. If the language model generates a space
character token, the decision of whether to switch
font styles is drawn from a Bernoulli distribution.
This ensures that the font style only changes at
space characters.
The font parameters ? are extended to contain
entries for the italic versions of all characters. This
means the shapes and widths of italic glyphs can
be learned separately from non-italic ones. Like
Berg-Kirkpatrick et al (2013), we initialize the
font parameters from mixtures of modern fonts,
using mixtures of modern italic font styles for
italic characters.
3 Streamlined Inference
Inference in our extended typesetting models is
costly because the state space is large; we propose
an new inference procedure that is fast and simple.
Berg-Kirkpatrick et al (2013) used EM to learn
the font parameters ?, and therefore required ex-
pected sufficient statistics (indicators on (e
i
, g
i
, v
i
)
tuples), which they computed using coarse-to-
fine inference (Petrov et al, 2008; Zhang and
Gildea, 2008) with a semi-Markov dynamic pro-
gram (Levinson, 1986). This approach is effec-
120
Document image:
Learned typesetting:
Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown
in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels.
The second line shows the corresponding portion of the input image.
tive, but slow. For example, while transcribing a
typical document consisting of 30 lines of text,
their system spends 63 minutes computing ex-
pected sufficient statistics and decoding when run
on a 4.5GHz 4-core CPU.
We instead use hard counts of the sufficient
statistics for learning (i.e. perform hard-EM). As a
result, we are free to use inference procedures that
are specialized for Viterbi computation. Specif-
ically, we use beam-search with estimated for-
ward costs. Because the model is semi-Markov,
our beam-search procedure is very similar the
one used by Pharaoh (Koehn, 2004) for phrase-
based machine translation, only without a distor-
tion model. We use a beam of size 20, and estimate
forward costs using a character bigram language
model. On the machine mentioned above, tran-
scribing the same document, our simplified system
that uses hard-EM and beam-search spends only
2.4 minutes computing sufficient statistics and de-
coding. This represents a 26x speedup.
4 Results
We ran experiments with four different systems.
The first is our baseline, the system presented
by Berg-Kirkpatrick et al (2013), which we re-
fer to as OCULAR. The second system uses the
original model, but uses beam-search for infer-
ence. We refer to this system as OCULAR-BEAM.
The final two systems use beam-search for infer-
ence, but use extended models: OCULAR-BEAM-
SV uses the slow-varying vertical offset extension
described in Section 2.1 and OCULAR-BEAM-
IT uses the italic font extension described in Sec-
tion 2.2.
We evaluate on two different test sets of histor-
ical documents. The first test set is called Trove,
and is used by Berg-Kirkpatrick et al (2013) for
evaluation. Trove consists of 10 documents that
were printed between 1803 and 1954, each con-
sisting of 30 lines, all taken from a collection of
historical Australian newspapers hosted by the Na-
tional Library of Australia (Holley, 2010). The
second test set, called Old Bailey, consists of 20
documents that were printed between 1716 and
1906, each consisting of 30 lines, all taken from
a the proceedings of the Old Bailey Courthouse
in London (Shoemaker, 2005).
2
Following Berg-
Kirkpatrick et al (2013), we train the language
model using 36 millions words from the New York
Times portion of the Gigaword corpus (Graff et al,
2007).
3
4.1 Document Recognition Performance
We evaluate predicted transcriptions using both
character error rate (CER) and word error rate
(WER). CER is the edit distance between the
guessed transcription and the gold transcription,
divided by the number of characters in the gold
transcription. WER is computed in the same way,
but words are treated as tokens instead of charac-
ters.
First we compare the baseline, OCULAR, to
our system with simplified inference, OCULAR-
BEAM. To our surprise, we found that OCULAR-
BEAM produced better transcriptions than OCU-
LAR. On Trove, OCULAR achieved a WER of
33.0 while OCULAR-BEAM achieved a WER of
30.7. On Old Bailey, OCULAR achieved a WER
of 30.8 while OCULAR-BEAM achieved a WER of
28.8. These results are shown in Table 1, where we
also report the performance of Google Tesseract
(Smith, 2007) and ABBYY FineReader, a state-
of-the-art commercial system, on the Trove test set
(taken from Berg-Kirkpatrick et al (2013)).
Next, we evaluate our slow-varying vertical off-
set model. OCULAR-BEAM-SV out-performs
OCULAR-BEAM on both test sets. On Trove,
OCULAR-BEAM-SV achieved a WER of 25.6,
and on Old Bailey, OCULAR-BEAM-SV achieved
a WER of 27.5. Overall, compared to our baseline
2
Old Bailey is comparable to the the second test set used
by Berg-Kirkpatrick et al (2013) since it is derived from the
same collection and covers a similar time span, but it consists
of different documents.
3
This means the language model is out-of-domain on both
test sets. Berg-Kirkpatrick et al (2013) also consider a per-
fectly in-domain language model, though this setting is some-
what unrealistic.
121
system, OCULAR-BEAM-SV achieved a relative
reduction in WER of 22% on Trove and 11% on
Old Bailey.
By looking at the predicted typesetting layouts
we can make a qualitative comparison between the
vertical offsets predicted by OCULAR-BEAM and
OCULAR-BEAM-SV. Figure 2 shows representa-
tions of the Viterbi estimates of the typesetting
random variables predicted by the models on a
portion of an example document. The first line
is the typesetting layout predicted by OCULAR-
BEAM-SV and the second line is same, but for
OCULAR-BEAM. The locations of padding boxes
are depicted in blue. The white glyph bounding
boxes reveal the values of the Bernoulli template
probabilities used to generate the observed pixels.
The Bernoulli templates are produced from type-
level font parameters, but are modulated by token-
level widths g
i
and vertical offsets v
i
(and ink-
ing random variables, whose description we have
omitted for brevity). The predicted vertical off-
sets are visible in the shifted baselines of the tem-
plate probabilities. The third line shows the corre-
sponding portion of the input image. In this ex-
ample, the text baseline predicted by OCULAR-
BEAM-SV is contiguous, while the one predicted
by OCULAR-BEAM is not. Given how OCULAR-
BEAM-SV was designed, this meets our expecta-
tions. The text baseline predicted by OCULAR-
BEAM has a discontinuity in the middle of its pre-
diction for the gold word Surplus. In contrast,
the vertical offsets predicted by OCULAR-BEAM-
SV at this location vary smoothly and more ac-
curately match the true text baseline in the input
image.
4.2 Font Detection Performance
We ran experiments with the italic font style
model, OCULAR-BEAM-IT, on the Old Bai-
ley test set (italics are infrequent in Trove). We
evaluated the learned styles by measuring how ac-
curately OCULAR-BEAM-IT was able to distin-
guish between italic and non-italic styles. Specifi-
cally, we computed the precision and recall for the
system?s predictions about which words were ital-
icized. We found that, across the entire Old Bai-
ley test set, OCULAR-BEAM-IT was able to detect
which words were italicized with 93% precision
at 74% recall, suggesting that the system did suc-
cessfully learn both italic and non-italic styles.
4
4
While it seems plausible that learning italics could also
improve transcription accuracy, we found that OCULAR-
System CER WER
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
OCULAR (baseline) 14.9 33.0
OCULAR-BEAM 12.9 30.7
OCULAR-BEAM-SV 11.2 25.6
Old Bailey
OCULAR (baseline) 14.9 30.8
OCULAR-BEAM 10.9 28.8
OCULAR-BEAM-SV 10.3 27.5
Table 1: We evaluate the output of each system on two test
sets: Trove, a collection of historical newspapers, and Old
Bailey, a collection of historical court proceedings. We report
character error rate (CER) and word error rate (WER), macro-
averaged across documents.
We can look at the typesetting layout predicted
by OCULAR-BEAM-IT to gain insight into what
has been learned by the model. The first line of
Figure 3 shows the typesetting layout predicted by
the OCULAR-BEAM-IT model for a line of a doc-
ument image that contains italics. The second line
of Figure 3 displays the corresponding portion of
the input document image. From this example,
it appears that the model has effectively learned
separate glyph shapes for italic and non-italic ver-
sions of certain characters. For example, compare
the template probabilities used to generate the d?s
in defraud to the template probabilities used to
generate the d in hard.
5 Conclusion
We began with an efficient simplification of the
state-of-the-art historical OCR system of Berg-
Kirkpatrick et al (2013) and demonstrated two ex-
tensions to its underlying model. We saw an im-
provement in transcription quality as a result of re-
moving a harmful independence assumption. This
suggests that it may be worthwhile to consider still
further extensions of the model, designed to more
faithfully reflect the generative process that pro-
duced the input documents.
Acknowledgments
This work was supported by Grant IIS-1018733
from the National Science Foundation and also a
National Science Foundation fellowship to the first
author.
BEAM-IT actually performed slightly worse than OCULAR-
BEAM. This negative result is possibly due to the extra diffi-
culty of learning a larger number of font parameters.
122
References
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &
Imaging Review.
Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical doc-
uments. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: From real
users to research, pages 115?124. Springer.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech & Language.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the Tesseract OCR
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
123
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 822?827,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
How much do word embeddings encode about syntax?
Jacob Andreas and Dan Klein
Computer Science Division
University of California, Berkeley
{jda,klein}@cs.berkeley.edu
Abstract
Do continuous word embeddings encode
any useful information for constituency
parsing? We isolate three ways in which
word embeddings might augment a state-
of-the-art statistical parser: by connecting
out-of-vocabulary words to known ones,
by encouraging common behavior among
related in-vocabulary words, and by di-
rectly providing features for the lexicon.
We test each of these hypotheses with a
targeted change to a state-of-the-art base-
line. Despite small gains on extremely
small supervised training sets, we find
that extra information from embeddings
appears to make little or no difference
to a parser with adequate training data.
Our results support an overall hypothe-
sis that word embeddings import syntac-
tic information that is ultimately redun-
dant with distinctions learned from tree-
banks in other ways.
1 Introduction
This paper investigates a variety of ways in
which word embeddings might augment a con-
stituency parser with a discrete state space. Word
embeddings?representations of lexical items as
points in a real vector space?have a long history
in natural language processing, going back at least
as far as work on latent semantic analysis (LSA)
for information retrieval (Deerwester et al, 1990).
While word embeddings can be constructed di-
rectly from surface distributional statistics, as in
LSA, more sophisticated tools for unsupervised
extraction of word representations have recently
gained popularity (Collobert et al, 2011; Mikolov
et al, 2013a). Semi-supervised and unsupervised
models for a variety of core NLP tasks, includ-
ing named-entity recognition (Freitag, 2004), part-
of-speech tagging (Sch?utze, 1995), and chunking
(Turian et al, 2010) have been shown to benefit
from the inclusion of word embeddings as fea-
tures. In the other direction, access to a syntac-
tic parse has been shown to be useful for con-
structing word embeddings for phrases composi-
tionally (Hermann and Blunsom, 2013; Andreas
and Ghahramani, 2013). Dependency parsers have
seen gains from distributional statistics in the form
of discrete word clusters (Koo et al, 2008), and re-
cent work (Bansal et al, 2014) suggests that simi-
lar gains can be derived from embeddings like the
ones used in this paper.
It has been less clear how (and indeed whether)
word embeddings in and of themselves are use-
ful for constituency parsing. There certainly exist
competitive parsers that internally represent lexi-
cal items as real-valued vectors, such as the neural
network-based parser of Henderson (2004), and
even parsers which use pre-trained word embed-
dings to represent the lexicon, such as Socher et
al. (2013). In these parsers, however, use of word
vectors is a structural choice, rather than an added
feature, and it is difficult to disentangle whether
vector-space lexicons are actually more powerful
than their discrete analogs?perhaps the perfor-
mance of neural network parsers comes entirely
from the model?s extra-lexical syntactic structure.
In order to isolate the contribution from word em-
beddings, it is useful to demonstrate improvement
over a parser that already achieves state-of-the-art
performance without vector representations.
The fundamental question we want to explore
is whether embeddings provide any information
beyond what a conventional parser is able to in-
duce from labeled parse trees. It could be that
the distinctions between lexical items that embed-
dings capture are already modeled by parsers in
other ways and therefore provide no further bene-
fit. In this paper, we investigate this question em-
pirically, by isolating three potential mechanisms
for improvement from pre-trained word embed-
822
0 0.1 0.2 0.3 0.4 0.5 0.6
?0.4
?0.2
0
0.2
0.4
0.6
0.8
a
the
this
that
mostfew
each
every
Figure 1: Word representations of English de-
terminers, projected onto their first two principal
components. Embeddings from Collobert et al
(2011).
dings. Our result is mostly negative. With ex-
tremely limited training data, parser extensions us-
ing word embeddings give modest improvements
in accuracy (relative error reduction on the order
of 1.5%). However, with reasonably-sized training
corpora, performance does not improve even when
a wide variety of embedding methods, parser mod-
ifications, and parameter settings are considered.
The fact that word embedding features result
in nontrivial gains for discriminative dependency
parsing (Bansal et al, 2014), but do not appear to
be effective for constituency parsing, points to an
interesting structural difference between the two
tasks. We hypothesize that dependency parsers
benefit from the introduction of features (like clus-
ters and embeddings) that provide syntactic ab-
stractions; but that constituency parsers already
have access to such abstractions in the form of su-
pervised preterminal tags.
2 Three possible benefits of word
embeddings
We are interested in the question of whether
a state-of-the-art discrete-variable constituency
parser can be improved with word embeddings,
and, more precisely, what aspect (or aspects) of
the parser can be altered to make effective use of
embeddings.
It seems clear that word embeddings exhibit
some syntactic structure. Consider Figure 1,
which shows embeddings for a variety of English
determiners, projected onto their first two princi-
pal components. We can see that the quantifiers
each and every cluster together, as do few and
most. These are precisely the kinds of distinc-
tions between determiners that state-splitting in
the Berkeley parser has shown to be useful (Petrov
and Klein, 2007), and existing work (Mikolov et
al., 2013b) has observed that such regular em-
bedding structure extends to many other parts of
speech. But we don?t know how prevalent or
important such ?syntactic axes? are in practice.
Thus we have two questions: Are such groupings
(learned on large data sets but from less syntacti-
cally rich models) better than the ones the parser
finds on its own? How much data is needed to
learn them without word embeddings?
We consider three general hypotheses about
how embeddings might interact with a parser:
1. Vocabulary expansion hypothesis: Word
embeddings are useful for handling out-of-
vocabulary words, because they automati-
cally ensure that unknown words are treated
the same way as known words with similar
representations. Example: the infrequently-
occurring treebank tag UH dominates greet-
ings (among other interjections). Upon en-
countering the unknown word hey, the parser
assigns a low posterior probability of hav-
ing been generated from UH. But its distri-
butional representation is very close to the
known word hello, and a model capable of
mapping hey to its neighbor should be able to
assign the right tag.
2. Statistic sharing hypothesis: Word embed-
dings are useful for handling in-vocabulary
words, by making it possible to pool statistics
for related words. Example: individual first
names are also rare in the treebank, but tend
to cluster together in distributional represen-
tations. A parser which exploited this effect
could use this to acquire a robust model of
name behavior by sharing statistics from all
first names together, preventing low counts
from producing noisy models of names.
3. Embedding structure hypothesis: The
structure of the space used for the embed-
dings directly encodes syntactic information
in its coordinate axes. Example: with the
exception of a, the vertical axis in Figure 1
823
seems to group words by definiteness. We
would expect a feature corresponding to a
word?s position along this axis to be a useful
feature in a feature-based lexicon.
Note that these hypotheses are not all mutually
exclusive, and two or all of them might provide in-
dependent gains. Our first task is thus to design a
set of orthogonal experiments which make it pos-
sible to test each of the three hypotheses in isola-
tion. It is also possible that other mechanisms are
at play that are not covered by these three hypothe-
ses, but we consider these three to be likely central
effects.
3 Parser extensions
For the experiments in this paper, we will use
the Berkeley parser (Petrov and Klein, 2007) and
the related Maryland parser (Huang and Harper,
2011). The Berkeley parser induces a latent, state-
split PCFG in which each symbol V of the (ob-
served) X-bar grammar is refined into a set of
more specific symbols {V
1
, V
2
, . . .} which cap-
ture more detailed grammatical behavior. This
allows the parser to distinguish between words
which share the same tag but exhibit very differ-
ent syntactic behavior?for example, between ar-
ticles and demonstrative pronouns. The Maryland
parser builds on the state-splitting parser, replac-
ing its basic word emission model with a feature-
rich, log-linear representation of the lexicon.
The choice of this parser family has two moti-
vations. First, these parsers are among the best in
the literature, with a test performance of 90.7 F
1
for the baseline Berkeley parser on the Wall Street
Journal corpus (compared to 90.4 for Socher et al
(2013) and 90.1 for Henderson (2004)). Second,
and more importantly, the fact that they use no
continuous state representations internally makes
it easy to design experiments that isolate the con-
tributions of word vectors, without worrying about
effects from real-valued operators higher up in the
model. We consider the following extensions:
Vocabulary expansion ? OOV model
To evaluate the vocabulary expansion hypothe-
sis, we introduce a simple but targeted out-of-
vocabulary (OOV) model in which every unknown
word is simply replaced by its nearest neighbor in
the training set. For OOV words which are not in
the dictionary of embeddings, we back off to the
unknown word model for the underlying parser.
Statistic sharing ? Lexicon pooling model
To evaluate the statistic sharing hypothesis, we
propose a novel smoothing technique. The Berke-
ley lexicon stores, for each latent (tag, word) pair,
the probability p(w|t) directly in a lookup ta-
ble. If we want to encourage similarly-embedded
words to exhibit similar behavior in the generative
model, we need to ensure that the are preferen-
tially mapped onto the same latent preterminal tag.
In order to do this, we replace this direct lookup
with a smoothed, kernelized lexicon, where:
p(w|t) =
1
Z
?
w
?
?
t,w
?
e
??||?(w)??(w
?
)||
2
(1)
with Z a normalizing constant to ensure that p(?|t)
sums to one over the entire vocabulary. ?(w) is the
vector representation of the word w, ?
t,w
are per-
basis weights, and ? is an inverse radius parame-
ter which determines the strength of the smooth-
ing. Each ?
t,w
is learned in the same way as
its corresponding probability in the original parser
model?during each M step of the training proce-
dure, ?
w,t
is set to the expected number of times
the word w appears under the refined tag t. Intu-
itively, as ? grows small groups of related words
will be assigned increasingly similar probabilities
of being generated from the same tag (in the limit
where ? = 0, Equation 1 is a uniform distribu-
tion over the entire vocabulary). As ? grows large
words become more independent (and in the limit
where ? = ?, each summand in Equation 1 is
zero except where w
?
= w, and we recover the
original direct-lookup model).
There are computational concerns associated
with this approach: the original scoring procedure
for a (word, tag) pair was a single (constant-time)
lookup; here it might take time linear in the size
of the vocabulary. This causes parsing to become
unacceptably slow, so an approximation is neces-
sary. Luckily, the exponential decay of the kernel
ensures that each word shares most of its weight
with a small number of close neighbors, and al-
most none with words farther away. To exploit
this, we pre-compute the k-nearest-neighbor graph
of points in the embedding space, and take the sum
in Equation 1 only over this set of nearest neigh-
bors. Empirically, taking k = 20 gives adequate
performance, and increasing it does not seem to
alter the behavior of the parser.
As in the OOV model, we also need to worry
about how to handle words for which we have no
824
vector representation. In these cases, we simply
treat the words as if their vectors were so far away
from everything else they had no influence, and
report their weights as p(w|t) = ?
w
. This ensures
that our model continues to include the original
Berkeley parser model as a limiting case.
Embedding structure ? embedding features
To evaluate the embedding structure hypothesis,
we take the Maryland featured parser, and replace
the set of lexical template features used by that
parser with a set of indicator features on a dis-
cretized version of the embedding. For each di-
mension i, we create an indicator feature corre-
sponding to the linearly-bucketed value of the fea-
ture at that index. In order to focus specifically
on the effect of word embeddings, we remove the
morphological features from the parser, but retain
indicators on the identity of each lexical item.
The extensions we propose are certainly not
the only way to target the hypotheses described
above, but they have the advantage of being min-
imal and straightforwardly interpretable, and each
can be reasonably expected to improve parser per-
formance if its corresponding hypothesis is true.
4 Experimental setup
We use the Maryland implementation of the
Berkeley parser as our baseline for the kernel-
smoothed lexicon, and the Maryland featured
parser as our baseline for the embedding-featured
lexicon.
1
For all experiments, we use 50-
dimensional word embeddings. Embeddings la-
beled C&W are from Collobert et al (2011); em-
beddings labeled CBOW are from Mikolov et al
(2013a), trained with a context window of size 2.
Experiments are conducted on the Wall Street
Journal portion of the English Penn Treebank. We
prepare three training sets: the complete training
set of 39,832 sentences from the treebank (sec-
tions 2 through 21), a smaller training set, consist-
ing of the first 3000 sentences, and an even smaller
set of the first 300.
Per-corpus-size settings of the parameter ? are
set by searching over several possible settings on
the development set. For each training corpus size
we also choose a different setting of the number of
splitting iterations over which the Berkeley parser
is run; for 300 sentences this is two splits, and for
1
Both downloaded from https://code.google.
com/p/umd-featured-parser/
Model 300 3000 Full
Baseline 71.88 84.70 91.13
OOV (C&W) 72.20 84.77 91.22
OOV (CBOW) 72.20 84.78 91.22
Pooling (C&W) 72.21 84.55 91.11
Pooling (CBOW) 71.61 84.73 91.15
Features (ident) 67.27 82.77 90.65
Features (C&W) 70.32 83.78 91.08
Features (CBOW) 69.87 84.46 90.86
Table 1: Contributions from OOV, lexical pooling
and featured models, for two kinds of embeddings
(C&W and CBOW). For both choices of embed-
ding, the pooling and OOV models provide small
gains with very little training data, but no gains
on the full training set. The featured model never
achieves scores higher than the generative base-
line.
Model 300 3000 Full
Baseline 72.02 84.09 90.70
Pool + OOV (C&W) 72.43
?
84.36
?
90.11
Table 2: Test set experiments with the best com-
bination of models (based on development exper-
iments). Again, we observe small gains with re-
stricted training sets but no gains on the full train-
ing set. Entries marked
?
are statistically signifi-
cant (p < 0.05) under a paired bootstrap resam-
pling test.
3000 four splits. This is necessary to avoid over-
fitting on smaller training sets. Consistent with the
existing literature, we stop at six splits when using
the full training corpus.
5 Results
Various model-specific experiments are shown in
Table 1. We begin by investigating the OOV
model. As can be seen, this model alone achieves
small gains over the baseline for a 300-word train-
ing corpus, but these gains become statistically in-
significant with more training data. This behavior
is almost completely insensitive to the choice of
embedding.
Next we consider the lexicon pooling model.
We began by searching over exponentially-spaced
values of ? to determine an optimal setting for
825
Experiment WSJ ? Brown French
Baseline 86.36 74.84
Pool + OOV 86.42 75.18
Table 3: Experiments for other corpora, using the
same combined model (lexicon pooling and OOV)
as in Table 2. Again, we observe no significant
gains over the baseline.
each training set size; as expected, for small set-
tings of ? (corresponding to aggressive smooth-
ing) performance decreased; as we increased the
parameter, performance increased slightly before
tapering off to baseline parser performance. The
first block in Table 1 shows the best settings of ?
for each corpus size; as can be seen, this also gives
a small improvement on the 300-sentence training
corpus, but no discernible once the system has ac-
cess to a few thousand labeled sentences.
Last we consider a model with a featured lex-
icon, as described in Huang and Harper (2011).
A baseline featured model (?ident?) contains only
indicator features on word identity (and performs
considerably worse than its generative counter-
part on small data sets). As described above, the
full featured model adds indicator features on the
bucketed value of each dimension of the word em-
bedding. Here, the trend observed in the other two
models is even more prominent?embedding fea-
tures lead to improvements over the featured base-
line, but in no case outperform the standard base-
line with a generative lexicon.
We take the best-performing combination of all
of these models (based on development experi-
ments, a combination of the lexical pooling model
with ? = 0.3, and OOV, both using C&W word
embeddings), and evaluate this on the WSJ test
set (Table 2). We observe very small (but statis-
tically significant) gains with 300 and 3000 train
sentences, but a decrease in performance on the
full corpus.
To investigate the possibility that improvements
from embeddings are exceptionally difficult to
achieve on the Wall Street Journal corpus, or on
English generally, we perform (1) a domain adap-
tation experiment, in which we use the OOV and
lexicon pooling models to train on WSJ and test
on the first 4000 sentences of the Brown corpus
(the ?WSJ ? Brown? column in Table 3), and (2)
a multilingual experiment, in which we train and
test on the French treebank (the ?French? column).
Apparent gains from the OOV and lexicon pooling
models remain so small as to be statistically indis-
tinguishable.
6 Conclusion
With the goal of exploring how much useful syn-
tactic information is provided by unsupervised
word embeddings, we have presented three vari-
ations on a state-of-the-art parsing model, with
extensions to the out-of-vocabulary model, lexi-
con, and feature set. Evaluation of these modi-
fied parsers revealed modest gains on extremely
small training sets, which quickly vanish as train-
ing set size increases. Thus, at least restricted to
phenomena which can be explained by the exper-
iments described here, our results are consistent
with two claims: (1) unsupervised word embed-
dings do contain some syntactically useful infor-
mation, but (2) this information is redundant with
what the model is able to determine for itself from
only a small amount of labeled training data.
It is important to emphasize that these results
do not argue against the use of continuous repre-
sentations in a parser?s state space, nor argue more
generally that constituency parsers cannot possi-
bly benefit from word embeddings. However, the
failure to uncover gains when searching across a
variety of possible mechanisms for improvement,
training procedures for embeddings, hyperparam-
eter settings, tasks, and resource scenarios sug-
gests that these gains (if they do exist) are ex-
tremely sensitive to these training conditions, and
not nearly as accessible as they seem to be in de-
pendency parsers. Indeed, our results suggest a
hypothesis that word embeddings are useful for
dependency parsing (and perhaps other tasks) be-
cause they provide a level of syntactic abstrac-
tion which is explicitly annotated in constituency
parses. We leave explicit investigation of this hy-
pothesis for future work.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014. The first
author is supported by a National Science Foun-
dation Graduate Research Fellowship.
826
References
Jacob Andreas and Zoubin Ghahramani. 2013. A gen-
erative model of vector space semantics. In Pro-
ceedings of the ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, GeorgeW. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Dayne Freitag. 2004. Trained named entity recog-
nition using distributional clusters. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904, Sofia, Bulgaria, August.
Zhongqiang Huang and Mary P. Harper. 2011.
Feature-rich log-linear lexical model for latent vari-
able pcfg grammars. In Proceedings of the Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 219?227.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 595?
603.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 746?751.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics. Assocation for Computational
Linguistics.
Hinrich Sch?utze. 1995. Distributional part-of-speech
tagging. In Proceedings of the European Associa-
tion for Computational Linguistics, pages 141?148.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
827
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46?54,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Monolingual Models with Unannotated Bilingual Text
David Burkett? Slav Petrov? John Blitzer? Dan Klein?
?University of California, Berkeley ?Google Research
{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.com
Abstract
This work shows how to improve state-of-the-art
monolingual natural language processing models
using unannotated bilingual text. We build a mul-
tiview learning objective that enforces agreement
between monolingual and bilingual models. In
our method the first, monolingual view consists of
supervised predictors learned separately for each
language. The second, bilingual view consists of
log-linear predictors learned over both languages
on bilingual text. Our training procedure estimates
the parameters of the bilingual model using the
output of the monolingual model, and we show how
to combine the two models to account for depen-
dence between views. For the task of named entity
recognition, using bilingual predictors increases F1
by 16.1% absolute over a supervised monolingual
model, and retraining on bilingual predictions
increases monolingual model F1 by 14.6%. For
syntactic parsing, our bilingual predictor increases
F1 by 2.1% absolute, and retraining a monolingual
model on its output gives an improvement of 2.0%.
1 Introduction
Natural language analysis in one language can be
improved by exploiting translations in another lan-
guage. This observation has formed the basis for
important work on syntax projection across lan-
guages (Yarowsky et al, 2001; Hwa et al, 2005;
Ganchev et al, 2009) and unsupervised syntax
induction in multiple languages (Snyder et al,
2009), as well as other tasks, such as cross-lingual
named entity recognition (Huang and Vogel, 2002;
Moore, 2003) and information retrieval (Si and
Callan, 2005). In all of these cases, multilingual
models yield increased accuracy because differ-
ent languages present different ambiguities and
therefore offer complementary constraints on the
shared underlying labels.
In the present work, we consider a setting where
we already possess supervised monolingual mod-
els, and wish to improve these models using unan-
notated bilingual parallel text (bitext). We cast this
problem in the multiple-view (multiview) learning
framework (Blum and Mitchell, 1998; Collins and
Singer, 1999; Balcan and Blum, 2005; Ganchev et
al., 2008). Our two views are a monolingual view,
which uses the supervised monolingual models but
not bilingual information, and a bilingual view,
which exploits features that measure agreement
across languages. The parameters of the bilin-
gual view are trained to reproduce the output of
the monolingual view. We show that by introduc-
ing weakened monolingual models into the bilin-
gual view, we can optimize the parameters of the
bilingual model to improve monolingual models.
At prediction time, we automatically account for
the between-view dependence introduced by the
weakened monolingual models with a simple but
effective view-combination heuristic.
We demonstrate the performance of this method
on two problems. The first is named en-
tity recognition (NER). For this problem, our
method automatically learns (a variation on) ear-
lier hand-designed rule-based bilingual NER pre-
dictors (Huang and Vogel, 2002; Moore, 2003),
resulting in absolute performance gains of up to
16.1% F1. The second task we consider is statis-
tical parsing. For this task, we follow the setup
of Burkett and Klein (2008), who improved Chi-
nese and English monolingual parsers using par-
allel, hand-parsed text. We achieve nearly iden-
tical improvements using a purely unlabeled bi-
text. These results carry over to machine transla-
tion, where we can achieve slightly better BLEU
improvements than the supervised model of Bur-
kett and Klein (2008) since we are able to train
our model directly on the parallel data where we
perform rule extraction.
Finally, for both of our tasks, we use our bilin-
gual model to generate additional automatically
labeled monolingual training data. We compare
46
this approach to monolingual self-training and
show an improvement of up to 14.4% F1 for entity
recognition. Even for parsing, where the bilingual
portion of the treebank is much smaller than the
monolingual, our technique still can improve over
purely monolingual self-training by 0.7% F1.
2 Prior Work on Learning from
Bilingual Text
Prior work in learning monolingual models from
bitexts falls roughly into three categories: Unsu-
pervised induction, cross-lingual projection, and
bilingual constraints for supervised monolingual
models. Two recent, successful unsupervised
induction methods are those of Blunsom et al
(2009) and Snyder et al (2009). Both of them es-
timate hierarchical Bayesian models and employ
bilingual data to constrain the types of models that
can be derived. Projection methods, on the other
hand, were among the first applications of parallel
text (after machine translation) (Yarowsky et al,
2001; Yarowsky and Ngai, 2001; Hwa et al, 2005;
Ganchev et al, 2009). They assume the existence
of a good, monolingual model for one language
but little or no information about the second lan-
guage. Given a parallel sentence pair, they use the
annotations for one language to heavily constrain
the set of possible annotations for the other.
Our work falls into the final category: We wish
to use bilingual data to improve monolingual mod-
els which are already trained on large amounts of
data and effective on their own (Huang and Vo-
gel, 2002; Smith and Smith, 2004; Snyder and
Barzilay, 2008; Burkett and Klein, 2008). Proce-
durally, our work is most closely related to that
of Burkett and Klein (2008). They used an an-
notated bitext to learn parse reranking models for
English and Chinese, exploiting features that ex-
amine pieces of parse trees in both languages. Our
method can be thought of as the semi-supervised
counterpart to their supervised model. Indeed, we
achieve nearly the same results, but without anno-
tated bitexts. Smith and Smith (2004) consider
a similar setting for parsing both English and Ko-
rean, but instead of learning a joint model, they
consider a fixed combination of two parsers and
a word aligner. Our model learns parameters for
combining two monolingual models and poten-
tially thousands of bilingual features. The result
is that our model significantly improves state-of-
the-art results, for both parsing and NER.
3 A Multiview Bilingual Model
Given two input sentences x = (x1, x2) that
are word-aligned translations of each other, we
consider the problem of predicting (structured)
labels y = (y1, y2) by estimating conditional
models on pairs of labels from both languages,
p(y1, y2|x1, x2). Our model consists of two views,
which we will refer to as monolingual and bilin-
gual. The monolingual view estimates the joint
probability as the product of independent marginal
distributions over each language, pM (y|x) =
p1(y1|x1)p2(y2|x2). In our applications, these
marginal distributions will be computed by state-
of-the-art statistical taggers and parsers trained on
large monolingual corpora.
This work focuses on learning parameters for
the bilingual view of the data. We parameterize
the bilingual view using at most one-to-one match-
ings between nodes of structured labels in each
language (Burkett and Klein, 2008). In this work,
we use the term node to indicate a particular com-
ponent of a label, such as a single (multi-word)
named entity or a node in a parse tree. In Fig-
ure 2(a), for example, the nodes labeled NP1 in
both the Chinese and English trees are matched.
Since we don?t know a priori how the components
relate to one another, we treat these matchings as
hidden. For each matching a and pair of labels
y, we define a feature vector ?(y1, a, y2) which
factors on edges in the matching. Our model is
a conditional exponential family distribution over
matchings and labels:
p?(y, a|x) = exp
[
?>?(y1, a, y2)?A(?;x)
]
,
where ? is a parameter vector, and A(?;x) is the
log partition function for a sentence pair x. We
must approximate A(?;x) because summing over
all at most one-to-one matchings a is #P-hard. We
approximate this sum using the maximum-scoring
matching (Burkett and Klein, 2008):
A?(?;x) = log
?
y
max
a
(
exp
[
?>?(y1, a, y2)
])
.
In order to compute the distribution on labels y, we
must marginalize over hidden alignments between
nodes, which we also approximate by using the
maximum-scoring matching:
q?(y|x)
def
= max
a
exp
[
?>?(y1, a, y2)?A?(?;x)
]
.
47
the reports of European Court
ORG1
of Auditors
die Berichte des Europ?ischen Rechnungshofes
ORG1
the
Figure 1: An example where English NER can be
used to disambiguate German NER.
We further simplify inference in our model by
working in a reranking setting (Collins, 2000;
Charniak and Johnson, 2005), where we only con-
sider the top k outputs from monolingual models
in both languages, for a total of k2 labels y. In
practice, k2 ? 10, 000 for our largest problem.
3.1 Including Weakened Models
Now that we have defined our bilingual model, we
could train it to agree with the output of the mono-
lingual model (Collins and Singer, 1999; Ganchev
et al, 2008). As we will see in Section 4, however,
the feature functions ?(y1, a, y2) make no refer-
ence to the input sentences x, other than through a
fixed word alignment. With such limited monolin-
gual information, it is impossible for the bilingual
model to adequately capture all of the information
necessary for NER or parsing. As a simple ex-
ample, a bilingual NER model will be perfectly
happy to label two aligned person names as ORG
instead of PER: both labelings agree equally well.
We briefly illustrate how poorly such a basic bilin-
gual model performs in Section 10.
One way to solve this problem is to include the
output of the full monolingual models as features
in the bilingual view. However, we are training the
bilingual view to match the output of these same
models, which can be trivially achieved by putting
weight on only the monolingual model scores and
never recruiting any bilingual features. There-
fore, we use an intermediate approach: we intro-
duce the output of deliberately weakened mono-
lingual models as features in the bilingual view.
A weakened model is from the same class as the
full monolingual models, but is intentionally crip-
pled in some way (by removing feature templates,
for example). Crucially, the weakened models will
make predictions that are roughly similar to the
full models, but systematically worse. Therefore,
model scores from the weakened models provide
enough power for the bilingual view to make accu-
Feat. types Examples
Algn Densty INSIDEBOTH=3 INENONLY=0
Indicators LBLMATCH=true BIAS=true
Table 1: Sample features used for named entity
recognition for the ORG entity in Figure 1.
rate predictions, but ensure that bilingual features
will be required to optimize the training objective.
Let `W1 = log p
W
1 (y1|x1), `
W
2 = log p
W
2 (y2|x2)
be the log-probability scores from the weakened
models. Our final approximation to the marginal
distribution over labels y is:
q?1,?2,?(y|x)
def
= max
a
exp
h
?1`
W
1 + ?2`
W
2 +
?>?(y1, a, y2)? A?(?1, ?2,?;x)
i
.
(1)
Where
A?(?1, ?2,?;x) =
log
X
y
max
a
exp
h
?1`
W
1 + ?2`
W
2 + ?
>?(y1, a, y2)
i
is the updated approximate log partition function.
4 NER and Parsing Examples
Before formally describing our algorithm for find-
ing the parameters [?1, ?2,?], we first give exam-
ples of our problems of named entity recognition
and syntactic parsing, together with node align-
ments and features for each. Figure 1 depicts a
correctly-labeled sentence fragment in both En-
glish and German. In English, the capitalization of
the phrase European Court of Auditors helps iden-
tify the span as a named entity. However, in Ger-
man, all nouns are capitalized, and capitalization
is therefore a less useful cue. While a monolin-
gual German tagger is likely to miss the entity in
the German text, by exploiting the parallel English
text and word alignment information, we can hope
to improve the German performance, and correctly
tag Europa?ischen Rechnungshofes.
The monolingual features are standard features
for discriminative, state-of-the-art entity recogniz-
ers, and we can produce weakened monolingual
models by simply limiting the feature set. The
bilingual features, ?(y1, a, y2), are over pairs of
aligned nodes, where nodes of the labels y1 and
y2 are simply the individual named entities. We
use a small bilingual feature set consisting of two
types of features. First, we use the word alignment
density features from Burkett and Klein (2008),
which measure how well the aligned entity pair
matches up with alignments from an independent
48
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
unannotated bilingual data: U
Output: bilingual parameters: ??, ??1, ??2
1. Label U with full monolingual models:
?x ? U, y?M = argmaxy p1(y1|x1)p2(y2|x2).
2. Return argmax?1,?2,?
Q
x?U q?,?1,?2 (y?M |x),
where q?,?1,?2 has the form in Equation 1.
Figure 3: Bilingual training with multiple views.
word aligner. We also include two indicator fea-
tures: a bias feature that allows the model to learn
a general preference for matched entities, and a
feature that is active whenever the pair of nodes
has the same label. Figure 1 contains sample val-
ues for each of these features.
Another natural setting where bilingual con-
straints can be exploited is syntactic parsing. Fig-
ure 2 shows an example English prepositional
phrase attachment ambiguity that can be resolved
bilingually by exploiting Chinese. The English
monolingual parse mistakenly attaches to to the
verb increased. In Chinese, however, this ambi-
guity does not exist. Instead, the word ?, which
aligns to to, has strong selectional preference for
attaching to a noun on the left.
In our parsing experiments, we use the Berke-
ley parser (Petrov et al, 2006; Petrov and Klein,
2007), a split-merge latent variable parser, for our
monolingual models. Our full model is the re-
sult of training the parser with five split-merge
phases. Our weakened model uses only two. For
the bilingual model, we use the same bilingual fea-
ture set as Burkett and Klein (2008). Table 2 gives
some examples, but does not exhaustively enumer-
ate those features.
5 Training Bilingual Models
Previous work in multiview learning has focused
on the case of agreement regularization (Collins
and Singer, 1999; Ganchev et al, 2008). If we had
bilingual labeled data, together with our unlabeled
data and monolingual labeled data, we could ex-
ploit these techniques. Because we do not possess
bilingual labeled data, we must train the bilingual
model in another way. Here we advocate train-
ing the bilingual model (consisting of the bilin-
gual features and weakened monolingual models)
to imitate the full monolingual models. In terms
of agreement regularization, our procedure may be
thought of as ?regularizing? the bilingual model to
be similar to the full monolingual models.
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
bilingual parameters: ??, ??1, ??2
bilingual input: x = (x1, x2)
Output: bilingual label: y?
Bilingual w/ Weak Bilingual w/ Full
1a. l1 = log
`
pw1 (y1|x1)
?
1b. l1 = log
`
p1(y1|x1)
?
2a. l2 = log
`
pw2 (y2|x2)
?
2b. l2 = log
`
p2(y2|x2)
?
3. Return argmaxy maxa ??1l1 + ??2l2+??
>
?(y1, a, y2)
Figure 4: Prediction by combining monolingual
and bilingual models.
Our training algorithm is summarized in Fig-
ure 3. For each unlabeled point x = (x1, x2), let
y?M be the joint label which has the highest score
from the independent monolingual models (line
1). We then find bilingual parameters ??, ??1, ??2
that maximize q??,??1,??2(y?x|x) (line 2). This max-
likelihood optimization can be solved by an EM-
like procedure (Burkett and Klein, 2008). This
procedure iteratively updates the parameter esti-
mates by (a) finding the optimum alignments for
each candidate label pair under the current pa-
rameters and then (b) updating the parameters to
maximize a modified version of Equation 1, re-
stricted to the optimal alignments. Because we re-
strict alignments to the set of at most one-to-one
matchings, the (a) step is tractable using the Hun-
garian algorithm. With the alignments fixed, the
(b) step just involves maximizing likelihood under
a log-linear model with no latent variables ? this
problem is convex and can be solved efficiently
using gradient-based methods. The procedure has
no guarantees, but is observed in practice to con-
verge to a local optimum.
6 Predicting with Monolingual and
Bilingual Models
Once we have learned the parameters of the bilin-
gual model, the standard method of bilingual pre-
diction would be to just choose the y that is most
likely under q??,??1,??2 :
y? = argmax
y
q??,??1,??2(y|x) . (2)
We refer to prediction under this model as ?Bilin-
gual w/ Weak,? to evoke the fact that the model is
making use of weakened monolingual models in
its feature set.
Given that we have two views of the data,
though, we should be able to leverage additional
information in order to make better predictions. In
49
VB 
NP1 
NP 
VP 
S 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
(a) 
NP PP PP 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
VB 
NP 
NP 
VP1 
S 
NP PP PP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
(b) 
Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese. In (a) the
correct parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.
Feature Types Feature Templates
Examples
Correct Incorrect
Alignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1
Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4
Syntactic Indicators LABEL?E,C?, NUMCHILDREN?E,C? LABEL?NP,NP?=true LABEL?VP,NP?=true
Table 2: Sample bilingual features used for parsing. The examples are features that would be extracted
by aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).
particular, the monolingual view uses monolingual
models that are known to be superior to the mono-
lingual information available in the bilingual view.
Thus, we would like to find some way to incorpo-
rate the full monolingual models into our predic-
tion method. One obvious choice is to choose the
labeling that maximizes the ?agreement distribu-
tion? (Collins and Singer, 1999; Ganchev et al,
2008). In our setting, this amounts to choosing:
y? = argmax
y
pM (y|x) q??,??1??2(y|x) . (3)
This is the correct decision rule if the views are
independent and the labels y are uniformly dis-
tributed a priori,1 but we have deliberately in-
troduced between-view dependence in the form
of the weakened monolingual models. Equa-
tion 3 implicitly double-counts monolingual infor-
mation.
One way to avoid this double-counting is to
simply discard the weakened monolingual models
when making a joint prediction:
y? = argmax
y
max
a
pM (y|x)
exp
[
??
>
?(y1, a, y2)
]
.
(4)
1See, e.g. Ando & Zhang(Ando and Zhang, 2007) for a
derivation of the decision rule from Equation 3 under these
assumptions.
This decision rule uniformly combines the two
monolingual models and the bilingual model.
Note, however, that we have already learned non-
uniform weights for the weakened monolingual
models. Our final decision rule uses these weights
as weights for the full monolingual models:
y? = argmax
y
max
a
exp
[
??1 log
(
p1(y1|x1)
)
+
??2 log
(
p2(y2|x2)
)
+??
>
?(y1, a, y2)
]
. (5)
As we will show in Section 10, this rule for com-
bining the monolingual and bilingual views per-
forms significantly better than the alternatives, and
comes close to the optimal weighting for the bilin-
gual and monolingual models.
We will refer to predictions made with Equa-
tion 5 as ?Bilingual w/ Full?, to evoke the use of
the full monolingual models alongside our bilin-
gual features. Prediction using ?Bilingual w/
Weak? and ?Bilingual w/ Full? is summarized in
Figure 4.
7 Retraining Monolingual Models
Although bilingual models have many direct ap-
plications (e.g. in machine translation), we also
wish to be able to apply our models on purely
monolingual data. In this case, we can still take
50
Input: annotated monolingual data: L1, L2
unannotated bilingual data: U
monolingual models: p1(y1|x1), p2(y2|x2)
bilingual parameters: ??, ??1, ??2
Output: retrained monolingual models:
pr1(y1|x1), p
r
2(y2|x2)
?x = (x1, x2) ? U:
Self-Retrained Bilingual-Retrained
1a. y?x1 = argmaxy1 p1(y1|x1) 1b. Pick y?x, Fig. 4
y?x2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)
2. Add (x1, y?x1 ) to L1 and add (x2, y?x2 ) to L2.
3. Return full monolingual models pr1(y1|x1),
pr2(y2|x2) trained on newly enlarged L1, L2.
Figure 5: Retraining monolingual models.
advantage of parallel corpora by using our bilin-
gual models to generate new training data for the
monolingual models. This can be especially use-
ful when we wish to use our monolingual models
in a domain for which we lack annotated data, but
for which bitexts are plentiful.2
Our retraining procedure is summarized in Fig-
ure 5. Once we have trained our bilingual param-
eters and have a ?Bilingual w/ Full? predictor (us-
ing Equation 5), we can use that predictor to an-
notate a large corpus of parallel data (line 1b). We
then retrain the full monolingual models on a con-
catenation of their original training data and the
newly annotated data (line 3). We refer to the new
monolingual models retrained on the output of the
bilingual models as ?Bilingual-Retrained,? and we
tested such models for both NER and parsing. For
comparison, we also retrained monolingual mod-
els directly on the output of the original full mono-
lingual models, using the same unannotated bilin-
gual corpora for self-training (line 1a). We refer to
these models as ?Self-Retrained?.
We evaluated our retrained monolingual mod-
els on the same test sets as our bilingual mod-
els, but using only monolingual data at test time.
The texts used for retraining overlapped with the
bitexts used for training the bilingual model, but
both sets were disjoint from the test sets.
8 NER Experiments
We demonstrate the utility of multiview learn-
ing for named entity recognition (NER) on En-
glish/German sentence pairs. We built both our
full and weakened monolingual English and Ger-
man models from the CoNLL 2003 shared task
2Of course, unannotated monolingual data is even more
plentiful, but as we will show, with the same amount of data,
our method is more effective than simple monolingual self-
training.
training data. The bilingual model parameters
were trained on 5,000 parallel sentences extracted
from the Europarl corpus. For the retraining
experiments, we added an additional 5,000 sen-
tences, for 10,000 in all. For testing, we used
the Europarl 2006 development set and the 2007
newswire test set. Neither of these data sets were
annotated with named entities, so we manually an-
notated 200 sentences from each of them.
We used the Stanford NER tagger (Finkel et
al., 2005) with its default configuration as our full
monolingual model for each language. We weak-
ened both the English and German models by re-
moving several non-lexical and word-shape fea-
tures. We made one more crucial change to our
monolingual German model. The German entity
recognizer has extremely low recall (44 %) when
out of domain, so we chose y?x from Figure 3 to
be the label in the top five which had the largest
number of named entities.
Table 3 gives results for named entity recogni-
tion. The first two rows are the full and weak-
ened monolingual models alone. The second two
are the multiview trained bilingual models. We
first note that for English, using the full bilin-
gual model yields only slight improvements over
the baseline full monolingual model, and in prac-
tice the predictions were almost identical. For this
problem, the monolingual German model is much
worse than the monolingual English model, and so
the bilingual model doesn?t offer significant im-
provements in English. The bilingual model does
show significant German improvements, however,
including a 16.1% absolute gain in F1 over the
baseline for parliamentary proceedings.
The last two rows of Table 3 give results for
monolingual models which are trained on data that
was automatically labeled using the our models.
English results were again mixed, due to the rel-
atively weak English performance of the bilin-
gual model. For German, though, the ?Bilingual-
Retrained? model improves 14.4% F1 over the
?Self-Retrained? baseline.
9 Parsing Experiments
Our next set of experiments are on syntactic pars-
ing of English and Chinese. We trained both our
full and weakened monolingual English models
on the Penn Wall Street Journal corpus (Marcus
et al, 1993), as described in Section 4. Our full
and weakened Chinese models were trained on
51
Eng Parliament Eng Newswire Ger Parliament Ger Newswire
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Monolingual Models (Baseline)
Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7
Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7
Multiview Trained Bilingual Models
Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3
Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9
Retrained Monolingual Models
Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6
Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4
Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their
group and beat the strongest monolingual baseline.
Chinese English
Monolingual Models (Baseline)
Weak Monolingual 78.3 67.6
Full Monolingual 84.2 75.4
Multiview Trained Bilingual Models
Bilingual w/ Weak 80.4 70.8
Bilingual w/ Full 85.9 77.5
Supervised Trained Bilingual Models
Burkett and Klein (2008) 86.1 78.2
Retrained Monolingual Models
Self-Retrained 83.6 76.7
Bilingual-Retrained 83.9 77.4
Table 4: Parsing results. Rows are grouped by data
condition. We bold entries that are best in their
group and beat the the Full Monolingual baseline.
the Penn Chinese treebank (Xue et al, 2002) (ar-
ticles 400-1151), excluding the bilingual portion.
The bilingual data consists of the parallel part of
the Chinese treebank (articles 1-270), which also
includes manually parsed English translations of
each Chinese sentence (Bies et al, 2007). Only
the Chinese sentences and their English transla-
tions were used to train the bilingual models ? the
gold trees were ignored. For retraining, we used
the same data, but weighted it to match the sizes
of the original monolingual treebanks. We tested
on the standard Chinese treebank development set,
which also includes English translations.
Table 4 gives results for syntactic parsing. For
comparison, we also show results for the super-
vised bilingual model of Burkett and Klein (2008).
This model uses the same features at prediction
time as the multiview trained ?Bilingual w/ Full?
model, but it is trained on hand-annotated parses.
We first examine the first four rows of Table 4. The
?Bilingual w/ Full? model significantly improves
performance in both English and Chinese relative
to the monolingual baseline. Indeed, it performs
Phrase-Based System
Moses (No Parser) 18.8
Syntactic Systems
Monolingual Parser 18.7
Supervised Bilingual (Treebank Bi-trees) 21.1
Multiview Bilingual (Treebank Bitext) 20.9
Multiview Bilingual (Domain Bitext) 21.2
Table 5: Machine translation results.
only slightly worse than the supervised model.
The last two rows of Table 4 are the results of
monolingual parsers trained on automatically la-
beled data. In general, gains in English, which
is out of domain relative to the Penn Treebank,
are larger than those in Chinese, which is in do-
main. We also emphasize that, unlike our NER
data, this bitext was fairly small relative to the an-
notated monolingual data. Therefore, while we
still learn good bilingual model parameters which
give a sizable agreement-based boost when doing
bilingual prediction, we don?t expect retraining to
result in a coverage-based boost in monolingual
performance.
9.1 Machine Translation Experiments
Although we don?t have hand-labeled data for our
largest Chinese-English parallel corpora, we can
still evaluate our parsing results via our perfor-
mance on a downstream machine translation (MT)
task. Our experimental setup is as follows: first,
we used the first 100,000 sentences of the English-
Chinese bitext from Wang et al (2007) to train
Moses (Koehn et al, 2007), a phrase-based MT
system that we use as a baseline. We then used the
same sentences to extract tree-to-string transducer
rules from target-side (English) trees (Galley et al,
2004). We compare the single-reference BLEU
scores of syntactic MT systems that result from
using different parsers to generate these trees.
52
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 
68-71 65-68 62-65 59-62 56-59 
English Weight 
German
 Weigh
t 
German F1 
70.3 70.1 59.1 
* + * + 
(a) 
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 
81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 
English Weight 
Chines
e Weig
ht 
Combined F1 
82.1 82.0 81.4 
* + ? 
* + 
? 
(b) 
Figure 6: (a) NER and (b) parsing results for different values of ?1 and ?2 (see Equation 6). ?*? shows
optimal weights, ?+? shows our learned weights, and ?-? shows uniform combination weights.
For our syntactic baseline, we used the mono-
lingual English parser. For our remaining experi-
ments, we parsed both English and Chinese simul-
taneously. The supervised model and the first mul-
tiview trained model are the same Chinese tree-
bank trained models for which we reported pars-
ing results. We also used our multiview method to
train an additional bilingual model on part of the
bitext we used to extract translation rules.
The results are shown in Table 5. Once again,
our multiview trained model yields comparable re-
sults to the supervised model. Furthermore, while
the differences are small, our best performance
comes from the model trained on in-domain data,
for which no gold trees exist.
10 Analyzing Combined Prediction
In this section, we explore combinations of the full
monolingual models, p1(y1|x1) and p2(y2|x2),
and the bilingual model, max
a
??
>
?(y1, a, y2). For
parsing, the results in this section are for combined
F1. This simply computes F1 over all of the sen-
tences in both the English and Chinese test sets.
For NER, we just use German F1, since English is
relatively constant across runs.
We begin by examining how poorly our model
performs if we do not consider monolingual in-
formation in the bilingual view. For parsing, the
combined Chinese and English F1 for this model
is 78.7%. When we combine this model uniformly
with the full monolingual model, as in Equation 4,
combined F1 improves to 81.2%, but is still well
below our best combined score of 82.1%. NER
results for a model trained without monolingual
information show an even larger decline.
Now let us consider decision rules of the form:
y? = argmax
y
max
a
exp[?1 log
`
p1(y1|x1)
?
+
?2 log
`
p2(y2|x2)
?
+??
>
?(y1, a, y2)] .
Note that when ?1 = ?2 = 1, this is exactly
the uniform decision rule (Equation 4). When
?1 = ??1 and ?2 = ??2, this is the ?Bilingual w/
Full? decision rule (Equation 5). Figure 6 is a
contour plot of F1 with respect to the parameters
?1 and ?2. Our decision rule ?Bilingual w/ Full?
(Equation 5, marked with a ?+?) is near the opti-
mum (?*?), while the uniform decision rule (?-?)
performs quite poorly. This is true for both NER
(Figure 6a) and parsing (Figure 6b).
There is one more decision rule which we have
yet to consider: the ?conditional independence?
decision rule from Equation 3. While this rule can-
not be shown on the plots in Figure 6 (because
it uses both the full and weakened monolingual
models), we note that it also performs poorly in
both cases (80.7% F1 for parsing, for example).
11 Conclusions
We show for the first time that state-of-the-art,
discriminative monolingual models can be signifi-
cantly improved using unannotated bilingual text.
We do this by first building bilingual models that
are trained to agree with pairs of independently-
trained monolingual models. Then we combine
the bilingual and monolingual models to account
for dependence across views. By automatically
annotating unlabeled bitexts with these bilingual
models, we can train new monolingual models that
do not rely on bilingual data at test time, but still
perform substantially better than models trained
using only monolingual resources.
Acknowledgements
This project is funded in part by NSF grants
0915265 and 0643742, an NSF graduate research
fellowship, the DNI under grant HM1582-09-1-
0021, and BBN under DARPA contract HR0011-
06-C-0022.
53
References
Rie Kubota Ando and Tong Zhang. 2007. Two-view
feature generation model for semi-supervised learn-
ing. In ICML.
Maria-Florina Balcan and Avrim Blum. 2005. A pac-
style model for learning from labeled and unlabeled
data. In COLT.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v 1.0. Web download. LDC2007T02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In NIPS.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured
and non-identical outputs. In UAI.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3):311?325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Robert Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In COLING-ACL.
Luo Si and Jamie Callan. 2005. Clef 2005: Multi-
lingual retrieval by combining multiple multilingual
ranked lists. In CLEF.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using english to
parse korean. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
AAAI.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In IEEE
ASRU Workshop.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Human Language Technologies.
54
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 102?106,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Mention Detection: Heuristics for the OntoNotes annotations
Jonathan K. Kummerfeld, Mohit Bansal, David Burkett and Dan Klein
Computer Science Division
University of California at Berkeley
{jkk,mbansal,dburkett,klein}@cs.berkeley.edu
Abstract
Our submission was a reduced version of
the system described in Haghighi and Klein
(2010), with extensions to improve mention
detection to suit the OntoNotes annotation
scheme. Including exact matching mention
detection in this shared task added a new and
challenging dimension to the problem, partic-
ularly for our system, which previously used
a very permissive detection method. We im-
proved this aspect of the system by adding
filters based on the annotation scheme for
OntoNotes and analysis of system behavior on
the development set. These changes led to im-
provements in coreference F-score of 10.06,
5.71, 6.78, 6.63 and 3.09 on the MUC, B3,
Ceaf-e, Ceaf-m and Blanc, metrics, respec-
tively, and a final task score of 47.10.
1 Introduction
Coreference resolution is concerned with identifying
mentions of entities in text and determining which
mentions are referring to the same entity. Previously
the focus in the field has been on the latter task.
Typically, mentions were considered correct if their
span was within the true span of a gold mention, and
contained the head word. This task (Pradhan et al,
2011) has set a harder challenge by only considering
exact matches to be correct.
Our system uses an unsupervised approach based
on a generative model. Unlike previous work, we
did not use the Bllip or Wikipedia data described in
Haghighi and Klein (2010). This was necessary for
the system to be eligible for the closed task.
The system detects mentions by finding the max-
imal projection of every noun and pronoun. For the
OntoNotes corpus this approach posed several prob-
lems. First, the annotation scheme explicitly rejects
noun phrases in certain constructions. And second,
it includes coreference for events as well as things.
In preliminary experiments on the development set,
we found that spurious mentions were our primary
source of error. Using an oracle to exclude all spu-
rious mentions at evaluation time yielded improve-
ments ranging from five to thirty percent across the
various metrics used in this task. Thus, we decided
to focus our efforts on methods for detecting and fil-
tering spurious mentions.
To improve mention detection, we filtered men-
tions both before and after coreference resolution.
Filters prior to coreference resolution were con-
structed based on the annotation scheme and partic-
ular cases that should never be mentions (e.g. single
word spans with the EX tag). Filters after corefer-
ence resolution were constructed based on analysis
of common errors on the development set.
These changes led to considerable improvement
in mention detection precision. The heuristics used
in post-resolution filtering had a significant negative
impact on recall, but this cost was out-weighed by
the improvements in precision. Overall, the use of
these filters led to a significant improvement in F1
across all the coreference resolution evaluation met-
rics considered in the task.
2 Core System
We use a generative approach that is mainly un-
supervised, as described in detail in Haghighi and
102
Klein (2010), and briefly below.
2.1 Model
The system uses all three of the standard abstrac-
tions in coreference resolution; mentions, entities
and types. A mention is a span in the text, the en-
tity is the actual object or event the mention refers
to, and each type is a group of entities. For example,
?the Mountain View based search giant? is a men-
tion that refers to the entity Google, which is of type
organization.
At each level we define a set of properties (e.g.
proper-head). For mentions, these properties are
linked directly to words from the span. For enti-
ties, each property corresponds to a list of words,
instances of which are seen in specific mentions of
that entity. At the type level, we assign a pair of
multinomials to each property. The first of these
multinomials is a distribution over words, reflecting
their occurrence for this property for entities of this
type. The second is a distribution over non-negative
integers, representing the length of word lists for this
property in entities of this type.
The only form of supervision used in the system
is at the type level. The set of types is defined and
lists of prototype words for each property of each
type are provided. We also include a small number
of extra types with no prototype words, for entities
that do not fit well in any of the specified types.
These abstractions are used to form a generative
model with three components; a semantic module, a
discourse module and a mention module. In addi-
tion to the properties and corresponding parameters
described above, the model is specified by a multi-
nomial prior over types (?), log-linear parameters
over discourse choices (pi), and a small number of
hyperparameters (?).
Entities are generated by the semantic module by
drawing a type t according to ?, and then using that
type?s multinomials to populate word lists for each
property.
The assignment of entities to mentions is handled
by the discourse module. Affinities between men-
tions are defined by a log-linear model with param-
eters pi for a range of standard features.
Finally, the mention module generates the ac-
tual words in the span. Words are drawn for each
property from the lists for the relevant entity, with
a hyper-parameter for interpolation between a uni-
form distribution over the words for the entity and
the underlying distribution for the type. This allows
the model to capture the fact that some properties
use words that are very specific to the entity (e.g.
proper names) while others are not at all specific
(e.g. pronouns).
2.2 Learning and Inference
The learning procedure finds parameters that are
likely under the model?s posterior distribution. This
is achieved with a variational approximation that
factors over the parameters of the model. Each set
of parameters is optimized in turn, while the rest are
held fixed. The specific update methods vary for
each set of parameters; for details see Section 4 of
Haghighi and Klein (2010).
3 Mention detection extensions
The system described in Haghighi and Klein (2010)
includes every NP span as a mention. When run on
the OntoNotes data this leads to a large number of
spurious mentions, even when ignoring singletons.
One challenge when working with the OntoNotes
data is that singleton mentions are not annotated.
This makes it difficult to untangle errors in coref-
erence resolution and errors in mention detection. A
mention produced by the system might not be in the
gold set for one of two reasons; either because it is
a spurious mention, or because it is not co-referent.
Without manually annotating the singletons in the
data, these two cases cannot be easily separated.
3.1 Baseline mention detection
The standard approach used in the system to detect
mentions is to consider each word and its maximal
projection, accepting it only if the span is an NP or
the word is a pronoun. This approach will intro-
duce spurious mentions if the parser makes a mis-
take, or if the NP is not considered a mention in the
OntoNotes corpus. In this work, we considered the
provided parses and parses produced by the Berke-
ley parser (Petrov et al, 2006) trained on the pro-
vided training data. We added a set of filters based
on the annotation scheme described by Pradhan et al
(2007). Some filters are applied before coreference
resolution and others afterward, as described below.
103
Data Set Filters P R F
Dev
None 37.59 76.93 50.50
Pre 39.49 76.83 52.17
Post 59.05 68.08 63.24
All 58.69 67.98 63.00
Test All 56.97 69.77 62.72
Table 1: Mention detection performance with various
subsets of the filters.
3.2 Before Coreference Resolution
The pre-resolution filters were based on three reli-
able features of spurious mentions:
? Appositive constructions
? Attributes signaled by copular verbs
? Single word mentions with a POS tag in the set:
EX, IN, WRB, WP
To detect appositive constructions we searched
for the following pattern:
NP
NP , NP . . .
And to detect attributes signaled by copular struc-
tures we searched for this pattern:
VP
cop verb NP
where we used the fairly conservative set of cop-
ular verbs: {is, are, was, ?m}. In both
cases, any mention whose maximal NP projection
appeared as the bold node in a subtree matching the
pattern was excluded.
In all three cases, errors from the parser (or POS
tagger) may lead to the deletion of valid mentions.
However, we found the impact of this was small and
was outweighed by the number of spurious mentions
removed.
3.3 After Coreference Resolution
To construct the post-coreference filters we analyzed
system output on the development set, and tuned
Filters MUC B3 Ceaf-e Blanc
None 25.24 45.89 50.32 59.12
Pre 27.06 47.71 50.15 60.17
Post 42.08 62.53 43.88 66.54
All 42.03 62.42 43.56 66.60
Table 2: Precision for coreference resolution on the dev
set.
Filters MUC B3 Ceaf-e Blanc
None 50.54 78.54 26.17 62.77
Pre 51.20 77.73 27.23 62.97
Post 45.93 64.72 39.84 61.20
All 46.21 64.96 39.24 61.28
Table 3: Recall for coreference resolution on the dev set.
based on MUC and B3 performance. The final set
of filters used were:
? Filter if the head word is in a gazetteer, which
we constructed based on behavior on the devel-
opment set (head words found using the Collins
(1999) rules)
? Filter if the POS tag is one of WDT, NNS, RB,
JJ, ADJP
? Filter if the mention is a specific case of you
or it that is more often generic (you know,
you can, it is)
? Filter if the mention is any cardinal other than
a year
A few other more specific filters were also in-
cluded (e.g. ?s when tagged as PRP) and one type
of exception (if all words are capitalized, the men-
tion is kept).
4 Other modifications
The parses in the OntoNotes data include the addi-
tion of structure within noun phrases. Our system
was not designed to handle the NML tag, so we
removed such nodes, reverting to the standard flat-
tened NP structures found in the Penn Treebank.
We also trained the Berkeley parser on the pro-
vided training data, and used it to label the develop-
ment and test sets.1 We found that performance was
1In a small number of cases, the Berkeley parser failed, and
we used the provided parse tree instead.
104
Filters MUC B3 Ceaf-e Ceaf-m Blanc
None 33.67 57.93 34.43 42.72 60.60
Pre 35.40 59.13 35.29 43.72 61.38
Post 43.92 63.61 41.76 49.74 63.26
All 44.02 63.66 41.29 49.46 63.34
Table 4: F1 scores for coreference resolution on the dev
set.
slightly improved by the use of these parses instead
of the provided parses.
5 Results
Since our focus when extending our system for this
task was on mention detection, we present results
with variations in the sets of mention filters used. In
particular, we have included results for our baseline
system (None), when only the filters before coref-
erence resolution are used (Pre), when only the fil-
ters after coreference resolution are used (Post), and
when all filters are used (All).
The main approach behind the pre-coreference fil-
ters was to consider the parse to catch cases that are
almost never mentions. In particular, these filters
target cases that are explicitly excluded by the an-
notation scheme. As Table 1 shows, this led to a
1.90% increase in mention detection precision and
0.13% decrease in recall, which is probably a result
of parse errors.
For the post-coreference filters, the approach was
quite different. Each filter was introduced based on
analysis of the errors in the mention sets produced
by our system on the development set. Most of the
filters constructed in this way catch some true men-
tions as well as spurious mentions, leading to signif-
icant improvements in precision at the cost of recall.
Specifically an increase of 21.46% in precision and
decrease of 8.85% in recall, but an overall increase
of 12.74% in F1-score.
As Tables 2 and 3 show, these changes in mention
detection performance generally lead to improve-
ments in precision at the expense of recall, with the
exception of Ceaf-e where the trends are reversed.
However, as shown in Table 4, there is an overall
improvement in F1 in all cases.
In general the change from only post-coreference
filters to all filters is slightly negative. The final sys-
Metric R P F1
MUC 46.39 39.56 42.70
B3 63.60 57.30 60.29
Ceaf-m 45.35 45.35 45.35
Ceaf-e 35.05 42.26 38.32
Blanc 58.74 61.58 59.91
Table 5: Complete results on the test set
tem used all of the filters because the process used to
create the post-coreference filters was more suscep-
tible to over-fitting, and the pre-coreference filters
provided such an unambiguously positive contribu-
tion to mention detection.
6 Conclusion
We modified the coreference system of Haghighi
and Klein (2010) to improve mention detection per-
formance. We focused on tuning using the MUC and
B3 metrics, but found considerable improvements
across all metrics.
One important difference between the system de-
scribed here and previous work was the data avail-
able. Unlike Haghighi and Klein (2010), no extra
data from Wikipedia or Bllip was used, a restriction
that was necessary to be eligible for the closed part
of the task.
By implementing heuristics based on the annota-
tion scheme for the OntoNotes data set and our own
analysis of system behavior on the development set
we were able to achieve the results shown in Table 5,
giving a final task score of 47.10.
7 Acknowledgments
We would like to thank the anonymous reviewers
for their helpful suggestions. This research is sup-
ported by the Office of Naval Research under MURI
Grant No. N000140911081, and a General Sir John
Monash Fellowship.
References
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
Philadelphia, PA, USA. AAI9926110.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
105
ings of NAACL, pages 385?393, Los Angeles, Califor-
nia, June. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, pages 433?440, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
ontonotes. In Proceedings of the International Confer-
ence on Semantic Computing, pages 446?453, Wash-
ington, DC, USA. IEEE Computer Society.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
106
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58?67,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Grounding Language with Points and Paths in Continuous Spaces
Jacob Andreas and Dan Klein
Computer Science Division
University of California, Berkeley
{jda,klein}@cs.berkeley.edu
Abstract
We present a model for generating path-
valued interpretations of natural language
text. Our model encodes a map from
natural language descriptions to paths,
mediated by segmentation variables which
break the language into a discrete set of
events, and alignment variables which
reorder those events. Within an event,
lexical weights capture the contribution of
each word to the aligned path segment.
We demonstrate the applicability of our
model on three diverse tasks: a new color
description task, a new financial news task
and an established direction-following
task. On all three, the model outperforms
strong baselines, and on a hard variant of
the direction-following task it achieves
results close to the state-of-the-art system
described in Vogel and Jurafsky (2010).
1 Introduction
This paper introduces a probabilistic model for
predicting grounded, real-valued trajectories from
natural language text. A long tradition of re-
search in compositional semantics has focused on
discrete representations of meaning. The origi-
nal focus of such work was on logical translation:
mapping statements of natural language to a for-
mal language like first-order logic (Zettlemoyer
and Collins, 2005) or database queries (Zelle and
Mooney, 1996). Subsequent work has integrated
this logical translation with interpretation against
a symbolic database (Liang et al., 2013).
There has been a recent increase in interest
in perceptual grounding, where lexical semantics
anchor in perceptual variables (points, distances,
etc.) derived from images or video. Bruni et al.
(2014) describe a procedure for constructing word
representations using text- and image-based dis-
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
U.S. stocks rebound after bruising two-day swoon
Figure 1: Example stock data. The chart displays
index value over a two-day period (divided by the
dotted line), while the accompanying headline de-
scribes the observed behavior.
tributional information. Yu and Siskind (2013)
describe a model for identifying scenes given de-
scriptions, and Golland et al. (2010), Kollar et al.
(2010), and Krishnamurthy and Kollar (2013) de-
scribe models for identifying individual compo-
nents of scenes described by text. These all have
the form of matching problems between text and
observed groundings?what has been missing so
far is the ability to generate grounded interpreta-
tions from scratch, given only text.
Our work continues in the tradition of this per-
ceptual grounding work, but makes two contribu-
tions. First, our approach is able to predict simple
world states (and their evolution): for a general
class of continuous domains, we produce a repre-
sentation of p(world | text) that admits easy sam-
pling and maximization. This makes it possible to
produce grounded interpretations of text without
reference to a pre-existing scene. Simultaneously,
we extend the range of temporal phenomena that
can be modeled?unlike the aforementioned spa-
tial semantics work, we consider language that de-
58
scribes time-evolving trajectories, and unlike Yu
and Siskind (2013), we allow these trajectories to
have event substructure, and model temporal or-
dering. Our class of models generalizes to a vari-
ety of different domains: a new color-picking task,
a new financial news task, and a more challenging
variant of the direction-following task established
by Vogel and Jurafsky (2010).
As an example of the kinds of phenomena we
want to model, consider Figure 1, which shows
the value of the Dow Jones Industrial Average
over June 3rd and 4th 2008, along with a finan-
cial news headline from June 4th. There are sev-
eral effects of interest here. One phenomenon we
want to capture is that the lexical semantics of in-
dividual words must be combined: swoon roughly
describes a drop while bruising indicates that the
drop was severe. We isolate this lexical combi-
nation in Section 4, where we consider a limited
model of color descriptions (Figure 2). A second
phenomenon is that the description is composed
of two separate events, a swoon and a rebound;
moreover, those events do not occur in their tex-
tual order, as revealed by after. In Section 5, we
extend the model to include segmentation and or-
dering variables and apply it to this stock data.
The situation where language describes a
path through some continuous space?literal or
metaphorical?is more general than stock head-
lines. Our claim is that a variety of problems
in language share these same characteristics. To
demonstrate generality of the model, we also ap-
ply it in Section 6 to a challenging variant of the
direction-following task described by Vogel and
Jurafsky (2010) (Figure 3), where we achieve re-
sults close to a state-of-the-art system that makes
stronger assumptions about the task.
2 Three tasks in grounded semantics
The problem of inferring a structured state repre-
sentation from sensory input is a hard one, but we
can begin to tackle grounded semantics by restrict-
ing ourselves to cases where we have sequences
of real-valued observations directly described by
text. In this paper we?ll consider the problems
of recognizing colors, describing time series, and
following navigational instructions. While these
tasks have been independently studied, we believe
that this is the first work which presents them in
a unified framework, and carries them out with a
single family of models.
.
dark pastel blue
(a) (b)
Figure 2: Example color data: (a) a named color;
(b) its coordinates in color space.
Colors Figure 2 shows a color called dark pas-
tel blue. English speakers, even if unfamiliar with
the specific color, can identify roughly what the
name signifies because of prior knowledge of the
meanings of the individual words.
Because the color domain exhibits lexical com-
positionality but not event structure, we present it
here to isolate the non-temporal compositional ef-
fects in our model. Any color visible to the human
eye can be identified with three coordinates, which
we?ll take to be hue, saturation and value (HSV).
As can be seen in Figure 2 the ?hue? axis corre-
sponds to the differentiation made by basic color
names in most languages. Other modifiers act on
the saturation and value axes: either simple ones
like dark (which decreases value), or more compli-
cated ones like pastel (which increases value and
decreases saturation). Given a set of named colors
and their HSV coordinates, a learning algorithm
should be able to identify the effects of each word
in the vocabulary and predict the appearance of
new colors with previously-unseen combinations
of modifiers.
Compositional interpretations of color have re-
ceived attention in linguistics and philosophy of
language (Kennedy andMcNally, 2010), but while
work in grounded computational semantics like
that of Krishnamurthy and Kollar (2013) has suc-
ceeded in learning simple color predicates, our
model is the first to capture the machine learning
of color in a fine-grained, compositional way.
Time series As a first step into temporal struc-
ture, we?ll consider language describing the be-
havior of stock market indices. Here, again, there
is a simple parameterization?in this case just a
single number describing the total value of the
index?but as shown by the headline example in
Figure 1, the language used to describe changes
in the stock market can be quite complex. Head-
59
right round the white water [. . . ] but stay quite close ?cause
you don?t otherwise you?re going to be in that stone creek
Figure 3: Example map data: a portion of a map,
and a single line from a dialog which describes
navigation relative to the two visible landmarks.
lines may describe multiple events, or multi-part
events like rebound or extend; stocks do not sim-
ply rise or fall, but stagger, stumble, swoon, and
so on. There are compositional effects here as
well: distinction is made between falling and
falling sharply; gradual trends are distinguished
from those which occur suddenly, at the beginning
or end of the trading day. Along with temporal
structure, the problem requires a more sophisti-
cated treatment of syntax than the colors case?
now we have to identify which subspans of the
sentence are associated with each event observed,
and determine the correspondence between sur-
face order and actual order in time.
The learning of correspondences between text
and time series has attracted more interest in nat-
ural language generation than in semantics (Yu et
al., 2007). Research on natural language process-
ing and stock data, meanwhile, has largely focused
on prediction of future events (Kogan et al., 2009).
Direction following We?ll conclude by apply-
ing our model to the well-studied problem of
following navigational directions. A variety of
reinforcement-learning approaches for following
directions on a map were previously investigated
by Vogel and Jurafsky (2010) using a corpus as-
sembled by Anderson et al. (1991). An example
portion of a path and its accompanying instruction
is shown in Figure 3. While also representable as
a set of real valued coordinates, here 2-d, this data
set looks very different?a typical example con-
sists of more than a hundred sentences of the kind
shown in Figure 3, accompanying a long path. The
language, a transcript of a spoken dialog, is also
considerably less formal than the language found
in the Wall Street Journal examples, involving dis-
fluency, redundancy and occasionally errors. Nev-
ertheless the underlying structure of this problem
and the stock problem are fundamentally similar.
In addition to Vogel and Jurafsky, Tellex et al.
(2011) give a weakly-supervised model for map-
ping single sentences to commands, and Brana-
van et al. (2009) give an alternative reinforcement-
learning approach for following long command se-
quences. An intermediate between this approach
and ours is the work of Chen and Mooney (2011)
and Artzi and Zettlemoyer (2013), which boot-
strap a semantic parser to generate logical forms
specifying the output path, rather than predicting
the path directly.
Between them, these tasks span a wide range of
linguistic phenomena relevant to grounded seman-
tics, and provide a demonstration of the useful-
ness and general applicability of our model. While
development of the perceptual groundwork neces-
sary to generalize these results to more complex
state spaces remains a major problem, our three
examples provide a starting point for studying the
relationship between perception, time and the se-
mantics of natural language.
3 Preliminaries
In the experiments that follow, each training ex-
ample will consist of:
? Natural language text, consisting of a con-
stituency parse tree or trees. For a given ex-
ample, we will denote the associated trees
(T
1
, T
2
, . . .). These are also observed at test
time, and used to predict new groundings.
? A vector-valued, grounded observation, or
a sequence of observations (a path), which
we will denote V for a given example. We
will further assume that each of these paths
has been pre-segmented (discussed in detail
in Section 5) into a sequence (V
1
,V
2
, . . .).
These are only observed during training.
The probabilistic backbone of our model is a
collection of linear and log-linear predictors. Thus
it will be useful to work with vector-valued rep-
resentations of both the language and the path,
which we accomplish with a pair of feature func-
tions ?
t
and ?
v
. As the model is defined only
in terms of these linear representations, we can
60
?t
(T )
?
Label at root of T
?
Lemmatized leaves of T
?
v
(V )
?
Last element of V
?
Curvature of quadratic
approx. to V (stocks only)
?
a
(T,A
i
, A
i?1
)
Cartesian prod. of ?
t
(T ) with:
?
I[A
i
is aligned]
?
I[A
i?1
is aligned]
?
A
1
?A
i?1
(if both aligned)
Table 1: Features used for linear parameterization
of the grounding model.
simplify notation by writing T
i
= ?
t
(T
i
) and
V
i
= ?
v
(V
i
). As the ultimate prediction task is to
produce paths, and not their featurized representa-
tions, we will assume that it is also straightforward
to compute ?
?1
v
, which projects path features back
into the original grounding domain.
All parse trees are predicted from input text us-
ing the Berkeley Parser (Petrov and Klein, 2007).
Feature representations for both trees and paths are
simple and largely domain-independent; they are
explicitly enumerated in Table 1.
The general framework presented here leaves
one significant problem unaddressed: given a large
state vector encoding properties of multiple ob-
jects, how do we resolve an utterance about a sin-
gle object to the correct subset of indices in the
vector? While none of the tasks considered in this
paper require an argument resolution step of this
kind, interpretation of noun phrases is one of the
better-studied problems in compositional seman-
tics (Zelle and Mooney (1996), inter alia), and
we expect generalization of this approach to be
straightforward using these tools.
We will consider the color, stock, and naviga-
tion tasks in turn. It is possible to view the models
we give for all three as instantiations of the same
graphical model, but for ease of presentation we
will introduce this model incrementally.
4 Predicting vectors
Prediction of a color variable from text has the
form of a regression problem: given a vector of
lexical features extracted from the name, we wish
to predict the entries of a vector in color space. It
seems linguistically plausible that this regression
is sparse and linear: that most words, if they pro-
vide any constraints at all, tend to express prefer-
ences about a subset of the available dimensions;
and that composition within the domain of a sin-
gle event largely consists of words additively pre-
dicting that event?s parameters, without complex
nonlinear interactions. This is motivated by the
observation that pragmatic concerns force linguis-
tic descriptors to orient themselves along a small
set of perceptual bases: once we have words for
north and east, we tend to describe intermediates
as northeast rather than inventing an additional
word which means ?a little of both?.
As discussed above, we can represent a color as
a point in a three-dimensional HSV space. Let T
denote features on the parse tree of the color name,
and V its representation in color space (consistent
with the definition of ?
v
given in Table 1). Linear-
ity suggests the following model:
p(T, V ) ? e
?
?
?
?
t
T?V
?
2
2
(1)
The learning problem is then:
argmin
?
t
?
T,V
?
?
?
?
?
t
T ? V
?
?
?
2
2
(2)
which, with a sparse prior on ?
t
, is the proba-
bilistic formulation of Lasso regression (Tibshi-
rani, 1996), for which standard tools are available
in the optimization literature.
To predict color space values from a new (fea-
turized) name T , we output:
argmax
V
p(T, V ) = ?
?
t
T
4.1 Evaluation
We collect a set of color names and their
corresponding HSV triples from the English
Wikipedia?s List of Colors, retaining only those
color names in which every word appears at least
three times in the training corpus. This leaves a
set of 419 colors, which we randomly divide into
a 377-item training set and 42-item test set. The
model?s goal will be to learn to identify new col-
ors given only their names.
We consider two evaluations: one which mea-
sures the model?s ability to distinguish the named
color from a random alternative?analogous to the
evaluation in Yu and Siskind (2013)?and one
which measures the absolute difference between
predicted and true color values. In particular, in
the first evaluation the model is presented with
the name of a color and a pair of candidates, one
61
Method Sel. ? H ? S ? V ?
Random 0.50 0.30 0.38 0.39
Last word 0.78 0.05 0.26 0.17
Full model 0.81 0.07 0.21 0.13
Human 0.86 - - -
Table 2: Results for the color selection task.
Sel(ection accuracy) is frequency with which the
system was able to correctly identify the color de-
scribed when paired with a random alternative.
Other columns are the magnitude of the average
prediction error along the axes of the color space.
Full model selection accuracy is a statistically sig-
nificant (p < 0.05) improvement over the baseline
using a paired sign test.
the color corresponding to the name and another
drawn randomly from the test set, and report the
fraction of times the true color is assigned a higher
probability than the random alternative. In the sec-
ond, we report the absolute value of the difference
between true and predicted hue, saturation, and lu-
minosity.
We compare against two baselines: one which
looks only at the last word in the color name (al-
most always a hue category), and so captures no
compositional effects, and another which outputs
random values for all three coordinates. Results
are shown in Table 2. The model with all lexical
features outperforms both baselines on selection
and all but one absolute error metric.
4.2 Error analysis
An informal experiment in which the color selec-
tion task was repeated on one of the authors? col-
leagues (the ?Human? row in Table 2) yielded an
accuracy of 86%, only 5% better than the system.
While not intended as a rigorous upper bound on
performance, this suggests that the model capac-
ity and training data are sufficient to capture most
interesting color behavior. The errors that do oc-
cur appear to mostly be of two kinds. In one case,
a base color is seen only with a small (or related)
set of modifiers, from which the system is unable
to infer the meaning of the base color (e.g. from
Japanese indigo, lavender indigo, and electric in-
digo, the learning algorithm infers that indigo is
bright purple). In the other, no part of the color
word is seen in training, and the system outputs an
unrelated ?default? color (teal is predicted to be
bright red).
5 Predicting paths
The idea that a sentence?s meaning is fundamen-
tally described by a set of events, each associated
with a set of predicates, is well-developed in neo-
Davidsonian formal semantics (Parsons, 1990).
We adopt the skeleton of this formal approach by
tying our model to (latent) partitions of the in-
put sentence into disjoint events. Rather than at-
tempting to pass through a symbolic meaning rep-
resentation, however, this event structure will be
used to map text directly into the grounding do-
main. We assume that this domain has pre-existing
structure?in particular, that in our input paths V ,
the boundaries of events have already been iden-
tified, and that the problem of aligning text to
portions of the segment only requires aligning to
segment indices rather than fine-grained time in-
dices. This is a strong assumption, and one that
future work may wish to revisit, but there exist
both computational tools from the changepoint de-
tection literature (Basseville and Nikiforov, 1995)
and pieces of evidence from cognitive science (Za-
cks and Swallow, 2007) which suggest that assum-
ing a pre-linguistic structuring of events is a rea-
sonable starting point.
In the text domain, we make the corresponding
assumption that each of these events is syntacti-
cally local?that a given span of the input sentence
provides information about at most one of these
segmented events.
The main structural difference between the
color example in Figure 2 and the stock market ex-
ample in Figure 1 is the introduction of a time di-
mension orthogonal to the dimensions of the state
space. To accommodate this change, we extend
the model described in the previous subsection in
the following way: Instead of a single vector, each
tree representation T is paired with a sequence of
path featuresV = (V
1
, V
2
, . . . , V
M
). For the time
being we continue to assume that there is only
one input tree per training example. As before,
we wish to model the probability p(T,V), but the
problem becomes harder: a single sentence might
describe multiple events, but we don?t know what
the correspondence is between regions of the sen-
tence and segmentsV.
Though the ultimate goal is still prediction of V
vectors from novel T instances, we cannot do this
without also inferring a set of latent alignments be-
tween portions of the path and input sentence dur-
ing training. To allow a sentence to explain mul-
62
? ? ?
A1 A2C1 C2
V1 V2
T
1
T
2
[Stocks rose] [Stocks rose, then fell]
 acv
 a?
 ta  tc
Figure 4: Factor graph for stocks grounding
model. Only a subset of the alignment candidates
are shown. ?
tc
maps text to constraints, ?
acv
maps
constraints to grounded segments, and ?
ta
deter-
mines which constraints act on which segments.
tiple events, we?ll break each T apart into a set of
alignment candidates T
i
. We?ll allow as an align-
ment candidate any subtree of T , and additionally
any subtree from which a single constituent has
been deleted.
We then introduce two groups of latent vari-
ables: alignment variables A = (A
1
, A
2
, . . .),
which together describe a mapping from pieces
of the input sentence to segments of the ob-
served path, and what we?ll call ?constraint? vari-
ables C = (C
1
, C
2
, . . .), which express each
aligned tree segment?s prediction about what its
corresponding path should look like (so that the
possibly-numerous parts of the tree aligned to a
single segment can independently express prefer-
ences about the segment?s path features).
In addition to ensuring that the alignment is
consistent with the bracketing of the tree, it might
be desirable to impose additional global con-
straints on the alignment. There are various ways
to do this in a graphical modeling framework; the
most straightforward is to add a combinatorial fac-
tor touching all alignment variables which checks
for satisfaction of the global constraint. In gen-
eral this makes alignment intractable. If the total
number of alignments licensed by this combina-
torial factor is small (i.e. if acceptable alignments
are sparse within the exponentially-large set of all
possible assignments to A), it is possible to di-
rectly sum them out during inference. Otherwise
approximate techniques (as discussed in the fol-
lowing section) will be necessary.
As discussed in Section 2, our financial time-
lines cover two-day periods, and it seems natural
to treat each day as a separate event. Then
the simple regression model described in the
preceding section, extended to include alignment
and constraint variables, has the form of the factor
graph shown in Figure 4. In particular, the joint
distribution p(T,V) is the product of four groups
of factors:
Alignment factors ?
ta
, which use a log-linear
model to score neighboring pairs of factors with
a feature function ?
a
:
?
ta
(T
i
, A
i
, A
i?1
) =
e
?
?
a
?
a
(T
i
,A
i
,A
i?1
)
?
A
?
i
,A
?
i?1
e
?
?
a
?
a
(T
i
,A
?
i
,A
?
i?1
)
(3)
Constraint factors ?
tc
, which map text features
onto constraint values:
?
tc
(T
i
, C
i
) = e
?||?
?
t
T
i
?C
i
||
2
2
(4)
Prediction factors ?
acv
which encourage pre-
dicted constraints and path features to agree:
?
acv
(A
i
, C
i
, V
j
) =
{
1 if A
i
?= j
e
?||C
i
?V
j
||
2
2
o.w.
(5)
A global factor ?
a
?
(A
1
, A
2
, ? ? ? ) which places
an arbitrary combinatorial constraint on the
alignment.
Note the essential similarity between Equations 1
and 4?in general, it can be shown that this factor
model reduces to the regression model we gave for
colors when there is only one of each T
i
and V
j
.
5.1 Learning
In order to make learning in the stocks domain
tractable, we introduce the following global
constraints on alignment: every terminal must be
aligned, and two constituents cannot be aligned
to the same segment. Together, these simplify
learning by ensuring that the number of terms
in the sum over A and C is polynomial (in fact
O(n
2
)) in the length of the input sentence. We
wish to find the maximum a posteriori estimate
p(?
t
, ?
a
|T,V) for ?
t
and ?
a
, which we can do
63
using the Expectation?Maximization algorithm.
To find regression scoring weights ?
t
, we have:
E step:
M = E
[
?
i
T
i
(T
i
)
?
]
; N = E
[
?
i
T
i
V
?
A
i
]
(6)
M step:
?
t
= M
?1
N (7)
To find alignment scoring weights ?
a
, we must
maximize:
?
i
E
?
?
log
?
?
e
?
?
a
?
a
(A
i
,A
i?1
,T
i
)
?
A
?
i
,A
?
i?1
e
?
?
a
?
a
(A
?
i
,A
?
i?1
,T
i
)
?
?
?
?
(8)
which can be done using a variety of convex op-
timization tools; we used L-BFGS (Liu and No-
cedal, 1989).
The predictive distribution p(V|T ) can also be
straightforwardly computed using the standard in-
ference procedures for graphical models.
5.2 Evaluation
Our stocks dataset consists of a set of headlines
from the ?Market Snapshot? column of the Wall
Street Journal?s MarketWatch website,
1
paired
with hourly stock charts for each day described
in a headline. Data is collected over a roughly
decade-long period between 2001 and 2012; af-
ter removing weekends and days with incomplete
stock data, we have a total of 2218 headline/time
series pairs. As headlines most often discuss a
single day or a short multi-day period, each train-
ing example consists of two days? worth of stock
data concatenated together. We use a 90%/10%
train/test split, with all test examples following all
training examples chronologically.
We compare against two baselines: one which
uses no text (and so learns only the overall mar-
ket trend during the training period), and another
which uses a fixed alignment instead of summing,
aligning the entire tree to the second day?s time se-
ries. Prediction error is the sum of squared errors
between the predicted and gold time series.
We report both the magnitude of the prediction
error, and the model?s ability to distinguish be-
tween the described path and a randomly-selected
alternative. The system scores poorly on squared
1
http://www.marketwatch.com/Search?m=
Column&mp=Market%20Snapshot
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
[U.S. stocks end lower]
2
[as economic worries persist]
1
Figure 5: Example output from the stocks task.
The model prediction is given in blue (solid), and
the reference time series in green (dashed). Brack-
ets indicate the predicted boundaries of event-
introducing spans, and subscripts their order in the
sentence. The model correctly identifies that end
lower refers to the current day, and persist pro-
vides information about the previous day.
Method Sel. acc. ? Pred. err. ?
No text 0.51 0.0012
Fixed alignment 0.59 0.0011
Full model 0.61 0.0018
Human 0.72 ?
Table 3: Results for the stocks task. Sel(ection
accuracy) measures the frequency with which the
system correctly identifies the stock described in
the headline when paired with a random alterna-
tive. Pred(iction error) is the mean sum of squared
errors between the real and predicted paths. Full
model selection accuracy is a statistically signif-
icant improvement (p < 0.05) over the baseline
using a paired sign test.
error (which disproportionately penalizes large de-
viations from the correct answer, preferring con-
servative models), but outperforms both base-
lines on the task of choosing the described stock
history?when it is wrong, its errors are often
large in magnitude, but its predictions more fre-
quently resemble the correct time series than the
other systems.
Figure 5 shows example system output for an
example sentence. The model correctly identifies
the two events, orders them in time and gets their
approximate trend correct. Table 4 shows some
64
% Chan
ge
0.98
0.99
1.00
1.01
1.02
Hour of day10 12 14 10 12 14
[U.S. stocks extend losing stretch]
1
Figure 6: Example error from the stocks task. The
system?s prediction, in blue (solid), fails to seg-
ment the input into two events, and thus incor-
rectly extends the losing trend to the entire output
time span.
features learned by the model?as desired, it cor-
rectly interprets a variety of different expressions
used to describe stock behavior.
5.3 Error analysis
As suggested by Table 4, learned weights for the
trajectory-grounded features ?
t
are largely correct.
Thus, most incorrect outputs from the system in-
volve alignment to time. Many multipart events
(like rebound) can be reasonably explained using
the curvature feature without splitting the text into
two segments; as a result, the system tends to be
fairly conservative about segmentation and often
under-segments. This results in examples like Fig-
ure 6, in which the downward trend suggested by
losing is incorrectly extended to the entire out-
put curve. Here, another informal experiment us-
ing humans as the predictors indicates that pre-
dictions are farther from human-level performance
Word Sign Magnitude ?10
3
rise 0.27 ?0.78
swoon ?0.57 0
sharply ?0.22 0.28
slammed ?0.36 0
lifted 0.66 0
Table 4: Learned parameter settings for overall
daily change, which the path featurization decom-
poses into a sign and a magnitude.
than they are on the colors task.
6 Generalizing the model
Last we consider the problem of following navi-
gational directions. The difference between this
and the previous task is largely one of scale: rather
than attempting to predict the values of only two
segments, we have a long string of them. The text,
rather than a single tree, consists of a sequence of
tens or hundreds of pre-segmented utterances.
There is one additional complication?rather
than being defined in an absolute space, as they are
in the case of stocks, constraints in the maps do-
main are provided relative to a set of known land-
marks (like the white water and stone creek in Fig-
ure 3). We resolve landmarks automatically based
on string matching, in a manner similar to Vogel
and Jurafsky (2010), and assign each sentence in
the discourse with a single referred-to landmark l
i
.
If no landmark is explicitly named, it inherits from
the previous utterance. We continue to score con-
straints as before, but update the prediction factor:
?
acv
(A
i
, C
i
, V
j
) =
{
1 if A
i
?= j
e
?||l
i
+C
i
?V
j
||
2
2
o.w.
(9)
The factor graph is shown in Figure 7; ob-
serve that this is simply an unrolled version of
Figure 4?the basic structure of the model is un-
changed. While pre-segmentation of the discourse
means we can avoid aligning internal constituents
of trees, we still need to treat every utterance as an
alignment candidate, without a sparse combinato-
rial constraint. As a result, the sum over A and
C is no longer tractable to compute explicitly, and
approximate inference will be necessary.
For the experiments described in this paper, we
do this with a sequence of Monte Carlo approxi-
mations. We run a Gibbs sampler, iteratively re-
sampling each A
i
and C
i
as well as the parameter
vectors ?
t
and ?
a
to obtain estimates of E?
t
and
E?
a
. The resampling steps for ?
t
and ?
a
are them-
selves difficult to perform exactly, so we perform
an internal Metropolis-Hastings run (with a Gaus-
sian proposal distribution) to obtain samples from
the marginal distributions over ?
t
and ?
a
.
We approximate the mode of the posterior dis-
tribution by its mean. To follow a new set of direc-
tions in the prediction phase, we fix the parameter
vectors and instead sample overA, C andV, and
output EV. To complete the prediction process
65
? ? ?
? ? ?
? ? ?
CNANA1 A2 A3C1 C2 C3
V1 V2 VM
T 1 T
2 T 3 T
N
 ta  tc
 a?
 acv
Figure 7: Factor graph for the general grounding model. Note that Figure 4 is a subgraph.
we must invert ?
v
, which we do by producing the
shortest path licensed by the features.
6.1 Evaluation
The Map Task Corpus consists of 128 dia-
logues describing paths on 16 maps, accompa-
nied by transcriptions of spoken instructions, pre-
segmented using prosodic cues. See Vogel and Ju-
rafsky (2010) for a more detailed description of the
corpus in a language learning setting. For com-
parability, we?ll use the same evaluation as Vogel
and Jurafsky, which rewards the system for mov-
ing between pairs of landmarks that also appear in
the reference path, and penalizes it for additional
superfluous movement. Note that we are solv-
ing a significantly harder problem: the version ad-
dressed by Vogel and Jurafsky is a discrete search
problem, and the system has hard-coded knowl-
edge that all paths pass along one of the four sides
of each landmark. Our system, by contrast, can
navigate to any point in R
2
, and must learn that
most paths stay close to a named landmark.
At test time, the system is given a new sequence
of text instructions, and must output the corre-
sponding path. It is scored on the fraction of
correct transitions in its output path (precision),
and the fraction of transitions in the gold path
recovered (recall). Vogel and Jurafsky compare
their system to a policy-gradient algorithm for us-
ing language to follow natural language instruc-
tions described by Branavan et al. (2009), and we
present both systems for comparison.
Results are shown in Table 5. Our system sub-
stantially outperforms the policy gradient baseline
of Branavan et al., and performs close (particularly
with respect to transition recall) to the system of
Vogel and Jurafsky, with fewer assumptions.
System Prec. Recall F
1
Branavan et al. (09) 0.31 0.44 0.36
Vogel & Jurafsky (10) 0.46 0.51 0.48
This work 0.43 0.51 0.45
Table 5: Results for the navigation task. Higher is
better for all of precision, recall and F
1
.
6.2 Error analysis
As in the case of stocks, most of the prediction
errors on this task are a result of misalignment.
In particular, many of the dialogues make passing
reference to already-visited landmarks, or define
destinations in empty regions of the map in terms
of multiple landmarks simultaneously. In each of
these cases, the system is prone to directly visit-
ing the named landmark or landmarks instead of
ignoring or interpolating as necessary.
7 Conclusion
We have presented a probabilistic model for
grounding natural language text in vector-valued
state sequences. The model is capable of seg-
menting text into a series of events, ordering these
events in time, and compositionally determining
their internal structure. We have evaluated on a va-
riety of new and established applications involving
colors, time series and navigation, demonstrating
improvements over strong baselines in all cases.
Acknowledgments
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014. The first
author is supported by a National Science Foun-
dation Graduate Research Fellowship.
66
References
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
JimMiller, et al. 1991. The HCRCmap task corpus.
Language and speech, 34(4):351?366.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michele Basseville and Igor V Nikiforov. 1995. De-
tection of abrupt changes: theory and applications.
Journal of the Royal Statistical Society-Series A
Statistics in Society, 158(1):185.
SRK Branavan, Harr Chen, Luke S Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 82?90. Association for Com-
putational Linguistics.
Elia Bruni, NamKhanh Tran, andMarco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In AAAI, volume 2.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
Empirical Methods in Natural Language Process-
ing, pages 410?419. Association for Computational
Linguistics.
Christopher Kennedy and Louise McNally. 2010.
Color, context, and compositionality. Synthese,
174(1):79?98.
Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 272?280. Association for Computa-
tional Linguistics.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Grounding verbs of motion in natu-
ral language commands to robots. In International
Symposium on Experimental Robotics.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Terence Parsons. 1990. Events in the semantics of En-
glish. MIT Press.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of Human
Language Technologies: The 2007 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics. Assocation for
Computational Linguistics.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267?288.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806?814. Association for
Computational Linguistics.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from videos described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics. Association for Computational Linguistics.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries of
large time-series data sets. Natural Language Engi-
neering, 13(1):25?49.
Jeffrey M Zacks and Khena M Swallow. 2007. Event
segmentation. Current Directions in Psychological
Science, 16(2):80?84.
John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, pages 658?
666.
67
