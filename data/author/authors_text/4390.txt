Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260?1269,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Using Morphological and Syntactic Structures  
for Chinese Opinion Analysis 
 
 
Lun-Wei Ku Ting-Hao Huang Hsin-Hsi Chen 
 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{lwku,thhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 
 
   
 
 
Abstract 
This paper employs morphological struc-
tures and relations between sentence seg-
ments for opinion analysis on words and 
sentences.  Chinese words are classified 
into eight morphological types by two 
proposed classifiers, CRF classifier and 
SVM classifier.  Experiments show that 
the injection of morphological information 
improves the performance of the word po-
larity detection.  To utilize syntactic struc-
tures, we annotate structural trios to repre-
sent relations between sentence segments.  
Experiments show that considering struc-
tural trios is useful for sentence opinion 
analysis.  The best f-score achieves 0.77 
for opinion word extraction, 0.62 for opin-
ion word polarity detection, 0.80 for opin-
ion sentence extraction, and 0.54 for opin-
ion sentence polarity detection. 
1 Introduction 
Sentiment analysis has attracted much attention 
in recent years because a large scale of subjective 
information is disseminated through various plat-
forms on the web.  Sentiment information can be 
applied to a wide variety of fields, including 
product recommendation, review summarization, 
public polling, and so on. 
Opinion dictionaries are important resources 
for identifying subjective information.  Several 
approaches were proposed to collect such re-
sources.  Wiebe (2000) learned subjective adjec-
tives from corpora.  Takamura et al (2005) ex-
tracted semantic orientations of words.  Ku et al 
(2007) measured sentiment degrees of Chinese 
words by averaging the sentiment scores of the 
composing characters.  When the opinion words 
are available, the polarities of sentences and 
documents can be determined by them.  Riloff 
and Wiebe (2003) learned the extraction patterns 
for subjective expressions.  Kim and Hovy (2004) 
found the polarity of subjective expressions.  
Pang et al (2002) and Dave et al (2003) ex-
plored various techniques at document level. 
Morphological information has been widely 
used in classifying words, telling the meanings, 
and doing other in-depth analysis (Tzeng and 
Chen, 2002).  However, morphological informa-
tion was seldom applied either in Chinese opin-
ion extraction, or in solving the coverage prob-
lem of opinion dictionary.  Instead of bag-of-
characters approach (Ku et al, 2007), this paper 
employs morphological structures of words to 
extract opinion words.   
Relations between sentence segments are also 
defined by linguistics in the Chinese language.  
These are similar to morphological structures 
between Chinese characters. Based on parsing 
trees of sentences, we identify these relations and 
utilize them for opinion analysis on sentences. 
As the experimental corpus, some researchers 
managed to generate annotated materials and 
gold standards under many constraints.  Ku set a 
standard for generating final answers from anno-
tations of multiple annotators (Ku et al, 2007), 
and Somasundaran annotated discourse informa-
tion from meeting dialogs to train a sentiment 
model (Somasundaran et al, 2007).  For multi-
lingual issues, researchers concerned mainly 
about the applicability of corpus and algorithms 
from the native language to foreign languages 
(Banea et al, 2008; Bautin et al, 2008). 
Several opinion analysis systems have been 
developed so far.  OASYS (Cesarano et al, 2007) 
and CopeOpi (Ku et al, 2007) allow users input 
their queries and select preferred data sources, 
1260
and then track opinions in a time zone.  For both 
systems, extracting opinions is the main focus, 
while holders and targets are identified implicitly 
when retrieving relevant documents.  Carenini?s 
team proposed a graphical user interface for 
evaluative texts (2006), in which color blocks 
were used to present the evaluations for compo-
nents of products.  Fair News Reader, a Japanese 
news Web system, incorporates sentiment infor-
mation insensibly in an interesting way (Kawai 
et al, 2007).   It provides readers ?balanced? re-
ports by analyzing the sentiment in news articles 
which readers have read, and suggests them new 
articles according to the analysis results.  It leads 
the application of opinion analysis to the direc-
tion of personalization. 
2 Chinese Morphological Structures 
In the Chinese language, a word is composed of 
one or more Chinese characters, and its meaning 
can be interpreted in terms of its composite char-
acters.  The morphological structures of Chinese 
words are formulated by three major processes in 
linguistics: compounding, affixation, and conver-
sion.  Compounding is a complex word-
formation process.  In most cases, two or more 
morphemes together are formed as a lexical item 
by this process.  Affixation is a morphological 
process, by which grammatical or lexical infor-
mation is added to a base form.  By the conver-
sion process, a word is changed from one part of 
speech into another without the addition or dele-
tion of any morphemes.   
Compounding is the most productive way to 
construct a Chinese word.  Mostly, a Chinese 
character itself carries meanings, so that a mor-
pheme can function as a character and has its 
own part of speech.  In some cases, a Chinese 
morpheme may carry no specific meaning and 
just makes a word more readable.  Cheng and 
Tian (1992) divided Chinese words into five 
morphological types based on the relations be-
tween the morphemes in compounding words.  
(1) Parallel Type: Two morphemes play coordi-
nate roles in a word.  For example, the mor-
phemes ??? (money) and ??? (wealth) are par-
allel in the word ???? (money-wealth). 
 
(2) Substantive-Modifier Type: A modified 
morpheme follows a modifying morpheme.  For 
example, the morpheme ??? (cry) is modified 
by ??? (bitterly) in the word ???? (bitterly-
cry). 
 
(3) Subjective-Predicate Type: One morpheme 
is an expresser and the other one is described.  
The structure is like a subject-verb sentence con-
densed in one word.  For example, the morpheme 
??? (heart) is a subject of the predicate ??? 
(hurt) in the word ???? (heart-hurt). 
 
(4) Verb-Object Type: The first morpheme is 
usually a verb which governs the second one, 
making this word similar to a verb followed by 
its object.  For example, the morpheme ??? 
(control) serves as the object of the verb ??? 
(lose) in the word ???? (lose-control). 
 
(5) Verb-Complement Type: The first mor-
pheme is usually a verb but sometimes can be an 
adjective, and the second morpheme explains the 
first one from different aspects.  For example, the 
morpheme ??? (clearly) expresses the aspects of 
the action ??? (look). 
 
Chinese words constructed by affixation proc-
ess can be one of the two cases ? say, morpheme 
and morpheme, or morpheme and non-morpheme.  
In the case of morpheme and morpheme, the af-
fixation word belongs to one of the above 5 types 
if the prefix and the suffix are neither negations 
nor confirmations.  Types 6 and 7 defined below 
represent the affixation words whose prefix or 
suffix is a negation or a confirmation.  The af-
fixation words whose prefix or suffix characters 
are not morphemes are classified into type 8. 
 
(6) Negation Type: There is at least one nega-
tion character in words of this type.  For example, 
the prefix ??? (no) is the negation morpheme in 
the word ???? (no-method). 
 
(7) Confirmation Type: There is at least one 
confirmation character in words of this type.  For 
example, the prefix  ??? (do) is a confirmation 
in the word ???? (do-depend on). 
 
(8) Others: Those words that do not belong to 
the above seven types are assigned to this type, 
such as words whose meanings are not a function 
of their composite characters, words whose com-
posite characters are not morphemes, such as ??
?? (nephew-suffix) and ???? (peppermint). 
3 Opinion Scores of Chinese Words  
The bag-of-characters approach proposed by Ku 
et al (2007) considers the observation probabili-
ties of characters in Chinese opinion words.  It 
calculates the observation probabilities of char-
acters from a set of seeds first, then dynamically 
enlarges the set and adjusts their probabilities.  In 
1261
this approach, the opinion score of a word is de-
termined by the combination of the observation 
probabilities of its composite characters defined 
by Formulas (1) and (2). 
 
??
?
==
=
+
= m
i
i
n
i
i
n
i
i
neg,Cf/neg,Cfpos,Cf/pos,Cf
pos,Cf/pos,Cf
posCP
11
1
)()()()(
)()(
),( (1)
??
?
==
=
+
= m
i
i
n
i
i
m
i
i
neg,Cf/neg,Cfpos,Cf/pos,Cf
neg,Cf/neg,Cf
negCP
11
1
)()()()(
)()(
),( (2)
),(),()( negCNposCPCS ?=
 
(3)
)(
1
)...(
1
21 ?
=
=
l
i
il CSl
CCCS
 
(4)
 
where C is an arbitrary Chinese character, f(C, 
polarity) counts the observed frequency of C in a 
set of Chinese words whose opinion polarity is 
positive (pos) or negative (neg); P(C, pos) and 
P(C, neg) denote the observation probabilities of 
C as a positive and a negative character, and n 
and m denote total number of unique characters 
in positive and negative words.  The difference 
of P(C, pos) and P(C, neg) in Formula (3) de-
termines the sentiment score of character C, de-
noted by S(C).  Formula (4) computes the opin-
ion score of a word of l characters C1C2?Cl by 
averaging their scores. 
Instead of counting the weights as in the bag-
of-characters approaches, we consider the word 
structures and propose a scoring function for 
each morphological type.  According to the Fre-
quency Dictionary of Modern Chinese, 96.5% of 
Chinese words are unigrams and bigrams (Chen, 
et al, 1997).  In the following functions, S(C1C2) 
computes the opinion scores of words with char-
acters C1 and C2.  SIGN(s) returns -1 if polarity 
degree s is smaller than 0, i.e., negative, and re-
turns 1 when positive. 
 
(1) Parallel Type: Since the two composite 
characters of a word of this type are homogene-
ous, the opinion score is the average score of two 
characters? opinion scores.   
 
 
2
)()(
)( 2121
CSCS
CCS
+
=
 (5)
 
 
(2) Substantive-Modifier Type: The first mor-
pheme of a word of this type modifies the second 
one, so that its opinion weight comes from the 
absolute opinion score of the first character, 
while the opinion polarity is determined by the 
occurrence of negative opinion characters.  If at 
least one negative opinion character appears, the 
word is negative, else it is positive.  For example, 
the word ???? (bitterly cry) is composed of 
??? (bitterly, negative) and ??? (cry, negative).  
Negative characters make this word negative and 
its opinion strength, i.e., the absolute value of the 
score, is decided by the first character for the 
degree of crying. 
 
)()()( else
)( 1- )( else      
 )(  )( then  )0)(  and  0)(( if      
 then)0)(  and  0)(( if
2121
121
12121
21
CSCSCCS
CSCCS
CSCCSCSCS
CSCS
+=
?=
=>>
??
 
(6)
      
 
(3) Subjective-Predicate Type: The first mor-
pheme of a word of this type is a subject and the 
second morpheme is the action it performs, so 
that the action decides the opinion score of the 
word.  If the action is not an opinion or it is neu-
tral, the subject determines the opinion score of 
this word.  For example, the word ???? (mud-
slide, negative) is composed of ??? (mountain, 
non-opinion) and ??? (collapse, negative). Its 
opinion score depends only on the second char-
acter ??? (collapse) since the first character is a 
subject and usually bears no opinions. 
 
)()( else
 )()( then )0)(( if
121
2212
CSCCS
CSCCSCS
=
=?  (7)
 
 
(4) Verb-Object Type: The first morpheme of 
words of this type acts upon the second mor-
pheme.  The effect depends not only on the ac-
tion but on the target.  The weight is determined 
by the action, but the polarity is the multiplica-
tion of the signs of the two morphemes.  For ex-
ample, the word ???? (to go away for the 
summer, positive) is composed of ??? (hide, 
negative) and ??? (hot summer, negative).  Its 
strength depends on the strength of ??? (hide) 
and polarity is positive from the multiplication of 
two negatives.  
 
)()()( else    
))(())(()()(    then  
)0)(  and  0)(( if
2121
21121
21
CSCSCCS
CSSIGNCSSIGNCSCCS
CSCS
+=
??=
??
(8)
 
 
(5) Verb-Complement Type: The scoring func-
tion for words of this type is defined the same as 
that of a Subjective-Predicate type in Formula 
(7).  The complement morpheme is the deciding 
factor of the opinion score.  For example, the 
word ???? (raise, positive) is composed of 
??? (carry or lift, non-opinion) and ??? (high, 
1262
positive).  The complement morpheme ?? ? 
(high) describes the resulting state of the verb 
morpheme ??? (raise), so both strength and po-
larity depend on the morpheme ??? (high). 
 
(6) Negation Type: A negative character speci-
fied in a predefined set NC has a negation effect 
on the opinion score of the other character.  The 
strength depends on the modified morpheme 
while the polarity of the word is the negation of 
the polarity of the modified morpheme. 
 
( )
( ) )(1)( else
 )(1)( then )( if
121
2211
CSCCS
CSCCSNCC
??=
??=?  (9)
 
 
(7) Confirmation Type: A positive character 
specified in a predefined set PC ensures that the 
opinion score of a word only comes from the 
other character.  Therefore, the opinion score of 
this word is determined by the modified mor-
pheme. 
 
)()( else )()( then )( if 1212211 CSCCSCSCCSPCC ==?  (10)
 
 
(8) Others: Since words of this type contain no 
clear cues for their morphological structures, we 
postulate that both characters have the same con-
tribution, and adopt Formula (5).  
4 Identification of Morphological Types 
To compute the opinion score of a word accord-
ing to formulae in Section 3, we must know its 
morphological type from the morphological 
structure, i.e., the parts of speech of the compos-
ite morphemes.  Currently, part of speech tag-
ging is performed at the word level rather than 
the morpheme level, and morpheme-tagging cor-
pus is not available.  We consider an on-line 
Chinese dictionary, Dictionary of Chinese Words 
by Ministry of Education, Taiwan (MOEDCW), 
as a corpus, and compute the statistics of each 
morpheme in it. 
Two classifiers, CRF classifier and SVM clas-
sifier are proposed to recognize morphological 
types (1)-(5).  Morphological types (6) to (8) are 
determined by rules such as whether two com-
posite characters are morphemes; whether there 
are confirmation/negation morphemes; and so on. 
4.1 MOEDCW Corpus 
MOEDCW corpus provides possible parts of 
speech for each morpheme by treating it as a uni-
gram word, and possible senses under each part 
of speech.  In each entry, there are a sense defini-
tion and some example words.  Figures 1 and 2 
show the specifications of two morphemes ??? 
and ???.  The morpheme ???  has three parts 
of speech (verb, adverb and noun) and includes 3, 
1, and 1 senses.  There are 3, 3, and 2 example 
words listed under the three verb senses. 
We can find the correct parts of speech of the 
composite characters of a word when it is an ex-
ample word in the dictionary.  However, not all 
words are listed in the corpus.  Consider the 
word ???? (sweat, verb).  Figure 1 shows that 
???? (sweat) is an example word listed under 
the verb sense of the character ??? (perspire), 
thus the character ??? (perspire) in the word ??
?? (sweat) functions as a verb.  However, ??
?? (sweat) is not an example for the character 
??? (sweat).  Figure 2 show that there are two 
possible parts of speech, noun and verb, for the 
character ??? (sweat).  We then show how to 
identify its function in the word ????.   
 
1
Goes out from the button to the top or 
from inside to outside.  For example, 
fume, smoking, and sweat. ?????
???????????????
?????????? 
2
Burst into or regardless of.  For example, 
take risk, to offend, and offense.  ?
??????????????
????????? 
verb 
3
Fake or on the pretext of.  For example, 
personate and to pretend to be. ???
??????????????? 
ad-
verb 1
Crude or rash.  For example, offensively 
and advance rashly.  ??????
???????????? 
noun 1 Family name. ?? 
Figure 1: Specification of ??? in MOEDCW 
1
Sweat.  For example, cold sweat, night 
sweat, sweatiness, and to drip with 
sweat. ?????????????
????????????????
??????????????? 
noun
2 Family name?? 
verb 1 To sweat ??????? 
Figure 2: Specification of  ??? in MOEDCW 
)( )( POS,CnsesNumberOfSePOS,CT =  (11)
 
The number of possible meanings one charac-
ter can bear when it functions as a certain part of 
1263
speech is employed to estimate how often this 
part of speech is used.  The function T(C, POS) 
shown in Formula (11) defines the score of a 
character C functioning as a particular part of 
speech POS.  Here, POS may be noun (N), adjec-
tive (ADJ), verb (V), adverb (ADV), auxiliary 
(AUX), conjunction (CONJ), pronoun (PRON), 
preposition (PREP), and interjection (INT).  In 
Figure 2, T(?<sweat>, N) = 2 and  T(?<sweat>, 
V) = 1. 
4.2 Features for Classifiers 
Features for training SVM and CRF classifiers 
include the pronunciation and the tone of the 
word, parts of speech of the first and the second 
characters of training words, and the position 
information of the composite characters.  The 
tone of the word is acquired from MOEDCW.  
The parts of speech are estimated by Formula 
(11).   f(C, POS, k, start/end) counts the number 
of k-grams (k=2, 3, 4).  In Figures 1 and 2, f(?, 
V, 2, start)=6, f(?, V, 2, end)= 2, f(?, ADV, 2, 
start) = 2, and f(?, ADV, 2, end)=0.  This ex-
ample shows that when the character ??? func-
tions as a verb or an adverb, it serves as the start-
ing character more often than the ending charac-
ter. 
4.3 CRF and SVM Classifier  
CRF and SVM are both common used algorithms 
for building classifiers (Lafferty et al, 2001).  
We adopted CRF++1 and libSVM (Chang and 
Lin, 2001) to develop our classifiers.  The fea-
tures for training our CRF and SVM classifiers 
include the input word W, the tone of W, the first 
and the second characters C1 and C2, T(C1, POS), 
T(C2, POS),  f(C1, POS, k, start), f(C1, POS, k, 
end), f(C2, POS, k, start), and f(C2, POS, k, end).  
POS denotes one of nine parts of speech in 
MOEDCW, and k equals to 2, 3 or 4. 
Using SVM is straightforward.  To classify a 
word into one of the morphological structure 
types, we construct the word's feature vector and 
input the vector into SVM.  When using CRF, a 
different approach is taken.  When predicting the 
classes of two successive instances, CRF takes 
the predicted class of the first instance into ac-
count when predicting the second instance's class.  
Here is how we exploit this capability.  In a nut-
shell, we perform classification at the character 
level instead of the word level.  Let W be a word 
composed of the two characters C1 and C2.  Let v 
                                                 
1 http://crfpp.sourceforge.net/ 
be the feature vector of W.  Let t be the morpho-
logical structure type of W.  We define C1's fea-
ture vector to be composed of the features in v 
which are related to C1, e.g., T(C1, verb).  Simi-
larly, C2's feature vector is composed of the fea-
tures in v which are related to C2.  C1's class and 
C2's class are defined as t_1 and t_2, respectively.  
Since t has five possible values, there are 10 
character classes. 
To determine a word W's morphological struc-
ture type, we first apply CRF on W's constituent 
characters C1 and C2's feature vectors.  For C1, 
CRF will return a set of probabilities P(C1,t_q), 
where q ? {1, 2}, indicating the likelihood of C1 
being an instance of class t_q.  Similarly, a set of 
probabilities P(C2,t_q) is returned for C2.  W's 
morphological structure type is defined as the 
value of t which maximizes the product of 
P(C1,t_1) and P(C2,t_2). 
Though CRF is mostly used for sequential la-
beling, the idea of using CRF is to tail this classi-
fication questions into a labeling question in or-
der to utilizing the position information of char-
acters.  As mentioned, if a word W of two char-
acters C1C2 is of type 1, CRF will label C1 1_1 
(type1_1st char) and C2 1_2 (type1_2nd char).  
The labeling of each character considers both the 
previous character's features and the next charac-
ter's features.  That is, if the current character is 
the first character, its previous character is an 
empty character (which is used for segmenting 
sequences in CRF); if the current character is the 
second character, its next character is an empty 
character. Hence the position information will be 
considered by CRF. 
5 Experiments and Discussion  
Experiments verify whether the morphological 
types benefit opinion polarity detection on words.  
The relation between the performance of mor-
phological classifiers and opinion polarity detec-
tion is discussed. 
5.1 Experimental Setup  
To compare the bag-of-characters approach (Ku 
et al, 2007) with our morphological structure 
approach, we adopt the same evaluation data set 
containing 836 words.  To evaluate the perform-
ance of our two morphological classifiers, we 
prepare two sets of words, including the testing 
set of 836 words for word-level opinion predic-
tion (abbreviated as OP), and a set of 8,186 
words selected from words in MOEDCW corpus 
and news documents except those can be classi-
1264
fied by patterns (abbreviated as TRAIN set), all 
with their morphological types annotated.  Table 
1 lists the distributions of morphological types in 
OP and TRAIN sets. 
The polarity of words is predicted by their 
opinion scores ranging between -1 to 1.  We set a 
positive threshold.  Those words with scores 
above it are considered as positive while those 
below this threshold multiplied by (-1) are re-
garded as negative.  The words with non-zero 
scores falling between the positive and negative 
thresholds are neutral.  Fifty grids from 0 to 0.5 
are searched for the best threshold.  Since the 
opinion extraction at word level concerns only 
word structure, no retraining for the best thresh-
old is need when domain shifts, which is a supe-
riority of our method. 
5.2 Morphological Type Classification and 
Polarity Detection  
The performances of CRF and SVM classifiers 
on each morphological type are listed in Table 2.  
We perform four-fold cross validation on the 
TRAIN set.  Results show that CRF classifier 
achieves better performance than SVM classifier 
in this task.  The accuracy of CRF classifier 
(0.70) is 8% higher than that of SVM classifier 
(0.62).  Note those type 8 words which could be 
extracted by rules are excluded from classifica-
tion experiment. The remaining type 8 words are 
usually proper names.  It is difficult for both 
classifiers to identify such words. 
Table 3 further shows the performance of po-
larity prediction using morphological types de-
termined by CRF classifier and SVM classifier.  
The performance of polarity detection is evalu-
ated by the f-score defined in Formula (12). 
The f-scores of polarity detection using CRF 
classified types and SVM classified types are 
0.5806 and 0.5938, respectively.  Both of them 
outperform baseline?s f-score 0.5455, i.e., the 
bag-of-characters approach (Ku et al, 2007).  
Experiments show that adopting morphological 
types annotated by two classifiers for polarity 
prediction has little difference.  In other words, 
CRF and SVM classifiers have an 8% f-score 
difference in their best performance of classifica-
tion, while the performance gap in word polarity 
prediction using morphological types provided 
by these two classifiers is around 1.3% only 
(0.5806 vs. 0.5938).  The reason may be that we 
define scoring functions of each morphological 
type in a straightforward way.  If they are not the 
best scoring functions, the benefit of considering 
the morphological type information could be re-
stricted.  Nevertheless, experimental results show 
that morphological type information is useful for 
word polarity detection (with p-value less than 
0.05). 
 
)(
)()(
opinionproposed
polaritycorrectopinioncorrect
P
?
= , 
)(
)()(
opiniongold
polaritycorrectopinioncorrect
R
?
= , 
RP
RP
scoref
+
??
=?
2 . 
(12)
 
set/type 1 2 3 4 5 6 7 8 
TRAIN 26.15 44.97 1.64 15.14 9.22 0 0 2.88 
OP 45.8 24.4 1.3 7.9 8.0 2.3 0.5 9.8 
Table 1: The Percentage of distribution for morphological types in TRAIN and OP sets 
MorphoType 1 2 3 4 5 8 Accuracy 
CRF 0.63 0.78 0.41 0.66 0.78 0.17 0.70 
SVM 0.49 0.73 0.22 0.52 0.55 0 0.62 
Table 2: The f-score of CRF and SVM classifiers 
We further examine how well our polarity de-
tection method works in combination with a 
word sentiment dictionary.  We use the NTUSD2 
word sentiment dictionary.  If a word appears in 
NTUSD, then the word's polarity is the one 
specified in NTUSD.  If a word does not appear 
in NTUSD, then the word's polarity is deter-
mined using our morphological type method. 
                                                 
2 http://nlg18.csie.ntu.edu.tw:8080/opinion/ 
After introducing a sentiment dictionary 
NTUSD3, CRF and SVM classifiers both achieve 
the f-score 0.77 for opinion word extraction, and 
achieve f-scores 0.61 and 0.62 for polarity detec-
tion, respectively.  Note that if only NTUSD is 
used to extract opinion words by string matching, 
the f-score is only 0.44. 
 
                                                 
3 http://nlg18.csie.ntu.edu.tw:8080/opinion/ 
1265
Polarity f-score Without NTUSD With NTUSD 
Ku 0.5455 0.5789 
CRF type 0.5806 0.6100 
SVM type 0.5938 0.6246 
 
Table 3: Prediction with Morphological Types 
We further analyze the improvement of polar-
ity prediction for each morphological type.  We 
find that the f-scores of polarity prediction of all 
morphological types are improved in different 
degrees, and among them the performance of 
type 2 words are improved the most.  We have 
shown that our method can assign an opinion 
score to an arbitrary word without any word 
thesauri by considering its morphological infor-
mation.  Moreover, since the Substantive-
Modifier (type 2) is the most common way to 
form a new word in the Chinese language 
(Cheng and Tian, 1992), the result presents the 
strength of our method in solving the coverage 
problem. 
6 Syntactic Structure for Chinese Opin-
ion Analysis 
As mentioned, the relations introduced in Section 
2 exist not only within words, but also between 
sentence segments.  Relations between sentence 
segments are represented by structural trios here-
after and will be introduced in next section.  We 
have already shown that morphological types are 
useful when extracting opinion words and would 
like to further testify whether structural trios also 
benefit the opinion analysis on sentences.  We 
annotate these relations manually, propose a 
method to identify these relations, and compare 
results of experimental settings using structural 
trios with those not using structural trios. 
6.1 Structural Trio 
Each node in a parsing tree dominates a word 
string in a sentence.  Linguistics have shown that 
there are also five relations between sentence 
segments: Parallel, Substantive-Modifier, Sub-
jective-Predicate, Verb-Object, and Verb-
Complement, same as morphological types (1) to 
(5).  Because parsing trees have hierarchical 
structures, we define a structural trio to represent 
a relation between two nodes as follows: 
(1) A structure trio contains two children 
nodes which bear a relation. 
(2) A structure trio contains one head node 
which is the nearest common parent of two 
children nodes in (1). 
 
 
Figure 3: Example of structural trios 
Figure 3 shows an example of a structure trio.  
It is a part of a parsing tree containing words ??
?? (obtain), ???? (happy), ???? (results).  
Two structural trios are shown in this example.  
The lower one contains two children nodes ??
?? (happy) and ???? (results), and is labeled 
as Substantive-Modifier (S-M (2)) in their near-
est common parent node, while the upper one 
contains two children nodes ???? (obtain) and 
?????? (happy results), and is labeled as 
Verb-Object (V-O (4)). 
6.2 Experimental Corpus 
To experiment with structural trios, we need the 
parsing trees of all experimental sentences.  For 
this purpose, we adopted Chinese Treebank 5.14 
as the experimental materials.  Chinese Treebank 
contains raw Chinese news documents together 
with their segmented, part of speech tagged, and 
parsed versions.  The parsed documents are 
adopted in experiments utilizing structural trios, 
and the part of speech tagged documents are used 
in experiments not utilizing structural trios. 
In Chinese Treebank, a unique ID is labeled 
on each sentence.  For each sentence, we had 
three annotators label their opinions and then we 
generate the gold standard following NTCIR 5 
MOAT protocol (Seki et al, 2008).  We also 
annotated structure trios in Chinese Treebank.  A 
total of 17,159 sentences are obtained after drop-
ping some faulty sentences such as empty sen-
tences and sentences composed of more than one 
parsing tree.  The statistics of opinion sentences 
and structural trios in the constructed experimen-
tal materials are shown in Table 4 and Table 5. 
 
 
                                                 
4 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? 
catalogId=LDC2005T01 
5 http://research.nii.ac.jp/ntcir/index-en.html 
1266
Opinion Non-Opinion 
Positive Neutral Negative 
6,380 1,537 1,714 # 
9,631 
7,528 
66.24 15.96 17.80 % 
56.13 
43.87 
Table 4: Statistics of opinion sentences 
Trio Type Number Percentage % 
2 18,483 36.85 
3 13,687 27.29 
4 15,970 31.84 
5 965 1.92 
Others 1,054 2.10 
Total 50,159 100.00 
Table 5: Statistics of structural trios 
6.3 Experiment Setup 
The aim of our experiments is to know how 
opinion analysis approach performs when mor-
phological and syntactic structures are incorpo-
rated.  They are compared with the bag-of-
character and bag-of-word approaches. We im-
plemented the bag-of-word approach proposed 
by Ku et al (2007) to show its performance on 
Chinese Treebank.  In their approach, the opin-
ion scores of words are summed to generate the 
opinion scores of sentences, and the negation 
words will negate the closest opinion words.  
Based on this approach, we further consider 
structural trios to experiment whether syntactic 
structures of sentences are beneficial for opinion 
analysis.  Because the scoring functions may not 
be straight forward as those we have adopted for 
opinion word extraction, we did not design scor-
ing functions for utilizing all types of structural 
trios.  Instead, we emphasize their original opin-
ion scores by multiplying a variable alpha to see 
whether these structures are important.  In this 
paper, alpha equals five. 
We have shown that word morphological 
structures benefit the word opinion extraction.  
When we experiment on sentences, we also in-
corporate the word morphological structures to 
see whether they are also useful for opinion 
analysis on sentences.  Five experimental set-
tings are listed as below:  
(1) bag[w]-bag[s]: structural information is 
not considered for both words and sen-
tences.  The bag-of-character approach 
is used to calculate the opinion scores of 
words, and the bag-of-word approach 
sentences. 
(2) struc[w]-bag[s]: morphological struc-
tures are utilized to calculate word opin-
ion scores, but structural trios are not 
considered. The bag-of-word approach 
is used to calculate the opinion scores of 
sentences. 
(3) bag[w]-struc[s]: structural trios are con-
sidered for calculating sentence opinion 
scores, while the bag-of-character ap-
proach is used to calculate the opinion 
scores of words. 
(4) struc[w]-(m)struc[s]: both word mor-
phological structures and manually la-
beled structural trios are adopted. 
(5) struc[w]-struc[s]: both morphological 
structure of words and system labeled 
structural trios are adopted. 
As we have shown that NTUSD is beneficial 
to the opinion analysis at word level, it is used as 
described in section 5.2 by default. 
Our system adopted CRF algorithm to label 
structural trios for setting (5).  The content string 
and the part of speech of the current node, its 
parent node, its offspring nodes in the next three 
generations, together with the depth of the cur-
rent node in the Chinese Treebank, are used as 
the features for each node in CRF.  The co-
occurrence of the current node and all its siblings 
are defined in CRF?s template file.  CRF will 
label whether the current node is the first child or 
the second child of a certain relation in a struc-
tural trio, or it is not part of any structural trios.  
A four-fold experiment is performed for the 
learning and testing of this labeling  process by 
CRF. 
6.4 Results and Discussion 
Table 6 shows the statistics of manually labeled 
structural trios in Chinese Treebank and identifi-
cation performance of CRF.  Table 7 shows the 
performance of five experiment settings de-
scribed in Section 6.3.  The experiment results 
show that the morphological structures of words 
do not have a large contribution for opinion sen-
tence analysis (setting 1 vs. setting 2; setting 3 vs. 
setting 4).  However, considering the structural 
trios improve the performance.   
 
 
 
 
 
 
 
 
1267
Trio Type Number Percentage f-Score
2 18,483 36.85% 0.4883
3 13,687 27.29% 0.4944
4 15,970 31.84% 0.6360
5 965 1.92% 0.2034
Others 1,054 2.10% 
Total 50159 100%  
Table 6: Statistics and Results of Identifying 
Structural Trios 
Setting Word [w] 
Sentence 
 [s] 
f-Score 
(opinion) 
f-Score 
(polarity)
1 bag bag 0.7073 0.4988 
2 struc bag 0.7162 0.5117 
3 bag struc 0.8000 0.5361 
4 struc (m)struc 0.7922 0.5297 
5 struc struc 0.7993 0.5187 
Table 7: Results of Opinion Extraction  
on Chinese Treebank 
By summarizing the experimental results in 
Section 5 and this section, we can conclude that 
considering the word morphological structures 
benefits the opinion polarity detection, but in the 
current approach its assistance to words does not 
propagate to sentences.  Considering the syntac-
tic structures, however, do help in opinion analy-
sis both for the opinion sentence extraction and 
the polarity detection.  The performance of opin-
ion extraction boosts to an f-score 0.80 and the 
performance of polarity detection an f-score 0.54.   
However, the utilization of structure trios 
needs the parsing tree of sentences as the prior 
knowledge.  Hence these two kinds of structural 
information may be suitable for different applica-
tions: structural trios for well written sentences 
such as those in the news articles, while the mor-
phological structures for casually written sen-
tences such as those appear in SMS messages or 
articles with limit length on the Web. 
Because there are no opinion experiments per-
formed on Chinese Treebank, we mention the 
performance of Ku?s approach (setting (1)) for 
opinion sentence extraction, f-score 0.6846, in 
NTCIR-7 MOAT task, on news articles, as a re-
sult for comparison.  Their approach was ranked 
the second in this task, and the best team 
achieved an f-score 0.7453. 
7 Conclusion and Future Work  
This paper considers morphological and syntac-
tic structures in analyzing Chinese opinion words 
and sentences.  For morphological structures, 
eight Chinese morphological types are defined. 
CRF classifier and SVM classifier for morpho-
logical type classification are proposed.  Experi-
ments show that CRF classifier achieves the best 
accuracy 0.70 in type classification, which is 8% 
better than SVM classifier.  We further show that 
word morphological structures benefit the opin-
ion word extraction significantly.  With the help 
of the sentiment dictionary NTUSD, the f-score 
of opinion word extraction achieves 0.77 and the 
f-score of the word polarity detection achieves 
0.62 when the word morphological types are 
provided by the SVM classifier.  They are com-
parably better than bag-of-character approach 
and the dictionary based approach. 
We defined structural trios to represent the re-
lations between sentence segments and also ex-
tract these relations using CRF algorithm.  Re-
sults show that considering structural trios bene-
fits the opinion analysis on sentences.  An f-
score 0.80 for opinion extraction and an f-score 
0.54 for polarity detection are achieved, which is 
a great improvement.  
The opinion scoring functions for morphologi-
cal types and structural trios are critical for polar-
ity detection, and scoring functions for words 
determine the scoring functions for sentences.  
Now we define these functions intuitively based 
on linguistic rules, but learning methods like re-
gression will be investigated in the future.  Ex-
amining the interaction of cues from word and 
sentence levels on the opinion sentence extrac-
tion and the opinion polarity detection is our next 
goal. 
Acknowledgement 
Research of this paper was partially supported by Na-
tional Science Council, Taiwan, under the contract 
NSC95-2221-E-002-265-MY3.  
References  
Banea, C., Mihalcea, R., Wiebe, J. and Hassan, S. 
2008. Multilingual Subjectivity Analysis Using 
Machine Translation. In Proceedings of Empirical 
Methods in Natural Language Processing (EMNLP 
2008). 
Bautin, M., Vijayarenu, L. and Skiena, S. 2008. Inter-
national sentiment analysis for news and blogs. In 
Proceedings of the International Conference on 
Weblogs and Social Media (ICWSM). 
Carenini, G., Ng, R. T. and Pauls, A. 2006. Interactive 
Multimedia Summaries of Evaluative Text. In Pro-
ceedings of the 11th International Conference on 
Intelligent User Interfaces (pp. 124-131), Sydney, 
Australia. 
1268
Cesarano, C., Picariello, A., Reforgiato, D. and 
Subrahmanian, V.S. 2007. The OASYS 2.0 Opin-
ion Analysis System.  Demo in Proceedings of In-
ternational Conference on Weblogs and Social 
Media (pp. 313-314), Boulder, CO USA. 
Chang, Chih-Chung and Lin, Chih-Jen. 2001. 
LIBSVM: a library for support vector machines, 
http://www.csie.ntu.edu.tw/~cjlin/libsvm  
Chen, A., Xu, L., Gey, F.C. and Meggs, J. 1997. Chi-
nese Text Retrieval without Using a Dictionary. 
ACM SIGIR Forum, Volume 31, Issue SI (pp. 42-
49). 
Cheng, X.-H. and Tian, X.-L. 1992. Modern Chinese.  
Bookman Books Ltd. 
Dave, K., Lawrence, S., and Pennock, D.M. 2003. 
Mining the Peanut Gallery: Opinion Extraction 
and Semantic Classification of Product Reviews. 
In Proc. of the 12th International WWW Confer-
ence (pp. 519-528). 
Kawai, Y., Kumamoto, T. and Tanaka, K. 2007. Fair 
News Reader: Recommending news articles with 
different sentiments based on user preference. In 
Proceedings of Knowledge-Based Intelligent In-
formation and Engineering Systems (KES), No. 
4692 in Lecture Notes in Computer Science (pp. 
612?622). 
Kim, S.-M. and Hovy, E. 2004. Determining the Sen-
timent of Opinions. In Proc. of the 20th ICCL (pp. 
1367-1373). 
Ku, L.-W. and Chen, H.-H. 2007. Mining Opinions 
from the Web: Beyond Relevance Retrieval. Jour-
nal of American Society for Information Science 
and Technology, Special Issue on Mining Web Re-
sources for Enhancing Information Retrieval, 
58(12), 1838-1850. 
Lafferty, J., McCallum, A. and Pereira, F. 2001. Con-
ditional Random Fields: Probabilistic Models for 
Segmenting and Labeling Sequence Data, In Proc. 
of ICML (pp.282-289). 
Pang, B., Lee, L. and Vaithyanathan, S. 2002. Thumbs 
up? Sentiment Classification Using Machine 
Learning Techniques. In Proc. of the 2002 Confer-
ence on EMNLP (pp. 79-86). 
Riloff, E. and Wiebe, J. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of the 
2003 Conference on EMNLP (pp. 105-112).  
Seki, Y., Evans, D. K., Ku, L.-W., Sun, L., Chen, H.-H. 
and Kando, N. 2008.  Overview of Multilingual 
Opinion Analysis Task at NTCIR-7. In Proceed-
ings of the 7th NTCIR Workshop Meeting on 
Evaluation of Information Access Technologies: 
Information Retrieval, Question Answering, and 
Cross-Lingual Information Access. 
Somasundaran, S., Ruppenhofer, J. and Wiebe, J. 
2007. Detecting arguing and sentiment in meetings. 
Proceedings of the SIGdial Workshop on Dis-
course and Dialogue, 2007.8.6 
Takamura, H., Inui, T. and Okumura, M. 2005. Ex-
tracting Semantic Orientations of Words Using 
Spin Model. In Proc. of the 43rd Annual Meeting 
of the ACL (pp. 133-140). 
Tzeng, H. and Chen, K.-J. 2002. Design of Chinese 
Morphological Analyzer.  In Proc. of the 1st 
SIGHAN Workshop on Chinese Language Process-
ing, vol.18, 1-7. 
Wiebe, J. 2000. Learning Subjective Adjectives from 
Corpora. In Proc. of the 17th National Conference 
on AAAI and Twelfth Conference on IAAI (pp. 735-
740). 
 
1269
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 89?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
Test Collection Selection and Gold Standard Generation  
for a Multiply-Annotated Opinion Corpus 
Lun-Wei Ku, Yong-Shen Lo and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
{lwku, yslo}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw 
Abstract 
Opinion analysis is an important research 
topic in recent years.  However, there are 
no common methods to create evaluation 
corpora.  This paper introduces a method 
for developing opinion corpora involving 
multiple annotators.  The characteristics of 
the created corpus are discussed, and the 
methodologies to select more consistent 
testing collections and their corresponding 
gold standards are proposed.    Under the 
gold standards, an opinion extraction sys-
tem is evaluated.  The experiment results 
show some interesting phenomena. 
1 Introduction 
Opinion information processing has been studied 
for several years.  Researchers extracted opinions 
from words, sentences, and documents, and both 
rule-based and statistical models are investigated  
(Wiebe et al, 2002; Pang et al, 2002).  The 
evaluation metrics precision, recall and f-measure 
are usually adopted.   
A reliable corpus is very important for the opin-
ion information processing because the annotations 
of opinions concern human perspectives.  Though 
the corpora created by researchers were analyzed 
(Wiebe et al, 2002), the methods to increase the 
reliability of them were seldom touched.  The strict 
and lenient metrics for opinions were mentioned, 
but not discussed in details together with the cor-
pora and their annotations. 
This paper discusses the selection of testing col-
lections and the generation of the corresponding 
gold standards under multiple annotations.  These 
testing collections are further used in an opinion 
extraction system and the system is evaluated with 
the corresponding gold standards.  The analysis of 
human annotations makes the improvements of 
opinion analysis systems feasible. 
2 Corpus Annotation 
Opinion corpora are constructed for the research of 
opinion tasks, such as opinion extraction, opinion 
polarity judgment, opinion holder extraction, 
opinion summarization, opinion question 
answering, etc..  The materials of our opinion 
corpus are news documents from NTCIR CIRB020 
and CIRB040 test collections.  A total of 32 topics 
concerning opinions are selected, and each 
document is annotated by three annotators.  
Because different people often feel differently 
about an opinion due to their own perspectives, 
multiple annotators are necessary to build a 
reliable corpus.  For each sentence, whether it is 
relevant to a given topic, whether it is an opinion, 
and if it is, its polarity, are assigned.   The holders 
of opinions are also annotated.  The details of this 
corpus are shown in Table 1. 
 
 Topics Documents Sentences
Quantity 32 843 11,907 
Table 1. Corpus size  
3 Analysis of Annotated Corpus  
As mentioned, each sentence in our opinion corpus 
is annotated by three annotators.  Although this is a 
must for building reliable annotations, the incon-
sistency is unavoidable.   In this section, all the 
possible combinations of annotations are listed and 
two methods are introduced to evaluate the quality 
of the human-tagged opinion corpora. 
3.1 Combinations of annotations 
Three major properties are annotated for sen-
tences in this corpus, i.e., the relevancy, the opin-
ionated issue, and the holder of the opinion.  The 
combinations of relevancy annotations are simple, 
and annotators usually have no argument over the 
opinion holders.  However, for the annotation of 
the opinionated issue, the situation is more com-
89
plex.  Annotations may have an argument about 
whether a sentence contains opinions, and their 
annotations may not be consistent on the polarities 
of an opinion.  Here we focus on the annotations of 
the opinionated issue.  Sentences may be consid-
ered as opinions only when more than two annota-
tors mark them opinionated.  Therefore, they are 
targets for analysis.   The possible combinations of 
opinionated sentences and their polarity are shown 
in Figure 1. 
 
A B 
  
C E 
  
D 
 
 
 
 
Positive/Neutral/Negative 
Figure 1. Possible combinations of annotations 
In Figure 1, Cases A, B, C are those sentences 
which are annotated as opinionated by all three 
annotators, while cases D, E are those sentences 
which are annotated as opinionated only by two 
annotators.  In case A and case D, the polarities 
annotated by annotators are identical.  In case B, 
the polarities annotated by two of three annotators 
are agreed.  However, in cases C and E, the polari-
ties annotated disagree with each other.  The statis-
tics of these five cases are shown in Table 2. 
 
Case A B C D E All
Number 1,660 1,076 124 2,413 1,826 7,099
Table 2. Statistics of cases A-E 
3.2 Inconsistency 
3 
P P P 
N N N 
X X X
3 
Multiple annotators bring the inconsistency.  There 
are several kinds of inconsistency in annotations, 
for example, relevant/non-relevant, opinion-
ated/non-opinionated, and the inconsistency of po-
larities.  The relevant/non-relevant inconsistency is 
more like an information retrieval issue.  For opin-
ions, because their strength varies, sometimes it is 
hard for annotators to tell if a sentence is opinion-
ated.  However, for the opinion polarities, the in-
consistency between positive and negative annota-
tions is obviously stronger than that between posi-
tive and neutral, or neutral and negative ones.  
Here we define a sentence ?strongly inconsistent? 
if both positive and negative polarities are assigned 
to a sentence by different annotators.  The strong 
inconsistency may occur in case B (171), C (124), 
and E (270).  In the corpus, only about 8% sen-
tences are strongly inconsistent, which shows the 
annotations are reliable. 
P P 
N N 
X X
2 3 
P X N 
P N X
N P X
N X P 
X P N 
X N P 
P N
3.3 Kappa value for agreement 
We further assess the usability of the annotated 
corpus by Kappa values.  Kappa value gives a 
quantitative measure of the magnitude of inter-
annotator agreement.  Table 3 shows a commonly 
used scale of the Kappa values. 
 
Kappa value Meaning 
<0 less than change agreement 
0.01-0.20 slight agreement 
0.21-0.40 fair agreement 
0.41-0.60 moderate agreement 
0.61-0.80 substantial agreement 
0.81-0.99 almost perfect agreement 
Table 3. Interpretation of Kappa value 
The inconsistency of annotations brings difficul-
ties in generating the gold standard.  Sentences 
should first be selected as the testing collection, 
N P
P X
N X
X P
NX
2
P P 
N N
X X
P NX 
90
and then the corresponding gold standard can be 
generated.  Our aim is to generate testing collec-
tions and their gold standards which agree mostly 
to annotators.  Therefore, we analyze the kappa 
value not between annotators, but between the an-
notator and the gold standard.  The methodologies 
are introduced in the next section. 
4 Testing Collections and Gold Standards 
The gold standard of relevance, the opinionated 
issue, and the opinion holder must be generated 
according to all the annotations.  Answers are cho-
sen based on the agreement of annotations.  Con-
sidering the agreement among annotations them-
selves, the strict and the lenient testing collections 
and their corresponding gold standard are gener-
ated.  Considering the Kappa values of each anno-
tator and the gold standard, topics with high agree-
ment are selected as the testing collection.  More-
over, considering the consistency of polarities, the 
substantial consistent testing collection is gener-
ated.  In summary, two metrics for generating gold 
standards and four testing collections are adopted. 
4.1 Strict and lenient 
Namely, the strict metric is different from the leni-
ent metric in the agreement of annotations.  For the 
strict metric, sentences with annotations agreed by 
all three annotators are selected as the testing col-
lection and the annotations are treated as the strict 
gold standard; for the lenient metric, sentences 
with annotations agreed by at least two annotators 
are selected as the testing collection and the major-
ity of annotations are treated as the lenient gold 
standard.  For example, for the experiments of ex-
tracting opinion sentences, sentences in cases A, B, 
and C in Figure 1 are selected in both strict and 
lenient testing collections, while sentences in cases 
D and E are selected only in the lenient testing col-
lection because three annotations are not totally 
agreed with one another.  For the experiments of 
opinion polarity judgment, sentences in case A in 
Figure 1 are selected in both strict and lenient test-
ing collections, while sentences in cases B, C, D 
and E are selected only in the lenient testing col-
lection.  Because every opinion sentence should be 
given a polarity, the polarities of sentences in cases 
B and D are the majority of annotations, while the 
polarity of sentences in cases C are given the po-
larity neutral in the lenient gold standard.  The po-
larities of sentences in case E are decided by rules 
P+X=P, N+X=N, and P+N=X.  As for opinion 
holders, holders are found in opinion sentences of 
each testing collection.  The strict and lenient met-
rics are also applied in annotations of relevance. 
4.2 High agreement 
To see how the generated gold standards agree 
with the annotations of all annotators, we analyze 
the kappa value from the agreements of each anno-
tator and the gold standard for all 32 topics.  Each 
topic has two groups of documents from NTCIR: 
very relevant and relevant to topic.  However, one 
topic has only the relevant type document, it re-
sults in a total of 63 (2*31+1) groups of documents.  
Note that the lenient metric is applied for generat-
ing the gold standard of this testing collection be-
cause the strict metric needs perfect agreement 
with each annotator?s annotations.  The distribu-
tion of kappa values of 63 groups is shown in Ta-
ble 4 and Table 5.  The cumulative frequency bar 
graphs of Table 4 and Table 5 are shown in Figure 
2 and Figure 3. 
 
Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 1 2 12 14 33 1 
Table 4. Kappa values for opinion extraction 
Kappa <=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 9 0 7 21 17 9 
Table 5. Kappa values for polarity judgment 
Figure 2. Cumulative frequency of Table 4 
1 3
15
29
62 63
0
10
20
30
40
50
60
70
<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
9 9
16
37
54
63
0
10
20
30
40
50
60
70
<=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
 
Figure 3. Cumulative frequency of Table 5 
According to Figure 2 and Figure 3, document 
groups with kappa values above 0.4 are selected as 
91
the high agreement testing collection, that is, 
document groups with moderate agreement in Ta-
ble 3.  A total of 48 document groups are collected 
for opinion extraction and 47 document groups are 
collected for opinion polarity judgment. 
4.3 Substantial Consistency 
In Section 3.2, sentences which are ?strongly in-
consistent? are defined.  The substantial consis-
tency test collection expels strongly inconsistent 
sentences to achieve a higher consistency.  Notice 
that this test collection is still less consistent than 
the strict test collection, which is perfectly consis-
tent with annotators.  The lenient metric is applied 
for generating the gold standard for this collection. 
5 An Opinion System -- CopeOpi 
A Chinese opinion extraction system for opinion-
ated information, CopeOpi, is introduced here. (Ku 
et al, 2007)  When judging the opinion polarity of 
a sentence in this system, three factors are consid-
ered: sentiment words, negation operators and 
opinion holders. Every sentiment word has its own 
sentiment score.  If a sentence consists of more 
positive sentiments than negative sentiments, it 
must reveal something good, and vice versa. How-
ever, a negation operator, such as ?not? 
and ?never?, may totally change the sentiment po-
larity of a sentiment word. Therefore, when a nega-
tion operator appears together with a sentiment 
word, the opinion score of the sentiment word S 
will be changed to -S to keep the strength but re-
verse the polarity. Opinion holders are also consid-
ered for opinion sentences, but how they influence 
opinions has not been investigated yet. As a result, 
they are weighted equally at first. A word is con-
sidered an opinion holder of an opinion sentence if 
either one of the following two criteria is met:  
1. The part of speech is a person name, organi-
zation name or personal. 
2. The word is in class A (human), type Ae (job) 
of the Cilin Dictionary (Mei et al, 1982). 
6 Evaluation Results and Discussions 
Experiment results of CopeOpi using four designed 
testing collections are shown in Table 6.  Under the 
lenient metric with the lenient test collection, f-
measure scores 0.761 and 0.383 are achieved by 
CopeOpi.  The strict metric is the most severe, and 
the performance drops a lot under it.  Moreover, 
when using high agreement (H-A) and substantial 
consistency (S-C) test collections, the performance 
of the system does not increase in portion to the 
increase of agreement.  According to the agree-
ment of annotators, people should perform best in 
the strict collection, and both high agreement and 
substantial consistency testing collections are eas-
ier than the lenient one.  This phenomenon shows 
that though this system?s performance is satisfac-
tory, its behavior is not like human beings.  For a 
computer system, the lenient testing collection is 
fuzzier and contains more information for judg-
ment.  However, this also shows that the system 
may only take advantage of the surface informa-
tion.  If we want our systems really judge like hu-
man beings, we should enhance the performance 
on strict, high agreement, and substantial consis-
tency testing collections.  This analysis gives us, or 
other researchers who use this corpus for experi-
ments, a direction to improve their own systems.  
 
 Opinion Extraction Opinion + Polarity 
Measure P R F P R F 
Lenient 0.664 0.890 0.761 0.335 0.448 0.383
Strict 0.258 0.921 0.404 0.104 0.662 0.180
H-A 0.677 0.885 0.767 0.339 0.455 0.388
S-C    0.308 0.452 0.367
Table 6. Evaluation results 
Acknowledgments  
Research of this paper was partially supported by Excel-
lent Research Projects of National Taiwan University, 
under the contract 95R0062-AE00-02.  
References 
Mei, J., Zhu, Y. Gao, Y. and Yin, H.. tong2yi4ci2ci2lin2. 
Shanghai Dictionary Press, 1982.  
Pang, B., Lee, L., and Vaithyanathan, S. (2002). 
Thumbs up? Sentiment classification using machine 
learning techniques. Proceedings of the 2002 Confer-
ence on EMNLP, pages 79-86.  
 Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P., 
Fraser, B., Litman, D., Pierce, D., Riloff, E., and 
Wilson, T. (2002). NRRC summer workshop on 
multi-perspective question answering, final report. 
ARDA NRRC Summer 2002 Workshop.  
Ku, L.-W., Wu, T.-H., Li, L.-Y. and Chen., H.-H. 
(2007). Using Polarity Scores of Words for Sentence-
level Opinion Extraction. Proceedings of the Sixth 
NTCIR Workshop. 
92
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 97?102,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Demonstration of IlluMe: Creating Ambient 
According to Instant Message Logs 
 
Lun-Wei Ku Cheng-Wei Sun Ya-Hsin Hsueh 
National Yunlin University of Science and Technology 
123 University Road, Section 3 
Douliou, Yunlin 64002, Taiwan 
lwku@yuntech.edu.tw;chengwei.kenny.sun@gmail.com;hsuehyh@yuntech.edu.tw 
 
Abstract 
We present IlluMe, a software tool pack 
which creates a personalized ambient using 
the music and lighting. IlluMe includes an 
emotion analysis software, the small space 
ambient lighting, and a multimedia 
controller. The software analyzes 
emotional changes from instant message 
logs and corresponds the detected emotion 
to the best sound and light settings. The 
ambient lighting can sparkle with different 
forms of light and the smart phone can 
broadcast music respectively according to 
different atmosphere. All settings can be 
modified by the multimedia controller at 
any time and the new settings will be 
feedback to the emotion analysis software. 
The IlluMe system, equipped with the 
learning function, provides a link between 
residential situation and personal emotion. 
It works in a Chinese chatting environment 
to illustrate the language technology in life. 
1 Introduction 
Emotion analysis as well as recommendation 
technology has drawn a lot attention in the natural 
language processing research community. The 
development of fundamental approaches as well as 
applications has been proposed (Das, 2011; Sarwar 
et al, 2001; Zheng et al, 2010). However, most of 
them were Internet applications, and to the best 
knowledge of the authors, these technologies have 
not yet been involved in the ambient creation. To 
create an intelligent living space, some researchers 
utilized the facial expression and speech recognizer 
to detect emotions (Busso et al, 2004), but then 
the accompanied cameras and microphones were 
necessary. Some researchers tried to use sensors to 
watch the heart beat and the body temperature of 
residents to know their current emotion for further 
applications, but the problem was that users had to 
wear sensors and it was inconvenient. Instead of 
watching body signals, we postulate that the 
communications among people is one of the 
important factors to influence their emotions. 
Therefore, we tried to find clues from the textual 
conversations of the residents in order to detect 
their psychological state. 
There are many ways to categorize emotions. 
Different emotion states were used for experiments 
in previous research (Bellegarda, 2010). To find 
suitable categories of emotions, we adopted the 
three-layered emotion hierarchy proposed by 
Parrott (2001)1. Six emotions are in the first layer, 
including love, joy, surprise, anger, sadness and 
fear. The second layer includes 25 emotions, and 
the third layer includes 135 emotions. Using this 
hierarchical classification benefits the system. We 
can categorize emotions from rough to fine 
granularities and degrade to the upper level when 
the experimental materials are insufficient. How to 
map categories in other researches to ours becomes 
clearer, and annotators have more information 
when marking their current emotion.  
As to the music, most researchers looked for the 
emotions in songs or rhythms (Yang and Chen, 
2011; Zbikowski, 2011). They classified music 
into different emotional categories and developed 
the system to tell what emotion a song might bring 
to a listener. However, if the aim is to create a 
                                                          
1  http://changingminds.org/explanations/emotions/ 
basic%20emotions.htm 
97
comfortable ambient, what songs a person in a 
certain emotional state wants to listen to becomes 
the question. A happy user does not always enjoy 
happy songs, and vice versa. In this case, the 
technology developed in the previous work did not 
meet the new requirement.  
IlluMe was designed for a small space personal 
environment. We expect that users would like to 
use it because this system could interactively 
respond to their personal status to provide a feeling 
of the companion. We view the IlluMe system as a 
realization of detecting emotions from users? 
textual conversations and then recommending the 
best ambient accordingly. There are three major 
contributions in the development of the system. 
First, a corpus for ambient creation according to 
emotions was constructed. Second, IlluMe 
demonstrates a way to apply the state of the art 
technology of emotion analysis and 
recommendation to create an intelligent living 
space. Third, along with the developed technology, 
several further applications utilizing the 
components of IlluMe become feasible. 
2 System Description 
The potential working area for IlluMe is home or a 
small space. The system was designed to fit in with 
the modern people?s life style: programs are 
installed in users? personal computer and smart 
phone. The smart phone functions as the remote 
control and the music player, while all setting 
signals are sent out from the personal computer. 
The smart phone and the personal computer 
communicate through the wireless network. The 
only additional hardware requirement is the 
lighting set.  
2.1 System Features 
Emotion Detection Switch: The system detects 
users? current emotion according to messenger logs 
once a preset time period. It is ON/OFF switchable 
if users do not want the conversations to be 
recorded or utilized when determining the ambient. 
Auto Ambient Setting: The system sets the 
current ambient by a specific combination of a 
song and a light group which corresponds to the 
emotion or represents a special atmosphere. 
Manual Ambient Adjustment: IlluMe provides 
a friendly user interface to change the settings of 
music and lighting at any time. 
Personal Preference Learning: When users 
change the settings, the new ones are recorded. 
IlluMe learns the preference and then performs the 
user adaptation. After a period of time users will 
have their unique ambient creating system. 
Unlimited Melodies and Rich Light Colors: 
Users can add their songs in the smart phone for 
selection at any time. The learning process will 
help propose the new songs to create ambient later. 
Instant State Update: IlluMe watches the user 
input from messenger when the software is on. 
Therefore, it is able to change the music and 
lighting according to the detected emotion within a 
preset time period and users will feel like the 
environment is interacting with them. 
2.2 System Framework 
Figure 1 demonstrates the system framework of 
IlluMe. The system automatically watches the 
User Messages from messenger logs. The Emotion 
Analysis component detects the emotion of users, 
while the Ambient Learning Model determines the 
music and lighting accordingly, considering also 
the Personal Information of users.  
After the lights are on and the music is played, 
the user can change the settings they are not 
satisfying. A smart phone (Mobile Device) is used 
to change the settings, with two controllers on it: 
the Preference Controller and the Ambient 
Controller. The former takes the User Input for 
new settings, and then the music and lighting are 
changed by the latter. At the same time, the 
Preference Controller also sends the new settings 
to Ambient Learning Model to be recorded for user 
adaptation when creating the next ambient. 
The Emotion Analysis Component and Ambient 
Learning Model are two programs in a personal 
computer, and the Personal Info is saved in the 
personal computer, too. ANT wireless personal 
network protocol (Dynastream) is adopted to send 
the control signals to the Lighting. The LED 
lighting board is utilized to implement the Lighting 
of 65,536 colors. 
2.3 Operation Flowchart of User Interface 
The IlluMe system provides a user interface to 
change the settings by a smart phone (Mobile 
Device), functioning as a remote control. Users can 
select the location of music or the lighting, e.g. the 
living room or the bedroom, and the control mode, 
98
Mode 
i.e. manual or automatic. In the manual mode, 
users can set the color of a specific light; in the 
automatic mode, users select an emotional color set 
or a desired atmosphere for the lighting. Figure 2 
shows the operational flow of the user interface. 
2.4 Ambient Lighting and Music Playing 
To design the ambient lighting, one has to take 
LED array board, controlling mode and the light-
mixing effect of the lampshade into consideration. 
The LED lamp should sprinkle the LED 
components of red, cyan, green, white and orange 
lights equally onto the LED array board, so as to 
achieve uniform distribution. The controlling 
module distinguishes each lamp by its own code to 
modify the brightness of different colored LEDs 
within.  
 
Figure 1. System Framework of IlluMe 
As the LED lighting changes its color according 
to the controlling signals from the remote 
controller, the system transfer appropriate RF 
signals from the user?s personal computer to the 
ANT board, and then the ANT board controls the 
LED lighting board to change the color of lights. 
Music is broadcasted according to the detected 
emotional state. The broadcasting function and the 
controlling function are both realized by the 
software in the smart phone. Music is broadcasted 
directly through the phone, which conforms to the 
habits of modern people.  Figure 3 shows the 
illustration of the usage of IlluMe. 
Figure 2. Operation Flowchart 
 
Figure 3. Usage Illustration 
3 Emotion Analysis  
The emotion analysis that IlluMe performed is to 
find the emotions that texts in messenger logs bear 
in order to create a comfort ambient by sound and 
lighting accordingly. To achieve this, the system 
needs to understand the Internet language first, and 
then detect emotions and categorize them. The 
system works on the Chinese chatting environment 
and analyzes Chinese texts to detect emotions. The 
materials, approaches, and preliminary results in 
the development phase are described in this section. 
3.1 Experimental Materials 
Two dictionaries, the Chinese sentiment dictionary 
NTUSD (Ku and Chen, 2007) and the Chinese 
emotion dictionary (Lin et al, 2008), were adopted 
for detecting emotions. The former categorized 
sentiment words into positive and negative, while 
the latter into eight emotion types: awesome, 
heartwarming, surprising, sad, useful, happy, 
boring, and angry. Notice that these eight emotion 
Ambient Learning 
Model 
Lighting
ANT
 
 
 Mobile Device 
Music 
Settings 
User Messages 
Emotion Analysis Component 
Personal Info 
Preference 
Controller 
Ambient 
Controller 
User  Input 
MediaControl
Location
Auto
Play Music
Location
Manual 
Display Lighting Effect 
Bedroom Living Room
Lights
Color EmotionSetAtmosphere
99
types appeared in Yahoo! News Taiwan in the year 
2008 and not all of them were general emotion 
states. Therefore, we tried to find Lin?s emotion 
categories in Parrott?s emotion hierarchy before 
using this dictionary. Those could not be found 
were categorized in the Other class. 
Messenger logs were used as the source to detect 
emotions. We collected texts from Yahoo! 
Messenger and MSN Messenger logs of 8 
annotators. When the installed collecting program 
in their computers was on, it ran as a service and 
continuously logged their messages. Whenever 
there was at least one new message, once an hour 
the collecting program would pop up the menu and 
ask them to annotate the current emotion together 
with the preferred settings of the music and 
lighting. There were 3,290 songs, 15 emotional 
lighting colors and 6 atmospheres for selection. 
When selecting the settings of lighting, a full-
screen colored photo would be displayed to help 
annotators make their decisions. A total of 150 
records are annotated for experiments and statistics 
are shown in Table 1. 
 
Emo 1 2 3 4 5 6  
 11 80 1 15 39 4  
Color 1 2 3 4 5 6 7 
14 6 5 25 9 5 11 14 
8 9 10 11 12 13 14 15 
11 7 4 13 7 15 5 13 
Atm 1 2 3 4 5 6  
 28 40 16 33 17 16  
Table 1. Statistics of Annotated Materials  
(Emo: Emotions, 1=Love, 2=Joy, 3=Surprise, 4=Angry, 
5=Sad, 6=Fear; Color:15 color sets; Atm:6 atmospheres) 
3.2 Interpretation of Zhuyin Wen 
When processing Internet Chinese texts, IlluMe 
transformed messenger logs and sentiment 
dictionaries into zhuyin (Su, 2003) before looking 
for emotions2. There were many reasons to do this. 
Zhuyin Wen (???) is one of many creative uses 
of writing systems in the Internet language. As 
Blakeman (2004) found in his study of English, 
Internet language is fraught with initializations. 
However, as to the traditional Chinese, both 
Wikipedia and Zhang and Dai (2006) indicated 
that stylized initials and stylized numbers are 
                                                          
2 Lookup Table: http://cclookup.cctserver.com/ 
rarely used in Taiwan. Su reported that the most 
popular type of creative use of writing systems is 
?Zhuyin Wen? (???).  In ?Zhuyin Wen? the 
complete phonetic representation of a character is 
reduced to a consonant, or sometimes a vowel. 
This creative use appeared commonly in the 
collected conversations. Generally we had to figure 
out the missing vowels to understand the word, but 
in our system a reversed approach (dropping 
vowels) was adopted to make sure the system did 
not miss any possible match of dictionary terms 
observed in the conversations. 
When messenger users typed characters by their 
phonetics (consonants and vowels), very often they 
selected the wrong one from candidates of the 
same pronunciation, or they were just too lazy to 
select so the writing system chose the default 
candidate for them. In these cases, the system 
could not find a match because of wrong 
composite characters. Transforming characters in 
both dictionaries and conversations into their 
zhuyin representations before detecting emotions 
also help recover this kind of errors. 
3.3 Emotion Detection from Texts 
Section 3.2 shows how the system dealt with the 
error prone Internet texts and found the 
dictionaries terms. Ku and Chen?s (2007) approach 
for calculating sentiment scores was then adopted 
to give scores to these terms. The scores of terms 
of different emotional categories were summed up 
and the emotion category of the highest score was 
selected as the detected emotion. The Ambient 
Learning Model takes the detected emotion and 
selects the corresponding music and lighting by the 
Na?ve Bayes classifier trained by the annotated 
materials. 
3.4 Experiment and Preliminary Results  
Table 2 shows that using enhanced NTUSD (an 
augmented version of NTUSD) together with 
zhuyin transformation achieves the best results for 
emotion classification (positive/negative).  
Ku (2008) reported the set precision of their 
approach was 0.489 when texts were categorized 
into positive, neutral and negative. Though they 
had one additional neutral category, our system 
achieved the precision of 0.620 when processing 
the noisy Internet texts without word segmentation 
and part of speech tagging, which was satisfactory. 
100
Because IlluMe would always recommend a 
new or unchanged ambient setting, it would always 
find the closest emotion category of the user?s 
current emotion. In other words, the chatting 
content would always be connected to one of six 
emotion categories, so precision is the best metric 
to evaluate the performance of the system. The 
micro-average precision of the emotion detection 
was 0.207, while the macro-average precision was 
0.338. Bellegarda reported that his best f-measure 
was 0.340 also for 6 categories. Notice that the 
categories in Lin?s Chinese emotional dictionary 
were not identical to ours and hence we could not 
find terms for some categories in it. Therefore, 
though Bellegarda?s and our results were done on 
different datasets and evaluated by different 
metrics, considering our system suffered for the 
lack of terms in some categories and the 
ambiguous texts from the creative writing, the 
performance was considered acceptable. 
For the ambient recommendation, the micro-
average precision of selecting the settings of 
lighting according to the detected emotion was 
0.441 for 15 color sets and 0.461 for 6 atmospheres. 
 Positive Negative Total 
A 0.489 0.534 0.507 
B 0.902 0.155 0.613 
A+C 0.902 0.172 0.620 
Table 2. Precision of Emotion Detection 
(A: NTUSD; B: Enhanced NTUSD; C:Zhuyin 
transformation) 
3.5 Ambient Learning Function 
Because bringing up the settings to users is like a 
behavior of recommendation, we adopted the 
concept of collaborative filtering to design the 
function of the Ambient Learning Model. In the 
early stage of using IlluMe, it proposes the most 
frequently selected settings, that is, the choice of a 
group of people in the specific emotional state. If 
the user is connected to the Internet, the user 
experience will be transferred back to the servers 
to help recommend a better ambient to other users. 
The user experience optimization was feasible in 
this system because of the use of the smart phone, 
and this function was also implemented. As the 
users update the settings, the system knows their 
preference. In the later stage of using IlluMe, the 
Ambient Learning Model considers the preference 
of both the individual and the group to create a 
unique ambient for each user. 
4 Conclusion and Future Work  
Through the work we aim to apply the language 
technology to redefine the concept of a small house 
or working space. They should be a family-like 
existence which possesses the intellectual capacity 
to observe human behavior and emotion, and 
create consoling spaces according to the residents? 
different status. Therefore we implemented 
emotion analysis technique to equip a space with 
the ability to observe the status of its residents and 
interact with them accordingly. The instant interior 
lightings and music change can be viewed as a new 
form of ?conversation?. Residents can not only 
take the ambient provided by IlluMe, but can also 
give feedbacks. The concept of collaborative 
filtering was also implemented as we viewed the 
proposing of ambient as a kind of recommendation. 
Through the demonstration of the IlluMe system, 
we hope to show another way to apply language 
technology in life and retrieve the positive and 
relaxing atmosphere to rebuild our sense of trust 
and safety toward space, and finally recollect the 
long-lost attachment toward it. 
  We will continue collecting annotated 
materials and user feedbacks for learning, and 
make the materials a corpus for the research 
community. Facebook will be a source of text 
collection to gather more complete personal 
conversations for emotion detection. Making the 
IlluMe components real products like the home 
lighting system, the intelligent table lamp, or the 
music album promoter is also a future plan. 
5 Demonstration  
As demonstrating the IlluMe system by our 
original model house may be difficult in 
transportation and it may need a large space for 
demonstration, we will demonstrate the lightings 
by several table lamps, in which the LED lighting 
board resides. Other software will be performed on 
the smart phone and the personal computer. 
5.1 Demonstration Outline 
There are three purposes of the demonstration: first, 
to show how we apply the emotion analysis and 
recommendation technique in an ambient creating 
system; second, to illustrate actual and live 
101
operation of the system to the potential users; third, 
to show the annotation process of the experiment 
materials and the underlying algorithms for those 
interested in the technical details.  
Potential users might be interested in how the 
system will work if they have it in their personal 
computers and smart phones. Therefore, we 
demonstrate the whole IlluMe system with the 
actual music and lighting. Users can type Chinese 
words in messengers from the personal computer, 
and then the IlluMe system will change the music 
and lighting according to the proposed settings in a 
short time. The user can also control the music and 
lighting from the interface by the smart phone. 
In addition to demonstrating the functionality of 
the system, we will also provide accompanying 
visual aids that illustrate the underlying algorithms 
and the technical details. For example, zhuyin, 
terms found in the dictionaries, emotion scores, the 
detected emotion and the suggested settings.  
Acknowledgements 
Research of this paper was partially supported by 
National Science Council, Taiwan, under the 
contract NSC100-2218-E-224-013-. 
References  
Bellegarda, Jerome R. 2010. Emotion Analysis Using 
Latent Affective Folding and Embedding. 
Proceedings of the NAACL HLT 2010 Workshop on 
Computational Approaches to Analysis and 
Generation of Emotion in Text, Los Angeles, 1-9.  
Blakeman, Adam. 2004. An Investigation of the 
Language of Internet Chat Rooms. 
http://www.lancs.ac.uk/fss/courses/ling/ling201/res/d
issertations.html. 
Busso, Carlos, Deng, Zhigang, Yildirim, Serdar, Bulut, 
Murtaza, Lee, Chul Min, Kazemzadeh, Abe, Lee, 
Sungbok, Neumann, Ulrich and Narayanan, 
Shrikanth. 2004. Analysis of Emotion Recognition 
using Facial Expressions, Speech and Multimodal 
Information. Proceedings of ACM 6th International 
Conference on Mutlmodal Interfaces (ICMI 2004), 
State College, PA, Oct 2004 
Das, Dipankar, 2011. Analysis and Tracking of 
Emotions in English and Bengali Texts: A 
Computational Approach. Proceedings of the 
International World Wide Web Conference (WWW 
2011), Ph. D. Symposium. 343-347.  
Dynastream Innovations Inc., ANT AT3 RF Transceiver 
Chipset_Datasheet_Rev1.2, 
http://www.thisisant.com/. 
Ku, Lun-Wei and Chen, Hsin-Hsi. 2007. Mining 
Opinions from the Web: Beyond Relevance Retrieval. 
Journal of American Society for Information Science 
and Technology, Special Issue on Mining Web 
Resources for Enhancing Information Retrieval, 
58(12), 1838-1850. 
Ku, Lun-Wei, Liu, I-Chien, Lee, Chia-Ying, Chen, 
Kuan-hua. and Chen, Hsin-His. 2008. Sentence-
Level Opinion Analysis by CopeOpi in NTCIR-7.  
Proceedings of the 7th NTCIR Workshop Meeting, 
Tokyo, Japan. 260-267. 
Lin, Kevin Hsin-Yih, Yang, Changhua, and Chen, Hsin-
His. 2008. Emotion Classification of Online News 
Articles from the Reader?s Perspective. Proceedings 
of the 2008 IEEE/WIC/ACM International 
Conference on Web Intelligence. 220-226. 
Ortony, A. and Turner, T. J. 1990. What's basic about 
basic emotions? Psychological Review, 97, 315-331. 
Parrott, W. 2001. Emotions in Social Psychology, 
Psychology Press, Philadelphia. 
Sarwar, Badrul, Karypis, George, Konstan, Joseph, and 
Riedl, John. 2001. ItemBased Collaborative Filtering 
Recommendation Algorithms. Proceedings of the 
International World Wide Web Conference (WWW 
2001), 285-295. 
Su, Hsi-Yao. 2003. The Multilingual and Multi-
Orthographic Taiwan-Based Internet: Creative Uses 
of Writing Systems on College-Affiliated BBSs. 
Journal of Computer-Mediated Communication 9(1). 
http://jcmc.indiana.edu/vol9/issue1/su.html. 
Yang, Yi-Hsuan and Chen, Homer H., Fellow, IEEE. 
2011. Ranking-Based Emotion Recognition for 
Music Organization and Retrieval. IEEE 
Transactions on audio, speech, and language 
processing, 19(4).  
Zbikowski, Lawrence M., 2011. Music, Emotion, 
Analysis. Music Analysis, Blackwell Publishing Ltd, 
Oxford, UK. 
Zhang, Jiawei and Dai, Jiaxing. 2006. Qiantan shixia 
qingnian wangluo yongyu ?huoxing wen ????
? ? ? ? ? ? ? ? ? ? ? 
http://www.shs.edu.tw/works/essay/2006/03/2006032
816043532.pdf 
Zheng, Vincent W., Cao, Bin, Zheng, Yu, Xie, Xing and 
Yang, Qiang. 2010. Collaborative Filtering Meets 
Mobile Recommendation: A User-centered Approach 
Proceedings of Twenty-Fourth National Conference 
on Artificial Intelligence (AAAI-10).  
102
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Cross-Lingual Information to the Rescue in Keyword Extraction 
 
1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang 
1,2,3Language Technologies Institute, CMU, United States 
4Institute of Information Science, Academia Sinica, Taiwan 
5Institute for Information Industry, Taipei, Taiwan 
{1u901571,4lunwei.jennifer.ku}@gmail.com 
{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.tw 
 
  
  
Abstract 
We introduce a method that extracts keywords 
in a language with the help of the other. In our 
approach, we bridge and fuse conventionally 
irrelevant word statistics in languages. The 
method involves estimating preferences for 
keywords w.r.t. domain topics and generating 
cross-lingual bridges for word statistics 
integration. At run-time, we transform parallel 
articles into word graphs, build cross-lingual 
edges, and exploit PageRank with word 
keyness information for keyword extraction. 
We present the system, BiKEA, that applies 
the method to keyword analysis. Experiments 
show that keyword extraction benefits from 
PageRank, globally learned keyword 
preferences, and cross-lingual word statistics 
interaction which respects language diversity. 
1 Introduction 
Recently, an increasing number of Web services 
target extracting keywords in articles for content 
understanding, event tracking, or opinion mining. 
Existing keyword extraction algorithm (KEA) 
typically looks at articles monolingually and 
calculate word significance in certain language. 
However, the calculation in another language 
may tell the story differently since languages 
differ in grammar, phrase structure, and word 
usage, thus word statistics on keyword analysis. 
Consider the English article in Figure 1. Based 
on the English content alone, monolingual KEA 
may not derive the best keyword set. A better set 
might be obtained by referring to the article and 
its counterpart in another language (e.g., 
Chinese). Different word statistics in articles of 
different languages may help, due to language 
divergence such as phrasal structure (i.e., word 
order) and word usage and repetition (resulting 
from word translation or word sense) and so on. 
For example, bilingual phrases ?social 
reintegration? and ?????? in Figure 1 have 
inverse word orders (?social? translates into ??
? ? and ?reintegration? into ? ? ? ?), both 
?prosthesis? and ?artificial limbs? translate into 
????, and ?physical? can be associated with ??
? ? and ??? ? in ?physical therapist? and 
?physical rehabilitation? respectively. Intuitively, 
using cross-lingual statistics (implicitly 
leveraging language divergence) can help look at 
articles from different perspectives and extract 
keywords more accurately. 
We present a system, BiKEA, that learns to 
identify keywords in a language with the help of 
the other. The cross-language information is 
expected to reinforce language similarities and 
value language dissimilarities, and better 
understand articles in terms of keywords. An 
example keyword analysis of an English article 
is shown in Figure 1. BiKEA has aligned the 
parallel articles at word level and determined the 
scores of topical keyword preferences for words. 
BiKEA learns these topic-related scores during 
training by analyzing a collection of articles. We 
will describe the BiKEA training process in more 
detail in Section 3. 
At run-time, BiKEA transforms an article in a 
language (e.g., English) into PageRank word 
graph where vertices are words in the article and 
edges between vertices indicate the words? co-
occurrences. To hear another side of the story, 
BiKEA also constructs graph from its counterpart 
in another language (e.g., Chinese). These two 
independent graphs are then bridged over nodes 
1
  
 
 
 
 
 
 
 
 
 
Figure 1. An example BiKEA keyword analysis for an article.
that are bilingually equivalent or aligned. The 
bridging is to take language divergence into 
account and to allow for language-wise 
interaction over word statistics. BiKEA, then in 
bilingual context, iterates with learned word 
keyness scores to find keywords. In our 
prototype, BiKEA returns keyword candidates of 
the article for keyword evaluation (see Figure 1); 
alternatively, the keywords returned by BiKEA 
can be used as candidates for social tagging the 
article or used as input to an article 
recommendation system. 
2 Related Work 
Keyword extraction has been an area of active 
research and applied to NLP tasks such as 
document categorization (Manning and Schutze, 
2000), indexing (Li et al., 2004), and text mining 
on social networking services ((Li et al., 2010); 
(Zhao et al., 2011); (Wu et al., 2010)). 
The body of KEA focuses on learning word 
statistics in document collection. Approaches 
such as tfidf and entropy, using local document 
and/or across-document information, pose strong 
baselines. On the other hand, Mihalcea and 
Tarau (2004) apply PageRank, connecting words 
locally, to extract essential words. In our work, 
we leverage globally learned keyword 
preferences in PageRank to identify keywords. 
Recent work has been done on incorporating 
semantics into PageRank. For example, Liu et al. 
(2010) construct PageRank synonym graph to 
accommodate words with similar meaning. And 
Huang and Ku (2013) weigh PageRank edges 
based on nodes? degrees of reference. In contrast, 
we bridge PageRank graphs of parallel articles to 
facilitate statistics re-distribution or interaction 
between the involved languages. 
In studies more closely related to our work, 
Liu et al. (2010) and Zhao et al. (2011) present 
PageRank algorithms leveraging article topic 
information for keyword identification. The main 
differences from our current work are that the 
article topics we exploit are specified by humans 
not by automated systems, and that our 
PageRank graphs are built and connected 
bilingually. 
In contrast to the previous research in keyword 
extraction, we present a system that 
automatically learns topical keyword preferences 
and constructs and inter-connects PageRank 
graphs in bilingual context, expected to yield 
better and more accurate keyword lists for 
articles. To the best of our knowledge, we are the 
first to exploit cross-lingual information and take 
advantage of language divergence in keyword 
extraction. 
3 The BiKEA System 
Submitting natural language articles to keyword 
extraction systems may not work very well. 
Keyword extractors typically look at articles 
from monolingual points of view. Unfortunately, 
word statistics derived based on a language may 
The English Article: 
I've been in Afghanistan for 21 years. I work for the Red Cross and I'm a physical therapist. My job is to 
make arms and legs -- well it's not completely true. We do more than that. We provide the patients, the 
Afghan disabled, first with the physical rehabilitation then with the social reintegration. It's a very logical 
plan, but it was not always like this. For many years, we were just providing them with artificial limbs. It 
took quite many years for the program to become what it is now. ? 
 
Its Chinese Counterpart: ??????? 21 ?? ????????? ?????????? ??????????? -- ?????????? ?????????? ???????? ???????? ???????, ??????? ???????????? ?????????? ??????????? ????? ???????? ?????????????? 
 
Word Alignment Information: 
physical (??), therapist (???), social (??), reintegration (??), physical (??), rehabilitation  (??), prosthesis (??), ? 
 
Scores of Topical Keyword Preferences for Words: 
(English)    prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ? 
(Chinese)   ??: 0.41; ?????: 0.15; ??:0.10; ???: 0.08, ? 
 
English Keywords from Bilingual Perspectives: 
prosthesis, artificial, leg, rehabilitation, orthopedic, ? 
2
be biased due to the language?s grammar, phrase 
structure, word usage and repetition and so on. 
To identify keyword lists from natural language 
articles, a promising approach is to automatically 
bridge the original monolingual framework with 
bilingual parallel information expected to respect 
language similarities and diversities at the same 
time.  
3.1 Problem Statement 
We focus on the first step of the article 
recommendation process: identifying a set of 
words likely to be essential to a given article. 
These keyword candidates are then returned as 
the output of the system. The returned keyword 
list can be examined by human users directly, or 
passed on to article recommendation systems for 
article retrieval (in terms of the extracted 
keywords). Thus, it is crucial that keywords be 
present in the candidate list and that the list not 
be too large to overwhelm users or the 
subsequent (typically computationally expensive) 
article recommendation systems. Therefore, our 
goal is to return reasonable-sized set of keyword 
candidates that, at the same time, must contain 
essential terms in the article. We now formally 
state the problem that we are addressing. 
Problem Statement: We are given a bilingual 
parallel article collection of various topics from 
social media (e.g., TED), an article ARTe in 
language e, and its counterpart ARTc in language 
c. Our goal is to determine a set of words that are 
likely to contain important words of ARTe. For 
this, we bridge language-specific statistics of 
ARTe and ARTc via bilingual information (e.g., 
word alignments) and consider word keyness 
w.r.t. ARTe?s topic such that cross-lingual 
diversities are valued in extracting keywords in e. 
In the rest of this section, we describe our 
solution to this problem. First, we define 
strategies for estimating keyword preferences for 
words under different article topics (Section 3.2). 
These strategies rely on a set of article-topic 
pairs collected from the Web (Section 4.1), and 
are monolingual, language-dependent 
estimations. Finally, we show how BiKEA 
generates keyword lists for articles leveraging 
PageRank algorithm with word keyness and 
cross-lingual information (Section 3.3). 
3.2 Topical Keyword Preferences 
We attempt to estimate keyword preferences 
with respect to a wide range of article topics. 
Basically, the estimation is to calculate word 
significance in a domain topic. Our learning 
process is shown in Figure 2. 
 
 
 
 
 
 
Figure 2. Outline of the process used 
to train BiKEA. 
In the first two stages of the learning process, we 
generate two sets of article and word information. 
The input to these stages is a set of articles and 
their domain topics. The output is a set of pairs 
of article ID and word in the article, e.g., 
(ARTe=1, we=?prosthesis?) in language e or 
(ARTc=1, wc=????) in language c, and a set of 
pairs of article topic and word in the article, e.g., 
(tpe=?disability?, we=?prosthesis?) in e and 
(tpe=?disability?, wc=????) in c. Note that the 
topic information is shared between the involved 
languages, and that we confine the calculation of 
such word statistics in their specific language to 
respect language diversities and the language-
specific word statistics will later interact in 
PageRank at run-time (See Section 3.3). 
The third stage estimates keyword preferences 
for words across articles and domain topics using 
aforementioned (ART,w) and (tp,w) sets. In our 
paper, two popular estimation strategies in 
Information Retrieval are explored. They are as 
follows. 
tfidf. tfidf(w)=freq(ART,w)/appr(ART?,w) where 
term frequency in an article is divided by its 
appearance in the article collection to distinguish 
important words from common words. 
ent. entropy(w)= -?
tp?
Pr(tp?|w)?log(Pr(tp?|w)) 
where  a word?s uncertainty in topics is used to 
estimate its associations with domain topics. 
These strategies take global information (i.e., 
article collection) into account, and will be used 
as keyword preference models, bilingually 
intertwined, in PageRank at run-time which 
locally connects words (i.e., within articles). 
3.3 Run-Time Keyword Extraction 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
(1) Generate article-word pairs in training data 
(2) Generate topic-word pairs in training data 
(3) Estimate keyword preferences for words w.r.t.  
      article topic based on various strategies 
(4) Output word-and-keyword-preference-score  
      pairs for various strategies 
3
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Extracting keywords at run-time. 
 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible.  
In Steps (1) and (2) we construct PageRank 
word graphs for the article ARTe in language e 
and its counterpart ARTc in language c. They are 
built individually to respect language properties 
(such as subject-verb-object or subject-object-
verb structure). Figure 4 shows the algorithm. In 
this algorithm, EW stores normalized edge 
weights for word wi and wj (Step (2)). And EW 
is a v by v matrix where v is the vocabulary size 
of ARTe and ARTc. Note that the graph is directed 
(from words to words that follow) and edge 
weights are words? co-occurrences within 
window size WS. Additionally we incorporate 
edge weight multiplier m>1 to propagate more 
PageRank scores to content words, with the 
intuition that content words are more likely to be 
keywords (Step (2)). 
 
 
 
 
 
 
 
Figure 4. Constructing PageRank word graph. 
Step (3) in Figure 3 linearly combines word 
graphs EWe and EWc using ?. We use ? to 
balance language properties or statistics, and 
BiKEA backs off to monolingual KEA if ? is one. 
In Step (4) of Figure 3 for each word 
alignment (wic, wje), we construct a link between 
the word nodes with the weight BiWeight. The 
inter-language link is to reinforce language 
similarities and respect language divergence 
while the weight aims to elevate the cross-
language statistics interaction. Word alignments 
are derived using IBM models 1-5 (Och and Ney, 
2003). The inter-language link is directed from 
wi
c
 to wj
e
, basically from language c to e based on 
the directional word-aligning entry (wic, wje). The 
bridging is expected to help keyword extraction 
in language e with the statistics in language c. 
Although alternative approach can be used for 
bridging, our approach is intuitive, and most 
importantly in compliance with the directional 
spirit of PageRank. 
Step (6) sets KP of keyword preference model 
using topical preference scores learned from 
Section 3.2, while Step (7) initializes KN of 
PageRank scores or, in our case, word keyness 
scores. Then we distribute keyness scores until 
the number of iteration or the average score 
differences of two consecutive iterations reach 
their respective limits. In each iteration, a word?s 
keyness score is the linear combination of its 
keyword preference score and the sum of the 
propagation of its inbound words? previous 
PageRank scores. For the word wje in ARTe, any 
edge (wie,wje) in ARTe, and any edge (wkc,wje) in 
WA, its new PageRank score is computed as 
below. 
procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,?,N) 
//Construct language-specific word graph for PageRank 
(1)  EWe=constructPRwordGraph(ARTe) 
(2)  EWc=constructPRwordGraph(ARTc) 
//Construct inter-language bridges 
(3)  EW=?? EWe+(1-?) ? EWc 
       for each word alignment (wic, wje) in WA 
         if IsContWord(wic) and IsContWord(wje) 
(4a)      EW[i,j]+=1? BiWeightcont 
         else 
(4b)      EW[i,j]+=1? BiWeightnoncont 
(5)  normalize each row of EW to sum to 1 
//Iterate for PageRank 
(6)  set KP1 ?v to 
             [KeyPrefs(w1), KeyPrefs(w2), ?,KeyPrefs(wv)] 
(7)  initialize KN1 ?v to [1/v,1/ v, ?,1/v] 
       repeat 
(8a)  KN?=?? KN? EW+(1-?) ? KP 
(8b)  normalize KN? to sum to 1 
(8c)  update KN with KN? after the check of KN and KN? 
       until maxIter or avgDifference(KN,KN?) ? smallDiff 
(9)  rankedKeywords=Sort words in decreasing order of KN 
       return the N rankedKeywords in e with highest 
scores 
procedure constructPRwordGraph(ART) 
(1) EWv ?v=0v ?v 
      for each sentence st in ART 
         for each word wi in st 
            for each word wj in st where i<j and j-i ? WS 
         if not IsContWord(wi) and IsContWord(wj) 
(2a)            EW[i,j]+=1? m 
               elif not IsContWord(wi) and not IsContWord(wj) 
(2b)            EW[i,j]+=1 ? (1/m) 
               elif IsContWord(wi) and not IsContWord(wj) 
(2c)            EW[i,j]+=1? (1/m) 
               elif IsContWord(wi) and IsContWord(wj) 
(2d)            EW[i,j]+=1 ? m 
       return EW 
4
???[1, ?] =? ?
??
? ? ????[1, ?] ? ???[?, ?] +???(1 ? ?) ????[1, ?] ? ??[?, ?]??? ??
?
+ (1 ??) ? ??[1, ?] 
 
Once the iterative process stops, we rank 
words according to their final keyness scores and 
return top N ranked words in language e as 
keyword candidates of the given article ARTe. An 
example keyword analysis for an English article 
on our working prototype is shown in Figure 1. 
Note that language similarities and dissimilarities 
lead to different word statistics in articles of 
difference languages, and combining such word 
statistics helps to generate more promising 
keyword lists. 
4 Experiments 
BiKEA was designed to identify words of 
importance in an article that are likely to cover 
the keywords of the article. As such, BiKEA will 
be trained and evaluated over articles. 
Furthermore, since the goal of BiKEA is to 
determine a good (representative) set of 
keywords with the help of cross-lingual 
information, we evaluate BiKEA on bilingual 
parallel articles. In this section, we first present 
the data sets for training BiKEA (Section 4.1). 
Then, Section 4.2 reports the experimental 
results under different system settings. 
4.1 Data Sets 
We collected approximately 1,500 English 
transcripts (3.8M word tokens and 63K word 
types) along with their Chinese counterparts 
(3.4M and 73K) from TED (www.ted.com) for 
our experiments. The GENIA tagger (Tsuruoka 
and Tsujii, 2005) was used to lemmatize and 
part-of-speech tag the English transcripts while 
the CKIP segmenter (Ma and Chen, 2003) 
segment the Chinese. 
30 parallel articles were randomly chosen and 
manually annotated for keywords on the English 
side to examine the effectiveness of BiKEA in 
English keyword extraction with the help of 
Chinese. 
4.2 Experimental Results 
Table 1 summarizes the performance of the 
baseline tfidf and our best systems on the test set. 
The evaluation metrics are nDCG (Jarvelin and 
Kekalainen, 2002), precision, and mean 
reciprocal rank. 
(a) @N=5 nDCG P MRR 
tfidf .509 .213 .469 
PR+tfidf .676 .400 .621 
BiKEA+tfidf .703 .406 .655 
 
(b) @N=7 nDCG P MRR 
tfidf .517 .180 .475 
PR+tfidf .688 .323 .626 
BiKEA+tfidf .720 .338 .660 
 
(c) @N=10 nDCG P MRR 
tfidf .527 .133 .479 
PR+tfidf .686 .273 .626 
BiKEA+tfidf .717 .304 .663 
Table 1. System performance at 
(a) N=5 (b) N=7 (c) N=10. 
As we can see, monolingual PageRank (i.e., 
PR) and bilingual PageRank (BiKEA), using 
global information tfidf, outperform tfidf. They 
relatively boost nDCG by 32% and P by 87%. 
The MRR scores also indicate their superiority: 
their top-two candidates are often keywords vs. 
the 2nd place candidates from tfidf. 
Encouragingly, BiKEA+tfidf achieves better 
performance than the strong monolingual 
PR+tfidf across N?s. Specifically, it further 
improves nDCG relatively by 4.6% and MRR 
relatively by 5.4%. 
Overall, the topical keyword preferences, and 
the inter-language bridging and the bilingual 
score propagation in PageRank are simple yet 
effective. And respecting language statistics and 
properties helps keyword extraction. 
5 Summary 
We have introduced a method for extracting 
keywords in bilingual context. The method 
involves estimating keyword preferences, word-
aligning parallel articles, and bridging language-
specific word statistics using PageRank. 
Evaluation has shown that the method can 
identify more keywords and rank them higher in 
the candidate list than monolingual KEAs. As for 
future work, we would like to explore the 
possibility of incorporating the articles? reader 
feedback into keyword extraction. We would 
also like to examine the proposed methodology 
in a multi-lingual setting.  
5
Acknowledgement 
This study is conducted under the ?Online and 
Offline integrated Smart Commerce Platform 
(1/4)? of the Institute for Information Industry 
which is subsidized by the Ministry of Economy 
Affairs of the Republic of China. 
References  
Scott A. Golder and Bernardo A. Huberman. 
2006. Usage patterns of collaborative tagging 
systems. Information Science, 32(2): 198-208. 
Harry Halpin, Valentin Robu, and Hana 
Shepherd. 2007. The complex dynamics of 
collaborative tagging. In Proceedings of the 
WWW, pages 211-220. 
Chung-chi Huang and Lun-wei Ku. 2013. 
Interest analysis using semantic PageRank and 
social interaction content. In Proceedings of 
the ICDM Workshop on Sentiment Elicitation 
from Natural Text for Information Retrieval 
and Extraction, pages 929-936. 
Kalervo Jarvelin and Jaana Kekalainen. 2002. 
Cumulated gain-based evaluation of IR 
technologies. ACM Transactions on 
Information Systems, 20(4): 422-446. 
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based 
translation. In Proceedings of the North 
American Chapter of the Association for 
Computational Linguistics, pages 48-54. 
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin 
Chen. 2004. Incorporating document 
keyphrases in search results. In Proceedings of 
the Americas Conference on Information 
Systems. 
Zhenhui Li, Ging Zhou, Yun-Fang Juan, and 
Jiawei Han. 2010. Keyword extraction for 
social snippets. In Proceedings of the WWW, 
pages 1143-1144. 
Marina Litvak and Mark Last. 2008. Graph-
based keyword extraction for single-document 
summarization. In Proceedings of the ACL 
Workshop on Multi-Source Multilingual 
Information Extraction and Summarization, 
pages 17-24. 
Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong 
Wang. 2010. Keyword extraction using 
PageRank on synonym networks. In 
Proceedings of the ICEEE, pages 1-4. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and 
Maosong Sun. 2010. Automatic keyphrase 
extraction via topic decomposition. In 
Proceedings of the EMNLP, pages 366-376. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. 
Introduction to CKIP Chinese word 
segmentation system for the first international 
Chinese word segmentation bakeoff. In 
Proceedings of the ACL Workshop on Chinese 
Language Processing. 
Chris D. Manning and Hinrich Schutze. 2000. 
Foundations of statistical natural language 
processing. MIT Press. 
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing orders into texts. In Proceedings of 
the EMNLP, pages 404-411. 
Franz Josef Och and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational Linguistics, 
29(1): 19-51. 
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. 
Bidirectional inference with the easiest-first 
strategy for tagging sequence data. In 
Proceedings of the EMNLP, pages 467-474. 
Peter D. Turney. 2000. Learning algorithms for 
keyphrase extraction. Information Retrieval, 
2(4): 303-336. 
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. 
Automatic generation of personalized 
annotation tags for Twitter users. In 
Proceedings of the NAACL, pages 689-692. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang 
Song, Palakorn Achananuparp, Ee-Peng Lim, 
and Xiaoming Li. 2011. Topical keyword 
extraction from Twitter. In Proceedings of the 
ACL, pages 379-388. 
 
6
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 28?37,
Dublin, Ireland, August 24 2014.
A Rule-Based Approach to Aspect Extraction from Product Reviews
Soujanya Poria
Dept of Computing Science & Maths
University of Stirling
soujanya.poria@cs.stir.ac.uk
Erik Cambria
School of Computer Engineering
Nanyang Technological University
cambria@ntu.edu.sg
Lun-Wei Ku
Institute of Information Science
Academia Sinica
lwku@iis.sinica.edu.tw
Chen Gui
SenticNet
chen@sentic.net
Alexander Gelbukh
Center for Computing Research
National Polytechnic Institute
gelbukh@cic.ipn.mx
Abstract
Sentiment analysis is a rapidly growing research field that has attracted both academia and in-
dustry because of the challenging research problems it poses and the potential benefits it can
provide in many real life applications. Aspect-based opinion mining, in particular, is one of the
fundamental challenges within this research field. In this work, we aim to solve the problem of
aspect extraction from product reviews by proposing a novel rule-based approach that exploits
common-sense knowledge and sentence dependency trees to detect both explicit and implicit as-
pects. Two popular review datasets were used for evaluating the system against state-of-the-art
aspect extraction techniques, obtaining higher detection accuracy for both datasets.
1 Introduction
In opinion mining, different levels of granularity analysis have been proposed, each one having its own
advantages and disadvantages. Aspect-based opinion mining (Hu and Liu, 2004; Ding et al., 2008)
focuses on the extraction of aspects (or product features) from opinionated text and on the inference of
polarity values associated with these. For example, a sentence like ?I love the touchscreen of my phone
but the battery life is so short? contains two aspects or opinion targets, namely touchscreen and battery
life. In this case, applying a sentence level polarity detection technique would mistakenly result in a
polarity value close to neutral, since the two opinions expressed by the users are opposite. Hence, aspect
extraction is necessary to first deconstruct sentences into product features and then assign a separate
polarity value to each of these features.
There are two types of aspects defined in aspect-based opinion mining: explicit and implicit. Explicit
aspects are concepts that explicitly denote targets in the opinionated sentence. For instance, in the above
example, touchscreen and battery life are explicit aspects as they are explicitly mentioned in the sentence.
On the other hand, an aspect can also be expressed indirectly through an implicit aspect clue (IAC), e.g.,
in the sentence ?This camera is sleek and very affordable?, which implicitly provides a positive opinion
about the aspects appearance and price of the entity camera.
Explicit aspect extraction has been widely researched and there exists several approaches for this
task. Still, limited work has been done in extracting implicit aspects. This task is very difficult yet very
important because the phenomenon of implicit aspects is present in nearly every opinionated document.
For example, the following document extracted from the corpus (Hu and Liu, 2004) uses only implicit
aspects:
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0
28
This is the best phone one could have. It has all the features one would need in a cellphone: It
is lightweight, sleek and attractive. I found it very user-friendly and easy to manipulate; very
convenient to scroll in menu etc.
Here, the word ?lightweight? refers to the weight of the phone; the words ?sleek? and ?attractive? to
its appearance; the compound ?user-friendly? to its interface; the phrase ?easy to manipulate? to its
functionality; finally, the phrase ?to scroll in menu? can be interpreted as a reference to the interface
of the phone or its menu. Even though the aspects appearance, weight and interface do not appear
in the sentence, the context contains clues that permit us to infer them. Namely, the words ?sleek,?
?lightweight,? and ?user-friendly? that do occur in the context suggest these aspects.
In contrast to the task of identification of explicit aspects, the general scheme for identification of
implicit aspects, a task called implicit aspect extraction, typically involves two steps:
1. Identify IACs (e.g., ?sleek?) in the opinionated document.
2. Map them to the corresponding aspects (e.g., appearance).
In this paper, we propose a novel approach to detect explicit aspects and IACs from opinionated
documents. We also map IACs to their respective aspect categories. IACs are either single words,
such as ?sleek,? or multi-word expressions, such as ?easy to manipulate? as in the above example. Each
IAC can be represented by a different part-of-speech (POS): in the example ?This MP3 player is really
expensive,? the IAC ?expensive? suggesting the aspect price is an adjective; in ?This camera looks
great,? the IAC ?look? suggesting appearance is a verb; in ?I hate this phone. It only lasted less than six
months!?, the IAC ?lasted? suggesting durability of the phone is a verb. In the following examples, IACs
are nouns or noun phrases: ?Even if I had paid full price I would have considered this phone a good deal,?
?Not to mention the sleekness of this phone?, ?The player keeps giving random errors?, ?This phone is a
piece of crap.?
In different contexts, the same implicit aspect can be implied by different IACs, as shown below for
the implicit aspect price:
? This mp3 player is very affordable.
? This mp3 player also costs a lot less than the ipod.
? This mp3 player is quite cheap.
? This mp3 is inexpensive.
? I bought this mp3 for almost nothing!
? This mp3 player has been fairly innovative and reasonably priced.
A common approach for IAC identification is to assume that sentiments or polarity words are good
candidates for IACs: for example, in ?This MP3 player is really expensive,? the word ?expensive?,
which bears negative polarity, is also the IAC for the aspect price. However, this is not always true.
For example, in ?This camera looks great,? the word ?looks? implies the appearance of the phone,
while polarity is conveyed through the word ?great.? In ?I hate this phone. It only lasted less than six
months!?, the word ?lasted? is the IAC for durability of the phone, while polarity is indicated by ?hate.?
Furthermore, the second sentence of this example could appear without the first one: ?This phone only
lasted less than six months? and still constitute a negative opinion of the phone?s durability, but not
expressed by any specific word.
This phenomenon is known in opinion mining as desirable fact: communicating fact that by common-
sense are good or bad, which indirectly implies polarity. For example, the objective fact ?The camera can
hold lots of pictures? does not contain any sentiment or polarity word yet gives a positive opinion about
the camera?s memory capacity (IAC ?hold?), because it is desirable for a camera to hold many pictures.
29
In this paper, we present a rule-based approach that exploits common-sense knowledge and sentence
dependency trees to detect both implicit and explicit aspects. In particular, the approach draws lessons
from recent developments in common-sense reasoning (Cambria et al., 2011; Cambria et al., 2014a)
and concept-level sentiment analysis (Xia et al., 2013; Poria et al., 2014) to first obtain the dependency
structure of each sentence and, hence, exploit external knowledge to extract aspects and infer the polarity
associated with them. The paper is organized as follows: Section 2 presents the literature in aspect ex-
traction; Section 3 explains the features used for the labeler; Section 4 discusses novelty of the proposed
methodology; Section 5 describes in detail the aspect extraction approach and results of the experimental
evaluation; finally, Section 6 concludes the paper.
2 Related Work
Aspect extraction from opinionated text was first studied by Hu and Liu (Hu and Liu, 2004), who also
introduced the distinction between explicit and implicit aspects. However, the authors only dealt with
explicit aspects by adopting a set of rules based on statistical observations. Hu and Liu?s method was im-
proved by Popescu and Etzioni (Popescu and Etzioni, 2005) and by Blair-Goldensonh (Blair-Goldensohn
et al., 2008). Popescu and Etzioni assumed the product class to be known as priori. Their algorithm
detects whether a noun or noun phrase is a product feature or not by computing PMI between the noun
phrase and the product class. Scaffidi et al. (Scaffidi et al., 2007) presented a method that uses a language
model to identify product features. They assumed that product features are more frequent in product re-
views than in general natural language text. However, their method seems to be very inaccurate in terms
of precision as the retrieved aspects extracted by their method were very noisy.
Aspect extraction can be seen as a general information extraction problem, for which techniques based
on sequential labeling are generally used. The most popular methods in this context, in particular, are
Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Jin and
Ho (Jin and Ho, 2009) used a lexicalized HMM for joint extraction of opinions along with their explicit
aspects. Niklas and Gurevych (Niklas and Gurevych, 2010) used CRF to extract explicit aspects in a
custom corpus with data of different domains. Li et al. (Li et al., 2010), Choi and Cardie (Choi and
Cardie, 2010) and Huang et al. (Huang et al., 2012) also used CRF for extraction of explicit aspects.
As to the implicit aspects, the OPINE extraction system developed by Popescu and Etzioni (Popescu
and Etzioni, 2005) was the first that leveraged on the extraction of this type of aspects to improve polarity
classification. However, their system is not described in detail and is not publicly available. To the
best of our knowledge, all existing methods for implicit aspect extraction are based on the use, in one
or another way, of what we term IAC. Su (Su et al., 2008) proposed a clustering method to map IACs
(which were assumed to be sentiment words) to their corresponding explicit aspects. The method exploits
the mutual reinforcement relationship between an explicit aspect and a sentiment word forming a co-
occurring pair in a sentence. Hai (Zhen et al., 2011) proposed a two-phase co-occurrence association
rule mining approach to match implicit aspects (which were also assumed to be sentiment words) with
explicit aspects. Finally, Zeng and Li (Zeng and Li, 2013) proposed a rule-based method to extract
explicit aspects and mapped implicit features by using a set of sentiment words and by clustering explicit
feature-word pairs.
3 Method
3.1 Corpus for aspect extraction
In order to evaluate the explicit aspect extraction algorithm, we use the corpus provided by (Hu and
Liu, 2004) and the Semeval 2014 dataset
1
(Table 1). As for the implicit aspect extraction algorithm and
lexicon, we use the corpus developed by Cruz-Garcia et al. (Cruz-Garcia et al., 2014), who manually
labeled each IAC and their corresponding aspects in a well-known corpus for opinion mining (Hu and
Liu, 2004). The corpus is publicly available for research purposes.
2
1
http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools
2
Available from www.gelbukh.com/resources/implicit-aspect-extraction-corpus, visited on
March 19, 2014.
30
Table 1: Description of Semeval 2014 dataset
Sentences Containing n aspect terms
Domain Name n = 0 n ? 1 n ? 2 total(n ? 0)
Restaurants 1,732 2,212 881 3,944
Laptops 1,883 2,065 456 3,948
3.2 Pre-Processing
Pre-processing is a key step for aspect parsing. The pre-processing module of the proposed framework
consists of two major steps: firstly, the sentence dependency tree is obtained through Stanford Depen-
dency Parser
3
; secondly, dependency structure elements are processed by means of Stanford Lemmatizer
for each sentence. It is important to build the dependency tree before lemmatization as swapping the two
steps results in several imprecisions caused by the lower grammatical accuracy of lemmatized sentences.
3.3 Aspect Parser
3.3.1 Implicit aspect lexicon
We use the implicit aspect corpus developed by Cruz-Garcia et al. (Cruz-Garcia et al., 2014), where
IACs are indicated and manually labeled by their corresponding aspect categories. For our task, we
extracted the sentences having implicit aspects and then extracted IACs for each of them, along with
their corresponding labeled categories. For example, in ?The car is expensive? the IAC is expensive and
it is labeled by the category price. Below is the list of the aspect categories extracted from the corpus:
? functionality
? weight
? price
? appearance
? behavior
? performance
? quality
? service
? size
For each IAC under every aspect category, synonyms and antonyms were obtained from WordNet (Fell-
baum, 1998) and stored under the same aspect category. For example, expensive and its antonym inex-
pensive both have the same category price. Semantics extracted from SenticNet (Cambria et al., 2014b)
have also been exploited to enlarge the set of conceptually related IACs. Thus, a lexicon of 1,128 IACs
categorized into the above categories was built.
3.3.2 Opinion Lexicon
We use SenticNet 3 as a concept-level opinion lexicon. The common-sense knowledge base contains
30,000 multi-word expressions labeled by their polarity scores. The proposed aspect parser is based on
two general rules:
? Rules for the sentences having subject verb.
? Rules for the sentences which do not have subject verb.
3
http://nlp.stanford.edu:8080/parser
31
A dependency relation is a binary relation characterized by the following features:
? The type of the relation that specifies the nature of the (syntactic) link between the two elements in
the relation.
? The head of the relation: this is the element that is the pivot of the relation. Core syntactic and
semantics properties (e.g., agreement) are inherited from the head.
? The dependent is the element that depends on the head and which usually inherits some of its
characteristics (e.g., number, gender in the case of agreement).
Most of the times, the active token is considered in a relation if it acts as the head of the relation, although
there are exceptions. Once the active token has been identified as the trigger for a rule, there are several
ways to compute its contribution, depending on how the dependency relation and the properties of the
tokens match with the rules. The preferred way is not to consider the contribution of the token alone,
but in combination with the other elements in the dependency relation. First of all, Stanford parser is
used to obtain the dependency parse structure of each sentence. Then, hand-crafted dependency rules are
employed on the parse trees to extract aspects.
3.3.3 Subject Noun Rule
Trigger: when the active token is found to be the syntactic subject of a token. Behavior: if an active
token h is in a subject noun relationship with a word t then:
1. if t has any adverbial or adjective modifier and the modifier exists in SenticNet, then t is extracted
as an aspect.
2. if the sentence does not have auxiliary verb, i.e., is, was, would, should, could, then:
? if the verb t is modified by an adjective or an adverb or it is in adverbial clause modifier relation
with another token, then both h and t are extracted as aspects. In (1), battery is in a subject
relation with lasts and lasts is modified by the adjective modifier little, hence both the aspects
last and battery are extracted.
(1) The battery lasts little.
? if t has any direct object relation with a token n and the POS of the token is Noun and n is not
in SenticNet, then n is extracted as an aspect. In (2), like is in direct object relation with lens
so the aspect lens is extracted.
(2) I like the lens of this camera.
? if t has any direct object relation with a token n and the POS of the token n is Noun and n exists
in SenticNet, then the token n extracted as aspect term. In the dependency parse tree of the
sentence, if another token n
1
is connected to n using any dependency relation and the POS of
n
1
is Noun, then n
1
is extracted as an aspect. In (3), like is in direct object relation with beauty
which is connected to screen via a preposition relation. So the aspects screen and beauty are
extracted.
(3) I like the beauty of the screen.
? if t is in open clausal complement relation with a token t
1
, then the aspect t-t
1
is extracted if t-t
1
exists in the opinion lexicon. If t
1
is connected with a token t
2
whose POS is Noun, then t
2
is
extracted as an aspect. In (4), like and comment is in clausal complement relation and comment
is connected to camera using a preposition relation. Here, the POS of camera is Noun and,
hence, camera is extracted as an aspect.
(4) I would like to comment on the camera of this phone.
32
3. A copula is the relation between the complement of a copular verb and the copular verb. If the
token t is in copula relation with a copular verb and the copular verb exists in the implicit aspect
lexicon, then t is extract as aspect term. In (5), expensive is extracted as an aspect.
(5) The car is expensive.
4. If the token t is in copula relation with a copular verb and the POS of h is Noun, then h is extracted
as an explicit aspect. In (6), camera is extracted as an aspect.
(6) The camera is nice.
5. If the token t is in copula relation with a copular verb and the copular verb is connected to a token
t
1
using any dependency relation and t
1
is a verb, then both t
1
and t are extracted as implicit aspect
terms, as long as they exist in the implicit aspect lexicon. In (7), lightweight is in copula relation
with is and lightweight is connected to the word carry by open clausal complement relation. Here,
both lightweight and carry are extracted as aspects.
(7) The phone is very lightweight to carry.
3.3.4 Sentences which do not have subject noun relation in their parse tree
For sentences that do not have noun subject relation in their parse trees, aspects are extracted using the
following rules:
1. if an adjective or adverb h is in infinitival or open clausal complement relation with a token t and h
exists in the implicit aspect lexicon, then h is extracted as an aspect. In (8), big is extracted as an
aspect as it is connected to hold using a clausal complement relation.
(8) Very big to hold.
2. if a token h is connected to a noun t using a prepositional relation, then both h and t are extracted as
aspects. In (9) sleekness is extracted as an aspect.
(9) Love the sleekness of the player.
3. if a token h is in a direct object relation with a token t, t is extracted as aspect. In (10), mention is in
a direct object relation with price, hence price is extracted as an aspect.
(10) Not to mention the price of the phone.
3.3.5 Additional Rules
? For each aspect term extracted above, if an aspect term h is in co-ordination or conjunct relation
with another token t, then t is also extracted as an aspect. In (11), amazing is firstly extracted as an
aspect term. As amazing is in conjunct relation with easy, then use is also extracted as an aspect.
(11) The camera is amazing and easy to use.
? A noun compound modifier of an NP is any noun that serves to modify the head noun. If t is
extracted as an aspect and t has noun compound modifier h, then the aspect h-t is extracted and t
is removed from the aspect list. In (12), as chicken and casserole are in noun compound modifier
relation, only chicken casserole is extracted as an aspect.
(12) We ordered the chicken casserole, but what we got were a few small pieces of chicken, all
dark meat and on the bone.
33
4 Novelty of the proposed work
First of all, the proposed method is fully unsupervised and depends on the accuracy of the dependency
parser and the opinion lexicon, rather then a training corpus and supervised learning accuracy. Only
(Qiu et al., 2011) follow an unsupervised learning approach but the proposed method uses an enhanced
set of rules and opinion lexicon. The proposed method also outperforms (Qiu et al., 2011) on the same
dataset they used. Implicit aspects extracted through the proposed method differ from implicit aspect
expressions defined by Liu (Liu, 2012) as ?aspect expressions that are not nouns or noun phrases? in that
implicit aspects extracted by the proposed algorithm semantically refer to the values of the pre-defined
aspects, irrespective of their own surface POS. Below are listed some examples where the implicit aspect
terms are either noun or noun phrases.
In (13), the IAC deal is extracted.
(13) Even if I had paid full price I would have considered this phone a good deal.
In (14), sleekness is extracted as an IAC.
(14) Not to mention the sleekness of this phone.
In (15), the IAC errors is extracted by the algorithm.
(15) The player keeps giving random errors.
In (16), piece of crap is a noun phrase and is extracted as an IAC by the proposed algorithm.
(16) This phone is a piece of crap.
A demo of the developed aspect parser is freely available at http://sentic.net/demo.
Table 2: Results on the DVD-player review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 75.00% 82.00%
Popescu and Etzioni 89.00% 80.00%
Dependency propagation method 87.00% 81.00%
Proposed approach 89.25% 91.25%
Table 3: Results on the Canon G3 review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 71.00% 79.00%
Popescu and Etzioni 87.00% 74.00%
Dependency propagation method 90.00% 81.00%
Proposed approach 90.15% 92.25%
Table 4: Results on the Jukebox review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 72.00% 76.00%
Popescu and Etzioni 89.00% 74.00%
Dependency propagation method 90.00% 86.00%
Proposed approach 92.25% 94.15%
34
Table 5: Results on the Nikon Coolpix review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 69.00% 82.00%
Popescu and Etzioni 86.00% 80.00%
Dependency propagation method 81.00% 84.00%
Proposed approach 82.15% 86.15%
Table 6: Results on the Nokia-6610 review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 74.00% 80.00%
Popescu and Etzioni 90.00% 78.00%
Dependency propagation method 92.00% 86.00%
Proposed approach 93.25% 93.32%
5 Experiments and Results
5.1 Experiment on the dataset provided by (Hu and Liu, 2004)
Experimental evaluation was carried out on the dataset derived from (Hu and Liu, 2004). As discussed
in Section 3, the proposed method is able to extract both explicit and implicit aspects. To the best of our
knowledge, there is no state-of-the-art benchmark to evaluate implicit aspect extraction.
We compare the proposed framework with those in Hu and Liu (Hu and Liu, 2004), Qiu et al. (Qiu et
al., 2011), and Popescu and Etzioni (Popescu and Etzioni, 2005) (which only carried out explicit aspect
extraction). Table 2, Table 3, Table 4, Table 5 and Table 6 show that the proposed framework outperforms
all existing methods in terms of both precision and recall.
6 Conclusion
We have illustrated a method for extracting both explicit and implicit aspects from opinionated text.
The proposed framework only leverages on common-sense knowledge and on the dependency structure
of sentences and, hence, is unsupervised. As future work, we aim to discover more rules for aspect
extraction. Another key future effort is to combine existing rules for complex aspect extraction. To
obtain the aspect categories of IACs, we have developed an aspect knowledge base using WordNet and
SenticNet. We will focus on extending the scalability of such knowledge base and on making it as much
noise-free as possible.
6.1 Experiment on Semeval 2014 dataset
We also carried out experiments on Semeval 2014 aspect based sentiment analysis data obtained from
http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools. Re-
sults are shown in Table 7. We cannot perform a comparative evaluation of such experimental results as
there is no state-of-art approach yet which used this dataset for the same kind of experiment. Overall,
results show high accuracy.
Table 7: Results on the Semeval 2014 dataset
Domain Precision Recall
Laptop 82.15% 84.32%
Restaurants 85.21% 88.15%
35
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George A. Reis, and Jeff Reynar. 2008.
Building a sentiment summarizer for local service reviews. In Proceedings of WWW-2008 workshop on NLP in
the Information Explosion Era, page 14.
Erik Cambria, Thomas Mazzocco, Amir Hussain, and Chris Eckl. 2011. Sentic medoids: Organizing affective
common sense knowledge in a multi-dimensional vector space. In D Liu, H Zhang, M Polycarpou, C Alippi,
and H He, editors, Advances in Neural Networks, volume 6677 of Lecture Notes in Computer Science, pages
601?610, Berlin. Springer-Verlag.
Erik Cambria, Paolo Gastaldo, Federica Bisio, and Rodolfo Zunino. 2014a. An ELM-based model for affective
analogical reasoning. Neurocomputing.
Erik Cambria, Daniel Olsher, and Dheeraj Rajagopal. 2014b. SenticNet 3: A common and common-sense knowl-
edge base for cognition-driven sentiment analysis. AAAI, pages 1515?1521.
Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In
Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2010), pages 268?274.
Ivan Cruz-Garcia, Alexander Gelbukh, and Grigori Sidorov. 2014. Implicit aspect indicator extraction for aspect-
based opinion mining. submitted.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In
Proceedings of First ACM International Conference on Web Search and Data Mining (WSDM-2008), pages
231?240, Stanford University, Stanford, California, USA, Feb.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication).
The MIT Press.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 168?177, Aug.
Sheng Huang, Xinlan Liu, Xueping Peng, and Zhendong Niu. 2012. Fine-grained product features extraction and
categorization in reviews opinion mining. In Proceedings of the IEEE 12th International Conference on Data
Mining Workshops, pages 680?686.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized HMM-based learning framework for web opinion mining.
In Proceedings of International Conference on Machine Learning (ICML-2009), pages 465?472.
John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pages 282?289. Morgan Kaufmann Publishers.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware
review mining and summarization. In Proceedings of the 23rd International Conference on Computational
Linguistics (COLING-2010), pages 653?661.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers.
Jakob Niklas and Iryna Gurevych. 2010. Extracting opinion targets in a single and cross-domain setting with con-
ditional random fields. In Proceedings of Conference on Empirical Methods in Natural Language Processing
(EMNLP-2010), pages 1035?1045.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceed-
ings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2005), pages 3?28.
Soujanya Poria, Erik Cambria, Gregoire Winterstein, and Guang-Bin Huang. 2014. Sentic patterns: Dependency-
based rules for concept-level sentiment analysis. Knowledge-Based Systems.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Christopher Scaffidi, Kevin Bierhoff, Eric Chang, Mikhael Felker, Herman Ng, and Chun Jin. 2007. Red opal:
product-feature scoring from reviews. In Proceedings of the 8th ACM conference on Electronic commerce,
pages 182?191. ACM.
36
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian Wu, Xiaoxun Zhang, Bin Swen, and Zhong Su. 2008. Hidden
sentiment association in chinese web opinion mining. In Proceedings of International Conference on World
Wide Web (WWW-2008), pages 959?968.
Rui Xia, Chengqing Zong, Xuelei Hu, and Erik Cambria. 2013. Feature ensemble plus sample selection: A
comprehensive approach to domain adaptation for sentiment classification. IEEE Intelligent Systems, 28(3):10?
18.
Lingwei Zeng and Fang Li. 2013. A classification-based approach for implicit feature identification. In Chinese
Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. 12th
China National Conference, CCL 2013 and First International Symposium, NLP-NABD 2013, Suzhou, China,
October 10?12, 2013, Proceedings, volume 8202 of Lecture Notes in Computer Science, pages 190?202.
Hai Zhen, Kuiyu Chang, and Jung-jae Kim. 2011. Implicit feature identification via co-occurrence association rule
mining. In Computational Linguistics and Intelligent Text Processing. 12th International Conference, CICLing
2011, Tokyo, Japan, February 20?26, 2011. Proceedings, Part I, volume 6608 of Lecture Notes in Computer
Science, pages 393?404.
37
