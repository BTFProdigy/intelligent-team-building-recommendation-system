Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 571?580, Prague, June 2007. c?2007 Association for Computational Linguistics
Cross-lingual Distributional Profiles of Concepts
for Measuring Semantic Distance
Saif Mohammad? Iryna Gurevych? Graeme Hirst? Torsten Zesch?
?Dept. of Computer Science
University of Toronto
Toronto, Canada
{smm,gh}@cs.toronto.edu
?Ubiquitous Knowledge Processing Group
Darmstadt University of Technology
Darmstadt, Germany
{gurevych,zesch}@tk.informatik.tu-darmstadt.de
Abstract
We present the idea of estimating seman-
tic distance in one, possibly resource-poor,
language using a knowledge source in an-
other, possibly resource-rich, language. We
do so by creating cross-lingual distributional
profiles of concepts, using a bilingual lexi-
con and a bootstrapping algorithm, but with-
out the use of any sense-annotated data or
word-aligned corpora. The cross-lingual
measures of semantic distance are evaluated
on two tasks: (1) estimating semantic dis-
tance between words and ranking the word
pairs according to semantic distance, and
(2) solving Reader?s Digest ?Word Power?
problems. In task (1), cross-lingual mea-
sures are superior to conventional monolin-
gual measures based on a wordnet. In task
(2), cross-lingual measures are able to solve
more problems correctly, and despite scores
being affected by many tied answers, their
overall performance is again better than the
best monolingual measures.
1 Introduction
Accurately estimating the semantic distance be-
tween concepts or between words in context has per-
vasive applications in computational linguistics, in-
cluding machine translation, information retrieval,
speech recognition, spelling correction, and text cat-
egorization (see Budanitsky and Hirst (2006) for dis-
cussion), and it is becoming clear that basing such
measures on a combination of corpus statistics with
a knowledge source, such as a dictionary, published
thesaurus, or WordNet, can result in higher accu-
racies (Mohammad and Hirst, 2006b). This is be-
cause such knowledge sources capture semantic in-
formation about concepts and, to some extent, world
knowledge. They also act as sense inventories for
the words in a language.
However, applying algorithms for semantic dis-
tance to most languages is hindered by the lack of
linguistic resources. In this paper, we propose a
new method that allows us to compute semantic dis-
tance in a possibly resource-poor language by seam-
lessly combining its text with a knowledge source
in a different, preferably resource-rich, language.
We demonstrate the approach by combining German
text with an English thesaurus to create English?
German distributional profiles of concepts, which in
turn will be used to measure the semantic distance
between German words.
Two classes of methods have been used in deter-
mining semantic distance. Semantic measures of
concept-distance, such as those of Jiang and Con-
rath (1997) and Resnik (1995), rely on the structure
of a knowledge source, such as WordNet, to deter-
mine the distance between two concepts defined in
it (see Budanitsky and Hirst (2006) for a survey).
Distributional measures of word-distance1, such
as cosine and ?-skew divergence (Lee, 2001), deem
1Many distributional approaches represent the sets of con-
texts of the target words as points in multidimensional co-
occurrence space or as co-occurrence distributions. A measure,
such as cosine, that captures vector distance or a measure, such
as ?-skew divergence, that captures distance between distribu-
tions is then used to measure distributional distance. We will
therefore refer to these measures as distributional measures.
571
two words to be closer or less distant if they occur in
similar contexts (see Mohammad and Hirst (2005)
for a comprehensive survey).
Distributional measures rely simply on raw text
and possibly some shallow syntactic processing.
They do not require any other manually-created re-
source, and tend to have a higher coverage. How-
ever, by themselves they perform poorly when com-
pared to semantic measures (Mohammad and Hirst,
2006b) because when given a target word pair
we usually need the distance between their closest
senses, but distributional measures of word-distance
tend to conflate the distances between all possible
sense pairs. Latent semantic analysis (LSA) (Lan-
dauer et al, 1998) has also been used to measure dis-
tributional distance with encouraging results (Rapp,
2003). However, it too measures the distance be-
tween words and not senses. Further, the dimen-
sionality reduction inherent to LSA has the effect of
making the predominant sense more dominant while
de-emphasizing the other senses. Therefore, an
LSA-based approach will also conflate information
from the different senses, and even more emphasis
will be placed on the predominant senses. Given the
semantically close target nouns play and actor, for
example, a distributional measure will give a score
that is some sort of a dominance-based average of
the distances between their senses. The noun play
has the predominant sense of ?children?s recreation?
(and not ?drama?), so a distributional measure will
tend to give the target pair a large (and thus erro-
neous) distance score. Also, distributional word-
distance approaches need to create large V ?V co-
occurrence and distance matrices, where V is the
size of the vocabulary (usually at least 100,000).2
Mohammad and Hirst (2006b) proposed a way
of combining written text with a published the-
saurus to measure distance between concepts (or
word senses) using distributional measures, thereby
eliminating sense-conflation and achieving results
better than the simple word-distance measures and
indeed also most of the WordNet-based semantic
measures. We called these measures distributional
measures of concept-distance. Concept-distance
2LSA is especially expensive as singular value decomposi-
tion, a key component for dimensionality reduction, requires
computationally intensive matrix operations; making it less
scalable to large amounts of text (Gorman and Curran, 2006).
measures can be used to measure distance between
a word pair by choosing the distance between their
closest senses. Thus, even though ?children?s recre-
ation? is the predominant sense of play, the ?drama?
sense is much closer to actor and so their dis-
tance will be chosen. These distributional concept-
distance approaches need to create only V ?C co-
occurrence and C?C distance matrices, where C is
the number of categories or senses (usually about
1000). It should also be noted that unlike the best
WordNet-based measures, distributional measures
(both word- and concept-distance ones) can be used
to estimate not just semantic similarity but also se-
mantic relatedness?useful in many tasks includ-
ing information retrieval. However, the high-quality
thesauri and (to a much greater extent) WordNet-like
resources that these methods require do not exist for
most of the 3000?6000 languages in existence today
and they are costly to create.
In this paper, we introduce cross-lingual distri-
butional measures of concept-distance, or simply
cross-lingual measures, that determine the distance
between a word pair belonging to a resource-poor
language using a knowledge source in a resource-
rich language and a bilingual lexicon3. We will use
the cross-lingual measures to calculate distances be-
tween German words using an English thesaurus and
a German corpus. Although German is not resource-
poor per se, Gurevych (2005) has observed that the
German wordnet GermaNet (Kunze, 2004) (about
60,000 synsets) is less developed than the English
WordNet (Fellbaum, 1998) (about 117,000 synsets)
with respect to the coverage of lexical items and lex-
ical semantic relations represented therein. On the
other hand, substantial raw corpora are available for
the German language. Crucially for our evaluation,
the existence of GermaNet alows comparison of our
cross-lingual approach with monolingual ones.
2 Monolingual Distributional Measures
In order to set the context for cross-lingual concept-
distance measures (Section 3), we first summarize
monolingual distributional approaches, with a focus
on distributional concept-distance measures.
3For most languages that have been the subject of academic
study, there exists at least a bilingual lexicon mapping the core
vocabulary of that language to a major world language and a
corpus of at least a modest size.
572
2.1 Word-distance
Words that occur in similar contexts tend to be se-
mantically close. In our experiments, we defined the
context of a target word, its co-occurring words, to
be ?5 words on either side (but not crossing sen-
tence boundaries). The set of contexts of a target
word is usually represented by the strengths of as-
sociation of the target with its co-occurring words,
which we refer to as the distributional profile (DP)
of the word. Here is a constructed example DP of
the word star:
DP of a word
star: space 0.28, movie 0.2, famous 0.13,
light 0.09, rich 0.04, . . .
Simple counts are made of how often the target word
co-occurs with other words in text and how often
the words occur individually. A suitable statistic,
such as pointwise mutual information (PMI), is then
applied to these counts to determine the strengths
of association between the target and co-occurring
words. The distributional profiles of two target
words represent their contexts as points in multi-
dimensional word-space. A suitable distributional
measure (for example, cosine) gives the distance be-
tween the two points, and thereby an estimate of the
semantic distance between the target words.
2.2 Concept-distance
In Mohammad and Hirst (2006b), we show how dis-
tributional profiles of concepts (DPCs) can be used
to measure semantic distance. Below are the DPCs
or DPs of two senses of the word star (the senses
or concepts themselves are glossed by a set of near-
synonymous words, placed in parentheses):
DPs of concepts
?celestial body? (celestial body,
sun, . . . ): space 0.36, light 0.27,
constellation 0.11, . . .
?celebrity? (celebrity, hero, . . . ):
famous 0.24, movie 0.14, rich 0.14, . . .
Thus the profiles of two target concepts represent
their contexts as points in multi-dimensional word-
space. A suitable distributional measure (for exam-
ple, cosine) can then be used to give the distribu-
tional distance between the two concepts in the same
way that distributional word-distance is measured.
But to calculate the strength of association of
a concept with co-occurring words, in order to
create DPCs, we must determine the number of
times a word used in that sense co-occurs with
surrounding words. In Mohammad and Hirst
(2006a), we proposed a way to determine these
counts without the use of sense-annotated data.
Briefly, a word?category co-occurrence matrix
(WCCM) is created having English word types
wen as one dimension and English thesaurus cat-
egories cen as another. We used the Macquarie
Thesaurus (Bernard, 1986) both as a very coarse-
grained sense inventory and a source of possibly
ambiguous English words that together unam-
biguously represent each category (concept). The
WCCM is populated with co-occurrence counts
from a large English corpus (we used the British
National Corpus (BNC)). A particular cell mi j,
corresponding to word weni and concept cenj , is
populated with the number of times weni co-occurs
(in a window of ?5 words) with any word that has
cenj as one of its senses (i.e., weni co-occurs with any
word listed under concept cenj in the thesaurus).
cen1 c
en
2 . . . c
en
j . . .
wen1 m11 m12 . . . m1 j . . .
wen2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
weni mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
This matrix, created after a first pass of the cor-
pus, is the base word?category co-occurrence ma-
trix (base WCCM) and it captures strong associa-
tions between a sense and co-occurring words.4 This
is similar to how Yarowsky (1992) identifies words
that are indicative of a particular sense of the target.
We know that words that occur close to a target
word tend to be good indicators of its intended sense.
Therefore, we make a second pass of the corpus, us-
ing the base WCCM to roughly disambiguate the
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
4From the base WCCM we can determine the number of
times a word w and concept c co-occur, the number of times
w co-occurs with any concept, and the number of times c co-
occurs with any word. A statistic such as PMI can then give the
strength of association between w and c.
573
with each of its senses is summed. The sense that
has the highest cumulative association is chosen as
the intended sense. A new bootstrapped WCCM
is created such that each cell mi j, corresponding to
word weni and concept cenj , is populated with the
number of times weni co-occurs with any word used
in sense cenj .
Mohammad and Hirst (2006a) used the DPCs cre-
ated from the bootstrapped WCCM to attain near-
upper-bound results in the task of determining word
sense dominance. Unlike the McCarthy et al (2004)
dominance system, our approach can be applied
to much smaller target texts (a few hundred sen-
tences) without the need for a large similarly-sense-
distributed text5. In Mohammad and Hirst (2006a),
the DPC-based monolingual distributional measures
of concept-distance were used to rank word pairs
by their semantic similarity and to correct real-
word spelling errors, attaining markedly better re-
sults than monolingual distributional measures of
word-distance. In the spelling correction task, the
distributional concept-distance measures performed
better than all WordNet-based measures as well, ex-
cept for the Jiang and Conrath (1997) measure.
3 Cross-lingual Distributional Measures
We now describe how distributional measures of
concept-distance can be used in a cross-lingual
framework to determine the distance between words
in (resource-poor) language L1 by combining its text
with a thesaurus in (resource-rich) language L2, us-
ing an L1?L2 bilingual lexicon. We will compare
this approach with the best monolingual approaches;
the smaller the loss in performance, the more ca-
pable the algorithm is of overcoming ambiguities
in word translation. An evaluation, therefore, re-
quires an L1 that in actuality has adequate knowl-
edge sources. Therefore we chose German to stand
in as the resource-poor language L1 and English as
the resource-rich L2; the monolingual evaluation in
German will use GermaNet. The remainder of the
paper describes our approach in terms of German
and English, but the algorithm itself is language in-
dependent.
5The McCarthy et al (2004) system needs to first gener-
ate a distributional thesaurus from the target text (if it is large
enough?a few million words) or from another large text with a
distribution of senses similar to the target text.
3.1 Concept-distance
Given a German word wde in context, we use a
German?English bilingual lexicon to determine its
different possible English translations. Each En-
glish translation wen may have one or more possi-
ble coarse senses, as listed in an English thesaurus.
These English thesaurus concepts (cen) will be re-
ferred to as cross-lingual candidate senses of the
German word wde.6 Figure 1 depicts examples.7
As in the monolingual distributional measures,
the distance between two concepts is calculated by
first determining their DPs. However, in the cross-
lingual approach, a concept is now glossed by near-
synonymous words in an English thesaurus, whereas
its profile is made up of the strengths of associ-
ation with co-occurring German words. Here are
constructed example cross-lingual DPs of the two
senses of star:
Cross-lingual DPs of concepts
?celestial body? (celestial body, sun,
. . . ): Raum 0.36, Licht 0.27,
Konstellation 0.11, . . .
?celebrity? (celebrity, hero, . . . ):
beru?hmt 0.24, Film 0.14, reich 0.14, . . .
In order to calculate the strength of association, we
must first determine individual word and concept
counts, as well as their co-occurrence counts.
3.2 Cross-lingual word?category
co-occurrence matrix
We create a cross-lingual word?category co-
occurrence matrix with German word types wde as
one dimension and English thesaurus concepts cen
6Some of the cross-lingual candidate senses of wde might
not really be senses of wde (e.g., ?celebrity?, ?river bank?, and
?judiciary? in Figure 1). However, as substantiated by experi-
ments in Section 4, our algorithm is able to handle the added
ambiguity.
7Vocabulary of German words needed to understand this dis-
cussion: Bank: 1. financial institution, 2. bench (furniture);
beru?hmt: famous; Film: movie (motion picture); Himmels-
ko?rper: heavenly body; Konstellation: constellation; Licht:
light; Morgensonne: morning sun; Raum: space; reich: rich;
Sonne: sun; Star: star (celebrity); Stern: star (celestial body)
574
}
star bank
river
bank
bench
furniture
judiciary
celestial body
celebrity
}
institution
financial }
Stern Bank wde
cen
wen
Figure 1: The cross-lingual candidate senses of Ger-
man words Stern and Bank.
as another.
cen1 c
en
2 . . . c
en
j . . .
wde1 m11 m12 . . . m1 j . . .
wde2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wdei mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
The matrix is populated with co-occurrence counts
from a large German corpus; we used the newspaper
corpus, taz8 (Sep 1986 to May 1999; 240 million
words). A particular cell mi j, corresponding to word
wdei and concept cenj , is populated with the number
of times the German word wdei co-occurs (in a win-
dow of ?5 words) with any German word having cenj
as one of its cross-lingual candidate senses. For ex-
ample, the Raum??celestial body? cell will have the
sum of the number of times Raum co-occurs with
Himmelsko?rper, Sonne, Morgensonne, Star, Stern,
and so on (see Figure 2). We used the Macquarie
Thesaurus (Bernard, 1986) (about 98,000 words)
for our experiments. The possible German trans-
lations of an English word were taken from the
German?English bilingual lexicon BEOLINGUS9
(about 265,000 entries).
This base word?category co-occurrence matrix
(base WCCM), created after a first pass of the cor-
pus captures strong associations between a category
(concept) and co-occurring words. For example,
even though we increment counts for both Raum?
?celestial body? and Raum??celebrity? for a particu-
lar instance where Raum co-occurs with Star, Raum
will co-occur with a number of words such as Him-
melsko?rper, Sonne, and Morgensonne that each have
the sense of celestial body in common (see Figure
2), whereas all their other senses are likely different
8http://www.taz.de
9http://dict.tu-chemnitz.de
... }
... }
}
sun
Sonne Morgensonne Star
celestial body
celestial body
Stern
star wen
wde
cen
Himmelsko?rper
Figure 2: Words having ?celestial body? as one of
their cross-lingual candidate senses.
and distributed across the set of concepts. There-
fore, the co-occurrence count of Raum and ?celestial
body? will be relatively higher than that of Raum and
?celebrity?.
As in the monolingual case, a second pass of
the corpus is made to disambiguate the (German)
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
with each of its cross-lingual candidate senses is
summed. The sense that has the highest cumula-
tive association with co-occurring words is chosen
as the intended sense. A new bootstrapped WCCM
is created by populating each cell mi j, correspond-
ing to word wdei and concept cenj , with the number of
times the German word wdei co-occurs with any Ger-
man word used in cross-lingual sense cenj . A statistic
such as PMI is then applied to these counts to deter-
mine the strengths of association between a target
concept and co-occurring words, giving the distri-
butional profile of the concept.
Following the ideas described above, Mohammad
et al (2007) created Chinese?English DPCs from
Chinese text, a Chinese?English bilingual lexicon,
and an English thesaurus. They used these DPCs to
implement an unsupervised na??ve Bayes word sense
classifier that placed first among all unsupervised
systems taking part in the Multilingual Chinese?
English Lexical Sample Task (task #5) of SemEval-
07 (Jin et al, 2007).
4 Evaluation
We evaluated the newly proposed cross-lingual dis-
tributional measures of concept-distance on the tasks
of (1) measuring semantic distance between German
words and ranking German word pairs according to
semantic distance, and (2) solving German ?Word
Power? questions from Reader?s Digest. In order
to compare results with state-of-the-art monolingual
approaches we conducted experiments using Ger-
575
(Cross-lingual) Distributional Measures (Monolingual) GermaNet Measures
Information Content?based Lesk-like
?-skew divergence (Lee, 2001) (ASD) Jiang and Conrath (1997) (JC) hypernym pseudo-gloss (HPG)
cosine (Schu?tze and Pedersen, 1997) (Cos) Lin (1998b) (LinGN) radial pseudo-gloss (RPG)
Jensen-Shannon divergence (JSD) Resnik (1995) (Res)
Lin?s measure (1998a) (Lindist)
Table 1: Distance measures used in our experiments.
Dataset Year Language # pairs PoS Scores # subjects Correlation
Gur65 2005 German 65 N discrete {0,1,2,3,4} 24 .810
Gur350 2006 German 350 N, V, A discrete {0,1,2,3,4} 8 .690
Table 2: Comparison of datasets used for evaluating semantic distance in German.
maNet measures as well. The specific distributional
measures10 and GermaNet-based measures we used
are listed in Table 1. The GermaNet measures are
of two kinds: (1) information content measures,11
and (2) Lesk-like measures that rely on n-gram over-
laps in the glosses of the target senses, proposed by
Gurevych (2005)12.
The cross-lingual measures combined the German
newspaper corpus taz with the English Macquarie
Thesaurus using the German?English bilingual lex-
icon BEOLINGUS. Multi-word expressions in the
thesaurus and the bilingual lexicon were ignored.
We used a context of ?5 words on either side of the
target word for creating the base and bootstrapped
WCCMs. No syntactic pre-processing was done,
nor were the words stemmed, lemmatized, or part-
of-speech tagged.
4.1 Measuring distance in word pairs
4.1.1 Data
A direct approach to evaluate distance measures is
to compare them with human judgments. Gurevych
10JSD and ASD calculate the difference in distributions of
words that co-occur with the targets. Lindist (distributional
measure) and LinGN (GermaNet measure) follow from Lin?s
(1998b) information-theoretic definition of similarity.
11Information content measures rely on finding the lowest
common subsumer (lcs) of the target synsets in a hypernym hi-
erarchy and using corpus counts to determine how specific or
general this concept is. In general, the more specific the lcs is
and the smaller the difference of its specificity with that of the
target concepts, the closer the target concepts are.
12As GermaNet does not have glosses for synsets, Gurevych
(2005) proposed a way of creating a bag-of-words-type pseudo-
gloss for a synset by including the words in the synset and in
synsets close to it in the network.
(2005) and Zesch et al (2007) asked native German
speakers to mark two different sets of German word
pairs with distance values. Set 1 (Gur65) consists
of a German translation of the English Rubenstein
and Goodenough (1965) dataset. It has 65 noun?
noun word pairs. Set 2 (Gur350) is a larger dataset
containing 350 word pairs made up of nouns, verbs,
and adjectives. The semantically close word pairs
in Gur65 are mostly synonyms or hypernyms (hy-
ponyms) of each other, whereas those in Gur350
have both classical and non-classical relations (Mor-
ris and Hirst, 2004) with each other. Details of these
semantic distance benchmarks13 are summarized
in Table 2. Inter-subject correlations are indicative
of the degree of ease in annotating the datasets.
4.1.2 Results and Discussion
Word-pair distances determined using different
distance measures are compared in two ways with
the two human-created benchmarks. The rank order-
ing of the pairs from closest to most distant is evalu-
ated with Spearman?s rank order correlation ?; the
distance judgments themselves are evaluated with
Pearson?s correlation coefficient r. The higher the
correlation, the more accurate the measure is. Spear-
man?s correlation ignores actual distance values af-
ter a list is ranked?only the ranks of the two sets
of word pairs are compared to determine correla-
tion. On the other hand, Pearson?s coefficient takes
into account actual distance values. So even if two
lists are ranked the same, but one has distances be-
13The datasets are publicly available at:
http://www.ukp.tu-darmstadt.de/data/semRelDatasets
576
tween consecutively-ranked word-pairs more in line
with human-annotations of distance than the other,
then Pearson?s coefficient will capture this differ-
ence. However, this makes Pearson?s coefficient
sensitive to outlier data points, and so one must in-
terpret the Pearson correlations with caution.
Table 3 shows the results.14 Observe that on both
datasets and by both measures of correlation, cross-
lingual measures of concept-distance perform not
just as well as the best monolingual measures, but in
fact better. In general, the correlations are lower for
Gur350 as it contains cross-PoS word pairs and non-
classical relations, making it harder to judge even
by humans (as shown by the inter-annotator corre-
lations for the datasets in Table 2). Considering
Spearman?s rank correlation, ?-skew divergence and
Jensen-Shannon divergence perform best on both
datasets. The correlations of cosine and Lindist are
not far behind. Amongst the monolingual GermaNet
measures, radial pseudo-gloss performs best. Con-
sidering Pearson?s correlation, Lindist performs best
overall and radial pseudo-gloss does best amongst
the monolingual measures. Thus, we see that on
both datasets and as per both measures of correla-
tion, the cross-lingual measures perform not just as
well as the best monolingual measures, but indeed
slightly better.
4.2 Solving word choice problems from
Reader?s Digest
4.2.1 Data
Issues of the German edition of Reader?s Digest
include a word choice quiz called ?Word Power?.
Each question has one target word and four alter-
native words or phrases; the objective is to pick the
alternative that is most closely related to the target.
The correct answer may be a near-synonym of the
target or it may be related to the target by some other
classical or non-classical relation (usually the for-
mer). For example:15
Duplikat (duplicate)
a. Einzelstu?ck (single copy) b. Doppelkinn (double chin)
c. Nachbildung (replica) d. Zweitschrift (copy)
Our approach to evaluating distance measures fol-
14In Table 3, all values are statistically significant at the 0.01
level (2-tailed), except for the one in italic (0.212), which is
significant at the 0.05 level (2-tailed).
15English translations are in parentheses.
lows that of Jarmasz and Szpakowicz (2003), who
evaluated semantic similarity measures through their
ability to solve synonym problems (80 TOEFL (Lan-
dauer and Dumais, 1997), 50 ESL (Turney, 2001),
and 300 (English) Reader?s Digest Word Power
questions). Turney (2006) used a similar approach
to evaluate the identification of semantic relations,
with 374 college-level multiple-choice word anal-
ogy questions.
The Reader?s Digest Word Power (RDWP)
benchmark for German consists of 1072 of these
word-choice problems collected from the January
2001 to December 2005 issues of the German-
language edition (Wallace and Wallace, 2005). We
discarded 44 problems that had more than one cor-
rect answer, and 20 problems that used a phrase in-
stead of a single term as the target. The remaining
1008 problems form our evaluation dataset, which is
significantly larger than any of the previous datasets
employed in a similar evaluation.
We evaluate the various cross-lingual and mono-
lingual distance measures by their ability to choose
the correct answer. The distance between the target
and each of the alternatives is computed by a mea-
sure, and the alternative that is closest is chosen. If
two or more alternatives are equally close to the tar-
get, then the alternatives are said to be tied. If one
of the tied alternatives is the correct answer, then
the problem is counted as correctly solved, but the
corresponding score is reduced. We assign a score
of 0.5, 0.33, and 0.25 for 2, 3, and 4 tied alterna-
tives, respectively (in effect approximating the score
obtained by randomly guessing one of the tied al-
ternatives). If more than one alternative has a sense
in common with the target, then the thesaurus-based
cross-lingual measures will mark them each as the
closest sense. However, if one or more of these tied
alternatives is in the same semicolon group of the
thesaurus16 as the target, then only these are chosen
as the closest senses.
The German RDWP dataset contains many
phrases that cannot be found in the knowledge
sources (GermaNet or Macquarie Thesaurus via
translation list). In these cases, we remove stop-
16Words in a thesaurus category are further partitioned into
different paragraphs and each paragraph into semicolon groups.
Words within a semicolon group are more closely related than
those in semicolon groups of the same paragraph or category.
577
Gur65 Gur350
Measure ? r ? r
Monolingual
HPG 0.672 0.702 0.346 0.331
RPG 0.764 0.565 0.492 0.420
JC 0.665 0.748 0.417 0.410
LinGN 0.607 0.739 0.475 0.495
Res 0.623 0.722 0.454 0.466
Cross-lingual
ASD 0.794 0.597 0.520 0.413
Cos 0.778 0.569 0.500 0.212
JSD 0.793 0.633 0.522 0.422
Lindist 0.775 0.816 0.498 0.514
Table 3: Correlations of distance measures with hu-
man judgments.
words (prepositions, articles, etc.) and split the
phrase into component words. As German words
in a phrase can be highly inflected, we lemmatize
all components. For example, the target ?imagina?r?
(imaginary) has ?nur in der Vorstellung vorhanden?
(?exists only in the imagination?) as one of its alter-
natives. The phrase is split into its component words
nur, Vorstellung, and vorhanden. We compute se-
mantic distance between the target and each phrasal
component and select the minimum value as the dis-
tance between target and potential answer.
4.2.2 Results and Discussion
Table 4 presents the results obtained on the Ger-
man RDWP benchmark for both monolingual and
cross-lingual measures. Only those questions for
which the measures have some distance information
are attempted; the column ?Att.? shows the number
of questions attempted by each measure, which is
the maximum score that the measure can hope to
get. Observe that the thesaurus-based cross-lingual
measures have a much larger coverage than the
GermaNet-based monolingual measures. The cross-
lingual measures have a much larger number of cor-
rect answers too (column ?Cor.?), but this number is
bloated due to the large number of ties.17 ?Score?
is the score each measure gets after it is penalized
for the ties. The cross-lingual measures Cos, JSD,
and Lindist obtain the highest scores. But ?Score?
by itself does not present the complete picture ei-
17We see more ties when using the cross-lingual measures
because they rely on the Macquarie Thesaurus, a very coarse-
grained sense inventory (around 800 categories), whereas the
cross-lingual measures operate on the fine-grained GermaNet.
Reader?s Digest Word Power benchmark
Measure Att. Cor. Ties Score P R F
Monolingual
HPG 222 174 11 171.5 .77 .17 .28
RPG 266 188 15 184.7 .69 .18 .29
JC 357 157 1 156.0 .44 .16 .23
LinGN 298 153 1 152.5 .51 .15 .23
Res 299 154 33 148.3 .50 .15 .23
Cross-lingual
ASD 438 185 81 151.6 .35 .15 .21
Cos 438 276 90 223.1 .51 .22 .31
JSD 438 276 90 229.6 .52 .23 .32
Lindist 438 274 90 228.7 .52 .23 .32
Table 4: Performance of distance measures on word
choice problems. (Att.: Attempted, Cor.: Correct)
ther as, given the scoring scheme, a measure that at-
tempts more questions may get a higher score just
from random guessing. We therefore present pre-
cision, recall, and F-scores (P = Score/Att; R =
Score/1008; F = 2?P?R/(P + R)). Observe that
the cross-lingual measures have a higher coverage
(recall) than the monolingual measures but lower
precision. The F scores show that the best cross-
lingual measures do slightly better than the best
monolingual ones, despite the large number of ties.
The measures of Cos, JSD, and Lindist remain the
best cross-lingual measures, whereas HPG and RPG
are the best monolingual ones.
5 Conclusion
We have proposed a new method to determine se-
mantic distance in a possibly resource-poor lan-
guage by combining its text with a knowledge
source in a different, preferably resource-rich, lan-
guage. Specifically, we combined German text with
an English thesaurus to create cross-lingual distri-
butional profiles of concepts?the strengths of as-
sociation between English thesaurus senses (con-
cepts) of German words and co-occurring German
words?using a German?English bilingual lexicon
and a bootstrapping algorithm designed to overcome
ambiguities of word-senses and translations. No-
tably, we do so without the use of sense-annotated
text or word-aligned parallel corpora. We did not
parse or chunk the text, nor did we stem, lemmatize,
or part-of-speech-tag the words.
We used the cross-lingual DPCs to estimate se-
mantic distance by developing new cross-lingual
578
distributional measures of concept-distance. These
measures are like the distributional measures of
concept-distance (Mohammad and Hirst, 2006a,
2006b), except they can determine distance between
words in one language using a thesaurus in a differ-
ent language. We evaluated the cross-lingual mea-
sures against the best monolingual ones operating
on a WordNet-like resource, GermaNet, through an
extensive set of experiments on two different Ger-
man semantic distance benchmarks. In the process,
we compiled a large German benchmark of Reader?s
Digest word choice problems suitable for evaluating
semantic-relatedness measures. Most previous se-
mantic distance benchmarks are either much smaller
or cater primarily to semantic similarity measures.
Even with the added ambiguity of translating
words from one language to another, the cross-
lingual measures performed better than the best
monolingual measures on both the word-pair task
and the Reader?s Digest word-choice task. Fur-
ther, in the word-choice task, the cross-lingual mea-
sures achieved a significantly higher coverage than
the monolingual measure. The richness of En-
glish resources seems to have a major impact, even
though German, with GermaNet, a well-established
resource, is in a better position than most other lan-
guages. This is indeed promising, because achieving
broad coverage for resource-poor languages remains
an important goal as we integrate state-of-the-art ap-
proaches in natural language processing into real-
life applications. These results show that our algo-
rithm can successfully combine German text with an
English thesaurus using a bilingual German?English
lexicon to obtain state-of-the-art results in measur-
ing semantic distance.
These results also support the broader and far-
reaching claim that natural language problems in
a resource-poor language can be solved using a
knowledge source in a resource-rich language (e.g.,
Cucerzan and Yarowsky?s (2002) cross-lingual PoS
tagger). Our future work will explore other tasks
such as information retrieval and text categoriza-
tion. Cross-lingual DPCs also have tremendous po-
tential in tasks inherently involving more than one
language, such as machine translation and multi-
language multi-document summarization. We be-
lieve that the future of natural language process-
ing lies not in standalone monolingual systems but
in those that are powered by automatically created
multilingual networks of information.
Acknowledgments
We thank Philip Resnik, Michael Demko, Suzanne
Stevenson, Frank Rudicz, Afsaneh Fazly, and Afra
Alishahi for helpful discussions. This research is fi-
nancially supported by the Natural Sciences and En-
gineering Research Council of Canada, the Univer-
sity of Toronto, the German Research Foundation
under the grant ?Semantic Information Retrieval?
(SIR), GU 798/1-2.
References
J.R.L. Bernard, editor. 1986. The Macquarie Thesaurus.
Macquarie Library, Sydney, Australia.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of the 6th Conference
on Computational Natural Language Learning, pages
132?138, Taipei, Taiwan.
Christiane Fellbaum. 1998. WordNet An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 361?368,
Sydney, Australia.
Iryna Gurevych. 2005. Using the Structure of a Concep-
tual Network in Computing Semantic Relatedness. In
Proceedings of the 2nd International Joint Conference
on Natural Language Processing, pages 767?778, Jeju
Island, Republic of Korea.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and semantic similarity. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP-2003), pages
212?219.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of International Conference
on Research on Computational Linguistics (ROCLING
X), Taiwan.
579
Peng Jin, Yunfang Wu, and Shiwen Yu. 2007. SemEval-
2007 task 05: Multilingual Chinese-English lexical
sample task. In Proceedings of the Fourth Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text (SemEval-07), Prague,
Czech Republic.
Claudia Kunze, 2004. Lexikalisch-semantische Wort-
netze, chapter Computerlinguistik und Sprachtech-
nologie, pages 423?431. Spektrum Akademischer
Verlag.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104:211?240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. Introduction to latent semantic analysis. Dis-
course Processes, 25(2?3):259?284.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
Dekang Lin. 1998a. Automatic retreival and cluster-
ing of similar words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING-98), pages 768?773, Montreal, Canada.
Dekang Lin. 1998b. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, pages 296?304, San
Francisco, CA. Morgan Kaufmann.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 280?267, Barcelona, Spain.
Saif Mohammad and Graeme Hirst. 2005.
Distributional measures as proxies for
semantic relatedness. In submission,
http://www.cs.toronto.edu/compling/Publications.
Saif Mohammad and Graeme Hirst. 2006a. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy.
Saif Mohammad and Graeme Hirst. 2006b. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Saif Mohammad, Graeme Hirst, and Philip Resnik. 2007.
Distributional profiles of concepts for unsupervised
word sense disambigution. In Proceedings of the
Fourth International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text (SemEval-
07), Prague, Czech Republic.
Jane Morris and Graeme Hirst. 2004. Non-classical lex-
ical semantic relations. In Proceedings of the Work-
shop on Computational Lexical Semantics, Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Boston, Massachusetts.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Machine Translation Summit IX, pages 315?322, New
Orleans, Louisiana.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-95), pages 448?453, Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communications
of the ACM, 8(10):627?633.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications to
information retreival. Information Processing and
Management, 33(3):307?318.
Peter Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), pages 491?502, Freiburg, Germany.
Peter Turney. 2006. Expressing implicit semantic rela-
tions without supervision. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
313?320, Sydney, Australia.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader?s Digest, das Beste fu?r Deutschland. Jan
2001?Dec 2005. Verlag Das Beste, Stuttgart.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget?s categories trained on
large corpora. In Proceedings of the 14th International
Conference on Computational Linguistics (COLING-
92), pages 454?460, Nantes, France.
Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.
2007. Comparing Wikipedia and German WordNet by
evaluating semantic relatedness on multiple datasets.
In Proceedings of Human Language Technologies:
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2007), pages 205?208, Rochester, New
York.
580
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 982?991,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Computing Word-Pair Antonymy
Saif Mohammad? Bonnie Dorr
 
Graeme Hirst?
?   Laboratory for Computational Linguistics and Information Processing
?   Institute for Advanced Computer Studies and
 
Computer Science
?   University of Maryland and
 
Human Language Technology Center of Excellence

saif,bonnie  @umiacs.umd.edu
?Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Knowing the degree of antonymy between
words has widespread applications in natural
language processing. Manually-created lexi-
cons have limited coverage and do not include
most semantically contrasting word pairs. We
present a new automatic and empirical mea-
sure of antonymy that combines corpus statis-
tics with the structure of a published the-
saurus. The approach is evaluated on a set of
closest-opposite questions, obtaining a preci-
sion of over 80%. Along the way, we discuss
what humans consider antonymous and how
antonymy manifests itself in utterances.
1 Introduction
Native speakers of a language intuitively recog-
nize different degrees of antonymy?whether two
words are strongly antonymous (hot?cold, good?
bad, friend?enemy), just semantically contrasting
(enemy?fan, cold?lukewarm, ascend?slip) or not
antonymous at all (penguin?clown, cold?chilly,
boat?rudder). Over the years, many definitions of
antonymy have been proposed by linguists (Cruse,
1986; Lehrer and Lehrer, 1982), cognitive scien-
tists (Kagan, 1984), psycholinguists (Deese, 1965),
and lexicographers (Egan, 1984), which differ from
each other in small and large respects. In its
strictest sense, antonymy applies to gradable adjec-
tives, such as hot?cold and tall?short, where the
two words represent the two ends of a semantic
dimension. In a broader sense, it includes other
adjectives, nouns, and verbs as well (life?death,
ascend?descend, shout?whisper). In its broadest
sense, it applies to any two words that represent
contrasting meanings. We will use the term de-
gree of antonymy to encompass the complete se-
mantic range?a combined measure of the contrast
in meaning conveyed by two words and the tendency
of native speakers to call them opposites. The higher
the degree of antonymy between a target word pair,
the greater the semantic contrast between them and
the greater their tendency to be considered antonym
pairs by native speakers.
Automatically determining the degree of
antonymy between words has many uses includ-
ing detecting and generating paraphrases (The
dementors caught Sirius Black / Black could not
escape the dementors) and detecting contradictions
(Marneffe et al, 2008; Voorhees, 2008) (Kyoto has
a predominantly wet climate / It is mostly dry in
Kyoto). Of course, such ?contradictions? may be
a result of differing sentiment, new information,
non-coreferent mentions, or genuinely contradictory
statements. Antonyms often indicate the discourse
relation of contrast (Marcu and Echihabi, 2002).
They are also useful for detecting humor (Mihalcea
and Strapparava, 2005), as satire and jokes tend
to have contradictions and oxymorons. Lastly, it
is useful to know which words are semantically
contrasting to a target word, even if simply to filter
them out. For example, in the automatic creation
of a thesaurus it is necessary to distinguish near-
synonyms from word pairs that are semantically
contrasting. Measures of distributional similarity
fail to do so. Detecting antonymous words is not
sufficient to solve most of these problems, but it
remains a crucial, and largely unsolved, component.
982
Lexicons of pairs of words that native speakers
consider antonyms have been created for certain lan-
guages, but their coverage has been limited. Further,
as each term of an antonymous pair can have many
semantically close terms, the contrasting word pairs
far outnumber those that are commonly considered
antonym pairs, and they remain unrecorded. Even
though a number of computational approaches have
been proposed for semantic closeness, and some for
hypernymy?hyponymy (Hearst, 1992), measures of
antonymy have been less successful. To some ex-
tent, this is because antonymy is not as well under-
stood as other classical lexical-semantic relations.
We first very briefly summarize insights and in-
tuitions about this phenomenon, as proposed by lin-
guists and lexicographers (Section 2). We discuss
related work (Section 3). We describe the resources
we use (Section 4) and present experiments that ex-
amine the manifestation of antonymy in text (Sec-
tions 5 and 6). We then propose a new empirical
approach to determine the degree of antonymy be-
tween two words (Section 7). We compiled a dataset
of 950 closest-opposite questions, which we used for
evaluation (Section 8). We conclude with a discus-
sion of the merits and limitations of this approach
and outline future work.
2 The paradoxes of antonymy
Antonymy, like synonymy and hyponymy, is a
lexical-semantic relation that, strictly speaking, ap-
plies to two lexical units?combinations of surface
form and word sense. (That said, for simplicity and
where appropriate we will use the term ?antonymous
words? as a proxy for ?antonymous lexical units?.)
However, accepting this leads to two interesting and
seemingly paradoxical questions (described below
in the two subsections).
2.1 Why are some pairs better antonyms?
Native speakers of a language consider certain con-
trasting word pairs to be antonymous (for example,
large?small), and certain other seemingly equivalent
word pairs as less so (for example, large?little). A
number of reasons have been suggested: (1) Cruse
(1986) observes that if the meaning of the target
words is completely defined by one semantic dimen-
sion and the words represent the two ends of this se-
mantic dimension, then they tend to be considered
antonyms. We will refer to this semantic dimension
as the dimension of opposition. (2) If on the other
hand, as Lehrer and Lehrer (1982) point out, there is
more to the meaning of the antonymous words than
the dimension of opposition?for example, more se-
mantic dimensions or added connotations?then the
two words are not so strongly antonymous. Most
people do not think of chubby as a direct antonym
of thin because it has the additional connotation of
being cute and informal. (3) Cruse (1986) also pos-
tulates that word pairs are not considered strictly
antonymous if it is difficult to identify the dimension
of opposition (for example, city?farm). (4) Charles
and Miller (1989) claim that two contrasting words
are identified as antonyms if they occur together in
a sentence more often than chance. However, Mur-
phy and Andrew (1993) claim that the greater-than-
chance co-occurrence of antonyms in sentences is
because together they convey contrast well, which
is rhetorically useful, and not really the reason why
they are considered antonyms in the first place.
2.2 Are semantic closeness and antonymy
opposites?
Two words (more precisely, two lexical units) are
considered to be close in meaning if there is a
lexical-semantic relation between them. Lexical-
semantic relations are of two kinds: classical
and non-classical. Examples of classical rela-
tions include synonymy, hyponymy, troponymy, and
meronymy. Non-classical relations, as pointed out
by Morris and Hirst (2004), are much more com-
mon and include concepts pertaining to another con-
cept (kind, chivalrous, formal pertaining to gentle-
manly), and commonly co-occurring words (for ex-
ample, problem?solution pairs such as homeless,
shelter). Semantic distance (or closeness) in this
broad sense is known as semantic relatedness. Two
words are considered to be semantically similar if
they are associated via the synonymy, hyponymy?
hypernymy, or the troponymy relation. So terms
that are semantically similar (plane?glider, doctor?
surgeon) are also semantically related, but terms that
are semantically related may not always be semanti-
cally similar (plane?sky, surgeon?scalpel).
Antonymy is unique among these relations be-
cause it simultaneously conveys both a sense of
983
closeness and of distance (Cruse, 1986). Antony-
mous concepts are semantically related but not se-
mantically similar.
3 Related work
Charles and Miller (1989) proposed that antonyms
occur together in a sentence more often than chance.
This is known as the co-occurrence hypothesis.
They also showed that this was empirically true for
four adjective antonym pairs. Justeson and Katz
(1991) demonstrated the co-occurrence hypothesis
for 35 prototypical antonym pairs (from an original
set of 39 antonym pairs compiled by Deese (1965))
and also for an additional 22 frequent antonym pairs.
All of these pairs were adjectives. Fellbaum (1995)
conducted similar experiments on 47 noun, verb, ad-
jective, and adverb pairs (noun?noun, noun?verb,
noun?adjective, verb?adverb and so on) pertaining
to 18 concepts (for example, lose(v)?gain(n) and
loss(n)?gain(n), where lose(v) and loss(n) pertain to
the concept of ?failing to have/maintain?). How-
ever, non-antonymous semantically related words
such as hypernyms, holonyms, meronyms, and near-
synonyms also tend to occur together more often
than chance. Thus, separating antonyms from them
has proven to be difficult.
Lin et al (2003) used patterns such as ?from X
to Y? and ?either X or Y? to separate antonym word
pairs from distributionally similar pairs. They eval-
uated their method on 80 pairs of antonyms and 80
pairs of synonyms taken from the Webster?s Colle-
giate Thesaurus (Kay, 1988). In this paper, we pro-
pose a method to determine the degree of antonymy
between any word pair and not just those that are
distributionally similar. Turney (2008) proposed a
uniform method to solve word analogy problems
that require identifying synonyms, antonyms, hyper-
nyms, and other lexical-semantic relations between
word pairs. However, the Turney method is super-
vised whereas the method proposed in this paper is
completely unsupervised.
Harabagiu et al (2006) detected antonyms
for the purpose of identifying contradictions
by using WordNet chains?synsets connected by
the hypernymy?hyponymy links and exactly one
antonymy link. Lucerto et al (2002) proposed de-
tecting antonym pairs using the number of words
between two words in text and also cue words such
as but, from, and and. Unfortunately, they evalu-
ated their method on only 18 word pairs. Neither of
these methods determines the degree of antonymy
between words and they have not been shown to
have substantial coverage. Schwab et al (2002) cre-
ate ?antonymous vector? for a target word. The
closer this vector is to the context vectors of the
other target word, the more antonymous the two tar-
get words are. However, the antonymous vectors are
manually created. Further, the approach is not eval-
uated beyond a handful of word pairs.
Work in sentiment detection and opinion mining
aims at determining the polarity of words. For ex-
ample, Pang, Lee and Vaithyanathan (2002) detect
that adjectives such as dazzling, brilliant, and grip-
ping cast their qualifying nouns positively whereas
adjectives such as bad, cliched, and boring portray
the noun negatively. Many of these gradable adjec-
tives have antonyms. but these approaches do not
attempt to determine pairs of positive and negative
polarity words that are antonyms.
4 Resources
4.1 Published thesauri
Published thesauri, such as the Roget?s and Mac-
quarie, divide the vocabulary into about a thousand
categories. Words within a category tend to be near-
synonymous or semantically similar. One may also
find antonymous and semantically related words in
the same category, but this is rare. The intuition
is that words within a category represent a coarse
concept. Words with more than one meaning may
be found in more than one category; these repre-
sent its coarse senses. Within a category, the words
are grouped into paragraphs. Words in the same
paragraph tend to be closer in meaning than those in
different paragraphs. We will take advantage of the
structure of the thesaurus in our approach.
4.2 WordNet
Unlike the traditional approach to antonymy, Word-
Net encodes antonymy as a lexical relationship?a
relation between two words (not concepts) (Gross et
al., 1989). Even though a synset (a WordNet con-
cept) may be represented by more than one word,
individual words across synsets are marked as (di-
984
rect) antonyms. Gross et al argue that other words
in the synsets form ?indirect antonyms?.
Even after including the indirect antonyms, Word-
Net?s coverage is limited. As Marcu and Echi-
habi (2002) point out, WordNet does not en-
code antonymy across part-of-speech (for exam-
ple, legally?embargo). Further, the noun?noun,
verb?verb, and adjective?adjective antonym pairs of
WordNet largely ignore near-opposites as revealed
by our experiments (Section 8 below). Also, Word-
Net (or any other manually-created repository of
antonyms for that matter) does not encode the de-
gree of antonymy between words. Nevertheless, we
investigate the usefulness of WordNet as a source of
seed antonym pairs for our approach.
4.3 Co-occurrence statistics
The distributional hypothesis of closeness states
that words that occur in similar contexts tend to
be semantically close (Firth, 1957). Distributional
measures of distance, such as those proposed by Lin
(1998), quantify how similar the two sets of contexts
of a target word pair are. Equation 1 is a modified
form of Lin?s measure that ignores syntactic depen-
dencies and hence it estimates semantic relatedness
rather than semantic similarity:
Lin   w1  w2 
?w  T  w1 	 T  w2    I   w1  w 
 I   w2  w 
?w  T  w1  I   w1  w  
 ?w   T  w2  I   w2  w   
(1)
Here w1 and w2 are the target words; I   x  y  is the
pointwise mutual information between x and y; and
T   x

is the set of all words y that have positive point-
wise mutual information with the word x (I   x

y

0).
Mohammad and Hirst (2006) showed that
these distributional word-distance measures per-
form poorly when compared with WordNet-based
concept-distance measures. They argued that this
is because the word-distance measures clump to-
gether the contexts of the different senses of the tar-
get words. They proposed a way to obtain distri-
butional distance between word senses, using any
of the distributional measures such as cosine or that
proposed by Lin, and showed that this approach per-
formed markedly better than the traditional word-
distance approach. They used thesaurus categories
as very coarse word senses. Equation 2 shows how
Lin?s formula is used to determine distributional dis-
tance between two thesaurus categories c1 and c2:
Lin   c1  c2 
?w  T  c1 	 T  c2    I   c1  w 
 I   c2  w 
?w  T  c1  I   c1  w  
 ?w   T  c2  I   c2  w   
(2)
Here T   c

is the set of all words w that have posi-
tive pointwise mutual information with the thesaurus
category c (I   c

w

0). We adopt this method
for use in our approach to determine word-pair
antonymy.
5 The co-occurrence hypothesis of
antonyms
As a first step towards formulating our approach,
we investigated the co-occurrence hypothesis on a
significantly larger set of antonym pairs than those
studied before. We randomly selected a thousand
antonym pairs (nouns, verbs, and adjectives) from
WordNet and counted the number of times (1) they
occurred individually and (2) they co-occurred in the
same sentence within a window of five words, in the
British National Corpus (BNC) (Burnard, 2000). We
then calculated the mutual information for each of
these word pairs and averaged it. We randomly gen-
erated another set of a thousand word pairs, without
regard to whether they were antonymous or not, and
used it as a control set. The average mutual infor-
mation between the words in the antonym set was
0.94 with a standard deviation of 2.27. The average
mutual information between the words in the con-
trol set was 0.01 with a standard deviation of 0.37.
Thus antonymous word pairs occur together much
more often than chance irrespective of their intended
senses (p  0  01). Of course, a number of non-
antonymous words also tend to co-occur more of-
ten than chance?commonly known as collocations.
Thus, strong co-occurrence is not a sufficient condi-
tion for detecting antonyms, but these results show
that it can be a useful cue.
6 The substitutional and distributional
hypotheses of antonyms
Charles and Miller (1989) also proposed that in
most contexts, antonyms may be interchanged. The
985
meaning of the utterance will be inverted, of course,
but the sentence will remain grammatical and lin-
guistically plausible. This came to be known as the
substitutability hypothesis. However, their exper-
iments did not support this claim. They found that
given a sentence with the target adjective removed,
most people did not confound the missing word with
its antonym. Justeson and Katz (1991) later showed
that in sentences that contain both members of an
antonymous adjective pair, the target adjectives do
indeed occur in similar syntactic structures at the
phrasal level. From this (and to some extent from the
co-occurrence hypothesis), we can derive the distri-
butional hypothesis of antonyms: antonyms occur
in similar contexts more often than non-antonymous
words.
We used the same set of one thousand antonym
pairs and one thousand control pairs as in the pre-
vious experiment to gather empirical proof of the
distributional hypothesis. For each word pair from
the antonym set, we calculated the distributional dis-
tance between each of their senses using Moham-
mad and Hirst?s (2006) method of concept distance
along with the modified form of Lin?s (1998) dis-
tributional measure (equation 2). The distance be-
tween the closest senses of the word pairs was av-
eraged for all thousand antonyms. The process was
then repeated for the control set.
The control set had an average semantic close-
ness of 0.23 with a standard deviation of 0.11 on
a scale from 0 (unrelated) to 1 (identical). On the
other hand, antonymous word pairs had an average
semantic closeness of 0.30 with a standard devia-
tion of 0.23.1 This demonstrates that relative to other
word pairs, antonymous words tend to occur in simi-
lar contexts (p  0  01). However, near-synonymous
and similar word pairs also occur in similar contexts.
(the distributional hypothesis of closeness). Thus,
just like the co-occurrence hypothesis, occurrence
in similar contexts is not sufficient, but rather yet
another useful cue towards detecting antonyms.
1It should be noted that absolute values in the range between
0 and 1 are meaningless by themselves. However, if a set of
word pairs is shown to consistently have higher values than an-
other set, then we can conclude that the members of the former
set tend to be semantically closer than those of the latter.
7 Our approach
We now present an empirical approach to determine
the degree of antonymy between words. In order
to maximize applicability and usefulness in natural
language applications, we model the broad sense of
antonymy. Given a target word pair, the approach
determines whether they are antonymous or not, and
if they are antonymous whether they have a high,
medium, or low degree of antonymy. More pre-
cisely, the approach presents a way to determine
whether one word pair is more antonymous than an-
other.
The approach relies on the structure of the pub-
lished thesaurus as well as the co-occurrence and
distributional hypotheses. As mentioned earlier, a
thesaurus organizes words in sets representing con-
cepts or categories. We first determine pairs of the-
saurus categories that are contrasting in meaning
(Section 7.1). We then use the co-occurrence and
distributional hypotheses to determine the degree of
antonymy (Section 7.2).
7.1 Detecting contrasting categories
We propose two ways of detecting thesaurus cate-
gory pairs that represent contrasting concepts (we
will call these pairs contrasting categories): (1) us-
ing a seed set of antonyms and (2) using a simple
heuristic that exploits how thesaurus categories are
ordered.
7.1.1 Seed sets
Affix-generated seed set Antonym pairs such as
hot?cold and dark?light occur frequently in text,
but in terms of type-pairs they are outnumbered
by those created using affixes, such as un- (clear?
unclear) and dis- (honest?dishonest). Further, this
phenomenon is observed in most languages (Lyons,
1977).
Table 1 lists sixteen morphological rules that tend
to generate antonyms in English. These rules were
applied to each of the words in the Macquarie The-
saurus and if the resulting term was also a valid
word in the thesaurus, then the word-pair was added
to the affix-generated seed set. These sixteen rules
generated 2,734 word pairs. Of course, not all of
them are antonymous, for example sect?insect and
coy?decoy. However, these are relatively few in
986
w1 w2 example pair w1 w2 example pair w1 w2 example pair
X abX normal?abnormal X misX fortune?misfortune imX exX implicit?explicit
X antiX clockwise?anticlockwise X nonX aligned?nonaligned inX exX introvert?extrovert
X disX interest?disinterest X unX biased?unbiased upX downX uphill?downhill
X imX possible?impossible lX illX legal?illegal overX underX overdone?underdone
X inX consistent?inconsistent rX irX regular?irregular Xless Xful harmless?harmful
X malX adroit?maladroit
Table 1: Sixteen affix rules to generate antonym pairs. Here ?X? stands for any sequence of letters common to both
words w1 and w2.
number and were found to have only a small impact
on the results.
WordNet seed set We compiled a list of 20,611
semantically contrasting word pairs from WordNet.
If two words from two synsets in WordNet are con-
nected by an antonymy link, then every possible
word pair across the two synsets was considered to
be semantically contrasting. A large number of them
include multiword expressions. For only 10,807 of
the 20,611 pairs were both words found in the Mac-
quarie Thesaurus?the vocabulary used for our ex-
periments. We will refer to them as the WordNet
seed set.
Then, given these two seed sets, if any word in
thesaurus category C1 is antonymous to any word
in category C2 as per a seed antonym pair, then the
two categories are marked as contrasting. It should
be noted, however, that the seed antonym pair may
be antonymous only in certain senses. For example,
consider the antonym pair work?play. Here, play is
antonymous to work only in its ACTIVITY FOR FUN
sense and not its DRAMA sense. In such cases, we
employ the distributional hypothesis of closeness:
two words are antonymous to each other in those
senses which are closest in meaning to each other.
Since the thesaurus category pertaining to WORK is
relatively closer in meaning to the ACTIVITY FOR
FUN sense than the DRAMA sense, those two cat-
egories will be considered contrasting and not the
categories pertaining to WORK and DRAMA.
If no word in C1 is antonymous to any word in C2,
then the categories are considered not contrasting.
As the seed sets, both automatically generated and
manually created, are relatively large in comparison
to the total number of categories in the Macquarie
Thesaurus (812), this simple approach has reason-
able coverage and accuracy.
7.1.2 Order of thesaurus categories
Most published thesauri are ordered such that
contrasting categories tend to be adjacent. This is
not a hard-and-fast rule, and often a category may be
contrasting in meaning to several other categories.
Further, often adjacent categories are not semanti-
cally contrasting. However, since this was an easy-
enough heuristic to implement, we investigated the
usefulness of considering adjacent categories as con-
trasting. We will refer to this as the adjacency
heuristic.
7.2 Determining the degree of antonymy
Once we know which category pairs are contrast-
ing (using the methods from the previous subsec-
tion), we determine the degree of antonymy be-
tween the two categories (Section 7.2.1). The aim
is to assign contrasting category pairs a non-zero
value signifying the degree of contrast. In turn, we
will use that information to determine the degree of
antonymy between any word pair whose members
belong to two contrasting categories (Sections 7.2.2
and 7.2.3).
7.2.1 Category level
Using the distributional hypothesis of antonyms,
we claim that the degree of antonymy between two
contrasting concepts (thesaurus categories) is di-
rectly proportional to the distributional closeness of
the two concepts. In other words, the more the words
representing two contrasting concepts occur in sim-
ilar contexts, the more the two concepts are consid-
ered to be antonymous.
Again we used Mohammad and Hirst?s (2006)
method along with Lin?s (1998) distributional mea-
sure to determine the distributional closeness of
two thesaurus concepts. Co-occurrence statistics re-
quired for the approach were computed from the
987
BNC. Words that occurred within a window of 5
words were considered to co-occur.
7.2.2 Lexical unit level
Recall that strictly speaking, antonymy (like other
lexical-semantic relations) applies to lexical units (a
combination of surface form and word sense). If
two words are used in senses pertaining to contrast-
ing categories (as per the methods described in Sec-
tion 7.1), then we will consider them to be antony-
mous (degree of antonymy is greater than zero).
If two words are used in senses pertaining to non-
contrasting senses, then we will consider them to be
not antonymous (degree of antonymy is equal to 0).
If the target words belong to the same thesaurus
paragraphs as any of the seed antonyms linking the
two contrasting categories, then the words are con-
sidered to have a high degree of antonymy. This is
because words that occur in the same thesaurus para-
graph tend to be semantically very close in mean-
ing. Relying on the co-occurrence hypothesis, we
claim that for word pairs listed in contrasting cate-
gories, the greater their tendency to co-occur in text,
the higher their degree of antonymy. We use mutual
information to capture the tendency of word?word
co-occurrence.
If the target words do not both belong to the same
paragraphs as a seed antonym pair, but occur in con-
trasting categories, then the target words are consid-
ered to have a low or medium degree of antonymy
(less antonymous than the word pairs discussed
above). Such word pairs that have a higher tendency
to co-occur are considered to have a medium degree
of antonymy, whereas those that have a lower ten-
dency to co-occur are considered to have a low de-
gree of antonymy.
Co-occurrence statistics for this purpose were col-
lected from the Google n-gram corpus (Brants and
Franz, 2006).2 Words that occurred within a window
of 5 words were considered to be co-occurring.
7.2.3 Word level
Even though antonymy applies to pairs of word
and sense combinations, most available texts are not
2We used the Google n-gram corpus is created from a text
collection of over 1 trillion words. We intend to use the same
corpus (and not the BNC) to determine semantic distance as
well, in the near future.
sense-annotated. If antonymous occurrences are to
be exploited for any of the purposes listed in the be-
ginning of this paper, then the text must be sense
disambiguated. However, word sense disambigua-
tion is a hard problem. Yet, and to some extent be-
cause unsupervised word sense disambiguation sys-
tems perform poorly, much can be gained by using
simple heuristics. For example, it has been shown
that cohesive text tends to have words that are close
in meaning rather than unrelated words. This, along
with the distributional hypothesis of antonyms, and
the findings by Justeson and Katz (1991) (antony-
mous concepts tend to occur more often than chance
in the same sentence), suggests that if we find a word
pair in a sentence such that two of its senses are
strongly contrasting (as per the algorithm described
in Section 7.2.2), then it is probable that the two
words are used in those contrasting senses.
8 Evaluation
8.1 Task and data
In order to best evaluate a computational measure
of antonymy, we need a task that not only requires
knowing whether two words are antonymous but
also whether one word pair is more antonymous than
another pair. Therefore, we evaluated our system on
a set of closest-opposite questions. Each question
has one target word and five alternatives. The objec-
tive is to identify that alternative which is the closest
opposite of the target. For example, consider:
adulterate: a. renounce b. forbid
c. purify d. criticize e. correct
Here the target word is adulterate. One of the al-
ternatives provided is correct, which as a verb has a
meaning that contrasts with that of adulterate; how-
ever, purify has a greater degree of antonymy with
adulterate than correct does and must be chosen
in order for the instance to be marked as correctly
answered. This evaluation is similar to how oth-
ers have evaluated semantic distance algorithms on
TOEFL synonym questions (Turney, 2001), except
that in those cases the system had to choose the al-
ternative which is closest in meaning to the target.
We looked on the World Wide Web for large sets
of closest antonym questions. We found two inde-
pendent sets of questions designed to prepare stu-
988
development data test data
P R F P R F
a. random baseline 0.20 0.20 0.20 0.20 0.20 0.20
b. affix-generated seeds only 0.72 0.53 0.61 0.71 0.51 0.60
c. WordNet seeds only 0.79 0.52 0.63 0.75 0.50 0.60
d. both seed sets 0.77 0.65 0.70 0.73 0.60 0.65
e. adjacency heuristic only 0.81 0.43 0.56 0.83 0.46 0.59
f. affix seed set + heuristic 0.75 0.60 0.67 0.76 0.61 0.68
g. both seed sets + heuristic 0.76 0.66 0.70 0.76 0.64 0.70
Table 2: Results obtained on closest-opposite questions.
dents for the Graduate Record Examination.3 The
first set consists of 162 questions. We used this set
to develop our approach and will refer to it as the de-
velopment set. Even though the algorithm does not
have any tuned parameters per se, the development
set helped determine which cues of antonymy were
useful and which were not. The second set has 1208
closest-opposite questions. We discarded questions
that had a multiword target or alternative. After re-
moving duplicates we were left with 950 questions,
which we used as the unseen test set.
Interestingly, the data contains many instances
that have the same target word used in different
senses. For example:
(1) obdurate: a. meager b. unsusceptible
c. right d. tender e. intelligent
(2) obdurate: a. yielding b. motivated
c. moribund d. azure e. hard
(3) obdurate: a. transitory b. commensurate
c. complaisant d. similar e. uncommunicative
In (1), obdurate is used in the HARDENED IN FEEL-
INGS sense and the closest opposite is tender. In (2),
it is used in the RESISTANT TO PERSUASION sense
and the closest opposite is yielding. In (3), it is used
in the PERSISTENT sense and the closest opposite is
transitory.
The datasets also contain questions in which one
or more of the alternatives is a near-synonym of the
target word. For example:
astute: a. shrewd b. foolish
c. callow d. winning e. debating
Observe that shrewd is a near-synonym of astute.
The closest-opposite of astute is foolish. A man-
ual check of a randomly selected set of 100 test-set
questions revealed that, on overage, one in four had
3Both datasets are apparently in the public domain and will
be made available on request.
a near-synonym as one of the alternative.
8.2 Experiments
We used the algorithm proposed in Section 7 to auto-
matically solve the closest-opposite questions. Since
individual words may have more than one mean-
ing, we relied on the hypothesis that the intended
sense of the alternatives are those which are most
antonymous to one of the senses of the target word.
(This follows from the discussion earlier in Section
7.2.3.) So for each of the alternatives we used the
target word as context (but not the other alterna-
tives). We think that using a larger context to de-
termine antonymy will be especially useful when
the target words are found in sentences and natural
text?something we intend to explore in the future.
Table 2 presents results obtained on the develop-
ment and test data using different combinations of
the seed sets and the adjacency heuristic. If the sys-
tem did not find any evidence of antonymy between
the target and any of its alternatives, then it refrained
from attempting that question. We therefore report
precision (number of questions answered correctly /
number of questions attempted), recall (number of
questions answered correctly / total number of ques-
tions), and F-score values (2   P   R  P  R  ).
Observe that all results are well above the ran-
dom baseline of 0.20 (obtained when a system ran-
domly guesses one of the five alternatives to be the
answer). Also, using only the small set of sixteen
affix rules, the system performs almost as well as
when it uses 10,807 WordNet antonym pairs. Using
both the affix-generated and the WordNet seed sets,
the system obtains markedly improved precision and
coverage. Using only the adjacency heuristic gave
best precision values (upwards of 0.8) with substan-
989
tial coverage (attempting close to half the questions).
However, best overall performance was obtained us-
ing both seed sets and the adjacency heuristic (F-
score of 0.7).
8.3 Discussion
These results show that, to some degree, the auto-
matic approach does indeed mimic human intuitions
of antonymy. In tasks that require higher precision,
using only the adjacency heuristic is best, whereas
in tasks that require both precision and coverage, the
seed sets may be included. Even when both seed sets
were included, only four instances in the develop-
ment set and twenty in the test set had target?answer
pairs that matched a seed antonym pair. For all re-
maining instances, the approach had to generalize to
determine the closest opposite. This also shows that
even the seemingly large number of direct and in-
direct antonyms from WordNet (more than 10,000)
are by themselves insufficient.
The comparable performance obtained using the
affix rules alone suggests that even in languages
without a wordnet, substantial accuracies may be
achieved. Of course, improved results when using
WordNet antonyms as well suggests that the infor-
mation they provide is complementary.
Error analysis revealed that at times the system
failed to identify that a category pertaining to the
target word contrasted with a category pertaining
to the answer. Additional methods to identify seed
antonym pairs will help in such cases. Certain other
errors occurred because one or more alternatives
other than the official answer were also antonymous
to the target. For example, the system chose accept
as the opposite of chasten instead of reward.
9 Conclusion
We have proposed an empirical approach to
antonymy that combines corpus co-occurrence
statistics with the structure of a published thesaurus.
The method can determine the degree of antonymy
or contrast between any two thesaurus categories
(sets of words representing a coarse concept) and
between any two word pairs. We evaluated the ap-
proach on a large set of closest-opposite questions
wherein the system not only identified whether two
words are antonymous but also distinguished be-
tween pairs of antonymous words of different de-
grees. It achieved an F-score of 0.7 in this task where
the random baseline was only 0.2. When aiming for
high precision it scores over 0.8, but there is some
drop in the number of questions attempted. In the
process of developing this approach we validated the
co-occurrence hypothesis proposed by Charles and
Miller (1989) on a large set of 1000 noun, verb, and
adjective pairs. We also gave empirical proof that
antonym pairs tend to be used in similar contexts?
the distributional hypothesis for antonyms.
Our future goals include porting this approach
to a cross-lingual framework in order to determine
antonymy in a resource-poor language by combin-
ing its text with a thesaurus from a resource-rich
language. We will use antonym pairs to identify
contrast relations between sentences to in turn im-
prove automatic summarization. We also intend to
use the approach proposed here in tasks where key-
word matching is especially problematic, for exam-
ple, separating paraphrases from contradictions.
Acknowledgments
We thank Smaranda Muresan, Siddharth Patward-
han, members of the CLIP lab at the University of
Maryland, College Park, and the anonymous review-
ers for their valuable feedback. This work was sup-
ported, in part, by the National Science Foundation
under Grant No. IIS-0705832, in part, by the Human
Language Technology Center of Excellence, and in
part, by the Natural Sciences and Engineering Re-
search Council of Canada. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the sponsor.
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford University
Computing Services.
Walter G. Charles and George A. Miller. 1989. Con-
texts of antonymous adjectives. Applied Psychology,
10:357?375.
David A. Cruse. 1986. Lexical semantics. Cambridge
University Press.
990
James Deese. 1965. The structure of associations in lan-
guage and thought. The Johns Hopkins Press.
Rose F. Egan. 1984. Survey of the history of English
synonymy. Webster?s New Dictionary of Synonyms,
pages 5a?25a.
Christiane Fellbaum. 1995. Co-occurrence and
antonymy. International Journal of Lexicography,
8:281?303.
John R. Firth. 1957. A synopsis of linguistic theory
1930?55. In Studies in Linguistic Analysis, pages 1?
32, Oxford: The Philological Society. (Reprinted in
F.R. Palmer (ed.), Selected Papers of J.R. Firth 1952-
1959, Longman).
Derek Gross, Ute Fischer, and George A. Miller. 1989.
Antonymy and the representation of adjectival mean-
ings. Memory and Language, 28(1):92?106.
Sanda M. Harabagiu, Andrew Hickl, and Finley Laca-
tusu. 2006. Lacatusu: Negation, contrast and contra-
diction in text processing. In Proceedings of the 23rd
National Conference on Artificial Intelligence (AAAI-
06), Boston, MA.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, pages 539?546, Nantes, France.
John S. Justeson and Slava M. Katz. 1991. Co-
occurrences of antonymous adjectives and their con-
texts. Computational Linguistics, 17:1?19.
Jerome Kagan. 1984. The Nature of the Child. Basic
Books.
Maire Weir Kay, editor. 1988. Webster?s Collegiate The-
saurus. Merrian-Webster.
Adrienne Lehrer and K. Lehrer. 1982. Antonymy. Lin-
guistics and Philosophy, 5:483?501.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493, Acapulco, Mexico.
Dekang Lin. 1998. Automatic retreival and cluster-
ing of similar words. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING-98), pages 768?773, Montreal, Canada.
Cupertino Lucerto, David Pinto, and He?ctor Jimie?nez-
Salazar. 2002. An automatic method to identify
antonymy. In Workshop on Lexical Resources and the
Web for Word Sense Disambiguation, pages 105?111,
Puebla, Mexico.
John Lyons. 1977. Semantics, volume 1. Cambridge
University Press.
Daniel Marcu and Abdesammad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02), Philadelphia, PA.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-08), Columbus, OH.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, pages 531?538, Van-
couver, Canada.
Saif Mohammad and Graeme Hirst. 2006. Distributional
measures of concept-distance: A task-oriented evalu-
ation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Sydney,
Australia.
Jane Morris and Graeme Hirst. 2004. Non-classical
lexical semantic relations. In Proceedings of the
Workshop on Computational Lexical Semantics, HLT,
Boston, MA.
Gregory L. Murphy and Jane M. Andrew. 1993. The
conceptual basis of antonymy and synonymy in adjec-
tives. Journal of Memory and Language, 32(3):1?19.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 79?86, Philadelphia, PA.
Didier Schwab, Mathieu Lafourcade, and Violaine
Prince. 2002. Antonymy and conceptual vectors. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING-02), pages 904?
910.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING-08), pages 905?912,
Manchester, UK.
Ellen M Voorhees. 2008. Contradictions and jus-
tifications: Extensions to the textual entailment task.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL-08),
Columbus, OH.
991
Determining Word Sense Dominance Using a Thesaurus
Saif Mohammad and Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4, Canada
fsmm,ghg@cs.toronto.edu
Abstract
The degree of dominance of a sense of a
word is the proportion of occurrences of
that sense in text. We propose four new
methods to accurately determine word
sense dominance using raw text and a pub-
lished thesaurus. Unlike the McCarthy
et al (2004) system, these methods can
be used on relatively small target texts,
without the need for a similarly-sense-
distributed auxiliary text. We perform an
extensive evaluation using artificially gen-
erated thesaurus-sense-tagged data. In the
process, we create a word?category co-
occurrence matrix, which can be used for
unsupervised word sense disambiguation
and estimating distributional similarity of
word senses, as well.
1 Introduction
The occurrences of the senses of a word usually
have skewed distribution in text. Further, the dis-
tribution varies in accordance with the domain or
topic of discussion. For example, the ?assertion
of illegality? sense of charge is more frequent in
the judicial domain, while in the domain of eco-
nomics, the ?expense/cost? sense occurs more of-
ten. Formally, the degree of dominance of a par-
ticular sense of a word (target word) in a given
text (target text) may be defined as the ratio of the
occurrences of the sense to the total occurrences of
the target word. The sense with the highest domi-
nance in the target text is called the predominant
sense of the target word.
Determination of word sense dominance has
many uses. An unsupervised system will benefit
by backing off to the predominant sense in case
of insufficient evidence. The dominance values
may be used as prior probabilities for the differ-
ent senses, obviating the need for labeled train-
ing data in a sense disambiguation task. Natural
language systems can choose to ignore infrequent
senses of words or consider only the most domi-
nant senses (McCarthy et al, 2004). An unsuper-
vised algorithm that discriminates instances into
different usages can use word sense dominance to
assign senses to the different clusters generated.
Sense dominance may be determined by sim-
ple counting in sense-tagged data. However, dom-
inance varies with domain, and existing sense-
tagged data is largely insufficient. McCarthy
et al (2004) automatically determine domain-
specific predominant senses of words, where the
domain may be specified in the form of an un-
tagged target text or simply by name (for exam-
ple, financial domain). The system (Figure 1) au-
tomatically generates a thesaurus (Lin, 1998) us-
ing a measure of distributional similarity and an
untagged corpus. The target text is used for this
purpose, provided it is large enough to learn a the-
saurus from. Otherwise a large corpus with sense
distribution similar to the target text (text pertain-
ing to the specified domain) must be used.
The thesaurus has an entry for each word type,
which lists a limited number of words (neigh-
bors) that are distributionally most similar to it.
Since Lin?s distributional measure overestimates
the distributional similarity of more-frequent word
pairs (Mohammad and Hirst, Submitted), the
neighbors of a word corresponding to the predom-
inant sense are distributionally closer to it than
those corresponding to any other sense. For each
sense of a word, the distributional similarity scores
of all its neighbors are summed using the semantic
similarity of the word with the closest sense of the
121
TARGET
A
U
X
L
A
R
Y
I
I
SIMILARLY SENSE DISTRIBUTED
DOMINANCE VALUES
THESAURUS
LIN?SD
C
R
P
U
S
O
WORDNET
  TEXT
Figure 1: The McCarthy et al system.
TARGET
A
U
X
L
A
R
Y
I
I
DOMINANCE VALUES
D
C
R
P
U
S
O
WCCM
  TEXT
PUBLISHED THESAURUS
Figure 2: Our system.
neighbor as weight. The sense that gets the highest
score is chosen as the predominant sense.
The McCarthy et al system needs to re-train
(create a new thesaurus) every time it is to de-
termine predominant senses in data from a differ-
ent domain. This requires large amounts of part-
of-speech-tagged and chunked data from that do-
main. Further, the target text must be large enough
to learn a thesaurus from (Lin (1998) used a 64-
million-word corpus), or a large auxiliary text with
a sense distribution similar to the target text must
be provided (McCarthy et al (2004) separately
used 90-, 32.5-, and 9.1-million-word corpora).
By contrast, in this paper we present a method
that accurately determines sense dominance even
in relatively small amounts of target text (a few
hundred sentences); although it does use a corpus,
it does not require a similarly-sense-distributed
corpus. Nor does our system (Figure 2) need
any part-of-speech-tagged data (although that may
improve results further), and it does not need to
generate a thesaurus or execute any such time-
intensive operation at run time. Our method stands
on the hypothesis that words surrounding the tar-
get word are indicative of its intended sense, and
that the dominance of a particular sense is pro-
portional to the relative strength of association be-
tween it and co-occurring words in the target text.
We therefore rely on first-order co-occurrences,
which we believe are better indicators of a word?s
characteristics than second-order co-occurrences
(distributionally similar words).
2 Thesauri
Published thesauri, such as Roget?s and Mac-
quarie, divide the English vocabulary into around
a thousand categories. Each category has a list
of semantically related words, which we will call
category terms or c-terms for short. Words with
multiple meanings may be listed in more than one
category. For every word type in the vocabulary
of the thesaurus, the index lists the categories that
include it as a c-term. Categories roughly cor-
respond to coarse senses of a word (Yarowsky,
1992), and the two terms will be used interchange-
ably. For example, in the Macquarie Thesaurus,
bark is a c-term in the categories ?animal noises?
and ?membrane?. These categories represent the
coarse senses of bark. Note that published the-
sauri are structurally quite different from the ?the-
saurus? automatically generated by Lin (1998),
wherein a word has exactly one entry, and its
neighbors may be semantically related to it in any
of its senses. All future mentions of thesaurus will
refer to a published thesaurus.
While other sense inventories such as WordNet
exist, use of a published thesaurus has three dis-
tinct advantages: (i) coarse senses?it is widely
believed that the sense distinctions of WordNet are
far too fine-grained (Agirre and Lopez de Lacalle
Lekuona (2003) and citations therein); (ii) compu-
tational ease?with just around a thousand cate-
gories, the word?category matrix has a manage-
able size; (iii) widespread availability?thesauri
are available (or can be created with relatively
less effort) in numerous languages, while Word-
Net is available only for English and a few ro-
mance languages. We use the Macquarie The-
saurus (Bernard, 1986) for our experiments. It
consists of 812 categories with around 176,000
c-terms and 98,000 word types. Note, however,
that using a sense inventory other than WordNet
will mean that we cannot directly compare perfor-
mance with McCarthy et al (2004), as that would
require knowing exactly how thesaurus senses
map to WordNet. Further, it has been argued that
such a mapping across sense inventories is at best
difficult and maybe impossible (Kilgarriff and Yal-
lop (2001) and citations therein).
122
3 Co-occurrence Information
3.1 Word?Category Co-occurrence Matrix
The strength of association between a particular
category of the target word and its co-occurring
words can be very useful?calculating word sense
dominance being just one application. To this
end we create the word?category co-occurrence
matrix (WCCM) in which one dimension is the
list of all words (w1;w2; : : :) in the vocabulary,
and the other dimension is a list of all categories
(c1;c2; : : :).
c1 c2 : : : c j : : :
w1 m11 m12 : : : m1 j : : :
w2 m21 m22 : : : m2 j : : :
.
.
.
.
.
.
.
.
.
.
.
.
: : : : : :
wi mi1 mi2 : : : mi j : : :
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A particular cell, mi j, pertaining to word wi and
category c j, is the number of times wi occurs in
a predetermined window around any c-term of c j
in a text corpus. We will refer to this particular
WCCM created after the first pass over the text
as the base WCCM. A contingency table for any
particular word w and category c (see below) can
be easily generated from the WCCM by collaps-
ing cells for all other words and categories into
one and summing up their frequencies. The ap-
plication of a suitable statistic will then yield the
strength of association between the word and the
category.
c :c
w nwc nw:
:w n
:c n::
Even though the base WCCM is created from
unannotated text, and so is expected to be noisy,
we argue that it captures strong associations rea-
sonably accurately. This is because the errors
in determining the true category that a word co-
occurs with will be distributed thinly across a
number of other categories (details in Section 3.2).
Therefore, we can take a second pass over the cor-
pus and determine the intended sense of each word
using the word?category co-occurrence frequency
(from the base WCCM) as evidence. We can
thus create a newer, more accurate, bootstrapped
WCCM by populating it just as mentioned ear-
lier, except that this time counts of only the co-
occurring word and the disambiguated category
are incremented. The steps of word sense disam-
biguation and creating new bootstrapped WCCMs
can be repeated until the bootstrapping fails to im-
prove accuracy significantly.
The cells of the WCCM are populated using a
large untagged corpus (usually different from the
target text) which we will call the auxiliary cor-
pus. In our experiments we use a subset (all except
every twelfth sentence) of the British National
Corpus World Edition (BNC) (Burnard, 2000) as
the auxiliary corpus and a window size of 5
words. The remaining one twelfth of the BNC is
used for evaluation purposes. Note that if the tar-
get text belongs to a particular domain, then the
creation of the WCCM from an auxiliary text of
the same domain is expected to give better results
than the use of a domain-free text.
3.2 Analysis of the Base WCCM
The use of untagged data for the creation of the
base WCCM means that words that do not re-
ally co-occur with a certain category but rather
do so with a homographic word used in a differ-
ent sense will (erroneously) increment the counts
corresponding to the category. Nevertheless, the
strength of association, calculated from the base
WCCM, of words that truly and strongly co-occur
with a certain category will be reasonably accurate
despite this noise.
We demonstrate this through an example. As-
sume that category c has 100 c-terms and each c-
term has 4 senses, only one of which corresponds
to c while the rest are randomly distributed among
other categories. Further, let there be 5 sentences
each in the auxiliary text corresponding to every
c-term?sense pair. If the window size is the com-
plete sentence, then words in 2,000 sentences will
increment co-occurrence counts for c. Observe
that 500 of these sentences truly correspond to cat-
egory c, while the other 1500 pertain to about 300
other categories. Thus on average 5 sentences cor-
respond to each category other than c. Therefore
in the 2000 sentences, words that truly co-occur
with c will likely occur a large number of times,
while the rest will be spread out thinly over 300 or
so other categories.
We therefore claim that the application of a
suitable statistic, such as odds ratio, will result
in significantly large association values for word?
category pairs where the word truly and strongly
co-occurs with the category, and the effect of noise
123
will be insignificant. The word?category pairs
having low strength of association will likely be
adversely affected by the noise, since the amount
of noise may be comparable to the actual strength
of association. In most natural language applica-
tions, the strength of association is evidence for a
particular proposition. In that case, even if associ-
ation values from all pairs are used, evidence from
less-reliable, low-strength pairs will contribute lit-
tle to the final cumulative evidence, as compared
to more-reliable, high-strength pairs. Thus even if
the base WCCM is less accurate when generated
from untagged text, it can still be used to provide
association values suitable for most natural lan-
guage applications. Experiments to be described
in section 6 below substantiate this.
3.3 Measures of Association
The strength of association between a sense or
category of the target word and its co-occurring
words may be determined by applying a suitable
statistic on the corresponding contingency table.
Association values are calculated from observed
frequencies (nwc;n:c;nw:; and n::), marginal fre-
quencies (nw = nwc+nw:; n: = n:c+n::; nc =
nwc + n:c; and n: = nw: + n::), and the sample
size (N = nwc +n:c+nw:+n::). We provide ex-
perimental results using Dice coefficient (Dice),
cosine (cos), pointwise mutual information (pmi),
odds ratio (odds), Yule?s coefficient of colligation
(Yule), and phi coefficient (?)1.
4 Word Sense Dominance
We examine each occurrence of the target word
in a given untagged target text to determine dom-
inance of any of its senses. For each occurrence
t 0 of a target word t, let T 0 be the set of words
(tokens) co-occurring within a predetermined win-
dow around t 0; let T be the union of all such T 0
and let Xt be the set of all such T 0. (Thus jXt j is
equal to the number of occurrences of t, and jT j is
equal to the total number of words (tokens) in the
windows around occurrences of t.) We describe
1Measures of association (Sheskin, 2003):
cos(w;c) =
nwc
p
nw
p
n
c
; pmi(w;c) = log nwcN
nwnc
;
odds(w;c) = nwcn::
nw:n:c
; Yule(w;c) =
p
odds(w;c) 1
p
odds(w;c)+1
;
Dice(w;c) =
2nwc
nw+nc
; ?(w;c) = (nwcn::)  (nw:n:c)p
nwn:ncn:
UnweightedWeighted
disambiguation
Implicit sense
Explicit sense
disambiguation
votingvoting
DI,W
DE,W
DI,U
E,UD
Figure 3: The four dominance methods.
four methods (Figure 3) to determine dominance
(DI;W ;DI;U ;DE ;W ; and DE ;U ) and the underlying
assumptions of each.
DI;W is based on the assumption that the more
dominant a particular sense is, the greater the
strength of its association with words that co-occur
with it. For example, if most occurrences of bank
in the target text correspond to ?river bank?, then
the strength of association of ?river bank? with all
of bank?s co-occurring words will be larger than
the sum for any other sense. Dominance DI;W of a
sense or category (c) of the target word (t) is:
DI;W (t;c) =
?w2T A(w;c)
?c02senses(t) ?w2T A(w;c0)
(1)
where A is any one of the measures of association
from section 3.3. Metaphorically, words that co-
occur with the target word give a weighted vote to
each of its senses. The weight is proportional to
the strength of association between the sense and
the co-occurring word. The dominance of a sense
is the ratio of the total votes it gets to the sum of
votes received by all the senses.
A slightly different assumption is that the more
dominant a particular sense is, the greater the num-
ber of co-occurring words having highest strength
of association with that sense (as opposed to any
other). This leads to the following methodol-
ogy. Each co-occurring word casts an equal, un-
weighted vote. It votes for that sense (and no
other) of the target word with which it has the
highest strength of association. The dominance
DI;U of the sense is the ratio of the votes it gets
to the total votes cast for the word (number of co-
occurring words).
DI;U(t;c) =
jfw 2 T : Sns1(w; t) = cgj
jT j
(2)
Sns1(w; t) = argmax
c02senses(t)
A(w;c0) (3)
Observe that in order to determine DI;W or
DI;U , we do not need to explicitly disambiguate
124
the senses of the target word?s occurrences. We
now describe alternative approaches that may be
used for explicit sense disambiguation of the target
word?s occurrences and thereby determine sense
dominance (the proportion of occurrences of that
sense). DE ;W relies on the hypothesis that the in-
tended sense of any occurrence of the target word
has highest strength of association with its co-
occurring words.
DE ;W (t;c) =
jfT 0 2Xt : Sns2(T 0; t) = cgj
jXt j
(4)
Sns2(T 0; t) = argmax
c02senses(t)
?
w2T 0
A(w;c0) (5)
Metaphorically, words that co-occur with the tar-
get word give a weighted vote to each of its senses
just as in DI;W . However, votes from co-occurring
words in an occurrence are summed to determine
the intended sense (sense with the most votes) of
the target word. The process is repeated for all
occurrences that have the target word. If each
word that co-occurs with the target word votes as
described for DI;U , then the following hypothesis
forms the basis of DE ;U : in a particular occurrence,
the sense that gets the maximum votes from its
neighbors is the intended sense.
DE ;U(t;c) =
jfT 0 2Xt : Sns3(T 0; t) = cgj
jXt j
(6)
Sns3(T 0; t) = argmax
c02senses(t)
jfw 2 T 0 : Sns1(w; t) = c0gj
(7)
In methods DE ;W and DE ;U , the dominance of
a sense is the proportion of occurrences of that
sense.
The degree of dominance provided by all four
methods has the following properties: (i) The
dominance values are in the range 0 to 1?a score
of 0 implies lowest possible dominance, while a
score of 1 means that the dominance is highest.
(ii) The dominance values for all the senses of a
word sum to 1.
5 Pseudo-Thesaurus-Sense-Tagged Data
To evaluate the four dominance methods we would
ideally like sentences with target words annotated
with senses from the thesaurus. Since human an-
notation is both expensive and time intensive, we
present an alternative approach of artificially gen-
erating thesaurus-sense-tagged data following the
ideas of Leacock et al (1998). Around 63,700
of the 98,000 word types in the Macquarie The-
saurus are monosemous?listed under just one
of the 812 categories. This means that on aver-
age around 77 c-terms per category are monose-
mous. Pseudo-thesaurus-sense-tagged (PTST)
data for a non-monosemous target word t (for
example, brilliant) used in a particular sense or
category c of the thesaurus (for example, ?intel-
ligence?) may be generated as follows. Identify
monosemous c-terms (for example, clever) be-
longing to the same category as c. Pick sentences
containing the monosemous c-terms from an un-
tagged auxiliary text corpus.
Hermione had a clever plan.
In each such sentence, replace the monosemous
word with the target word t. In theory the c-
terms in a thesaurus are near-synonyms or at least
strongly related words, making the replacement of
one by another acceptable. For the sentence above,
we replace clever with brilliant. This results in
(artificial) sentences with the target word used
in a sense corresponding to the desired category.
Clearly, many of these sentences will not be lin-
guistically well formed, but the non-monosemous
c-term used in a particular sense is likely to have
similar co-occurring words as the monosemous c-
term of the same category.2 This justifies the use
of these pseudo-thesaurus-sense-tagged data for
the purpose of evaluation.
We generated PTST test data for the head words
in SENSEVAL-1 English lexical sample space3 us-
ing the Macquarie Thesaurus and the held out sub-
set of the BNC (every twelfth sentence).
6 Experiments
We evaluate the four dominance methods, like
McCarthy et al (2004), through the accuracy of
a naive sense disambiguation system that always
gives out the predominant sense of the target word.
In our experiments, the predominant sense is de-
termined by each of the four dominance methods,
individually. We used the following setup to study
the effect of sense distribution on performance.
2Strong collocations are an exception to this, and their ef-
fect must be countered by considering larger window sizes.
Therefore, we do not use a window size of just one or two
words on either side of the target word, but rather windows
of 5 words in our experiments.
3SENSEVAL-1 head words have a wide range of possible
senses, and availability of alternative sense-tagged data may
be exploited in the future.
125
(phi, pmi, odds, Yule): .11I,UD
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
baselinebaseline
Ac
cu
rac
y
Distribution (alpha)
Mean distance below upper bound
DE,W (pmi, odds, Yule)
(pmi)
(phi, pmi,
D
DI,U
I,W
E,U
I,W
(phi, pmi, odds, Yule): .16
(pmi): .03
D
DDE,W(pmi, odds, Yule): .02
(phi, pmi,DE,U
upper boundupper bound
odds, Yule)
odds, Yule)
lower bound lower bound
Figure 4: Best results: four dominance methods
6.1 Setup
For each target word for which we have PTST
data, the two most dominant senses are identified,
say s1 and s2. If the number of sentences annotated
with s1 and s2 is x and y, respectively, where x > y,
then all y sentences of s2 and the first y sentences
of s1 are placed in a data bin. Eventually the bin
contains an equal number of PTST sentences for
the two most dominant senses of each target word.
Our data bin contained 17,446 sentences for 27
nouns, verbs, and adjectives. We then generate dif-
ferent test data sets d? from the bin, where ? takes
values 0; :1; :2; : : : ;1, such that the fraction of sen-
tences annotated with s1 is ? and those with s2 is
1 ?. Thus the data sets have different dominance
values even though they have the same number of
sentences?half as many in the bin.
Each data set d? is given as input to the naive
sense disambiguation system. If the predominant
sense is correctly identified for all target words,
then the system will achieve highest accuracy,
whereas if it is falsely determined for all target
words, then the system achieves the lowest ac-
curacy. The value of ? determines this upper
bound and lower bound. If ? is close to 0:5, then
even if the system correctly identifies the predom-
inant sense, the naive disambiguation system can-
not achieve accuracies much higher than 50%. On
the other hand, if ? is close to 0 or 1, then the
system may achieve accuracies close to 100%. A
disambiguation system that randomly chooses one
of the two possible senses for each occurrence of
the target word will act as the baseline. Note that
no matter what the distribution of the two senses
(?), this system will get an accuracy of 50%.
DI,W (odds), base: .08E,W(odds), bootstrapped: .02D
Mean distance below upper bound
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 upper bound upper bound
baselinebaseline
Ac
cu
rac
y
Distribution (alpha)
DE,W (odds), bootstrapped
(odds), baseDI,W
lower bound lower bound
Figure 5: Best results: base vs. bootstrapped
6.2 Results
Highest accuracies achieved using the four dom-
inance methods and the measures of association
that worked best with each are shown in Figure 4.
The table below the figure shows mean distance
below upper bound (MDUB) for all ? values
considered. Measures that perform almost iden-
tically are grouped together and the MDUB val-
ues listed are averages. The window size used was
5 words around the target word. Each dataset
d?, which corresponds to a different target text in
Figure 2, was processed in less than 1 second on
a 1.3GHz machine with 16GB memory. Weighted
voting methods, DE ;W and DI;W , perform best with
MDUBs of just .02 and .03, respectively. Yule?s
coefficient, odds ratio, and pmi give near-identical,
maximal accuracies for all four methods with a
slightly greater divergence in DI;W , where pmi
does best. The ? coefficient performs best for
unweighted methods. Dice and cosine do only
slightly better than the baseline. In general, re-
sults from the method?measure combinations are
symmetric across ? = 0:5, as they should be.
Marked improvements in accuracy were
achieved as a result of bootstrapping the WCCM
(Figure 5). Most of the gain was provided by
the first iteration itself, whereas further iterations
resulted in just marginal improvements. All
bootstrapped results reported in this paper pertain
to just one iteration. Also, the bootstrapped
WCCM is 72% smaller, and 5 times faster at
processing the data sets, than the base WCCM,
which has many non-zero cells even though the
corresponding word and category never actually
co-occurred (as mentioned in Section 3.2 earlier).
126
6.3 Discussion
Considering that this is a completely unsupervised
approach, not only are the accuracies achieved us-
ing the weighted methods well above the baseline,
but also remarkably close to the upper bound. This
is especially true for ? values close to 0 and 1. The
lower accuracies for ? near 0.5 are understandable
as the amount of evidence towards both senses of
the target word are nearly equal.
Odds, pmi, and Yule perform almost equally
well for all methods. Since the number of times
two words co-occur is usually much less than
the number of times they occur individually, pmi
tends to approximate the logarithm of odds ra-
tio. Also, Yule is a derivative of odds. Thus all
three measures will perform similarly in case the
co-occurring words give an unweighted vote for
the most appropriate sense of the target as in DI;U
and DE ;U . For the weighted voting schemes, DI;W
and DE ;W , the effect of scale change is slightly
higher in DI;W as the weighted votes are summed
over the complete text to determine dominance. In
DE ;W the small number of weighted votes summed
to determine the sense of the target word may be
the reason why performances using pmi, Yule, and
odds do not differ markedly. Dice coefficient and
cosine gave below-baseline accuracies for a num-
ber of sense distributions. This suggests that the
normalization4 to take into account the frequency
of individual events inherent in the Dice and co-
sine measures may not be suitable for this task.
The accuracies of the dominance methods re-
main the same if the target text is partitioned as per
the target word, and each of the pieces is given in-
dividually to the disambiguation system. The av-
erage number of sentences per target word in each
dataset d? is 323. Thus the results shown above
correspond to an average target text size of only
323 sentences.
We repeated the experiments on the base
WCCM after filtering out (setting to 0) cells with
frequency less than 5 to investigate the effect on
accuracies and gain in computation time (propor-
tional to size of WCCM). There were no marked
changes in accuracy but a 75% reduction in size
of the WCCM. Using a window equal to the com-
plete sentence as opposed to 5 words on either
side of the target resulted in a drop of accuracies.
4If two events occur individually a large number of times,
then they must occur together much more often to get sub-
stantial association scores through pmi or odds, as compared
to cosine or the Dice coefficient.
7 Related Work
The WCCM has similarities with latent semantic
analysis, or LSA, and specifically with work by
Schu?tze and Pedersen (1997), wherein the dimen-
sionality of a word?word co-occurrence matrix is
reduced to create a word?concept matrix. How-
ever, there is no non-heuristic way to determine
when the dimension reduction should stop. Fur-
ther, the generic concepts represented by the re-
duced dimensions are not interpretable, i.e., one
cannot determine which concepts they represent
in a given sense inventory. This means that LSA
cannot be used directly for tasks such as unsuper-
vised sense disambiguation or determining seman-
tic similarity of known concepts. Our approach
does not have these limitations.
Yarowsky (1992) uses the product of a mutual
information?like measure and frequency to iden-
tify words that best represent each category in the
Roget?s Thesaurus and uses these words for sense
disambiguation with a Bayesian model. We im-
proved the accuracy of the WCCM using sim-
ple bootstrapping techniques, used all the words
that co-occur with a category, and proposed four
new methods to determine sense dominance?
two of which do explicit sense disambiguation.
Ve?ronis (2005) presents a graph theory?based ap-
proach to identify the various senses of a word in a
text corpus without the use of a dictionary. Highly
interconnected components of the graph represent
the different senses of the target word. The node
(word) with the most connections in a component
is representative of that sense and its associations
with words that occur in a test instance are used as
evidence for that sense. However, these associa-
tions are at best only rough estimates of the associ-
ations between the sense and co-occurring words,
since a sense in his system is represented by a
single (possibly ambiguous) word. Pantel (2005)
proposes a framework for ontologizing lexical re-
sources. For example, co-occurrence vectors for
the nodes in WordNet can be created using the co-
occurrence vectors for words (or lexicals). How-
ever, if a leaf node has a single lexical, then once
the appropriate co-occurring words for this node
are identified (coup phase), they are assigned the
same co-occurrence counts as that of the lexical.5
5A word may have different, stronger-than-chance
strengths of association with multiple senses of a lexical.
These are different from the association of the word with the
lexical.
127
8 Conclusions and Future Directions
We proposed a new method for creating a word?
category co-occurrence matrix (WCCM) using a
published thesaurus and raw text, and applying
simple sense disambiguation and bootstrapping
techniques. We presented four methods to deter-
mine degree of dominance of a sense of a word us-
ing the WCCM. We automatically generated sen-
tences with a target word annotated with senses
from the published thesaurus, which we used to
perform an extensive evaluation of the dominance
methods. We achieved near-upper-bound results
using all combinations of the the weighted meth-
ods (DI;W and DE ;W ) and three measures of asso-
ciation (odds, pmi, and Yule).
We cannot compare accuracies with McCarthy
et al (2004) because use of a thesaurus instead
of WordNet means that knowledge of exactly how
the thesaurus senses map to WordNet is required.
We used a thesaurus as such a resource, unlike
WordNet, is available in more languages, pro-
vides us with coarse senses, and leads to a smaller
WCCM (making computationally intensive oper-
ations viable). Further, unlike the McCarthy et
al. system, we showed that our system gives accu-
rate results without the need for a large similarly-
sense-distributed text or retraining. The target
texts used were much smaller (few hundred sen-
tences) than those needed for automatic creation
of a thesaurus (few million words).
The WCCM has a number of other applications,
as well. The strength of association between a
word and a word sense can be used to determine
the (more intuitive) distributional similarity of
word senses (as opposed to words). Conditional
probabilities of lexical features can be calculated
from the WCCM, which in turn can be used in un-
supervised sense disambiguation. In conclusion,
we provided a framework for capturing distribu-
tional properties of word senses from raw text and
demonstrated one of its uses?determining word
sense dominance.
Acknowledgments
We thank Diana McCarthy, Afsaneh Fazly, and
Suzanne Stevenson for their valuable feedback.
This research is financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and the University of Toronto.
References
Eneko Agirre and O. Lopez de Lacalle Lekuona. 2003.
Clustering WordNet word senses. In Proceedings
of the Conference on Recent Advances on Natural
Language Processing (RANLP?03), Bulgaria.
J.R.L. Bernard, editor. 1986. The Macquarie The-
saurus. Macquarie Library, Sydney, Australia.
Lou Burnard. 2000. Reference Guide for the British
National Corpus (World Edition). Oxford Univer-
sity Computing Services.
Adam Kilgarriff and Colin Yallop. 2001. What?s in
a thesaurus. In Proceedings of the Second Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 1371?1379, Athens, Greece.
Claudia Leacock, Martin Chodrow, and George A.
Miller. 1998. Using corpus statistics and WordNet
relations for sense identification. Computational
Linguistics, 24(1):147?165.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics
(COLING-98), pages 768?773, Montreal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL-04), pages 280?267, Barcelona,
Spain.
Saif Mohammad and Graeme Hirst. Submitted. Dis-
tributional measures as proxies for semantic related-
ness.
Patrick Pantel. 2005. Inducing ontological co-
occurrence vectors. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL-05), pages 125?132, Ann Arbor,
Michigan.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retreival. Information Processing
and Management, 33(3):307?318.
David Sheskin. 2003. The handbook of paramet-
ric and nonparametric statistical procedures. CRC
Press, Boca Raton, Florida.
Jean Ve?ronis. 2005. Hyperlex: Lexical cartography
for information retrieval. To appear in Computer
Speech and Language. Special Issue on Word Sense
Disambiguation.
David Yarowsky. 1992. Word-sense disambiguation
using statistical models of Roget?s categories trained
on large corpora. In Proceedings of the 14th Inter-
national Conference on Computational Linguistics
(COLING-92), pages 454?460, Nantes, France.
128
c? 2002 Association for Computational Linguistics
Near-Synonymy and Lexical Choice
Philip Edmonds? Graeme Hirst?
Sharp Laboratories of Europe Limited University of Toronto
We develop a new computational model for representing the fine-grained meanings of near-
synonyms and the differences between them. We also develop a lexical-choice process that can
decide which of several near-synonyms is most appropriate in a particular situation. This research
has direct applications in machine translation and text generation.
We first identify the problems of representing near-synonyms in a computational lexicon
and show that no previous model adequately accounts for near-synonymy. We then propose a
preliminary theory to account for near-synonymy, relying crucially on the notion of granularity
of representation, in which the meaning of a word arises out of a context-dependent combination
of a context-independent core meaning and a set of explicit differences to its near-synonyms. That
is, near-synonyms cluster together.
We then develop a clustered model of lexical knowledge, derived from the conventional on-
tological model. The model cuts off the ontology at a coarse grain, thus avoiding an awkward
proliferation of language-dependent concepts in the ontology, yet maintaining the advantages
of efficient computation and reasoning. The model groups near-synonyms into subconceptual
clusters that are linked to the ontology. A cluster differentiates near-synonyms in terms of fine-
grained aspects of denotation, implication, expressed attitude, and style. The model is general
enough to account for other types of variation, for instance, in collocational behavior.
An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a
clustered model of lexical knowledge. To make it work, we formalize criteria for lexical choice
as preferences to express certain concepts with varying indirectness, to express attitudes, and
to establish certain styles. The lexical-choice process itself works on two tiers: between clusters
and between near-synonyns of clusters. We describe our prototype implementation of the system,
called I-Saurus.
1. Introduction
A word can express a myriad of implications, connotations, and attitudes in addition
to its basic ?dictionary? meaning. And a word often has near-synonyms that differ
from it solely in these nuances of meaning. So, in order to find the right word to
use in any particular situation?the one that precisely conveys the desired meaning
and yet avoids unwanted implications?one must carefully consider the differences
between all of the options. Choosing the right word can be difficult for people, let
alone present-day computer systems.
For example, how can a machine translation (MT) system determine the best En-
glish word for the French be?vue when there are so many possible similar but slightly
? Sharp Laboratories of Europe Limited, Oxford Science Park, Edmund Halley Road, Oxford OX4 4GB,
England. E-mail: phil@sharp.co.uk.
? Department of Computer Science, University of Toronto, Ontario, Canada M5S 3G4. E-mail:
gh@cs.toronto.edu.
106
Computational Linguistics Volume 28, Number 2
different translations? The system could choose error, mistake, blunder, slip, lapse, boner,
faux pas, boo-boo, and so on, but the most appropriate choice is a function of how be?vue
is used (in context) and of the difference in meaning between be?vue and each of the En-
glish possibilities. Not only must the system determine the nuances that be?vue conveys
in the particular context in which it has been used, but it must also find the English
word (or words) that most closely convey the same nuances in the context of the other
words that it is choosing concurrently. An exact translation is probably impossible, for
be?vue is in all likelihood as different from each of its possible translations as they are
from each other. That is, in general, every translation possibility will omit some nuance
or express some other possibly unwanted nuance. Thus, faithful translation requires
a sophisticated lexical-choice process that can determine which of the near-synonyms
provided by one language for a word in another language is the closest or most
appropriate in any particular situation. More generally, a truly articulate natural lan-
guage generation (NLG) system also requires a sophisticated lexical-choice process.
The system must to be able to reason about the potential effects of every available
option.
Consider, too, the possibility of a new type of thesaurus for a word processor that,
instead of merely presenting the writer with a list of similar words, actually assists
the writer by ranking the options according to their appropriateness in context and
in meeting general preferences set by the writer. Such an intelligent thesaurus would
greatly benefit many writers and would be a definite improvement over the simplistic
thesauri in current word processors.
What is needed is a comprehensive computational model of fine-grained lexical
knowledge. Yet alhough synonymy is one of the fundamental linguistic phenomena
that influence the structure of the lexicon, it has been given far less attention in lin-
guistics, psychology, lexicography, semantics, and computational linguistics than the
equally fundamental and much-studied polysemy. Whatever the reasons?philosophy,
practicality, or expedience?synonymy has often been thought of as a ?non-problem?:
either there are synonyms, but they are completely identical in meaning and hence
easy to deal with, or there are no synonyms, in which case each word can be handled
like any other. But our investigation of near-synonymy shows that it is just as com-
plex a phenomenon as polysemy and that it inherently affects the structure of lexical
knowledge.
The goal of our research has been to develop a computational model of lexical
knowledge that can adequately account for near-synonymy and to deploy such a
model in a computational process that could ?choose the right word? in any situa-
tion of language production. Upon surveying current machine translation and natural
language generation systems, we found none that performed this kind of genuine
lexical choice. Although major advances have been made in knowledge-based mod-
els of the lexicon, present systems are concerned more with structural paraphrasing
and a level of semantics allied to syntactic structure. None captures the fine-grained
meanings of, and differences between, near-synonyms, nor the myriad of criteria in-
volved in lexical choice. Indeed, the theories of lexical semantics upon which present-
day systems are based don?t even account for indirect, fuzzy, or context-dependent
meanings, let alne near-synonymy. And frustratingly, no one yet knows how to
implement the theories that do more accurately predict the nature of word mean-
ing (for instance, those in cognitive linguistics) in a computational system (see Hirst
[1995]).
In this article, we present a new model of lexical knowledge that explicitly accounts
for near-synonymy in a computationally implementable manner. The clustered model
of lexical knowledge clusters each set of near-synonyms under a common, coarse-
107
Edmonds and Hirst Near-Synonymy and Lexical Choice
grained meaning and provides a mechanism for representing finer-grained aspects of
denotation, attitude, style, and usage that differentiate the near-synonyms in a cluster.
We also present a robust, efficient, and flexible lexical-choice algorithm based on the
approximate matching of lexical representations to input representations. The model
and algorithm are implemented in a sentence-planning system called I-Saurus, and
we give some examples of its operation.
2. Near-Synonymy
2.1 Absolute and Near-Synonymy
Absolute synonymy, if it exists at all, is quite rare. Absolute synonyms would be able
to be substituted one for the other in any context in which their common sense is
denoted with no change to truth value, communicative effect, or ?meaning? (however
?meaning? is defined). Philosophers such as Quine (1951) and Goodman (1952) argue
that true synonymy is impossible, because it is impossible to define, and so, perhaps
unintentionally, dismiss all other forms of synonymy. Even if absolute synonymy were
possible, pragmatic and empirical arguments show that it would be very rare. Cruse
(1986, page 270) says that ?natural languages abhor absolute synonyms just as nature
abhors a vacuum,? because the meanings of words are constantly changing. More for-
mally, Clark (1992) employs her principle of contrast, that ?every two forms contrast
in meaning,? to show that language works to eliminate absolute synonyms. Either an
absolute synonym would fall into disuse or it would take on a new nuance of mean-
ing. At best, absolute synonymy is limited mostly to dialectal variation and technical
terms (underwear (AmE) : pants (BrE); groundhog : woodchuck; distichous : two-ranked; ple-
sionym : near-synonym), but even these words would change the style of an utterance
when intersubstituted.
Usually, words that are close in meaning are near-synonyms (or plesionyms)1?
almost synonyms, but not quite; very similar, but not identical, in meaning; not fully
intersubstitutable, but instead varying in their shades of denotation, connotation, im-
plicature, emphasis, or register (DiMarco, Hirst, and Stede 1993).2 Section 4 gives a
more formal definition.
Indeed, near-synonyms are pervasive in language; examples are easy to find. Lie,
falsehood, untruth, fib, and misrepresentation, for instance, are near-synonyms of one
another. All denote a statement that does not conform to the truth, but they differ
from one another in fine aspects of their denotation. A lie is a deliberate attempt
to deceive that is a flat contradiction of the truth, whereas a misrepresentation may
be more indirect, as by misplacement of emphasis, an untruth might be told merely
out of ignorance, and a fib is deliberate but relatively trivial, possibly told to save
one?s own or another?s face (Gove 1984). The words also differ stylistically; fib is an
informal, childish term, whereas falsehood is quite formal, and untruth can be used
euphemistically to avoid some of the derogatory implications of some of the other
terms (Gove [1984]; compare Coleman and Kay?s [1981] rather different analysis). We
will give many more examples in the discussion below.
1 In some of our earlier papers, we followed Cruse (1986) in using the term plesionym for near-synonym,
the prefix plesio- meaning ?near?. Here, we opt for the more-transparent terminology. See Section 4 for
discussion of Cruse?s nomenclature.
2 We will not add here to the endless debate on the normative differentiation of the near-synonyms
near-synonym and synonym (Egan 1942; Sparck Jones 1986; Cruse 1986; Church et al 1994). It is
sufficient for our purposes at this point to simply say that we will be looking at sets of words that are
intuitively very similar in meaning but cannot be intersubstituted in most contexts without changing
some semantic or pragmatic aspect of the message.
108
Computational Linguistics Volume 28, Number 2
Error implies a straying from a proper course and suggests guilt as may lie in failure to take
proper advantage of a guide. . . . Mistake implies misconception, misunderstanding, a wrong
but not always blameworthy judgment, or inadvertence; it expresses less severe criticism than
error. Blunder is harsher than mistake or error; it commonly implies ignorance or stupidity, some-
times blameworthiness. Slip carries a stronger implication of inadvertence or accident than mis-
take, and often, in addition, connotes triviality. Lapse, though sometimes used interchangeably
with slip, stresses forgetfulness, weakness, or inattention more than accident; thus, one says a
lapse of memory or a slip of the pen, but not vice versa. Faux pas is most frequently applied to
a mistake in etiquette. Bull, howler, and boner are rather informal terms applicable to blunders
that typically have an amusing aspect.
Figure 1
An entry (abridged) from Webster?s New Dictionary of Synonyms (Gove 1984).
2.2 Lexical Resources for Near-Synonymy
It can be difficult even for native speakers of a language to command the differences
between near-synonyms well enough to use them with invariable precision, or to ar-
ticulate those differences even when they are known. Moreover, choosing the wrong
word can convey an unwanted implication. Consequently, lexicographers have com-
piled many reference books (often styled as ?dictionaries of synonyms?) that explicitly
discriminate between members of near-synonym groups. Two examples that we will
cite frequently are Webster?s New Dictionary of Synonyms (Gove 1984), which discrimi-
nates among approximately 9,000 words in 1,800 near-synonym groups, and Choose the
Right Word (Hayakawa 1994), which covers approximately 6,000 words in 1,000 groups.
The nuances of meaning that these books adduce in their entries are generally much
more subtle and fine-grained than those of standard dictionary definitions. Figure 1
shows a typical entry from Webster?s New Dictionary of Synonyms, which we will use as
a running example. Similar reference works include Bailly (1970), Be?nac (1956), Fer-
nald (1947), Fujiwara, Isogai, and Muroyama (1985), Room (1985), and Urdang (1992),
and usage notes in dictionaries often serve a similar purpose. Throughout this article,
examples that we give of near-synonyms and their differences are taken from these
references.
The concept of difference is central to any discussion of near-synonyms, for if two
putative absolute synonyms aren?t actually identical, then there must be something
that makes them different. For Saussure (1916, page 114), difference is fundamental to
the creation and demarcation of meaning:
In a given language, all the words which express neighboring ideas help define
one another?s meaning. Each of a set of synonyms like redouter (?to dread?),
craindre (?to fear?), avoir peur (?to be afraid?) has its particular value only because
they stand in contrast with one another. . . . No word has a value that can be
identified independently of what else is in its vicinity.
There is often remarkable complexity in the differences between near-synonyms.3
Consider again Figure 1. The near-synonyms in the entry differ not only in the ex-
pression of various concepts and ideas, such as misconception and blameworthiness,
but also in the manner in which the concepts are conveyed (e.g., implied, suggested,
3 This contrasts with Markman and Gentner?s work on similarity (Markman and Gentner 1993; Gentner
and Markman 1994), which suggests that the more similar two items are, the easier it is to represent
their differences.
109
Edmonds and Hirst Near-Synonymy and Lexical Choice
Table 1
Examples of near-synonymic variation.
Type of variation Example
Abstract dimension seep : drip
Emphasis enemy : foe
Denotational, indirect error : mistake
Denotational, fuzzy woods : forest
Stylistic, formality pissed : drunk : inebriated
Stylistic, force ruin : annihilate
Expressed attitude skinny : thin : slim, slender
Emotive daddy : dad : father
Collocational task : job
Selectional pass away : die
Subcategorization give : donate
expressed, connoted, and stressed), in the frequency with which they are conveyed
(e.g., commonly, sometimes, not always), and in the degree to which they are conveyed
(e.g., in strength).
2.3 Dimensions of Variation
The previous example illustrates merely one broad type of variation, denotational
variation. In general, near-synonyms can differ with respect to any aspect of their
meaning (Cruse 1986):
? denotational variations, in a broad sense, including propositional, fuzzy,
and other peripheral aspects
? stylistic variations, including dialect and register
? expressive variations, including emotive and attitudinal aspects
? structural variations, including collocational, selectional, and syntactic
variations
Building on an earlier analysis by DiMarco, Hirst, and Stede (1993) of the types of
differentiae used in synonym discrimination dictionaries, Edmonds (1999) classifies
near-synonymic variation into 35 subcategories within the four broad categories above.
Table 1 gives a number of examples, grouped into the four broad categories above,
which we will now discuss.
2.3.1 Denotational Variations. Several kinds of variation involve denotation, taken in
a broad sense.4 DiMarco, Hirst, and Stede (1993) found that whereas some differen-
tiae are easily expressed in terms of clear-cut abstract (or symbolic) features such as
4 The classic opposition of denotation and connotation is not precise enough for our needs here. The
denotation of a word is its literal, explicit, and context-independent meaning, whereas its connotation
is any aspect that is not denotational, including ideas that color its meaning, emotions, expressed
attitudes, implications, tone, and style. Connotation is simply too broad and ambiguous a term. It often
seems to be used simply to refer to any aspect of word meaning that we don?t yet understand well
enough to formalize.
110
Computational Linguistics Volume 28, Number 2
continuous/intermittent (Wine {seeped | dripped} from the barrel), many are not. In fact,
denotational variation involves mostly differences that lie not in simple features but
in full-fledged concepts or ideas?differences in concepts that relate roles and aspects
of a situation. For example, in Figure 1, ?severe criticism? is a complex concept that
involves both a criticizer and a criticized, the one who made the error. Moreover, two
words can differ in the manner in which they convey a concept. Enemy and foe, for
instance, differ in the emphasis that they place on the concepts that compose them,
the former stressing antagonism and the latter active warfare rather than emotional
reaction (Gove 1984).
Other words convey meaning indirectly by mere suggestion or implication. There
is a continuum of indirectness from suggestion to implication to denotation; thus slip
?carries a stronger implication of inadvertence? than mistake. Such indirect meanings
are usually peripheral to the main meaning conveyed by an expression, and it is usu-
ally difficult to ascertain definitively whether or not they were even intended to be
conveyed by the speaker; thus error merely ?suggests guilt? and a mistake is ?not al-
ways blameworthy.? Differences in denotation can also be fuzzy, rather than clear-cut.
The difference between woods and forest is a complex combination of size, primitive-
ness, proximity to civilization, and wildness.5
2.3.2 Stylistic Variations. Stylistic variation involves differences in a relatively small,
finite set of dimensions on which all words can be compared. Many stylistic dimen-
sions have been proposed by Hovy (1988), Nirenburg and Defrise (1992), Stede (1993),
and others. Table 1 illustrates two of the most common dimensions: inebriated is formal
whereas pissed is informal; annihilate is a more forceful way of saying ruin.
2.3.3 Expressive Variations. Many near-synonyms differ in their marking as to the
speaker?s attitude to their denotation: good thing or bad thing. Thus the same person
might be described as skinny, if the speaker wanted to be deprecating or pejorative,
slim or slender, if he wanted to be more complimentary, or thin if he wished to be
neutral. A hindrance might be described as an obstacle or a challenge, depending upon
how depressed or inspired the speaker felt about the action that it necessitated.6 A
word can also indirectly express the emotions of the speaker in a possibly finite set
of emotive ?fields?; daddy expresses a stronger feeling of intimacy than dad or father.
Some words are explicitly marked as slurs; a slur is a word naming a group of people,
the use of which implies hatred or contempt of the group and its members simply by
virtue of its being marked as a slur.
2.3.4 Structural Variations. The last class of variations among near-synonyms involves
restrictions upon deployment that come from other elements of the utterance and, re-
ciprocally, restrictions that they place upon the deployment of other elements. In either
case, the restrictions are independent of the meanings of the words themselves.7 The
5 ?A ?wood? is smaller than a ?forest?, is not so primitive, and is usually nearer to civilization. This
means that a ?forest? is fairly extensive, is to some extent wild, and on the whole not near large towns
or cities. In addition, a ?forest? often has game or wild animals in it, which a ?wood? does not, apart
from the standard quota of regular rural denizens such as rabbits, foxes and birds of various kinds?
(Room 1985, page 270).
6 Or, in popular psychology, the choice of word may determine the attitude: ?[Always] substitute
challenge or opportunity for problem. . . . Instead of saying I?m afraid that?s going to be a problem, say That
sounds like a challenging opportunity? (Walther 1992, page 36).
7 It could be argued that words that differ only in these ways should count not merely as near-synonyms
but as absolute synonyms.
111
Edmonds and Hirst Near-Synonymy and Lexical Choice
restrictions may be either collocational, syntactic, or selectional?that is, dependent ei-
ther upon other words or constituents in the utterance or upon other concepts denoted.
Collocational variation involves the words or concepts with which a word can be
combined, possibly idiomatically. For example, task and job differ in their collocational
patterns: one can face a daunting task but not ?face a daunting job. This is a lexical restric-
tion, whereas in selectional restrictions (or preferences) the class of acceptable objects
is defined semantically, not lexically. For example, unlike die, pass away may be used
only of people (or anthropomorphized pets), not plants or animals: ?Many cattle passed
away in the drought.
Variation in syntactic restrictions arises from differing syntactic subcategorization.
It is implicit that if a set of words are synonyms or near-synonyms, then they are
of the same syntactic category.8 Some of a set of near-synonyms, however, might be
subcategorized differently from others. For example, the adjective ajar may be used
predicatively, not attributively (The door is ajar; ?the ajar door), whereas the adjective
open may be used in either position. Similarly, verb near-synonyms (and their nomi-
nalizations) may differ in their verb class and in the alternations that they they may
undergo (Levin 1993). For example, give takes the dative alternation, whereas donate
does not: Nadia gave the Van Gogh to the museum; Nadia gave the museum the Van Gogh;
Nadia donated the Van Gogh to the museum; ?Nadia donated the museum the Van Gogh.
Unlike the other kinds of variation, collocational, syntactic, and selectional varia-
tions have often been treated in the literature on lexical choice, and so we will have
little more to say about them here.
2.4 Cross-Linguistic Near-Synonymy
Near-synonymy rather than synonymy is the norm in lexical transfer in translation:
the word in the target language that is closest to that in the source text might be a
near-synonym rather than an exact synonym. For example, the German word Wald is
similar in meaning to the English word forest, but Wald can denote a rather smaller
and more urban area of trees than forest can; that is, Wald takes in some of the English
word woods as well, and in some situations, woods will be a better translation of Wald
than forest. Similarly, the German Geho?lz takes in the English copse and the ?smaller?
part of woods. We can think of Wald, Geho?lz, forest, woods, and copse as a cross-linguistic
near-synonym group.
Hence, as with a group of near-synonyms from a single language, we can speak
of the differences in a group of cross-linguistic near-synonyms. And just as there are
reference books to advise on the near-synonym groups of a single language, there
are also books to advise translators and advanced learners of a second language on
cross-linguistic near-synonymy. As an example, we show in Figures 2 and 3 (abridge-
ments of) the entries in Farrell (1977) and Batchelor and Offord (1993) that explicate,
from the perspective of translation to and from English, the German and French near-
synonym clusters that correspond to the English cluster for error that we showed in
Figure 1.
2.5 Summary
We know that near-synonyms can often be intersubstituted with no apparent change
of effect on a particular utterance, but, unfortunately, the context-dependent nature
8 A rigorous justification of this point would run to many pages, especially for near-synonyms. For
example, it would have to be argued that the verb sleep and the adjective asleep are not merely
near-synonyms that just happen to differ in their syntactic categories, even though the sentences Emily
sleeps and Emily is asleep are synonymous or nearly so.
112
Computational Linguistics Volume 28, Number 2
MISTAKE, ERROR. Fehler is a definite imperfection in a thing which ought not to be there. In
this sense, it translates both mistake and error. Irrtum corresponds to mistake only in the sense of
?misunderstanding?, ?misconception?, ?mistaken judgment?, i.e. which is confined to the mind,
not embodied in something done or made. [footnote:] Versehen is a petty mistake, an oversight,
a slip due to inadvertence. Mi?griff and Fehlgriff are mistakes in doing a thing as the result
of an error in judgment.
Figure 2
An entry (abridged) from Dictionary of German Synonyms (Farrell 1977).
impair (3) blunder, error
be?vue (3?2) blunder (due to carelessness or ignorance)
faux pas (3?2) mistake, error (which affects a person adversely socially or in his/her career,
etc)
bavure (2) unfortunate error (often committed by the police)
be?tise (2) stupid error, stupid words
gaffe (2?1) boob, clanger
Figure 3
An entry (abridged) from Using French Synonyms (Batchelor and Offord 1993). The
parenthesized numbers represent formality level from 3 (most formal) to 1 (least formal).
of lexical knowledge is not very well understood as yet. Lexicographers, for in-
stance, whose job it is to categorize different uses of a word depending on context,
resort to using mere ?frequency? terms such as sometimes and usually (as in Fig-
ure 1). Thus, we cannot yet make any claims about the influence of context on near-
synonymy.
In summary, to account for near-synonymy, a model of lexical knowledge will
have to incorporate solutions to the following problems:
? The four main types of variation are qualitatively different, so each must
be separately modeled.
? Near-synonyms differ in the manner in which they convey concepts,
either with emphasis or indirectness (e.g., through mere suggestion
rather than denotation).
? Meanings, and hence differences among them, can be fuzzy.
? Differences can be multidimensional. Only for clarity in our above
explication of the dimensions of variation did we try to select examples
that highlighted a single dimension. However, as Figure 1 shows, blunder
and mistake, for example, actually differ on several denotational
dimensions as well as on stylistic and attitudinal dimensions.
? Differences are not just between simple features but involve concepts
that relate roles and aspects of the situation.
? Differences often depend on the context.
3. Near-Synonymy in Computational Models of the Lexicon
Clearly, near-synonymy raises questions about fine-grained lexical knowledge repre-
sentation. But is near-synonymy a phenomenon in its own right warranting its own
113
Edmonds and Hirst Near-Synonymy and Lexical Choice
legs=2
Bird
elegant
Peacock
gray
elegant
Junco
legs=4
smart
Dog
legs=0,2,4
live-bearing
Mammal
elegant
legs=4
Cat
legs=2
smart
Human
blue-green
egg-laying
Animal
Pfau
juncocat
puss
Katze
Mieze
peacock
bird
Hund
spuglet
animal
human
person
Mensch
Person
Tier
S?ugetier
mammal
hound
dog
Junko
Vogel
Figure 4
A simplistic hierarchy of conceptual schemata with connections to their lexical entries for
English and German.
special account, or does it suffice to treat near-synonyms the same as widely differing
words? We will argue now that near-synonymy is indeed a separately characterizable
phenomenon of word meaning.
Current models of lexical knowledge used in computational systems, which are
based on decompositional and relational theories of word meaning (Katz and Fodor
1963; Jackendoff 1990; Lyons 1977; Nirenburg and Defrise 1992; Lehrer and Kittay 1992;
Evens 1988; Cruse 1986), cannot account for the properties of near-synonyms. In these
models, the typical view of the relationship between words and concepts is that each
element of the lexicon is represented as a conceptual schema or a structure of such
schemata. Each word sense is linked to the schema or the conceptual structure that it
lexicalizes. If two or more words denote the same schema or structure, all of them are
connected to it; if a word is ambiguous, subentries for its different senses are connected
to their respective schemata. In this view, then, to understand a word in a sentence
is to find the schema or schemata to which it is attached, disambiguate if necessary,
and add the result to the output structure that is being built to represent the sentence.
Conversely, to choose a word when producing an utterance from a conceptual structure
is to find a suitable set of words that ?cover? the structure and assemble them into a
sentence in accordance with the syntactic and pragmatic rules of the language (Nogier
and Zock 1992; Stede 1999).
A conceptual schema in models of this type is generally assumed to contain a
set of attributes or attribute?value pairs that represent the content of the concept and
differentiate it from other concepts. An attribute is itself a concept, as is its value. The
conceptual schemata are themselves organized into an inheritance hierarchy, taxon-
omy, or ontology; often, the ontology is language-independent, or at least language-
neutral, so that it can be used in multilingual applications. Thus, the model might look
114
Computational Linguistics Volume 28, Number 2
fib
untruth
mensonge
menteriemisrepresentation
contrev?rit?
lie
Untrue-Assertion
Accidental-Untruth
Accidental-Contrary-Untruth
Indirect-Deliberate-Untruth
Direct-Deliberate-Untruth
Deliberate-Untruth
Small-Joking-Untruth
Small-Face-Saving-Deliberate-Untruth
Figure 5
One possible hierarchy for the various English and French words for untrue assertions.
Adapted from Hirst (1995).
like the simplified fragment shown in Figure 4. In the figure, the rectangles represent
concept schemata with attributes; the arrows between them represent inheritance. The
ovals represent lexical entries in English and German; the dotted lines represent their
connection to the concept schemata.9
Following Frege?s (1892) or Tarski?s (1944) truth-conditional semantics, the concept
that a lexical item denotes in such models can be thought of as a set of features that are
individually necessary and collectively sufficient to define the concept. Such a view
greatly simplifies the word?concept link. In a text generation system, for instance, the
features amount to the necessary applicability conditions of a word; that is, they have
to be present in the input in order for the word to be chosen. Although such models
have been successful in computational systems, they are rarely pushed to represent
near-synonyms. (The work of Barnett, Mani, and Rich [1994] is a notable exception;
they define a relation of semantic closeness for comparing the denotations of words
and expressions; see Section 9.) They do not lend themselves well to the kind of fine-
grained and often fuzzy differentiation that we showed earlier to be found in near-
synonymy, because, in these models, except as required by homonymy and absolute
synonymy, there is no actual distinction between a word and a concept: each member
of a group of near-synonyms must be represented as a separate concept schema (or
group of schemata) with distinct attributes or attribute values. For example, Figure 5
shows one particular classification of the fib group of near-synonyms in English and
French.10 A similar proliferation of concepts would be required for various error clusters
(as shown earlier in Figures 1, 2, and 3).
9 This outline is intended as a syncretism of many models found in the interdisciplinary literature and is
not necessarily faithful to any particular one. For examples, see the papers in Evens (1988) (especially
Sowa [1988]) and in Pustejovsky and Bergler (1992) (especially Nirenburg and Levin [1992], Sowa
[1992], and Burkert and Forster [1992]); for a theory of lexico-semantic taxonomies, see Kay (1971). For
a detailed construction of the fundamental ideas, see Barsalou (1992); although we use the term schema
instead of frame, despite Barsalou?s advice to the contrary, we tacitly accept most elements of his
model. For bilingual aspects, see Kroll and de Groot (1997).
10 We do not claim that a bilingual speaker necessarily stores words and meanings from different
languages together. In this model, if the concepts are taken to be language independent, then it does
not matter if one overarching hierarchy or many distinct hierarchies are used. It is clear, however, that
cross-linguistic near-synonyms do not have exactly the same meanings and so require distinct concepts
in this model.
115
Edmonds and Hirst Near-Synonymy and Lexical Choice
Although some systems have indeed taken this approach (Emele et al 1992), this
kind of fragmentation is neither easy nor natural nor parsimonious. Hirst (1995) shows
that even simple cases lead to a multiplicity of nearly identical concepts, thereby
defeating the purpose of a language-independent ontology. Such a taxonomy cannot
efficiently represent the multidimensional nature of near-synonymic variation, nor can
it account for fuzzy differences between near-synonyms. And since the model defines
words in terms of only necessary and sufficient truth-conditions, it cannot account
for indirect expressions of meaning and for context-dependent meanings, which are
clearly not necessary features of a word?s meaning.
Moreover, a taxonomic hierarchy emphasizes hyponymy, backgrounding all other
relations, which appear to be more important in representing the multidimensional
nature of fine-grained word meaning. It is not even clear that a group of synonyms
can be structured by hyponymy, except trivially (and ineffectively) as hyponyms all of
the same concept.
The model also cannot easily or tractably account for fuzzy differences or the full-
fledged concepts required for representing denotational variation. First-order logic,
rather than the description logic generally used in ontological models, would at least
be required to represent such concepts, but reasoning about the concepts in lexical
choice and other tasks would then become intractable as the model was scaled up to
represent all near-synonyms.
In summary, present-day models of the lexicon have three kinds of problems with
respect to near-synonymy and fine-grained lexical knowledge: the adequacy of cov-
erage of phenomena related to near-synonymy; engineering, both in the design of
an efficient and robust lexical choice process and in the design of lexical entries for
near-synonyms; and the well-known issues of tractability of reasoning about concepts
during natural language understanding and generation.
Nevertheless, at a coarse grain, the ontological model does have practical and the-
oretical advantages in efficient paraphrasing, lexical choice, and mechanisms for infer-
ence and reasoning. Hence, to build a new model of lexical knowledge that takes into
account the fine-grainedness of near-synonymy, a logical way forward is to start with
the computationally proven ontological model and to modify or extend it to account
for near-synonymy. The new model that we will present below will rely on a much
more coarsely grained ontology. Rather than proliferating conceptual schemata to ac-
count for differences between near-synonyms, we will propose that near-synonyms
are connected to a single concept, despite their differences in meaning, and are dif-
ferentiated at a subconceptual level. In other words, the connection of two or more
words to the same schema will not imply synonymy but only near-synonymy. Dif-
ferentiation between the near-synonyms?the fine tuning?will be done in the lexical
entries themselves.
4. Near-Synonymy and Granularity of Representation
To introduce the notion of granularity to our discussion, we first return to the problem
of defining near-synonymy.
Semanticists such as Ullmann (1962), Cruse (1986), and Lyons (1995) have at-
tempted to define near-synonymy by focusing on ?propositional? meaning. Cruse, for
example, contrasts cognitive synonyms and plesionyms; the former are words that,
when intersubstituted in a sentence, preserve its truth conditions but may change the
expressive meaning, style, or register of the sentence or may involve different idiosyn-
116
Computational Linguistics Volume 28, Number 2
cratic collocations (e.g., violin : fiddle),11 whereas intersubstituting the latter changes
the truth conditions but still yields semantically similar sentences (e.g., misty : foggy).
Although these definitions are important for truth-conditional semantics, they are not
very helpful for us, because plesionymy is left to handle all of the most interesting
phenomena discussed in Section 2. Moreover, a rigorous definition of cognitive syn-
onymy is difficult to come up with, because it relies on the notion of granularity, which
we will discuss below.
Lexicographers, on the other hand, have always treated synonymy as near-
synonymy. They define synonymy in terms of likeness of meaning, disagreeing only
in how broad the definition ought to be. For instance, Roget followed the vague prin-
ciple of ?the grouping of words according to ideas? (Chapman 1992, page xiv). And
in the hierarchical structure of Roget?s Thesaurus, word senses are ultimately grouped
according to proximity of meaning: ?the sequence of terms within a paragraph, far
from being random, is determined by close, semantic relationships? (page xiii). The
lexicographers of Webster?s New Dictionary of Synonyms define a synonym as ?one of
two or more words . . . which have the same or very nearly the same essential mean-
ing. . . . Synonyms can be defined in the same terms up to a certain point? (Egan 1942,
pages 24a?25a). Webster?s Collegiate Thesaurus uses a similar definition that involves the
sharing of elementary meanings, which are ?discrete objective denotations uncolored
by . . . peripheral aspects such as connotations, implications, or quirks of idiomatic
usage? (Kay 1988, page 9a). Clearly, the main point of these definitions is that near-
synonyms must have the same essential meaning but may differ in peripheral or
subordinate ideas. Cruse (1986, page 267) actually refines this idea and suggests that
synonyms (of all types) are words that are identical in ?central semantic traits? and
differ, if at all, only in ?peripheral traits.? But how can we specify formally just how
much similarity of central traits and dissimilarity of peripheral traits is allowed? That
is, just what counts as a central trait and what as a peripheral trait in defining a word?
To answer this question, we introduce the idea of granularity of representation
of word meaning. By granularity we mean the level of detail used to describe or
represent the meanings of a word. A fine-grained representation can encode subtle
distinctions, whereas a coarse-grained representation is crude and glosses over vari-
ation. Granularity is distinct from specificity, which is a property of concepts rather
than representations of concepts. For example, a rather general (unspecific) concept,
say Human, could have, in a particular system, a very fine-grained representation, in-
volving, say, a detailed description of the appearance of a human, references to related
concepts such as Eat and Procreate, and information to distinguish the concept from
other similar concepts such as Animal. Conversely, a very specific concept could have
a very coarse-grained representation, using only very general concepts; we could rep-
resent a Lexicographer at such a coarse level of detail as to say no more than that it
is a physical object.
Near-synonyms can occur at any level of specificity, but crucially it is the fine
granularity of the representations of their meanings that enables one to distinguish
one near-synonym from another. Thus, any definition of near-synonymy that does not
take granularity into account is insufficient. For example, consider Cruse?s cognitive
synonymy, discussed above. On the one hand, at an absurdly coarse grain of rep-
resentation, any two words are cognitive synonyms (because every word denotes a
?thing?). But on the other hand, no two words could ever be known to be cognitive
synonyms, because, even at a fine grain, apparent cognitive synonyms might be fur-
11 What?s the difference between a violin and a fiddle? No one minds if you spill beer on a fiddle.
117
Edmonds and Hirst Near-Synonymy and Lexical Choice
ther distinguishable by a still more fine-grained representation. Thus, granularity is
essential to the concept of cognitive synonymy, as which pairs of words are cognitive
synonyms depends on the granularity with which we represent their propositional
meanings. The same is true of Cruse?s plesionyms. So in the end, it should not be
necessary to make a formal distinction between cognitive synonyms and plesionyms.
Both kinds of near-synonyms should be representable in the same formalism.
By taking granularity into account, we can create a much more useful definition
of near-synonymy, because we can now characterize the difference between essential
and peripheral aspects of meaning. If we can set an appropriate level of granularity,
the essential meaning of a word is the portion of its meaning that is representable
only above that level of granularity, and peripheral meanings are those portions rep-
resentable only below that level.
But what is the appropriate level of granularity, the dividing line between coarse-
grained and fine-grained representations? We could simply use our intuition?or
rather, the intuitions of lexicographers, which are filtered by some amount of ob-
jectivity and experience. Alternatively, from a concern for the representation of lexical
knowledge in a multilingual application, we can view words as (language-specific)
specializations of language-independent concepts. Given a hierarchical organization
of coarse-grained language-independent concepts, a set of near-synonyms is simply a
set of words that all link to the same language-independent concept (DiMarco, Hirst,
and Stede 1993; Hirst 1995). So in this view, near-synonyms share the same proposi-
tional meaning just up to the point in granularity defined by language dependence.
Thus we have an operational definition of near-synonymy: If the same concept has
several reasonable lexicalizations in different languages, then it is a good candidate for
being considered a language-independent concept, its various lexicalizations forming
sets of near-synonyms in each language.12
Granularity also explains why it is more difficult to represent near-synonyms in a
lexicon. Near-synonyms are so close in meaning, sharing all essential coarse-grained
aspects, that they differ, by definition, in only aspects representable at a fine grain.
And these fine-grained representations of differences tend to involve very specific
concepts, typically requiring complex structures of more general concepts that are
difficult to represent and to reason with. The matter is only made more complicated
by there often being several interrelated near-synonyms with interrelated differences.
On the other hand, words that are not near-synonyms?those that are merely similar in
meaning (dog : cat) or not similar at all (dog : hat)?could presumably be differentiated
by concepts at a coarse-grained, and less complex, level of representation.
5. A Model of Fine-Grained Lexical Knowledge
Our discussion of granularity leads us to a new model of lexical knowledge in which
near-synonymy is handled on a separate level of representation from coarse-grained
concepts.
5.1 Outline of the Model
Our model is based on the contention that the meaning of an open-class content word,
however it manifests itself in text or speech, arises out of a context-dependent combina-
tion of a basic inherent context-independent denotation and a set of explicit differences
12 EuroWordNet?s Inter-Lingual-Index (Vossen 1998) links the synsets of different languages in such a
manner, and Resnik and Yarowsky (1999) describe a related notion for defining word senses
cross-lingually.
118
Computational Linguistics Volume 28, Number 2
to its near-synonyms. (We don?t rule out other elements in the combination, but these
are the main two.) Thus, word meaning is not explicitly represented in the lexicon but
is created (or generated, as in a generative model of the lexicon [Pustejovsky 1995])
when a word is used. This theory preserves some aspects of the classical theories?the
basic denotation can be modeled by an ontology?but the rest of a word?s meaning
relies on other nearby words and the context of use (cf. Saussure). In particular, each
word and its near synonyms form a cluster.13
The theory is built on the following three ideas, which follow from our observa-
tions about near-synonymy. First, the meaning of any word, at some level of gran-
ularity, must indeed have some inherent context-independent denotational aspect to
it?otherwise, it would not be possible to define or ?understand? a word in isolation
of context, as one in fact can (as in dictionaries). Second, nuances of meaning, although
difficult or impossible to represent in positive, absolute, and context-independent
terms, can be represented as differences, in Saussure?s sense, between near-synonyms.
That is, every nuance of meaning that a word might have can be thought of as a rela-
tion between the word and one or more of its near-synonyms. And third, differences
must be represented not by simple features or truth conditions, but by structures that
encode relations to the context, fuzziness, and degrees of necessity.
For example, the word forest denotes a geographical tract of trees at a coarse
grain, but it is only in relation to woods, copse, and other near-synonyms that one
can fully understand the significance of forest (i.e., that it is larger, wilder, etc.). The
word mistake denotes any sort of action that deviates from what is correct and also
involves some notion of criticism, but it is only in relation to error and blunder that
one sees that the word can be used to criticize less severely than these alternatives
allow. None of these differences could be represented in absolute terms, because that
would require defining some absolute notion of size, wildness, or severity, which
seems implausible. So, at a fine grain, and only at a fine grain, we make explicit
use of Saussure?s notion of contrast in demarcating the meanings of near-synonyms.
Hence, the theory holds that near-synonyms are explicitly related to each other not
at a conceptual level but at a subconceptual level?outside of the (coarser-grained)
ontology. In this way, a cluster of near-synonyms is not a mere list of synonyms; it
has an internal structure that encodes fine-grained meaning as differences between
lexical entries, and it is situated between a conceptual model (i.e., the ontology) and
a linguistic model.
Thus the model has three levels of representation. Current computational theo-
ries suggest that at least two levels of representation, a conceptual?semantic level
and a syntactic?semantic level, are necessary to account for various lexico-semantic
phenomena in computational systems, including compositional phenomena such as
paraphrasing (see, for instance, Stede?s [1999] model). To account for fine-grained
meanings and near-synonymy, we postulate a third, intermediate level (or a splitting
of the conceptual?semantic level). Thus the three levels are the following:
A conceptual?semantic level.
|
A subconceptual/stylistic?semantic level.
|
A syntactic?semantic level.
13 It is very probable that many near-synonym clusters of a language could be discovered automatically
by applying statistical techniques, such as cluster analysis, on large text corpora. For instance, Church
et al (1994) give some results in this area.
119
Edmonds and Hirst Near-Synonymy and Lexical Choice
French
German
English
d?cr?ter
sommer
soul
English
individual
enjoindre
article
object
entity
thing item
English
commander
ordonner
blunder
mistake
erreur
someone
faute
mortal
faux pas
bavure
slip
b?vue
error
Fehler
Schnitzer
Mi?griff
person
Versehen
b?tise
impair
French
howler
lapse
Irrtum
human
Generic-Order
English
bid
direct
enjoin
command
order
Object
Thing
Generic-Error
PersonActivity
Situation
Figure 6
A clustered model of lexical knowledge
So, taking the conventional ontological model as a starting point, we cut off the
ontology at a coarse grain and cluster near-synonyms under their shared concepts
rather than linking each word to a separate concept. The resulting model is a clus-
tered model of lexical knowledge. On the conceptual?semantic level, a cluster has
a core denotation that represents the essential shared denotational meaning of its
near-synonyms. On the subconceptual/stylistic?semantic level, we represent the fine-
grained differences between the near-synonyms of a cluster in denotation, style, and
expression. At the syntactic?semantic level, syntactic frames and collocational relations
represent how words can be combined with others to form sentences.
Figure 6 depicts a fragment of the clustered model. It shows how the clusters of
the near-synonyms of error, order, person, and object in several languages could be rep-
resented in this model. In the figure, each set of near-synonyms forms a cluster linked
to a coarse-grained concept defined in the ontology: Generic-Error, Generic-Order,
Person, and Object, respectively. Thus, the core denotation of each cluster is the con-
cept to which it points. Within each cluster, the near-synonyms are differentiated at the
subconceptual/stylistic level of semantics, as indicated by dashed lines between the
words in the cluster. (The actual differences are not shown in the figure.) The dashed
lines between the clusters for each language indicate similar cross-linguistic differenti-
120
Computational Linguistics Volume 28, Number 2
error
blunder
CAUSE-OF
CORE
ATTRIBUTE
ACTOR
ATTRIBUTE
DEGREE
ACTOR
ATTRIBUTE
Pejorative
Misconception
Stupidity
Concreteness
high
low
Blameworthiness
mediumlow high
Activity
Person Deviation
Figure 7
The core denotation and some of the peripheral concepts of the cluster of error nouns. The two
large regions, bounded by the solid line and the dashed line, show the concepts (and attitudes
and styles) that can be conveyed by the words error and blunder in relation to each other.
ation between some or all of the words of each cluster. Not all words in a cluster need
be differentiated, and each cluster in each language could have its own ?vocabulary?
for differentiating its near-synonyms, though in practice one would expect an overlap
in vocabulary. The figure does not show the representation at the syntactic?semantic
level. We can now describe the internal structure of a cluster in more detail, starting
with two examples.
Figure 7 depicts part of the representation of the cluster of error nouns (error,
mistake, blunder, . . . ); it is explicitly based on the entry from Webster?s New Dictionary
of Synonyms shown in Figure 1. The core denotation, the shaded region, represents
an activity by a person (the actor) that is a deviation from a proper course.14 In the
model, peripheral concepts are used to represent the denotational distinctions of near-
synonyms. The figure shows three peripheral concepts linked to the core concept:
Stupidity, Blameworthiness, and Misconception. The peripheral concepts represent
that a word in the cluster can potentially express, in relation to its near-synonyms,
the stupidity of the actor of the error, the blameworthiness of the actor (of different
degrees: low, medium, or high), and misconception as cause of the error. The repre-
sentation also contains an expressed attitude, Pejorative, and the stylistic dimension
of Concreteness. (Concepts are depicted as regular rectangles, whereas stylistic di-
mensions and attitudes are depicted as rounded rectangles.) The core denotation and
peripheral concepts together form a directed graph of concepts linked by relations;
14 Specifiying the details of an actual cluster should be left to trained knowledge representation experts,
who have a job not unlike a lexicographer?s. Our model is intended to encode such knowledge once it
is elucidated.
121
Edmonds and Hirst Near-Synonymy and Lexical Choice
CORE
enjoin order
DEGREE
ATTRIBUTE
low highmedium
ACTEE
ACTOR
ATTRIBUTE
highlow medium
ATTRIBUTE
ACTOR ACTEE
SAYER
SAYING
SAYEE
ACTOR
Person
Authority
Official
ActivityPerson
Communicate
Peremptory
Warn
Formality
Perform
Imperative
Figure 8
The core denotation and peripheral concepts of the cluster of order verbs. The two large
regions, bounded by the solid line and the dashed line, show the concepts that can be
conveyed by the words order and enjoin in relation to each other.
the individual concepts and relations are defined in the ontology. But although all
of the near-synonyms in the cluster will convey the concepts in the core denotation,
the peripheral concepts that will be conveyed depend on each near-synonym. This is
depicted by the two large regions in the figure (bounded by the solid line and the
dashed line), which each contain the concepts, styles, and attitudes conveyed by their
associated near-synonyms, blunder and error, respectively. Thus, error conveys a degree
of Blameworthiness compared to the higher degree that blunder conveys; error does
not convey Stupidity whereas blunder does; blunder can also express a Pejorative
attitude toward the actor, but error does not express any attitude; and error and blunder
differ stylistically in their degree of Concreteness. Notice that the attitude connects
to the concept Person, because all attitudes must be directed toward some entity in
the situation. Stylistic dimensions such as Concreteness, on the other hand, are com-
pletely separate from the graph of concepts. Also, the indirectness of expression of
each of the peripheral concepts by each of the near-synonyms is not shown in this di-
agram (but see below). The Appendix gives the complete representation of this cluster
in the formalism of our model.
Similarly, Figure 8 depicts the cluster of order verbs (order, enjoin, command, . . . ),
including three of its peripheral concepts and one stylistic dimension. In this cluster,
the core represents a communication by a person (the sayer) to another person (the
sayee) of an activity that the sayee must perform. The core includes several concepts
that are not actually lexicalized by any of the words in the cluster (e.g., the sayer of the
122
Computational Linguistics Volume 28, Number 2
order) but that nevertheless have to be represented because the peripheral concepts
refer to them. (Such concepts are indicated by dashed rectangles.) The peripheral
concepts represent the idea that a near-synonym can express the authority of the
sayer (with possible values of Official or Peremptory), a warning to the sayee, and
the imperativeness of the activity (with possible values of low, medium, or high). The
figure shows the difference between order (the region bounded by the solid line) and
enjoin (the region bounded by the dashed line).
5.2 Core Denotation
The core denotation of a cluster is the inherent context-independent (and in this formu-
lation of the model, language-neutral) denotation shared by all of its near-synonyms.
The core denotation must be specified at a level of granularity sufficient to form a use-
ful cluster of near-synonyms (i.e., at the right level of granularity so that, for instance,
human and person fall into the same cluster, but dwarf and giant do not; see Section 4).
A core denotation is represented as a directed graph of concepts linked by rela-
tions. The graph can be of arbitrary size, from a single concept (such as Generic-Error)
up to any number of interrelated concepts (as shown in Figures 7 and 8). It must
be specified in enough detail, however, for the peripheral concepts to also be speci-
fied. For instance, in the error cluster, it was not possible to use the simple concept
Generic-Error, because the peripheral concepts of the cluster refer to finer-grained
aspects of the concept (the actor and the deviation); hence we used a finer-grained
representation of the concept.
5.3 Peripheral Concepts
Peripheral concepts form the basic vocabulary of fine-grained denotational distinc-
tions. They are used to represent non-necessary and indirect aspects of word meaning.
That is, they are concepts that might be implied, suggested, emphasized, or otherwise
when a word is used, but not always. For instance, in differentiating the error words, a
lexicographer would first decide that the basic peripheral concepts required might be
?stupidity?, ?blameworthiness?, ?criticism?, ?misconception?, ?accidentalness?, and ?inat-
tention?. Then the lexicographer would proceed to distinguish the near-synonyms in
terms of these concepts, for instance, by specifying that blunder involves a higher
degree of blameworthiness than error.
More formally, peripheral concepts are structures of concepts defined in the same
ontology that core denotations are defined in. In fact, every peripheral concept in a
cluster must ?extend? the core denotation in some way, because, after all, peripheral
concepts represent ideas related to the core meaning of a cluster of near-synonyms.
But peripheral concepts are represented separately from the core denotation.
Moreover, since peripheral concepts are defined in the ontology, they can be rea-
soned about, which, in principle, makes the formalism robust to variation in repre-
sentation. That is, if a lexicographer used, say, ?responsibility? to define mistake and
?blameworthiness? to define blunder, the words could still be compared, because in-
ference would find a connection between ?responsibility? and ?blameworthiness?. See
Section 6.1 below for more discussion on this point.
5.4 Distinctions between Near-Synonyms
Following Hirst (1995), we would like to represent differences explicitly as first-class
objects (so that we can reason about them during processing). While we don?t adopt
an explicit formalism, for reasons of practicality of representation, our implicit for-
malism provides a method for computing explicit differences as needed (as we?ll
see in Section 6.1). Thus we associate with each near-synonym in a cluster a set of
123
Edmonds and Hirst Near-Synonymy and Lexical Choice
Table 2
Examples of distinctions of words.
Denotational distinctions:
Binary: blunder: (usually medium implication Stupidity)
Continuous: blunder: (always medium implication
(Blameworthiness (DEGREE high)))
Discrete: order: (always medium implication
(Authority (ATTRIBUTE (Peremptory))))
Expressive distinctions:
blunder: (always medium pejorative V1)
Stylistic distinctions:
blunder: (high concreteness)
error: (low concreteness)
Note: See the Appendix for the details. In the expressive distinction, V1
is a variable that refers to the actor of the error as specified in the core
denotation, and in the denotational distinction high is a fuzzy set of values
in the range [0, 1].
distinctions that are taken to be relative within the cluster; the cluster establishes the
local frame of reference for comparing them. So a word?s set of distinctions implic-
itly differentiates the word from its near-synonyms. In other words, if one consid-
ers the peripheral concepts, attitudes, styles, and so on, to be dimensions, then the
set of distinctions situates a word in a multidimensional space relative to its near-
synonyms. We define three types of distinction below: denotational, expressive, and
stylistic.
5.4.1 Denotational Distinctions. In our formalism, each denotational distinction refers
to a particular peripheral concept and specifies a value on that dimension, which can
be binary (i.e., is or isn?t expressed), continuous (i.e., takes a possibly fuzzy value in
the range [0, 1]), or discrete (i.e., takes a conceptual structure as a value).
Now, in Section 2.3.1 we observed that indirectness forms a continuum (sugges-
tion, implication, denotation), and, following the method used by lexicographers in near-
synonym guides, points on the continuum are modulated up or down by a strength,
which can take the values weak, medium, or strong. To also account for context depen-
dence at least as well as lexicographers do, we include a measure of the frequency
with which the peripheral concept is conveyed by the word. This can take one of five
values (never, seldom, sometimes, often, always). When the problem of context dependence
is better understood, this part of the formalism will need to be changed.
Thus, a denotational distinction of a word w is a quadruple of components as
follows:
w: (frequency strength indirectness concept)
The first part of Table 2 gives some examples for the distinctions of Figures 7 and 8.
5.4.2 Expressive Distinctions. Since a word can express a speaker?s attitude toward
potentially any entity in a situation, an expressive distinction must include a reference
to the entity. As for the attitude itself, we take a conservative approach, for now, and
124
Computational Linguistics Volume 28, Number 2
define only three possible attitudes: favorable, neutral, and pejorative. Thus, an expressive
distinction has the following form:
w: (frequency strength attitude entity)
Frequency and strength have the same role as above. The entity is actually a reference
(i.e., a variable) to one of the concepts specified in the core denotation of peripheral
concepts. The second part of Table 2 gives an example.
5.4.3 Stylistic Distinctions. Although we take a rather basic approach to representing
stylistic distinctions, that does not imply that style is easy to capture. Style is one of the
most difficult of lexical phenomena to account for, since it affects the text at a pragmatic
level and is highly influenced by context. Since there is as yet no comprehensive theory
of style, our approach is similar to past approaches, such as those of DiMarco and Hirst
(1993), Stede (1993), and Hovy (1988).
Unlike the denotational distinctions discussed above, stylistic features have a
global or absolute quality to them. We can compare all words, whether or not they are
near-synonyms, on various stylistic dimensions, such as formality and concreteness.
Because style is a global aspect of text, a certain style can be (and should be) achieved
by more than just lexical choice; structural choices are just as important (DiMarco and
Hirst 1993). Hence, in defining a set of stylistic dimensions, we must look for global
stylistic features that can be carried not only by words but also by syntactic and larger
text structures. Our stylistic dimensions include, but are not limited to, formality,
force, concreteness, floridity, and familiarity.
Stylistic variation also differs from the other types of variation in being related
solely to the lexeme itself and not to its denotation or conceptual meaning (though
in a deeper sense style is certainly related to meaning). So in representing stylistic
distinctions we don?t have to make any reference to entities or other aspects of the core
denotation or peripheral concepts in a cluster. Thus, we represent a stylistic distinction
as follows:
w: (degree dimension)
where degree can take a value of low, medium, or high (though more values could easily
be added to increase the precision). The third part of Table 2 gives two examples.
6. Lexical Similarity
It is not sufficient merely to represent differences between near-synonyms; we must
also be able to use these representations effectively. For lexical choice, among other
tasks, we need to be able to compare the similarities of pairs of near-synonyms. For
example, in a transfer-based MT system, in order to translate the French word bavure
into English, we need to compare the similarities of at least the three pairs bavure : error,
bavure : mistake, and bavure : blunder and choose the English word whose meaning
is closest to bavure, subject to any constraints arising from the context. And in text
generation or interlingual MT, we need to be able to compare the similarities of each of
several near-synonyms to a particular semantic representation or conceptual structure
in order to choose the one that is closest to it in meaning.
Now, the general problem of measuring the semantic distance between words
or concepts has received much attention. This century, Wittgenstein (1953) formu-
lated the notion of family resemblance?that several things can be related because
125
Edmonds and Hirst Near-Synonymy and Lexical Choice
they overlap with respect to a set of properties, no property being common to all of
the words?which Rosch (1978) then used as the basis for the prototype theory of
meaning. Recent research in computational linguistics has focused more on develop-
ing methods to compute the degree of semantic similarity between any two words,
or, more precisely, between the simple or primitive concepts15 denoted by any two
words.
There are many different similarity measures, which variously use taxonomic lex-
ical hierarchies or lexical-semantic networks, large text corpora, word definitions in
machine-readable dictionaries or other semantic formalisms, or a combination of these
(Dagan, Marcus, and Markovitch 1993; Kozima and Furugori 1993; Pereira, Tishby, and
Lee 1993; Church et al 1994; Grefenstette 1994; Resnik 1995; McMahon and Smith 1996;
Jiang and Conrath 1997; Schu?tze 1998; Lin 1998; Resnik and Diab 2000; Budanitsky
1999; Budanitsky and Hirst 2001, 2002). Unfortunately, these methods are generally un-
helpful in computing the similarity of near-synonyms because the measures lack the
required precision. First, taxonomic hierarchies and semantic networks inherently treat
near-synonyms as absolute synonyms in grouping near-synonyms into single nodes
(e.g., in WordNet). In any case, as we argued in Section 3, taxonomies are inappropriate
for modeling near-synonyms. Second, as we noted in Section 2.2, standard dictionary
definitions are not usually fine-grained enough (they define the core meaning but not
all the nuances of a word) and can even be circular, defining each of several near-
synonyms in terms of the other near-synonyms. And third, although corpus-based
methods (e.g., Lin?s [1998]) do compute different similarity values for different pairs
of near-synonyms of the same cluster, Church et al (1994) and Edmonds (1997) show
that such methods are not yet capable of uncovering the more subtle differences in
the use of near-synonyms for lexical choice.
But one benefit of the clustered model of lexical knowledge is that it naturally
lends itself to the computation of explicit differences or degrees of similarity between
near-synonyms. Although a fully effective similarity measure for near-synonyms still
eludes us, in this section we will characterize the problem and give a solution to one
part of it: computing the similarity of individual lexical distinctions.
6.1 Computing the Similarity of Near-Synonyms
In the clustered model of lexical knowledge, a difference between two near-synonyms
is encoded implicitly in two sets of relative distinctions. From two such sets of distinc-
tions, one can compute, or build, an explicit representation of the difference between
two near-synonyms. Thus, the difference between, or similarity of, two near-synonyms
depends on the semantic content of their representations on the subconceptual/stylistic
level (cf. Resnik and Diab [2000], in which similarity is computed according to the
structure, rather than content, of lexical conceptual structure representations of verbs;
see Jackendoff [1983] and Dorr [1993]).
Comparing two sets of distinctions is not straightforward, however, because, near-
synonyms often differ on seemingly incommensurate dimensions. That is, the dis-
tinctions of one near-synonym will often not match those of another near-synonym,
leaving no basis for comparison. For instance, in Figure 9, bavure and mistake align on
only two of five denotational dimensions (Blameworthiness and Criticism), and this
assumes that each of the near-synonyms was represented using the exact same pe-
15 By primitive concepts, we mean named concepts, or concepts that can be lexicalized by a single word,
even though they may be defined in terms of other concepts in an ontology.
126
Computational Linguistics Volume 28, Number 2
Diff ("bavure" / "mistake") =
(( [usually / unknown] [medium / unknown] [implication / unknown]
(Stupidity (ATTRIBUTE-OF V1)) )
( [always / sometimes] medium implication
(Blameworthiness (ATTRIBUTE-OF V1) (DEGREE [more / ])) )
( always medium implication\\
(Criticism (ACTEE V1) (ATTRIBUTE (Severity (DEGREE [more / ])))) )
( [unknown / always] [unknown / medium] [unknown / implication]
(Misconception (CAUSE-OF V2) (ACTOR V1)) )
( [unknown / always] [unknown / weak] [unknown / implication]
(Accident (CAUSE-OF V2) (ACTOR V1)) )
( [always / unknown] [medium / unknown] [implication / unknown]
(Unfortunate (ATTRIBUTE-OF ROOT)) )
( [usually / always] medium [pejorative / neutral] V1 )
( [more / ] concreteness ) )
Figure 9
A structure that explicitly represents the difference between bavure and mistake. The separate
structures were merged, and where they differed, the two values are shown within square
brackets separated by a /.
ripheral concepts to begin with (i.e., both with Blameworthiness rather than, say,
one with Blameworthiness and the other with a closely related concept such as
Responsibility). Can one even compare an error that is caused by a misconception
to an error that is stupid? (See Figure 3 for bavure.)
When several dimensions are commensurate, how should one compute similarity?
Consider the near-synonyms of forest: Is it possible to decide whether a ?large and
wild? tract of trees is closer to a ?small wild? one or to a ?medium-sized non-wild?
one? In other words, how much of a change in the size of a forest will compensate for
an opposite change in its wildness?
Part of the solution lies in the fact that the dimensions of any cluster are never
actually completely incommensurate; usually they will have interrelationships that can
be both modeled in the ontology and exploited when comparing the representations
of words. For instance, in the cluster of near-synonyms of forest, the wildness of a tract
of trees is related to its size and distance from civilization (which one can infer from
one?s knowledge about forests and wildlife; e.g., most wildlife tries to avoid people);
so there may be a way of comparing a ?wild? tract of trees to a ?large? tract of trees.
And in the error cluster, the dimensions are related in similar ways because of their
semantics and pragmatics (e.g., responsibility leads to blameworthiness, which often
leads to criticism, and stupidity often leads to a pejorative attitude). Certainly these
interrelationships influence both what can be coherently represented in a cluster and
how similar near-synonyms are. And such relationships can be represented in the
knowledge base, and hence reasoned about; a complete model, however, is out of the
scope of this article.
The interaction of the dimensions within a cluster is not yet very well studied, so
for a partial solution, we make the simplifying assumptions that the dimensions of a
cluster are independent and that each can be reduced to a true numeric dimension.16
16 Certainly, numeric values are necessary at some level of representation. As we?ve seen, nuances of
meaning and style are not always clear-cut but can be vague, fuzzy, and continuously variable. Using a
numerical method would seem to be the most intuitive way of computing similarity, which we have to
do to compare and choose appropriate lexical items.
127
Edmonds and Hirst Near-Synonymy and Lexical Choice
Thus, two distinctions d1 and d2 are commensurate if the following two conditions
hold:
? d1 and d2 are of the same type (i.e., stylistic, expressive, or denotational).
? If d1 and d2 are stylistic, then they involve the same stylistic dimension;
if they are expressive, then they refer to the same entity; and if they are
denotational, then they involve the same peripheral concept.
6.2 Computing the Similarity of Distinctions
Given our simplifications from above, a word?s set of distinctions situates it in a numeric
multidimensional space. Consider a function Sim: D ? D ? [0, 1], for computing the
similarity of two commensurate lexical distinctions taken from the set D of all possible
distinctions that can be represented in a particular cluster. A value of 0 means that
the distinctions are completely different (or can?t even be compared), and a value of
1 means that they are equivalent (though not necessarily identical, as two equivalent
distinctions might be structurally different).
Hence, each type of distinction requires its own similarity function:
Sim(d1, d2) =
?
?
?
?
?
?
?
?
?
0.0 if d1 and d2 are not commensurate
Simdenotational(d1, d2) if d1 and d2 are denotational
Simexpressive(d1, d2) if d1 and d2 are expressive
Simstylistic(d1, d2) if d1 and d2 are stylistic
(1)
Each of the similarity functions must compare the values that the pair of distinctions
has on each of their components (see Section 5.4). To arrive at a final numerical value,
we must reduce each component to a real-valued dimension and assign each symbolic
value for that component to a numeric position on the line. Edmonds (1999) gives
complete details of the formulas we developed.
There is, however, a remaining interesting problem: How does one compute the
degree of similarity of two conceptual structures? Denotational distinctions sometimes
involve complex structures of concepts, and these structures must be somehow com-
pared to determine their numeric degree of similarity. For instance, we might need
to decide how similar a high degree of blameworthiness is to a moderate degree of
blameworthiness, or to blameworthiness. Or, we might need to decide how similar
official authority is to peremptory authority, or how similar arbitrary power is to
peremptory authority (where arbitrariness is a kind of peremptoriness and authority
is a kind of power). Computing this type of similarity is clearly different from, but
related to, the problem of computing the similarity of primitive concepts (or words).
We have to consider not only the content but also the structure of the representations.
We are not aware of any research on the general problem of computing the similar-
ity of arbitrary conceptual structures, though some related work has been done in the
area of description logics. Cohen, Borgida, and Hirsh (1992), for example, formalize a
?least common subsumer? operation that returns the largest set of commonalities be-
tween two descriptions. And Resnik and Diab (2000) use a technique, attributed to Lin,
of decomposing a structure into feature sets. Edmonds (1999) describes a technique for
simultaneously traversing a pair of conceptual structures under the assumption that
the structures will be ?similar? because they are commensurate. Still, a good solution
to this problem remains an open issue.
128
Computational Linguistics Volume 28, Number 2
GenerationAnalysis
nuancesnuances
Recover
clusters
Interlingual
rep. Target Text
Express
French
clusters
English
ONTOLOGY
Source Text
Context
instantiates
Figure 10
Lexical analysis and choice in machine translation.
7. Lexical Choice
7.1 Architectures for Lexical Choice
The clustered model of lexical knowledge is applicable to both the lexical-analysis
and lexical-choice phases of a machine translation system. Figure 10 shows that dur-
ing analysis, fine-grained lexical knowledge of the source language is accessed, in
conjunction with the context, to determine possibilities of what is expressed in the
source language text. Then, depending on the type of MT system (i.e., transfer or
interlingual), the appropriate target language words can be chosen: The possibilities
become preferences for choice. Recovering nuances of expression from source text is
currently an open problem, which we do not explore further here (but see Edmonds
[1998] for some preliminary work). In this section we concentrate on the second phase
of MT and show that robust, efficient, flexible, and accurate fine-grained lexical choice
is a natural consequence of a clustered model.
Lexical choice, as we see it, is more than a problem of mapping from concepts to
words, as the previous section might have implied; it is a problem of selecting words
so as to meet or satisfy a large set of possibly conflicting preferences to express certain
nuances in certain ways, to establish the desired style, and to respect collocational
and syntactic constraints. So lexical choice?genuine lexical choice?is making choices
between options rather than merely finding the words for concepts, as was the case in
many early text generation systems (for instance, BABEL [Goldman 1975], MUMBLE
[McDonald 1983], and TEXT [McKeown 1985]). This kind of lexical choice is now
thought to be the central task in text generation (or, at least, sentence generation),
because it interacts with almost every other task involved. Indeed, many recent text
generation systems, including MOOSE (Stede 1999), ADVISOR II (Elhadad, McKeown,
and Robin 1997), and Hunter-Gatherer (Beale et al 1998), among others (see Reiter
and Dale?s [1997] survey), adopt this view, yet their lexical-choice components do not
account for near-synonymy. Without loss of generality, we will look at fine-grained
lexical choice in the context of one of these systems: Stede?s MOOSE (1999).
The input to MOOSE is a ?SitSpec,? that is, a specification of a situation repre-
sented on the conceptual?semantic level as a graph of instances of concepts linked
129
Edmonds and Hirst Near-Synonymy and Lexical Choice
by relations. MOOSE outputs a complete well-formed ?SemSpec,? or semantic speci-
fication on the syntactic?semantic level, from which the Penman sentence realization
system can generate language.17 MOOSE processes the input in two stages. It first
gathers all of the lexical items (as options for choice) whose conceptual?semantic rep-
resentation covers any part of the SitSpec. Then it chooses a set of lexical items that
satisfy Stede?s three criteria for sentence planning: the input SitSpec is completely cov-
ered (and so is completely lexicalized without redundancy); a well-formed SemSpec
can be built out of the partial SemSpecs associated with each of the chosen lexical
items; and as many of the preferences are satisfied as possible. MOOSE supports pref-
erences, but only those that require structural decisions, such as choosing a causative
over inchoative verb alternation. The main goal of Stede?s work was to account for
structural paraphrase in sentence generation, not near-synonymy.
In the general case of sentence planning, given a set of input constraints and
preferences, a sentence planner will make a large number of decisions of different
types?lexical, syntactic, and structural?each of which has the potential to satisfy any,
some, or all of the input preferences (while trying to satisfy the constraints, of course).
It is unlikely that any particular set of preferences can all be satisfied simultaneously,
so some kind of conflict resolution strategy is required in order to manage the decision-
making task. It is not within the scope of this paper to develop solutions to this general
problem (but see Nirenburg, Lesser, and Nyberg [1989], Wanner and Hovy [1996],
Elhadad, McKeown, and Robin [1997], and Stede [1999] for a variety of solutions).
Instead, we will discuss the following two new issues that arise in managing the
interactions between lexical choices that a clustered model brings out:
? We will argue for a unified model for representing any type of
preference for lexical choice.
? We describe a two-tiered model of lexical choice that is the
consequence of a clustered model of lexical knowledge.
Then, we will end the section with a brief description of our software implementation
of the model, called I-Saurus.
7.2 Constraints and Preferences
Simple systems for lexical choice need only to make sure that the denotations of the
words chosen in response to a particular input exactly match the input. But when
we use fine-grained aspects of meaning, the lexical-choice process, and so, in turn, its
input, will ultimately be more complex. But with so many possibilities and options,
choosing from among them will necessarily involve not only degrees of satisfying
various criteria, but also trade-offs among different criteria. Some of the criteria will be
hard constraints (i.e., a SitSpec), ensuring that the basic desired meaning is accurately
conveyed, and others will be preferences.
The main difference between a constraint and a preference is that a preference
is allowed to be satisfied to different degrees, or even not at all, depending on the
decisions that are made during sentence planning. A preference can be satisfied by
17 A SemSpec is a fully lexicalized sentence plan in Penman?s Sentence Plan Language (SPL). SPL is
defined in terms of the Penman Upper Model, a model of meaning at the syntactic?semantic level,
which ensures that the SemSpec is well-formed linguistically. Penman can thus turn any SemSpec into
a well-formed sentence without having to make any open-class lexical decisions (Penman Natural
Language Group 1989; Stede 1999)
130
Computational Linguistics Volume 28, Number 2
a single decision or collectively by a group of decisions.18 And because conflicts and
trade-offs might arise in the satisfaction of several preferences at once, each preference
must have an externally assigned importance factor.
Many types of preference pertain to lexical choice, including emphasizing an as-
pect of an entity in a situation, using normal words or a certain dialect, using words
with a particular phonology (e.g., words that rhyme), using different near-synonyms
for variety or the same word as before for consistency, and so on. All should be
formalizable in a unified model of preference, but we have started with three types
corresponding to the subconceptual level of the clustered model: denotational (or se-
mantic), expressive, and stylistic preferences.
Denotational preferences are distinct from denotational constraints, but there is
no theoretical difference in the nature of a ?preferred? meaning to a ?constrained?
meaning. Hence, we can represent both in the same SitSpec formalism. Thus, a deno-
tational preference is a tuple consisting of a partial SitSpec and a preferred method of
expression, which takes a value on the continuum of indirectness (see Section 5.4). An
expressive preference requests the expression of a certain attitude toward a certain
entity that is part of the situation. Thus, an expressive preference is a tuple consist-
ing of a reference to the entity and the stance that the system should take: favor,
remain neutral, or disfavor. A stylistic preference, for now, is simply a value (of low,
medium, or high) on one of the stylistic dimensions. We will see some examples in
Section 8.
7.2.1 Satisfying Preferences by Lexical Choice. In the best case, it will be possible
to simultaneously satisfy all of the input preferences by choosing appropriate near-
synonyms from appropriate clusters. But if none of the available options will sat-
isfy, to any degree, a particular preference, then that preference is trivially impossible
to satisfy (by lexical choice). But even when there are options available that satisfy
a particular preference, various types of conflicts can arise in trying to satisfy sev-
eral preferences at once, making it impossible to use any of those options. At the
level of clusters, for instance, in choosing a particular cluster in order to satisfy one
preference, we might be therefore unable to satisfy another preference that can be
satisfied only by a different, competing cluster: We might choose the cluster of the
err verbs (to err, to blunder) because of the simplicity or directness of its syntax: John
erred; but we would not be able simultaneously to satisfy a preference for implying
a misconception by choosing, say, mistake from the cluster of error nouns: John made a
mistake.
Similar trade-offs occur when choosing among the near-synonyms of the same
cluster. Such lexical gaps, where no single word can satisfy all of the input preferences
that a cluster can potentially satisfy, are common. For instance, in English, it?s hard
to talk about a mistake without at least some overtones of criticism; in Japanese one
can: with ayamari instead of machigai (Fujiwara, Isogai, and Muroyama 1985). There
is also no near-synonym of error in English that satisfies preferences to imply both
stupidity and misconception; blunder satisfies the former but not the latter, and mistake
vice versa. Similarly, there is no formal word for an untrue statement (i.e., a lie) that
also expresses that the lie is insignificant; fib is an option, but it is not a formal word.
And there is no word for a tract of trees that is both large and not wild; forest has the
former property, woods the latter.
18 A preference is like a floating constraint (Elhadad, McKeown, and Robin 1997) in that it can be
satisfied by different types of decision in sentence planning but differs in that it may be satisfied to
different degrees.
131
Edmonds and Hirst Near-Synonymy and Lexical Choice
Two separate simultaneous choices might also conflict in their satisfaction of a sin-
gle preference. That is, the preference might be satisfied by one choice and negatively
satisfied by another choice. For instance, it might happen that one word is chosen in
order to express a favorable attitude toward a participant in a particular situation and
another word is chosen that inadvertently expresses a pejorative attitude toward the
same person if that second word is chosen in order to satisfy some other preference.
And of course, stylistic decisions can often conflict (e.g., if one has to choose both
formal and informal words).
Our solution to resolving such lexical gaps and conflicting preferences is to use an
approximate matching algorithm that attempts to satisfy collectively as many of the
preferences as possible (each to the highest degree possible) by choosing, on two tiers,
the right words from the right clusters.19 We will describe this model in Section 7.3.
7.2.2 Compatibility of Preferences. But what happens when it is impossible to simul-
taneously satisfy two preferences under any circumstances? We have assumed up to
now that the set of preferences in the input is consistent or well-formed. This is often a
reasonable assumption. In the context of MT, for instance, we can assume that a ?good?
analysis stage would output only well-formed expressions free of incompatibilities.
But two preferences may be incompatible, and we would like our system to be
able to detect such situations. For instance, preferences for both low and high severity
are incompatible; not only is it impossible for a word to simultaneously express both
ideas, but if the system were to attempt to satisfy both, it might output a dissonant
expression20 such as ?I (gently) chided Bill for his (careless) blunder? (the preference
to harshly criticize Bill is satisfied by blunder, and the preference to gently criticize Bill
is satisfied by chide). (Of course, a dissonant expression is not always undesirable; it
might be used for special effect.) This kind of incompatibility is easy to detect in our
formalism, because peripheral concepts are explicitly modeled as dimensions. There
are, of course, other types of incompatibility, such as denotational and contextual
incompatibilities, but we won?t discuss them further here (see Edmonds [1999]).
7.3 Two-Tiered Lexical Choice
Assume now that all of the options for choice have been identified by the system. In
our system, these options are the clusters whose core denotations match part of the
input SitSpec. Ignoring the coverage and well-formedness constraints for now, two
different, mutually constraining types of decision must be made:
? Choosing from several competing cluster options.
? Choosing a near-synonym from a cluster option.
We believe that it is important to separate the processes for making these two types
of decision?even though they must interact?because of their different choice criteria
and effects. The former type involves choosing between options of differing coarse-
grained semantic content and resulting syntactic structure (i.e., paraphrases): clusters
19 A complementary approach is to paraphrase the input and hence explicitly express a preferred
implication or mitigate against an unwanted implication (for instance, by generating insignificant lie
when fib is too informal). A sentence planner, like MOOSE, is designed to generate such structural
paraphrases, so we have concentrated on the lexical issues here.
20 Dissonance is one form of semantic anomaly that Cruse (1986) defines by example: ?Arthur is a
married bachelor.?
132
Computational Linguistics Volume 28, Number 2
have different core denotations, after all. Here, issues of syntactic and semantic style
are involved, as one can choose how the semantic content is to be incorporated. On
the other hand, the latter type of decision involves options that might have subtle
semantic and stylistic differences but result in the same syntactic structure (though
collocational and subcategorization structure can vary).
In other words, lexical choice is a two-tiered process that must find both the
appropriate set of cluster options and the appropriate set of lexical items (one from
each chosen cluster option) whose contributing SemSpec fragments can be unified into
a complete well-formed SemSpec. Of course, many possible SemSpecs can usually be
generated, but the real problem is to find the combination of cluster options and lexical
items that globally satisfy as many of the input preferences as possible.
For instance, Figure 11 depicts the state of processing the SitSpec for the utterance
by John of an untrue statement just before lexical choice occurs. There are four clus-
ter options (denoted by the suffix C): say C and tell-a-lie C match subgraphs of
the SitSpec rooted at say1, untruth C matches the graph rooted at lie1, and John C
matches john1. Now, the system could choose the tell-a-lie C cluster and the John C
cluster, which fully cover the SitSpec, and then choose the words John and lie to come
up with John lies, or the system could choose John and prevaricate for John prevaricates.
The system could also choose the say C, untruth C and John C clusters, and then the
words tell, fib, and John, to end up with John tells a fib. These alternatives?there are
many others?are different in structure, meaning, and style. Which best satisfies the
input preferences, whatever they may be?
We can formally define fine-grained lexical choice (within sentence planning) as
follows. Given an input SitSpec S and a set of compatible preferences P, the goal is to
find a set C of i cluster options and a word wi from each ci ? C such that
? every node of S is covered by exactly one ci
? the partial SemSpecs of all the words wi can be combined into a
well-formed SemSpec SP
? Satisfaction(P, SP) is maximized over all possible SemSpecs
The first criterion ensures complete coverage without redundancy of the input SitSpec,
so the desired meaning, at a coarse grain, is expressed; the second ensures that a
SemSpec can be constructed that will lead to a grammatical sentence; and the third
ensures that the preferences are collectively satisfied as much as is possible by any
sentence plan. The third criterion concerns us here; the first two are dealt with in
MOOSE.
As we said earlier, a thorough understanding of the interacting decisions in lexical
choice is still an open problem, because it is context dependent. Our present solution
is simply to assume that no complex interaction between decisions takes place. So,
assuming that each option has an associated numeric score (the degree to which it
satisfies all of the preferences), we can simply choose the set of options that maximizes
the sum of the scores, subject to the other constraints of building a proper sentence
plan. Thus, we do not provide a solution to how the context affects the combination
of the scores. So, given a sentence plan SP and a set of preferences P, we have
Satisfaction(P, SP) =
?
w?SP
WSat(P, w). (2)
133
Edmonds and Hirst Near-Synonymy and Lexical Choice
say_C
Cluster option
untruth_C
John_C
SitSpec
tell-a-lie_C
Nonconformity
equivocatelie Thing
Untruth
fib
Person
Communicate
Person
Statement
lie
prevarication
John
falsehood
misrepresentation
John
fib
nonconform1
john1 lie1
say1
untruth
Communicate
prevaricate
Person
saytell
ATTRIBUTESAYER
ATTRIBUTESAYER
SAYING
SAYING
SAYER
SAYER
SAYINGSAYER
Figure 11
The state of processing just before lexical choice on the input for John tells a lie. Four clusters
have become options; each is shown with its core denotation and near-synonyms. Solid arrows
in the SitSpec indicate relations between instances of concepts. Solid arrows in the cluster
options relate concepts in the core denotations. Dashed arrows link SitSpec nodes to the
cluster options that cover subgraphs rooted at the nodes.
where WSat is the degree to which w satisfies the preferences P (see Equation (3)).
The most preferred sentence plan SP? is thus the one whose set of word choices
maximizes Satisfaction(P, SP?). This function accounts for trade-offs in the satisfaction
of preferences, because it finds the set of words that collectively satisfy as many of the
preferences as possible, each to the highest degree possible.
Each word in SP has to be chosen from a distinct cluster. Thus, given
? a particular cluster c in the set of all cluster options
? a list W of candidate near-synonyms of c, ordered according to a
prespecified criterion (some candidates of the cluster might have already
been ruled out because of collocational constraints)
134
Computational Linguistics Volume 28, Number 2
? a set Pc ? P of compatible preferences, each of which can potentially be
satisfied by a word in c, with associated importances: Imp: P ? [0, 1]
find the first candidate w? ? W such that WSat(P, w?) is maximized.
We use an approximate-matching algorithm to compute WSat(P, w). Under the
simplification that its value depends on the degree to which w individually satisfies
each of the preferences in Pc, the algorithm computes WSat(P, w) by combining the
set of scores Sat(p, w) for all p ? Pc. Various combination functions are plausible, in-
cluding simple functions, such as a weighted average or a distance metric, and more
complex functions that could, for instance, take into account dependencies between
preferences.21 Deciding on this function is a subject for future research that will em-
pirically evaluate the efficacy of various possibilities. For now, we define WSat as a
weighted average of the individual scores, taking into account the importance factors:
WSat(P, w) = WSat(Pc, w) =
?
p?Pc
Imp(p)
|Pc|
Sat(p, w) (3)
For a given preference p ? Pc, the degree to which p is satisfied by w, Sat(p, w),
is reducible to the problem of computing similarity between lexical distinctions, for
which we already have a solution (see Equation (1)). Thus,
Sat(p, w) = Sim(d(p), d(w)) (4)
where d(p) is a kind of pseudo-distinction generated from p to have the same form
as a lexical distinction, putting it on equal footing to d(w), and d(w) is the distinction
of w that is commensurate with d(p), if one exists.
7.4 Implementation: I-Saurus
I-Saurus, a sentence planner that splits hairs, extends Stede?s MOOSE (1999) with the
modifications formalized above for fine-grained lexical choice. It takes a SitSpec and
a set of preferences as input, and outputs a sentence plan in Penman?s SPL, which
Penman generates as a sentence in English. (Section 8 provides an example.)
Now, finding the best set of options could involve a lengthy search process. An
exhaustive search through all possible sentence plans to find the one that maximizes
Satisfaction(P, SP) can be very time-inefficient: In the relatively small example given in
Section 8, there are 960 different sentence plans to go through. To avoid an exhaustive
search, we use the following heuristic, adopted from Stede (1999): In order to find
the globally preferred sentence plan, make the most preferred local choices. That is,
whenever a (local) decision is made between several options, choose the option with
the highest score. Thus, we postulate that the most preferred sentence plan will be
one of the first few sentence plans generated, though we offer no proof beyond our
intuition that complex global effects are relatively rare, which is also a justification for
the simplifications we made above.
Figure 12 gives an algorithm for two-tiered lexical choice embedded in MOOSE?s
sentence planner. The main additions are the procedures Next-Best-Cluster-Option and
21 For instance, we might want to consider a particular preference only after some other preference has
been satisfied (or not) or only to resolve conflicts when several words satisfy another preference to the
same degree.
135
Edmonds and Hirst Near-Synonymy and Lexical Choice
Build-Sentence-Plan(node, P)
(1) c ?Next-Best-Cluster-Option(node, P)
if we?ve tried all the options then return ?fail?
(2) w ?Next-Best-Near-Synonym(c, P)
if we?ve tried all the near-synonyms in c then backtrack to (1)
p ? partial SemSpec of w
if p has external variables then
for each external variable v in p
s ? Build-Sentence-Plan(node bound to v, P)
if s = ?fail? then
backtrack to (2)
else
attach s to p at v
return p
Figure 12
The sentence-planning algorithm. This algorithm outputs the most preferred complete
well-formed SemSpec for a subgraph rooted at given node in the SitSpec.
Next-Best-Near-Synonym. (Note, however, that this version of the algorithm does not
show how complete coverage or well-formedness is ensured.) Next-Best-Cluster-Option
moves through the cluster options that cover part of the SitSpec rooted at node in
order of preference. As we said above, structural decisions on this tier of lexical choice
are outside the scope of this article, but we can assume that an algorithm will in due
course be devised for ranking the cluster options according to criteria supplied in the
input. (In fact, MOOSE can rank options according to preferences to foreground or
background participants, in order to make them more or less salient, but this is only
a start.) Next-Best-Near-Synonym steps through the near-synonyms for each cluster in
order of preference as computed by WSat(P, w).
7.5 Summary
The two-tiered lexical-choice algorithm (and sentence-planning algorithm) developed
in this section is as efficient as any algorithm developed to date for a conventional
model of lexical knowledge (without near-synonyms), because it can find the appro-
priate cluster or clusters just as easily as the latter can find a word; A cluster in our
model corresponds to an individual word in the conventional model. And choosing a
near-synonym from a cluster is efficient because there are normally only a few of them
per cluster. The system does not have to search the entire lexicon. The full complex-
ity of representing and using fine-grained lexical differences is partitioned into small
clusters. The process is also robust, ensuring that the right meaning (at a coarse grain)
is lexicalized even if a ?poor? near-synonym is chosen in the end. And when the right
preferences are specified in the input, the algorithm is accurate in its choice, attempting
to meet as many preferences as possible while also satisfying the constraints.
8. Example
A formal evaluation of I-Saurus would require both a substantial lexicon of clusters
and a large test suite of input data correlated with the desired output sentences. Build-
ing such a suite would be a substantial undertaking in itself. Barring this, we could
136
Computational Linguistics Volume 28, Number 2
Table 3
Four simultaneous preferences and the six candidates of the untruth C cluster.
Preferences:
1 (imply (significance1 (ATTRIBUTE-OF lie1) (DEGREE low)))
2 (imply (intend1 (ACTOR john1) (ACTEE mislead1)))
3 (disfavor john1)
4 (low formality)
Candidate
Preference fib lie misrepresentation untruth prevarication falsehood
1 Insignificance 1.00 0.00 0.00 0.00 0.00 0.00
2 Deliberateness 0.50 1.00 0.75 0.25 0.75 0.00
3 Disfavor 0.50 0.63 0.50 0.50 0.50 0.50
4 Low formality 1.00 0.50 0.50 0.50 0.00 0.50
Total Score 3.00 2.13 1.75 1.25 1.25 1.00
Note: For each candidate, we show the satisfaction scores (Sat) for each individual
preference and the total satisfaction scores (WSat): fib scores highest.
evaluate I-Saurus as an MT system, in terms of coverage and of quality (intelligibility,
fidelity, and fluency). Unfortunately, I-Saurus is but a prototype with a small exper-
imental lexicon, so we can only show by a few examples that it chooses the most
appropriate words given a variety of input preferences.
Returning again the situation of John and his lie (Figure 11), consider the set of four
simultaneous preferences shown in the top part of Table 3. The bottom part shows the
scores for each candidate in the untruth C cluster. If this cluster option were chosen,
then I-Saurus would choose the noun fib, because it simultaneously and maximally
satisfies all of the preferences as shown by the score of WSat({1, 2, 3, 4}, fib) = 3.00. But
note that if fib were not available, then the second-place lie would be chosen, leaving
unsatisfied the preference to express insignificance.
Now, for a whole sentence, consider the SitSpec shown in Table 4. For this, I-
Saurus can generate 960 different sentence plans, including plans that realize the
sentences John commands an alcoholic to lie and John orders a drunkard to tell a fib. I-
Saurus can be so prolific because of the many possible combinations of the near-
synonyms of the six clusters involved: John C (one near-synonym), alcoholic C (ten
near-synonyms), order C (six near-synonyms), say C (two near-synonyms), untruth C
(six near-synonyms), and tell-a-lie C (four near-synonyms).
The bottom part of Table 4 shows the variety of output that is possible when each
individual preference and various combinations of simultaneous preferences (cases
i?x) are input to the system. (The numbered preferences are listed at the top of the
table.) So for example, if we input preference 3 (high formality), the system outputs
John enjoins an inebriate to prevaricate. The output appears stilted in some cases because
no other parameters, such as desired verb tense, were given to Penman and because
the system has no knowledge of collocational constraints. Of course, we could have
defined many other preferences (and combinations of preferences), but we chose these
particular ones in order to show some of the interesting interactions that occur among
the cluster options during processing; they are not meant to be representative of what
a user would normally ask of the system.
Consider, for instance, case iv. Here, one cluster can satisfy preference 6 (pejorative
attitude), and another cluster can satisfy preference 10 (misconception), but neither
137
Edmonds and Hirst Near-Synonymy and Lexical Choice
Table 4
A sample of output sentences of I-Saurus given an input SitSpec and various preferences and
combinations of preferences (cases i?x).
SitSpec:
(order1 (SAYER john1)
(SAYEE alcoholic1)
(SAYING (perform1 (ACTOR alcoholic1)
(ACTEE (tell1 (SAYER alcoholic1)
(SAYING (lie1
(ATTRIBUTE nonconform1))))))))
Preferences:
1 (low formality)
2 (medium formality)
3 (high formality)
4 (high concreteness)
5 (favor alcoholic1)
6 (disfavor alcoholic1)
7 (imply (authority1 (ATTRIBUTE-OF john1) (ATTRIBUTE official1)))
8 (imply (authority1 (ATTRIBUTE-OF john1) (ATTRIBUTE peremptory1)))
9 (imply (significance1 (ATTRIBUTE-OF lie1) (DEGREE low)))
10 (imply (misconceive1 (ACTOR alcoholic1) (CAUSE-OF lie1)))
11 (imply (contradict2 (ACTOR lie1) (ATTRIBUTE categorical2)))
Case Input preferences Output
None John commands an alcoholic to lie.
1 John commands a drunk to fib.
2 John commands an alcoholic to lie.
3 John enjoins an inebriate to prevaricate.
4 John directs a drunkard to tell a lie.
5 John commands a tippler to fib.
6 John commands a drunk to lie.
7 John commands an alcoholic to lie.
8 John orders an alcoholic to lie.
9 John commands an alcoholic to fib.
10 John commands an alcoholic to tell an untruth.
11 John commands an alcoholic to lie.
i 2, 4 John directs a drunkard to tell a lie.
ii 1, 9 John commands a drunk to fib.
iii 3, 6 John enjoins a drunkard to prevaricate.
iv 6, 10 John commands a drunkard to tell an untruth.
v 3, 9 John enjoins an inebriate to fib.
vi 3, 7, 9 John commands an inebriate to fib.
vii 3, 8, 9 John orders an inebriate to fib.
viii 3, 6, 8, 9 John orders a drunkard to fib.
ix 3, 5 John enjoins a tippler to tell a prevarication.
x 3, 5, 11 John enjoins a tippler to tell a prevarication.
cluster can satisfy both preferences on its own. So the system chooses drunkard, because
it is pejorative, and untruth, because it implies a misconception. No other combination
of choices from the two clusters could have simultaneously satisfied both preferences.
And finally, consider case v, which illustrates a clash in the satisfaction of one of
the preferences. Fib is chosen despite the fact that it is informal, because it is the only
word that implies an insignificant lie. But the system compensates by choosing two
138
Computational Linguistics Volume 28, Number 2
other formal words: enjoin and inebriate. If we add a preference to this case to imply
that John has official authority (case vi), then I-Saurus system chooses command instead
of enjoin, further sacrificing high formality.
9. Related Work
Most computational work on near-synonymy has been motivated by lexical mismatches
in machine translation (Kameyama et al 1991). In interlingual MT, an intermediate
representational scheme, such as an ontology in knowledge-based machine translation
(KBMT) (Nirenburg et al 1992), or lexical-conceptual structures in UNITRAN (Dorr
1993) is used in encoding lexical meaning (and all other meaning). But as we showed
in Section 3, such methods don?t work at the fine grain necessary for near-synonymy,
despite their effectiveness at a coarse grain. To overcome these problems but retain the
interlingual framework, Barnett, Mani, and Rich (1994) describe a method of generat-
ing natural-sounding text that is maximally close in meaning to the input interlingual
representation. Like us, they define the notion of semantic closeness, but whereas they
rely purely on denotational representations and (approximate) logical inference in ad-
dition to lexical features for relative naturalness, we explicitly represent fine-grained
aspects on a subconceptual level and use constraints and preferences, which gives flex-
ibility and robustness to the lexical-choice process. Viegas (1998), on the other hand,
describes a preliminary solution that accounts for semantic vagueness and underspec-
ification in a generative framework. Although her model is intended to account for
near-synonymy, she does not explicitly discuss it.
Transfer-based MT systems use a bilingual lexicon to map words and expressions
from one language to another. Lists, sometimes huge, of handcrafted language-pair-
specific rules encode the knowledge to use the mapping (e.g., in SYSTRAN [Gerber
and Yang 1997]). EuroWordNet (Vossen 1998) could be used in such a system. Its
Inter-Lingual-Index provides a language-independent link between synsets in different
languages and has an explicit relation, EQ NEAR SYNONYM, for relating synsets that
are not directly equivalent across languages. But, as in individual WordNets, there is
no provision for representing differences between near-synonyms.
In statistical MT, there would seem to be some promise for handling near-synon-
ymy. In principle, a system could choose the near-synonym that is most probable given
the source sentence and the target-language model. Near-synonymy seems to have
been of little concern, however, in statistical MT research: The seminal researchers,
Brown et al (1990), viewed such variations as a matter of taste; in evaluating their
system, two different translations of the same source that convey roughly the same
meaning (perhaps with different words) are considered satisfactory translations. More
recently, though, Foster, Isabelle, and Plamondon (1997) show how such a model can
be used in interactive MT, and Langkilde and Knight (1998) in text generation. Such
methods are unfortunately limited in practice, because it is too computationally ex-
pensive to go beyond a trigram model (only two words of context). Even if a statistical
approach could account for near-synonymy, Edmonds (1997) showed that its strength
is not in choosing the right word, but rather in determining which near-synonym is
most typical or natural in a given context. So such an approach would not be so useful
in goal-directed applications such as text generation, or even in sophisticated MT.
10. Conclusion
Every natural language processing system needs some sort of lexicon, and for many
systems, the lexicon is the most important component. Yet, real natural language pro-
139
Edmonds and Hirst Near-Synonymy and Lexical Choice
cessing systems today rely on a relatively shallow coverage of lexical phenomena,
which unavoidably restricts their capabilities and thus the quality of their output. (Of
course, shallow lexical semantics is a necessary starting point for a practical system,
because it allows for broad coverage.) The research reported here pushes the lexical
coverage of natural language systems to a deeper level.
The key to the clustered model of lexical knowledge is its subconceptual/stylistic
level of semantic representation. By introducing this level between the traditional con-
ceptual and syntactic levels, we have developed a new model of lexical knowledge
that keeps the advantages of the conventional model?efficient paraphrasing, lexical
choice (at a coarse grain), and mechanisms for reasoning?but overcomes its short-
comings concerning near-synonymy. The subconceptual/stylistic level is more expres-
sive than the top level, yet it allows for tractable and efficient processing because
it ?partitions,? or isolates, the expressiveness (i.e., the non-truth-conditional seman-
tics and fuzzy representations) in small clusters. The model reconciles fine-grained
lexical knowledge with coarse-grained ontologies using the notion of granularity of
representation.
The next stage in this work is to build a more extensive lexicon of near-synonym
clusters than the few handwritten clusters that were built for the simple implementa-
tion described in this article. To this end, Inkpen and Hirst (2001a, 2001b) are develop-
ing a method to automatically build a clustered lexicon of 6,000 near-synonyms (1,000
clusters) from the machine-readable text of Hayakawa?s Choose the Right Word (1994).
Besides MT and NLG, we envision other applications of the model presented
in this article. For instance, an interactive dictionary?an intelligent thesaurus?would
actively help a person to find and choose the right word in any context. Rather than
merely list possibilities, it would rank them according to the context and to parameters
supplied by the user and would also explain potential effects of any choice, which
would be especially useful in computer-assisted second-language instruction. Or the
model could be applied in the automatic (post)editing of text in order to make the
text conform to a certain stylistic standard or to make a text more readable or natural
to a given audience.
We leave a number of open problems for another day, including recovering nu-
ances from text (see Edmonds [1998] for a preliminary discussion); evaluating the
effectiveness of the similarity measures; determining the similarity of conceptual struc-
tures; understanding the complex interaction of lexical and structural decisions during
lexical choice; exploring the requirements for logical inference in the model; model-
ing other aspects of fine-grained meaning, such as emphasis; and understanding the
context-dependent nature of lexical differences and lexical knowledge.
Appendix: An Example Representation: The Error Cluster
The following is the representation of the cluster of error nouns in our formalism.
Tokens ending in l represent lexical items. In upper case are either variables (for
cross-reference) or relations; it should be clear from the context which is which. Cap-
italized tokens are concepts. In lower case are values of various features (such as
?indirectness? and ?strength?) defined in the model. We have not discussed many of
the implementation details in this article, including p-link and covers (see Edmonds
[1999]).
(defcluster error C
;;; from Gove (1984)
:syns (error l mistake l blunder l slip l lapse l howler l)
140
Computational Linguistics Volume 28, Number 2
:core (ROOT Generic-Error)
:p-link ((V1 (:and (Person V1) (ACTOR ROOT V1)))
(V2 (:and (Deviation V2) (ATTRIBUTE ROOT V2))))
:covers (ROOT)
:periph((P1 Stupidity (ATTRIBUTE-OF V1))
(P2 Blameworthiness (ATTRIBUTE-OF V1))
(P3 Criticism (ACTEE V1) (ATTRIBUTE (P31 Severity)))
(P4 Misconception (CAUSE-OF V2) (ACTOR V1))
(P5 Accident (CAUSE-OF V2) (ACTOR V1))
(P6 Inattention (CAUSE-OF V2) (ACTOR V1)))
:distinctions (
;; Blunder commonly implies stupidity.
(blunder l usually medium implication P1)
;; Mistake does not always imply blameworthiness, blunder sometimes.
(mistake l sometimes medium implication (P2 (DEGREE ?medium)))
(error l always medium implication (P2 (DEGREE ?medium)))
(blunder l sometimes medium implication (P2 (DEGREE ?high)))
;; Mistake implies less severe criticism than error.
;; Blunder is harsher than mistake or error.
(mistake l always medium implication (P31 (DEGREE ?low)))
(error l always medium implication (P31 (DEGREE ?medium)))
(blunder l always medium implication (P31 (DEGREE ?high)))
;; Mistake implies misconception.
(mistake l always medium implication P4)
;; Slip carries a stronger implication of accident than mistake.
;; Lapse implies inattention more than accident.
(slip l always medium implication P5)
(mistake l always weak implication P5)
(lapse l always weak implication P5)
(lapse l always medium implication P6)
;; Blunder expresses a pejorative attitude towards the person.
(blunder l always medium pejorative V1)
;; Blunder is a concrete word, error and mistake are abstract.
(blunder l high concreteness)
(error l low concreteness)
(mistake l low concreteness)
;; Howler is an informal term
(howler l low formality))
)
Acknowledgments
Our work is financially supported by the
Natural Sciences and Engineering Research
Council of Canada, the Ontario Graduate
Scholarship program, and the University of
Toronto. For discussions, suggestions, and
comments on this work, we are grateful to
Jack Chambers, Mark Chignell, Robert Dale,
Chrysanne DiMarco, Paul Deane, Steve
Green, Eduard Hovy, Brian Merrilees, John
Mylopoulos, Kazuko Nakajima, Sergei
Nirenburg, Geoffrey Nunberg, Henry
Schoght, Manfred Stede and the anonymous
reviewers of Computational Linguistics.
References
Bailly, Rene?. 1970. Dictionnaire des synonymes
de la langue franc?aise. Librairie Larousse,
Paris.
141
Edmonds and Hirst Near-Synonymy and Lexical Choice
Barnett, James, Inderjeet Mani, and Elaine
Rich. 1994. Reversible machine
translation: What to do when the
languages don?t match up. In Tomek
Strzalkowski, editor, Reversible Grammar in
Natural Language Processing. Kluwer
Academic, pages 321?364.
Barsalou, Lawrence W. 1992. Frames,
concepts, and conceptual fields. In
Adrienne Lehrer and Eva Fedder Kittay,
editors, Frames, Fields, and Contrasts: New
Essays in Semantic and Lexical Organization,
pages 21?74. Lawrence Erlbaum.
Batchelor, Ronald E. and Malcolm H.
Offord. 1993. Using French Synonyms.
Cambridge University Press.
Beale, Stephen, Sergei Nirenburg, Evelyne
Viegas, and Leo Wanner. 1998.
De-constraining text generation. In
Proceedings of the Ninth International
Workshop on Natural Language Generation,
pages 48?57.
Be?nac, Henri. 1956. Dictionnaire des
synonymes. Librairie Hachette, Paris.
Brown, Peter F., John Cooke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Budanitsky, Alexander. 1999. Measuring
semantic relatedness and its applications.
Master?s thesis, technical report
CSRG-390, Department of Computer
Science, University of Toronto, Toronto,
Canada. Available at http://www.cs.
toronto.edu/compling/Publications/
Abstracts/Theses/Budanistsky-
thabs.html.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet: An
experimental, application-oriented
evaluation of five measures. In Workshop
on WordNet and Other Lexical Resources:
Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 29?34, Pittsburgh.
Budanitsky, Alexander and Graeme Hirst.
2002. Lexical semantic relatedness.
Manuscript in preparation.
Burkert, Gerrit and Peter Forster. 1992.
Representation of semantic knowledge
with term subsumption languages. In
James Pustejovsky and Sabine Bergler,
editor, Lexical Semantics and Knowledge
Representation: First SIGLEX Workshop.
Lecture Notes in Artificial Intelligence
627. Springer-Verlag, pages 75?85.
Chapman, Robert L, editor. 1992. Roget?s
International Thesaurus. 5th edition.
HarperCollins Publishers.
Church, Kenneth Ward, William Gale,
Patrick Hanks, Donald Hindle, and
Rosamund Moon. 1994. Lexical
substitutability. In B. T. S. Atkins and
A. Zampolli, editors, Computational
Approaches to the Lexicon. Oxford
University Press, pages 153?177.
Clark, Eve V. 1992. Conventionality and
contrast: Pragmatic principles with lexical
consequences. In Adrienne Lehrer and
Eva Fedder Kittay, editors, Frames, Fields,
and Contrasts: New Essays in Semantic and
Lexical Organization. Lawrence Erlbaum,
pages 171?188.
Cohen, William W., Alex Borgida, and
Haym Hirsh. 1992. Computing least
common subsumers in description logic.
In Proceedings of the Tenth National
Conference on Artificial Intelligence
(AAAI-92), pages 754?760.
Coleman, Linda and Paul Kay. 1981.
Prototype semantics: The English word
lie. Language, 57(1):26?44.
Cruse, D. Alan. 1986. Lexical Semantics.
Cambridge University Press.
Dagan, Ido, Shaul Marcus, and Shaul
Markovitch. 1993. Contextual word
similarity and estimation from sparse
data. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, pages 164?171.
DiMarco, Chrysanne and Graeme Hirst.
1993. A computational theory of
goal-directed style in syntax.
Computational Linguistics, 19(3):451?500.
DiMarco, Chrysanne, Graeme Hirst, and
Manfred Stede. 1993. The semantic and
stylistic differentiation of synonyms and
near-synonyms. In AAAI Spring
Symposium on Building Lexicons for Machine
Translation, pages 114?121, Stanford, CA,
March.
Dorr, Bonnie J. 1993. Machine Translation: A
View from the Lexicon. MIT Press.
Edmonds, Philip. 1997. Choosing the word
most typical in context using a lexical
co-occurrence network. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 507?509,
Madrid, Spain.
Edmonds, Philip. 1998. Translating
near-synonyms: Possibilities and
preferences in the interlingua. In
Proceedings of the AMTA/SIG-IL Second
Workshop on Interlinguas, pages 23?30,
Langhorne, PA. (Proceedings published as
technical report MCCS-98-316,
Computing Research Laboratory, New
Mexico State University.)
142
Computational Linguistics Volume 28, Number 2
Edmonds, Philip. 1999. Semantic
Representations of Near-Synonyms for
Automatic Lexical Choice. Ph.D. thesis,
Department of Computer Science,
University of Toronto. Available at
http://www.cs.toronto.edu/compling/
Publications/ Abstracts/Theses/
EdmondsPhD-thabs.html.
Egan, Rose F. 1942. ?Survey of the history of
English synonymy? and ?Synonym:
Analysis and definition.? Reprinted in
Philip B. Gove, editor, Webster?s New
Dictionary of Synonyms. Merriam-Webster,
Springfield, MA, pp. 5a?31a.
Elhadad, Michael, Kathleen McKeown, and
Jacques Robin. 1997. Floating constraints
in lexical choice. Computational Linguistics,
23(2):195?240.
Emele, Martin, Ulrich Heid, Stefan Momma,
and Re?mi Zajac. 1992. Interactions
between linguistic constraints: Procedural
vs. declarative approaches. Machine
Translation, 7(1?2):61?98.
Evens, Martha, editor. 1988. Relational
Models of the Lexicon: Representing
Knowledge in Semantic Networks.
Cambridge University Press.
Farrell, Ralph Barstow. 1977. Dictionary of
German Synonyms. 3rd edition. Cambridge
University Press.
Fernald, James C., editor. 1947. Funk &
Wagnall?s Standard Handbook of Synonyms,
Antonyms, and Prepositions. Funk &
Wagnall?s, New York.
Foster, George, Pierre Isabelle, and Pierre
Plamondon. 1997. Target-text mediated
interactive machine translation. Machine
Translation, 12:175?194.
Frege, Gottlob. 1892. U?ber Sinn und
Bedeutung. Zeitschrift fu?r Philosophie und
Philosophie Kritik, 100:25?50. English
translation: On sense and reference. In P.
Geach and M. Black, editors, Translations
from the Philosophical Writings of Gottlob
Frege. Blackwell, 1960.
Fujiwara, Yoichi, Hideo Isogai, and Toshiaki
Muroyama. 1985. Hyogen ruigo jiten.
Tokyodo Shuppan, Tokyo.
Gentner, Dedre and Arthur B. Markman.
1994. Structural alignment in comparison:
No difference without similarity.
Psychological Science, 5(3):152?158.
Gerber, Laurie, and Jin Yang. 1997.
SYSTRAN MT dictionary development. In
Machine Translation: Past, Present, and
Future: Proceedings of Machine Translation
Summit VI, pages 211?218, San Diego, CA.
Goldman, Neil M. 1975. Conceptual
generation. In Roger C. Schank, editor,
Conceptual Information Processing.
North-Holland, Amsterdam, pages
289?371.
Goodman, Nelson. 1952. On likeness of
meaning. In L. Linsky, editor, Semantics
and the Philosophy of Language. University
of Illinois Press, pages 67?74.
Gove, Philip B., editor. 1984. Webster?s New
Dictionary of Synonyms. Merriam-Webster,
Springfield, MA.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers.
Hayakawa, S. I., editor. 1994. Choose the
Right Word: A Contemporary Guide to
Selecting the Precise Word for Every Situation.
2nd edition, revised by Eugene Ehrlich.
HarperCollins Publishers, New York.
Hirst, Graeme. 1995. Near-synonymy and
the structure of lexical knowledge. In
AAAI Symposium on Representation and
Acquisition of Lexical Knowledge: Polysemy,
Ambiguity, and Generativity, pages 51?56,
Stanford, CA, March.
Hovy, Eduard. 1988. Generating Natural
Language Under Pragmatic Constraints.
Lawrence Erlbaum Associates.
Inkpen, Diana Zaiu and Graeme Hirst.
2001a. Experiments on extracting
knowledge from a machine-readable
dictionary of synonym differences. In
Alexander Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing
(Proceedings, Second Conference on Intelligent
Text Processing and Computational
Linguistics, Mexico City, February 2001),
Lecture Notes in Computer Science 2004.
Springer-Verlag, pages 264?278.
Inkpen, Diana Zaiu and Graeme Hirst.
2001b. Building a lexical knowledge-base
of near-synonym differences. Workshop on
WordNet and Other Lexical Resources, Second
Meeting of the North American Chapter of the
Association for Computational Linguistics,
pages 47?52, Pittsburgh.
Jackendoff, Ray. 1983. Semantic and
Cognition. MIT Press.
Jackendoff, Ray. 1990. Semantic Structures.
MIT Press.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference for
Research on Computational Linguistics
(ROCLING X), Taiwan.
Kameyama, Megumi, Ryo Ochitani, Stanley
Peters, and Hidetoshi Sirai. 1991.
Resolving translation mismatches with
information flow. In Proceedings of the 29th
Annual Meeting of the Association for
Computational Linguistics, pages 193?200.
Katz, Jerrold J. and Jerry A. Fodor. 1963.
The structure of a semantic theory.
143
Edmonds and Hirst Near-Synonymy and Lexical Choice
Language, 39:170?210.
Kay, Maire? Weir, editor. 1988. Webster?s
Collegiate Thesaurus. Merriam-Webster,
Springfield, MA.
Kay, Paul. 1971. Taxonomy and semantic
contrast. Language, 47(4):866?887.
Kozima, Hideki and Teiji Furugori. 1993.
Similarity between words computed by
spreading activation on an English
dictionary. In Proceedings of the Sixth
Conference of the European Chapter of the
Association for Computational Linguistics,
pages 232?239, Utrecht, Netherlands.
Kroll, Judith F. and Annette M.B. de Groot.
1997. Lexical and conceptual memory in
the bilingual: Mapping form to meaning
in two languages. In Annette M.B. de
Groot and Judith F. Kroll, editors, Tutorials
in Bilingualism: Psycholinguistic Perspectives.
Lawrence Erlbaum, pages 169?199.
Langkilde, Irene and Kevin Knight. 1998.
The practical value of n-grams in
generation. In Proceedings of the Ninth
International Workshop on Natural Language
Generation, pages 248?255,
Niagara-on-the-Lake, Canada.
Lehrer, Adrienne and Eva Feder Kittay.
1992. Introduction. In Adrienne Lehrer
and Eva Feder Kittay, editors, Frames,
Fields, and Contrasts: New Essays in Semantic
and Lexical Organization. Lawrence
Erlbaum, pages 1?20.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th
International Conference on Computational
Linguistics (COLING-ACL-98), pages
768?774, Montreal.
Lyons, John. 1977. Semantics. Cambridge
University Press.
Lyons, John. 1995. Linguistic Semantics: An
Introduction. Cambridge University Press.
Markman, Arthur B. and Dedre Gentner.
1993. Splitting the differences: A
structural alignment view of similarity.
Journal of Memory and Language, 32:517?535.
McDonald, David D. 1983. Description
directed control: Its implications for
natural language generation. In Nick
Cercone, editor, Computational Linguistics,
International Series in Modern Applied
Mathematics and Computer Science 5.
Plenum Press, New York, pages 111?129.
Reprinted in B. J. Grosz, K. Sparck Jones,
and B. L. Webber, editors, Readings in
Natural Language Processing. Morgan
Kaufmann, 1986, pages 519?537.
McKeown, Kathleen R. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Cambridge University Press.
McMahon, John G. and Francis J. Smith.
1996. Improving statistical language
model performance with automatically
generated word hierarchies. Computational
Linguistics, 22(2):217?248.
Nirenburg, Sergei, Jaime Carbonell, Masaru
Tomita, and Kenneth Goodman. 1992.
Machine Translation: A Knowledge-Based
Approach. Morgan Kaufmann.
Nirenburg, Sergei and Christine Defrise.
1992. Application-oriented computational
semantics. In Michael Rosner and
Roderick Johnson, editors, Computational
Linguistics and Formal Semantics.
Cambridge University Press, pages
223?256.
Nirenburg, Sergei, Victor Lesser, and Eric
Nyberg. 1989. Controlling a language
generation planner. In Proceedings of the
11th International Joint Conference on
Artificial Intelligence, pages 1524?1530.
Nirenburg, Sergei and Lori Levin. 1992.
Syntax-driven and ontology-driven lexical
semantics. In James Pustejovsky and
Sabine Bergler, editors, Lexical Semantics
and Knowledge Representation: First SIGLEX
Workshop. Lecture Notes in Artificial
Intelligence 627. Springer-Verlag, pages
5?20.
Nogier, Jean-Franc?ois and Michael Zock.
1992. Lexical choice as pattern matching.
Knowledge-Based Systems, 5:200?212.
Penman Natural Language Group. 1989.
The Penman reference manual. Technical
report, Information Sciences Institute of
the University of Southern California.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press.
Pustejovsky, James and Sabine Bergler,
editors. 1992. Lexical Semantics and
Knowledge Representation: First SIGLEX
Workshop. Lecture Notes in Artificial
Intelligence 627. Springer-Verlag.
Quine, W. V. O. 1951. Two dogmas of
empiricism. Philosophical Review, 60:20?43.
Reiter, Ehud and Robert Dale. 1997.
Building applied natural language
generation systems. Natural Language
Engineering, 3(1):57?88.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity in
a taxonomy. In Proceedings of the 14th
144
Computational Linguistics Volume 28, Number 2
International Joint Conference on Artificial
Intelligence, pages 448?453, Montreal.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Proceedings
of the 22nd Annual Meeting of the Cognitive
Science Society (COGSCI 2000).
Resnik, Philip, and David Yarowsky. 1999.
Distinguishing systems and
distinguishing senses: New evaluation
methods for word sense disambiguation.
Natural Language Engineering, 5(2):135?146.
Room, Adrian. 1985. Dictionary of Confusing
Words and Meanings. Dorset, New York.
Rosch, Eleanor. 1978. Principles of
categorization. In Eleanor Rosch and
Barbara B. Lloyd, editors, Cognition and
categorization. Lawrence Erlbaum
Associates, pages 27?48.
Saussure, Ferdinand de. 1916. Cours de
linguistique ge?ne?rale. Translated by Roy
Harris as Course in General Linguistics,
London: G. Duckworth, 1983.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Sowa, John F. 1988. Using a lexicon of
canonical graphs in a semantic interpreter.
In Martha Evens, editor, Relational Models
of the Lexicon: Representing Knowledge in
Semantic Networks. Cambridge University
Press, pages 113?137.
Sowa, John F. 1992. Logical structures in the
lexicon. In James Pustejovsky and Sabine
Bergler, editors, Lexical Semantics and
Knowledge Representation: First SIGLEX
Workshop. Lecture Notes in Artificial
Intelligence 627. Springer-Verlag, pages
39?60.
Sparck Jones, Karen. 1986. Synonymy and
Semantic Classification. Edinburgh
University Press.
Stede, Manfred. 1993. Lexical choice criteria
in language generation. In Proceedings of
the Sixth Conference of the European Chapter
of the Association for Computational
Linguistics, pages 454?459, Utrecht,
Netherlands.
Stede, Manfred. 1999. Lexical Semantics and
Knowledge Representation in Multilingual
Text Generation. Kluwer Academic.
Tarski, Alfred. 1944. The semantic
conception of truth. Philosophy and
Phenomenological Research, 4:341?375.
Ullmann, Stephen. 1962. Semantics: An
Introduction to the Science of Meaning.
Blackwell.
Urdang, Laurence. 1992. Dictionary of
Differences. Bloomsbury, London.
Viegas, Evelyne. 1998. Multilingual
computational semantic lexicons in
action: The WYSINWYG approach to
NLG. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL-98), pages 1321?1327,
Montreal, Canada.
Vossen, Piek. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic.
Walther, George. 1992. Power Talking: 50
Ways to Say What You Mean and Get What
You Want. Berkley, New York.
Wanner, Leo and Eduard Hovy. 1996. The
HealthDoc sentence planner. In
Proceedings of the Eighth International
Workshop on Natural Language Generation,
pages 1?10.
Wittgenstein, Ludwig. 1953. Philosophical
Investigations. Blackwell.
Evaluating WordNet-based Measures
of Lexical Semantic Relatedness
Alexander Budanitsky?
University of Toronto
Graeme Hirst?
University of Toronto
The quantification of lexical semantic relatedness has many applications in NLP, and many
different measures have been proposed. We evaluate five of these measures, all of which use
WordNet as their central resource, by comparing their performance in detecting and correcting
real-word spelling errors. An information-content?based measure proposed by Jiang and
Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin,
and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for
lexical semantic relatedness.
1. Introduction
The need to determine semantic relatedness or its inverse, semantic distance, be-
tween two lexically expressed concepts is a problem that pervades much of natural
language processing. Measures of relatedness or distance are used in such applica-
tions as word sense disambiguation, determining the structure of texts, text sum-
marization and annotation, information extraction and retrieval, automatic indexing,
lexical selection, and the automatic correction of word errors in text. It?s important
to note that semantic relatedness is a more general concept than similarity; similar
entities are semantically related by virtue of their similarity (bank?trust company), but
dissimilar entities may also be semantically related by lexical relationships such as
meronymy (car?wheel) and antonymy (hot?cold), or just by any kind of functional rela-
tionship or frequent association (pencil?paper, penguin?Antarctica, rain?flood). Computa-
tional applications typically require relatedness rather than just similarity; for example,
money and river are cues to the in-context meaning of bank that are just as good as
trust company.
However, it is frequently unclear how to assess the relative merits of the many
competing approaches that have been proposed for determining lexical semantic re-
latedness. Given a measure of relatedness, how can we tell whether it is a good
one or a poor one? Given two measures, how can we tell whether one is bet-
ter than the other, and under what conditions it is better? And what is it that
makes some measures better than others? Our purpose in this paper is to compare
the performance of a number of measures of semantic relatedness that have been
proposed for use in applications in natural language processing and information
retrieval.
? Department of Computer Science, Toronto, Ontario, Canada M5S 3G4; {abm, gh}@cs.toronto.edu.
Submission received: 8 July 2004; revised submission received: 30 July 2005; accepted for publication:
20 August 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 1
1.1 Terminology and Notation
In the literature related to this topic, at least three different terms are used by differ-
ent authors or sometimes interchangeably by the same authors: semantic relatedness,
similarity, and semantic distance.
Resnik (1995) attempts to demonstrate the distinction between the first two by way
of example. ?Cars and gasoline?, he writes, ?would seem to be more closely related
than, say, cars and bicycles, but the latter pair are certainly more similar.? Similarity
is thus a special case of semantic relatedness, and we adopt this perspective in this
paper. Among other relationships that the notion of relatedness encompasses are the
various kinds of meronymy, antonymy, functional association, and other ?non-classical
relations? (Morris and Hirst 2004).
The term semantic distance may cause even more confusion, as it can be used
when talking about either just similarity or relatedness in general. Two concepts are
?close? to one another if their similarity or their relatedness is high, and otherwise
they are ?distant?. Most of the time, these two uses are consistent with one another,
but not always; antonymous concepts are dissimilar and hence distant in one sense,
and yet are strongly related semantically and hence close in the other sense. We would
thus have very much preferred to be able to adhere to the view of semantic distance
as the inverse of semantic relatedness, not merely of similarity, in the present paper.
Unfortunately, because of the sheer number of methods measuring similarity, as well
as those measuring distance as the ?opposite? of similarity, this would have made
for an awkward presentation. Therefore, we have to ask the reader to rely on context
when interpreting what exactly the expressions semantic distance, semantically distant,
and semantically close mean in each particular case.
Various approaches presented below speak of concepts and words. As a means of
acknowledging the polysemy of language, in this paper the term concept will refer to a
particular sense of a given word. We want to be very clear that, throughout this paper,
when we say that two words are ?similar?, this is a short way of saying that they denote
similar concepts; we are not talking about similarity of distributional or co-occurrence
behavior of the words, for which the term word similarity has also been used (Dagan
2000; Dagan, Lee, and Pereira 1999). While similarity of denotation might be inferred
from similarity of distributional or co-occurrence behavior (Dagan 2000; Weeds 2003),
the two are distinct ideas. We return to the relationship between them in Section 6.2.
When we refer to hierarchies and networks of concepts, we will use both the terms
link and edge to refer to the relationships between nodes; we prefer the former term
when our view emphasizes the taxonomic aspect or the meaning of the network, and
the latter when our view emphasizes algorithmic or graph-theoretic aspects. In running
text, examples of concepts are typeset in sans-serif font, whereas examples of words
are given in italics; in formulas, concepts and words will usually be denoted by c
and w, with various subscripts. For the sake of uniformity of presentation, we have
taken the liberty of altering the original notation accordingly in some other authors?
formulas.
2. Lexical Resource?based Approaches to Measuring Semantic Relatedness
All approaches to measuring semantic relatedness that use a lexical resource construe
the resource, in one way or another, as a network or directed graph, and then base the
measure of relatedness on properties of paths in this graph.
14
Budanitsky and Hirst Lexical Semantic Relatedness
2.1 Dictionary-based Approaches
Kozima and Furugori (1993) turned the Longman Dictionary of Contemporary English
(LDOCE) (Procter 1978) into a network by creating a node for every headword and
linking each node to the nodes for all the words used in its definition. The 2851-
word controlled defining vocabulary of LDOCE thus becomes the densest part of the
network: the remaining nodes, which represent the headwords outside of the defining
vocabulary, can be pictured as being situated at the fringe of the network, as they are
linked only to defining-vocabulary nodes and not to each other. In this network, the
similarity function simKF between words of the defining vocabulary is computed by
means of spreading activation on this network. The function is extended to the rest
of LDOCE by representing each word as a list W = {w1, . . . , wr} of the words in its
definition; thus, for instance,
simKF(linguistics, stylistics)
= simKF({the, study, of, language, in, general, and, of, particular,
languages, and, their, structure, and, grammar, and, history},
{the, study, of, style, in, written, or, spoken, language})
Kozima and Ito (1997) built on this work to derive a context-sensitive, or dynamic,
measure that takes into account the ?associative direction? of a given word pair. For
example, the context {car, bus} imposes the associative direction of vehicle (close words
are then likely to include taxi, railway, airplane, etc.), whereas the context {car, engine}
imposes the direction of components of car (tire, seat, headlight, etc.).
2.2 Approaches Based on Roget-structured Thesauri
Roget-structured thesauri, such as Roget?s Thesaurus itself, the Macquarie Thesaurus
(Bernard 1986), and others, group words in a structure based on categories within which
there are several levels of finer clustering. The categories themselves are grouped into
a number of broad, loosely defined classes. However, while the classes and categories
are named, the finer divisions are not; the words are clustered without attempting to
explicitly indicate how and why they are related. The user?s main access is through
the index, which contains category numbers along with labels representative of those
categories for each word. Polysemes are implicitly disambiguated, to a certain extent,
by the other words in their cluster and in their index entry. Closely related concepts
might or might not be physically close in the thesaurus: ?Physical closeness has some
importance . . . but words in the index of the thesaurus often have widely scattered
categories, and each category often points to a widely scattered selection of cate-
gories? (Morris and Hirst 1991). Methods of semantic distance that are based on Roget-
structured thesauri therefore rely not only on the category structure but also on the
index and on the pointers within categories that cross-reference other categories. In
part as a consequence of this, typically no numerical value for semantic distance can
be obtained: rather, algorithms using the thesaurus compute a distance implicitly and
return a boolean value of ?close? or ?not close?.
Working with an abridged version of Roget?s Thesaurus, Morris and Hirst (1991)
identified five types of semantic relations between words. In their approach, two words
15
Computational Linguistics Volume 32, Number 1
were deemed to be related to one another, or semantically close, if their base forms
satisfy any one of the following conditions:
1. They have a category in common in their index entries.
2. One has a category in its index entry that contains a pointer to a category
of the other.
3. One is either a label in the other?s index entry or is in a category of the
other.
4. They are both contained in the same subcategory.
5. They both have categories in their index entries that point to a common
category.
These relations account for such pairings as wife and married, car and driving, blind and
see, reality and theoretically, brutal and terrified. (However, different editions of Roget?s
Thesaurus yield somewhat different sets of relations.) Of the five types of relations,
perhaps the most intuitively plausible ones ? the first two in the list above ? were
found to validate over 90% of the intuitive lexical relationships that the authors used as
a benchmark in their experiments.
Jarmasz and Szpakowicz (2003) also implemented a similarity measure with Roget?s
Thesaurus; but because this measure is based strictly on hierachy rather than the index
structure, we discuss it in Section 2.4 below.
2.3 Approaches Using WordNet and Other Semantic Networks
Most of the methods discussed in the remainder of Section 2 use WordNet (Fellbaum
1998), a broad coverage lexical network of English words. Nouns, verbs, adjectives,
and adverbs are each organized into networks of synonym sets (synsets) that each
represent one underlying lexical concept and are interlinked with a variety of relations.
(A polysemous word will appear in one synset for each of its senses.) In the first versions
of WordNet (those numbered 1.x), the networks for the four different parts of speech
were not linked to one another.1 The noun network of WordNet was the first to be richly
developed, and most of the researchers whose work we will discuss below therefore
limited themselves to this network.2
The backbone of the noun network is the subsumption hierarchy (hyponymy/
hypernymy), which accounts for close to 80% of the relations. At the top of the hier-
archy are 11 abstract concepts, termed unique beginners, such as entity (?something
having concrete existence; living or nonliving?) and psychological feature (?a feature of
the mental life of a living organism?). The maximum depth of the noun hierarchy is
16 nodes. The nine types of relations defined on the noun subnetwork, in addition to
the synonymy relation that is implicit in each node are: the hyponymy (IS-A) relation,
1 We began this work with WordNet 1.5, and stayed with this version despite newer releases in order
to maintain strict comparability. Our experiments were complete before WordNet 2.0 was released.
2 It seems to have been tacitly assumed by these researchers that results would generalize to the network
hierarchies of other parts of speech. Nonetheless, Resnik and Diab (2000) caution that the properties of
verbs and nouns might be different enough that they should be treated as separate problems, and recent
research by Banerjee and Pedersen (2003) supports this assumption: they found that in a word-sense
disambiguation task, their gloss-overlap measure of semantic relatedness (see Section 6.1 below)
performed far worse on verbs (and slightly worse on adjectives) than it did on nouns.
16
Budanitsky and Hirst Lexical Semantic Relatedness
and its inverse, hypernymy; six meronymic (PART-OF) relations ? COMPONENT-OF,
MEMBER-OF and SUBSTANCE-OF and their inverses; and antonymy, the COMPLEMENT-
OF relation.
In discussing WordNet, we use the following definitions and notation:
 The length of the shortest path in WordNet from synset ci to synset cj
(measured in edges or nodes) is denoted by len(ci, cj). We stipulate a
global root root above the 11 unique beginners to ensure the existence
of a path between any two nodes.
 The depth of a node is the length of the path to it from the global root,
i.e., depth(ci) = len(root, ci).
 We write lso(c1, c2) for the lowest super-ordinate (or most specific
common subsumer) of c1 and c2.
 Given any formula rel(c1, c2) for semantic relatedness between two
concepts c1 and c2, the relatedness rel(w1, w2) between two words w1
and w2 can be calculated as
rel(w1, w2) = max
c1?s(w1),c2?s(w2 )
[rel(c1, c2)] (1)
where s(wi) is ?the set of concepts in the taxonomy that are senses of word
wi? (Resnik 1995). That is, the relatedness of two words is equal to that of
the most-related pair of concepts that they denote.
2.4 Computing Taxonomic Path Length
A simple way to compute semantic relatedness in a taxonomy such as WordNet is to
view it as a graph and identify relatedness with path length between the concepts:
?The shorter the path from one node to another, the more similar they are? (Resnik
1995). This approach was taken, for example, by Rada and colleagues (Rada et al 1989;
Rada and Bicknell 1989), not on WordNet but on MeSH (Medical Subject Headings),
a semantic hierarchy of terms used for indexing articles in the bibliographic retrieval
system Medline. The network?s 15,000 terms form a nine-level hierarchy based on the
BROADER-THAN relationship. The principal assumption of Rada and colleagues was
that ?the number of edges between terms in the MeSH hierarchy is a measure of
conceptual distance between terms?. Despite the simplicity of this distance function,
the authors were able to obtain surprisingly good results in their information retrieval
task. In part, their success can be explained by the observation of Lee, Kim, and
Lee (1993) that while ?in the context of . . . semantic networks, shortest path lengths
between two concepts are not sufficient to represent conceptual distance between
those concepts . . . when the paths are restricted to IS-A links, the shortest path length
does measure conceptual distance.? Another component of their success is certainly
the specificity of the domain, which ensures relative homogeneity of the hierarchy.
Notwithstanding these qualifications, Jarmasz and Szpakowicz (2003) also achieved
good results with Roget?s Thesaurus by ignoring the index and treating the thesaurus
as a simple hierarchy of clusters. They computed semantic similarity between two
words as the length of the shortest path between them. The words were not explicitly
disambiguated.
17
Computational Linguistics Volume 32, Number 1
Hirst and St-Onge (1998; St-Onge 1995) adapted Morris and Hirst?s (1991) seman-
tic distance algorithm from Roget?s Thesaurus to WordNet.3 They distinguished two
strengths of semantic relations in WordNet. Two words are strongly related if one of
the following holds:
1. They have a synset in common (for example, human and person).
2. They are associated with two different synsets that are connected by the
antonymy relation (for example, precursor and successor).
3. One of the words is a compound (or a phrase) that includes the other and
?there is any kind of link at all between a synset associated with each
word? (for example, school and private school).
Two words are said to be in a medium-strong, or regular, relation if there exists an
allowable path connecting a synset associated with each word (for example, carrot and
apple). A path is allowable if it contains no more than five links and conforms to one
of eight patterns, the intuition behind which is that ?the longer the path and the more
changes of direction, the lower the weight?. The details of the patterns are outside of
the scope of this paper; all we need to know for the purposes of subsequent discussion
is that an allowable path may include more than one link and that the directions of
links on the same path may vary (among upward (hypernymy and meronymy), downward
(hyponymy and holonymy) and horizontal (antonymy)). Hirst and St-Onge?s approach may
thus be summarized by the following formula for two WordNet concepts c1 = c2:
relHS(c1, c2) = C ? len(c1, c2) ? k ? turns(c1, c2) (2)
where C and k are constants (in practice, they used C = 8 and k = 1), and turns(c1, c2) is
the number of times the path between c1 and c2 changes direction.
2.5 Scaling the Network
Despite its apparent simplicity, a widely acknowledged problem with the edge-
counting approach is that it typically ?relies on the notion that links in the taxonomy
represent uniform distances?, which is typically not true: ?there is a wide variability
in the ?distance? covered by a single taxonomic link, particularly when certain sub-
taxonomies (e.g., biological categories) are much denser than others? (Resnik 1995).
For instance, in WordNet, the link rabbit ears IS-A television antenna covers an intuitively
narrow distance, whereas white elephant IS-A possession covers an intuitively wide one.
The approaches discussed below are attempts undertaken by various researchers to
overcome this problem.
2.5.1 Sussna?s Depth-relative Scaling. Sussna?s (1993, 1997) approach to scaling is
based on his observation that sibling-concepts deep in a taxonomy appear to be more
closely related to one another than those higher up. His method construes each edge
3 The original ideas and definitions of Hirst and St-Onge (1998) (including those for the direction of
links ? see below) were intended to apply to all parts of speech and the entire range of relations featured
in the WordNet ontology (which include cause, pertinence, also see, etc.). Like other researchers, however,
they had to resort to the noun subnetwork only. In what follows, therefore, we will use appropriately
restricted versions of their notions.
18
Budanitsky and Hirst Lexical Semantic Relatedness
in the WordNet noun network as consisting of two directed edges representing inverse
relations. Each relation r has a weight or a range [minr; maxr] of weights associated with
it: for example, hypernymy, hyponymy, holonymy, and meronymy have weights between
minr = 1 and maxr = 2.4 The weight of each edge of type r from some node c1 is
reduced by a factor that depends on the number of edges, edgesr, of the same type
leaving c1:
wt(c1 ?r) = maxr ?
maxr ? minr
edgesr(c1)
(3)
The distance between two adjacent nodes c1 and c2 is then the average of the weights
on each direction of the edge, scaled by the depth of the nodes:
distS(c1, c2) =
wt(c1 ?r) + wt(c2 ?r? )
2 ? max{depth(c1), depth(c2)}
(4)
where r is the relation that holds between c1 and c2 and r? is its inverse (i.e., the relation
that holds between c2 and c1). Finally, the semantic distance between two arbitrary
nodes ci and cj is the sum of the distances between the pairs of adjacent nodes along
the shortest path connecting them.
2.5.2 Wu and Palmer?s Conceptual Similarity. In a paper on translating English verbs
into Mandarin Chinese, Wu and Palmer (1994) introduce a scaled metric for what they
call conceptual similarity between a pair of concepts c1 and c2 in a hierarchy as
simWP(c1, c2) =
2 ? depth(lso(c1, c2))
len(c1, lso(c1, c2)) + len(c2, lso(c1, c2)) + 2 ? depth(lso(c1, c2))
(5)
Note that depth(lso(c1, c2)) is the ?global? depth in the hierarchy; its role as a scaling
factor can be seen more clearly if we recast equation 5 from similarity into distance:
distWP(c1, c2) = 1 ? simWP(c1, c2)
=
len(c1, lso(c1, c2)) + len(c2, lso(c1, c2))
len(c1, lso(c1, c2)) + len(c2, lso(c1, c2)) + 2 ? depth(lso(c1, c2))
(6)
2.5.3 Leacock and Chodorow?s Normalized Path Length. Leacock and Chodorow
(1998) proposed the following formula for computing the scaled semantic similarity
between concepts c1 and c2 in WordNet:
simLC(c1, c2) = ? log
len(c1, c2)
2 ? max
c?WordNet
depth(c)
(7)
Here, the denominator includes the maximum depth of the hierarchy.
4 Sussna?s experiments proved the precise details of the weighting scheme to be material only in
fine-tuning the performance.
19
Computational Linguistics Volume 32, Number 1
2.6 Information-based and Integrated Approaches
Like the methods in the preceding subsection, the final group of approaches that we
present attempt to counter problems inherent in a general ontology by incorporating an
additional, and qualitatively different, knowledge source, namely information from a
corpus.
2.6.1 Resnik?s Information-based Approach. The key idea underlying Resnik?s (1995)
approach is the intuition that one criterion of similarity between two concepts is ?the
extent to which they share information in common?, which in an IS-A taxonomy can
be determined by inspecting the relative position of the most-specific concept that
subsumes them both. This intuition seems to be indirectly captured by edge-counting
methods (such as that of Rada and colleagues; section 2.4 above), in that ?if the minimal
path of IS-A links between two nodes is long, that means it is necessary to go high in the
taxonomy, to more abstract concepts, in order to find a least upper bound?. An example
given by Resnik is the difference in the relative positions of the most-specific subsumer
of nickel and dime ? coin ? and that of nickel and credit card ? medium of exchange, as
seen in Figure 1.
In mathematical terms, for any concept c in the taxonomy, let p(c) be the probabil-
ity of encountering an instance of concept c. Following the standard definition from
information theory, the information content of c, IC(c), is then ? log p(c). Thus, we can
define the semantic similarity of a pair of concepts c1 and c2, as
simR(c1, c2) = ? log p(lso(c1, c2)). (8)
Notice that p is monotonic as one moves up the taxonomy: if c1 IS-A c2 then p(c1) ? p(c2).
For example, whenever we encounter a nickel, we have encountered a coin (Figure 1),
so p(nickel) ? p(coin). As a consequence, the higher the position of the most-specific
subsumer for given two concepts in the taxonomy (i.e., the more abstract it is), the lower
their similarity. In particular, if the taxonomy has a unique top node, its probability
will be 1, so if the most-specific subsumer of a pair of concepts is the top node, their
similarity will be ? log(1) = 0, as desired.
Figure 1
Fragment of the WordNet taxonomy, showing most-specific subsumers of nickel and dime and of
nickel and credit card. Solid lines represent IS-A links; dashed lines indicate that some intervening
nodes have been omitted. Adapted from Resnik (1995).
20
Budanitsky and Hirst Lexical Semantic Relatedness
In Resnik?s experiments, the probabilities of concepts in the taxonomy were esti-
mated from noun frequencies gathered from the one-million-word Brown Corpus of
American English (Francis and Kuc?era 1982). The key characteristic of his counting
method is that an individual occurrence of any noun in the corpus ?was counted as
an occurrence of each taxonomic class containing it?. For example, an occurrence of
the noun nickel was, in accordance with Figure 1, counted towards the frequency of
nickel, coin, and so forth. Notice that, as a consequence of using raw (non-disambiguated)
data, encountering a polysemous word contributes to the counts of all its senses. So in
the case of nickel, the counts of both the coin and the metal senses will be increased.
Formally,
p(c) =
?
w?W(c) count(w)
N (9)
where W(c) is the set of words (nouns) in the corpus whose senses are subsumed by
concept c, and N is the total number of word (noun) tokens in the corpus that are also
present in WordNet.
Thus Resnik?s approach attempts to deal with the problem of varying link distances
(see Section 2.5) by generally downplaying the role of network edges in the determi-
nation of the degree of semantic proximity: Edges are used solely for locating super-
ordinates of a pair of concepts; in particular, the number of links does not figure in any
of the formulas pertaining to the method; and numerical evidence comes from corpus
statistics, which are associated with nodes. This rather selective use of the structure
of the taxonomy has its drawbacks, one of which is the indistinguishability, in terms of
semantic distance, of any two pairs of concepts having the same most-specific subsumer.
For example, in Figure 1, we find that simR(money, credit) = simR(dime, credit card), be-
cause in each case the lso is medium of exchange, whereas, for an edge-based method
such as Leacock and Chodorow?s (Section 2.5.3), clearly this is not so, as the number of
edges in each case is different.
2.6.2 Jiang and Conrath?s Combined Approach. Reacting to the disadvantages of
Resnik?s method, Jiang and Conrath?s (1997) idea was to synthesize edge- and node-
based techniques by restoring network edges to their dominant role in similarity com-
putations, and using corpus statistics as a secondary, corrective factor. A complete
exegesis of their work is presented by Budanitsky (1999); here we summarize only their
conclusions.
In the framework of the IS-A hierarchy, Jiang and Conrath postulated that the
semantic distance of the link connecting a child-concept c to its parent-concept par(c)
is proportional to the conditional probability p(c | par(c)) of encountering an instance of
c given an instance of par(c). More specifically,
distJC(c, par(c)) = ? log p(c | par(c)) (10)
By definition,
p(c | par(c)) = p(c&par(c))
p(par(c))
(11)
21
Computational Linguistics Volume 32, Number 1
If we adopt Resnik?s scheme for assigning probabilities to concepts (Section 2.6.1),
then p(c&par(c)) = p(c), since any instance of a child is automatically an instance of
its parent. Then,
p(c|par(c)) = p(c)
p(par(c))
(12)
and, recalling the definition of information content,
distJC(c, par(c)) = IC(c) ? IC(par(c)) (13)
Given this as the measure of semantic distance from a node to its immediate parent,
the semantic distance between an arbitrary pair of nodes was taken, as per common
practice, to be the sum of the distances along the shortest path that connects the nodes:
distJC(c1, c2) =
?
c?Path(c1,c2 )lso(c1,c2 )
distJC(c, par(c)) (14)
where Path(c1, c2) is the set of all the nodes in the shortest path from c1 to c2. The node
lso(c1, c2) is removed from Path(c1, c2) in (14)), because it has no parent in the set. Ex-
panding the sum in the right-hand side of equation (14), plugging in the expression for
parent?child distance from equation (13), and performing necessary eliminations results
in the following final formula for the semantic distance between concepts c1 and c2:
distJC(c1, c2) = IC(c1) + IC(c2) ? 2 ? IC(lso(c1, c2)) (15)
= 2 log p(lso(c1, c2)) ? (log p(c1) + log p(c2)) (16)
2.6.3 Lin?s Universal Similarity Measure. Noticing that all of the similarity measures
known to him were tied to a particular application, domain, or resource, Lin (1998b)
attempted to define a measure of similarity that would be both universal (applicable
to arbitrary objects and ?not presuming any form of knowledge representation?) and
theoretically justified (?derived from a set of assumptions?, instead of ?directly by a
formula?, so that ?if the assumptions are deemed reasonable, the similarity measure
necessarily follows?). He used the following three intuitions as a basis:
1. The similarity between arbitrary objects A and B is related to their
commonality; the more commonality they share, the more similar they are.
2. The similarity between A and B is related to the differences between them;
the more differences they have, the less similar they are.
3. The maximum similarity between A and B is reached when A and B are
identical, no matter how much commonality they share.
Lin defined the commonality between A and B as the information content of ?the
proposition that states the commonalities? between them, formally
IC(comm(A, B)) (17)
22
Budanitsky and Hirst Lexical Semantic Relatedness
and the difference between A and B as
IC(descr(A, B)) ? IC(comm(A, B)) (18)
where descr(A, B) is a proposition describing what A and B are.
Given these assumptions and definitions and the apparatus of information theory,
Lin proved the following:
Similarity Theorem: The similarity between A and B is measured by the ratio between
the amount of information needed to state their commonality and the information
needed to fully describe what they are:
simL(A, B) =
log p(comm(A, B))
log p(descr(A, B))
(19)
His measure of similarity between two concepts in a taxonomy is a corollary of this
theorem:
simL(c1, c2) =
2 ? log p(lso(c1, c2))
log p(c1) + log p(c2)
(20)
where the probabilities p(c) are determined in a manner analogous to Resnik?s p(c)
(equation (9)).
3. Evaluation Methods
How can we reason about and evaluate computational measures of semantic related-
ness? Three kinds of approaches are prevalent in the literature.
The first kind (Wei 1993; Lin 1998b) is a (chiefly) theoretical examination of a pro-
posed measure for those mathematical properties thought desirable, such as whether
it is a metric (or the inverse of a metric), whether it has singularities, whether its
parameter-projections are smooth functions, and so on. In our opinion, such analyses
act at best as a coarse filter in the comparison of a set of measures and an even coarser
one in the assessment of a single measure.
The second kind of evaluation is comparison with human judgments. Insofar as
human judgments of similarity and relatedness are deemed to be correct by definition,
this clearly gives the best assessment of the ?goodness? of a measure. Its main drawback
lies in the difficulty of obtaining a large set of reliable, subject-independent judgments
for comparison ? designing a psycholinguistic experiment, validating its results, and so
on. (In Section 4.1 below, we will employ the rather limited data that such experiments
have obtained to date.)
The third approach is to evaluate the measures with respect to their performance
in the framework of a particular application. If some particular NLP system requires a
measure of semantic relatedness, we can compare different measures by seeing which
one the system is most effective with, while holding all other aspects of the system
constant.
In the remainder of this paper, we will use the second and the third methods
to compare several different measures (Sections 4 and 5 respectively). We focus on
measures that use WordNet (Fellbaum 1998) as their knowledge source (to keep
that as a constant) and that permit straightforward implementation as functions in a
23
Computational Linguistics Volume 32, Number 1
programming language. Therefore, we select the following five measures: Hirst and
St-Onge?s (Section 2.4), Jiang and Conrath?s (Section 2.6.2), Leacock and Chodorow?s
(Section 2.5.3), Lin?s (Section 2.6.3), and Resnik?s (Section 2.6.1).5 The first is claimed as
a measure of semantic relatedness because it uses all noun relations in WordNet; the
others are claimed only as measures of similarity because they use only the hyponymy
relation. We implemented each measure, and used the Brown Corpus as the basis for
the frequency counts needed in the information-based approaches.6
4. Comparison with Human Ratings of Semantic Relatedness
In this section we compare the five chosen measures by how well they reflect human
judgments of semantic relatedness. In addition, we will use the data that we obtain
in this section to set closeness thresholds for the application-based evaluation of each
measure in Section 5.
4.1 Data
As part of an investigation into ?the relationship between similarity of context and
similarity of meaning (synonymy)?, Rubenstein and Goodenough (1965) obtained
?synonymy judgements? from 51 human subjects on 65 pairs of words. The pairs
ranged from ?highly synonymous? to ?semantically unrelated?, and the subjects were
asked to rate them, on the scale of 0.0 to 4.0, according to their ?similarity of mean-
ing? (see Table 1, columns 2 and 3). For a similar study, Miller and Charles (1991)
chose 30 pairs from the original 65, taking 10 from the ?high level (between 3 and
4. . . ), 10 from the intermediate level (between 1 and 3), and 10 from the low level
(0 to 1) of semantic similarity?, and then obtained similarity judgments from 38 sub-
jects, given the same instructions as above, on those 30 pairs (see Table 2, columns 2
and 3).7
4.2 Method
For each of our five implemented measures, we obtained similarity or relatedness scores
for the human-rated pairs. Where either or both of the words had more than one synset
in WordNet, we took the most-related pair of synsets. For the measures of Resnik, Jiang
and Conrath, and Lin, this replicates and extends a study by each of the original authors
of their own measure.
5 We also attempted to implement Sussna?s (1993, 1997) measure (Section 2.5.1), but ran into problems
because a key element depended closely on the particulars of an earlier version of WordNet; see
(Budanitsky 1999) for details. We did not include Wu and Palmer?s measure (Section 2.5.2) because Lin
(1998b) has shown it to be a special case of his measure in which all child?parent probabilities are equal.
6 In their original experiments, Lin and Jiang and Conrath used SemCor, a sense-tagged subset of the
Brown Corpus, as their empirical data; but we decided to follow Resnik in using the full and untagged
corpus. While this means trading accuracy for size, we believe that using a non-disambiguated corpus
constitutes a more-general approach, as the availability and size of disambiguated texts such as SemCor
is highly limited.
7 As a result of a typographical error that occurred in the course of either Miller and Charles?s actual
experiments or in its publication, the Rubenstein?Goodenough pair cord?smile became chord?smile.
Probably because of the comparable degree of (dis)similarity, the error was not discovered and the
latter pair has been used in all subsequent work.
24
Budanitsky and Hirst Lexical Semantic Relatedness
Table 1
Human and computer ratings of the Rubenstein?Goodenough set of word pairs (part 1 of 2).
# Pair Humans relHS distJC simLC simL simR
1 cord smile 0.02 0 19.6 1.38 0.09 1.17
2 rooster voyage 0.04 0 26.9 0.91 0.00 0.00
3 noon string 0.04 0 22.6 1.50 0.00 0.00
4 fruit furnace 0.05 0 18.5 2.28 0.14 1.85
5 autograph shore 0.06 0 22.7 1.38 0.00 0.00
6 automobile wizard 0.11 0 17.8 1.50 0.09 0.97
7 mound stove 0.14 0 17.2 2.28 0.22 2.90
8 grin implement 0.18 0 16.6 1.28 0.00 0.00
9 asylum fruit 0.19 0 19.5 2.28 0.14 1.85
10 asylum monk 0.39 0 25.6 1.62 0.07 0.97
11 graveyard madhouse 0.42 0 29.7 1.18 0.00 0.00
12 glass magician 0.44 0 22.8 1.91 0.07 0.97
13 boy rooster 0.44 0 17.8 1.50 0.21 2.38
14 cushion jewel 0.45 0 22.9 2.28 0.13 1.85
15 monk slave 0.57 94 18.9 2.76 0.21 2.53
16 asylum cemetery 0.79 0 28.1 1.50 0.00 0.00
17 coast forest 0.85 0 20.2 2.28 0.12 1.50
18 grin lad 0.88 0 20.8 1.28 0.00 0.00
19 shore woodland 0.90 93 19.3 2.50 0.13 1.50
20 monk oracle 0.91 0 22.7 2.08 0.18 2.53
21 boy sage 0.96 93 19.9 2.50 0.20 2.53
22 automobile cushion 0.97 98 15.0 2.08 0.27 2.90
23 mound shore 0.97 91 12.4 2.76 0.49 6.19
24 lad wizard 0.99 94 16.5 2.76 0.23 2.53
25 forest graveyard 1.00 0 24.5 1.76 0.00 0.00
26 food rooster 1.09 0 17.4 1.38 0.10 0.97
27 cemetery woodland 1.18 0 25.0 1.76 0.00 0.00
28 shore voyage 1.22 0 23.7 1.38 0.00 0.00
29 bird woodland 1.24 0 18.1 2.08 0.13 1.50
30 coast hill 1.26 94 10.8 2.76 0.53 6.19
31 furnace implement 1.37 93 15.8 2.50 0.18 1.85
32 crane rooster 1.41 0 12.8 2.08 0.58 8.88
33 hill woodland 1.48 93 18.2 2.50 0.14 1.50
34 car journey 1.55 0 16.3 1.28 0.00 0.00
35 cemetery mound 1.69 0 23.8 1.91 0.00 0.00
36 glass jewel 1.78 0 22.0 2.08 0.14 1.85
37 magician oracle 1.82 98 1.0 3.50 0.96 13.58
38 crane implement 2.37 94 15.6 2.76 0.27 2.90
39 brother lad 2.41 94 16.3 2.76 0.23 2.53
40 sage wizard 2.46 93 22.8 2.50 0.18 2.53
41 oracle sage 2.61 0 26.2 2.08 0.16 2.53
42 bird crane 2.63 97 7.4 3.08 0.70 8.88
43 bird cock 2.63 150 5.4 4.08 0.76 8.88
44 food fruit 2.69 0 10.2 2.28 0.22 1.50
45 brother monk 2.74 93 19.2 2.50 0.20 2.53
46 asylum madhouse 3.04 150 0.2 4.08 0.99 15.70
47 furnace stove 3.11 0 20.5 2.08 0.13 1.85
48 magician wizard 3.21 200 0.00 5.08 1.00 13.58
49 hill mound 3.29 200 0.00 5.08 1.00 12.08
50 cord string 3.41 150 2.2 4.08 0.89 9.25
51 glass tumbler 3.45 150 5.9 4.08 0.79 11.34
52 grin smile 3.46 200 0.0 5.08 1.00 10.41
53 serf slave 3.46 0 19.8 2.28 0.34 5.28
54 journey voyage 3.58 150 5.2 4.08 0.74 7.71
25
Computational Linguistics Volume 32, Number 1
Table 1
(cont.)
# Pair Humans relHS distJC simLC simL simR
55 autograph signature 3.59 150 2.4 4.08 0.92 14.29
56 coast shore 3.60 150 0.8 4.08 0.96 11.12
57 forest woodland 3.65 200 0.0 5.08 1.00 11.23
58 implement tool 3.66 150 1.1 4.08 0.91 6.20
59 cock rooster 3.68 200 0.0 5.08 1.00 14.29
60 boy lad 3.82 150 5.3 4.08 0.72 8.29
61 cushion pillow 3.84 150 0.7 4.08 0.97 13.58
62 cemetery graveyard 3.88 200 0.0 5.08 1.00 13.76
63 automobile car 3.92 200 0.0 5.08 1.00 8.62
64 midday noon 3.94 200 0.0 5.08 1.00 15.96
65 gem jewel 3.94 200 0.0 5.08 1.00 14.38
4.3 Results
The mean ratings from Rubenstein and Goodenough?s and Miller and Charles?s original
experiments (labeled ?Humans?) and the ratings of the Rubenstein?Goodenough and
Miller?Charles word pairs produced by (our implementations of) the Hirst?St-Onge,
Jiang?Conrath, Leacock?Chodorow, Lin, and Resnik measures of relatedness are given
in Tables 1 and 2, and in Figures 2 and 3.8
4.4 Discussion
When comparing two sets of ratings, we are interested in the strength of the linear asso-
ciation between two quantitative variables, so we follow Resnik (1995) in summarizing
the comparison results by means of the coefficient of correlation of each computational
measure with the human ratings; see Table 3. (For Jiang and Conrath?s measure, the
coefficients are negative because their measure returns distance rather than similarity;
so for convenience, we show absolute values in the table.)9
4.4.1 Comparison to Upper Bound. To get an idea of the upper bound on perfor-
mance of a computational measure, we can again refer to human performance. We
have such an upper bound for the Miller and Charles word pairs (but not for the
complete set of Rubenstein and Goodenough pairs): Resnik (1995) replicated Miller and
8 We have kept the original orderings of the pairs: from dissimilar to similar for the Rubenstein?
Goodenough data and from similar to dissimilar for Miller?Charles. This explains why the two groups
of graphs (Figures 2 and 3) as wholes have the opposite directions. Notice that because distJC measures
distance, the Jiang?Conrath plot has a slope opposite to the rest of each group.
9 Resnik (1995), Jiang and Conrath (1997), and Lin (1998b) report the coefficients of correlation between
their measures and the Miller?Charles ratings to be 0.7911, 0.8282, and 0.8339, respectively, which differ
slightly from the corresponding figures in Table 3. These discrepancies can be explained by possible
minor differences in implementation (e.g., the compound-word recognition mechanism used in collecting
the frequency data), differences between the versions of WordNet used in the experiments (Resnik), and
differences in the corpora used to obtain the frequency data (Jiang and Conrath, Lin). Also, the
coefficients reported by Resnik and Lin are actually based on only 28 out of the 30 Miller?Charles pairs
because of a noun missing from an earlier version of WordNet. Jarmasz and Szpakowicz (2003) repeated
the experiment, obtaining similar results to ours in some cases and markedly different results in others; in
their experiment, the correlations obtained with their measure that uses the hierarchy of Roget?s Thesaurus
exceeded those of all the WordNet measures.
26
Budanitsky and Hirst Lexical Semantic Relatedness
Table 2
Human and computer ratings of the Miller?Charles set of word pairs.
# Pair Humans relHS distJC simLC simL simR
1 car automobile 3.92 200 0.0 5.08 1.00 8.62
2 gem jewel 3.84 200 0.0 5.08 1.00 14.38
3 journey voyage 3.84 150 5.2 4.08 0.74 7.71
4 boy lad 3.76 150 5.3 4.08 0.72 8.29
5 coast shore 3.70 150 0.9 4.08 0.96 11.12
6 asylum madhouse 3.61 150 0.2 4.08 0.99 15.70
7 magician wizard 3.50 200 0.0 5.08 1.00 13.58
8 midday noon 3.42 200 0.0 5.08 1.00 15.96
9 furnace stove 3.11 0 20.5 2.08 0.13 1.85
10 food fruit 3.08 0 10.2 2.28 0.22 1.50
11 bird cock 3.05 150 5.4 4.08 0.76 8.88
12 bird crane 2.97 97 7.4 3.08 0.70 8.88
13 tool implement 2.95 150 1.1 4.08 0.91 6.20
14 brother monk 2.82 93 19.2 2.50 0.20 2.53
15 lad brother 1.66 94 16.3 2.76 0.23 2.53
16 crane implement 1.68 94 15.7 2.76 0.27 2.90
17 journey car 1.16 0 16.3 1.28 0.00 0.00
18 monk oracle 1.10 0 22.7 2.08 0.18 2.53
19 cemetery woodland 0.95 0 25.0 1.76 0.00 0.00
20 food rooster 0.89 0 17.4 1.38 0.10 0.97
21 coast hill 0.87 94 10.9 2.76 0.53 6.19
22 forest graveyard 0.84 0 24.6 1.76 0.00 0.00
23 shore woodland 0.63 93 19.3 2.50 0.13 1.50
24 monk slave 0.55 94 18.9 2.76 0.21 2.53
25 coast forest 0.42 0 20.2 2.28 0.12 1.50
26 lad wizard 0.42 94 16.5 2.76 0.23 2.53
27 chord smile 0.13 0 20.2 1.62 0.18 2.23
28 glass magician 0.11 0 22.8 1.91 0.07 0.97
29 rooster voyage 0.08 0 26.9 0.91 0.00 0.00
30 noon string 0.08 0 22.6 1.50 0.00 0.00
Charles?s experiment with 10 subjects and found that the average correlation with the
Miller?Charles mean ratings over his subjects was 0.8848. While the difference between
the (absolute) values of the highest and lowest correlation coefficients in the ?M&C?
column of Table 3 is of the order of 0.1, all of the coefficients compare quite favorably
with this estimate of the upper bound; furthermore, the difference diminishes almost
twofold as we consider the larger Rubenstein?Goodenough dataset (column ?R&G? of
Table 3).10 In fact, the measures are divided in their reaction to increasing the size of
the dataset: the correlations of relHS, simLC, and simR improve but those of distJC and
simL deteriorate. This division might not be arbitrary: the last two depend on the same
three quantities, log p(c1), log p(c2), and log p(lso(c1, c2)) (see equations (16) and (20)).
(In fact, the coefficient for simR, which depends on only one of the three quantities,
log p(lso(c1, c2)), improves only in the third digit.) However, with the present paucity of
evidence, this connection remains hypothetical.
10 None of the differences in either column are statistically significant at the .05 level.
27
Computational Linguistics Volume 32, Number 1
4.4.2 Differences in the Performance and Behavior of the Measures. We now examine
the results of each of the measures and the differences between them. To do this, we will
sometimes look at differences in their behavior on individual word pairs.
Looking at the graphs in Figures 2 and 3, we see that the discrete nature of the
Hirst?St-Onge and Leacock?Chodorow measures is much more apparent than that of
the others: i.e., the values that they can take on are just a fixed number of levels. This is,
of course, a result of their being based on the same highly discrete factor: the path length.
As a matter of fact, a more substantial correspondence between the two measures can
be recognized from the graphs and explained in the same way. In each dataset, the
upper portions of the two graphs are identical: namely, the sets of pairs affording the
highest and second-highest values of the two measures (relHS ? 150, simLC > 4). This
happens because these sets are composed of WordNet synonym and parent-child pairs,
respectively.11
Further down the Y-axis, we find that for the Miller?Charles data, the two graphs
still follow each other quite closely in the middle region (2.4?3.2 for simLC and 90?100 for
relHS). For the larger set of Rubenstein and Goodenough?s, however, differences appear.
The pair automobile?cushion (#22), for instance, is ranked even with magician?oracle (#37)
by the Hirst?St-Onge measure but far below both magician?oracle (#37) and bird?crane
(#42) by Leacock?Chodorow (and, in fact, by all the other measures). The cause of such
a high ranking in the former case is the following meronymic connection in WordNet:
automobile/. . . /car HAS-A suspension/suspension system (?a system of springs or shock
absorbers connecting the wheels and axles to the chassis of a wheeled vehicle?)
HAS-A cushion/shock absorber/shock (?a mechanical damper; absorbs energy of sudden
impulses?).
Since relHS is the only measure that takes meronymy (and other WordNet relations
beyond IS-A) into account, no other measure detected this connection ? nor did the hu-
man judges, whose task was to assess similarity, not generic relatedness; see Section 4.1).
Finally, at the bottom portion of these two graphs, the picture becomes very dif-
ferent, because relHS assigns all weakly-related pairs the value of zero. (In fact, it is this
cut-off that we believe to be largely responsible for the relatively low ranking of the
correlation coefficient of the Hirst?St-Onge measure.) In contrast, two other measures,
Resnik?s and Lin?s, behave quite similarly to each other in the low-similarity region. In
particular, their sets of zero-similarity pairs are identical, because the definitions of both
measures include the term log p(lso(c1, c2)), which is zero for the pairs in question.12 For
instance, for the pair rooster?voyage (M&C #29, R&G #2), the synsets rooster and voyage
have different ?unique beginners?, and hence their lso ? in fact their sole common
subsumer ? is the (fake) global root (see Section 2.5.3), which is the only concept
whose probability is 1:
cock/rooster (?adult male chicken?) IS-A . . . IS-A domestic fowl/. . . /poultry IS-A . . . IS-A
bird IS-A . . . IS-A animal/animate being/. . . /fauna IS-A life form/. . . /living thing (?any living
entity?) IS-A entity (?something having concrete existence; living or nonliving?) IS-A
global root,
11 More generally, the inverse image of the second highest value for simLC is a proper subset of that for
relHS, for the latter would also include all the antonym and meronym?holonym pairs. The two datasets
at hand, however, do not contain any instances from these categories.
12 Again (cf. footnote 11), the former set actually constitutes a proper subset of the latter, as simL(c1, c2)
will also be zero if either concept does not occur in the frequency-corpus (see equation (20)). However,
no such instances appear in the data.
28
Budanitsky and Hirst Lexical Semantic Relatedness
Figure 2
Human and computer ratings of the Rubenstein?Goodenough set of word pairs, with sparse
bands marked (see text). From left to right and top to bottom: The word pairs rated by
(a) Rubenstein and Goodenough?s subjects; (b) by the Hirst?St-Onge similarity measure;
(c) by the Jiang?Conrath distance measure; (d) by the Leacock?Chodorow similarity measure;
(e) by the Lin similarity measure; and (f) by the Resnik similarity measure.
29
Computational Linguistics Volume 32, Number 1
Figure 3
Human and computer ratings of the Miller?Charles set of word pairs, with sparse bands marked
(see text). From left to right and top to bottom: The word pairs rated by (a) Miller and Charles?s
subjects; (b) by the Hirst?St-Onge similarity measure; (c) by the Jiang?Conrath distance measure;
(d) by the Leacock?Chodorow similarity measure; (e) by the Lin similarity measure; and (f) by
the Resnik similarity measure.
30
Budanitsky and Hirst Lexical Semantic Relatedness
Table 3
The absolute values of the coefficients of correlation between human ratings of similarity (by
Miller and Charles and by Rubenstein and Goodenough) and the five computational measures.
Measure M&C R&G
?????????????????????
Hirst and St-Onge, relHS .744 .786
Jiang and Conrath, distJC .850 .781
Leacock and Chodorow, simLC .816 .838
Lin, simL .829 .819
Resnik, simR .774 .779
voyage IS-A journey/journeying IS-A travel/. . . /traveling IS-A change of location/. . . /motion IS-A
change (?the act of changing something?) IS-A action (?something done (usually as
opposed to something said)?) IS-A act/human action/human activity (?something that
people do or cause to happen?) IS-A global root.
Analogously, although perhaps somewhat more surprisingly for a human reader, the
same is true of the pair asylum?cemetery (R&G #16):
asylum/insane asylum/. . . /mental hospital IS-A hospital/infirmary IS-A medical building
(?a building where medicine is practiced?) IS-A building/edifice IS-A . . . IS-A artifact/
artefact (?a man-made object?) IS-A object/inanimate object/physical object (?a nonliving
entity?) IS-A entity IS-A global root,
cemetery/graveyard/. . . /necropolis (?a tract of land used for burials?) IS-A site (?the piece
of land on which something is located (or is to be located)?) IS-A position/place (?the
particular portion of space occupied by a physical object?) IS-A . . . IS-A location (?a point
or extent in space?) IS-A global root.
Looking back at the high-similarity portion of the graphs, but now taking into con-
sideration the other three measures, we can make a couple more observations. First, the
graphs of all of the measures except Resnik?s exhibit a ?line? of synonyms (comprising
four points for the Miller?Charles dataset and nine points for Rubenstein?Goodenough)
at the top (bottom for Jiang and Conrath?s measure). In the case of Resnik?s measure,
simR(c, c) = ? log p(lso(c, c)) = ? log p(c) (see equation (8)), and hence the similarity of
a concept to itself varies from one concept to another. Second, these ?lines? are not
continuous, as one might expect from the graphs of the human judgments: For the
Miller?Charles set, for instance, the line includes pairs 1, 2, 7, and 8, but omits pairs
3?6. This peculiarity is due entirely to WordNet, according to which gem and jewel (#2)
and magician and wizard (#7) are synonyms, whereas journey and voyage (#3), boy and lad
(#4), and even asylum and madhouse (#6) are not, but rather are related by IS-A:
voyage (?a journey to some distant place?) IS-A journey/journeying (?the act of traveling
from one place to another?),
lad/laddie/cub/sonny/sonny boy (?a male child (a familiar term of address to a boy)?) IS-A
boy/male child/child (?a young male person?),
madhouse/nuthouse/. . . /sanatorium (?pejorative terms for an insane asylum?) IS-A
asylum/insane asylum/. . . /mental hospital (?a hospital for mentally incompetent or
unbalanced persons?).
31
Computational Linguistics Volume 32, Number 1
Although, as we saw above, already for two measures the details of their medium-
similarity regions differ, there appears to be an interesting commonality at the level of
general structure: in the vicinity of sim = 2, the plots of human similarity ratings for
both the Miller?Charles and the Rubenstein?Goodenough word pairs display a very
clear horizontal band that contains no points. For the Miller?Charles data (Figure 3),
the band separates the pair crane?implement (#16) from brother?monk (#14),13 and for the
Rubenstein-Goodenough set (Figure 2), it separates magician?oracle (#37) from crane?
implement (#38).
On the graphs of the computed ratings, these empty bands correspond to regions
with at most a few points?no more than two points for the Miller?Charles set and
no more than four for the Rubenstein?Goodenough set. These regions are shown in
Figures 3(b)?(f) and 2(b)?(f). This commonality among the measures suggests that if
we were to partition the set of all word pairs into those that are deemed to be related
and those that are deemed unrelated, the boundary between the two subsets for each
measure (and for the human judgments, for that matter) would lie somewhere within
these regions.
4.4.3 The Limitations of this Analysis. While comparison with human judgments is the
ideal way to evaluate a measure of similarity or semantic relatedness, in practice the tiny
amount of data available (and only for similarity, not relatedness) is quite inadequate.
But constructing a large-enough set of pairs and obtaining human judgments on them
would be a very large task.14
Even more importantly, there are serious methodological problems with this ap-
proach. It was implicit in the Rubenstein?Goodenough and Miller?Charles experiments
that subjects were to use the dominant sense of the target words or mutually triggering
related senses. But often what we are really interested in is the relationship between the
concepts for which the words are merely surrogates; the human judgments that we need
are of the relatedness of word-senses, not words. So the experimental situation would
need to set up contexts that bias the sense selection for each target word and yet don?t
bias the subject?s judgment of their a priori relationship, an almost self-contradictory
situation.15
5. An Application-based Evaluation of Measures of Relatedness
We now turn to a different approach to the evaluation of similarity and relatedness
measures that tries to overcome the problems of comparison to human judgments that
were described in the previous section. Here, we compare the measures through the
13 For some reason, Miller and Charles, while generally ordering their pairs from least to most similar, put
crane?implement (#16) after lad?brother(#15), even though the former was rated more similar.
14 Evgeniy Gabrilovich has recently made available a dataset of similarity judgments of 353 English word
pairs that were used by Finkelstein et al (2002). Unfortunately, this set is still very small, and, as Jarmasz
and Szpakowicz (2003) point out, is culturally and politically biased. And the scarcely larger set of
synonymy norms for nouns created by Whitten, Suter, and Frank (1979) covers only words with quite
closely related senses, and hence is not useful here either.
15 In their creation of a set of synonymy norms for nouns, Whitten, Suter, and Frank (1979) observed
frequent artifacts stemming from the order of presentation of the stimuli that seem to be due to the
practical impossibility of forcing a context of interpretation in the experimental setting.
32
Budanitsky and Hirst Lexical Semantic Relatedness
performance of an application that uses them: the detection and correction of real-word
spelling errors in open-class words, i.e., malapropisms.
While malapropism correction is also a useful application in its own right, it is
particularly appropriate for evaluating measures of semantic relatedness. Naturally
occurring coherent texts, by their nature, contain many instances of related pairs of
words (Halliday and Hasan 1976; Morris and Hirst 1991; Hoey 1991; Morris and Hirst
2004). That is, they implicitly contain human judgments of relatedness that we could
use in the evaluation of our relatedness measures. But, of course, we don?t know in
practice just which pairs of words in a text are and aren?t related. We can get around
this problem, however, by deliberately perturbing the coherence of the text ? that is,
introduding semantic anomalies such as malapropisms ? and looking at the ability of
the different relatedness measures to detect and correct the perturbations.
5.1 Malapropism Detection and Correction as a Testbed
Our malapropism corrector (Hirst and Budanitsky 2005) is based on the idea behind that
of Hirst and St-Onge (1998): Look for semantic anomalies that can be removed by small
changes to spelling.16 Words are (crudely) disambiguated where possible by accepting
senses that are semantically related to possible senses of other nearby words. If all senses
of any open-class, non?stop-list word that occurs only once in the text are found to be
semantically unrelated to accepted senses of all other nearby words, but some sense of
a spelling variation of that word would be related (or is identical to another token in the
context), then it is hypothesized that the original word is an error and the variation is
what the writer intended; a user would be warned of this possibility. For example, if no
nearby word in a text is related to diary but one or more are related to dairy, we suggest
to the user that it is the latter that was intended. The exact window size implied by
?nearby? is a parameter to the algorithm, as is the precise definition of spelling variation;
see Hirst and Budanitsky (2005).
This method makes the following assumptions:
 A real-word spelling error is unlikely to be semantically related to the
text.17
 Frequently, the writer?s intended word will be semantically related to
nearby words.
 It is unlikely that an intended word that is semantically unrelated to all
those nearby will have a spelling variation that is related.
While the performance of the malapropism corrector is inherently limited by these
assumptions, we can nonetheless evaluate measures of semantic relatedness by com-
paring their effect on its performance, as its limitations affect all measures equally.
Regardless of the degree of adequacy of its performance, it is a ?level playing field? for
16 Although it shares underlying assumptions, our algorithm differs from that of Hirst and St-Onge in its
mechanisms. In particular, Hirst and St-Onge?s algorithm was based on lexical chains (Morris and Hirst
1991), whereas our algorithm regards regions of text as bags of words.
17 In fact, there is a semantic bias in human typing errors (Fromkin 1980), but not in the malapropism
generator to be described below.
33
Computational Linguistics Volume 32, Number 1
comparison of the measures. Hirst and Budanitsky (2005) discuss the practical aspects
of the method and compare it with other approaches to the same problem.
5.2 Method
To test the measures in this application, we need a sufficiently large corpus of
malapropisms in their context, each identified and annotated with its correction. Since
no such corpus of naturally occurring malapropisms exists, we created one artificially.
Following Hirst and St-Onge (1998), we took 500 articles from the Wall Street Journal
corpus and, after removing proper nouns and stop-list words from consideration,
replaced one word in every 200 with a spelling variation, choosing always WordNet
nouns with at least one spelling variation.18 For example, in a sentence beginning
To win the case, which was filed shortly after the indictment and is pending in Manhattan
federal court . . . , the word case was replaced by cage. This gave us a corpus with 1,408
malapropisms among 107,233 candidates.19 We then tried to detect and correct the
malapropisms by the algorithm outlined above, using in turn each of the five measures
of semantic relatedness. For each, we used four different search scopes, i.e., window sizes:
just the paragraph containing the target word (scope = 1); that paragraph plus one
or two adjacent paragraphs on each side (scope = 3 and 5); and the complete article
(scope = MAX).
We also needed to set a threshold of ?relatedness? for each of the measures. This
is because the malapropism-detection algorithm requires a boolean related?unrelated
judgment, but each of the measures that we tested instead returns a numerical value
of relatedness or similarity, and nothing in the measure (except for the Hirst?St-Onge
measure) indicates which values count as ?close?. Moreover, the values from the dif-
ferent measures are incommensurate. We therefore set the threshold of relatedness of
each measure at the value at which it separated the higher level of the Rubenstein?
Goodenough pairs (the near-synonyms) from the lower level, as we described in Sec-
tion 4.4.2.
5.3 Results
Malapropism detection was viewed as a retrieval task and evaluated in terms of preci-
sion, recall, and F-measure. Observe that semantic relatedness is used at two different
places in the algorithm ? to judge whether an original word of the text is related to
any nearby word and to judge whether a spelling variation is related ? and success in
malapropism detection requires success at both stages. For the first stage, we say that
a word is suspected of being a malapropism (and the word is a suspect) if it is judged
to be unrelated to other words nearby; the word is a correct suspect if it is indeed a
malapropism and a false suspect if it isn?t. At the second stage, we say that, given a
suspect, an alarm is raised when a spelling variation of the suspect is judged to be
related to a nearby word or words; and if an alarm word is a malapropism, we say
that the alarm is a true alarm and that the malapropism has been detected; otherwise,
it is a false alarm. Then we can define precision (P), recall (R), and F-measure (F) for
18 Articles too small to warrant such a replacement (19 in total) were excluded from further consideration.
19 We assume that the original Wall Street Journal, being carefully edited text, contains essentially no
malapropisms of its own.
34
Budanitsky and Hirst Lexical Semantic Relatedness
suspicion (S), involving only the first stage, and detection (D), involving both stages,
as follows:
Suspicion:
PS =
number of correct suspects
number of suspects
(21)
RS =
number of correct suspects
number of malapropisms in text
(22)
FS =
2 ? PS ? RS
PS + RS
(23)
Detection:
PD =
number of true alarms
number of alarms
(24)
RD =
number of true alarms
number of malapropisms in text
(25)
FD =
2 ? PD ? RD
PD + RD
(26)
The precision, recall, and F values are computed as the mean values of these
statistics across our collection of 481 articles, which constitute a random sample from
the population of all WSJ articles. All the comparisons that we make below, except
for comparisons to baseline, are performed with the Bonferroni multiple-comparison
technique (Agresti and Finlay 1997), with an overall significance level of .05.
5.3.1 Suspicion. We look first at the results for suspicion ? just identifying words that
have no semantically related word nearby. Obviously, the chance of finding some word
that is judged to be related to the target word will increase with the size of the scope of
the search (with a large enough scope, e.g., a complete book, we would probably find
a relative for just about any word). So we expect recall to decrease as scope increases,
because some relationships will be found even for malapropisms (i.e., there will be more
false negatives). But we expect that precision will increase with scope, as it becomes
more likely that (genuine) relationships will be found for non-malapropisms (i.e., there
will be fewer false positives), and this factor will outweigh the decrease in the overall
number of suspects found.
Table 4 and Figure 4 show suspicion precision, recall, and F for each of the 5 ? 4
combinations of measure and scope. The values of precision range from 3.3% (Resnik,
scope = 1) to 11% (Jiang?Conrath, scope = MAX), with a mean of 6.2%, increasing with
scope, as expected, for all measures except Hirst?St-Onge. More specifically, differences
in precision are statistically significant for the difference between scope = 5 and scope =
MAX for Leacock?Chodorow and between 1 and larger scopes for Lin, Resnik, and
Jiang?Conrath; there are no significant differences for Hirst?St-Onge, which hence ap-
pears flat overall. The values of recall range from just under 6% (Hirst?St-Onge, scope =
MAX) to more than 72% (Resnik, scope = 1), with a mean of 39.7%, decreasing with
scope, as expected. All differences in recall are statistically significant, except between
scope = 3 and scope = 5 for all measures other than Resnik?s. F ranges from 5% (Hirst?
St-Onge, scope = MAX) to 14% (Jiang?Conrath, scope = 5), with a mean of just under
35
Computational Linguistics Volume 32, Number 1
Table 4
Precision (PS), recall (RS), and F-measure (FS) for malapropism suspicion with five measures of
semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs
or the complete news article (MAX).
Measure Scope PS RS FS??????????????????????
Hirst?St-Onge 1 .056 .298 .091
3 .067 .159 .089
5 .069 .114 .079
MAX .051 .059 .049
Jiang?Conrath 1 .064 .536 .112
3 .086 .383 .135
5 .097 .326 .141
MAX .111 .233 .137
Leacock?Chodorow 1 .042 .702 .079
3 .052 .535 .094
5 .058 .463 .101
MAX .073 .356 .115
Lin 1 .047 .579 .086
3 .062 .421 .105
5 .067 .350 .110
MAX .078 .253 .110
Resnik 1 .033 .727 .063
3 .038 .589 .070
5 .039 .490 .072
MAX .043 .366 .075
10%. Even though values at the lower ends of these ranges appear small, they are
still significantly (p < .001) better than chance, for which precision, recall, and F are
all 1.29%. Moreover, the value for precision is inherently limited by the likelihood
that, especially for small search scopes, there will be words other than our deliberate
malapropisms that are genuinely unrelated to all others in the scope.
Because it combines recall and precision, we focused on the results for FS by
measure and scope to determine whether the performance of the five measures was
significantly different and whether scope of search for relatedness made a significant
difference.
Scope differences. For Jiang?Conrath and Resnik, the analysis confirms only that
these methods perform significantly better with scope 5 than scope 1; for Lin, that
scope 3 is significantly better than scope 1; for Leacock?Chodorow, that 3 is significantly
better than 1 and MAX better than 3; and for Hirst?St-Onge, that MAX is significantly
worse than 3. (From the standpoint of simple detection of unrelatedness (suspicion in
malapropism detection), these data point to overall optimality of scopes 3 or 5.)
Differences between measures. Jiang?Conrath significantly outperforms the others in
all scopes (except for Leacock?Chodorow and Lin at scope MAX, where it does better
but not significantly so), followed by Lin and Leacock?Chodorow (whose performances
are not significantly different from each other), in turn followed by Resnik. Hirst?St-
Onge, with its irregular behavior, performs close to Lin and Leacock?Chodorow for
scopes 1 and 3 but falls behind as the scope size increases, finishing worst for scope MAX.
36
Budanitsky and Hirst Lexical Semantic Relatedness
Figure 4
Precision (PS), recall (RS), and F-measure (FS) for malapropism suspicion by measure and scope.
37
Computational Linguistics Volume 32, Number 1
Thus the Jiang?Conrath measure does best for the suspicion phase (and is optimal with
scope = 5).
5.3.2 Detection. We now turn to the results for malapropism detection. During the
detection phase, the suspects are winnowed by checking the spelling variations of each
for relatedness to their context. Since (true) alarms can only result from (true) suspects,
recall can only decrease (or, more precisely, not increase) from that for suspicion (cf.
equations (22) and (25)). However, if a given measure of semantic relatedness is good,
we expect the proportion of false alarms to reduce more considerably ? far fewer false
suspects will become alarms than correct suspects ? thus resulting in higher precision
for detection than for suspicion (cf. Equations 21 and 24).
Table 5 and Figure 5 show precision, recall, and F for each of the 5 ? 4 measure?
scope combinations, determined by the same method as those for suspicion. The
values of recall range from 5.9% (Hirst?St-Onge, scope = MAX) to over 60% (Leacock?
Chodorow, scope = 1). While these values are, as expected, lower than those for suspi-
cion recall ? RD for each measure?scope combination is from 1 to 16 percentage points
lower than the corresponding RS ? the decline is statistically significant for only 3
out of the 20 combinations.
The values of precision range from 6.7% (Hirst?St-Onge, scope = MAX) to just under
25% (Jiang?Conrath, scope = MAX), increasing, as expected, from suspicion precision;
each combination increases from 1 to 14 percentage points; the increase is statistically
significant for 18 out of the 20 combinations. Moreover, the increase in precision out-
weighs the decline in recall, and thus F, which ranges from 6% to 25%, increases by 7.6%
on average; the increase is significant for 17 out of the 20 combinations. Again, even the
Table 5
Precision (PD), recall (RD), and F-measure (FD) for malapropism detection with five measures of
semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs
or the complete news article (MAX).
Measure Scope PD RD FD
??????????????????????
Hirst?St-Onge 1 .105 .286 .145
3 .107 .159 .117
5 .101 .114 .096
MAX .067 .059 .056
Jiang?Conrath 1 .184 .498 .254
3 .205 .372 .245
5 .219 .322 .243
MAX .247 .231 .211
Leacock?Chodorow 1 .111 .609 .184
3 .115 .499 .180
5 .118 .440 .178
MAX .132 .338 .177
Lin 1 .125 .514 .195
3 .145 .398 .201
5 .150 .335 .197
MAX .168 .242 .176
Resnik 1 .088 .562 .150
3 .087 .512 .146
5 .088 .454 .145
MAX .093 .344 .140
38
Budanitsky and Hirst Lexical Semantic Relatedness
Figure 5
Precision (PD), recall (RD), and F-measure (FD) for malapropism detection by measure and scope.
39
Computational Linguistics Volume 32, Number 1
lower ends of the precision, recall, and F ranges are significantly (p < .001) better than
chance (which again is 1.29% for each), and the highest results are quite good (e.g.,
18% precision, 50% recall for Jiang?Conrath at scope = 1, which had the highest FD,
though not the highest precision or recall), despite the fact that the method is inherently
limited in the ways described earlier (Section 5.1). (See Hirst and Budanitsky (2005) for
discussion of the practical usefulness of the method.)
Scope differences. Our analysis of scope differences in F shows a somewhat different
picture for detection from that for suspicion: There are significant differences between
scopes only for the Hirst?St-Onge measure. The F graphs of the other four methods thus
are not significantly different from being flat ? that is, scope doesn?t affect the results.
(Hence we can choose 1 as the optimal scope, since it involves the least amount of work,
and Jiang and Conrath?s method with scope = 1 as the optimal parameter combination
for the malapropism detector.)
Differences between measures. The relative position of each measure?s precision,
recall, and F graphs for detection appears identical to that for suspicion, except for
the precision and F graphs for Hirst?St-Onge, which slide further down. Statistical
testing for F confirms this, with Jiang?Conrath leading, followed by Lin and Leacock?
Chodorow together, Resnik, and then Hirst?St-Onge.
5.4 Interpretation of the Results
In our interpretation, we focus largely on the results for suspicion; those for detection
both add to the pool of relatedness judgments on which we draw and corroborate what
we observe for suspicion.
The Resnik measure?s comparatively poor precision and good recall suggest that the
measure simply marks too many words as potential malapropisms?it ?under-relates?,
being far too conservative in its judgments of relatedness. For example, it was the only
measure that flagged crowd as a suspect in a context in which all the other measures
found it to be related to house: crowd IS-A gathering / assemblage SUBSUMES house /
household / family / menage.20 Indeed, for every scope, Resnik?s measure generates more
suspects than any other measure?e.g., an average of 62.5 per article for scope = 1,
compared to a range of 15 to 47, with an average of 37, for the other measures. The
Leacock?Chodorow measure?s superior precision and comparable recall (the former
difference is statistically significant, the latter is not), which result in a statistically-
significantly better F-value, indicate its better ability at discerning relatedness.
The same comparison can be made between the Jiang?Conrath and Lin measures.
Even though both use the same information-content?based components, albeit in differ-
ent arithmetic combinations, and show similar recall, the Jiang?Conrath measure shows
superior precision and is best overall (see above). The Lin and Leacock?Chodorow
measures, in turn, have statistically indistinguishable values of F and hence similar
ratios of errors to true positives.
Finally, the steady downward slope that distinguishes the F-graph of Hirst?St-Onge
from those of the other four measures in Figure 4 evidently reflects the corresponding
20 It is debatable whether this metonymic sense of house should appear in WordNet at all, though given that
it does, its relationship to crowd follows, and, as it happens, this sense was the correct one in the context
for this particular case.
40
Budanitsky and Hirst Lexical Semantic Relatedness
difference in precision behavior: The Hirst?St-Onge suspicion precision graph is statis-
tically flat, unlike the others. Ironically, given that this measure is the only one of the
five that promises the semantic relatedness that we want rather than mere similarity,
this poor performance appears to be a result of the measure?s ?over-relating? ? it is
far too promiscuous in its judgments of relatedness. For example, it was the only
measure that considered cation (a malapropism for nation) to be related to group:
cation IS-A ion IS-A atom PART-OF molecule HAS-A group / radical (?two or more atoms
bound together as a single unit and forming part of a molecule?). Because of its
promiscuity, the Hirst?St-Onge measure?s mean number of suspects for scope = 1 is
15.07, well below the average, and moreover it drops to one-ninth of that, 1.75, at
scope = MAX; the number of articles without a single suspect grows from 1 to 93. By
comparison, for the other measures, the number of suspects drops only to around a
third or a quarter from scope = 1 to scope = MAX, and the number of articles with no
suspect stays at 1 for both Leacock?Chodorow and Resnik and increases only from 1 to
4 for Lin and from 1 to 12 for Jiang?Conrath.
6. Related Work
6.1 Other Applications of WordNet-based Measures
Since the first publication of the initial results of this work (Budanitsky and Hirst 2001),
Pedersen and his colleagues (Pedersen, Patwardhan, and Michelizzi 2004) have made
available a Perl implementation of the five WordNet-based measures (plus Wu and
Palmer?s and their own; see below) that has been used by a number of researchers in
published work on other NLP applications. Generally, these results are consistent with
our own. For example, Stevenson and Greenwood (2005) found Jiang?Conrath to be the
best measure (out of ?several?, which they do not list) for their task of pattern induction
for information extraction. Similarly, Kohomban and Lee (2005) found Jiang?Conrath
the best (out of ?various schemes?, which they do not list) for their task of learning
coarse-grained semantic classes. In word-sense disambiguation, Patwardhan, Banerjee,
and Pedersen (2003) found Jiang?Conrath to be clearly the best of the five measures
evaluated here, albeit edged out by their own new ?Lesk? measure based on gloss
overlaps;21 and McCarthy et al (2004) found that the Jiang?Conrath and Lesk measures
gave the best accuracy in their task of finding predominant word senses, with the results
of the two being ?comparable? but Jiang?Conrath being far more efficient. On the other
hand, Corley and Mihalcea (2005) found little difference between the measures when
using them in an algorithm for computing text similarity.
6.2 Measures of Distributional Similarity as Proxies for Measures
of Semantic Relatedness
In Section 1.1, we mentioned that the lexical semantic relatedness or similarity that
we have dealt with in this paper is a notion distinct from that of lexical distributional
21 Patwardhan et al?s measure is based on the idea, originally due to Lesk (1986), of measuring the degree
of relatedness of two words by the number of string overlaps in their dictionary definitions or glosses.
Patwardhan et al extend this idea by also including overlaps with definitions of words that are one
WordNet edge away from the comparison words. It is thus a hybrid method, with characteristics of
both dictionary-based and network-based methods (see Sections 2.1 and 2.3 above).
41
Computational Linguistics Volume 32, Number 1
or co-occurrence similarity. However, a number of researchers, such as Dagan (2000),
have promoted the hypothesis that distributional similarity can act as a useful proxy
for semantic relatedness in many applications because it is based on corpus-derived
data rather than manually created lexical resources; indeed, it could perhaps be used
to automatically create (first-draft) lexical resources (Grefenstette 1994). It is therefore
natural to ask how distributional-similarity measures compare with the WordNet-based
measures that we have looked at above.
Formally, by distributional similarity (or co-occurrence similarity) of two words
w1 and w2, we mean that they tend to occur in similar contexts, for some definition of
context; or that the set of words that w1 tends to co-occur with is similar to the set that
w2 tends to co-occur with; or that if w1 is substituted for w2 in a context, its ?plausibility?
(Weeds 2003; Weeds and Weir 2005) is unchanged. The context considered may be a
small or large window around the word, or an entire document; or it may be a syntactic
relationship. For example, Weeds (2003; Weeds and Weir, 2005) (see below) took verbs
as contexts for nouns in object position: so they regarded two nouns to be similar to
the extent that they occur as direct objects of the same set of verbs. Lin (1998b, 1998a)
considered other syntactic relationships as well, such as subject?verb and modifier?
noun, and looked at both roles in the relationship.
Given this framework, many different methods of measuring distributional similar-
ity have been proposed; see Dagan (2000), Weeds (2003), or Mohammad and Hirst (2005)
for a review. For example, the set of words that co-occur with w1 and those that co-occur
with w2 may be regarded as a feature vector of each and their similarity measured as
the cosine between the vectors; or a measure may be based on the Kullback?Leibler di-
vergence between the probability distributions P(w |w1) and P(w |w2), as, for example,
Lee?s (1999) ?-skew divergence. Lin (1998b) uses his similarity theorem (equation (19)
above) to derive a measure based on the degree of overlap of the sets of words with
which w1 and w2, respectively, have positive mutual information.22
Words that are distributionally similar do indeed often represent semantically re-
lated concepts, and vice versa, as the following examples demonstrate. Weeds (2003),
in her study of 15 distributional-similarity measures, found that words distributionally
similar to hope (noun) included confidence, dream, feeling, and desire; Lin (1998b) found
pairs such as earnings?profit, biggest?largest, nylon?silk, and pill?tablet. It is intuitively
clear why these results occur: if two concepts are similar or related, it is likely that
their role in the world will be similar, so similar things will be said about them, and so
the contexts of occurrence of the corresponding words will be similar. And conversely
(albeit with less certainty), if the contexts of occurrence of two words are similar, then
similar things are being said about each, so they are playing similar roles in the world
and hence are semantically similar ? at least to the extent of these roles. Nonetheless,
the limitations of this observation will become clear in our discussion below.
Three differences between semantic relatedness and distributional similarity are
immediately apparent. First, while semantic relatedness is inherently a relation on con-
cepts, as we emphasized in Section 1.1, distributional similarity is a (corpus-dependent)
relation on words. In theory, of course, if one had a large-enough sense-tagged corpus,
one could derive distributional similarities of word-senses. But in practice, apart from
the lack of such corpora, distributional similarities are promoted exactly for applications
22 Do not confound Lin?s distributional similarity measure with his semantic relatedness measure, simL,
which has been discussed in earlier sections of this paper; but observe that both are derived from the
same theorem.
42
Budanitsky and Hirst Lexical Semantic Relatedness
such as various kinds of ambiguity resolution in which it is words rather than senses
that are available (see Weeds (2003) for an extensive list).
Second, whereas semantic relatedness is symmetric, distributional similarity is a
potentially asymmetrical relationship. If distributional similarity is conceived of as
substitutability, as Weeds and Weir (2005) and Lee (1999) emphasize, then asymme-
tries arise when one word appears in a subset of the contexts in which the other
appears; for example, the adjectives that typically modify apple are a subset of those that
modify fruit, so fruit substitutes for apple better than apple substitutes for fruit. While
some distributional similarity measures, such as cosine, are symmetric, many, such as
?-skew divergence and the co-occurrence retrieval models developed by Weeds and
Weir, are not. But this is simply not an adequate model of semantic relatedness, for
which substitutability is far too strict a requirement: window and house are semantically
related, but they are not plausibly substitutable in most contexts.
Third, lexical semantic relatedness depends on a pre-defined lexicographic or other
knowledge resource, whereas distributional similarity is relative to a corpus. In each
case, matching the measures to the resource is a research problem in itself, as this
paper and Weeds (2003) show, and anomalies can arise.23 But the knowledge source
for semantic relatedness is created by humans and may be presumed to be (in a weak
sense) ?true, unbiased, and complete?. A corpus, on the other hand, is not. Imbalance
in the corpus and data sparseness is an additional source of anomalous results even
for ?good? measures. For example, Lin (1998b) found ?peculiar? similarities that were
?reasonable? for his corpus of news articles, such as captive?westerner (because in the
news articles, more than half of the ?westerners? mentioned were being held captive)
and audition?rite (because both were infrequent and were modified by uninhibited).
We now turn to the hypothesis that distributional similarity can usefully stand in for
semantic relatedness in NLP applications such as malapropism detection. Weeds (2003)
considered the hypothesis in detail. She carried out a number of experiments using data
gathered from the British National Corpus on the distribution of a set of 2000 nouns
with respect to the verbs of which they were direct objects, comparing a large number
of proposed measures of distributional similarity. She applied ten of these measures to
the Miller and Charles word-pairs (see Section 4.1 above); the absolute values of the
correlations with the Miller and Charles human judgments was at best .62 (and at worst
.26), compared with .74 to .85 for the semantic measures (Table 3 above). Weeds also
compared these measures on their ability to predict the k words that are semantically
closest to a target word in WordNet, as measured by Lin?s semantic similarity measure,
simL. She found performance to be ?generally fairly poor? (page 162), and undermined
by the effects of varying word frequencies.
Last, Weeds experimented with distributional measures in real-word spelling cor-
rection, much as we have defined it in Hirst and Budanitsky (2005) and in Section 5.1
above, but replacing the semantic relatedness measures with distributional similarity
measures. However, she varied the experimental procedure in a number of ways, with
the consequence that her results are not directly comparable to ours: her test data was
the British National Corpus; scope was measured in words, not paragraphs; and relat-
edness thresholds were replaced by considering the k words most similar to the target
word (and k was a parameter). The most significant difference, however, arose from the
23 We have already remarked in Section 5.4 above on the promiscuity of the Hirst?St-Onge measure
and its tendency to find connections such as cation?group. Similarly, one of the poorer measures that
Weeds experimented with returned this list as the ten words most distributionally similar to hope:
hem, dissatisfaction, dismay, scepticism, concern, outrage, break, warrior, optimism, readiness.
43
Computational Linguistics Volume 32, Number 1
limitations due to data sparseness that are inherent in methods based on distributional
similarity: the very small size of the set of words that could be corrected. Specifically,
only malapropisms for which both the error and the correction occurred in the set of
2,000 words for which Weeds had distributional data could be considered; and the abil-
ity to detect and correct the malapropism depended on other members of that set alo
being within the scope of the target word. It is therefore not surprising that the results
were generally poor (and so were results for simL run under the same conditions). This
severe limitation on the data means that this was not really a fair test of the principles
underlying the hypothesis; a fair test would require data allowing the comparison of
any two nouns (or better still, any two words) in WordNet, but obtaining such data for
less-frequent words (possibly using the Web as the corpus) would be a massive task.
7. Conclusion
Our goal in this paper has been to evaluate resource-based measures of lexical semantic
distance, or, equivalently, semantic relatedness, for use in natural language processing
applications. Most of the work, however, was limited to the narrower notion of mea-
sures of similarity and how well they fill the broader role, because those measures are
what current resources support best and hence what most current research has focused
on. But ultimately it is the more-general idea of relatedness, not just similarity, that we
need for most NLP methods and applications, because the goal, in one form or another,
is to determine whether two smaller or larger pieces of text share a topic or some kind
of closeness in meaning, and this need not depend on the presence of words that denote
similar concepts. In word sense disambiguation, such an association with the context
is frequently a sufficient basis for selecting or rejecting candidate senses (Banerjee
and Pedersen 2003); in our malapropism corrector, a word should be considered non-
anomalous in the context of another if there is any kind of semantic relationship at all
apparent between them. These relationships include not just hyponymy and the non-
hyponymy relationships in WordNet such as meronymy but also associative and ad hoc
relationships. As mentioned in the introduction, these can include just about any kind
of functional relation or frequent association in the world.24
For the last century, many researchers have attempted to enumerate these kinds
of relationships. Some elements from a typical list (that of Spellman, Holyoak, and
Morrison (2001)) are shown in Table 6. Morris and Hirst (2004, 2005) have termed these
non-classical lexical semantic relationships (following Lakoff?s (1987) non-classical cat-
egories), and Morris has shown in experiments with human subjects that around 60%
of the lexical relationships that readers perceive in a text are of this nature (Morris
2006). There is presently no catalogue of instances of these kinds of relationships let
alone any incorporation of such relationships into a quantification of semantic distance.
Nonetheless, there are clear intuitions to be captured here, and this should be a focus
for future research.
But lists of such relationships can never be exhaustive, as lexical relationships can
also arise ad hoc in context (Barsalou 1983, 1989) ? in particular, as co-membership
of an ad hoc category. For example, Morris?s subjects reported that the words sex,
drinking, and drag racing were semantically related, by all being ?dangerous behaviors?,
in the context of an article about teenagers emulating what they see in movies. Thus
24 Don?t confound ?frequent association in the world? with the lexical co-occurrences that underlie the
distributional similarity of Section 6.2.
44
Budanitsky and Hirst Lexical Semantic Relatedness
Table 6
From Spellman, Holyoak, and Morrison?s (2001) list of associative semantic relations.
Name Example
????????????????
IS-USED-TO bed?sleep
WORKS-IN judge?court
LIVES-IN camel?desert
IS-THE-OUTSIDE-OF husk?corn
lexical semantic relatedness is sometimes constructed in context and cannot always be
determined purely from an a priori lexical resource such as WordNet.25 It?s very unclear
how ad hoc semantic relationships could be quantified in any meaningful way, let alne
compared with prior quantifications of the classical and non-classical relationships.
However, ad hoc relationships accounted for only a small fraction of those reported
by Morris?s subjects (Morris 2006). The fact of their existence does not undermine
the usefulness of computational methods for quantifying semantic distances for non?
ad hoc relationships, and the continued development of such methods is an important
direction for research.
Acknowledgments
This research was supported financially by
the Natural Sciences and Research Council of
Canada, the Ontario Graduate Scholarship
Program, and the University of Toronto. For
discussions, comments, and advice, we are
grateful to Mark Chignell, Stephen Green,
Jay Jiang, Keith Knight, Claudia Leacock,
Dekang Lin, Saif Mohammad, Jane Morris,
Radford Neal, Manabu Okumura, Gerald
Penn, Philip Resnik, David St-Onge, Suzanne
Stevenson, Michael Sussna, and some of the
anonymous reviewers of Computational
Linguistics.
References
Agresti, Alan and Barbara Finlay. 1997.
Statistical Methods for the Social Sciences.
Prentice Hall, Upper Saddle River, NJ,
3rd edition.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure of
semantic relatedness. In Proceedings of the
Eighteenth International Joint Conference on
Artificial Intelligence, Acapulco, Mexico,
pages 805?810, August.
Barsalou, Lawrence W. 1983. Ad hoc
categories. Memory and Cognition,
11:211?227.
Barsalou, Lawrence W. 1989. Intra-concept
similarity and its implications for inter-
concept similarity. In Stella Vosniadou and
Andrew Ortony, editors, Similarity and
Analogical Reasoning. Cambridge
University Press, pages 76?121.
Bernard, J. R. L., editor. 1986. The Macquarie
Thesaurus. Macquarie Library, Sydney,
Australia.
Budanitsky, Alexander. 1999. Lexical
semantic relatedness and its application
in natural language processing. Technical
Report CSRG-390, Computer Systems
Research Group, University of Toronto,
August.
Budanitsky, Alexander and Graeme
Hirst. 2001. Semantic distance in
WordNet: An experimental,
application-oriented evaluation of five
measures. In Workshop on WordNet and
Other Lexical Resources, Second Meeting
of the North American Chapter of the
Association for Computational Linguistics,
Pittsburgh, PA, pages 29?34.
Corley, Courtney and Rada Mihalcea. 2005.
Measuring the semantic similarity of texts.
In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence
and Entailment, Ann Arbor, MI, June,
pages 13?18.
Dagan, Ido. 2000. Contextual word similarity.
In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of
25 Indeed Murphy (2003) has suggested that semantic relations (of all types) are best conceived of as
metalexical: derived from a (pre-existing) lexicon, but not part of it.
45
Computational Linguistics Volume 32, Number 1
Natural Language Processing. Marcel
Dekker, pages 459?475.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models of
word cooccurrence probabilities. Machine
Learning, 34(1?3):43?69.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
Finkelstein, Lev, Evgeniy Gabrilovich, Yossi
Matias, Ehud Rivlin, Zach Solan, Gadi
Wolfman, and Eytan Ruppin. 2002. Placing
search in context: The concept revisited.
ACM Transactions on Information Systems,
20:116?131.
Francis, Winthrop Nelson and Henry Kuc?era.
1982. Frequency Analysis of English Usage:
Lexicon and Grammar. Houghton Mifflin,
Boston.
Fromkin, Victoria A. 1980. Errors in Linguistic
performance: Slips of the Tongue, Ear, Pen, and
Hand. Academic Press, San Francisco.
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Norwell, MA.
Halliday, M. A. K. and Ruqaiya Hasan. 1976.
Cohesion in English. English Language
Series. Longman, New York.
Hirst, Graeme and Alexander Budanitsky.
2005. Correcting real-word spelling errors
by restoring lexical cohesion. Natural
Language Engineering, 11:87?111.
Hirst, Graeme and David St-Onge. 1998.
Lexical chains as representations of
context for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. The MIT Press, Cambridge, MA,
pages 305?332.
Hoey, Michael. 1991. Patterns of Lexis in Text.
Describing English Language. Oxford
University Press.
Jarmasz, Mario and Stan Szpakowicz. 2003.
Roget?s Thesaurus and semantic similarity.
In Proceedings of the International Conference
on Recent Advances in Natural Language
Processing (RANLP-2003), Borovetz,
Bulgaria, pages 212?219, September.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of International Conference on
Research in Computational Linguistics
(ROCLING X), Taiwan, pages 19?33.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2005. Learning semantic
classes for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 34?41, Ann
Arbor, MI, June.
Kozima, Hideki and Teiji Furugori. 1993.
Similarity between words computed by
spreading activation on an English
dictionary. In Proceedings of 6th Conference
of the European Chapter of the Association
for Computational Linguistics (EACL-93),
pages 232?239, Utrecht.
Kozima, Hideki and Akira Ito. 1997.
Context-sensitive word distance by
adaptive scaling of a semantic space. In
Ruslan Mitkov and Nicolas Nicolov,
editors, Recent Advances in Natural
Language Processing: Selected Papers from
RANLP?95, volume 136 of Amsterdam
Studies in the Theory and History of Linguistic
Science: Current Issues in Linguistic Theory.
John Benjamins Publishing Company,
Amsterdam/Philadelphia, chapter 2,
pages 111?124.
Lakoff, George. 1987. Women, Fire, and
Dangerous Things: What Categories Reveal
About the Mind. University of Chicago
Press, Chicago.
Leacock, Claudia and Martin Chodorow.
1998. Combining local context and
WordNet similarity for word sense
identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. The MIT Press, Cambridge, MA,
pages 265?283.
Lee, Joon Ho, Myong Ho Kim, and Yoon Joon
Lee. 1993. Information retrieval based on
conceptual distance in IS-A hierarchies.
Journal of Documentation, 49(2):188?207.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99), College Park, MD,
pages 25?31.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone
from an ice cream cone. In Proceedings,
Fifth International Conference on
Systems Documentation (SIGDOC ?86),
Toronto, Canada, pages 24?26.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th
International Conference on Computational
Linguistics (COLING?ACL ?98), Montreal,
Canada, pages 768?774, August.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of the
15th International Conference on Machine
Learning, Madison, WI, pages 296?304, July.
46
Budanitsky and Hirst Lexical Semantic Relatedness
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding predominant
word senses in untagged text. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona,
Spain, pages 280?287.
Miller, George A. and Walter G. Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28.
Mohammad, Saif and Graeme Hirst. 2005.
Distributional measures as proxies for
semantic relatedness. Submitted for
publication.
Morris, Jane. 2006. Readers? Perceptions of
Lexical Cohesion and Lexical Semantic
Relations in Text. Ph.D. thesis, University
of Toronto.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21?48.
Morris, Jane and Graeme Hirst. 2004.
Non-classical lexical semantic relations.
In Workshop on Computational Lexical
Semantics, Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
Boston, MA, pages 46?51, May.
Morris, Jane and Graeme Hirst. 2005. The
subjectivity of lexical cohesion in text. In
James G. Shanahan, Yan Qu, and Janyce
Wiebe, editors, Computing Attitude
and Affect in Text. Springer, New York,
pages 41?48.
Murphy, M. Lynne. 2003. Semantic Relations
and the Lexicon. Cambridge University Press.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics,
Mexico City, Mexico, pages 241?257, February.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004. Wordnet::
similarity ? measuring the relatedness of
concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence
(AAAI-04). AAAI Press, Cambridge, MA,
pages 1024?1025.
Procter, Paul, editor. 1978. Longman Dictionary
of Contemporary English. Longman.
Rada, Roy and Ellen Bicknell. 1989.
Ranking documents with a thesaurus.
JASIS, 40(5):304?310.
Rada, Roy, Hafedh Mili, Ellen Bicknell,
and Maria Blettner. 1989. Development
and application of a metric on semantic
nets. IEEE Transactions on Systems, Man,
and Cybernetics, 19(1):17?30.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity.
In Proceedings of the 14th International
Joint Conference on Artificial Intelligence,
pages 448?453, Montreal, Canada, August.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Proceedings
of the 22nd Annual Meeting of the Cognitive
Science Society, Philadelphia, PA,
pages 399?404.
Rubenstein, Herbert and John B.
Goodenough. 1965. Contextual
correlates of synonymy. Communications
of the ACM, 8(10):627?633.
Spellman, Barbara A., Keith J. Holyoak, and
Robert G. Morrison. 2001. Analogical
priming via semantic relations. Memory
and Cognition, 29(3):383?393.
St-Onge, David. 1995. Detecting and
correcting malapropisms with lexical
chains. Master?s thesis, University of
Toronto. Published as Technical
Report CSRI-319.
Stevenson, Mark and Mark Greenwood.
2005. A semantic approach to IE pattern
induction. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 379?386,
Ann Arbor, MI, June.
Sussna, Michael. 1993. Word sense
disambiguation for free-text indexing
using a massive semantic network. In
Proceedings of the Second International
Conference on Information and Knowledge
Management (CIKM-93), pages 67?74,
Arlington, VA.
Sussna, Michael John. 1997. Text Retrieval
Using Inference in Semantic Metanetworks.
Ph.D. thesis, University of California,
San Diego.
Weeds, Julie and David Weir. 2005. Co-
occurrence retrieval: A flexible framework
for lexical distributional similarity.
Computational Linguistics, 31(4):433?475.
Weeds, Julie E. 2003. Measures and applications
of lexical distributional similarity. Ph.D.
thesis, University of Sussex, September.
Wei, Mei. 1993. An analysis of word
relatedness correlation measures. Master?s
thesis, University of Western Ontario,
London, Ontario.
Whitten, William B., W. Newton Suter,
and Michael L. Frank. 1979. Bidirectional
synonym ratings of 464 noun pairs. Journal
of Learning and Verbal Behavior, 18:109?127.
Wu, Zhibiao and Martha Palmer. 1994. Verb
semantics and lexical selection. In
Proceedings of the 32nd Annual Meeting of
the Association for Computational Linguistics,
pages 133?138, Las Cruces, NM, June.
47

Building and Using
a Lexical Knowledge Base
of Near-Synonym Differences
Diana Inkpen?
University of Ottawa
Graeme Hirst?
University of Toronto
Choosing the wrong word in a machine translation or natural language generation system can
convey unwanted connotations, implications, or attitudes. The choice between near-synonyms
such as error, mistake, slip, and blunder?words that share the same core meaning, but differ
in their nuances?can be made only if knowledge about their differences is available.
We present a method to automatically acquire a new type of lexical resource: a knowledge
base of near-synonym differences. We develop an unsupervised decision-list algorithm that learns
extraction patterns from a special dictionary of synonym differences. The patterns are then used
to extract knowledge from the text of the dictionary.
The initial knowledge base is later enriched with information from other machine-readable
dictionaries. Information about the collocational behavior of the near-synonyms is acquired from
free text. The knowledge base is used by Xenon, a natural language generation system that shows
how the new lexical resource can be used to choose the best near-synonym in specific situations.
1. Near-Synonyms
Near-synonyms are words that are almost synonyms, but not quite. They are not fully
intersubstitutable, but vary in their shades of denotation or connotation, or in the com-
ponents of meaning they emphasize; they may also vary in grammatical or collocational
constraints. For example, the word foe emphasizes active warfare more than enemy does
(Gove 1984); the distinction between forest and woods is a complex combination of size,
proximity to civilization, and wildness (as determined by the type of animals and plants
therein) (Room 1981); among the differences between task and job is their collocational
behavior with the word daunting: daunting task is a better collocation than daunting job.
More examples are given in Table 1 (Hirst 1995).
There are very few absolute synonyms, if they exist at all. So-called dictionaries
of synonyms actually contain near-synonyms. This is made clear by dictionaries such
as Webster?s New Dictionary of Synonyms (Gove 1984) and Choose the Right Word (here-
after CTRW) (Hayakawa 1994), which list clusters of similar words and explicate the
differences between the words in each cluster. An excerpt from CTRW is presented in
Figure 1. These dictionaries are in effect dictionaries of near-synonym discrimination.
? School of Information Technology and Engineering, Ottawa, ON, Canada, K1N 6N5;
diana@site.uottawa.ca.
? Department of Computer Science, Toronto, ON, Canada, M5S 3G4; gh@cs.toronto.edu.
Submission received: 5 October 2004; revised submission received: 15 June 2005; accepted for publication:
4 November 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 2
Table 1
Examples of near-synonym variations.
Type of variation Example
Stylistic, formality pissed : drunk : inebriated
Stylistic, force ruin : annihilate
Expressed attitude skinny : thin : slim
Emotive daddy : dad : father
Continuousness seep : drip
Emphasis on different aspects of meaning enemy : foe
Fuzzy boundary woods : forest
Collocational task : job (in the context of daunting)
Writers often turn to such resources when confronted with a choice between near-
synonyms, because choosing the wrong word can be imprecise or awkward or convey
unwanted implications. These dictionaries are made for human use, and they are avail-
able only on paper, not in electronic format.
Understanding the differences between near-synonyms is important for fine-
grained distinctions in machine translation. For example, when translating the French
word erreur to English, one of the near-synonyms mistake, blooper, blunder, boner, con-
tretemps, error, faux pas, goof, slip, solecism could be chosen, depending on the context and
on the nuances that need to be conveyed. More generally, knowledge of near-synonyms
is vital in natural language generation systems that take a nonlinguistic input (semantic
representation) and generate text. When more than one word can be used, the choice
should be based on some explicit preferences. Another application is an intelligent
thesaurus, which would assist writers not only with lists of possible synonyms but also
with the nuances they carry (Edmonds 1999).
1.1 Distinctions among Near-Synonyms
Near-synonyms can vary in many ways. DiMarco, Hirst, and Stede (1993) analyzed
the types of differences adduced in dictionaries of near-synonym discrimination. They
found that there was no principled limitation on the types, but a small number of types
occurred frequently. A detailed analysis of the types of variation is given by Edmonds
(1999). Some of the most relevant types of distinctions, with examples from CTRW, are
presented below.
Denotational distinctions Near-synonyms can differ in the frequency with which
they express a component of their meaning (e.g., Occasionally, invasion suggests a large-
scale but unplanned incursion), in the latency (or indirectness) of the expression of the
component (e.g., Test strongly implies an actual application of these means), and in fine-
grained variations of the idea itself (e.g., Paternalistic may suggest either benevolent
rule or a style of government determined to keep the governed helpless and dependent). The
frequency is signaled in the explanations in CTRW by words such as always, usually,
sometimes, seldom, never. The latency is signaled by many words, including the obvious
words suggests, denotes, implies, and connotes. The strength of a distinction is signaled by
words such as strongly and weakly.
Attitudinal distinctions Near-synonyms can convey different attitudes of the
speaker toward an entity in the situation. Attitudes can be pejorative, neutral, or favor-
able. Examples of sentences in CTRW expressing attitudes, in addition to denotational
distinctions, are these: Blurb is also used pejoratively to denote the extravagant and insincere
224
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 1
An excerpt from Choose the Right Word (CTRW) by S. I. Hayakawa. Copyright ?1987. Reprinted
by arrangement with HarperCollins Publishers, Inc.
praise common in such writing. Placid may have an unfavorable connotation in suggesting an
unimaginative, bovine dullness of personality.
Stylistic distinctions Stylistic variations of near-synonyms concern their level of
formality, concreteness, force, floridity, and familiarity (Hovy 1990). Only the first
three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions
is this: Assistant and helper are nearly identical except for the latter?s greater informality.
Words that signal the degree of formality include formal, informal, formality, and slang.
The degree of concreteness is signaled by words such as abstract, concrete, and concretely.
Force can be signaled by words such as emphatic and intensification.
1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions
among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of
225
Computational Linguistics Volume 32, Number 2
distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO-
TATIONAL DISTINCTIONS, ATTITUDE, and STYLE. The last two are grouped together in
a class ATTITUDE-STYLE DISTINCTIONS because they are expressed by similar syntactic
constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2
will treat them together.
The leaf classes of DENOTATIONAL DISTINCTIONS are SUGGESTION, IMPLICATION,
and DENOTATION; those of ATTITUDE are FAVORABLE, NEUTRAL, and PEJORATIVE;
those of STYLE are FORMALITY, CONCRETENESS, and FORCE. All these leaf nodes have
the attribute STRENGTH, which takes the values low, medium, and high. All the leaf nodes
except those in the class STYLE have the attribute FREQUENCY, which takes the values
always, usually, sometimes, seldom, and never. The DENOTATIONAL DISTINCTIONS
have an additional attribute: the peripheral concept that is suggested, implied, or
denoted.
1.2 The Clustered Model of Lexical Knowledge
Hirst (1995) and Edmonds and Hirst (2002) show that current models of lexical
knowledge used in computational systems cannot account well for the properties of
near-synonyms.
The conventional view is that the denotation of a lexical item is represented as
a concept or a structure of concepts (i.e., a word sense is linked to the concept it
lexicalizes), which are themselves organized into an ontology. The ontology is often
language independent, or at least language neutral, so that it can be used in multilin-
gual applications. Words that are nearly synonymous have to be linked to their own
slightly different concepts. Hirst (1995) showed that such a model entails an awkward
taxonomic proliferation of language-specific concepts at the fringes, thereby defeating
the purpose of a language-independent ontology. Because this model defines words
Figure 2
The class hierarchy of distinctions: rectangles represent classes, ovals represent attributes that a
class and its descendants have.
226
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
in terms of necessary and sufficient truth conditions, it cannot account for indirect
expressions of meaning or for fuzzy differences between near-synonyms.
Edmonds and Hirst (2002) modified this model to account for near-synonymy. The
meaning of each word arises out of a context-dependent combination of a context-
independent denotation and a set of explicit differences from its near-synonyms, much
as in dictionaries of near-synonyms. Thus the meaning of a word consists both of
necessary and sufficient conditions that allow the word to be selected by a lexical choice
process and a set of nuances of indirect meaning that may be conveyed with different
strengths. In this model, a conventional ontology is cut off at a coarse grain and the
near-synonyms are clustered under a shared concept, rather than linking each word to
a separate concept. The result is a clustered model of lexical knowledge. Thus, each cluster
has a core denotation that represents the essential shared denotational meaning of its
near-synonyms. The internal structure of a cluster is complex, representing semantic
(or denotational), stylistic, and expressive (or attitudinal) differences between the near-
synonyms. The differences or lexical nuances are expressed by means of peripheral
concepts (for denotational nuances) or attributes (for nuances of style and attitude).
The clustered model has the advantage that it keeps the ontology language neutral
by representing language-specific distinctions inside the cluster of near-synonyms. The
near-synonyms of a core denotation in each language do not need to be in separate
clusters; they can be part of one larger cross-linguistic cluster.
However, building such representations by hand is difficult and time-consuming,
and Edmonds and Hirst (2002) completed only nine of them. Our goal in the present
work is to build a knowledge base of these representations automatically by extracting
the content of all the entries in a dictionary of near-synonym discrimination. Un-
like lexical resources such as WordNet (Miller 1995), in which the words in synsets
are considered ?absolute? synonyms, ignoring any differences between them, and
thesauri such as Roget?s (Roget 1852) and Macquarie (Bernard 1987), which contain
hierarchical groups of similar words, the knowledge base will include, in addition
to the words that are near-synonyms, explicit explanations of differences between
these words.
2. Building a Lexical Knowledge Base of Near-Synonym Differences
As we saw in Section 1, each entry in a dictionary of near-synonym discrimination
lists a set of near-synonyms and describes the differences among them. We will use
the term cluster in a broad sense to denote both the near-synonyms from an entry and
their differences. Our aim is not only to automatically extract knowledge from one such
dictionary in order to create a lexical knowledge base of near-synonyms (LKB of NS),
but also to develop a general method that could be applied to any such dictionary
with minimal adaptation. We rely on the hypothesis that the language of the entries
contains enough regularity to allow automatic extraction of knowledge from them.
Earlier versions of our method were described by Inkpen and Hirst (2001).
The task can be divided into two phases, treated by two consecutive modules, as
shown in Figure 3. The first module, the extraction module, will be described in this
section. The generic clusters produced by this module contain the concepts that near-
synonyms may involve (the peripheral concepts) as simple strings. This generic LKB
of NS can be adapted for use in any Natural Language Processing (NLP) application.
The second module customizes the LKB of NS so that it satisfies the requirements of
the particular system that is to employ it. This customization module transforms the
227
Computational Linguistics Volume 32, Number 2
Figure 3
The two modules of the task.
strings from the generic clusters into concepts in the particular ontology. An example of
a customization module will be described in Section 6.
The dictionary that we use is Choose the Right Word (Hayakawa 1994) (CTRW),1
which was introduced in Section 1. CTRW contains 909 clusters, which group 5,452
near-synonyms (more precisely, near-synonym senses, because a word can be in more
than one cluster) with a total of 14,138 sentences (excluding examples of usage), from
which we derive the lexical knowledge base. An example of the results of this phase,
corresponding to the second, third, and fourth sentence for the absorb cluster in Figure 1,
is presented in Figure 4.
This section describes the extraction module, whose architecture is presented in
Figure 5. It has two main parts. First, it learns extraction patterns; then it applies the
patterns to extract differences between near-synonyms.
2.1 Preprocessing the Dictionary
After OCR scanning of CTRW and error correction, we used XML markup to segment
the text of the dictionary into cluster name, cluster identifier, members (the near-
synonyms in the cluster), entry (the textual description of the meaning of the near-
synonyms and of the differences among them), cluster?s part of speech, cross-references
to other clusters, and antonyms list. Sentence boundaries were detected by using
general heuristics, plus heuristics specific for this particular dictionary; for example,
examples appear in square brackets and after a colon.
2.2 The Decision-List Learning Algorithm
Before the system can extract differences between near-synonyms, it needs to learn
extraction patterns. For each leaf class in the hierarchy (Figure 2) the goal is to learn
a set of words and expressions from CTRW?that is, extraction patterns?that charac-
terizes descriptions of the class. Then, during the extraction phase, for each sentence (or
fragment of a sentence) in CTRW the program will decide which leaf class is expressed,
with what strength, and what frequency. We use a decision-list algorithm to learn sets
of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and
ATTITUDE-STYLE DISTINCTIONS. These are split further for each leaf class, as explained
in Section 2.3.
The algorithm we implemented is inspired by the work of Yarowsky (1995) on word
sense disambiguation. He classified the senses of a word on the basis of other words
that the given word co-occurs with. Collins and Singer (1999) classified proper names
1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project.
228
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 4
Example of distinctions extracted from CTRW.
as PERSON, ORGANIZATION, or LOCATION using contextual rules (that rely on other
words appearing in the context of the proper names) and spelling rules (that rely on
words in the proper names). Starting with a few spelling rules (using some proper-name
features) in the decision list, their algorithm learns new contextual rules; using these
rules, it then learns more spelling rules, and so on, in a process of mutual bootstrapping.
Riloff and Jones (1999) learned domain-specific lexicons and extraction patterns (such
as shot in ?x? for the terrorism domain). They used a mutual bootstrapping technique to
alternately select the best extraction pattern for a category and add its extractions to the
semantic lexicon; the newly added entries in the lexicon help in the selection of the next
best extraction pattern.
Our decision-list (DL) algorithm (Figure 6) is tailored for extraction from CTRW.
Like the algorithm of Collins and Singer (1999), it learns two different types of rules:
Main rules are for words that are significant for distinction classes; auxiliary rules are
for frequency words, strength words, and comparison words. Mutual bootstrapping in
the algorithm alternates between the two types. The idea behind the algorithm is that
starting with a few main rules (seed words), the program selects examples containing
them and learns a few auxiliary rules. Using these, it selects more examples and learns
new main rules. It keeps iterating until no more new rules are learned.
The rules that the program learns are of the form x ? h(x), meaning that word
x is significant for the given class with confidence h(x). All the rules for that class
form a decision list that allows us to compute the confidence with which new patterns
Figure 5
The architecture of the extraction module.
229
Computational Linguistics Volume 32, Number 2
Figure 6
The decision-list learning algorithm.
are significant for the class. The confidence h(x) for a word x is computed with the
formula:
h(x) =
count(x, E?) + ?
count(x, E) + k?
(1)
where E? is the set of patterns selected for the class, and E is the set of all input data.
So, we count how many times x is in the patterns selected for the class versus the total
number of occurrences in the training data. Following Collins and Singer (1999), k =
2, because there are two partitions (relevant and irrelevant for the class). ? = 0.1 is a
smoothing parameter.
In order to obtain input data, we replace all the near-synonyms in the text of the dic-
tionary with the term near syn; then we chunk the text with Abney?s chunker (Abney
1996). The training set E is composed of all the verb phrases, noun phrases, adjectival
phrases, and adverbial phrases (denoted vx, nx, ax, rx, respectively) that occur more
230
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
than t times in the text of the dictionary (where t = 3 in our experiments). Phrases
that occur very few times are not likely to be significant patterns and eliminating them
makes the process faster (fewer iterations are needed).
We apply the DL algorithm for each of the classes DENOTATIONAL DISTINCTIONS
and ATTITUDE-STYLE DISTINCTIONS. The input to the algorithm is as follows: the set
E of all chunks, the main seed words, and the restrictions on the part of speech of
the words in main and auxiliary rules. For the class DENOTATIONAL DISTINCTIONS
the main seed words are suggest, imply, denote, mean, designate, connote; the words in
main rules are verbs and nouns, and the words in auxiliary rules are adverbs and
modals. For the class ATTITUDE-STYLE DISTINCTIONS the main seed words are formal,
informal, pejorative, disapproval, favorable, abstract, concrete; the words in main rules are
adjectives and nouns, and the words in auxiliary rules are adverbs. For example, for the
class DENOTATIONAL DISTINCTIONS, starting with the rule suggest ? 0.99, the program
selects examples such as these (where the numbers give the frequency in the training
data):
[vx [md can] [vb suggest]]--150
[vx [rb sometimes] [vb suggest]]--12
Auxiliary rules are learned for the words sometimes and can, and using these rules, the
program selects more examples such as these:
[vx [md can] [vb refer]]--268
[vx [md may] [rb sometimes] [vb imply]]--3
From these, new main rules are learned for the words refer and imply. With these rules,
more auxiliary rules are selected?for the word may and so on.
The ATTITUDE and STYLE classes had to be considered together because both of
them use adjectival comparisons. Examples of ATTITUDE-STYLE DISTINCTIONS class are
these:
[ax [rbs most] [jj formal]]--54
[ax [rb much] [more more] [jj formal]]--9
[ax [rbs most] [jj concrete]]--5
2.3 Classification and Extraction
After we run the DL algorithm for the class DENOTATIONAL DISTINCTIONS, we split
the words in the list of main rules into its three sub-classes, as shown in Figure 2. This
sub-classification is manual for lack of a better procedure. Furthermore, some words can
be insignificant for any class (e.g., the word also) or for the given class; during the sub-
classification we mark them as OTHER. We repeat the same procedure for frequencies
and strengths with the words in the auxiliary rules. The words marked as OTHER and
the patterns that do not contain any word from the main rules are ignored in the next
processing steps. Similarly, after we run the algorithm for the class ATTITUDE-STYLE
DISTINCTIONS, we split the words in the list of main rules into its sub-classes and
sub-sub-classes (Figure 2). Frequencies are computed from the auxiliary rule list, and
strengths are computed by a module that resolves comparisons.
Once we had obtained the words and patterns for all the classes, we implemented
an automatic knowledge-extraction program that takes each sentence in CTRW and
231
Computational Linguistics Volume 32, Number 2
tries to extract one or more pieces of knowledge from it. Examples of results after this
stage are presented in Figure 4. The information extracted for denotational distinctions
is the near-synonym itself, the class, frequency, and strength of the distinction, and
the peripheral concept. At this stage, the peripheral concept is a string extracted from
the sentence. Strength takes the value low, medium, or high; frequency takes the value
always, usually, sometimes, seldom, or never. Default values (usually and medium)
are used when the strength or the frequency are not specified in the sentence. The
information extracted for attitudinal and stylistic distinctions is analogous.
The extraction program considers what near-synonyms each sentence fragment is
about (most often expressed as the subject of the sentence), what the expressed distinc-
tion is, and with what frequency and relative strength. If it is a denotational distinction,
then the peripheral concept involved has to be extracted too (from the object position in
the sentence). Therefore, our program looks at the subject of the sentence (the first noun
phrase before the main verb) and the object of the sentence (the first noun phrase after
the main verb). This heuristic works for sentences that present one piece of information.
There are many sentences that present two or more pieces of information. In such cases,
the program splits a sentence into coordinated clauses (or coordinated verb phrases) by
using a parser (Collins 1996) to distinguish when a coordinating conjunction (and, but,
whereas) is conjoining two main clauses or two parts of a complex verb phrase. From 60
randomly selected sentences, 52 were correctly dealt with (41 needed no split, 11 were
correctly split). Therefore, the accuracy was 86.6%. The eight mistakes included three
sentences that were split but should not have been, and five that needed splitting but
were not. The mistakes were mainly due to wrong parse trees.
When no information is extracted in this way, a few general patterns are matched
with the sentence in order to extract the near-synonyms; an example of such pattern
is: To NS1 is to NS2 .... There are also heuristics to retrieve compound subjects of
the form near-syn and near-syn and near-syn, near-syn, and near-syn. Once the class is
determined to be either DENOTATIONAL DISTINCTIONS or ATTITUDE-STYLE DISTINC-
TIONS, the target class (one of the leaves in the class hierarchy in Figure 2) is deter-
mined by using the manual partitions of the rules in the main decision list of the two
classes.
Sometimes the subject of a sentence refers to a group of near-synonyms. For ex-
ample, if the subject is the remaining words, our program needs to assert information
about all the near-synonyms from the same cluster that were not yet mentioned in the
text. In order to implement coreference resolution, we applied the same DL algorithm
to retrieve expressions used in CTRW to refer to near-synonyms or groups of near-
synonyms.
Sometimes CTRW describes stylistic and attitudinal distinctions relative to other
near-synonyms in the cluster. Such comparisons are resolved in a simple way by consid-
ering only three absolute values: low, medium, high. We explicitly tell the system which
words represent what absolute values of the corresponding distinction (e.g., abstract is
at the low end of Concreteness) and how the comparison terms increase or decrease
the absolute value (e.g., less abstract could mean a medium value of Concreteness).
2.4 Evaluation
Our program was able to extract 12,365 distinctions from 7,450 of the 14,138 sentences of
CTRW. (The rest of the sentences usually do not contain directly expressed distinctions;
for example: A terror-stricken person who is drowning may in panic resist the efforts of
someone who is trying to save him.)
232
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
In order to evaluate the final results, we randomly selected 25 clusters as a de-
velopment set, and another 25 clusters as a test set. The development set was used
to tune the program by adding new patterns if they helped improve the results. The
test set was used exclusively for testing. We built by hand a standard solution for
each set. The baseline algorithm is to choose the default values whenever possible.
There are no defaults for the near-synonyms the sentence is about or for peripheral
concepts; therefore, for these, the baseline algorithm assigns the sentence subject and
object, respectively, using only tuples extracted by the chunker.
The measures that we used for evaluating each piece of information extracted from
a sentence fragment were precision and recall. The results to be evaluated have four com-
ponents for ATTITUDE-STYLE DISTINCTIONS and five components for DENOTATIONAL
DISTINCTIONS. There could be missing components (except strength and frequency,
which take default values). Precision is the total number of correct components found
(summed over all the sentences in the test set) divided by the total number of com-
ponents found. Recall is the total number of correct components found divided by the
number of components in the standard solution.
For example, for the sentence Sometimes, however, profit can refer to gains outside the
context of moneymaking, the program obtains ?profit, usually, medium, Denotation,
gains outside the context of moneymaking?, whereas the solution is ?profit, some-
times, medium, Denotation, gains outside the context of money-making?. The pre-
cision is .80 (four correct out of five found), and the recall is also .80 (four correct out of
five in the standard solution).
Table 2 presents the results of the evaluation.2 The first row of the table presents
the results as a whole (all the components of the extracted lexical knowledge base). Our
system increases precision by 36 percentage points and recall by 46 percentage points
over baseline on the development set.3 Recall and precision are both slightly higher still
on the test set; this shows that the patterns added during the development stage were
general.
The second row of the table gives the evaluation results for extracting only the
class of the distinction expressed, ignoring the strengths, frequencies, and peripheral
concepts. This allows for a more direct evaluation of the acquired extraction patterns.
The baseline algorithm attained higher precision than in the case when all the com-
ponents are considered because the default class Denotation is the most frequent in
CTRW. Our algorithm attained slightly higher precision and recall on the development
set than it did in the complete evaluation, probably due to a few cases in which the
frequency and strength were incorrectly extracted, and slightly lower on the test set,
probably due to some cases in which the frequency and strength were easy to extract
correctly.
2.5 Conclusion
The result of this stage is a generic lexical knowledge base of near-synonym differences.
In subsequent sections, it will be enriched with knowledge from other sources; informa-
tion about the collocational behavior of the near-synonyms is added in Section 3, and
2 The improvement over the baseline is statistically significant (at p = 0.005 level or better) for all the results
presented in this article, except in one place to be noted later. Statistical significance tests were done with
the paired t test, as described by Manning and Schu?tze (1999, pages 208?209).
3 These values are improved over those of earlier systems presented by Inkpen and Hirst (2001).
233
Computational Linguistics Volume 32, Number 2
Table 2
Precision and recall of the baseline and of our algorithm (for all the components and for the
distinction class only; boldface indicates best results).
Baseline algorithm Our system (dev. set) Our system (test set)
Precision Recall Precision Recall Precision Recall
All .40 .23 .76 .69 .83 .73
Class only .49 .28 .79 .70 .82 .71
more distinctions acquired from machine-readable dictionaries are added in Section 4.
To be used in a particular NLP system, the generic LKB of NS needs to be customized
(Section 6). Section 7 shows how the customized LKB of NS can actually be used in
Natural Language Generation (NLG).
The method for acquiring extraction patterns can be applied to other dictionaries
of synonym differences. The extraction patterns that we used to build our LKB of NS
are general enough to work on other dictionaries of English synonyms. To verify this,
we applied the extraction programs presented in Section 2.3, without modification, to
the usage notes in the Merriam-Webster Online Dictionary.4 The distinctions expressed in
these usage notes are similar to the explanations from CTRW, but the text of these notes
is shorter and simpler. In a sample of 100 usage notes, we achieved a precision of 90%
and a recall of 82%.
3. Adding Collocational Knowledge from Free Text
In this section, the lexical knowledge base of near-synonym differences will be enriched
with knowledge about the collocational behavior of the near-synonyms. We take col-
locations here to be pairs of words that appear together, consecutively or separated by
only a few non-content words, much more often than by chance. Our definition is purely
statistical, and we make no claim that the collocations that we find have any element of
idiomaticity; rather, we are simply determining the preferences of our near-synonyms
for combining, or avoiding combining, with other words. For example daunting task is a
preferred combination (a collocation, in our terms), whereas daunting job is less preferred
(it should not be used in lexical choice unless there is no better alternative), and daunting
duty is an anti-collocation (Pearce 2001) that sounds quite odd and must not be used in
lexical choice.
There are three main steps in acquiring this knowledge, which are shown in Fig-
ure 7. The first two look in free text?first the British National Corpus, then the World
Wide Web?for collocates of all near-synonyms in CTRW, removing any closed-class
words (function words). For example, the phrase defeat the enemy will be treated as defeat
enemy; we will refer to such pairs as bigrams, even if there were intervening words. The
third step uses the t-test (Church et al 1991) to classify less frequent or unobserved
bigrams as less preferred collocations or anti-collocations. We outline the three steps
below; a more-detailed discussion is presented by Inkpen and Hirst (2002).
4 http://www.m-w.com/cgi-bin/dictionary
234
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 7
The three steps in acquiring collocational knowledge for near-synonyms.
3.1 Extracting Collocations from the British National Corpus
In step 1 of our procedure, our data was the 100-million-word part-of-speech-tagged
British National Corpus (BNC).5 Only 2.61% of the near-synonyms are absent from the
BNC; and only 2.63% occur between one and five times. We first preprocessed the BNC
by removing all words tagged as closed-class and all the proper names, and then used
the Ngram Statistics Package6 (Pedersen and Banerjee 2003), which counts bigram (or
n-gram) frequencies in a corpus and computes various statistics to measure the degree
of association between two words: pointwise mutual information (MI), Dice, chi-square
(?2), log-likelihood (LL), and Fisher?s exact test. (See Manning and Schu?tze [1999] for a
review of statistical methods that can be used to identify collocations.)
Because these five measures rank potential collocations in different ways and have
different advantages and drawbacks, we decided to combine them by choosing as a
collocation any bigram that was ranked by at least two of the measures as one of
that measure?s T most-highly ranked bigrams; the threshold T may differ for each
measure. Lower values for T increase the precision (reduce the chance of accepting
noncollocations) but may not get many collocations for some of the near-synonyms;
higher values increase the recall at the expense of lower precision. Because there is
no principled way of choosing these values, we opted for higher recall, with step 2
of the process (Section 3.2) filtering out many noncollocations in order to increase
the precision. We took the first 200,000 bigrams selected by each measure, except for
Fisher?s measure for which we took all 435,000 that were ranked equal top. From these
lists, we retained only those bigrams in which one of the words is a near-synonym
in CTRW.7
5 http://www.hcu.ox.ac.uk/BNC/
6 http://www.d.umn.edu/?tpederse/nsp.html. We used version 0.4, known at the time as the Bigram
Statistics Package (BSP).
7 Collocations of a near-synonym with the wrong part of speech were not considered (the collocations are
tagged), but when a near-synonym has more than one major sense, collocations for senses other than the
one required in the cluster could be retrieved. For example, for the cluster job, task, duty, and so on, the
collocation import duty is likely to be for a different sense of duty (the tariff sense). Therefore
disambiguation is required (assuming one sense per collocation). We experimented with a simple
Lesk-style method (Lesk 1986). For each collocation, instances from the corpus were retrieved, and the
content words surrounding the collocations were collected. This set of words was then intersected with
the entry for the near-synonym in CTRW. A non-empty intersection suggests that the collocation and the
entry use the near-synonym in the same sense. If the intersection was empty, the collocation was not
retained. However, we ran the disambiguation algorithm only for a subset of CTRW, and then abandoned
it, because hand-annotated data are needed to evaluate how well it works and because it was very
time-consuming (due to the need to retrieve corpus instances for each collocation). Moreover, skipping
disambiguation is relatively harmless because including these wrong senses in the final lexical
knowledge base of near-synonym collocations will not hurt. For example, if the collocation import duty is
associated with the cluster job, duty, etc., it simply will not be used in practice because the concepts of
import and this sense of duty are unlikely to occur together in coherent interlingual input.
235
Computational Linguistics Volume 32, Number 2
3.2 Filtering with Mutual Information from Web Data
In the previous step we emphasized recall at the expense of precision: Because of
the relatively small size of the BNC, it is possible that the classification of a bigram
as a collocation in the BNC was due to chance. However, the World Wide Web (the
portion indexed by search engines) is big enough that a result is more reliable. So we
can use frequency on the Web to filter out the more dubious collocations found in the
previous step.8 We did this for each putative collocation by counting its occurrence on
the Web, the occurrence of each component word, and computing the pointwise mutual
information (PMI) between the words. Only those whose pointwise mutual information
exceeded a threshold Tpmi were retained.
More specifically, if w is a word that collocates with one of the near-synonyms x in
a cluster, a proxy PMIprox for the pointwise mutual information between the words can
be given by the ratio
P(w, x)
P(x)
=
nwx
nx
where nwx and nx are the number of occurrences of wx and x, respectively. The for-
mula does not include P(w) because it is the same for various x. We used an inter-
face to the AltaVista search engine to do the counts, using the number of hits (i.e.,
matching documents) as a proxy for the actual number of bigrams.9 The threshold
Tpmi for PMIprox was determined empirically by finding the value that optimized re-
sults on a standard solution, constructed as follows. We selected three clusters from
CTRW, with a total of 24 near-synonyms. For these, we obtained 916 candidate collo-
cations from the BNC. Two human judges (computational linguistics students, native
speakers of English) were asked to mark the true collocations (what they considered
to be good usage in English). The candidate pairs were presented to the judges in
random order, and each was presented twice.10 A bigram was considered to be a
true collocation only if both judges considered it so. We used this standard solution
to choose the value of Tpmi that maximizes the accuracy of the filtering program.
Accuracy on the test set was 68.3% (compared to approximately 50% for random
choice).
3.3 Finding Less Preferred Collocations and Anti-Collocations
In seeking knowledge of less preferred collocations and anti-collocations, we are look-
ing for bigrams that occur infrequently or not at all. The low frequency or absence of a
8 Why not just use the Web and skip the BNC completely? Because we would then have to count Web
occurrences of every near-synonym in CTRW combined with every content word in English. Using the
BNC as a first-pass filter vastly reduces the search space.
9 The collocations were initially acquired from the BNC with the right part of speech for the near-synonym
because the BNC is part-of-speech-tagged, but on the Web there are no part-of-speech tags; therefore a
few inappropriate instances may be included in the counts.
10 One judge was consistent (judged a collocation in the same way both times it appeared) in 90.4% of the
cases and the other in 88% of the cases. The agreement between the two judges was 78% (computed in a
strict way; i.e., we considered agreement only when the two judges had the same opinion including the
cases when they were not consistent), yielding ? = .51 with a 95% confidence interval of 0.47 to 0.55. (The
notion of confidence intervals for ? is defined, e.g., by Sim and Wright [2005]. The computations were
done with the PEPI statistical package [http://sagebrushpress.com/pepibook.html].) These figures show
that the task is not easy for humans.
236
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
bigram in the BNC may be due to chance. However, the World Wide Web is big enough
that a negative result is more reliable. So we can again use frequency on the Web?this
time to determine whether a bigram that was infrequent or unseen in the BNC is truly
a less preferred collocation or anti-collocation.
The bigrams of interest now are those in which collocates for a near-synonym that
were found in step 1 and filtered in step 2 are combined with another member of the
same near-synonym cluster. For example, if the collocation daunting task was found,
we now look on the Web for the apparent noncollocations daunting job, daunting duty,
and other combinations of daunting with near-synonyms of task. A low number of
co-occurrences indicates a less preferred collocation or anti-collocation. We employ the
t-test, following Manning and Schu?tze (1999, pages 166?168), to look for differences.
The collocations of each near-synonym with a given collocate are grouped in three
classes, depending on the t values of pairwise collocations. A t value comparing each
collocation and the collocation with maximum frequency is computed, and so is the
t value between each collocation and the collocation with minimum frequency. Table 3
presents an example.
After the t-test scores were computed, a set of thresholds was determined to classify
the collocations in the three groups: preferred collocations, less preferred collocations,
and anti-collocations. Again, we used a standard solution in the procedure. Two judges
manually classified a sample of 2,838 potential collocations obtained for the same three
clusters of near-synonyms from 401 collocations that remained after filtering. They
were instructed to mark as preferred collocations all the potential collocations that
they considered good idiomatic use of language, as anti-collocations the ones that they
would not normally use, and as less preferred collocations the ones that they were not
comfortable classifying in either of the other two classes. When the judges agreed, the
class was clear. When they did not agree, we used simple rules, such as these: When
one judge chose the class-preferred collocation, and the other chose the class anti-
collocation, the class in the solution was less preferred collocation (because such cases
seemed to be difficult and controversial); when one chose preferred collocation, and the
other chose less preferred collocation, the class in the solution was preferred collocation;
when one chose anti-collocation, and the other chose less preferred collocation, the class
in the solution was anti-collocation. The agreement between judges was 84%, ? = 0.66
(with a 95% confidence interval of 0.63 to 0.68).
Table 3
Example of counts, mutual information scores, and t-test scores for the collocate daunting with
near-synonyms of task. The second column shows the number of hits for the collocation daunting
x, where x is the near-synonym in the first column. The third column shows PMIprox (scaled by
105 for readability), the fourth column, the t values between the collocation with maximum
frequency (daunting task) and daunting x, and the last column, the t-test between daunting x and
the collocations with minimum frequency (daunting stint and daunting hitch).
x Hits PMIprox t max t min
task 63,573 0.011662 ? 252.07
job 485 0.000022 249.19 22.02
assignment 297 0.000120 250.30 17.23
chore 96 0.151899 251.50 9.80
duty 23 0.000022 251.93 4.80
stint 0 0 252.07 ?
hitch 0 0 252.07 ?
237
Computational Linguistics Volume 32, Number 2
Table 4
Example of results of our program for collocations of near-synonyms in the task cluster.
?
marks
preferred collocations, ? marks less preferred collocations, and ? marks anti-collocations. The
combined opinion of the judges about the same pairs of words is shown in parentheses.
Collocates
Near-synonyms daunting particular tough
task
?
(
?
)
?
(
?
)
?
(
?
)
job ? (
?
)
?
(
?
)
?
(
?
)
assignment ? (?) ? (?) ? (?)
chore ? ( ?) ? (?) ? ( ?)
duty ? ( ?) ? (?) ? ( ?)
stint ? ( ?) ? ( ?) ? ( ?)
hitch ? ( ?) ? ( ?) ? ( ?)
We used this standard solution as training data to C4.511 to learn a decision tree
for the three-way classifier. The features in the decision tree are the t-test between
each collocation and the collocation from the same group that has maximum frequency
on the Web, and the t-test between the current collocation and the collocation that
has minimum frequency (as presented in Table 3). We did 10-fold cross-validation to
estimate the accuracy on unseen data. The average accuracy was 84.1%, with a standard
error of 0.5%; the baseline of always choosing the most frequent class, anti-collocations,
yields 71.4%. We also experimented with including PMIprox as a feature in the decision
tree, and with manually choosing thresholds (without a decision tree) for the three-
way classification, but the results were poorer. The three-way classifier can fix some
of the mistakes of the PMI filter: If a wrong collocation remains after the PMI filter,
the classifier can classify it in the anti-collocations class. We conclude that the acquired
collocational knowledge has acceptable quality.
3.4 Results
We obtained 1,350,398 distinct bigrams that occurred at least four times. We selected col-
locations for all 909 clusters in CTRW (5,452 near-synonyms in total). Table 4 presents an
example of results for collocational classification of bigrams, where
?
marks preferred
collocations, ? marks less preferred collocations, and ? marks anti-collocations. This
gave us a lexical knowledge base of near-synonym collocational behavior. An example
of collocations extracted for the near-synonym task is presented in Table 5, where the
columns are, in order, the name of the measure, the rank given by the measure, and the
value of the measure.
4. Adding Knowledge from Machine-Readable Dictionaries
Information about near-synonym differences can be found in other types of dictionaries
besides those explicitly on near-synonyms. Although conventional dictionaries, unlike
CTRW, treat each word in isolation, they may nonetheless contain useful information
11 http://www.cse.unsw.edu.au/?quinlan
238
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Table 5
Example of collocations extracted for the near-synonym task. The first collocation was selected
(ranked in the set of first T collocations) by four measures; the second collocation was selected
by two measures.
Collocation Measure Rank Score
daunting/A task/N MI 24,887 10.85
LL 5,998 907.96
?2 16,341 122,196.82
Dice 2,766 0.02
repetitive/A task/N MI 64,110 6.77
?2 330,563 430.40
about near-synonyms because some definitions express a distinction relative to another
near-synonym. From the SGML-marked-up text of the Macquarie Dictionary12 (Delbridge
et al 1987), we extracted the definitions of the near-synonyms in CTRW for the expected
part of speech that contained another near-synonym from the same cluster. For example,
for the CTRW cluster burlesque, caricature, mimicry, parody, takeoff, travesty, one definition
extracted for the near-synonym burlesque was any ludicrous take-off or debasing caricature
because it contains caricature from the same cluster. A series of patterns was used
to extract the difference between the two near-synonyms wherever possible. For the
burlesque example, the extracted information was
?burlesque, usually, medium, Denotation, ludicrous?,
?burlesque, usually, medium, Denotation, debasing?.
The number of new denotational distinctions acquired by this method was 5,731.
We also obtained additional information from the General Inquirer13 (Stone et al
1966), a computational lexicon that classifies each word in it according to an extendable
number of categories, such as pleasure, pain, virtue, and vice; overstatement and un-
derstatement; and places and locations. The category of interest here is Positiv/Negativ.
There are 1,915 words marked as Positiv (not including words for yes, which is a separate
category of 20 entries), and 2,291 words marked as Negativ (not including the separate
category of no in the sense of refusal). For each near-synonym in CTRW, we used
this information to add a favorable or unfavorable attitudinal distinction accordingly.
If there was more than one entry (several senses) for the same word, the attitude
was asserted only if the majority of the senses had the same marker. The number
of attitudinal distinctions acquired by this method was 5,358. (An attempt to use the
occasional markers for formality in WordNet in a similar manner resulted in only 11
new distinctions.)
As the knowledge from each source is merged with the LKB, it must be checked for
consistency in order to detect conflicts and resolve them. The algorithm for resolving
conflicts is a voting scheme based on the intuition that neutral votes should have less
weight than votes for the two extremes. The algorithm outputs a list of the conflicts
and a proposed solution. This list can be easily inspected by a human, who can change
12 http://www.macquariedictionary.com.au/
13 http://www.wjh.harvard.edu/?inquirer/
239
Computational Linguistics Volume 32, Number 2
Figure 8
Fragment of the representation of the error cluster (prior to customization).
the solution of the conflict in the final LKB of NS, if desired. The consistency-checking
program found 302 conflicts for the merged LKB of 23,469 distinctions. After conflict
resolution, 22,932 distinctions remained. Figure 8 shows a fragment of the knowledge
extracted for the near-synonyms of error after merging and conflict resolution.
5. Related Work
5.1 Building Lexical Resources
Lexical resources for natural language processing have also been derived from other
dictionaries and knowledge sources. The ACQUILEX14 Project explored the utility of
constructing a multilingual lexical knowledge base (LKB) from machine-readable ver-
sions of conventional dictionaries. Ide and Ve?ronis (1994) argue that it is not possible to
build a lexical knowledge base from a machine-readable dictionary (MRD) because the
information it contains may be incomplete, or it may contain circularities. It is possible
to combine information from multiple MRDs or to enhance an existing LKB, they say,
although human supervision may be needed.
Automatically extracting world knowledge from MRDs was attempted by projects
such as MindNet at Microsoft Research (Richardson, Dolan, and Vanderwende 1998),
and Barrie`rre and Popowich?s (1996) project, which learns from children?s dictionaries.
IS-A hierarchies have been learned automatically from MRDs (Hearst 1992) and from
corpora (Caraballo [1999] among others).
14 http://www.cl.cam.ac.uk/Research/NL/acquilex/acqhome.html
240
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Research on merging information from various lexical resources is related to the
present work in the sense that the consistency issues to be resolved are similar. One
example is the construction of Unified Medical Language System (UMLS)15 (Lindberg,
Humphreys, and McCray 1993), in the medical domain. UMLS takes a wide range of
lexical and ontological resources and brings them together as a single resource. Most of
this work is done manually at the moment.
5.2 Acquiring Collocational Knowledge
There has been much research on extracting collocations for different applications. Like
Church et al (1991), we use the t-test and mutual information (MI), but unlike them
we use the Web as a corpus for this task (and a modified form of mutual information),
and we distinguish three types of collocations. Pearce (2001) improved the quality of
retrieved collocations by using synonyms from WordNet (Pearce 2001); a pair of words
was considered a collocation if one of the words significantly prefers only one (or
several) of the synonyms of the other word. For example, emotional baggage is a good
collocation because baggage and luggage are in the same synset and ?emotional luggage
is not a collocation. Unlike Pearce, we use a combination of t-test and MI, not just
frequency counts, to classify collocations.
There are two typical approaches to collocations in previous NLG systems: the
use of phrasal templates in the form of canned phrases, and the use of automatically
extracted collocations for unification-based generation (McKeown and Radev 2000).
Statistical NLG systems (such as Nitrogen [Langkilde and Knight 1998]) make good
use of the most frequent words and their collocations, but such systems cannot choose
a less-frequent synonym that may be more appropriate for conveying desired nuances
of meaning if the synonym is not a frequent word.
Turney (2001) used mutual information to choose the best answer to questions about
near-synonyms in the Test of English as a Foreign Language (TOEFL) and English as
a Second Language (ESL). Given a problem word (with or without context) and four
alternative words, the question is to choose the alternative most similar in meaning
to the problem word (the problem here is to detect similarities, whereas in our work
differences are detected). His work is based on the assumption that two synonyms are
likely to occur in the same document (on the Web). This can be true if the author needs
to avoid repeating the same word, but not true when the synonym is of secondary
importance in a text. The alternative that has the highest pointwise mutual information
for information retrieval (PMI-IR) with the problem word is selected as the answer. We
used the same measure in Section 3.3?the mutual information between a collocation
and a collocate that has the potential to discriminate between near-synonyms. Both
works use the Web as a corpus, and a search engine to estimate the mutual information
scores.
5.3 Near-Synonyms
As noted in the introduction, our work is based on that of Edmonds and Hirst (2002) and
Hirst (1995), in particular the model for representing the meaning of the near-synonyms
presented in Section 1.2 and the preference satisfaction mechanism used in Section 7.
15 http://www.nml.nih.gov/research/umls/
241
Computational Linguistics Volume 32, Number 2
Other related research involving differences between near-synonyms has a lin-
guistic or lexicographic, rather than computational, flavor. Apresjan built a bilin-
gual dictionary of synonyms, more specifically a dictionary of English synonyms
explained in Russian (Apresjan et al 1980). It contains 400 entries selected from
the approximately 2,500 entries from Webster?s New Dictionary of Synonyms, but re-
organized by splitting or merging clusters of synonyms, guided by lexicographic
principles described by Apresjan (2000). An entry includes the following types of
differences: semantic, evaluative, associative and connotational, and differences in
emphasis or logical stress. These differences are similar to the ones used in our
work.
Gao (2001) studied the distinctions between near-synonym verbs, more specifically
Chinese physical action verbs such as verbs of cutting, putting, throwing, touching,
and lying. Her dissertation presents an analysis of the types of semantic distinctions
relevant to these verbs, and how they can be arranged into hierarchies on the basis of
their semantic closeness.
Ploux and Ji (2003) investigated the question of which words should be considered
near-synonyms, without interest in their nuances of meaning. They merged clusters of
near-synonyms from several dictionaries in English and French and represented them
in a geometric space. In our work, the words that are considered near-synonyms are
taken from CTRW; a different dictionary of synonyms may present slightly different
views. For example, a cluster may contain some extra words, some missing words, or
sometimes the clustering could be done in a different way. A different approach is to
automatically acquire near-synonyms from free text. Lin et al (2003) acquire words that
are related by contextual similarity and then filter out the antonyms by using a small
set of manually determined patterns (such as ?either X or Y?) to construct Web queries
for pairs of candidate words. The problem of this approach is that it still includes words
that are in relations other than near-synonymy.
6. Customizing the Lexical Knowledge Base of Near-Synonym Differences
The initial LKB of NS built in Sections 2 to 4 is a general one, and it could, in principle, be
used in any (English) NLP system. For example, it could be used in the lexical-analysis
or lexical-choice phase of machine translation. Figure 9 shows that during the analysis
phase, a lexical knowledge base of near-synonym differences in the source language
is used, together with the context, to determine the set of nuances that are expressed
in the source-language text (in the figure, the source language is French and the target
language is English). In the generation phase, these nuances become preferences for the
lexical-choice process. Not only must the target-language text express the same meaning
as the source-language text (necessary condition), but the choice of words should satisfy
the preferences as much as possible.
In order to be integrated with the other components of the NLP system, the LKB will
probably need some adaptation?in particular, the core denotations and the peripheral
concepts will need to be mapped to the ontology that the system employs. This might
be a general-purpose ontology, such as Cyc (Lenat 1995) and WordNet, or an ontology
built specially for the system (such as Mikrokosmos (Mahesh and Nirenburg 1995) or
domain-specific ontologies). In this section, we focus on the generation phase of an in-
terlingual machine translation system, specifically the lexical-choice process, and show
how the LKB was adapted for Xenon, a natural language generation system. Xenon
is a general-purpose NLG system that exploits our LKB of NS. To implement Xenon,
242
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 9
Lexical analysis and choice in machine translation; adapted from Edmonds and Hirst (2002). The
solid lines show the flow of data: input, intermediate representations, and output; the dashed
lines show the flow of knowledge from the knowledge sources to the analysis and the generation
module. The rectangles denote the main processing modules; the rest of the boxes denote data or
knowledge sources.
we modified the lexical-choice component of a preexisting NLG system, HALogen
(Langkilde 2000; Langkilde and Knight 1998), to handle knowledge about the near-
synonym differences. (Xenon will be described in detail in Section 7.) This required
customization of the LKB to the Sensus ontology (Knight and Luk 1994) that HALogen
uses as its representation.
Customization of the core denotations for Xenon was straightforward. The core
denotation of a cluster is a metaconcept representing the disjunction of all the Sensus
concepts that could correspond to the near-synonyms in a cluster. The names of meta-
concepts, which must be distinct, are formed by the prefix generic, followed by the
name of the first near-synonym in the cluster and the part of speech. For example, if the
cluster is lie, falsehood, fib, prevarication, rationalization, untruth, the name of the cluster is
generic lie n.
Customizing the peripheral concepts, which are initially expressed as strings, could
include parsing the strings and mapping the resulting syntactic representation into a
semantic representation. For Xenon, however, we implemented a set of 22 simple rules
that extract the actual peripheral concepts from the initial peripheral strings. A trans-
formation rule takes a string of words part-of-speech tagged and extracts a main word,
several roles, and fillers for the roles. The fillers can be words or recursive structures. In
Xenon, the words used in these representations are not sense-disambiguated. Here are
two examples of input strings and extracted peripheral concepts:
"an embarrassing breach of etiquette"
=> (C / breach :GPI etiquette :MOD embarrassing)
"to an embarrassing or awkward occurrence"
=> (C / occurrence :MOD (OR embarrassing awkward))
The roles used in these examples are MOD (modifier) and GPI (generalized possession
inverse). The rules that were used for these two examples are these:
Adj Noun1 of Noun2 => (C / Noun1 :GPI Noun2 :MOD Adj)
Adj1 or Adj2 Noun => (C / Noun :MOD (OR Adj1 Adj2))
243
Computational Linguistics Volume 32, Number 2
We evaluated our customization of the LKB on a hand-built standard solution
for a set of peripheral strings: 139 strings to be used as a test set and 97 strings to
be used as a development set. The rules achieved a coverage of 75% on the test set
with an accuracy of 55%.16 In contrast, a baseline algorithm of taking the first word
in each string as the peripheral concept covers 100% of the strings, but with only 16%
accuracy.
Figure 10 shows the full customized representation for the near-synonyms of error,
derived from the initial representation that was shown earlier in Figure 8. (See Inkpen
[2003] for more examples of customized clusters.) The peripheral concepts are factored
out, and the list of distinctions contains pointers to them. This allows peripheral con-
cepts to be shared by two or more near-synonyms.
7. Xenon: An NLG System that Uses Knowledge of Near-Synonym Differences
This section presents Xenon, a large-scale NLG system that uses the lexical knowledge-
base of near-synonyms customized in Section 6. Xenon integrates a new near-synonym
choice module with the sentence realization system HALogen17 (Langkilde 2000;
Langkilde and Knight 1998). HALogen is a broad-coverage general-purpose natural
language sentence generation system that combines symbolic rules with linguistic
information gathered statistically from large text corpora. The internal architecture
of HALogen is presented in Figure 11. A forest of all possible sentences (combi-
nations of words) for the input is built, and the sentences are then ranked ac-
cording to an n-gram language model in order to choose the most likely one as
output.
Figure 12 presents the architecture of Xenon. The input is a semantic representation
and a set of preferences to be satisfied. The final output is a set of sentences and
their scores. A concrete example of input and output is shown in Figure 13. Note
that HALogen may generate some ungrammatical constructs, but they are (usually)
assigned lower scores. The first sentence (the highest ranked) is considered to be the
solution.
7.1 Metaconcepts
The semantic representation input to Xenon is represented, like the input to HAL-
ogen, in an interlingua developed at University of Southern California/Information
Sciences Institute (USC/ISI).18 As described by Langkilde-Geary (2002b), this lan-
guage contains a specified set of 40 roles, whose fillers can be either words, con-
cepts from Sensus (Knight and Luk 1994), or complex interlingual representations.
The interlingual representations may be underspecified: If some information needed
by HALogen is not present, it will use its corpus-derived statistical information to
16 We found that sometimes a rule would extract only a fragment of the expected configuration of concepts
but still provided useful knowledge; however, such cases were not considered to be correct in this
evaluation, which did not allow credit for partial correctness. For example, if the near-synonym command
denotes the/TD stated/VB demand/NN of/IN a/TD superior/JJ, the expected peripheral concept is
(C1 / demand :GPI superior :MOD stated). If the program extracted only (C1 / demand :GPI
superior), the result was not considered correct, but the information might still help in an NLP system.
17 http://www.isi.edu/licensed-sw/halogen/
18 http://www.isi.edu/licensed-sw/halogen/interlingua.html
244
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 10
The final representation of the error cluster.
make choices. Xenon extends this representation language by adding metaconcepts
that correspond to the core denotation of the clusters of near-synonyms. For example,
in Figure 13, the metaconcept is generic lie n. As explained in Section 6, metacon-
cepts may be seen as a disjunction of all the senses of the near-synonyms of the
cluster.
7.2 Near-Synonym Choice
The near-synonym choice module has to choose the most appropriate near-synonym
from the cluster specified in the input. It computes a satisfaction score that becomes
a weight (to be explained in section 7.3) for each near-synonym in the cluster. HAL-
ogen makes the final choice by adding these weights to n-gram probabilities from its
language model (more precisely, the negative logarithms of these values) and choosing
245
Computational Linguistics Volume 32, Number 2
Figure 11
The architecture of the sentence realizer HALogen.
the highest-ranked sentence. For example, the expanded representation of the input
in Figure 13 is presented in Figure 14. The near-synonym choice module gives higher
weight to fib because it satisfies the preferences better than the other near-synonyms in
the cluster, lie, falsehood, fib, prevarication, rationalization, and untruth.
7.3 Preferences and Similarity of Distinctions
The preferences that are input to Xenon could be given by the user, or they could come
from an analysis module if Xenon is used in a machine translation system (correspond-
ing to nuances of near-synonyms in a different language, see Figure 9). The preferences,
like the distinctions expressed in the LKB of NS, are of three types: attitudinal, stylistic,
and denotational. Examples of each:
(low formality)
(disfavour :agent)
(imply (C / assessment :MOD ( M / (*OR* ignorant uninformed)).
The formalism for expressing preferences is from I-Saurus (Edmonds 1999). The
preferences are transformed internally into pseudodistinctions that have the same form
as the corresponding type of distinctions so that they can be directly compared with the
distinctions. The pseudodistinctions corresponding to the previous examples are these:
(-? low Formality)
(- always high Pejorative :agent)
(- always medium Implication
(C/assessment :MOD (M/(OR ignorant uninformed)).
Figure 12
The architecture of the natural language generation system Xenon.
246
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 13
Example of input and output of Xenon.
Figure 14
The interlingual representation of the input in Figure 13 after expansion by the near-synonym
choice module.
For each near-synonym NS in a cluster, a weight is computed by summing the
degree to which the near-synonym satisfies each preference from the set P of input
preferences:
Weight(NS, P) =
?
p?P
Sat(p, NS). (2)
The weights are transformed through an exponential function so that numbers are
comparable with the differences of probabilities from HALogen?s language model:
f (x) = e
xk
e ? 1 . (3)
We set k = 15 as a result of experiments with a development set.
For a given preference p ? P, the degree to which it is satisfied by NS is reduced to
computing the similarity between each of NS?s distinctions and a pseudodistinction d(p)
generated from p. The maximum value over i is taken (where di(w) is the ith distinction
of NS):
Sat(p, NS) = max
i
Sim(d(p), di(NS)). (4)
247
Computational Linguistics Volume 32, Number 2
where the similarity of two distinctions, or of a distinction and a preference (trans-
formed into a distinction), is computed with the three types of similarity measures that
were used by Edmonds and Hirst (2002) in I-Saurus:
Sim(d1, d2) =
?
?
?
?
?
?
?
Simden(d1, d2) if d1 and d2 are denotational distinctions
Simatt(d1, d2) if d1 and d2 are attitudinal distinctions
Simsty(d1, d2) if d1 and d2 are stylistic distinctions
0 otherwise
(5)
Distinctions are formed out of several components, represented as symbolic values
on certain dimensions, such as frequency (seldom, sometimes, etc.) and strength (low,
medium, high). In order to compute a numeric score, each symbolic value is mapped
into a numeric one. The numeric values are not as important as their relative difference.
If the two distinctions are not of the same type, they are incommensurate and their
similarity is zero. The formulas for Simatt and Simsty involve relatively straightforward
matching. However, Simden requires the matching of complex interlingual structures.
This boils down to computing the similarity between the main concepts of the two
interlingual representations, and then recursively mapping the shared semantic roles
(and compensating for the roles that appear in only one). When atomic concepts or
words are reached, we use a simple measure of word/concept similarity based on
the hierarchy of Sensus. All the details of these formulas, along with examples, are
presented by Inkpen and Hirst (2003).
7.4 Integrating the Knowledge of Collocational Behavior
Knowledge of collocational behavior is not usually present in NLG systems. Adding it
will increase the quality of the generated text, making it more idiomatic: The system
will give priority to a near-synonym that produces a preferred collocation and will not
choose one that causes an anti-collocation to appear in the generated sentence.
Unlike most other NLG systems, HALogen already incorporates some collocational
knowledge implicitly encoded in its language model (bigrams or trigrams), but this is
mainly knowledge of collocations between content words and function words. There-
fore, in its integration into Xenon, the collocational knowledge acquired in Section 3
will be useful, as it includes collocations between near-synonyms and other nearby
content words. Also, it is important whether the near-synonym occurs before or after
the collocate; if both positions are possible, both collocations are in the knowledge
base.
The architecture of Xenon extended with the near-synonym collocation module
is presented in Figure 15. The near-synonym collocation module intercepts the forest
structure, modifies its weights as necessary, and then forwards it to the statistical
ranking module. If a potential anti-collocation is seen in the forest structure, the weight
of the near-synonym is discounted by Wanti colloc; if a less preferred collocation is seen,
the weight of the near-synonym is discounted by Wless pref colloc. For preferred collo-
cations, the weight is unchanged. If the collocate is not the only alternative, the other
alternatives should be discounted, unless they also form a preferred collocation. Sec-
tion 7.5.2 explains how the discount weights were chosen.
248
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 15
The architecture of Xenon extended with the near-synonym collocation module. In this figure,
the knowledge sources are not shown.
7.5 Evaluation of Xenon
The components of Xenon to be evaluated here are the near-synonym choice module
and the near-synonym collocation module. We evaluate each module in interaction with
the sentence-realization module HALogen,19 first individually and then both working
together.20
An evaluation of HALogen itself was presented by Langkilde-Geary (2002a) using
a section of the Penn Treebank as test set. HALogen was able to produce output for
80% of a set of 2,400 inputs (automatically derived from the test sentences by an input
construction tool). The output was 94% correct when the input representation was
fully specified, and between 94% and 55% for various other experimental settings. The
accuracy was measured using the BLEU score (Papineni et al 2001) and the string
edit distance by comparing the generated sentences with the original sentences. This
evaluation method can be considered as English-to-English translation via meaning
representation.
7.5.1 Evaluation of the Near-Synonym Choice Module. For the evaluation of the near-
synonym choice module, we conducted two experiments. (The collocation module was
disabled for these experiments.) Experiment 1 involved simple monolingual generation.
Xenon was given a suite of inputs: Each was an interlingual representation of a sentence
and the set of nuances that correspond to a near-synonym in the sentence (see Figure 16).
The sentence generated by Xenon was considered correct if the expected near-synonym,
whose nuances were used as input preferences, is chosen. The sentences used in this
first experiment were very simple; therefore, the interlingual representations were easily
built by hand. In the interlingual representation, the near-synonym was replaced with
the corresponding metaconcept. There was only one near-synonym in each sentence.
Two data sets were used in Experiment 1: a development set of 32 near-synonyms of
the five clusters presented in Figure 17 in order to set the exponent k of the scaling
function in equation (3), and a test set of 43 near-synonyms selected from six clusters,
namely, the set of English near-synonyms shown in Figure 18.
19 All the evaluation experiments presented in this section used HALogen?s trigram language model. The
experiments were repeated with the bigram model, and the results were almost identical.
20 Preliminary evaluation experiments of only the near-synonym choice module were presented by Inkpen
and Hirst (2003).
249
Computational Linguistics Volume 32, Number 2
Figure 16
The architecture of Experiment 1.
Figure 17
Development data set used in Experiment 1.
Some of Xenon?s choices could be correct solely because the expected near-synonym
happens to be the default one (the one with the highest probability in the language
model). So as a baseline (the performance that can be achieved without using the LKB
of NS), we ran Xenon on all the test cases, but without input preferences.
The results of Experiment 1 are presented in Table 6. For each data set, the second
column shows the number of test cases. The column labeled ?Total correct? shows the
number of answers considered correct (when the expected near-synonym was chosen).
The column labeled ?Ties? shows the number of cases when the expected near-synonym
had weight 1.0, but there were other near-synonyms that also had weight 1.0 because
they happen to have identical nuances in the LKB of NS. The same column shows in
parentheses how many of these ties caused an incorrect near-synonym choice. In such
cases, Xenon cannot be expected to make the correct choice, or, more precisely, the other
choices are equally correct, at least as far as Xenon?s LKB is concerned. Therefore, the
Figure 18
Test data set used in Experiment 1 (English only) and Experiment 2 (English and French).
250
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Table 6
Results of Experiment 1 (boldface indicates best results).
No. Correct Base- Accuracy
of Total by line (no ties) Accuracy
Data set cases correct default Ties % % %
Development 32 27 5 5 (4) 15.6 84.3 96.4
Test 43 35 6 10 (5) 13.9 81.3 92.1
accuracies computed without considering these cases (the seventh column) are under-
estimates of the real accuracy of Xenon. The last column presents accuracies taking the
ties into account, defined as the number of correct answers divided by the difference
between the total number of cases and the number of incorrectly resolved ties.
Experiment 2 is based on machine translation. These experiments measure how
successful the translation of near-synonyms is, both from French into English and
from English into English. The experiments used pairs of French and English sen-
tences that are translations of one another (and that contain near-synonyms of interest),
extracted from sentence-aligned parallel text, the bilingual Canadian Hansard. Ex-
amples are shown in Figure 19.21 For each French sentence, Xenon should generate
an English sentence that contains an English near-synonym that best matches the nu-
ances of the French original. If Xenon chooses exactly the English near-synonym used
in the parallel text, then Xenon?s behavior is correct. This is a conservative evaluation
measure because there are cases in which more than one of the possibilities would be
acceptable.
As illustrated earlier in Figure 9, an analysis module is needed. For the evalua-
tion experiments, a simplified analysis module is sufficient. Because the French and
English sentences are translations of each other, we can assume that their interlingual
representation is essentially the same. So for the purpose of these experiments, we
can use the interlingual representation of the English sentence to approximate the
interlingual representation of the French sentence and simply add the nuances of the
French near-synonym to the representation. This is a simplification because there may
be some sentences for which the interlingual representation of the French sentence is
different because of translation divergences between languages (Dorr 1993). For the
sentences in our test data, a quick manual inspection shows that this happens very
rarely or not at all. This simplification eliminates the need to parse the French sen-
tence and the need to build a tool to extract its semantics. As depicted in Figure 20,
the interlingual representation is produced with a preexisting input construction tool
that was previously used by Langkilde-Geary (2002a) in her HALogen evaluation
experiments. In order to use this tool, we parsed the English sentences with Charniak?s
parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn
TreeBank, which have some extra annotations; it worked on parse trees produced
by Charniak?s parser, but it failed on some parse trees probably more often than it
21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/)
(approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations
of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik,
Olsen, and Diab 1999), happened to contain very few occurrences of the near-synonyms of interest.
22 ftp://ftp.cs.brown.edu/pub/nlparser/
251
Computational Linguistics Volume 32, Number 2
Figure 19
Examples of parallel sentences used in Experiment 2.
Figure 20
The architecture of Experiment 2 (French to English).
did in HALogen?s evaluation experiments. We replaced each near-synonym with the
metaconcept that is the core meaning of its cluster. The interlingual representation for
the English sentence is semantically shallow; it does not reflect the meaning of the
French sentence perfectly, but in these experiments we are interested only in the near-
synonyms from the test set; therefore, the other words in the French sentence are not
important.
The analyzer of French nuances of Figure 20 needs to extract nuances from an
LKB of French synonyms. We created by hand an LKB for six clusters of French near-
synonyms (those from Figure 18) from two paper dictionaries of French synonyms,
Be?nac (1956) and Bailly (1973). For each peripheral string, in French, an equivalent
concept is found in Sensus by looking for English translations of the words and then
finding Sensus concepts for the appropriate senses of the English words. Figure 21
presents a fragment of a cluster of French near-synonyms. For example, if we are
told that erreur denotes fausse opinion, the equivalent peripheral concept is (P8 (c8 /
|view<belief| :mod |false>untrue|)). If we are told that gaffe denotes be?vue
grossiere, then the equivalent peripheral concept is (P7 (c7 / |glaring,gross|)).
A similar experiment, translating not from French into English but from English
into English, is useful for evaluation purposes. An English sentence containing a near-
synonym is processed to obtain its semantic representation (where the near-synonym is
replaced with a metaconcept), and the lexical nuances of the near-synonym are input as
preferences to Xenon. Ideally, the same near-synonym as in the original sentence would
be chosen by Xenon (we consider it to be the correct choice). The percentage of times
this happens is an evaluation measure. The architecture of this experiment is presented
in Figure 22.
It happens that not all the near-synonyms in the test data set were found in Han-
sard?in fact, only 13 distinct pairs occur as translations of each other. Some of these
pairs are very frequent, and some are rare. In order to evaluate the system for all these
near-synonyms, both with and without regard to their frequency, we prepared two
252
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Figure 21
Fragment of a cluster of French near-synonyms.
Figure 22
The architecture of Experiment 2 (English to English).
data sets by sampling Hansard in two different ways. Sentence data set 1 contains,
for each French and English near-synonym pair in the test data set, two pairs of
sentences in which the English word appears as a translation of the French. The
sentences selected for each pair were the first two for which the input construction
tool produced a valid interlingual representation. Sentence data set 2 is similar to
set 1, but instead of having two sentences for a near-synonym pair, it contains all the
sentence pairs in a large fragment of Hansard in which the near-synonyms of interest
occurred. Therefore, this data set has the advantage of a natural frequency distribution.
It has the disadvantage that the results for the less-frequent near-synonyms, which
tend to be the ?harder? and more-interesting cases (see below), may be swamped by
the more-frequent, relatively ?easy? cases. Initially there were 564 pairs of sentences,
but the input construction tool worked successfully only on 298 English sentences. The
interlingual representations that it produced are quite complex, typically several pages
long.
The results of Experiment 2 are presented in Table 7;23 the interpretation of the
columns is the same as for Table 6. For each set of sentences, the baseline is the same
23 The improvement over baseline is statistically significant for all the results in Table 7, except the third line.
253
Computational Linguistics Volume 32, Number 2
Table 7
Results of Experiment 2 (boldface indicates best results).
Data set No. Correct Base- Accuracy
and of Total by line (no ties) Accuracy
condition cases correct default Ties % % %
Sentence set 1 26 13 10 5 (3) 38.4 50.0 56.5
French to English
Sentence set 1 26 26 10 2 (0) 38.4 100 100
English to English
Sentence set 2 298 217 214 7 (1) 71.8 72.8 73.0
French to English
Sentence set 2 298 296 214 2 (0) 71.8 99.3 99.3
English to English
for the French-to-English and English-to-English experiments because no nuances were
used as input for the baseline experiments. The baseline for data set 2 is quite high
(71.8%), because it contains sentences with frequent near-synonyms, which happen
to be the ones that Xenon chooses by default in the absence of input preferences.
Xenon?s performance is well above baseline, with the exception of the French-to-English
condition on sentence data set 2.
In the English-to-English experiments, there are two reasons to expect Xenon?s
accuracy to be less than 100% even if the input nuances are the nuances of a particular
English near-synonym. The first reason is that there are cases in which two or more
near-synonyms get an equal, maximal score because they do not have nuances that
differentiate them (either they are perfectly interchangeable, or the LKB of NS does not
contain enough knowledge) and the one chosen is not the expected one. The second
reason is that sometimes Xenon does not choose the expected near-synonym even if it is
the only one with maximal weight. This may happen because HALogen makes the final
choice by combining the weight received from the near-synonym choice module with
the probabilities from the language model that is part of HALogen. Frequent words may
have high probabilities in the language model. If the expected near-synonym is very
rare, or maybe was not seen at all by the language model, its probability is very low;
yet it is exactly those cases where a writer chooses a rare near-synonym over a more-
frequent alternative that the choice is the most telling. When combining the weights
with the probabilities, a frequent near-synonym may win even if it has a lower weight
assigned by the near-synonym choice module. In such cases, the default near-synonym
(the most frequent of the cluster) wins. Sometimes such behavior is justified because
there may be other constraints that influence HALogen?s choice.
In the French-to-English experiments, the performance of Xenon is lower than in the
English-to-English experiments. There are two explanations. First, there is some overlap
between the nuances of the French and the English near-synonyms, but less than one
would expect. For example, the English adjective alcoholic is close in nuances to the
French adjective alcoolique, but they have no nuance in common in Xenon?s knowledge
bases simply because of the incompleteness of the explanations given by lexicographers
in the dictionary entries.
The second explanation is related to what is considered the ?correct? choice of near-
synonym. Sometimes more than one translation of a French near-synonym could be
254
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
correct, but in this conservative evaluation, the solution is the near-synonym that was
used in the equivalent English sentence. Therefore, test cases that would be considered
correct by a human judge are harshly penalized. Moreover, the near-synonym choice
module always chooses the same translation for a near-synonym, even if the near-
synonym is translated in Hansard in more than one way, because Xenon does not
consider the context of the near-synonym in the sentence. (The context is taken into
account only when the collocation module is enabled and a preferred collocation is
detected in the sentences.) For example, the French noun erreur is translated in Hansard
sometimes as error, sometimes as mistake. Both have nuances in common with erreur,
but mistake happened to have higher overlap with erreur than error; as a result, the near-
synonym choice module always chooses mistake (except when the collocation module
is enabled and finds a preferred collocation such as administrative error). All the cases
in which error was used as translation of erreur in Hansard are penalized as incorrect
in the evaluation of the near-synonym choice module. A few of these cases could be
indeed incorrect, but probably many of them would be considered correct by a human
judge.
Another way to look at the performance of Xenon is to measure how many times it
makes appropriate choices that cannot be made by HALogen?that is, cases that make
good use of the nuances from the LKB of NS. This excludes the test cases with default
near-synonyms?those in which Xenon makes the right choice simply because of its
language model?and cases of ties in which Xenon cannot make the expected choice.
Accuracies for nondefault cases vary from 84.3% to 100%.
7.5.2 Evaluation of the Near-Synonym Collocation Module. For the evaluation of the
near-synonym collocation module, we collected sentences from the BNC that contain
preferred collocations from the knowledge base of near-synonym collocational behav-
ior. The BNC was preferred over Hansard for these evaluation experiments because it
is a balanced corpus and contains the collocations of interest, whereas Hansard does
not contain some of the collocations and near-synonyms of interest. The sentences were
collected from the first half of the BNC (50 million words). Sentence data sets 3 and 4
contain collocations for the development set of near-synonyms in Figure 17; sentence
data sets 5 and 6 contain collocations for the English near-synonyms in Figure 18.
Sets 3 and 5 include at most two sentences per collocation (the first two sentences from
the corpus, except in cases when the input construction tool failed to produce valid
interlingual representations); sets 4 and 6 include all the sentences with collocations
as they occurred in the fragment of the corpus (except the sentences for which the
input construction tool failed). For example, for set 4 there were initially 527 sentences,
and the input construction tool succeeded on 297 of them. Set 3 was used for develop-
ment?to choose the discount weights (see below)?and the others only for testing. The
architecture of this experiment is the same as that of the English-to-English experiments
(Figure 22), except that in this case it was the near-synonym choice module that was
disabled.
We observe that the sentence data sets may contain collocations for the wrong
senses of some near-synonyms because, as explained in Section 3.4, the near-synonym
collocations knowledge base may contain, for a cluster, collocations for a different sense.
For example, the collocation trains run appears in the cluster flow, gush, pour, run, spout,
spurt, squirt, stream, when it should appear only in another cluster. In this case the near-
synonym run should not be replaced with the metaconcept generic flow v because
it corresponds to a different metaconcept. These sentences should be eliminated from
255
Computational Linguistics Volume 32, Number 2
Table 8
The results of the evaluation of Xenon?s collocations module (boldface indicates best results).
HALogen only (baseline) HALogen + collocations
Sentence No. of Correct Pref. Anti- Correct Pref. Anti-
data set cases NS choice collocs collocs NS choice collocs collocs
Set 3 105 62% 88% 12% 70% 100% 0%
Set 4 297 83% 91% 9% 87% 99% 1%
Set 5 44 59% 80% 20% 75% 100% 0%
Set 6 185 58% 68% 32% 86% 99% 1%
the data sets, but this would involve disambiguation or manual elimination. However,
they do not affect the evaluation results because they are unlikely to produce anti-
collocations. This is because trains run is a frequent bigram, whereas trains flow is not;
Xenon will make the correct choice by default.
Sentence data set 3 was used to choose the best values of the discount weights
Wanti colloc and Wless pref colloc. In fact, the latter could be approximated by the former,
treating less preferred collocations as anti-collocations, because the number of less
preferred collocations is very small in the knowledge base. As the value of the dis-
count weight Wanti colloc increased (from 0.0 and 1.0), the number of anti-collocations
generated decreased; there were no anti-collocations left for Wanti colloc = 0.995.
Table 8 presents the results of the evaluation experiments. These results refer to
the evaluation of Xenon with the near-synonym collocations module enabled and the
near-synonym choice module disabled (lexical nuances are ignored in this experiment).
The baseline used for comparison is obtained by running HALogen only, without any
extension modules (no knowledge of collocations). For each test, the first four columns
contain the number of test cases, the number of near-synonyms correctly chosen by
the baseline system, the number of preferred collocations, and the number of anti-
collocations produced by the baseline system. The remainder of the columns present
results obtained by running Xenon with only the near-synonym collocations module
enabled (i.e., HALogen and the collocations module): the number of near-synonyms
correctly chosen, the number of preferred collocations produced, and the number of
anti-collocations produced. The number of anti-collocations was successfully reduced
to zero, except for sentence sets 4 and 6 where 1% of the anti-collocations remained. The
sixth column (correct choices or accuracy) differs from the seventh column (preferred
collocations) in the following way: The correct choice is the near-synonym used in the
original BNC sentence; sometimes the generated sentence can choose a different near-
synonym that is not the expected one but which participates in a preferred collocation
(this happens when more than one near-synonym from the same cluster collocates
well with the collocate word). For example, both serious mistake and serious blunder are
preferred collocations, while only one of mistake and blunder is the correct choice in any
particular context. The number of correct choices is relevant in this experiment only to
show that the collocations module does not have a negative effect on correctness; it even
increases the accuracy.24
24 The accuracy without ties was used here; therefore the numbers are conservative.
256
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
7.5.3 Evaluation of the Two Modules in Interaction. The interaction between the near-
synonym choice module and the collocations module increases Xenon?s performance.
To prove this, we repeated the experiments of the previous section, but this time with
input preferences (the nuances of the near-synonym from the original sentence). The
architecture of this test is the same as that of the English-to-English experiments in
Section 7.5.1, depicted in Figure 22. Table 9 shows the number of correct near-synonym
choices (and the percent accuracy) for the baseline case (no nuances, no collocation
module; i.e., HALogen by itself), for the collocations module alone (i.e., HALogen and
the collocations module only; this column is also part of Table 8), for the near-synonym
choice module alone (i.e., HALogen and the nuances module only), and for Xenon with
both modules enabled. When both modules are enabled there is a slight increase in
accuracy on sentence data sets 4, 5, and 6; the accuracy on set 3 is the same as using the
near-synonyms module only.
7.6 Summary
This section presented Xenon, an NLG system capable of choosing the near-synonym
that best satisfies a set of input preferences (lexical nuances). The input preferences
could come from an analysis module for a different language; in this case the translation
into English would preserve not only the meaning but also nuances of meaning. The
evaluation of Xenon?s two new modules shows that they behave well, both indepen-
dently and in interaction.
The evaluation showed that we were successful in dealing with lexical nuances in
general. One weak point of the evaluation was the relatively small overlap in coverage
of the French and English knowledge bases. Another bottle-neck was the need for a
language-neutral ontology.
8. Conclusion
We have presented a method for extracting knowledge from dictionaries of near-
synonym discrimination. The method can potentially be applied to any dictionary
Table 9
Correct near-synonym choices for the baseline system (HALogen only), for HALogen with each
module of Xenon separately, and for HALogen with both modules of Xenon (boldface indicates
best results).
HALogen
(baseline) Xenon
Correct NS Correct NS Correct NS
Sentence Number collocations nuances nuances +
data set of cases Correct NS module module collocations
Set 3 105 62% 70% 93% 93%
Set 4 297 83% 87% 95% 95%
Set 5 44 59% 75% 93% 95%
Set 6 185 58% 86% 91% 95%
257
Computational Linguistics Volume 32, Number 2
of near-synonym discrimination for any language for which preprocessing tools,
such as part-of-speech taggers and parsers, are available. We built a new lexical re-
source, a lexical knowledge base of differences among English near-synonyms, by
applying the extraction method to Choose the Right Word. The precision and recall
of the extracted knowledge was estimated to be in the range of 70?80%. If higher
precision and recall are needed for particular applications, a human could vali-
date each extraction step. We enriched the initial lexical knowledge base of near-
synonyms with distinctions extracted from machine-readable dictionaries.
We have presented Xenon, a natural language generation system that uses the
LKB of NS to choose the near-synonym that best matches a set of input preferences.
Xenon extends a previous NLG system with two new modules: a module that chooses
near-synonyms on the basis of their lexical nuances, and a module that chooses near-
synonyms on the basis of their collocations. To evaluate Xenon, we manually built
a small LKB of French near-synonyms. The test set consisted of English and French
sentences that are mutual translations. An interlingual representation (with the near-
synonym replaced by the core denotation of its cluster) was input to Xenon, together
with the nuances of the near-synonym from the French sentence. The generated sen-
tence was considered correct if the chosen English near-synonym was the one from the
original English sentence. We also evaluated the near-synonym collocation module and
the interaction of the two modules.
Short-term future work includes overcoming some limitations and extending the
current work, such as extending the near-synonym representation with other types
of distinctions such as information about more general and more specific words, and
information about special meanings that some words have in particular contexts or
domains (e.g., in a legal context).
Longer-term future work directions include the following:
Analysis of lexical nuances A full-fledged analysis module could be developed.
Sense disambiguation would be required when a near-synonym is a member of more
than one cluster. It is more difficult to model the influence of the context and the
complex interaction of the lexical nuances. Such an analysis module could be used
in an Machine Translation (MT) system that preserves lexical nuances. It could also
be used to determine nuances of text for different purposes. For example, a system
could decide if a text is positive, neutral, or negative in its semantic orientation. Then
Xenon could be used to generate a new text that has the same meaning as the origi-
nal text but a different semantic orientation. This could be useful, for example, in an
application that sends letters to customers: If the initial draft of the text is found to be
too negative, it could be transformed into a more positive text before it is sent to the
customer.
Lexical and conceptual associations The method presented in Section 3 could
be extended to acquire lexical associations (i.e., longer-distance collocations) of near-
synonyms. Words that strongly associate with the near-synonyms can be useful,
especially those that associate with only one of the near-synonyms in the cluster.
These strong associations could provide additional knowledge about nuances of near-
synonyms.
An experiment similar to that presented in Section 3 could look for words that co-
occur in a window of size K > 2 to acquire lexical associations, which would include
the collocations extracted in Section 3.2. Church et al (1991) presented associations for
the near-synonyms ship and boat; they suggest that a lexicographer looking at these
associations can infer that boats are generally smaller than ships because they are found
in rivers and lakes and are used for small jobs (e.g., fishing, police, pleasure), whereas
258
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
ships are found in seas and are used for serious business (e.g., cargo, war). It could be
possible to automatically infer this kind of knowledge or to validate already acquired
knowledge.
Words that do not associate with a near-synonym but associate with all the other
near-synonyms in a cluster can tell us something about its nuances of meaning. For
example, terrible slip is an anti-association, whereas terrible associates with mistake,
blunder, error. This is an indication that slip is a minor error. By further generalization,
the associations could become conceptual associations. This may allow the automatic
learning of denotational distinctions between near-synonyms from free text. The con-
cepts that are common to all the near-synonyms in a cluster could be part of the core
denotation, whereas those that associate only with one near-synonym could be part of
a distinction.
Cross-lingual lexical nuances The method presented in Section 2 could be used to
automatically build a lexical knowledge base of near-synonym differences for other lan-
guages, such as French, for which dictionaries of synonym discriminations are available
(in paper form) along with other resources, such as part-of-speech taggers and parsers.
In order to use the French and the English knowledge bases in the same system, a study
of the cross-lingual lexical nuances will be needed.
Analysis of types of peripheral nuances Linguists and lexicographers have looked
at differences between particular types of near-synonyms. For example, Gao (2001)
studied the semantic distinctions between Chinese physical action verbs; one type of
distinctive peripheral nuance is the manner in which the movement is made for each
verb. This kind of study could help to develop a list of the main types of peripheral
nuances (peripheral concepts). In our work, the form that the peripheral nuances can
take is not restricted, because the list of peripheral nuances is open-ended. However,
it may be possible to keep the form unrestricted but add restrictions for the most
important types of peripheral nuances.
Intelligent thesaurus The acquired lexical knowledge base of near-synonym differ-
ences could be used to develop an intelligent thesaurus that assists a writer not only
with a list of words that are similar to a given word but also with explanations about
the differences in nuances of meaning between the possible choices. The intelligent
thesaurus could order the choices to suit a particular writing context. The knowledge
about the collocational behavior of near-synonyms can be used in determining the
order: Near-synonyms that produce anti-collocations would be ranked lower than near-
synonyms that produce preferred collocations.
Automatic acquisition of near-synonyms This work considered only the near-
synonyms and distinctions that were listed by the lexicographers of CTRW. Other
dictionaries of synonym discrimination may have slightly different views. Merging
clusters from different dictionaries is possible. Also, near-synonym clusters could be
acquired from free text. This would distinguish near-synonyms from the pool of re-
lated words. As mentioned in Section 5.3, Lin et al (2003) acquired words that are
related by contextual similarity, and then filtered out the antonyms. Words that are
related by relations other than near-synonymy could also be filtered out. One way to
do this could be to collect signatures for each potential near-synonym?words that as-
sociate with it in many contexts. For two candidate words, if one signature is contained
in the other, the words are probably in an IS-A relation; if the signatures overlap totally,
it is a true near-synonymy relation; if the signatures overlap partially, it is a different
kind of relation. The acquisition of more near-synonyms, followed by the acquisition of
more distinctions, is needed to increase the coverage of our lexical knowledge base of
near-synonym differences.
259
Computational Linguistics Volume 32, Number 2
Acknowledgments
We thank Irene Langkilde-Geary for making
the input construction tool available and for
her advice on how to connect our preference
satisfaction mechanism to HALogen. We
thank Phil Edmonds for making available
the source code of I-Saurus. We thank
Suzanne Stevenson, Gerald Penn, Ted
Pedersen, and anonymous reviewers for
their feedback on various stages of this work.
We thank Ol?ga Feiguina for being an
enthusiastic research assistant and for
implementing the programs from Section 4.
Our work is financially supported by the
Natural Sciences and Engineering Research
Council of Canada, the University of
Toronto, and the University of Ottawa.
References
Abney, Steven. 1996. Partial parsing via
finite-state cascades. In Proceedings of the
8th European Summer School in Logic,
Language and Information (ESS-LLI?96),
Robust Parsing Workshop, pages 124?131,
Prague, Czech Republic.
Apresjan, J. D., V. V. Botiakova, T. E.
Latiskeva, M. A. Mosiagina, I. V. Polik, V. I.
Rakitina, A. A. Rozenman, and E. E.
Sretenskaia. 1980. Anglo-Russkii
Sinonimicheskii Slovar. Izdatelstvo Russkii
Jazik.
Apresjan, Juri. 2000. Systematic Lexicography.
Translation, Oxford University Press.
Bailly, Rene?, editor. 1973. Dictionnaire des
Synonymes de la Langue Franc?aise. Larousse,
Paris.
Barrie`re, Caroline and Fred Popowich. 1996.
Concept clustering and knowledge
integration from a children?s dictionary. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?96), pages 65?70, Copenhagen,
Denmark.
Be?nac, Henri, editor. 1956. Dictionnaire des
Synonymes. Librarie Hachette, Paris.
Bernard, J. R. L., editor. 1986. The Macquarie
Thesaurus?Companion to The Macquarie
Dictionary. Macquarie Library, Sydney,
Australia.
Caraballo, Sharon. 1999. Automatic
acquisition of a hypernym-labeled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 120?126,
College Park, MD.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st Conference of the North American
Chapter of the Association for Computational
Linguistics and the 6th Conference on Applied
Natural Language Processing (NAACL-ANLP
2000), pages 132?139, Seattle, WA.
Church, Kenneth, William Gale, Patrick
Hanks, and Donald Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik,
editor, Lexical Acquisition: Using On-line
Resources to Build a Lexicon, pages 115?164.
Lawrence Erlbaum.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184?191,
Santa Cruz.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
(EMNLP/VLC-99), pages 100?110,
College Park, MD.
Delbridge, A., J. R. L. Bernard, D. Blair, W. S.
Ramson, and Susan Butler, editors. 1987.
The Macquarie Dictionary. Macquarie
Library, Sydney, Australia.
DiMarco, Chrysanne, Graeme Hirst, and
Manfred Stede. 1993. The semantic and
stylistic differentiation of synonyms and
near-synonyms. In Proceedings of AAAI
Spring Symposium on Building Lexicons for
Machine Translation, pages 114?121,
Stanford, CA.
Dorr, Bonnie J. 1993. The Use of Lexical
Semantics in Interlingual Machine
Translation. The MIT Press, Cambridge,
MA.
Edmonds, Philip. 1999. Semantic
representations of near-synonyms for
automatic lexical choice. Ph.D. thesis,
University of Toronto.
Edmonds, Philip and Graeme Hirst. 2002.
Near-synonymy and lexical choice.
Computational Linguistics, 28(2):105?145.
Gao, Hong. 2001. The Physical Foundation of
the Patterning of Physical Action Verbs. Ph.D.
thesis, Lund University.
Gove, Philip B., editor. 1984. Webster?s New
Dictionary of Synonyms. G.&C. Merriam Co.
Hayakawa, S. I., editor. 1994. Choose the
Right Word. Second Edition, revised
by Eugene Ehrlich. HarperCollins
Publishers.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large corpora. In
Proceedings of the 14th International
Conference on Computational Linguistics
(COLING?92), pages 538?545, Nantes,
France.
260
Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences
Hirst, Graeme. 1995. Near-synonymy and the
structure of lexical knowledge. In Working
Notes, AAAI Symposium on Representation
and Acquisition of Lexical Knowledge:
Polysemy, Ambiguity, and Generativity,
pages 51?56, Stanford University.
Hovy, Eduard. 1990. Pragmatics and
language generation. Artificial Intelligence,
43:153?197.
Ide, Nancy and Jean Ve?ronis. 1994.
Knowledge extraction from
machine-readable dictionaries: An
evaluation. In P. Steffens, editor, Machine
Translation and the Lexicon, pages 19?34,
Springer-Verlag.
Inkpen, Diana. 2003. Building a Lexical
Knowledge Base of Near-Synonym Differences.
Ph.D. thesis, University of Toronto.
Inkpen, Diana Zaiu and Graeme Hirst. 2001.
Building a lexical knowledge base of
near-synonym differences. In Proceedings of
the Workshop on WordNet and Other Lexical
Resources, Second Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2001),
pages 47?52, Pittsburgh, PA.
Inkpen, Diana Zaiu and Graeme Hirst. 2002.
Acquiring collocations for lexical choice
between near-synonyms. In Proceedings of
the Workshop on Unsupervised Lexical
Acquisition, 40th Annual Meeting of the
Association for Computational Linguistics
(ACL 2002), pages 67?76, Philadelphia, PA.
Inkpen, Diana Zaiu and Graeme Hirst. 2003.
Near-synonym choice in natural language
generation. In Proceedings of the
International Conference RANLP-2003
(Recent Advances in Natural Language
Processing), pages 204?211, Borovets,
Bulgaria.
Knight, Kevin and Steve Luk. 1994. Building
a large knowledge base for machine
translation. In Proceedings of the 12th
National Conference on Artificial Intelligence
(AAAI-94), pages 773?778, Seattle, WA.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of the 1st Conference of the North
American Chapter of the Association for
Computational Linguistics and the 6th
Conference on Applied Natural Language
Processing (NAACL-ANLP 2000),
pages 170?177, Seattle, WA.
Langkilde, Irene and Kevin Knight. 1998. The
practical value of N-grams in generation.
In Proceedings of the 9th International
Natural Language Generation Workshop,
pages 248?255, Niagara-on-the-Lake,
Canada.
Langkilde-Geary, Irene. 2002a. An empirical
verification of coverage and correctness for
a general-purpose sentence generator. In
Proceedings of the 12th International Natural
Language Generation Workshop, pages 17?24,
New York, NY.
Langkilde-Geary, Irene. 2002b. A Foundation
for a General-Purpose Natural Language
Generation: Sentence Realization Using
Probabilistic Models of Language. Ph.D.
thesis, University of Southern California.
Lenat, Doug. 1995. Cyc: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of
SIGDOC Conference, pages 24?26, Toronto,
Canada.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and
Ming Zhou. 2003. Identifying synonyms
among distributionally similar words. In
Proceedings of the Eighteenth Joint
International Conference on Artificial
Intelligence (IJCAI-03), pages 1492?1493,
Acapulco, Mexico.
Lindberg, Donald A. B., Betsy L. Humphreys,
and Alexa T. McCray. 1993. The unified
medical language system. Methods of
Information in Medicine, 32(4):281?289.
Mahesh, Kavi and Sergei Nirenburg. 1995. A
situated ontology for practical NLP. In
Proceedings of Workshop on Basic Ontological
Issues in Knowledge Sharing, International
Joint Conference on Artificial Intelligence
(IJCAI-95), Montreal, Canada.
Manning, Christopher and Hinrich Schu?tze.
1999. Foundations of Statistical Natural
Language Processing. The MIT Press,
Cambridge, MA.
McKeown, Kathleen and Dragomir Radev.
2000. Collocations. In Robert Dale,
Hermann Moisl, and Harold Somers,
editors, Handbook of Natural Language
Processing, pages 507?524. Marcel Dekker.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the
ACM, 38(11):39?41.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. Technical report
RC22176, IBM Research Division, Thomas
J. Watson Research Center.
Pearce, Darren. 2001. Synonymy in
collocation extraction. In Proceedings of the
Workshop on WordNet and Other Lexical
Resources, Second meeting of the North
261
Computational Linguistics Volume 32, Number 2
American Chapter of the Association for
Computational Linguistics, pages 41?46,
Pittsburgh, PA.
Pedersen, Ted and Satanjeev Banerjee. 2003.
The design, implementation, and use of
the ngram statistical package. In
Proceedings of the 4th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing 2003),
pages 370?381, Mexico City, Mexico.
Ploux, Sabine and Hyungsuk Ji. 2003. A
model for matching semantic maps
between languages (French/English,
English/French). Computational Linguistics,
29(2):155?178.
Resnik, Philip. 1999. Mining the web for
bilingual text. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, pages 527?534,
College Park, MD.
Resnik, Philip, Mari Broman Olsen, and
Mona Diab. 1999. The bible as a parallel
corpus: Annotating the ?Book of 2000
Tongues?. Computers and the Humanities,
33(1?2):129?153.
Richardson, Stephen, William Dolan, and
Lucy Vanderwende. 1998. MindNet:
Acquiring and structuring semantic
information from text. In Proceedings of the
36th Annual Meeting of the Association for
Computational Linguistics joint with 17th
International Conference on Computational
Linguistics (ACL-COLING?98),
pages 1098?1102, Montreal, Quebec,
Canada.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
the 16th National Conference on Artificial
Intelligence (AAAI-99), pages 474?479,
Orlando, FL.
Roget, Peter Mark, editor. 1852. Roget?s
Thesaurus of English Words and Phrases.
Longman Group Ltd., Harlow, Essex,
England.
Room, Adrian, editor. 1981. Room?s Dictionary
of Distinguishables. Routhledge & Kegan
Paul, Boston.
Sim, Julius and Chris C. Wright. 2005. The
kappa statistic in reliability studies: Use,
interpretation, and sample size
requirements. Physical Therapy,
85(3):257?268.
Stone, Philip J., Dexter C. Dunphy,
Marshall S. Smith, Daniel M. Ogilvie, and
associates. 1966. The General Inquirer: A
Computer Approach to Content Analysis. The
MIT Press, Cambridge, MA.
Turney, Peter. 2001. Mining the web for
synonyms: PMI-IR versus LSA on
TOEFL. In Proceedings of the Twelfth
European Conference on Machine Learning
(ECML 2001), pages 491?502, Freiburg,
Germany.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge,
MA.
262
                                                               Edmonton, May-June 2003
                                                                     Tutorials , pg. 1
                                                              Proceedings of HLT-NAACL
Acquiring Collocations for Lexical Choice between Near-Synonyms
Diana Zaiu Inkpen and Graeme Hirst
Department of Computer Science
University of Toronto
{dianaz,gh}@cs.toronto.edu
Abstract
We extend a lexical knowledge-base of
near-synonym differences with knowl-
edge about their collocational behaviour.
This type of knowledge is useful in the
process of lexical choice between near-
synonyms. We acquire collocations for
the near-synonyms of interest from a cor-
pus (only collocations with the appropri-
ate sense and part-of-speech). For each
word that collocates with a near-synonym
we use a differential test to learn whether
the word forms a less-preferred collo-
cation or an anti-collocation with other
near-synonyms in the same cluster. For
this task we use a much larger corpus
(the Web). We also look at associations
(longer-distance co-occurrences) as a pos-
sible source of learning more about nu-
ances that the near-synonyms may carry.
1 Introduction
Edmonds and Hirst (2002 to appear) developed a
lexical choice process for natural language gener-
ation (NLG) or machine translation (MT) that can
decide which near-synonyms are most appropriate
in a particular situation. The lexical choice process
has to choose between clusters of near-synonyms (to
convey the basic meaning), and then to choose be-
tween the near-synonyms in each cluster. To group
near-synonyms in clusters we trust lexicographers?
judgment in dictionaries of synonym differences.
For example task, job, duty, assignment, chore, stint,
hitch all refer to a one-time piece of work, but which
one to choose depends on the duration of the work,
the commitment and the effort involved, etc.
In order to convey desired nuances of mean-
ing and to avoid unwanted implications, knowledge
about the differences among near-synonyms is nec-
essary. I-Saurus, a prototype implementation of (Ed-
monds and Hirst, 2002 to appear), uses a small num-
ber of hand-built clusters of near-synonyms.
Our goal is to automatically acquire knowledge
about distinctions among near-synonyms from a
dictionary of synonym differences and from other
sources such as free text, in order to build a new lex-
ical resource, which can be used in lexical choice.
Preliminary results on automatically acquiring a lex-
ical knowledge-base of near-synonym differences
were presented in (Inkpen and Hirst, 2001). We ac-
quired denotational (implications, suggestions, de-
notations), attitudinal (favorable, neutral, or pejo-
rative), and stylistic distinctions from Choose the
Right Word (Hayakawa, 1994) (hereafter CTRW)1.
We used an unsupervised decision-list algorithm to
learn all the words used to express distinctions and
then applied information extraction techniques.
Another type of knowledge that can help in the
process of choosing between near-synonyms is col-
locational behaviour, because one must not choose
a near-synonym that does not collocate well with
the other word choices for the sentence. I-Saurus
does not include such knowledge. The focus of
the work we present in this paper is to add knowl-
edge about collocational behaviour to our lexical
knowledge-base of near-synonym differences. The
lexical choice process implemented in I-Saurus gen-
1We are grateful to HarperCollins Publishers, Inc. for per-
mission to use CTRW in this project.
                     July 2002, pp. 67-76.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
erates all the possible sentences with a given mean-
ing, and ranks them according to the degree to which
they satisfy a set of preferences given as input (these
are the denotational, attitudinal, and stylistic nu-
ances mentioned above). We can refine the rank-
ing so that it favors good collocations, and penal-
izes sentences containing words that do not collocate
well.
We acquire collocates of all near-synonyms in
CTRW from free text. We combine several statis-
tical measures, unlike other researchers who rely on
only one measure to rank collocations.
Then we acquire knowledge about less-preferred
collocations and anti-collocations2. For exam-
ple daunting task is a preferred collocation, while
daunting job is less preferred (it should not be used
in lexical choice unless there is no better alternative),
and daunting duty is an anti-collocation (it must not
be used in lexical choice). Like Church et al(1991),
we use the t-test and mutual information. Unlike
them we use the Web as a corpus for this task, we
distinguish three different types of collocations, and
we apply sense disambiguation to collocations.
Collocations are defined in different ways by dif-
ferent researchers. For us collocations consist of
consecutive words that appear together much more
often than by chance. We also include words sep-
arated by a few non-content words (short-distance
co-occurrence in the same sentence).
We are interested in collocations to be used in lex-
ical choice. Therefore we need to extract lexical
collocations (between open-class words), not gram-
matical collocations (which could contain closed-
class words, for example put on). For now, we con-
sider only two-word fixed collocations. In future
work we will consider longer and more flexible col-
locations.
We are also interested in acquiring words that
strongly associate with our near-synonyms, espe-
cially words that associate with only one of the near-
synonyms in the cluster. Using these strong asso-
ciations, we plan to learn about nuances of near-
synonyms in order to validate and extend our lexical
knowledge-base of near-synonym differences.
In our first experiment, described in sections 2
and 3 (with results in section 4, and evaluation in
2This term was introduced by Pearce (2001).
section 5), we acquire knowledge about the collo-
cational behaviour of the near-synonyms. In step 1
(section 2), we acquire potential collocations from
the British National Corpus (BNC)3, combining sev-
eral measures. In section 3 we present: (step2) se-
lect collocations for the near-synonyms in CTRW;
(step 3) filter out wrongly selected collocations us-
ing mutual information on the Web; (step 4) for each
cluster we compose new collocations by combin-
ing the collocate of one near-synonym with the the
other near-synonym, and we apply the differential t-
test to classify them into preferred collocations, less-
preferred collocations, and anti-collocations. Sec-
tion 6 sketches our second experiment, involving
word associations. The last two sections present re-
lated work, and conclusions and future work.
2 Extracting collocations from free text
For the first experiment we acquired collocations for
near-synonyms from a corpus. We experimented
with 100 million words from the Wall Street Journal
(WSJ). Some of our near-synonyms appear very few
times (10.64% appear fewer than 5 times) and 6.87%
of them do not appear at all in WSJ (due to its busi-
ness domain). Therefore we need a more general
corpus. We used the 100 million word BNC. Only
2.61% of our near-synonyms do not occur; and only
2.63% occur between 1 and 5 times.
Many of the near-synonyms appear in more than
one cluster, with different parts-of-speech. We ex-
perimented on extracting collocations from raw text,
but we decided to use a part-of-speech tagged corpus
because we need to extract only collocations rele-
vant for each cluster of near-synonyms. The BNC is
a good choice of corpus for us because it has been
tagged (automatically by the CLAWS tagger).
We preprocessed the BNC by removing all words
tagged as closed-class. To reduce computation time,
we also removed words that are not useful for our
purposes, such as proper names (tagged NP0). If we
keep the proper names, they are likely to be among
the highest-ranked collocations.
There are many statistical methods that can be
used to identify collocations. Four general meth-
ods are presented by Manning and Schu?tze (1999).
The first one, based on frequency of co-occurrence,
3http://www.hcu.ox.ac.uk/BNC/
does not consider the length of the corpus. Part-of-
speech filtering is needed to obtain useful colloca-
tions. The second method considers the means and
variance of the distance between two words, and can
compute flexible collocations (Smadja, 1993). The
third method is hypothesis testing, which uses sta-
tistical tests to decide if the words occur together
with probability higher than chance (it tests whether
we can reject the null hypothesis that the two words
occurred together by chance). The fourth method
is (pointwise) mutual information, an information-
theoretical measure.
We use Ted Pedersen?s Bigram Statistics Pack-
age4. BSP is a suite of programs to aid in analyz-
ing bigrams in a corpus (newer versions allow N-
grams). The package can compute bigram frequen-
cies and various statistics to measure the degree of
association between two words: mutual information
(MI), Dice, chi-square (?2), log-likelihood (LL), and
Fisher?s exact test.
The BSP tools count for each bigram in a corpus
how many times it occurs, and how many times the
first word occurs.
We briefly describe the methods we use in our ex-
periments, for the two-word case. Each bigram xy
can be viewed as having two features represented by
the binary variables X and Y . The joint frequency
distribution of X and Y is described in a contingency
table. Table 1 shows an example for the bigram
daunting task. n11 is the number of times the bi-
gram xy occurs; n12 is the number of times x occurs
in bigrams at the left of words other than y; n21 is
the number of times y occurs in bigrams after words
other that x; and n22 is the number of bigrams con-
taining neither x nor y. In Table 1 the variable X
denotes the presence or absence of daunting in the
first position of a bigram, and Y denotes the pres-
ence or absence of task in the second position of a
bigram. The marginal distributions of X and Y are
the row and column totals obtained by summing the
joint frequencies: n+1 = n11 + n21, n1+ = n11 + n12,
and n++ is the total number of bigrams.
The BSP tool counts for each bigram in a corpus
how many times it occurs, how many times the first
word occurs at the left of any bigram (n+1), and how
many times the second words occurs at the right of
4http://www.d.umn.edu/?tpederse/code.html
y ?y
x n11 = 66 n12 = 54 n1+ = 120
?x n21 = 4628 n22 = 15808937 n2+ = 15813565
n+1 = 4694 n+2 = 15808991 n++ = 15813685
Table 1: Contingency table for daunting task
(x = daunting, y = task).
any bigram (n1+).
Mutual information, I(x;y), compares the prob-
ability of observing words x and word y together (the
joint probability) with the probabilities of observing
x and y independently (the probability of occurring
together by chance) (Church and Hanks, 1991).
I(x;y) = log2
P(x,y)
P(x)P(y)
The probabilities can be approximated by: P(x) =
n+1/n++, P(y) = n1+/n++, P(x,y) = n11/n++.
Therefore:
I(x;y) = log2
n++n11
n+1n1+
The Dice coefficient is related to mutual informa-
tion and it is calculated as:
Dice(x,y) =
2P(x,y)
P(x)+ P(y)
=
2n11
n+1 + n1+
The next methods fall under hypothesis test-
ing methods. Pearson?s Chi-square and Log-
likelihood ratios measure the divergence of ob-
served (ni j) and expected (mi j) sample counts (i =
1,2, j = 1,2). The expected values are for the model
that assumes independence (assumes that the null
hypothesis is true). For each cell in the contingency
table, the expected counts are: mi j = ni+n+ jn++ . The
measures are calculated as (Pedersen, 1996):
?2 = ?i, j
(ni j?mi j)2
mi j
LL = 2 ?i, j
log2 n2i j
mi j
Log-likelihood ratios (Dunning, 1993) are more
appropriate for sparse data than chi-square.
Fisher?s exact test is a significance test that is
considered to be more appropriate for sparse and
skewed samples of data than statistics such as the
log-likelihood ratio or Pearson?s Chi-Square test
(Pedersen, 1996). Fisher?s exact test is computed
by fixing the marginal totals of a contingency table
and then determining the probability of each of the
possible tables that could result in those marginal to-
tals. Therefore it is computationally expensive. The
formula is:
P =
n1+!n2+!n+1!n+2!
n++!n11!n12!n21!n22!
Because these five measures rank collocations in
different ways (as the results in the Appendix will
show), and have different advantages and draw-
backs, we decided to combine them in choosing col-
locations. We choose as potential collocations for
each near-synonym a collocation that is selected by
at least two of the measures. For each measure
we need to choose a threshold T , and consider as
selected collocations only the T highest-ranked bi-
grams (where T can differ for each measure). By
choosing higher thresholds we increase the precision
(reduce the chance of accepting wrong collocations).
By choosing lower thresholds we get better recall.
If we opt for low recall we may not get many col-
locations for some of the near-synonyms. Because
there is no principled way of choosing these thresh-
olds, we prefer to choose lower thresholds (the first
200,000 collocations selected by each measure, ex-
cept Fisher?s measure for which we take all 435,000
collocations ranked 1) and to filter out later (in step
2) the bigrams that are not true collocations, using
mutual information on the Web.
3 Differential collocations
For each cluster of near-synonyms, we now have
the words that occur in preferred collocations with
each near-synonym. We need to check whether these
words collocate with the other near-synonyms in the
same cluster. For example, if daunting task is a pre-
ferred collocation, we check whether daunting col-
locates with the other near-synonyms of task.
We use the Web as a corpus for differential col-
locations. We don?t use the BNC corpus to rank
less-preferred and anti-collocations, because their
absence in BNC may be due to chance. We can as-
sume that the Web (the portion retrieved by search
engines) is big enough that a negative result can be
trusted.
We use an interface to AltaVista search engine to
count how often a collocation is found. (See Table 2
for an example.5) A low number of co-occurrences
indicates a less-preferred collocation. But we also
need to consider how frequent the two words in the
collocation are. We use the differential t-test to find
collocations that best distinguish between two near-
synonyms (Church et al, 1991), but we use the Web
as a corpus. Here we don?t have part-of-speech tags
but this is not a problem because in the previous
step we selected collocations with the right part-of-
speech for the near-synonym. We approximate the
number of occurrences of a word on the Web with
the number of documents containing the word.
The t-test can also be used in the hypothesis test-
ing method to rank collocations. It looks at the mean
and variance of a sample of measurements, where
the null hypothesis is that the sample was drawn
from a normal distribution with mean ?. It measures
the difference between observed (x?) and expected
means, scaled by the variance of the data (s2), which
in turn is scaled by the sample size (N).
t =
x???
?
s2
N
We are interested in the Differential t-test, which
can be used for hypothesis testing of differences. It
compares the means of two normal populations:
t =
x?1? x?2
?
s21
N +
s22
N
Here the null hypothesis is that the average differ-
ence is ? = 0.Therefore x?? ? = ? = x?1? x?2. In the
denominator we add the variances of the two popu-
lations.
If the collocations of interest are xw and yw (or
similarly wx and wy), then we have the approxima-
tions x?1 = s21 = P(x,w) and x?2 = s22 = P(y,w); there-
fore:
t =
P(x,w)?P(y,w)
?
P(x,w)+P(y,w)
n++
=
nxw?nyw
?
nxw + nyw
If w is a word that collocates with one of the near-
synonyms in a cluster, and x is each of the near-
5The search was done on 13 March 2002.
synonyms, we can approximate the mutual informa-
tion relative to w:
P(w,x)
P(x)
=
nwx
nx
where P(w) was dropped because it is the same for
various x (we cannot compute if we keep it, because
we don?t know the total number of bigrams on the
Web).
We use this measure to eliminate collocations
wrongly selected in step 1. We eliminate those with
mutual information lower that a threshold. We de-
scribe the way we chose this threshold (Tmi) in sec-
tion 5.
We are careful not to consider collocations of a
near-synonym with a wrong part-of-speech (our col-
locations are tagged). But there is also the case when
a near-synonym has more than one major sense. In
this case we are likely to retrieve collocations for
senses other than the one required in the cluster. For
example, for the cluster job, task, duty, etc., the col-
location import/N duty/N is likely to be for a differ-
ent sense of duty (the customs sense). Our way of
dealing with this is to disambiguate the sense used
in each collocations (we assume one sense per collo-
cation), by using a simple Lesk-style method (Lesk,
1986). For each collocation, we retrieve instances in
the corpus, and collect the content words surround-
ing the collocations. This set of words is then in-
tersected with the context of the near-synonym in
CTRW (that is the whole entry). If the intersection
is not empty, it is likely that the collocation and the
entry use the near-synonym in the same sense. If the
intersection is empty, we don?t keep the collocation.
In step 3, we group the collocations of each near-
synonym with a given collocate in three classes,
based on the t-test values of pairwise collocations.
We compute the t-test between each collocation and
the collocation with maximum frequency, and the
t-test between each collocation and the collocation
with minimum frequency (see Table 2 for an exam-
ple). Then, we need to determine a set of thresholds
that classify the collocations in the three groups:
preferred collocations, less preferred collocations,
and anti-collocations. The procedure we use in this
step is detailed in section 5.
x Hits MI t max t min
task 63573 0.011662 - 252.07
job 485 0.000022 249.19 22.02
assignment 297 0.000120 250.30 17.23
chore 96 0.151899 251.50 9.80
duty 23 0.000022 251.93 4.80
stint 0 0 252.07 -
hitch 0 0 252.07 -
Table 2: The second column shows the number of
hits for the collocation daunting x, where x is one
of the near-synonyms in the first column. The third
column shows the mutual information, the fourth
column, the differential t-test between the colloca-
tion with maximum frequency (daunting task) and
daunting x, and the last column, the t-test between
daunting x and the collocation with minimum fre-
quency (daunting hitch).
4 Results
We obtained 15,813,685 bigrams. From these,
1,350,398 were distinct and occurred at least 4
times.
We present some of the top-ranked collocations
for each measure in the Appendix. We present the
rank given by each measure (1 is the highest), the
value of the measure, the frequency of the colloca-
tion, and the frequencies of the words in the collo-
cation.
We selected collocations for all 914 clusters in
CTRW (5419 near-synonyms in total). An example
of collocations extracted for the near-synonym task
is:
daunting/A task/N
-- MI 24887 10.8556
-- LL 5998 907.96
-- X2 16341 122196.8257
-- Dice 2766 0.0274
repetitive/A task/N
-- MI 64110 6.7756
-- X2 330563 430.4004
where the numbers are, in order, the rank given by
the measure and the value of the measure.
We filtered out the collocations using MI on the
Web (step 2), and then we applied the differential
t-test (step 3). Table 2 shows the values of MI
between daunting x and x, where x is one of the
near-synonyms of task. It also shows t-test val-
Near-synonyms daunting particular tough
task
? ? ?
job ? ? ?
assignment ?
? ?
chore ? ? ?
duty ?
?
?
stint ? ? ?
hitch ? ? ?
Table 3: Example of results for collocations.
ues between (some) pairs of collocations. Table 3
presents an example of results for differential col-
locations, where
?
marks preferred collocations, ?
marks less-preferred collocations, and ? marks anti-
collocations.
Before proceeding with step 3, we filtered out the
collocations in which the near-synonym is used in
a different sense, using the Lesk method explained
above. For example, suspended/V duty/N is kept
while customs/N duty/N and import/N duty/N are re-
jected. The disambiguation part of our system was
run only for a subset of CTRW, because we have yet
to evaluate it. The other parts of our system were run
for the whole CTRW. Their evaluation is described
in the next section.
5 Evaluation
Our evaluation has two purposes: to get a quanti-
tative measure of the quality of our results, and to
choose thresholds in a principled way.
As described in the previous sections, in step 1
we selected potential collocations from BNC (the
ones selected by at least two of the five measures).
Then, we selected collocations for each of the near-
synonyms in CTRW (step 2). We need to evaluate
the MI filter (step 3), which filters out the bigrams
that are not true collocations, based on their mutual
information computed on the Web. We also need to
evaluate step 4, the three way classification based on
the differential t-test on the Web.
For evaluation purposes we selected three clusters
from CTRW, with a total of 24 near-synonyms. For
these, we obtained 916 collocations from BNC ac-
cording to the method described in section 2.
We had two human judges reviewing these collo-
cations to determine which of them are true colloca-
tions and which are not. We presented the colloca-
tions to the judges in random order, and each collo-
cation was presented twice. The first judge was con-
sistent (judged a collocation in the same way both
times it appeared) in 90.4% of the cases. The second
judge was consistent in 88% of the cases. The agree-
ment between the two judges was 67.5% (computed
in a strict way, that is we considered agreement only
when the two judges had the same opinion including
the cases when they were not consistent). The con-
sistency and agreement figures show how difficult
the task is for humans.
We used the data annotated by the two judges to
build a standard solution, so we can evaluate the
results of our MI filter. In the standard solution
a bigram was considered a true collocation if both
judges considered it so. We used the standard solu-
tion to evaluate the results of the filtering, for various
values of the threshold Tmi. That is, if a bigram had
the value of MI on the Web lower than a threshold
Tmi, it was filtered out. We choose the value of Tmi so
that the accuracy of our filtering program is the high-
est. By accuracy we mean the number of true collo-
cations (as given by the standard solution) identified
by our program over the total number of bigrams we
used in the evaluation. The best accuracy was 70.7%
for Tmi = 0.0017. We used this value of the threshold
when running our programs for all CTRW.
As a result of this first part of the evaluation, we
can say that after filtering collocations based on MI
on the Web, approximately 70.7% of the remaining
bigrams are true collocation. This value is not ab-
solute, because we used a sample of the data for the
evaluation. The 70.7% accuracy is much better than
a baseline (approximately 50% for random choice).
Table 4 summarizes our evaluation results.
Next, we proceeded with evaluating the differ-
ential t-test three-way classifier. For each cluster,
for each collocation, new collocations were formed
from the collocate and all the near-synonyms in the
cluster. In order to learn the classifier, and to evalu-
ate its results, we had the two judges manually clas-
sify a sample data into preferred collocations, less-
preferred collocations, and anti-collocations. We
used 2838 collocations obtained for the same three
clusters from 401 collocations (out of the initial 916)
that remained after filtering. We built a standard so-
lution for this task, based on the classifications of
Step Baseline Our system
Filter (MI on the Web) 50% 70.7%
Dif. t-test classifier 71.4% 84.1%
Table 4: Accuracy of our main steps.
both judges. When the judges agreed, the class was
clear. When they did not agree, we designed sim-
ple rules, such as: when one judge chose the class
preferred collocation, and the other judge chose the
class anti-collocation, the class in the solution was
less-preferred collocation. The agreement between
judges was 80%; therefore we are confident that the
quality of our standard solution is high. We used
this standard solution as training data to learn a de-
cision tree6 for our three-way classifier. The fea-
tures in the decision tree are the t-test between each
collocation and the collocation from the same group
that has maximum frequency on the Web, and the
t-test between the current collocation and the col-
location that has minimum frequency (as presented
in Table 2). We could have set aside a part of the
training data as a test set. Instead, we did 10-fold
cross validation to quantify the accuracy on unseen
data. The accuracy on the test set was 84.1% (com-
pared with a baseline that chooses the most frequent
class, anti-collocations, and achieves an accuracy of
71.4%). We also experimented with including MI
as a feature in the decision tree, and with manually
choosing thresholds (without a decision tree) for the
three-way classification, but the accuracy was lower
than 84.1%.
The three-way classifier can fix some of the mis-
takes of the MI filter. If a wrong collocation re-
mained after the MI filter, the classifier can classify
it in the anti-collocations class.
We can conclude that the collocational knowledge
we acquired has acceptable quality.
6 Word Association
We performed a second experiment, where we
looked for long distance co-occurrences (words that
co-occur in a window of size K). We call these as-
sociations, and they include the lexical collocations
we extracted in section 2.
6We used C4.5, http://www.cse.unsw.edu.au/?quinlan
We use BSP with the option of looking for bi-
grams in a window larger than 2. For example
if the window size is 3, and the text is vaccine/N
cure/V available/A, the extracted bigrams are vac-
cine/N cure/V, cure/V available/A, and vaccine/N
available/A. We would like to choose a large (4?
15) window size; the only problem is the increase
in computation time. We look for associations of a
word in the paragraph, not only in the sentence. Be-
cause we look for bigrams, we may get associations
that occur to the left or to the right of the word. This
is an indication of strong association.
We obtained associations similar to those pre-
sented by Church et al(1991) for the near-synonyms
ship and boat. Church et al suggest that a lexicog-
rapher looking at these associations can infer that a
boat is generally smaller than a ship, because they
are found in rivers and lakes, while the ships are
found in seas. Also, boats are used for small jobs
(e.g., fishing, police, pleasure), whereas ships are
used for serious business (e.g., cargo, war). Our in-
tention is to use the associations to automatically in-
fer this kind of knowledge and to validate acquired
knowledge.
For our purpose we need only very strong associ-
ations, and we don?t want words that associate with
all near-synonyms in a cluster. Therefore we test for
anti-associations using the same method we used in
section 3, with the difference that the query asked to
AltaVista is: x NEAR y (where x and y are the words
of interest).
Words that don?t associate with a near-synonym
but associate with all the other near-synonyms in
a cluster can tell us something about its nuances
of meaning. For example terrible slip is an anti-
association, while terrible associates with mistake,
blunder, error. This is an indication that slip is a
minor error.
Table 5 presents some preliminary results we
obtained with K = 4 (on half the BNC and then
on the Web), for the differential associations of
boat (where ? marks preferred associations, ?
marks less-preferred associations, and ? marks anti-
associations). We used the same thresholds as for
our experiment with collocations.
Near-synonyms fishing club rowing
boat
? ? ?
vessel
?
? ?
craft ? ? ?
ship ? ? ?
Table 5: Example of results for associations.
7 Related work
There has been a lot of work done in extracting col-
locations for different applications. We have already
mentioned some of the most important contributors.
Like Church et al(1991), we use the t-test and
mutual information, but unlike them we use the Web
as a corpus for this task (and a modified form of
mutual information), and we distinguish three types
of collocations (preferred, less-preferred, and anti-
collocations).
We are concerned with extracting collocations for
use in lexical choice. There is a lot of work on
using collocations in NLG (but not in the lexical
choice sub-component). There are two typical ap-
proaches: the use of phrasal templates in the form
of canned phrases, and the use of automatically ex-
tracted collocations for unification-based generation
(McKeown and Radev, 2000).
Statistical NLG systems (such as Nitrogen
(Langkilde and Knight, 1998)) make good use of the
most frequent words and their collocations. But such
a system cannot choose a less-frequent synonym that
may be more appropriate for conveying desired nu-
ances of meaning, if the synonym is not a frequent
word.
Finally, there is work related to ours from the
point of view of the synonymy relation.
Turney (2001) used mutual information to detect
the best answer to questions about synonyms from
Test of English as a Foreign Language (TOEFL) and
English as a Second Language (ESL). Given a prob-
lem word (with or without context), and four alter-
native words, the question is to choose the alterna-
tive most similar in meaning with the problem word.
His work is based on the assumption that two syn-
onyms are likely to occur in the same document (on
the Web). This can be true if the author needs to
avoid repeating the same word, but not true when
the synonym is of secondary importance in a text.
The alternative that has the highest PMI-IR (point-
wise mutual information for information retrieval)
with the problem word is selected as the answer. We
used the same measure in section 3 ? the mutual
information between a collocation and a collocate
that has the potential to discriminate between near-
synonyms. Both works use the Web as a corpus, and
a search engine to estimate the mutual information
scores.
Pearce (2001) improves the quality of retrieved
collocations by using synonyms from WordNet
(Pearce, 2001). A pair of words is considered a
collocation if one of the words significantly prefers
only one (or several) of the synonyms of the other
word. For example, emotional baggage is a good
collocation because baggage and luggage are in the
same synset and ?emotional luggage is not a col-
location. As in our work, three types of colloca-
tions are distinguished: words that collocate well;
words that tend to not occur together, but if they
do the reading is acceptable; and words that must
not be used together because the reading will be un-
natural (anti-collocations). In a similar manner with
(Pearce, 2001), in section 3, we don?t record collo-
cations in our lexical knowledge-base if they don?t
help discriminate between near-synonyms. A differ-
ence is that we use more than frequency counts to
classify collocations (we use a combination of t-test
and MI).
Our evaluation was partly inspired by Evert and
Krenn (2001). They collect collocations of the form
noun-adjective and verb-prepositional phrase. They
build a solution using two human judges, and use
the solution to decide what is the best threshold for
taking the N highest-ranked pairs as true colloca-
tions. In their experiment MI behaves worse that
other measures (LL, t-test), but in our experiment
MI on the Web achieves good results.
8 Conclusions and Future Work
We presented an unsupervised method to acquire
knowledge about the collocational behaviour of
near-synonyms.
Our future work includes improving the way we
combine the five measures for ranking collocations,
maybe by giving more weight to the collocations se-
lected by the log-likelihood ratio. We also plan to
experiment more with disambiguating the senses of
the words in a collocation.
Our long-term goal is to acquire knowledge about
near-synonyms from corpora and other sources, by
bootstrapping with our initial lexical knowledge-
base of near-synonym differences. This includes
validating the knowledge already asserted and learn-
ing more distinctions.
Acknowledgments
We thank Gerald Penn, Olga Vechtomova, and three anonymous
reviewers for their helpful comments on previous drafts of this
paper. We thank Eric Joanis and Tristan Miller for helping with
the judging task. Our work is financially supported by the Nat-
ural Sciences and Engineering Research Council of Canada and
the University of Toronto.
Appendix
The first 10 collocations selected by each mea-
sure are presented below. Note that some of
the measures rank many collocations equally at
rank 1: MI 358 collocations; LL one collocation;
?2 828 collocations; Dice 828 collocations; and
Fisher 435,000 collocations (when the measure is
computed with a precision of 10 digits ? higher
precision is recommended, but the computation
time becomes a problem). The rest of the columns
are: the rank assigned by the measure, the value
of the measure, the frequency of the collocation in
BNC, the frequency of the first word in the first
position in bigrams, and the frequency of the second
word in the second position in bigrams.
Some of the collocations ranked 1 by MI:
source-level/A debugger/N 1 21.9147 4 4 4
prosciutto/N crudo/N 1 21.9147 4 4 4
rumpy/A pumpy/A 1 21.9147 4 4 4
thrushes/N blackbirds/N 1 21.9147 4 4 4
clickity/N clickity/N 1 21.9147 4 4 4
bldsc/N microfilming/V 1 21.9147 4 4 4
chi-square/A variate/N 1 21.9147 4 4 4
long-period/A comets/N 1 21.9147 4 4 4
tranquillizers/N sedatives/N 1 21.9147 4 4 4
one-page/A synopsis/N 1 21.9147 4 4 4
First 10 collocations selected by LL:
prime/A minister/N 1 123548 9464 11223 18825
see/V p./N 2 83195 8693 78213 10640
read/V studio/N 3 67537 5020 14172 5895
ref/N no/N 4 62486 3630 3651 4806
video-taped/A report/N 5 52952 3765 3765 15886
secretary/N state/N 6 51277 5016 10187 25912
date/N award/N 7 48794 3627 8826 5614
hon./A friend/N 8 47821 4094 10345 10566
soviet/A union/N 9 44797 3894 8876 12538
report/N follows/V 10 44785 3776 16463 6056
Some of the collocations ranked 1 by ?2:
lymphokine/V activated/A 1 15813684 5 5 5
config/N sys/N 1 15813684 4 4 4
levator/N depressor/N 1 15813684 5 5 5
nobile/N officium/N 1 15813684 11 11 11
line-printer/N dot-matrix/A 1 15813684 4 4 4
dermatitis/N herpetiformis/N 1 15813684 9 9 9
self-induced/A vomiting/N 1 15813684 5 5 5
horoscopic/A astrology/N 1 15813684 5 5 5
mumbo/N jumbo/N 1 15813684 12 12 12
long-period/A comets/N 1 15813684 4 4 4
Some of the collocations ranked 1 by Dice:
clarinets/N bassoons/N 1 1.00 5 5 5
email/N footy/N 1 1.00 4 4 4
tweet/V tweet/V 1 1.00 5 5 5
garage/parking/N vehicular/A 1 1.00 4 4 4
growing/N coca/N 1 1.00 5 5 5
movers/N seconders/N 1 1.00 5 5 5
elliptic/A integrals/N 1 1.00 8 8 8
viscose/N rayon/N 1 1.00 15 15 15
cause-effect/A inversions/N 1 1.00 5 5 5
first-come/A first-served/A 1 1.00 6 6 6
Some of the collocations ranked 1 by Fisher:
roman/A artefacts/N 1 1.00 4 3148 108
qualitative/A identity/N 1 1.00 16 336 1932
literacy/N education/N 1 1.00 9 252 20350
disability/N pension/N 1 1.00 6 470 2555
units/N transfused/V 1 1.00 5 2452 12
extension/N exceed/V 1 1.00 9 1177 212
smashed/V smithereens/N 1 1.00 5 194 9
climbing/N frames/N 1 1.00 5 171 275
inclination/N go/V 1 1.00 10 53 51663
trading/N connections/N 1 1.00 6 2162 736
References
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analy-
sis. In Uri Zernik, editor, Lexical Acquisition: Using
On-line Resources to Build a Lexicon, pages 115?164.
Lawrence Erlbaum.
Ted Dunning. 1993. Accurate methods for statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
Philip Edmonds and Graeme Hirst. 2002 (to appear).
Near-synonymy and lexical choice. Computational
Linguistics, 28(2).
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting of
the of the Association for Computational Linguistics
(ACL?2001), Toulouse, France.
S. I. Hayakawa. 1994. Choose the Right Word. Harper-
Collins Publishers.
Diana Zaiu Inkpen and Graeme Hirst. 2001. Build-
ing a lexical knowledge-base of near-synonym differ-
ences. In Proceedings of the Workshop on WordNet
and Other Lexical Resources, Second Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?2001), Pittsburgh.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of N-grams in generation. In Proceedings of
the International Natural Language Generation Work-
shop, Niagara-on-the-Lake, Ontario.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of SIG-
DOC Conference, Toronto.
Christopher Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
The MIT Press, Cambridge, Massachusetts.
Kathleen McKeown and Dragomir Radev. 2000. Col-
locations. In R. Dale, H. Moisl, and H. Somers, edi-
tors, Handbook of Natural Language Processing. Mar-
cel Dekker.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Proceedings of the Workshop on WordNet
and Other Lexical Resources, Second meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Pittsburgh.
Ted Pedersen. 1996. Fishing for exactness. In Proceed-
ings of the South-Central SAS Users Group Confer-
ence (SCSUG-96), Austin, Texas.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
Peter Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), pages 491?502, Freiburg, Germany.
Answering Clinical Questions with Role Identification
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-Gianolli
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
yun,gh,gregm,prg@cs.toronto.edu
Abstract
We describe our work in progress on natu-
ral language analysis in medical question-
answering in the context of a broader med-
ical text-retrieval project. We analyze the
limitations in the medical domain of the
technologies that have been developed for
general question-answering systems, and
describe an alternative approach whose or-
ganizing principle is the identification of
semantic roles in both question and answer
texts that correspond to the fields of PICO
format.
1 Motivation
In every aspect of patient treatment, questions arise
for which a search of the published medical evidence
is appropriate, as it is very likely that the answer has
already been found from the work of other clinicians.
For example:1
Q: In a child with asthma, do increased doses of inhaled cor-
ticosteroids lead to a decrease in growth?
A: Growth was significantly slower in the group receiving
higher dose inhaled steroids (3.6 cm, 95% CI 3.0 to 4.2
with double dose beclometasone v 5.1 cm, 95% CI 4.5
to 5.7 with salmeterol v 4.5 cm, 95% CI 3.8 to 5.2 with
placebo). (Barton, 2002)
Studies have shown that searching in the literature
can help clinicians in answering questions generated
in patient treatment (Gorman et al, 1994; Cimino,
1All the examples in this paper are taken from a collection of
questions that arose over a two-week period in August 2001 in
a clinical teaching unit at the University of Toronto.
1996; Mendonc?a et al, 2001). It has also been
found that if high-quality evidence is available in
this way at the point of care?for example, the pa-
tients? bedside?clinicians will use it in their deci-
sion making, and it frequently results in additional or
changed decisions (Sackett and Straus, 1998; Straus
and Sackett, 1999). But speed is very important.
Investigation of potential end-users has shown that
physicians need access to the information within 30
seconds, and that if the search takes longer, it is likely
to be abandoned (Takeshita et al, 2002).
The practice of using the current best evidence
to help clinicians in making decisions on the
treatment of patients is called Evidence-Based
Medicine (EBM). Finding relevant evidence is a
typical question-answering (QA) problem in the
medical area. There are many QA systems that have
achieved some success in other domains, such as
finding the answer to ?factoid? questions in a corpus
of general news stories, as in the question-answering
track of recent Text Retrieval Conferences (TREC,
2001). However, we have found that there are large
differences between general QA (GQA) and med-
ical QA (MQA) (see section 3 below). This paper
analyzes the challenges of applying QA technology
to answer clinical questions automatically, and then
describes our on-going work on the problem.
2 The EPoCare Project
Our work is part of the EPoCare project (?Evidence
at Point of Care?) at the University of Toronto. The
project aims to provide fast access at the point of
care to the best available medical information. Clin-
icians will be able to query sources that summarize
and appraise the evidence about the diagnosis, treat-
ment, prognosis, etiology, and prevalence of medi-
cal conditions. In order to make the system available
at the point of care, the question-answering system
will be accessible using hand-held computers. The
project is an interdisciplinary collaboration that in-
volves research in several disciplines. Project mem-
bers in Industrial Engineering and Cognitive Psy-
chology are investigating the design of the system
through a user-centered design process, in which re-
quirements are elicited from end users who are also
involved in the evaluation of prototypes. Project
members in Knowledge Management and Natural
Language Processing aim to ensure that the answers
to queries are accurate and complete. And project
members in Health Informatics will test the influence
of the system on clinical decision-making and clini-
cal outcomes.
The system is presently based on keyword queries
and retrieval, as we describe in section 2.2 below.
The goal of the work that we will report in the later
sections of the paper is to allow the system to accept
questions in natural language2 and to better identify
answers in its natural-language data sources. Our
initial emphasis is on the latter.
2.1 System architecture
There are two main components in the system.
The data sources are stored in an XML document
database. The EPoCare server uses this database to
provide answers to queries posed by clinicians.
The architecture of the system is shown in Fig-
ure 1. A clinical query is passed to the front con-
troller to form a database query of keywords. The
query is sent by the retriever to the XML document
database to retrieve relevant documents in the data
sources using keyword matching. The results are
then passed to the query?answer matcher to find the
best answer candidates. Finally, the best answer is
determined and returned to the user.
The current data sources include the reviews of ex-
perimental results for clinical problems that are pub-
lished in Clinical Evidence (CE) (version 7) (Barton,
2002), and Evidence-based On Call (EBOC) (Ball
2Here and throughout the paper, we make the conven-
tional distinction between query and question; the former is a
keyword-basedstring or structure, and the latter is in natural lan-
guage. A query may represent a question, and vice versa.
UMLS
FTI
ToX Engine
keywords
candidate answers
EPoCare Server
expanded
retrieved
documents
Catalog
clinical answers
answers
  keywords
Client Application
Front Controller
clinical query
   Query-Answer
Matcher
Query Processor
(relevant) 
documents
candidate 
    CE
Retriever
EBOC
expansion of keywords
ToX query / answer
Answer Extractor
Figure 1: EPoCare system architecture.
and Phillips, 2001). The texts are stored with XML
mark-up in the database. The XML database is ma-
nipulated by ToX, a repository manager for XML
data (Barbosa et al, 2001). Repositories of dis-
tributed XML documents may be stored in a file sys-
tem, a relational database, or remotely on the Web.
ToX supports document registration, collection man-
agement, storage and indexing choice, and queries
on document content and structure.
2.2 PICO-format queries
At present, the system accepts queries in a format
known in Evidence-Based Medicine as PICO format
(Sackett et al, 2000). In this format, a clinical ques-
tion is represented by a set of four fields that corre-
spond to the basic elements of the question:
P: a description of the patient (or the problem);
I: an intervention;
C: a comparison or control intervention (may be omitted);
O: the clinical outcome.
For example, the sample question in section 1 can be
represented in a simple PICO format as follows:
P: asthma
I: inhaled corticosteroids
C: ?
O: growth
A more-complete PICO representation of the same
question is this:
P: child with asthma
I: increased doses of inhaled corticosteroids
C: ?
O: decrease in growth
This representation contains more information; but
neither of the two expresses the complete semantics
of the natural-language question. Thus, the PICO
format is limited in its ability to represent the mean-
ing of questions. Especially in the case of yes?no
questions, the point of the question is likely to be un-
clear. However, the PICO format indicates the basic
semantics of the question, and it is commonly used
in question representation in EBM. Thus it was used
as a starting point in the development of the system.
The keyword-based retrieval procedure is com-
posed of three steps:
Retrieving. For each query keyword, XML paths
in which the keyword appears are found.
Filtering. Paths that are not meaningful contexts
for the PICO category of the keyword are filtered out.
For each PICO category in the question, some XML
context is meaningful for it while others not. For ex-
ample, a chapter title is meaningful (and valuable)
context for an instance of patient population in the
keyword matching. But titles of cited references are
not.
Building answers. In the filtered paths, the system
identifies cases in which all the key concepts in the
question have been found, in context, in such a way
that an answer pattern is satisfied. Then it returns
the related segment of text in XML format so that
the user can view it with a browser. A set of answer
patterns were constructed for this matching process;
each answer pattern consists of a set of XML paths
for each of the four PICO categories. To identify a
path as relevant, all four components should find a
match in it.
Clinical question: In a patient with a suspected MI does
thrombolysis decrease the risk of death if it is administered 10
hours after the onset of chest pain?
PICO format:
P: myocardial infarction
I: thrombolysis
C: ?
O: mortality
Keywords: myocardial infarction thrombolysis mortality
Answer: Systematic reviews of RCTs have found that prompt
thrombolytic treatment (within 6 hours and perhaps up to 12
hours and longer after the onset of symptoms) reduces mortality
in people with AMI and ST elevation or bundle branch block
on their presenting ECG.
Fifty six people would need treatment in the acute
phase to prevent one additional death. Strokes, intracranial
haemorrhage, and major bleeds are more common in people
given thrombolysis; with one additional stroke for every
250 people treated and one additional major bleed for every
143 people treated. The reviews have found that intracranial
haemorrhage is more common in people of advanced age and
low body weight, those with hypertension on admission, and
those given tPA rather than another thrombolytic agent.
Figure 2: Example of clinical question, with corre-
sponding EPoCare query and answer from Clinical
Evidence.
While this searching strategy is based on the PICO
format, it is not confined to it. The patterns can be
extended so that additional categories (components)
are included. Thus, it could be applied to questions
that are not expressed in PICO.
Figure 2 shows an example of a clinical question
with the corresponding EPoCare query and the seg-
ment of text that was retrieved from Clinical Evi-
dence in response. The segment that was retrieved is
clearly relevant to the question, but it has too much
irrelevant data.
3 QA in medicine: The problem
We will now discuss medical question-answering,
with the goal of refining the current EPoCare system
by accepting natural-language questions and better
identifying answers in the data sources.
In this section, we examine the difference between
general and medical QA from the perspective of the
three main research problems of QA: question pro-
cessing, question?answer matching, and answer ex-
traction. For each problem, we describe features that
current QA technology is not appropriate for, and
features that are not addressed by existing technol-
ogy.
3.1 Question processing
For a question to be answered correctly, a QA sys-
tem first has to understand what the question is ask-
ing about. This is an important task of question pro-
cessing. Most current QA systems address it by iden-
tifying the type of answer sought. As GQA systems
focus on wh- questions, many of which have named
entities (NEs) as their answer, they usually classify
answers according to different types of NE, such as
product, organization, person, and so on. This clas-
sification is not appropriate in the medical domain,
in which questions often ask about the treatment for
a disease, outcome of a treatment, possible disease,
and so on. As a result, the method of identifying an
answer type must be different in MQA from GQA.
Even for the same answer type, there may be a
different understanding. For example, when ques-
tions ask for the time that an event happens. In GQA
systems, they are usually answered by an absolute
date, e.g., 15 May 1932. However, in the medical
area, when questions are usually answered by rel-
ative time, e.g., two hours after the onset of chest
pain. Sometimes the answers are not even a time; in-
stead, they are a clinical condition, e.g., in response
to When should antibiotics be applied?
Some problems of MQA are not addressed at all
by current QA technologies:
Question focus. Sometimes, the answer type
is not enough to determine what a question is
about. Other information contained in the question
is needed to understand its goal. This information is
defined as the focus of the question (Moldovan and
Harabagiu, 2000). Although different systems use
different names for the idea of question focus, it is re-
garded to be very important in question processing.
However, there is still no special technique to tackle
this problem.
Yes?no questions. As mentioned, most current
QA systems focus on wh- questions; yes?no ques-
tions are still left untouched. However, we have
found that they are very common in our collection
of clinical questions that arose in patient treatment.
Efficient processing of yes?no questions is an impor-
tant task in MQA.
3.2 Question?answer matching
The matching of question and answer is the pro-
cess that most GQA systems put great effort into.
Different methods are applied according to different
views of the problem. The approaches can be clas-
sified into two categories: knowledge-intensive and
data-intensive. Knowledge-intensive approaches try
to find the correct match between a question and
the answer by using effective natural language pro-
cessing techniques that combine linguistic and real-
world knowledge. Typical systems include those of
Pas?ca and Harabagiu (2001) and Hovy, Hermjakob,
and Lin (2001). Data-intensive approaches explore
information embedded in the data sources to extract
the evidence that supports a good answer. They can
be further divided into information extraction?based
(Soubbotin, 2001), redundancy-based (Clarke et al,
2001; Dumais et al, 2002), and statistical QA (It-
tycheriah et al, 2001). Many systems contain ele-
ments of both approaches.
Although there have been many technologies de-
veloped for matching the answer with the question,
they are not applicable to the medical area directly
for the following reasons.
Knowledge taxonomy. WordNet is the main
knowledge base that most current GQA systems use
in analyzing relationships among words when cal-
culating the similarity of a question and a candidate
answer. However, as a general-purpose knowledge
base, it is not possible for WordNet to cover all the
concepts in any particular domain, such as medicine.
A domain-specific knowledge base is needed. For
example, it may be important to know that meto-
prolol is an instance of ?-blocker in order to locate
the correct answer. A good complement to WordNet
is the Unified Medical Language System (UMLS)
(Lindberg et al, 1993), developed by the National
Library of Medicine. UMLS contains three knowl-
edge sources: the Metathesaurus, the Semantic Net-
work, and the Specialist Lexicon. The Metathe-
saurus represents biomedical knowledge by orga-
nizing concepts according to their relationships and
meanings. It will be very helpful in tasks such as
query expansion and answer-type identification in
MQA.
Named entity identification. As the types of
NE in the medical area are different, the method of
identifying them must be changed accordingly. For
example, an MQA system must be able to distin-
guish medication from diseases. Medical terminol-
ogy plays an important role in NE identification, as
before a concept can be classified, the correspond-
ing terminology has to be recognized to make sure
that the correct concept is found. In the medical do-
main, different phrases can be used to refer to the
same medical concept. For example, a drug may be
referred to by its abbreviation, its common name, or
its formal name (ASA, Aspirin, acetylsalicylic acid).
Also, different medical concepts may have the same
abbreviation, which will lead to ambiguities in con-
cept understanding.
Data source. A medical data source is often or-
ganized in accordance with a hierarchy of medical
concepts. For example, Clinical Evidence (Barton,
2002) groups clinical data according to disease cat-
egories. The positive aspect of such well-organized
data is that once the candidate answers are found, it
is very likely that they include the correct answer.
However, it is unlikely that the answer for a ques-
tion will appear redundantly in many different places
in the data source. This is different from GQA sys-
tems, which usually require a relatively large num-
ber of redundant answer candidates to support good
performance by the system.
In current GQA systems, a correct answer to a
question is often independent of its context. This is
not the case in the medical data, in which the con-
text containing a candidate answer may be important
to the question?answer matching. The context usu-
ally explains a conclusion, provides more evidence,
or even presents contrary evidence. A correct answer
may be missed or the incorrect answer may be ex-
tracted if the context is not considered in the match-
ing process.
Complicated constraints. Clinical questions of-
ten contain a very specific description of the patient
conditions, as shown in the following examples:
Q: Should ?-blocker (metoprolol) be used to continue treat-
ment for a male with hypertension and coronary artery
disease even though he has Type 2 diabetes mellitus?
Q: Do patients surviving an AMI and experiencing transient
or ongoing congestive heart failure (CHF) have reduced
mortality and morbidity when treated with an ACE in-
hibitor (ex. Ramipril)?
The detailed description of the patient acts as a
constraint in matching with candidate answers.
As the complexity of questions increases, more-
sophisticated techniques are needed to find a
matching answer.
3.3 Answer extraction
An MQA system should be able to answer clin-
ical questions in the course of patient treatment.
Hence the format of the answer is important, and
this will affect the answer extraction process. For
the three types of questions?wh- questions, yes?no
questions, and no-answer3 questions?the EPoCare
study of user requirements shows that both a short
answer and a long answer should be prepared. The
short answer provides accurate and concise informa-
tion to the physicians so that they can make the deci-
sion quickly. For yes?no questions, the answer can
be just yes or no. If the system cannot find an an-
swer for a question, it should indicate this explic-
itly as its short answer. But sometimes clinicians
want to read a long answer that may contain expla-
nation of the evidence or other results of related ex-
periments. For the no-answer questions, physicians
may expect to read at least some relevant informa-
tion. It is thus important to determine what relevant
information should be included in the answer extrac-
tion.
3.4 Evaluation metrics
Evaluation of QA systems in the medical area is dif-
ferent from current evaluation methods for general
QA systems. The Text Retrieval Conference uses
the Mean Reciprocal Rank (MRR) as an evaluation
metric. In this method, a system may return an or-
dered list of up to five different candidate answers
to a question, and the score received is 1=n, where
n is the position in the list of the correct answer (if it
appears at all); for example, if the correct answer is
fourth in the list, the system receives a score of 0.25
for that test item. This metric cannot be applied here,
since returning a list of alternative candidate answers
to a question, each of which must then be further ver-
ified, is not acceptable for a clinical question that is
posed on site.
Different answer formats should be evaluated sep-
arately. The short answer has to be concise. So what
3A no-answer question is one for which an answer cannot be
found. It is not a yes?no question for which the answer happens
to be no.
a concise answer is must be defined (at least for the
wh- questions). A long answer needs to provide de-
tailed information that explains the short answer. For
no-answer questions, relevant information (if there is
some) should be returned. For these two types of an-
swers, it has to be clear (1) what information can be
viewed as ?detail? or ?relevant?; (2) what the differ-
ence between the two is; and (3) how much informa-
tion should be included.
Partial answers should be considered in the eval-
uation. If part of the correct answer is included in
the system output, it should be evaluated according
to the importance of the correct information. A par-
tial answer that contains more crucial information
should obtain a higher score. Similarly, if an answer
helps make a wrong decision, it should be punished
in the evaluation.
4 Locating answers by role identification
From the discussion in the previous section, we can
see that MQA poses new challenges for QA research
that require new approaches. We have found that the
use of roles and role identification is effective, and
we take them as an organizing principle for MQA
that goes beyond the use of named entities in GQA.
This section will explain the principle. In this ap-
proach, the four roles represented by PICO will first
be located in both the natural-language question and
the candidate answer texts obtained by the retrieval
phase. For example, PICO roles would be identified
in these candidate answers as shown by the labelled
bracketing.
One RCT found [no evidence that (low molecular weight
heparin)I is superior to (aspirin alone)C]O for the treatment
of (acute ischaemic stroke in people with atrial fibrillation)P.
(Thrombolysis)I (reduces the risk of dependency, but in-
creases the risk of death)O.
We found (no evidence of benefit)O from (surgical evacua-
tion of cerebral or cerebellar haematomas)I.
In the matching process, the roles in the question will
be compared with the corresponding roles in the an-
swer candidates to determine whether a candidate is
a correct answer.
4.1 Why roles?
In GQA systems, as mentioned, in the question?
answer matching process, usually the answer candi-
dates are first checked to see if they contain the ex-
pected answer type, in order to rule out irrelevant
candidates. This is shown to be efficient, as indi-
cated by Harabagiu et al (2001): systems that did
not include NE recognizers performed poorly in the
TREC evaluations. The effectiveness of this method
depends on successfully recognizing NEs in the an-
swer candidates. However, for questions that cannot
be answered by named entities, the QA task is more
complex, as it will be more difficult to recognize the
corresponding answer type in the answer candidates.
The same problem occurs in MQA. The important
information in medical text usually corresponds to
the basic PICO fields. For example, therapy-related
text describes the relationshipsamong four elements:
the status of the patient, the therapy, the compari-
son therapy, and the clinical outcome. Descriptions
of the diagnosis process often consist of the patient
status, the test method, and the outcome. These el-
ements are the key concepts of understanding medi-
cal text. They act as different roles, which together
construct the meaning of the text. While some of the
roles correspond to NEs, others do not. For exam-
ple, in answering a therapy-related question, the pa-
tient status and the therapy can often be treated as
NEs, but the clinical outcome often cannot be. In
a description of diagnosis, the test process often is
not represented by an NE. While medical NEs can be
expected to be recognized by applying terminology
techniques with the support of UMLS, the recogni-
tion of non-NE roles in the answer candidates, on the
other hand, becomes the main challenge.
Thus, it is not sufficient to use information-
extraction techniques, as in some GQA systems
(Pas?ca and Harabagiu, 2001; Soubbotin, 2001), in
which patterns are matched against the text to fill in
the roles in the template. In such systems, the cover-
age of the pattern set is quite limited; it is very time-
consuming to manually construct a large set of suit-
able patterns, especially for complicated phrasings;
and the patterns are very specific: specific words or
phrases are usually required to occur at a fixed lo-
cation in each pattern, making it applicable only to
expressions phrased in exactly the same way. While
we will need to look for some specific words, we
need much greater flexibility than is afforded by sim-
ple pattern-matching to identify the PICO roles in
the text. This can be done by analyzing the different
roles and their relationships.
4.2 Understanding the data
To apply a role-based method in MQA, we need to
deal with the following problems:
1. Identifying the roles in text.
2. Determining the textual boundary of each role.
3. Analyzing the relationships among different roles.
4. Determining which combinations of roles are most likely
to contain correct answers.
Our work currently focuses on therapy-related
questions. We manually analyzed 170 sentences
from the Cardiovascular Disorders section of Clini-
cal Evidence to obtain a better understandingof these
problems. Among the sentences, 141 contained at
least one role that we are interested in. For therapy-
related questions, we found that often if an outcome
role appeared in a sentence, then the sentence pro-
vided some interesting information related to clinical
evidence. But clinical outcome is the most difficult
non-NE role to locate.
4.2.1 Identifying clinical outcomes
In our analysis, we found that the lexical iden-
tifiers of clinical outcome belong to three part-of-
speech categories: noun, verb, and adjective. For ex-
ample:
Thrombolysis reduces the risk of dependency, but increases
the risk of death.
Lubeluzole has also been noted to have adverse outcome, es-
pecially at higher doses.
Some words that identify outcomes are listed below:
Nouns: death, benefit, dependency, effect, evidence, out-
come.
Verbs: improve, reduce, prevent, produce, increase.
Adjectives: responsible, negative, adverse, slower.
Clinical outcomes must be carefully distinguished
in the text from the outcomes of clinical trials them-
selves. We refer to the latter as results in the follow-
ing. A result might or might not include a clinical
outcome. They often involve a comparison of the
effects of two (or more) interventions on a disease.
Sometimes a result will state that an outcome did not
occur:
One RCT found evidence that hormone treatment plus radio-
therapy versus radiotherapy alone improved survival in lo-
cally advanced breast cancer.
In the systematic review of calcium channel antagonists, in-
direct and limited comparisons of intravenous versus oral
administration found no significant difference in adverse
events.
We found no evidence of benefit from surgical evacuation of
cerebral or cerebellar haematomas.
The identifiers of results form another group:
Result: evidence, difference, comparison, superior to, ver-
sus.
4.2.2 Determining the textual boundary of
clinical outcomes
In determining the textual boundary of an out-
come, the four groups of words are treated sepa-
rately. Our finding is that for the noun identifiers, the
noun phrase that contains the nouns will be an out-
come. For the verb identifiers, the verb and its ob-
ject together constitutean outcome. For the adjective
identifiers, usually the adjective itself is an outcome.
If several identifiers occur in one sentence, the out-
come is all the text indicated by one or more of the
identifiers.
Determining the textual boundary of the results of
clinical trials is more complicated. If a result is a
comparison of two or more interventions, it will con-
tain the interventions, words that indicate a compar-
ison relationship, and often the aspects that are com-
pared. In the first of the previous group of exam-
ples, the elements of the results are evidence, hor-
mone treatment plus radiotherapy versus radiother-
apy, and improved survival. However, if the inter-
ventions can be identified as NEs, it will not be too
difficult to determine the boundary.
We tested these simple rules manually on 50 sen-
tences from Clinical Evidence on the topic of acute
otitis media. Out of 54 outcomes (including both
clinical outcomes and clinical trial results), 45 were
identified correctly, and 40 correct textual bound-
aries were found.
4.2.3 Relationships among roles
We have also found that roles are helpful in un-
derstanding the relationshipsbetween sentences. For
example, if a sentence contains only the interven-
tion role and the following sentence contains only the
problem and outcome, then it is very likely that the
combination of the two sentences represents a com-
plete idea and the roles themselves are related. We
believe that as the work continues, more interesting
relations will be found.
5 Conclusion
We have described our work in progress on adding
natural language analysis to querry-answering the
EPoCare project. Although this work is at a rela-
tively early stage, we have analyzed the limitations
of GQA technologies in MQA and are developing
techniques whose organizing principle is the identifi-
cation of the semantic roles in both question and an-
swer texts, with our initial emphasis being on the lat-
ter.
Acknowledgements
The EPoCare project is supported by grants from
Bell University Laboratories at the University
of Toronto. This research is also supported by a
grant from the Natural Sciences and Engineering
Research Council of Canada. We are grateful to
Sharon Straus, MD, and other members of the
project for discussion and assistance.
References
Christopher M. Ball and Robert S. Phillips. 2001.
Evidence-based On-Call: Acute Medicine. Churchill
Livingstone, Edinburgh.
Denilson Barbosa, Attila Barta, Alberto Mendel-
zon, George Mihaila, Flavio Rizzolo, and Patricia
Rodriguez-Gianolli. 2001. ToX ? The Toronto XML
Engine. In Proceedings of the International Workshop
on Information Integration on the Web, Rio de Janeiro.
Stuart Barton. 2002. Clinical Evidence. BMJ Publishing
Group, London.
James J. Cimino. 1996. Linking patient information
systems to bibliographic resources. Methods of Infor-
mation in Medicine, 35(2):122?126.
Charles L. A. Clarke, Gordon V. Cormack, and Thomas R.
Lynam. 2001. Exploiting redundancy in question
answering. In Proceedings of the 24th International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-2001), pages 358?365.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: is
more always better? In Proceedings of the 25th Inter-
national Conference on Research and Development in
Information Retrieval (SIGIR-2002), pages 291?298.
Paul N. Gorman, Joan Ash, and L. Wykoff. 1994. Can
primary care physicians? questions be answered using
the medical journal literature? Bulletin of Medical
Library Association, 82(2): 140?146.
Sanda M. Harabagiu, et al 2001. The role of lexico-
semantic feedback in open-domain textual question?
answering. In Proceedings of the 39th Meeting of
the Association for Computational Linguistics (ACL-
2001), pages 274?281.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2001.
The use of external knowledge in factoid QA. In
(TREC, 2001), pages 644?652.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system ?
TREC-10. In (TREC, 2001), pages 258?264.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Eneida A. Mendonc?a, James J. Cimino, Stephen B.
Johnson, and Yoon-Ho Seol. 2001. Accessing het-
erogeneous sources of evidence to answer clinical
questions. Journal of Biomedical Informatics, 34:
85?98.
Dan Moldovan and Sanda M. Harabagiu. 2000. The
structure and performance of an open-domain question
answering system. In Proceedings of the 38th Meeting
of the Association for Computational Linguistics,
(ACL-2000), pages 563?570.
Marius A. Pas?ca and Sanda M. Harabagiu. 2001. High
performance question/answering. In Proceedings of
the 24th International Conference on Research and
Development in Information Retrieval (SIGIR-2001),
pages 366?374.
David L. Sackett and Sharon E. Straus. 1998. Finding
and applying evidence during clinical rounds: the
?evidence cart?. Journal of the American Medical
Association, 280(15):1336?1338.
David L. Sackett, Sharon E. Straus, W. Scott Richardson,
William Rosenberg, and R. Brian Haynes. 2000.
Evidence-Based Medicine: How to Practice and
Teach EBM. Churchill Livingstone, Edinburgh.
Martin M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In (TREC,
2001), pages 293?302.
Sharon E. Straus and David L. Sackett. 1999. Bringing
evidence to the point of care. Journal of the American
Medical Association, 281:1171?1172.
Harumi Takeshita, Dianne Davis, and Sharon E. Straus.
2002. Clinical evidence at the point of care in acute
medicine: a handheld usability case study. In Proceed-
ings of the Human Factors and Ergonomics Society
46th Annual Meeting, pages 1409?1413, Baltimore.
TREC. 2001. Proceedings of the Tenth Text Retrieval
Conference, Gaithersburg, MD, November 13?16. Na-
tional Institute of Standards and Technology.
Analysis of Semantic Classes in Medical Text for Question Answering
Yun Niu and Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 3G4
Canada
yun@cs.toronto.edu, gh@cs.toronto.edu
Abstract
To answer questions from clinical-evidence texts,
we identify occurrences of the semantic classes ?
disease, medication, patient outcome ? that are
candidate elements of the answer, and the relations
among them. Additionally, we determine whether
an outcome is positive or negative.
1 Motivation
The published medical literature is an important
source to help clinicians make decisions in patient
treatment (Sackett and Straus, 1998; Straus and
Sackett, 1999). Clinicians often need to consult lit-
erature on the latest information in patient care, such
as side effects of a medication, symptoms of a dis-
ease, or time constraints in the use of a medication.
For example:1
Q: In a patient with a suspected MI does throm-
bolysis decrease the risk of death if it is ad-
ministered 10 hours after the onset of chest
pain?
The answer to the question can be found in Clini-
cal Evidence (CE) (Barton, 2002), a regularly up-
dated publication that reviews and consolidates ex-
perimental results for clinical problems:
A: Systematic reviews of RCTs have found
that prompt thrombolytic treatment (within 6
hours and perhaps up to 12 hours and longer
after the onset of symptoms) reduces mor-
tality in people with AMI and ST elevation
or bundle branch block on their presenting
ECG.
The goal of the EpoCare project (?Evidence at
Point of Care?) at the University of Toronto is to
develop methods for answering questions automati-
cally with CE as the source text. (We do not look at
1All the examples in this paper are taken from a collection
of questions that arose over a two-week period in August 2001
in a clinical teaching unit at the University of Toronto.
primary medical research text.) Currently, the sys-
tem accepts keyword queries in PICO format (Sack-
ett et al, 2000). In this format, a clinical question is
represented by a set of four fields that correspond to
the basic elements of the question:
P: a description of the patient (or the problem);
I: an intervention;
C: a comparison or control intervention (may
be omitted);
O: the clinical outcome.
For example, the question shown above can be rep-
resented in PICO format as follows:
P: myocardial infarction
I: thrombolysis
C: ?
O: mortality
Our work in the project is to extend the keyword
retrieval to a system that can answer questions ex-
pressed in natural language.
In our earlier work (Niu et al, 2003), we showed
that current technologies for factoid question an-
swering (QA) are not adequate for clinical ques-
tions, whose answers must often be obtained by
synthesizing relevant context. To adapt to this new
characteristic of QA in the medical domain, we ex-
ploit semantic classes and relations between them in
medical text. Semantic classes are important for our
task because the information contained in them is
often a good candidate for answering clinical ques-
tions. In the example above, PICO elements cor-
respond to three semantic classes: DISEASE (med-
ical problem of the patient), INTERVENTION (med-
ication applied to the disease) and the CLINICAL
OUTCOME. They together constitute a SCENARIO
of treatment. Similarly, a diagnosis scenario often
includes SYMPTOMS, TESTING PROCEDURE, and
HYPOTHESIZED DISEASES. To understand the se-
mantics of medical text and find answers to clinical
questions, we need to know how these classes relate
to each other in a specific scenario. For example, is
this medication a special type of another one; is this
medication applied to this disease? These are the
kind of relations that we are interested in. In this
work, we use a cue-word?based approach to iden-
tify semantic classes in the treatment scenario and
analyze the relations between them. We also apply
an automatic classification process to determine the
polarity of an outcome, as it is important in answer-
ing clinical questions.
2 Identifying Semantic Classes in Medical
Text
2.1 Diseases and Medications
The identification of named entities (NEs) in the
biomedical area, such as PROTEINS and CELLS, has
been extensively explored; e.g., Lee et al (2003),
Shen et al (2003). However, we are not aware of
any satisfactory solution that focuses on the recog-
nition of semantic classes such as MEDICATION and
DISEASE. To straightforwardly identify DISEASE
and MEDICATION in the text, we use the knowledge
base Unified Medical Language System (UMLS)
(Lindberg et al, 1993) and the software MetaMap
(Aronson, 2001).
UMLS contains three knowledge sources: the
Metathesaurus, the Semantic Network, and the Spe-
cialist Lexicon. Given an input sentence, MetaMap
separates it into phrases, identifies the medical con-
cepts embedded in the phrases, and assigns proper
semantic categories to them according to the knowl-
edge in UMLS. For example, for the phrase imme-
diate systemic anticoagulants, MetaMap identifies
immediate as a TEMPORAL CONCEPT, systemic as
a FUNCTIONAL CONCEPT, and anticoagulants as a
PHARMACOLOGIC SUBSTANCE. More than one se-
mantic category in UMLS may correspond to MED-
ICATION or DISEASE. For example, either a PHAR-
MACOLOGIC SUBSTANCE or a THERAPEUTIC OR
PREVENTIVE PROCEDURE can be a MEDICATION;
either a DISEASE OR SYNDROME or a PATHOLOGIC
FUNCTION can be a DISEASE.
We use some training text to find the mapping
between UMLS categories and the two semantic
classes in the treatment scenario. The training
text was tagged for us by a clinician to mark DIS-
EASE and MEDICATION. It was also processed by
MetaMap. After that, the annotated text was com-
pared with the output of MetaMap to find the corre-
sponding UMLS categories. Medical text contain-
ing these categories can then be identified as either
MEDICATION or DISEASE. In the example above,
anticoagulants will be taken as a MEDICATION. The
problem of identification of medical terminology is
still a big challenge in this area. MetaMap does not
provide a full solution to it. For cases in which the
output of MetaMap is not consistent with the judg-
ment of the clinician who annotated our text, our
decisions rely on the latter.
2.2 Clinical Outcome
The task of identifying clinical outcomes is more
complicated. Outcomes are often not just noun
phrases; instead, they usually are expressed in com-
plex syntactic structures. The following are some
examples:
(1) Thrombolysis reduces the risk of depen-
dency, but increases the risk of death.
(2) The median proportion of symptom free days
improved more with salmeterol than with
placebo.
In our analysis of the text, we found another type
of outcome which is also very important: the out-
come of clinical trials:
(3) Several small comparative RCTs [random-
ized clinical trials] have found sodium cro-
moglicate to be less effective than inhaled
corticosteroids in improving symptoms and
lung function.
(4) In the systematic review of calcium chan-
nel antagonists, indirect and limited compar-
isons of intravenous versus oral administra-
tion found no significant difference in ad-
verse events.
We treat these as a special type of clinical outcome.
For convenience, we refer to them as ?results? in the
following description when necessary. A ?result?
might contain a clinical outcome within it, as results
often involve a comparison of the effects of two (or
more) interventions on a disease.
In medical text, the appearance of some words is
found often to be a signal of the occurrence of an
outcome, and usually several words signal the oc-
currence of one single outcome. The combination
approach that we applied for identifying outcomes
is based on this observation. Our approach does
not extract the whole outcome at once. Instead, it
tries to identify the different parts of an outcome
that may be scattered in the sentence, and then com-
bines them to form the complete outcome.
2.2.1 Related work
Rule-based methods and machine-learning ap-
proaches have been used for similar problems.
Gildea and Jurafsky (2002) used a supervised learn-
ing method to learn both the identifier of the seman-
tic roles defined in FrameNet such as theme, target,
goal, and the boundaries of the roles (Baker et al,
2003). A set of features were learned from a large
training set, and then applied to the unseen data to
detect the roles. The performance of the system was
quite good. However, it requires a large training
set for related roles, which is not available in many
tasks, including tasks in the medical area.
Rule-based methods are explored in information
extraction (IE) to identify roles to fill slots in some
pre-defined templates (Catala` et al, 2003). The
rules are represented by a set of patterns, and tem-
plate role identification is usually conducted by pat-
tern matching. Slots indicating roles are embed-
ded in these patterns. Text that satisfies the con-
straints of a pattern will be identified, and the con-
tents corresponding to the slots are extracted. This
approach has been proved to be effective in many IE
tasks. However, pattern construction is very time-
consuming, especially for complicated phrasings.
In order to select the roles and only the roles, their
expression has to be customized specifically in pat-
terns. This results in increasing difficulties in pat-
tern construction, and reduces the coverage of the
patterns.
2.2.2 A combination approach
Different pieces of an outcome are identified by var-
ious cue words. Each occurrence of a cue word sug-
gests a portion of the expression of the outcome.
Detecting all of them will increase the chance of
obtaining the complete outcome. Also, different oc-
currences of cue words provide more evidence of
the existence of an outcome.
The first step of the combination approach is to
collect the cue words. Two sections of CE (stroke
management, asthma in children) were analyzed for
detection of outcome. The text was annotated by a
clinician in the EpoCare project. About two-thirds
of each section (267 sentences in total) was taken as
the analysis examples for collecting the cue words,
and the rest (156 sentences) as the test set. Some
words we found in the analysis are the following:
Nouns: death, benefit, dependency, outcome, evi-
dence, harm, difference.
Verbs: improve, reduce, prevent, produce, in-
crease.
Adjectives: beneficial, harmful, negative, adverse,
superior.
After the cue words are identified, the next ques-
tion is what portion of text each cue word suggests
as the outcome, which determines the boundary of
the outcome. The text was pre-processed by the
Apple Pie parser (Sekine, 1997) to obtain the part-
of-speech and phrase information. We found that
for the noun cues, the noun phrase that contains the
noun will be part of the outcome. For the verb cue
words, the verb and its object together constitute
one portion of the outcome. For the adjective cue
words, often the corresponding adjective phrase or
the noun phrase belongs to the outcome. Cue words
for the results of clinical trials are processed in a
slightly different way. For example, for difference
and superior, any immediately following preposi-
tional phrase is also included in the results of the
trial.
Our approach does not rely on specific patterns,
it is more flexible than pattern-matching techniques
in IE systems, and it does not need a large training
set. A limitation of this approach is that some con-
nections between different portions of an outcome
may be missing.
2.2.3 Evaluation and analysis of results
We evaluated the cue word method of detecting the
outcome on the remaining one-third of the sections
of CE. (The test set is rather small because of the
difficulty in obtaining the annotations.) The out-
come detection task was broken into two sub-tasks,
each evaluated separately: to identify the outcome
itself and to determine its textual boundary. The re-
sult of identification is shown in Table 1. Eighty-one
sentences in the test set contain either an outcome or
result, which is 52% of all the test sentences. This
was taken as the baseline of the evaluation: taking
all sentences in the test set as positive (i.e., contain-
ing an outcome or result). By contrast, the accuracy
of the combination approach is 83%.
There are two main reasons why some outcomes
were not identified. One is that some outcomes do
not have any cue word:
(5) Gastrointestinal symptoms and headaches
have been reported with both montelukast
and zafirlukast.
The other reason is that although some outcomes
contained words that might be regarded as cue
words, we did not include them in our set; for ex-
ample, fewer and higher. Adjectives were found to
have the most irregular usages. It is normal for them
to modify both medications and outcomes, as shown
in the following examples:
(6) . . . children receiving higher dose inhaled
corticosteroids . . .
Table 1: Results of identifying outcomes in CE
False False
Method Correct Positives Negatives Precision% Recall% Accuracy%
Baseline 81 75 0 52 (81/156) 100 52
Combination approach 67 14 14 83 (67/81) 83 82
Table 2: Results of boundary detection of correctly
identified outcomes in CE. A: Identified fragments;
B: true boundary.
Type of Overlap Number Percentage
Exact match 26 39
A entirely within B 19 28
B entirely within A 13 19
Each partially
within the other 8 12
No match 1 1
(7) . . . mean morning PEFR was 4% higher in
the salmeterol group.
Other adjectives such as less, more, lower, shorter,
longer, and different have similar problems. If they
are taken as identifiers of outcomes then some false
positives are very likely to be generated. However,
if they are excluded, some true outcomes will be
missed. There were 14 samples of false positives.
The main cause was sentences containing cue words
that did not have any useful information:
(8) We found that the balance between bene-
fits and harms has not been clearly estab-
lished for the evacuation of supratentorial
haematomas.
(9) The third systematic review did not evaluate
these adverse outcomes.
Table 2 shows the result of boundary detection
for those outcomes that were correctly identified.
The true boundary is the boundary of an outcome
that was annotated manually. The no match case
means that there is a true outcome in the sentence
but the program missed the correct portions of text
and marked some other portions as the outcome.
The program identified 39% of the boundaries ex-
actly the same as the true boundaries. In 19% of the
samples, the true boundaries were entirely within
the identified fragments. The spurious text in them
(the text that was not in the true boundary) was
found to be small in many cases, both in terms of
number of words and in terms of the importance of
the content. The average number of words correctly
identified was 7 for each outcome and the number
of spurious words was 3.4. The most frequent con-
tent in the spurious text was the medication applied
to obtain the outcome. In the following examples,
text in ?hi? is the outcome (result) identified auto-
matically, and text in ?fg? is spurious.
(10) The RCTs found hno significant adverse ef-
fects fassociated with salmeterolgi.
(11) The second RCT . . . also found hno sig-
nificant difference in mortality at 12 weeks
fwith lubeluzole versus placebogi . . .
Again, adjectives are most problematic. Even
when a true adjective identifier is found, the bound-
ary of the outcome is hard to determine by an un-
supervised approach because of the variations in
the expression. In the following examples, the true
boundaries of outcomes are indicated by ?[ ]?, ad-
jectives are highlighted.
(12) Nebulised . . . , but [hserious adverse
effectsi are rare].
(13) Small RCTs . . . found that [. . . was
heffectivei, with . . . ].
The correctness of the output of the parser also
had an important impact on the performance, as
shown in the following example:
(14) RCTs found no evidence that lubeluzole
improved clinical outcomes in people with
acute ischaemic stroke.
(S . . . (NPL (DT that) (JJ lubeluzole) (JJ im-
proved) (JJ clinical) (NNS outcomes)) . . . )
In this parse, the verb improve was incorrectly as-
signed to be an adjective in a noun phrase. Thus im-
prove as a verb cue word was missed in identifying
the outcome. However, another cue word outcomes
was matched, so the whole noun phrase of outcomes
was identified as the outcome. On the one hand,
the example shows that the wrong parsing output
directly affects the identification process. On the
other hand, it also shows that missing one cue word
in identifying the outcome can be corrected by the
occurrence of other cue words in the combination
approach.
3 Analysis of Relations
Recognition of individual semantic classes is not
enough for text understanding; we also need to
know how different entities in the same semantic
class are connected, as well as what relations hold
between different classes. Currently, all these rela-
tions are considered at the sentence level.
3.1 Relations within the same semantic class
Relations between different medications are the fo-
cus of this sub-section, as a sentence often men-
tioned more than one medication. Relations be-
tween diseases can be analyzed in a similar way, al-
though they occur much less often than medications.
Text from CE was analyzed manually to understand
what relations are often involved and how they are
represented. The text for the analysis is the same
as in the class-identification task discussed above.
As with classes themselves, it was found that these
relations can be identified by a group of cue words
or symbols. For example, the word plus refers to
the COMBINATION of two or more medications, the
word or, as well as a comma, often suggests the AL-
TERNATIVE relation, and the word versus (or v) usu-
ally implies a COMPARISON relation, as shown in
the following examples:
(15) The combination of aspirin plus streptoki-
nase significantly increased mortality at 3
months.
(16) RCTs found no evidence that calcium chan-
nel antagonists, lubeluzole, aminobutyric
acid (GABA) agonists, glycine antagonists,
or N-methyl-D-aspartate (NMDA) antago-
nists improve clinical outcomes in people
with acute ischaemic stroke.
(17) One systematic review found no short or
long term improvement in acute ischaemic
stroke with immediate systemic anticoagu-
lants (unfractionated heparin, low molecu-
lar weight heparin, heparinoids, or specific
thrombin inhibitors) versus usual care with-
out systemic anticoagulants.
It is worth noting that in CE, the experimental con-
ditions are often explained in the description of the
outcomes, for example:
(18) . . . receiving higher dose inhaled corticos-
teroids (3.6cm, 95% CI 3.0 to 4.2 with dou-
ble dose beclometasone v 5.1cm, 95% CI 4.5
to 5.7 with salmeterol v 4.5cm, 95% CI 3.8
to 5.2 with placebo).
(19) It found that . . . oral theophylline . . . ver-
sus placebo increased the mean number of
symptom free days (63% with theophylline v
42% with placebo; P=0.02).
(20) Studies of . . . inhaled steroid (see salme-
terol v high dose inhaled corticosteroids un-
der adult asthma).
These descriptions are usually in parentheses. They
are often phrases and even just fragments of strings
that are not represented in a manner that is uniform
with the other parts of the sentence. Their behavior
is more difficult to capture and therefore the rela-
tions among the concepts in these descriptions are
more difficult to identify. Because they usually are
examples and data, omission of them will not af-
fect the understanding of the whole sentence in most
cases.
Six common relations and their cue words were
found in the text which are shown in Table 3. Cue
words and symbols between medical concepts were
first collected from the training text. Then the re-
lations they signal were analyzed. Some cue words
are ambiguous, for example, or, and, and with. Or
could also suggest a comparison relation although
most of the time it means alternative, and could rep-
resent an alternative relation, and with could be a
specification relation. It is interesting to find that
and in the text when it connects two medications
often suggests an alternative relation rather than a
combination relation (e.g., the second and in exam-
ple 5). Also, compared with versus, plus, etc., and
and with are weak cues as most of their appearances
in the text do not suggest a relation between two
medications.
On the basis of this analysis, an automatic re-
lation analysis process was applied to the test set,
which was the same as in outcome identification.
The test process was divided into two parts: one
took parenthetical descriptions into account (case 1)
and the other one did not (case 2). In the evaluation,
for sentences that contain at least two medications,
?correct? means that the relation that holds between
the medications is correctly identified. We do not
evaluate the relation between any two medications
in a sentence; instead, we only considered two med-
ications that are related to each other by a cue word
or symbol (including those connected by cue words
Table 3: Cue words/symbols for relations between
medications
Relation(s) Cue Words/Symbols
comparison superior to, more than, versus, or,
compare with, between . . . and . . .
alternative or, ?,?, and
combination plus, add to, addition of . . . to . . . ,
combined use of, and, with, ?(?
specification with, ?(?
substitute substitute, substituted for
preference rather than
Table 4: Results of relation analysis
Correct Wrong Missing False Positive
Case 1 49 7 10 9
Case 2 48 7 3 6
other than the set collected from the training text).
The results of the two cases are shown in Table 4.
Most errors are because of the weak indicators
with and and. As in the outcome identification task,
both the training and test sets are rather small, as no
standard annotated text is available.
Some of the surface relationships in Table 3 re-
flect deeper relationships of the semantic classes.
For example, COMPARISON, ALTERNATIVE, and
PREFERENCE imply that the two (or more) medi-
cations have some common effects on the disease(s)
they are applied to. The SPECIFICATION relation, on
the other hand, suggests a hierarchical relation be-
tween the first medication and the following ones, in
which the first medication is a higher-level concept
and the following medications are at a lower level.
For example, in example 17 above, systemic anti-
coagulants is a higher-level concept, unfractionated
heparin, low molecular weight heparin, etc., are ex-
amples of it that lie at a lower level.
3.2 Relations between different semantic
classes
In a specific domain such as medicine, some default
relations often hold between semantic classes. For
example, a CAUSE?EFFECT relation is strongly em-
bedded in the three semantic classes appearing in
a sentence of the form: ?medication . . . disease
. . . outcome?, even if not in this exact order. This
default relation helps the relation analysis because
in most cases we do not need to depend on the text
between the classes to understand the whole sen-
tence. For instance, the CAUSE?EFFECT relation
is very likely to express the idea that applying the
intervention on the disease will have the outcome.
This is another reason that semantic classes are im-
portant, especially in a specific domain.
4 The polarity of outcomes
Most clinical outcomes and the results of clinical
trials are either positive or negative:
(21) Positive: Thrombolysis reduced the risk of
death or dependency at the end of the stud-
ies.
(22) Negative: In the systematic review, throm-
bolysis increased fatal intracranial haemor-
rhage compared with placebo.
Polarity information is useful for several reasons.
First of all, it can filter out positive outcomes if the
question is about the negative aspects of a medica-
tion. Secondly, negative outcomes may be crucial
even if the question does not explicitly ask about
them. Finally, from the number of positive or neg-
ative descriptions of the outcome of a medication
applying to a disease, clinicians can form a general
idea about how ?good? the medication is. As a first
step in understanding opposing relations between
scenarios in medical text, the polarity of outcomes
was determined by an automatic classification pro-
cess.
We use support vector machines (SVMs) to dis-
tinguish positive outcomes from negative ones.
SVMs have been shown to be efficient in text clas-
sification tasks (Joachims, 1998). Given a training
sample, the SVM finds a hyperplane with the max-
imal margin of separation between the two classes.
The classification is then just to determine which
side of the hyperplane the test sample lies in. We
used the SVMlight package (Joachims, 2002) in our
experiment.
4.1 Training and test examples
The training and test sets were built by collecting
sentences from different sections in CE; 772 sen-
tences were used, 500 for training (300 positive, 200
negative), and 272 for testing (95 positive, 177 neg-
ative). All examples were labeled manually.
4.2 Evaluation
The classification used four different sets of fea-
tures. The first feature set includes every unigram
that appears at least three times in the whole train-
ing set. To improve the performance by attenuating
the sparse data problem, in the second feature set,
all names of diseases were replaced by the same tag
disease. This was done by pre-processing the text
using MetaMap to identify all diseases in both the
training and the test examples. Then the identified
diseases were replaced by the disease tag automat-
ically. As medications often are not mentioned in
outcomes, they were not generalized in this manner.
The third feature set represents changes described
in outcomes. Our observation is that outcomes of-
ten involve the change in a clinical value. For ex-
ample, after a medication was applied to a disease,
something was increased (enhanced, more, . . . ) or
decreased (reduced, less, . . . ). Thus the polarity
of an outcome is often determined by how change
happens: if a bad thing (e.g., mortality) is reduced
then it is a positive outcome; if the bad thing is in-
creased, then the outcome is negative. We try to
capture this observation by adding context features
to the feature set. The way they were added is sim-
ilar to incorporating the negation effect described
by Pang et al (2002). But instead of just finding a
?negation word? (not, isn?t, didn?t, etc.), we need to
find two groups of words: those indicating more and
those indicating less. In the training text, we found
9 words in the first group and 7 words in the second
group. When pre-processing text for classification,
following the method of Pang et al, we attached the
tag MORE to all words between the more-words
and the following punctuation mark, and the tag
LESS to the words after the less-words.
The fourth feature set is the combination of the
effects of feature set two and three. In representing
each sentence by a feature vector, we tested both
presence (feature appears or not) and frequency
(count the number of occurrences of the feature in
the sentence).
The accuracy of the classification is shown in Ta-
ble 5. The baseline is to assign a random class (here
we use negative, as they are more frequent in the test
set) to all test samples.
The presence of features performs better than fre-
quency of features in general. Using a more gen-
eral category instead of specific diseases has a pos-
itive effect on the presence-based classification. We
speculate that the effect of this generalization will
be bigger if a larger test set were used. Pang et al
(2002) did not compare the result of using and not
using the negation context effect, so it is not clear
how much it improved their result. In our task, it
is clear that the MORE/ LESS feature has a signif-
icant effect on the performance, especially for the
frequency features.
Table 5: Results of outcome polarity classification
Presence Frequency
Features (%) (%)
Baseline 65.07 65.07
Original unigrams 88.97 87.87
Unigrams with disease 90.07 88.24
Unigrams with
MORE/ LESS tag 91.54 91.91
Unigrams with disease
and MORE/ LESS tag 92.65 92.28
5 Conclusion
We have described our work in medical text anal-
ysis by identifying semantic classes and the rela-
tions between them. Our work suggests that seman-
tic classes in medical scenarios play an important
role in understanding medical text. The scenario
view may be extended to a framework that acts as
a guideline for further semantic analysis.
Semantic classes and their relations have di-
rect applications in medical question answering and
query refinement in information retrieval. In ques-
tion answering, the question and answer candidates
will contain some semantic classes. After identify-
ing them on both sides, the question can be com-
pared with the answer to find whether there is a
match. In information retrieval, relations between
semantic classes can be added to the index. If the
query posed by the user is too general, the system
will ask the user to refine the query by adding more
concepts and even relations so that it will be more
pertinent according to the content of the source. For
example, a user may search for a document describ-
ing the comparison of aspirin and placebo. Instead
of just using aspirin and placebo as the query terms,
the user can specify the comparison relation as well
in the query.
We will continue working on the second level of
the semantic analysis, to explore the relations on
the scenario level. A complete scenario contains all
three semantic classes. One scenario may be the ex-
planation or justification of the previous scenario(s),
or contradictory to the previous scenario(s). De-
tecting these relationships will be of great help for
understanding-based tasks, such as context-related
question answering, topic-related summarization,
etc. As different scenarios might not be adjacent to
each other in the texts, classical rhetorical analysis
cannot provide a complete solution for this problem.
Acknowledgements
The EpoCare project is supported by grants from
Bell University Laboratories at the University of
Toronto. Our work is also supported by a grant
from the Natural Sciences and Engineering Re-
search Council of Canada and an Ontario Graduate
Scholarship. We are grateful to Sharon Straus, MD,
and other members of the EpoCare project for dis-
cussion and assistance.
References
Alan R. Aronson. 2001. Effective mapping of
biomedical text to the UMLS metathesaurus: The
MetaMap program. In Proceedings of Ameri-
can Medical Informatics Association Symposium,
pages 17?21.
Collin F. Baker, Charles J. Fillmore, and Beau
Cronin. 2003. The structure of the Framenet
database. International Journal of Lexicography,
16(3):281?296.
Stuart Barton. 2002. Clinical Evidence. BMJ Pub-
lishing Group, London.
Neus Catala`, Nu?ria Castell, and Mario Mart?in.
2003. A portable method for acquiring infor-
mation extraction patterns without annotated cor-
pora. Natural Language Engineering, 9(2):151?
179.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the European
Conference on Machine Learning (ECML), pages
137?142.
Thorsten Joachims. 2002. SVMlight homepage. In
http://svmlight.joachims.org/.
Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang
Rim. 2003. Two-phase biomedical NE recog-
nition based on SVMs. In Proceedings of 41st
annual meeting of the Association for Compu-
tational Linguistics, Workshop on Natural Lan-
guage Processing in Biomedicine, pages 33?40.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa. T. McCray. 1993. The Unified Medical
Language System. Methods of Information in
Medicine, 32(4):281?291.
Yun Niu, Graeme Hirst, Gregory McArthur, and
Patricia Rodriguez-Gianolli. 2003. Answering
clincal questions with role identification. In Pro-
ceedings of 41st annual meeting of the Associ-
ation for Computational Linguistics, Workshop
on Natural Language Processing in Biomedicine,
pages 73?80.
Bo Pang, Lillian Le, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In Proceedings of 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
David L. Sackett and Sharon E. Straus. 1998. Find-
ing and applying evidence during clinical rounds:
The ?evidence cart?. Journal of the American
Medical Association, 280(15):1336?1338.
David L. Sackett, Sharon E. Straus, W. Scott
Richardson, William Rosenberg, and R. Brian
Haynes. 2000. Evidence-Based Medicine: How
to Practice and Teach EBM. Harcourt Publishers
Limited, Edinburgh.
Satoshi Sekine. 1997. Apple pie parser homepage.
In http://nlp.cs.nyu.edu/app/.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and
Chew-Lim Tan. 2003. Effective adaptation of
hidden Markov model?based named entity rec-
ognizer for biomedical domain. In Proceedings
of 41st annual meeting of the Association for
Computational Linguistics, Workshop on Natural
Language Processing in Biomedicine, pages 49?
56.
Sharon E. Straus and David L. Sackett. 1999.
Bringing evidence to the point of care. Journal
of the American Medical Association, 281:1171?
1172.
Non-Classical Lexical Semantic Relations
Jane Morris
Faculty of Information Studies
University of Toronto
Toronto, Ontario, Canada M5S 3G6
morris@fis.utoronto.ca
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
gh@cs.toronto.edu
Abstract
NLP methods and applications need to take
account not only of ?classical? lexical rela-
tions, as found in WordNet, but the less-
structural, more context-dependent ?non-
classical? relations that readers intuit in text.
In a reader-based study of lexical relations in
text, most were found to be of the latter type.
The relationships themselves are analyzed,
and consequences for NLP are discussed.
1 Introduction
Many NLP applications, such as text summarization and
discourse segmentation, require, or can be helped by,
the identification of lexical semantic relations in text.
However, the resources that are presently available,
such as WordNet (Fellbaum, 1998) provide only ?clas-
sical? relations: taxonomy or hyponymy (robin / bird),
hypernymy (tool / hammer), troponymy (drink / guzzle),
meronymy (hand / finger), antonymy (go / come), and
synonymy (car / automobile).  These relations, which
have been widely studied and applied, are characterized
by a sharing of the same individual defining properties
between the words and a requirement that the words be
of the same syntactic class.1
Intuitively, however, we see many other kinds of
lexical relations in text.  As an example, consider the
following two sentences taken from a Reader?s Digest
article:
I attended a funeral service recently.  Kind words,
Communion, chapel overflowing, speeches by law-
                                                           
1 Causality as a lexical relation (teach / learn), of which there
are just a few examples in WordNet, falls in a grey area here.
yers, government workers, friends, all speaking of
the deceased?s kindness, his brilliance in mathe-
matics, his love of SCRABBLE and CHESS, his
great humility and compassion, his sense of humor.
There are four groups of related words in this text:  the
italicized group is about funerals, the bolded group is
positive human characteristics, the underlined group is
job types, and the capitalized group is games.  Some of
the lexical relations here are of the classical kind that we
mentioned earlier (e.g., chess and  Scrabble have a
common subsumer); but others are examples of rela-
tions that we will refer to as ?non-classical?, such as
funeral / chapel and humility / kindness.  The goal of
this research is to investigate these non-classical rela-
tions, and to determine what the different types are and
how they are used, with a view to eventual automatic
detection of the relationships in text.
Most prior research on types of lexical semantic re-
lations has been context-free: the relations are consid-
ered out of any textual context and are then assumed to
be relevant within textual contexts.  And in lexical co-
hesion research, the analysis of lexical relations has
been done by professional linguists with particular
points of view (Hasan, 1984; Martin, 1992).  A better
understanding of the types of lexical semantic relations
that are actually identified in context by readers of text
will potentially lead to improvements in the types of
relations used in NLP applications.
2 Theoretical Background
2.1 The lexical semantic relations used in lexical
cohesion
When people read a text, the relations between the
words contribute to their understanding of it. Related
word pairs may join together to form larger groups of
related words that can extend freely over sentence
boundaries.  These larger word groups contribute to the
meaning of text through ?the cohesive effect achieved
by the continuity of lexical meaning? (Halliday and
Hasan, 1976, p. 320, emphasis added).  Lexical seman-
tic relations are the building blocks of lexical cohesion ,
and so a clear understanding of their nature and behav-
ior is crucial.  Lexical cohesion analysis has been used
in such NLP applications as determining the structure of
text (Morris and Hirst, 1991) and automatic text sum-
marization (Barzilay and Elhadad, 1999).
In recent lexical cohesion research in linguistics
(Hasan, 1984; Halliday and Hasan, 1989; Martin, 1992)
non-classical relations are largely ignored, and the same
is true in implementations of lexical cohesion in com-
putational linguistics (Barzilay and Elhadad, 1999; Sil-
ber and McCoy, 2002), as the lexical resource used is
WordNet.  It is notable, however, that the original view
of lexical semantic relations in the lexical cohesion
work of Halliday and Hasan (1976) was very broad and
general; the only criterion was that there had to be a
recognizable relation between two words.  Most re-
search on lexical semantic relations in linguistics
(Cruse, 1986) and psychology has also ignored non-
classical relations (with the exception of Chaffin and
Herrmann, 1984); however there have been recent calls
to broaden the focus and include non-classical relations
as well (McRae and Boisvert, 1998; Hodgson, 1991).
A notable exception to this trend is in library and in-
formation science (LIS), and is likely a pragmatic re-
flection of the fact that it is a field with a large user base
that demanded this type of access to reference materials.
In LIS thesauri, most of the word pairs that are classed
as Related Terms (RTs) are related non-classically, but
unfortunately are listed as an undifferentiated group.
Standards for their use have been developed (ISO,
1986); but since 1985, the Library of Congress has been
encouraging a minimization of their use (El-Hoshy,
2001).  Since RTs are all grouped together in an unclas-
sified manner, the result has been inconsistencies and
subjective judgments about what word pairs are in-
cluded; but this is an issue of implementation rather
than whether RTs can, in principle, be useful.
Roget?s Thesaurus, which was used to form the lexi-
cal chains in Morris and Hirst (1991), also gives non-
classically related word groups.  Although this thesaurus
is hierarchically classified, it makes frequent use within
its basic categories of unclassified pointers to other
widely dispersed basic categories.  In this respect the
structure of LIS thesauri and Roget?s Thesaurus are
similar.  They are both hierarchically organized ? Ro-
get?s by Roget?s own principles of domain and topic
division and LIS thesauri by a broad-term / narrow-term
structure ? but they also both have a non-hierarchical,
non-classified ?structure? (or at least mechanism) for
representing non-classical relations.  But while both,
unlike WordNet, give access to non-classically related
word pairs, they don?t give any indication of what the
actual relation between the words is.  Other recent com-
putational work such as that of Ji, Ploux, and Wehrli
(2003) suffers from the same problem, in that groups of
related words are created (in this case through automatic
processing of text corpora), but the actual relations that
hold between the members of the groups are not deter-
mined.
2.2 Non-classical lexical semantic relations
Lakoff (1987) gives the name ?classical? to categories
whose members are related by shared properties. We
will extend Lakoff?s terminology and refer to relations
that depend on the sharing of properties of classical
categories as classical relations.  Hence we will use the
term non-classical  for relations that do not depend on
the shared properties required of classical relations.
Lakoff emphasizes the importance of non-classical
categories, providing support for the importance of non-
classical relations.  The classical category structure has
been a limiting factor in the study of lexical relations:
since relations create categories (and vice versa), if the
categories that are considered are severely restricted in
nature, so too will be the relations; and, as mentioned,
related words must be of the same part of speech. This
is thus a restriction found in both Hasan?s (1984) rela-
tions in lexical cohesion work and Cruse?s (1986, p. 16)
concept of patterns of lexical affinity, where a mecha-
nism is given for relating inter-sentence and, in fact,
inter-text words that are both in the same grammatical
class.  The lexical chains of Morris and Hirst (1991) had
no such restriction, and frequently nouns, verbs, adjec-
tives, adverbs, and verbs were joined together in one
chain.
Lakoff (1987) mentions Barsalou?s (1989) concept
of creating ad hoc categories, his term for categories
that are ?made up on the fly for some immediate pur-
pose?, which would presumably require some type of
processing interaction with a specific text instead of the
assumption that all categories pre-exist (Lakoff, 1987, p.
45).  Two examples of these categories are ?things to
take on a camping trip? and ?what to do for entertain-
ment on a weekend? (ibid, p. 45).  Barsalou?s ad hoc
categories seem to be of (at least) two types:  (1) differ-
ent activities or actions pertaining to the same or similar
objects; (2) different objects pertaining to the same or
similar activities or actions.  This process has similari-
ties to the mechanisms of Hasan (1984), Martin (1992),
and Cruse (1986) that use both intra-sentence case-like
relations and inter-sentence classical relations. Catego-
ries created this way are not classical, as they seem to be
ways of joining ?different? objects, actions, or activities,
and so the relations between their members are not clas-
sical either.  The mix of classical categories and rela-
tions with non-classical categories and relations appears
to be a rich source of lexico-grammatical cohesion.
The following are the major (not necessarily mutu-
ally exclusive) types of non-classical relations found in
the literature:
? Relations between members of Lakoff?s non-
classical categories:  ball, field and umpire, that
are part of the structured activity of cricket (or
baseball).
? Case relations:
o General:  d o g  / b a r k  (Chaffin and
Herrmann, 1984).
o Sentence-specific (Fillmore, 1968):  stroke
/ it in the sentence: They stroked it.
? LIS RTs (Milstead, 2001).
The relations between members of non-classical
categories are unnamable except with reference to the
category name (one can?t describe the relations between
ball / field or bal l / umpire without using the word
cricket).  For word pairs consisting of a member and the
category name, the relation has often been covered, ei-
ther as a general case relation (ball / cricket as instru-
ment / activity) or as an RT (field / cricket as the activity
/ location relation of Neelameghan (2001), or the loca-
tive general case relation).
Case relations come in two varieties:  general and
specific (to a sentence).  The general inter-sentence and
inter-text case relations (Chaffin and Herrmann, 1984)
are given also by several of the LIS researchers who
have provided lists of RT types (Neelameghan, 2001;
Milstead, 2001).  Cruse deals almost exclusively with
classical relations, but does mention two general case-
like relations that he calls ?zero-derived paronymy?
(1986, p. 132).  The instrumental case (dig / spade or
sweep  / broom) and the objective case (drive / vehicle or
ride / bicycle) are given as examples.  He observes that
in the instrumental case, the definition of the noun will
most likely contain the verb, and in the objective case,
the definition of the verb will most likely contain the
noun.  To Cruse, these are not ?real? relations but
merely ?quasi? relations, as the word classes involved
differ.
The case relations as defined by Fillmore (1968) are
intra-sentence grammatical relations that always apply
to the specific text and sentence they are situated in.
Sometimes these relations can be both text-specific and
general at the same time (d o g  / barked in The  dog
barked).  Hasan (1984) and Martin (1992) also use these
intra-sentence case relations to further link together
word groups that have been created through classical
relations, as does Cruse (1986) with his concept of pat-
terns of lexical affinity mentioned above.
LIS can lay claim to the most extensive amount of
research on non-classical relations.  It is interesting to
note that during the development of the Art and Archi-
tecture Thesaurus (AAT), RTs were not included in the
initial design, but rather added in afterwards due to user
demand (Moholt, 1996).  Of the LIS researchers, Nee-
lameghan (2001) has produced the most extensive list of
non-classical relations, which has changed little since
Neelameghan and Ravichandra (1976).  Apart from
relations between members of non-classical categories
(see above), his list includes most of the text-general
relations (recognizable out of the context of a text)
mentioned by other researchers.  Obviously any text-
specific relations such as sentence-specific case cannot
be included, since word pairs are considered out of text.
Note again, however, that both Hasan (1984) and Martin
(1992) use relations similar to text-specific case rela-
tions to strengthen cohesive ties created by the classical
relations.  This combination of text-specific and text-
general relations could prove to be useful computation-
ally.  A couple of exceptions to the above mentioned
relation types have been noted.  Evens et al (1983) have
a provenience relation ( water  / well ), and Cruse (1986)
has a proportional series relation made up of what he
calls recurring endonymy  (university / lecturer / stu-
dent, prison / warden  / convict, hospital / doctor / pa-
tient), that is a relation that ?involves the incorporation
of the meaning of one lexical item in the meaning of
another?, such as education in university / lecturer /
student (1986, p. 123?125).
In the research on domain-neutral lexical semantic
relations, hundreds (Cassidy, 2000) or thousands (Lenat,
1995) of relations are defined, or perhaps even more in
the case of Barri?re and Popowich (2000).  The question
of whether there is a smallish set of field- (domain-)
neutral non-classical relations that will provide (good)
coverage for all (or most) fields is one of the questions
we are investigating.  Encouragingly, LIS has tackled an
extensive number of specific domains with just such a
smallish set of field-neutral non-classical relations.
However, due to the reportedly subjective implementa-
tion of these relations, this may not in fact be true in
practice.  WordNet?s approach uses domain-neutral re-
lations for a general domain, but mostly for classical
relations.  Databases use domain-specific relations for
specific domains.
3  Experiment
3.1  Introduction
We are interested in determining and analyzing the
types of lexical semantic relations that can be identified
in text.  To this end, a study was conducted with nine
participants who read the first 1.5 pages of a general-
interest article from the Reader?s Digest on the topic of
the funeral of a homeless alcoholic who had nonetheless
achieved many positive aspects and qualities in his life.
The study reported here is part of a larger study of three
texts from the Reader?s Digest that investigates not only
the relation types used but also the nature of the larger
word groups, the interactions among the word groups,
how much of and what type of text meaning this infor-
mation represents, and the degree of subjectivity in the
readers? perceptions of both the relation types and word
groups as measured by individual differences (see Mor-
ris and Hirst, 2004).
3.2  Method
Subjects were given a large set of colored pencils and a
supply of data sheets for recording their observations.
They were instructed to first read the article and mark
the words that they perceived to be related, using a dif-
ferent color of pencil to underline the words of each
different group of related words.  (In effect, they built
lexical chains; two words could be in the same group
even if not directly related to one another if both were
related to another word in the group.)  They were told
that they could re-read the text and add new underlining
at any time during this part of the study. Once this task
was completed, the subjects were instructed to transfer
each separate word group to a new data sheet, and for
each group to indicate which pairs of words within the
group they perceived to be related,and what the relation
was.  Finally, they were asked to describe what each
word group was ?about?, and to indicate whether and
how any of the word groups themselves were related to
another.
3.3  Results
We will briefly present some statistics that summarize
the degree of agreement between the subjects, and then
turn to a qualitative analysis.
In general, the subjects were in broad agreement
about many of the groups of related words ? for exam-
ple, that there was a ?funerals? group and a ?positive
human qualities? group ? but, as one would expect,
they differed on the exact membership of the groups.
Eleven groups were identified by at least four of the
nine subjects.  For each of these groups, we computed
the subjects? agreement on membership of the group in
following manner:  We took all possible pairs of sub-
jects, and for each pair computed the number of words
on which they agreed as a percentage of the total num-
ber of words they used.  Averaged over all possible
pairs of subjects, the agreement was 63%.
Next, we looked at agreement on the word pairs that
were identified as directly related (within the groups
that were identified by at least four subjects).  We re-
stricted this analysis to core words, which we defined to
be those marked by a majority of subjects.  We counted
all distinct instances of word pairs that were marked by
at least 50% of the subjects, and divided this by the total
number of distinct word pairs marked.  We found that
25% of the word pairs were marked by at least 50% of
the subjects.
For this set of word pairs that were identified by
more than one subject, we then computed agreement on
what the relationship between the pair was deemed to
be.  We found that the subjects agreed in 86% of the
cases.
We now turn to the nature of lexical relations that
the subjects reported perceiving in the text in each of the
eleven word groups that were used by at least four of
the readers.  As we would expect, the individual word-
ing of the descriptions of relation types varied greatly
by reader: the subjects often used different ways to de-
scribe what were clearly intended to the same relations.
Thus, we had to analyze and interpret their descriptions.
We were careful in this analysis to try to determine the
subjects? intent and generalize the conceptual meaning
of the individual wordings that were given, but not im-
pose any view of what the relations ?should be?.
We found that for this one text, there seems to be an
emerging ?smallish? set of 13 commonly used relations,
listed below.  Not included in the list are the outlier re-
lations ? the relation types used only by one reader.
1. Positive qualities (brilliant / kind).
2. Negative qualities (homeless / alcoholic).
3. Qualities in opposition ( drunk / drying out).
4. Large categories such as positive human char-
acteristics (humility / humour), typical major
life events (funeral / born / married), and jobs /
types of people (lawyer  / volunteer).
5. Words that are each related to a third concept;
for example caring (kind / gentlemanly), re-
member (speeches /   deceased), and education
(people /   professors).
6. Descriptive noun / adjective pairs (born  /
young, professors / brilliant).
7.  Commonly co-occurring words often described
as words that are associated, or related:  (alco-
holic / beer).   In many cases the readers used
subgroups of this category:
a .  Location (homeless / shelter, funeral /
chapel, kitchen / home)
b. Problem / solution / cause / one word leads
to the other  (homeless / drunk, date / love,
date / relationship, alcoholic / rehab pro-
gram).
c. Case relations  (volunteer / service, people /
living, speeches / friends).
d. Aspects of an institution: married (son /
married), funeral (speeches / communion),
and education (college / jobs ).
8. Stereotypical relations (homeless / drunk,  peo-
ple / home).
9. One word related to a large group of words,
seemingly with a lot of import:  (homeless /
the group of positive human characteristics
such as brilliant / kind / humility).
10.  Definitional:  (alcoholic / drunk ) .
11. Quasi-hyponymy relations  ( friend / relation-
ship).
12. Synonymy (relaxed  / at ease).
13. Antonymy (died / born).
The data show that while individual differences occur
(Morris and Hirst, 2004), the readers in the study identi-
fied a common core of groups of related words in the
text.  Agreement on which exact word pairs within a
group are related is much lower at 25%, and possible
reasons for this are, briefly, that this is a much more
indirect task for the readers than initially identifying
word groups and that the word groups might be com-
prehended more as gestalts or wholes.  In cases where
subjects identified word pairs as related, they also
showed a marked tendency, at an average of 86%, to
agree on what the relation was.  This high level of
reader agreement on what the relations were is a reflec-
tion of the importance of considering lexical semantic
relations as being situated in their surrounding context.
In other words, while explaining or perceiving linguistic
meaning out of context is hard, as noted by Cruse
(1986), doing so within text seems here not to be, and is
therefore likely a meaningful area for further study.
One clear pattern was evident in the analysis:  the
overwhelming use of non-classical relations.  There
were a few uses of hyponymy, synonymy, and an-
tonymy (relations 11, 12, and 13 above), but these clas-
sical relations were used only for a minority of the word
pairs identified by the readers from within the word
groups in the text.
4  Discussion
The subjects in this study identified a common ?core? of
groups of related words in the text, as well as exhibiting
subjectivity or individual differences.  Within these
word groups, the subjects identified a ?smallish? group
of common relation types.  Most of these relation types
are non-classical.  This result supports the integration of
these relations into lexical resources or methods used by
NLP applications that need to identify and use lexical
semantic relations and lexical cohesion in text.  There
are two related computational issues.  The easier one is
to be able to automatically identify words in a text that
are related.  Much harder is to be able to provide the
semantically rich information on what the relation actu-
ally is.
Clearly this work is preliminary in the sense that, to
date, one text has been analyzed.  Our next step is to
complete the analysis of the data from the other two
texts in this study, which has been collected but not yet
analyzed.  An obvious area for future research is the
effect of different types of both texts and readers.  Our
readers were all masters-level students from the Faculty
of Information Studies, and the three texts are all gen-
eral-interest articles from Reader?s Digest.
It would be very useful to do a thorough analysis of
the correspondence between the readers? relation types
reported above, and the relation types discussed earlier
from the literature.  A preliminary look indicates over-
lap, for example of inter-sentence case relations, ad hoc
non-classical categories, and words related through a
third concept.  We would like to investigate the poten-
tial of using both classical and non-classical relation
types along with the intra-sentence case relations for the
automatic generation of relations and relation learning.
This work would incorporate and build on the related
ideas discussed above of Cruse (1986), Hasan (1984),
and Barsalou (1989), along with the actual relation
types and word group interactions found by readers.
We are also interested in how text-specific the word
groups and relations are, since non?text-specific infor-
mation can be added to existing resources, but text-
specific knowledge will require further complex inter-
action with the rest of the text.  We intend to investigate
any potential linkages between the word groups in the
texts and other theories that provide pre-determined
structures of text, such as Rhetorical Structure Theory
(Marcu, 1997).  It will also be useful for computational
purposes to have a clearer understanding of what as-
pects of text understanding exist ?in it? and what can be
expected to contribute to subjectivity of interpretation or
individual differences in comprehension.
Acknowledgments
This research was supported by a grant and scholarship
from the Natural Sciences and Engineering Research
Council of Canada.  We are grateful to Clare Beghtol
for ongoing comments.
References
Barri?re, Caroline and Popowich, Fred (200 0).  Ex-
panding the type hierarchy with nonlexical con-
cepts.   In Howard Hamilton (Ed.), Canadian AI
2000 (pp.  53?68) .   Berlin: Springer-Verlag.
Barsalou, L.  (1989).  Intra-concept similarity and its
implications for inter-concept similarity.  In S.
Vosniadou and A. Ortony (Eds.),  Similarity and
analogical reasoning (pp. 76?121).  Cambridge,
England:  Cambridge University Press.
Barzilay, Regina and Elhadad, Michael (1999).  Using
lexical chains for text summarization.  In Inderjeet
Mani and  Mark Maybury (Eds.), Advances in text
summarization (pp. 111?121).  Cambridge, Mass.:
The MIT Press.
Cassidy, P.  (2000).  An investigation of the semantic
relations in the Roget?s Thesaurus:  Preliminary re-
sults.  In A. Gelbukh (Ed.). CICLing-2000 : Confer-
ence on Intelligent Te xt Processing and
Computational Linguistics, February 13? 1 9 , Mex-
ico City, Mexico ,  181?204.
Chaffin, R., and Herrmann, D.  (1984).  The similarity
and diversity of semantic relations.  Memory and
Cognition, 12(2),  134?141.
Cruse, D.  (1986).  Lexical semantic relations.  Cam-
bridge, England:  Cambridge University Press.
El-Hoshy, S.  (2001).  Relationships in Library of Con-
gress Subject Headings.  In C. Bean, and R. Green
(Eds.),  Relationships in the organization of knowl-
edge (pp. 135?152).   Norwell, Mass:  Kluwer Aca-
demic Publishers.
Evens, M., Markowitz, J., Smith, R., and Werner, O.
(Eds.). (1983). Lexical semantic relations:  A com-
parative survey.  Edmonton, Alberta:  Linguistic
Research Inc.
Fellbaum, Christiane (1998).  WordNet: An electronic
lexical database.   Cambridge, Mass.: The MIT
Press.
Fillmore, Charles  (1968).  The Case for Case.  In E.
Bach and R. Harms (Eds.),  Universals in linguistic
theory (pp. 1?88).  New York: Holt, Rinehart and
Winston.
Halliday, M.A.K., and Hasan, Ruqaiya (1976).  Cohe-
sion in English.  London:  Longman.
Halliday, M.A.K., and Hasan, Ruqaiya (1989).  L an-
guage, Context and Text: Aspects of language in a
social-semiotic perspective .  Geelong, Victoria:
Deakin University Press.  (republished by Oxford
University Press, 1989).
Hasan, Ruqaiya (1984).  Coherence and Cohesive Har-
mony.  In J. Flood (Ed.),  Understanding reading
comprehension: Cognition, language and the
structure of prose (pp. 181?219) . Newark, Dela-
ware: International Reading Association.
Hodgson, J.  (1991).  Informational constraints on pre-
lexical priming.  Language and Cognitive Proc-
esses,  6(3),  169?205.
ISO.  (1986).  Guidelines for the establishment and de-
velopment of monolingual thesauri. [Geneva:].
ISO.  (ISO2788-1986(E)).
Ji, Hyungsuk, Ploux, Sabine, and Wehrli, Eric (2003).
Lexical knowledge representation with contex-
onyms. Proceedings, Machine Translation Summit
IX , New Orleans, September 2003.
Lakoff, George  (1987).  Women, Fire and Dangerous
Things.  Chicago:  University of Chicago Press.
Lenat, D. B. (1995).  CYC: A large-scale investment in
knowledge infrastructure. Communications of the
ACM,  38(11),  33?38.
Marcu, Daniel (1997).  From Discourse Structures to
Text Summaries.  In Inderjeet Mani, and Mark
Maybury (Eds.), Intelligent Scalable Text Summari-
zation, Proceedings of a Workshop Sponsored by
the ACL , 82?88.  Somerset NJ:  Association for
Computational Linguistics.
Martin, James (1992).  English text:  System and struc-
ture.  Amsterdam: John Benjamins Publishing Co.
McRae, K., and Boisvert, S.  (1998).  Automatic se-
mantic similarity priming.  Journal of Experimental
Psychology:  Learning, Memory and Cognition,
24(3),   558?572.
Milstead, J.L. (2001). Standards for relationships be-
tween subject indexing terms. In C.A. Bean and R.
Green (Eds.). Relationships in the organization of
knowledge (pp. 53?66).  Kluwer Academic Pub-
lishers.
Molholt, P. (1996). A Model for Standardization in the
Definition and Form of Associative, Interconcept
Links.   (Doctoral dissertation, Rensselaer Polytech-
nic Institute).
Morris, Jane and Hirst, Graeme (1991).  Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text.  Computational Linguistics,
17(1),  21?48.
Morris, Jane and Hirst, Graeme (2004).  The subjectiv-
ity of lexical cohesion in text.  A A A I Spring Sympo-
sium on Exploring Affect and Attitude in Text ,
Stanford.
Neelameghan, A.  (2001).  Lateral relationships in mul-
ticultural, multilingual databases in the spiritual and
religious domains:  The OM Information Service.
In C. Bean and R. Green (Eds.),  Relationships in
the organization of knowledge (pp. 185?198).
Norwell, Mass.:  Kluwer Academic Publishers.
Neelameghan, A., and Ravichandra, R.  (1976).  Non-
hierarchical associative relationships:  Their types
and computer generation of RT links.  Library Sci-
ence,  (13),  24?42.
Roget, Peter Mark.  Roget?s International Thesaurus.
Many editions and publishers.
Silber, H. Gregory and McCoy, Kathleen F. (2002).
Efficiently computed lexical chains as an interme-
diate representation for automatic text summariza-
tion.  Computational Linguistics , 28(4), 487?496.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 35?43,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Distributional Measures of Concept-Distance:
A Task-oriented Evaluation
Saif Mohammad and Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4, Canada
fsmm,ghg@cs.toronto.edu
Abstract
We propose a framework to derive the
distance between concepts from distribu-
tional measures of word co-occurrences.
We use the categories in a published
thesaurus as coarse-grained concepts, al-
lowing all possible distance values to
be stored in a concept?concept matrix
roughly .01% the size of that created
by existing measures. We show that
the newly proposed concept-distance mea-
sures outperform traditional distributional
word-distance measures in the tasks of
(1) ranking word pairs in order of se-
mantic distance, and (2) correcting real-
word spelling errors. In the latter task,
of all the WordNet-based measures, only
that proposed by Jiang and Conrath out-
performs the best distributional concept-
distance measures.
1 Semantic and distributional measures
Measures of distance of meaning are of two kinds.
The first kind, which we will refer to as seman-
tic measures, rely on the structure of a resource
such as WordNet or, in some cases, a semantic
network, and hence they measure the distance be-
tween the concepts or word-senses that the nodes
of the resource represent. Examples include the
measure for MeSH proposed by Rada et al (1989)
and those for WordNet proposed by Leacock and
Chodorow (1998) and Jiang and Conrath (1997).
(Some of the more successful measures, such as
Jiang?Conrath, also use information content de-
rived from word frequency.) Typically, these mea-
sures rely on an extensive hierarchy of hyponymy
relationships for nouns. Therefore, these measures
are expected to perform poorly when used to es-
timate distance between senses of part-of-speech
pairs other than noun?noun, not just because the
WordNet hierarchies for other parts of speech are
less well developed, but also because the hierar-
chies for the different parts of speech are not well
connected.
The second kind of measures, which we will
refer to as distributional measures, are inspired
by the maxim ?You shall know a word by the
company it keeps? (Firth, 1957). These measures
rely simply on raw text, and hence are much less
resource-hungry than the semantic measures; but
they measure the distance between words rather
than word-senses or concepts. In these measures,
two words are considered close if they occur in
similar contexts. The context (or ?company?) of
a target word is represented by its distributional
profile (DP), which lists the strength of associ-
ation between the target and each of the lexical,
syntactic, and/or semantic units that co-occur with
it. Commonly used measures of strength of as-
sociation are conditional probability (0 to 1) and
pointwise mutual information ( ? to ?)1. Com-
monly used units of co-occurrence with the target
are other words, and so we speak of the lexical dis-
tributional profile of a word (lexical DPW). The
co-occurring words may be all those in a prede-
termined window around the target, or may be re-
stricted to those that have a certain syntactic (e.g.,
verb?object) or semantic (e.g., agent?theme) re-
lation with the target word. We will refer to the
former kind of DPs as relation-free. Usually in
1In our experiments, we set negative PMI values to 0, be-
cause Church and Hanks (1990), in their seminal paper on
word association ratio, show that negative PMI values are not
expected to be accurate unless co-occurrence counts are made
from an extremely large corpus.
35
Table 1: Measures of DP distance and measures of
strength of association.
DP distance Strength of association
?-skew divergence conditional probability
cosine pointwise mutual information
Jensen?Shannon divergence
Lin
the latter case, separate association values are cal-
culated for each of the different relations between
the target and the co-occurring units. We will refer
to such DPs as relation-constrained.
Typical relation-free DPs are those of Schu?tze
and Pedersen (1997) and Yoshida et al (2003).
Typical relation-constrained DPs are those of
Lin (1998) and Lee (2001). Below are contrived,
but plausible, examples of each for the word pulse;
the numbers are conditional probabilities.
relation-free DP
pulse: beat (.28), racing (.2), grow
(.13), beans (.09), heart (.04), . . .
relation-constrained DP
pulse: <beat, subject?verb> (.34),
<racing, noun?qualifying adjective>
(.22), <grow, subject?verb> (.14), . . .
The distance between two words, given their
DPs, is calculated using a measure of DP dis-
tance, such as cosine. While any of the mea-
sures of DP distance may be used with any of the
measures of strength of association (see Table 1),
in practice ?-skew divergence (ASD), cosine, and
Jensen?Shannon divergence (JSD) are used with
conditional probability (CP), whereas Lin is used
with PMI, resulting in the distributional measures
ASDcp (Lee, 2001), Coscp (Schu?tze and Pedersen,
1997), JSDcp, and Linpmi (Lin, 1998), respectively.
ASDcp is a modification of Kullback-Leibler diver-
gence that overcomes the latter?s problem of divi-
sion by zero, which can be caused by data sparse-
ness. JSDcp is another relative entropy?based
measure (like ASDcp) but it is symmetric. JSDcp
and ASDcp are distance measures that give scores
between 0 (identical) and infinity (maximally dis-
tant). Linpmi and Coscp are similarity measures that
give scores between 0 (maximally distant) and 1
(identical). See Mohammad and Hirst (2005) for a
detailed study of these and other measures.
2 The distributional hypothesis and its
limitations
The distributional hypothesis (Firth, 1957) states
that words that occur in similar contexts tend to be
semantically similar. It is often suggested, there-
fore, that a distributional measure can act as a
proxy for a semantic measure: the distance be-
tween the DPs of words will approximate the dis-
tance between their senses. But when words have
more than one sense, it is not at all clear what se-
mantic distance between them actually means. A
word in each of its senses is likely to co-occur
with different sets of words. For example, bank
in the ?financial institution? sense is likely to co-
occur with interest, money, accounts, and so on,
whereas the ?river bank? sense might have words
such as river, erosion, and silt around it. If we de-
fine the distance between two words, at least one
of which is ambiguous, to be the closest distance
between some sense of one and some sense of the
other, then distributional distance between words
may indeed be used in place of semantic distance
between concepts. However, because measures of
distributional distance depend on occurrences of
the target word in all its senses, this substitution is
inaccurate. For example, observe that both DPWs
of pulse above have words that co-occur with its
?throbbing arteries? sense and words that co-occur
with its ?edible seed? sense. Relation-free DPs of
pulse in its two separate senses might be as fol-
lows:
pulse ?throbbing arteries?: beat (.36),
racing (.27), heart (.11), . . .
pulse ?edible seeds?: grow (.24), beans
(.14), . . .
Thus, it is clear that different senses of a word have
different distributional profiles (?different com-
pany?). Using a single DP for the word will mean
the union of those profiles. While this might be
useful for certain applications, we believe that in
a number of tasks (including estimating linguistic
distance), acquiring different DPs for the differ-
ent senses is not only more intuitive, but also, as
we will show through experiments in Section 5,
more useful. We argue that distributional pro-
files of senses or concepts (DPCs) can be used to
infer semantic properties of the senses: ?You shall
know a sense by the company it keeps.?
36
3 Conceptual grain size and storage
requirements
As applications for linguistic distance become
more sophisticated and demanding, it becomes at-
tractive to pre-compute and store the distance val-
ues between all possible pairs of words or senses.
But both kinds of measures have large space re-
quirements to do this, requiring matrices of size
N N, where N is the size of the vocabulary (per-
haps 100,000 for most languages) in the case of
distributional measures and the number of senses
(75,000 just for nouns in WordNet) in the case of
semantic measures.
It is generally accepted, however, that WordNet
senses are far too fine-grained (Agirre and Lopez
de Lacalle Lekuona (2003) and citations therein).
On the other hand, published thesauri, such as Ro-
get?s and Macquarie, group near-synonymous and
semantically related words into a relatively small
number of categories?typically between 800 and
1100?that roughly correspond to very coarse
concepts or senses (Yarowsky, 1992). Words with
more than one sense are listed in more than one
category. A published thesaurus thus provides us
with a very coarse human-developed set or inven-
tory of word senses or concepts2 that are more in-
tuitive and discernible than the ?concepts? gener-
ated by dimensionality-reduction methods such as
latent semantic analysis. Using coarse senses from
a known inventory means that the senses can be
represented unambiguously by a large number of
possibly ambiguous words (conveniently available
in the thesaurus)?a feature that we exploited in
our earlier work (Mohammad and Hirst, 2006) to
determine useful estimates of the strength of asso-
ciation between a concept and co-occurring words.
In this paper, we go one step further and use
the idea of a very coarse sense inventory to de-
velop a framework for distributional measures of
concepts that can more naturally and more ac-
curately be used in place of semantic measures
of word senses. We use the Macquarie The-
saurus (Bernard, 1986) as a sense inventory and
repository of words pertaining to each sense. It has
812 categories with around 176,000 word tokens
and 98,000 word types. This allows us to have
much smaller concept?concept distance matri-
ces of size just 812 812 (roughly .01% the size
2We use the terms senses and concepts interchangeably.
This is in contrast to studies, such as that of Cooper (2005),
that attempt to make a principled distinction between them.
of matrices required by existing measures). We
evaluate our distributional concept-distance mea-
sures on two tasks: ranking word pairs in order
of their semantic distance, and correcting real-
word spelling errors. We compare performance
with distributional word-distance measures and
the WordNet-based concept-distance measures.
4 Distributional measures of
concept-distance
4.1 Capturing distributional profiles of
concepts
We use relation-free lexical DPs?both DPWs and
DPCs?in our experiments, as they allow deter-
mination of semantic properties of the target from
just its co-occurring words.
Determining lexical DPWs simply involves
making word?word co-occurrence counts in a
corpus. A direct method to determine lexical
DPCs, on the other hand, requires information
about which words occur with which concepts.
This means that the text from which counts are
made has to be sense annotated. Since exist-
ing labeled data is minimal and manual annota-
tion is far too expensive, indirect means must be
used. In an earlier paper (Mohammad and Hirst,
2006), we showed how this can be done with sim-
ple word sense disambiguation and bootstrapping
techniques. Here, we summarize the method.
First, we create a word?category co-
occurrence matrix (WCCM) using the British
National Corpus (BNC) and the Macquarie
Thesaurus. The WCCM has the following form:
c1 c2 : : : c j : : :
w1 m11 m12 : : : m1 j : : :
w2 m21 m22 : : : m2 j : : :
.
.
.
.
.
.
.
.
.
.
.
.
: : : : : :
wi mi1 mi2 : : : mi j : : :
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A cell mi j, corresponding to word wi and cate-
gory c j, contains the number of times wi co-occurs
(in a window of 5 words in the corpus) with
any of the words listed under category c j in the
thesaurus. Intuitively, the cell mi j captures the
number of times c j and wi co-occur. A contin-
gency table for a single word and single category
can be created by simply collapsing all other rows
and columns into one and summing their frequen-
cies. Applying a suitable statistic, such as odds
37
distributional measures
BNC
distributional relatedness of words
word?word co-occurrence matrix
co-occurrence counting
word?word
Figure 1: Distributional word-distance.
ratio, on the contingency table gives the strength
of association between a concept (category) and
co-occurring word. Therefore, the WCCM can be
used to create the lexical DP for any concept.
The matrix that is created after one pass of the
corpus, which we call the base WCCM, although
noisy (as it is created from raw text and not sense-
annotated data), captures strong associations be-
tween categories and co-occurring words. There-
fore the intended sense (thesaurus category) of a
word in the corpus can now be determined using
frequencies of co-occurring words and its various
senses as evidence. A new bootstrapped WCCM
is created, after a second pass of the corpus, in
which the cell mi j contains the number of times
any word used in sense c j co-occurs with wi. We
have shown (Mohammad and Hirst, 2006) that the
bootstrapped WCCM captures word?category co-
occurrences much more accurately than the base
WCCM, using the task of determining word sense
dominance3 as a test bed.
4.2 Applying distributional measures to
DPCs
Recall that in computing distributional word-
distance, we consider two target words to be dis-
tributionally similar (less distant) if they occur in
similar contexts. The contexts are represented by
the DPs of the target words, where a DP gives the
strength of association between the target and the
co-occurring units. A distributional measure uses
a measure of DP distance to determine the distance
between two DPs and thereby between the two tar-
get words (see Figure 1). The various measures
differ in what statistic they use to calculate the
strength of association and the measure of DP dis-
3Near-upper-bound results were achieved in the task of
determining predominant senses of 27 words in 11 target texts
with a wide range of sense distributions over their two most
dominant senses.
distributional measures
BNC Thesaurus
distributional relatedness of concepts
word?category co-occurrence matrix
sense disambiguation
bootstrapping and
co-occurrence counting
word?category
Figure 2: Distributional concept-distance.
tance they use (see Mohammad and Hirst (2005)
for details). For example, following is the cosine
formula for distance between words w1 and w2 us-
ing relation-free lexical DPWs, with conditional
probability of the co-occurring word given the tar-
get as the strength of association:
Coscp(w1;w2) =
?w2C(w1)[C(w2) (P(wjw1)P(wjw2))
q
?w2C(w1) P(wjw1)2 
q
?w2C(w2) P(wjw2)2
Here, C(x) is the set of words that co-occur with
word x within a pre-determined window.
In order to calculate distributional concept-
distance, consider the same scenario, except that
the targets are now senses or concepts. Two con-
cepts are closer if their DPs are similar, and these
DPCs require the strength of association between
the target concepts and their co-occurring words.
The associations can be estimated from the boot-
strapped WCCM, described in Section 4.1 above.
Any of the distributional measures used for DPWs
can now be used to estimate concept-distance with
DPCs. Figure 2 illustrates our methodology. Be-
low is the formula for cosine with conditional
probabilities when applied to concepts:
Coscp(c1;c2) =
?w2C(c1)[C(c2) (P(wjc1)P(wjc2))
q
?w2C(c1) P(wjc1)2 
q
?w2C(c2) P(wjc2)2
Now, C(x) is the set of words that co-occur with
concept x within a pre-determined window.
We will refer to such measures as distributional
measures of concept-distance (Distribconcept),
in contrast to the earlier-described distribu-
tional measures of word-distance (Distribword)
and WordNet-based (or semantic) measures of
concept-distance (WNetconcept). We shall refer
38
to these three kinds of distance measures as
measure-types. Individual measures in each kind
will be referred to simply as measures.
A distributional measure of concept-distance
can be used to populate a small 812  812
concept?concept distance matrix where a cell
mi j, pertaining to concepts ci and c j, contains
the distance between the two concepts. In con-
trast, a word?word distance matrix for a conserva-
tive vocabulary of 100,000 word types will have
a size 100,000  100,000, and a WordNet-based
concept?concept distance matrix will have a size
75,000  75,000 just for nouns. Our concept?
concept distance matrix is roughly .01% the size
of these matrices.
Note that the DPs we are using are relation-free
because (1) we use all co-occurring words (not just
those that are related to the target by certain syn-
tactic or semantic relations) and (2) the WCCM,
as described in Section 4.1, does not maintain sep-
arate counts for the different relations between the
target and co-occurring words. Creating a larger
matrix with separate counts for the different rela-
tions would lead to relation-constrained DPs.
5 Evaluation
To evaluate the distributional concept-distance
measures, we used them in the tasks of ranking
word pairs in order of their semantic distance and
of correcting real-word spelling errors, and com-
pared our results to those that we obtained on the
same tasks with distributional word-distance mea-
sures and those that Budanitsky and Hirst (2006)
obtained with WordNet-based semantic measures.
The distributional concept-distance measures
used a bootstrapped WCCM created from the BNC
and the Macquarie Thesaurus. The word-distance
measures used a word?word co-occurrence matrix
created from the BNC alone. The BNC was not
lemmatized, part of speech tagged, or chunked.
The vocabulary was restricted to the words present
in the thesaurus (about 98,000 word types) both
to provide a level evaluation platform and to keep
the matrix to a manageable size. Co-occurrence
counts less than 5 were reset to 0, and words
that co-occurred with more than 2000 other words
were stoplisted (543 in all). We used ASDcp (? =
0:99), Coscp, JSDcp, and Linpmi4 to populate corre-
sponding concept?concept distance matrices and
4Whereas Lin (1998) used relation-constrained DPs, in
our experiments all DPs are relation-free.
Table 2: Correlation of distributional measures
with human ranking. Best results for each
measure-type are shown in boldface.
Measure-type
Distribword Distribconcept
Measure closest average
ASDcp .45 .60 ?
Coscp .54 .69 .42
JSDcp .48 .61 ?
Linpmi .52 .71 .59
word?word distance matrices. Applications that
require distance values will enjoy a run-time ben-
efit if the distances are precomputed. While it is
easy to completely populate the concept?concept
co-occurrence matrix, completely populating the
word?word distance matrix is a non-trivial task be-
cause of memory and time constraints.5
5.1 Ranking word pairs
A direct approach to evaluating linguistic dis-
tance measures is to determine how close they
are to human judgment and intuition. Given a
set of word-pairs, humans can rank them in or-
der of their distance?placing near-synonyms on
one end of the ranking and unrelated pairs on the
other. Rubenstein and Goodenough (1965) pro-
vide a ?gold-standard? list of 65 human-ranked
word-pairs (based on the responses of 51 sub-
jects). One automatic word-distance estimator,
then, is deemed to be more accurate than another
if its ranking of word-pairs correlates more closely
with this human ranking. Measures of concept-
distance can perform this task by determining
word-distance for each word-pair by finding the
concept-distance between all pairs of senses of the
two words, and choosing the distance of the clos-
est sense pair. This is based on the assumption that
when humans are asked to judge the semantic dis-
tance between a pair of words, they implicitly con-
sider its closest senses. For example, most people
will agree that bank and interest are semantically
related, even though both have multiple senses?
most of which are unrelated. Alternatively, the
method could take the average of the distance of
all pairs of senses.
5As we wanted to perform experiments with both
concept?concept and word?word distance matrices, we pop-
ulated them as and when new distance values were calculated.
39
Table 3: Hirst and St-Onge metrics for evaluation
of real-word spelling correction.
suspect ratio =
no. of true-suspects
no. of malaps
no. of false-suspects
no. of non-malaps
alarm ratio =
no. of true-alarms
no. of true-suspects
no. of false-alarms
no. of false-suspects
detection ratio =
no. of true-alarms
no. of malaps
no. of false-alarms
no. of non-malaps
correction ratio =
no. corrected malaps
no. of malaps
no. of false-alarms
no. of non-malaps
correction accuracy = no. of corrected malaps
no. of true-alarms
Table 2 lists correlations of human rank-
ings with those created using distributional mea-
sures. Observe that Distribconcept measures
give markedly higher correlation values than
Distribword measures. Also, using the distance of
the closest sense pair (for Coscp and Linpmi) gives
much better results than using the average dis-
tance of all relevant sense pairs. (We do not report
average distance for ASDcp and JSDcp because
they give very large distance values when sense-
pairs are unrelated?values that dominate the av-
erages, overwhelming the others, and making the
results meaningless.) These correlations are, how-
ever, notably lower than those obtained by the best
WordNet-based measures (not shown in the table),
which fall in the range .78 to .84 (Budanitsky and
Hirst, 2006).
5.2 Real-word spelling error correction
The set of Rubenstein and Goodenough word pairs
is much too small to safely assume that measures
that work well on them do so for the entire En-
glish vocabulary. Consequently, semantic mea-
sures have traditionally been evaluated through ap-
plications that use them, such as the work by Hirst
and Budanitsky (2005) on correcting real-word
spelling errors (or malapropisms). If a word
in a text is not ?semantically close? to any other
word in its context, then it is considered a sus-
pect. If the suspect has a spelling-variant that
is ?semantically close? to a word in its context,
then the suspect is declared a probable real-word
spelling error and an ?alarm? is raised; the related
spelling-variant is considered its correction. Hirst
and Budanitsky tested the method on 500 articles
from the 1987?89 Wall Street Journal corpus for
their experiments, replacing every 200th word by
a spelling-variant. We adopt this method and this
test data, but whereas Hirst and Budanitsky used
WordNet-based semantic measures, we use distri-
butional measures Distribword and Distribconcept.
In order to determine whether two words are
?semantically close? or not as per any measure
of distance, a threshold must be set. If the dis-
tance between two words is less than the threshold,
then they will be considered semantically close.
Hirst and Budanitsky (2005) pointed out that there
is a notably wide band between 1.83 and 2.36
(on a scale of 0?4), such that all Rubenstein and
Goodenough word pairs were assigned values ei-
ther higher than 2.36 or lower than 1.83 by human
subjects. They argue that somewhere within this
band is a suitable threshold between semantically
close and semantically distant, and therefore set
thresholds for the WordNet-based measures such
that there was maximum overlap in what the mea-
sures and human judgments considered semanti-
cally close and distant. Following this idea, we
use an automatic method to determine thresholds
for the various Distribword and Distribconcept mea-
sures. Given a list of Rubenstein and Goodenough
word pairs ordered according to a distance mea-
sure, we repeatedly consider the mean of all con-
secutive distance values as candidate thresholds.
Then we determine the number of word-pairs cor-
rectly classified as semantically close or semanti-
cally distant for each candidate threshold, consid-
ering which side of the band they lie as per human
judgments. The candidate threshold with highest
accuracy is chosen as the threshold.
We follow Hirst and St-Onge (1998) in the met-
rics that we use to evaluate real-word spelling cor-
rection; they are listed in Table 3. Suspect ratio
and alarm ratio evaluate the processes of identify-
ing suspects and raising alarms, respectively. De-
tection ratio is the product of the two, and mea-
sures overall performance in detecting the errors.
Correction ratio indicates overall correction per-
formance, and is the ?bottom-line? statistic that we
focus on. Values greater than 1 for each of these
ratios indicate results better than random guessing.
The ability of the system to determine the intended
word, given that it has correctly detected an error,
is indicated by the correction accuracy (0 to 1).
40
Table 4: Real-word error correction using distributional word-distance (Distribword), distributional
concept-distance (Distribconcept), and Hirst and Budanitsky?s (2005) results using WordNet-based
concept-distance measures (WNetconcept). Best results for each measure-type are shown in boldface.
suspect alarm detection correction correction detection correction
Measure ratio ratio ratio accuracy ratio P R F performance
Distribword
ASDcp 3.36 1.78 5.98 0.84 5.03 7.37 45.53 12.69 10.66
Coscp 2.91 1.64 4.77 0.85 4.06 5.97 37.15 10.28 8.74
JSDcp 3.29 1.77 5.82 0.83 4.88 7.19 44.32 12.37 10.27
Linpmi 3.63 2.15 7.78 0.84 6.52 9.38 58.38 16.16 13.57
Distribconcept
ASDcp 4.11 2.54 10.43 0.91 9.49 12.19 25.28 16.44 14.96
Coscp 4.00 2.51 10.03 0.90 9.05 11.77 26.99 16.38 14.74
JSDcp 3.58 2.46 8.79 0.90 7.87 10.47 34.66 16.08 14.47
Linpmi 3.02 2.60 7.84 0.88 6.87 9.45 36.86 15.04 13.24
WNetconcept
Hirst?St-Onge 4.24 1.95 8.27 0.93 7.70 9.67 26.33 14.15 13.16
Jiang?Conrath 4.73 2.97 14.02 0.92 12.91 14.33 46.22 21.88 20.13
Leacock?Chodrow 3.23 2.72 8.80 0.83 7.30 11.56 60.33 19.40 16.10
Lin 3.57 2.71 9.70 0.87 8.48 9.56 51.56 16.13 14.03
Resnik 2.58 2.75 7.10 0.78 5.55 9.00 55.00 15.47 12.07
Notice that the correction ratio is the product of the
detection ratio and correction accuracy. The over-
all (single-point) precision P (no. of true-alarms /
no. of alarms), recall R (no. of true-alarms / no.
of malapropisms), and F-score (2PRP+R ) of detec-
tion are also computed. The product of detection
F-score and correction accuracy, which we will
call correction performance, can also be used as
a bottom-line performance metric.
Table 4 details the performance of Distribword
and Distribconcept measures. For comparison, re-
sults obtained by Hirst and Budanitsky (2005)
with the use of WNetconcept measures are also
shown. Observe that the correction ratio results
for the Distribword measures are poor compared to
Distribconcept measures; the concept-distance mea-
sures are clearly superior, in particular ASDcp and
Coscp. Moreover, if we consider correction ratio to
be the bottom-line statistic, then the Distribconcept
measures outperform all WNetconcept measures ex-
cept the Jiang?Conrath measure. If we con-
sider correction performance to be the bottom-line
statistic, then again we see that the distributional
concept-distance measures outperform the word-
distance measures, except in the case of Linpmi,
which gives slightly poorer results with concept-
distance. Also, in contrast to correction ratio val-
ues, using the Leacock?Chodorow measure results
in relatively higher correction performance values
than the best Distribconcept measures. While it is
clear that the Leacock?Chodorow measure is rela-
tively less accurate in choosing the right spelling-
variant for an alarm (correction accuracy), detec-
tion ratio and detection F-score present contrary
pictures of relative performance in detection. As
correction ratio is determined by the product of
a number of ratios, each evaluating the various
stages of malapropism correction (identifying sus-
pects, raising alarms, and applying the correction),
we believe it is a better indicator of overall per-
formance than correction performance, which is
a not-so-elegant product of an F-score and accu-
racy. However, no matter which of the two is
chosen as the bottom-line performance statistic,
the results show that the newly proposed distri-
butional concept-distance measures are clearly su-
perior to word-distance measures. Further, of all
the WordNet-based measures, only that proposed
by Jiang and Conrath outperforms the best dis-
tributional concept-distance measures consistently
with respect to both bottom-line statistics.
6 Related Work
Patwardhan and Pedersen (2006) create aggregate
co-occurrence vectors for a WordNet sense by
adding the co-occurrence vectors of the words in
its WordNet gloss. The distance between two
senses is then determined by the cosine of the an-
41
gle between their aggregate vectors. However, as
we pointed out in Mohammad and Hirst (2005),
such aggregate co-occurrence vectors are expected
to be noisy because they are created from data that
is not sense-annotated. Therefore, we employed
simple word sense disambiguation and bootstrap-
ping techniques on our base WCCM to create
more-accurate co-occurrence vectors, which gave
markedly higher accuracies in the task of deter-
mining word sense dominance. In the exper-
iments described in this paper, we used these
bootstrapped co-occurrence vectors to determine
concept-distance.
Pantel (2005) also provides a way to create
co-occurrence vectors for WordNet senses. The
lexical co-occurrence vectors of words in a leaf
node are propagated up the WordNet hierarchy.
A parent node inherits those co-occurrences that
are shared by its children. Lastly, co-occurrences
not pertaining to the leaf nodes are removed from
its vector. Even though the methodology at-
tempts at associating a WordNet node or sense
with only those co-occurrences that pertain to it,
no attempt is made at correcting the frequency
counts. After all, word1?word2 co-occurrence fre-
quency (or association) is likely not the same as
SENSE1?word2 co-occurrence frequency (or asso-
ciation), simply because word1 may have senses
other than SENSE1, as well. The co-occurrence
frequency of a parent is the weighted sum of co-
occurrence frequencies of its children. The fre-
quencies of the child nodes are used as weights.
Sense ambiguity issues apart, this is still prob-
lematic because a parent concept (say, BIRD) may
co-occur much more frequently (or infrequently)
with a word than its children (such as, hen, ar-
chaeopteryx, aquatic bird, trogon, and others). In
contrast, the bootstrapped WCCM we use not only
identifies which words co-occur with which con-
cepts, but also has more sophisticated estimates of
the co-occurrence frequencies.
7 Conclusion
We have proposed a framework that allows dis-
tributional measures to estimate concept-distance
using a published thesaurus and raw text. We
evaluated them in comparison with traditional dis-
tributional word-distance measures and WordNet-
based measures through their ability in ranking
word-pairs in order of their human-judged linguis-
tic distance, and in correcting real-word spelling
errors. We showed that distributional concept-
distance measures outperformed word-distance
measures in both tasks. They do not perform
as well as the best WordNet-based measures in
ranking a small set of word pairs, but in the task
of correcting real-word spelling errors, they beat
all WordNet-based measures except for Jiang?
Conrath (which is markedly better) and Leacock-
Chodorow (which is slightly better if we consider
correction performance as the bottom-line statis-
tic, but slightly worse if we rely on correction
ratio). It should be noted that the Rubenstein
and Goodenough word-pairs used in the ranking
task, as well as all the real-word spelling errors
in the correction task are nouns. We expect that
the WordNet-based measures will perform poorly
when other parts of speech are involved, as those
hierarchies of WordNet are not as extensively de-
veloped. On the other hand, our DPC-based mea-
sures do not rely on any hierarchies (even if they
exist in a thesaurus) but on sets of words that un-
ambiguously represent each sense. Further, be-
cause our measures are tied closely to the corpus
from which co-occurrence counts are made, we
expect the use of domain-specific corpora to result
in even better results.
All the distributional measures that we have
considered in this paper are lexical?that is, the
distributional profiles of the target word or con-
cept are based on their co-occurrence with words
in a text. By contrast, semantic DPs would be
based on information such as what concepts usu-
ally co-occur with the target word or concept. Se-
mantic profiles of words can be obtained from
the WCCM itself (using the row entry for the
word). It would be interesting to see how distri-
butional measures of word-distance that use these
semantic DPs of words perform. We also intend
to explore the use of semantic DPs of concepts
acquired from a concept?concept co-occurrence
matrix (CCCM). A CCCM can be created from
the WCCM by setting the row entry for a concept
or category to be the average of WCCM row val-
ues for all the words pertaining to it.
Both DPW- and WordNet-based measures have
large space and time requirements for pre-
computing and storing all possible distance val-
ues for a language. However, by using the cate-
gories of a thesaurus as very coarse concepts, pre-
computing and storing all possible distance values
for our DPC-based measures requires a matrix of
42
size only about 800 800. This level of concept-
coarseness might seem drastic at first glance, but
we have shown that distributional measures of dis-
tance between these coarse concepts are quite use-
ful. Part of our future work will be to try an inter-
mediate degree of coarseness (still much coarser
than WordNet) by using the paragraph subdivi-
sions of the thesaurus instead of its categories to
see if this gives even better results.
Acknowledgments
We thank Afsaneh Fazly, Siddharth Patwardhan,
and the CL group at the University of Toronto
for their valuable feedback. We thank Alex Bu-
danitsky for helping us adapt his malapropism-
correction software to work with distributional
measures. This research is financially supported
by the Natural Sciences and Engineering Research
Council of Canada and the University of Toronto.
References
Eneko Agirre and O. Lopez de Lacalle Lekuona. 2003.
Clustering WordNet word senses. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing (RANLP?03), Bulgaria.
J.R.L. Bernard, editor. 1986. The Macquarie The-
saurus. Macquarie Library, Sydney, Australia.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic dis-
tance. Computational Linguistics, 32(1).
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Martin C. Cooper. 2005. A mathematical model
of historical semantics and the grouping of word
meanings into concepts. Computational Linguistics,
31(2):227?248.
John R. Firth. 1957. A synopsis of linguistic theory
1930?55. In Studies in Linguistic Analysis (special
volume of the Philological Society), pages 1?32, Ox-
ford. The Philological Society.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87?111.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Christiane
Fellbaum, editor, WordNet: An Electronic Lexical
Database, chapter 13, pages 305?332. The MIT
Press, Cambridge, MA.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lexical
taxonomy. In Proceedings of International Con-
ference on Research on Computational Linguistics
(ROCLING X), Taiwan.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
chapter 11, pages 265?283. The MIT Press, Cam-
bridge, MA.
Lillian Lee. 2001. On the effectiveness of the skew
divergence for statistical language analysis. In Arti-
ficial Intelligence and Statistics 2001, pages 65?72.
Dekang Lin. 1998. Automatic retreival and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics
(COLING-98), pages 768?773, Montreal, Canada.
Saif Mohammad and Graeme Hirst. 2005.
Distributional measures as proxies for
semantic relatedness. In submission,
http://www.cs.toronto.edu/compling/Publications.
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), Trento, Italy.
Patrick Pantel. 2005. Inducing ontological co-
occurrence vectors. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL-05), pages 125?132, Ann Arbor,
Michigan.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense?
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1?8, Trento, Italy.
Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria
Blettner. 1989. Development and application of a
metric on semantic nets. IEEE Transactions on Sys-
tems, Man, and Cybernetics, 19(1):17?30.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Hinrich Schu?tze and Jan O. Pedersen. 1997. A
cooccurrence-based thesaurus and two applications
to information retreival. Information Processing
and Management, 33(3):307?318.
David Yarowsky. 1992. Word-sense disambiguation
using statistical models of Roget?s categories trained
on large corpora. In Proceedings of the 14th Inter-
national Conference on Computational Linguistics
(COLING-92), pages 454?460, Nantes, France.
Sen Yoshida, Takashi Yukawa, and Kazuhiro
Kuwabara. 2003. Constructing and examin-
ing personalized cooccurrence-based thesauri on
web pages. In Proceedings of the 12th Interna-
tional World Wide Web Conference, pages 20?24,
Budapest, Hungary.
43
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326?333,
Prague, June 2007. c?2007 Association for Computational Linguistics
TOR, TORMD: Distributional Profiles of Concepts
for Unsupervised Word Sense Disambiguation
Saif Mohammad
Dept. of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
smm@cs.toronto.edu
Graeme Hirst
Dept. of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
gh@cs.toronto.edu
Philip Resnik
Dept. of Linguistics and UMIACS
University of Maryland
College Park, MD 20742
USA
resnik@umiacs.umd.edu
Abstract
Words in the context of a target word
have long been used as features by su-
pervised word-sense classifiers. Moham-
mad and Hirst (2006a) proposed a way to
determine the strength of association be-
tween a sense or concept and co-occurring
words?the distributional profile of a con-
cept (DPC)?without the use of manually
annotated data. We implemented an unsu-
pervised na??ve Bayes word sense classifier
using these DPCs that was best or within
one percentage point of the best unsuper-
vised systems in the Multilingual Chinese?
English Lexical Sample Task (task #5) and
the English Lexical Sample Task (task #17).
We also created a simple PMI-based classi-
fier to attempt the English Lexical Substi-
tution Task (task #10); however, its perfor-
mance was poor.
1 Introduction
Determining the intended sense of a word is poten-
tially useful in many natural language tasks includ-
ing machine translation and information retrieval.
The best approaches for word sense disambiguation
are supervised and they use words that co-occur with
the target as features. These systems rely on sense-
annotated data to identify words that are indicative
of the use of the target in each of its senses.
However, only limited amounts of sense-
annotated data exist and it is expensive to create. In
our previous work (Mohammad and Hirst, 2006a),
we proposed an unsupervised approach to determine
the strength of association between a sense or con-
cept and its co-occurring words?the distributional
profile of a concept (DPC)?relying simply on raw
text and a published thesaurus. The categories in a
published thesaurus were used as coarse senses or
concepts (Yarowsky, 1992). We now show how dis-
tributional profiles of concepts can be used to cre-
ate an unsupervised na??ve Bayes word-sense classi-
fier. We also implemented a simple classifier that
relies on the pointwise mutual information (PMI)
between the senses of the target and co-occurring
words. These DPC-based classifiers participated in
three SemEval 2007 tasks: the English Lexical Sam-
ple Task (task #17), the English Lexical Substitu-
tion Task (task #10), and the Multilingual Chinese?
English Lexical Sample Task (task #5).
The English Lexical Sample Task (Pradhan et al,
2007) is a traditional word sense disambiguation
task wherein the intended (WordNet) sense of a tar-
get word is to be determined from its context. We
manually mapped the WordNet senses to the cate-
gories in a thesaurus and the DPC-based na??ve Bayes
classifier was used to identify the intended sense
(category) of the target words.
The object of the Lexical Substitution Task (Mc-
Carthy and Navigli, 2007) is to replace a target word
in a sentence with a suitable substitute that preserves
the meaning of the utterance. The list of possible
substitutes for a given target word is usually contin-
gent on its intended sense. Therefore, word sense
disambiguation is expected to be useful in lexical
substitution. We used the PMI-based classier to de-
termine the intended sense.
326
The objective of the Multilingual Chinese?
English Lexical Sample Task (Jin et al, 2007) is to
select from a given list a suitable English translation
of a Chinese target word in context. Mohammad et
al. (2007) proposed a way to create cross-lingual
distributional profiles of a concepts (CL-DPCs)?
the strengths of association between the concepts of
one language and words of another. For this task, we
mapped the list of English translations to appropri-
ate thesaurus categories and used an implementation
of a CL-DPC?based unsupervised na??ve Bayes clas-
sifier to identify the intended senses (and thereby the
English translations) of target Chinese words.
2 Distributional profiles of concepts
In order to determine the strength of association be-
tween a sense of the target word and its co-occurring
words, we need to determine their individual and
joint occurrence counts in a corpus. Mohammad and
Hirst (2006a) and Mohammad et al (2007) proposed
ways to determine these counts in a monolingual and
cross-lingual framework without the use of sense-
annotated data. We summarize the ideas in this sec-
tion; the original papers give more details.
2.1 Word?category co-occurrence matrix
We create a word?category co-occurrence matrix
(WCCM) having English word types wen as one di-
mension and English thesaurus categories cen as an-
other. We used the Macquarie Thesaurus (Bernard,
1986) both as a very coarse-grained sense inventory
and a source of words that together represent each
category (concept). The WCCM is populated with
co-occurrence counts from a large English corpus
(we used the British National Corpus (BNC)). A par-
ticular cell mi j, corresponding to word weni and con-
cept cenj , is populated with the number of times weni
co-occurs with any word that has cenj as one of its
senses (i.e., weni co-occurs with any word listed un-
der concept cenj in the thesaurus).
cen1 c
en
2 . . . c
en
j . . .
wen1 m11 m12 . . . m1 j . . .
wen2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
weni mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
A particular cell mi j, corresponding to word weni
and concept cenj , is populated with the number of
times weni co-occurs with any word that has cenj
as one of its senses (i.e., weni co-occurs with any
word listed under concept cenj in the thesaurus).
This matrix, created after a first pass of the corpus,
is the base word?category co-occurrence matrix
(base WCCM) and it captures strong associations
between a sense and co-occurring words (see dis-
cussion of the general principle in Resnik (1998)).
From the base WCCM we can determine the num-
ber of times a word w and concept c co-occur, the
number of times w co-occurs with any concept, and
the number of times c co-occurs with any word. A
statistic such as PMI can then give the strength of
association between w and c. This is similar to how
Yarowsky (1992) identifies words that are indicative
of a particular sense of the target word.
Words that occur close to a target word tend to
be good indicators of its intended sense. Therefore,
we make a second pass of the corpus, using the base
WCCM to roughly disambiguate the words in it. For
each word, the strength of association of each of
the words in its context (?5 words) with each of its
senses is summed. The sense that has the highest cu-
mulative association is chosen as the intended sense.
A new bootstrapped WCCM is created such that
each cell mi j, corresponding to word weni and con-
cept cenj , is populated with the number of times weni
co-occurs with any word used in sense cenj .
Mohammad and Hirst (2006a) used the DPCs
created from the bootstrapped WCCM to attain
near-upper-bound results in the task of determin-
ing word sense dominance. Unlike the McCarthy
et al (2004) dominance system, this approach can
be applied to much smaller target texts (a few
hundred sentences) without the need for a large
similarly-sense-distributed text1. Mohammad and
Hirst (2006b) used the DPC-based monolingual dis-
tributional measures of concept-distance to rank
word pairs by their semantic similarity and to correct
real-word spelling errors, attaining markedly better
results than monolingual distributional measures of
word-distance. In the spelling correction task, the
1The McCarthy et al (2004) system needs to first gener-
ate a distributional thesaurus from the target text (if it is large
enough?a few million words) or from another large text with a
distribution of senses similar to the target text.
327
Figure 1: The cross-lingual candidate senses of Chi-
nese words and .
distributional concept-distance measures performed
better than all WordNet-based measures as well, ex-
cept for the Jiang and Conrath (1997) measure.
2.2 Cross-lingual word?category
co-occurrence matrix
Given a Chinese word wch in context, we use a
Chinese?English bilingual lexicon to determine its
different possible English translations. Each En-
glish translation wen may have one or more possi-
ble coarse senses, as listed in an English thesaurus.
These English thesaurus concepts (cen) will be re-
ferred to as cross-lingual candidate senses of the
Chinese word wch.2 Figure 1 depicts examples.
We create a cross-lingual word?category co-
occurrence matrix (CL-WCCM) with Chinese word
types wch as one dimension and English thesaurus
concepts cen as another.
cen1 c
en
2 . . . c
en
j . . .
wch1 m11 m12 . . . m1 j . . .
wch2 m21 m22 . . . m2 j . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
wchi mi1 mi2 . . . mi j . . .
.
.
.
.
.
.
.
.
. . . .
.
.
.
.
.
.
The matrix is populated with co-occurrence counts
from a large Chinese corpus; we used a collection of
LDC-distributed corpora3?Chinese Treebank En-
glish Parallel Corpus, FBIS data, Xinhua Chinese?
English Parallel News Text Version 1.0 beta 2, Chi-
nese English News Magazine Parallel Text, Chinese
2Some of the cross-lingual candidate senses of wch might not
really be senses of wch (e.g., ?celebrity?, ?practical lesson?, and
?state of the atmosphere? in Figure 1). However, as substanti-
ated by experiments by Mohammad et al (2007), our algorithm
is able to handle the added ambiguity.
3http://www.ldc.upenn.edu
Figure 2: Chinese words having ?celestial body? as
one of their cross-lingual candidate senses.
News Translation Text Part 1, and Hong Kong Paral-
lel Text. A particular cell mi j, corresponding to word
wchi and concept cenj , is populated with the number
of times the Chinese word wchi co-occurs with any
Chinese word having cenj as one of its cross-lingual
candidate senses. For example, the cell for
(?space?) and ?celestial body? will have the sum of
the number of times co-occurs with , ,
, , , and so on (see Figure 2). We used
the Macquarie Thesaurus (Bernard, 1986) (about
98,000 words). The possible Chinese translations
of an English word were taken from the Chinese?
English Translation Lexicon version 3.0 (Huang and
Graff, 2002) (about 54,000 entries).
This base word?category co-occurrence matrix
(base WCCM), created after a first pass of the cor-
pus, captures strong associations between a cate-
gory (concept) and co-occurring words. For ex-
ample, even though we increment counts for both
??celestial body? and ??celebrity? for a par-
ticular instance where co-occurs with ,
will co-occur with a number of words such as
, , and that each have the sense of ce-
lestial body in common (see Figure 2), whereas all
their other senses are likely different and distributed
across the set of concepts. Therefore, the co-
occurrence count of and ?celestial body? will
be relatively higher than that of and ?celebrity?.
As in the monolingual case, a second pass of
the corpus is made to disambiguate the (Chinese)
words in it. For each word, the strength of associ-
ation of each of the words in its context (?5 words)
with each of its cross-lingual candidate senses is
summed. The sense that has the highest cumula-
tive association with co-occurring words is chosen
as the intended sense. A new bootstrapped WCCM
is created by populating each cell mi j, correspond-
ing to word wchi and concept cenj , with the number of
times the Chinese word wchi co-occurs with any Chi-
328
nese word used in cross-lingual sense cenj . A statistic
such as PMI is then applied to these counts to deter-
mine the strengths of association between a target
concept and co-occurring words, giving the distri-
butional profile of the concept.
Mohammad et al (2007) combined German text
with an English thesaurus using a German?English
bilingual lexicon to create German?English DPCs.
These DPCs were used to determine semantic dis-
tance between German words, showing that state-of-
the-art accuracies for one language can be achieved
using a knowledge source (thesaurus) from another.
Given that a published thesaurus has about 1000
categories and the size of the vocabulary N is at
least 100,000, the CL-WCCM and the WCCM are
much smaller matrices (about 1000?N) than the tra-
ditional word?word co-occurrence matrix (N ?N).
Therefore the WCCMs are relatively inexpensive
both in terms of memory and computation.
3 Classification
We implemented two unsupervised classifiers. The
words in context were used as features.
3.1 Unsupervised Na??ve Bayes Classifier
The na??ve Bayes classifier has the following formula
to determine the intended sense cnb:
cnb = argmax
c j?C
P(c j) ?
wi?W
P(wi|c j) (1)
where C is the set of possible senses (as listed in
the Macquarie Thesaurus) and W is the set of words
that co-occur with the target (we used a window of
?5 words).
Traditionally, prior probabilities of the senses
(P(c j)) and the conditional probabilities in the like-
lihood (?wi?W P(wi|c j)) are determined by sim-
ple counts in sense-annotated data. We approx-
imate these probabilities using counts from the
word?category co-occurrence matrix (monolingual
or cross-lingual), thereby obviating the need for
manually-annotated data.
P(c j) =
?i mi j
?i, j mi j
(2)
P(wi|c j) =
mi j
?i mi j
(3)
For the English Lexical Task, mi j is the number of
times the English word wi co-occurs with the En-
glish category c j?as listed in the word?category
co-occurrence matrix (WCCM). For the Multilin-
gual Chinese?English Lexical Task, mi j is the num-
ber of times the Chinese word wi co-occurs with the
English category c j?as listed in the cross-lingual
word?category co-occurrence matrix (CL-WCCM).
3.2 PMI-based classifier
We calculate the pointwise mutual information be-
tween a sense of the target word and a co-occurring
word using the following formula:
PMI(wi,c j) = log
P(wi,c j)
P(wi)?P(c j)
(4)
where P(wi,c j) =
mi j
?i, j mi j
(5)
and P(wi) =
? j mi j
?i, j mi j
(6)
mi j is the count in the WCCM or CL-WCCM (as de-
scribed in the previous subsection). For each sense
of the target, the sum of the strength of association
(PMI) between it and each of the co-occurring words
(in a window of ?5 words) is calculated. The sense
with the highest sum is chosen as the intended sense.
cpmi = argmax
c j?C
?
wi?W
PMI(wi,c j) (7)
Note that this PMI-based classifier does not capital-
ize on prior probabilities of the different senses.
4 Data
4.1 English Lexical Sample Task
The English Lexical Sample Task training and test
data (Pradhan et al, 2007) have 22281 and 4851
instances respectively for 100 target words (50
nouns and 50 verbs). WordNet 2.1 is used as
the sense inventory for most of the target words,
but certain words have one or more senses from
OntoNotes (Hovy et al, 2006). Many of the fine-
grained senses are grouped into coarser senses.
Our approach relies on representing a sense with
a number of near-synonymous words, for which a
thesaurus is a natural source. Even though the ap-
proach can be ported to WordNet4, there was no easy
4The synonyms within a synset, along with its one-hop
neighbors and all its hyponyms, can represent that sense.
329
TRAINING DATA TEST DATA
WORDS BASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYES
all 27.8 41.4 50.8 37.4 49.4 52.1
nouns only 25.6 43.4 53.6 18.1 49.6 49.7
verbs only 29.2 38.4 44.5 58.9 49.1 54.7
Table 1: English Lexical Sample Task: Results obtained using the PMI-based classifier on the training data
and the na??ve Bayes classifier on both training and test data
way of representing OntoNotes senses with near-
synonymous words. Therefore, we asked four na-
tive speakers of English to map the WordNet and
OntoNotes senses of the 100 target words to the
Macquarie Thesaurus and use it as our sense inven-
tory. We also wanted to examine the effect of using
a very coarse sense inventory such as the categories
in a published thesaurus (811 in all).
The annotators were presented with a target word,
its WordNet/OntoNotes senses, and the Macquarie
senses. WordNet senses were represented by syn-
onyms, gloss, and example usages. The OntoNotes
senses were described through syntactic patterns and
example usages (provided by the task organizers).
The Macquarie senses (categories) were described
by the category head (a representative word for
the category) and five other words in the category.
Specifically, words in the same semicolon group5 as
the target were chosen. Annotators 1 and 2 labeled
each WordNet/OntoNotes sense of the first 50 target
words with one or more appropriate Macquarie cat-
egories. Annotators 3 and 4 labeled the senses of the
other 50 words. We combined all four annotations
into a WordNet?Macquarie mapping file by taking,
for each target word, the union of categories chosen
by the two annotators.
4.2 English Lexical Substitution Task
The English Lexical Substitution Task has 1710 test
instances for 171 target words (nouns, verbs, adjec-
tives, and adverbs) (McCarthy and Navigli, 2007).
Some instances were randomly extracted from an
Internet corpus, whereas others were selected man-
ually from it. The target word might or might not be
part of a multiword expression. The task is not tied
to any particular sense inventory.
5Words within a semicolon group of a thesaurus tend to be
more closely related than words across groups.
4.3 Multilingual Chinese?English Lexical
Sample Task
The Multilingual Chinese?English Lexical Sample
Task training and test data (Jin et al, 2007) have
2686 and 935 instances respectively for 40 target
words (19 nouns and 21 verbs). The instances are
taken from a corpus of People?s Daily News. The
organizers used the Chinese Semantic Dictionary
(CSD), developed by the Institute of Computational
Linguistics, Peking University, both as a sense in-
ventory and bilingual lexicon (to extract a suitable
English translation of the target word once the in-
tended Chinese sense is determined).
In order to determine the English translations of
Chinese words in context, our system relies on Chi-
nese text and an English thesaurus. As the thesaurus
is used as our sense inventory, the first author and a
native speaker of Chinese mapped the English trans-
lations of the target to appropriate Macquarie cate-
gories. We used three examples (from the training
data) per English translation for this purpose.
5 Evaluation
5.1 English Lexical Sample Task
Both the na??ve Bayes classifier and the PMI-based
one were applied to the training data. For each in-
stance, the Macquarie category c that best captures
the intended sense of the target was determined. The
instance was labeled with all the WordNet senses
that are mapped to c in the WordNet?Macquarie
mapping file (described earlier in Section 4.1).
5.1.1 Results
Table 1 shows the performances of the two clas-
sifiers. The system attempted to label all instances
and so we report accuracy values instead of pre-
cision and recall. The na??ve Bayes classifier per-
formed markedly better in training than the PMI-
330
based one and so was applied to the test data. The
table also lists baseline results obtained when a sys-
tem randomly guesses one of the possible senses for
each target word. Note that since this is a com-
pletely unsupervised system, it is not privy to the
dominant sense of the target words. We do not rely
on the ranking of senses in WordNet as that would
be an implicit use of the sense-tagged SemCor cor-
pus. Therefore, the most-frequent-sense baseline
does not apply. Table 1 also shows results obtained
using just the prior probability and likelihood com-
ponents of the na??ve Bayes formula. Note that the
combined accuracy is higher than individual com-
ponents for nouns but not for verbs.
5.1.2 Discussion
The na??ve Bayes classifier?s accuracy is only
about one percentage point lower than that of the
best unsupervised system taking part in the task
(Pradhan et al, 2007). One reason that it does bet-
ter than the PMI-based one is that it takes into ac-
count prior probabilities of the categories. However,
using just the likelihood also outperforms the PMI
classifier. This may be because of known problems
of using PMI with low frequencies (Manning and
Schu?tze, 1999). In case of verbs, lower combined
accuracies compared to when using just prior proba-
bilities suggests that the bag-of-words type features
are not very useful. It is expected that more syntac-
tically oriented features will give better results. Us-
ing window sizes (?1,?2, and ?10) on the training
data resulted in lower accuracies than that obtained
using a window of ?5 words. A smaller window
size is probably missing useful co-occurring words,
whereas a larger window size is adding words that
are not indicative of the target?s intended sense.
The use of a sense inventory (Macquarie The-
saurus) different from that used to label the data
(WordNet) clearly will have a negative impact on
the results. The mapping from WordNet/OntoNotes
to Macquarie is likely to have some errors. Further,
for 19 WordNet/OntoNotes senses, none of the an-
notators found a thesaurus category close enough in
meaning. This meant that our system had no way
of correctly disambiguating instances with these
senses. Also impacting accuracy is the significantly
fine-grained nature of WordNet compared to the the-
saurus. For example, following are the three coarse
BEST OOT
Acc Mode Acc Acc Mode Acc
all 2.98 4.72 11.19 14.63
Further Analysis
NMWT 3.22 5.04 11.77 15.03
NMWS 3.32 4.90 12.22 15.26
RAND 3.10 5.20 9.98 13.00
MAN 2.84 4.17 12.61 16.49
Table 2: English Lexical Substitution Task: Results
obtained using the PMI-based classifier
senses for the noun president in WordNet: (1) exec-
utive officer of a firm or college, (2) the chief exec-
utive of a republic, and (3) President of the United
States. The last two senses will fall into just one cat-
egory for most, if not all, thesauri.
5.2 English Lexical Substitution Task
We used the PMI-based classifier6 for the English
Lexical Substitution Task. Once it identifies a suit-
able thesaurus category as the intended sense for a
target, ten candidate substitutes are chosen from that
category. Specifically, the category head word and
up to nine words in the same semicolon group as the
target are selected (words within a semicolon group
are closer in meaning). Of the ten candidates, the
single-word expression that is most frequent in the
BNC is chosen as the best substitute; the motivation
is that the annotators, who created the gold standard,
were instructed to give preference to single words
over multiword expressions as substitutes.
5.2.1 Results
The system was evaluated not only on the best
substitute (BEST) but also on how good the top ten
candidate substitutes are (OOT). Table 2 presents the
results.7 The system attempted all instances. The
table also lists performances of the system on in-
stances where the target is not part of a multiword
expression (NMWT), on instances where the substi-
tute is not a multiword expression (NMWS), on in-
stances randomly extracted from the corpus (RAND),
and on instances manually selected (MAN).
6Due to time constraints, we were able to upload results only
with the PMI-based classifier by the task deadline.
7The formulae for accuracy and mode accuracy are as de-
scribed by Pradhan et al (2007).
331
TRAINING DATA TEST DATA
BASELINE PMI-BASED NAI?VE BAYES PRIOR LIKELIHOOD NAI?VE BAYES
WORDS micro macro micro macro micro macro micro macro micro macro micro macro
all 33.1 38.3 33.9 40.0 38.5 44.7 35.4 41.7 38.8 44.6 37.5 43.1
nouns only 41.9 43.5 43.6 45.0 49.4 50.5 45.3 47.1 48.1 50.8 50.0 51.6
verbs only 28.0 34.1 28.0 35.6 31.9 39.6 29.1 36.8 32.9 39.0 29.6 35.5
Table 3: Multilingual Chinese?English Lexical Sample Task: Results obtained using the PMI-based classi-
fier on the training data and the na??ve Bayes classifier on both training and test data
5.2.2 Discussion
Competitive performance of our DPC-based sys-
tem on the English Lexical Sample Task and the
Chinese?English Lexical Sample Task (see next
subsection) suggests that DPCs are useful for sense
disambiguation. Poor results on the substitution task
can be ascribed to several factors. First, we used
the PMI-based classifier that we found later to be
markedly less accurate than the na??ve Bayes clas-
sifier in the other two tasks. Second, the words in
the thesaurus categories may not always be near-
synonyms; they might just be strongly related. Such
words will be poor substitutes for the target. Also,
we chose as the best substitute simply the most fre-
quent of the ten candidates. This simple technique
is probably not accurate enough. On the other hand,
because we chose the candidates without any regard
to frequency in a corpus, the system chose certain
infrequent words such as wellnigh and ecchymosed,
which were not good candidate substitutes.
5.3 Multilingual Chinese?English Lexical
Sample Task
In the Multilingual Chinese?English Lexical Sample
Task, both the na??ve Bayes classifier and the PMI-
based classifier were applied to the training data.
For each instance, the Macquarie category, say c,
that best captures the intended sense of the target
word is determined. Then the instance is labeled
with all the English translations that are mapped to c
in the English translations?Macquarie mapping file
(described earlier in Section 4.3).
5.3.1 Results
Table 3 shows accuracies of the two classifiers.
Macro average is the ratio of number of instances
correctly disambiguated to the total, whereas micro
average is the average of the accuracies achieved
on each target word. As in the English Lexical
Sample Task, both classifiers, especially the na??ve
Bayes classifier, perform well above the random
baseline. Since the na??ve Bayes classifier also per-
formed markedly better than the PMI-based one in
training, it was applied to the test data. Table 3
also shows results obtained using just the likelihood
and prior probability components of the na??ve Bayes
classifier on the test data.
5.3.2 Discussion
Our na??ve Bayes classifier scored highest of all
unsupervised systems taking part in the task (Jin et
al., 2007). As in the English Lexical Sample Task,
using just the likelihood again outperforms the PMI
classifier on the training data. The use of a sense
inventory different from that used to label the data
again will have a negative impact on the results as
the mapping may have a few errors. The anno-
tator believed none of the given Macquarie cate-
gories could be mapped to two Chinese Semantic
Dictionary senses. This meant that our system had
no way of correctly disambiguating instances with
these senses.
There were also a number of cases where more
than one CSD sense of a word was mapped to the
same Macquarie category. This occurred for two
reasons: First, the categories of the Macquarie The-
saurus act as very coarse senses. Second, for cer-
tain target words, the two CSD senses may be differ-
ent in terms of their syntactic behavior, yet semanti-
cally very close (for example, the ?be shocked? and
?shocked? senses of ). This many-to-one map-
ping meant that for a number of instances more than
one English translation was chosen. Since the task
required us to provide exactly one answer (and there
was no partial credit in case of multiple answers), a
category was chosen at random.
332
6 Conclusion
We implemented a system that uses distributional
profiles of concepts (DPCs) for unsupervised word
sense disambiguation. We used words in the con-
text as features. Specifically, we used the DPCs
to create a na??ve Bayes word-sense classifier and a
simple PMI-based classifier. Our system attempted
three SemEval-2007 tasks. On the training data
of the English Lexical Sample Task (task #17) and
the Multilingual Chinese?English Lexical Sample
Task (task #5), the na??ve Bayes classifier achieved
markedly better results than the PMI-based classi-
fier and so was applied to the respective test data.
On both test and training data of both tasks, the
system achieved accuracies well above the random
baseline. Further, our system placed best or close to
one percentage point from the best among the unsu-
pervised systems. In the English Lexical Substitu-
tion Task (task #10), for which there was no train-
ing data, we used the PMI-based classifier. The
system performed poorly, which is probably a re-
sult of using the weaker classifier and a simple brute
force method for identifying the substitute among
the words in a thesaurus category. Markedly higher-
than-baseline performance of the na??ve Bayes clas-
sifier on task #17 and task #5 suggests that the DPCs
are useful for word sense disambiguation.
Acknowledgments
We gratefully acknowledge Xiaodan Zhu, Michael Demko,
Christopher Parisien, Frank Rudicz, and Timothy Fowler for
mapping training-data labels to categories in the Macquarie
Thesaurus. We thank Michael Demko, Siddharth Patwardhan,
Xinglong Wang, Vivian Tsang, and Afra Alishahi for helpful
discussions. This research is financially supported by the Natu-
ral Sciences and Engineering Research Council of Canada, the
University of Toronto, ONR MURI Contract FCPO.810548265
and Department of Defense contract RD-02-5700.
References
J.R.L. Bernard, editor. 1986. The Macquarie Thesaurus.
Macquarie Library, Sydney, Australia.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL, pages 57?60, New York, NY.
Shudong Huang and David Graff. 2002. Chinese?
english translation lexicon version 3.0. Linguistic
Data Consortium.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In Proceedings of International Conference on
Research on Computational Linguistics, Taiwan.
Peng Jin, Yunfang Wu, and Shiwen Yu. 2007. SemEval-
2007 task 05: Multilingual Chinese-English lexical
sample task. In Proceedings of the Fourth Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Prague, Czech Republic.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, Massachusetts.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text (SemEval-2007), Prague, Czech Republic.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04), pages 280?267, Barcelona, Spain.
Saif Mohammad and Graeme Hirst. 2006a. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy.
Saif Mohammad and Graeme Hirst. 2006b. Distribu-
tional measures of concept-distance: A task-oriented
evaluation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2006), Sydney, Australia.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-lingual distributional
profiles of concepts for measuring semantic dis-
tance. In Proceedings of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL-2007), Prague, Czech Republic.
Sameer Pradhan, Martha Palmer, and Edward Loper.
2007. SemEval-2007 task 17: English lexical sample,
English SRL and English all-words tasks. In Proceed-
ings of the Fourth International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text
(SemEval-2007), Prague, Czech Republic.
Philip Resnik. 1998. Wordnet and class-based prob-
abilities. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 239?263. The
MIT Press, Cambridge, Massachusetts.
David Yarowsky. 1992. Word-sense disambiguation us-
ing statistical models of Roget?s categories trained on
large corpora. In Proceedings of the 14th International
Conference on Computational Linguistics (COLING-
92), pages 454?460, Nantes, France.
333
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1182?1190,
Beijing, August 2010
Near-synonym Lexical Choice in Latent Semantic Space
Tong Wang
Department of Computer Science
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We explore the near-synonym lexical
choice problem using a novel representa-
tion of near-synonyms and their contexts
in the latent semantic space. In contrast to
traditional latent semantic analysis (LSA),
our model is built on the lexical level
of co-occurrence, which has been empir-
ically proven to be effective in provid-
ing higher dimensional information on the
subtle differences among near-synonyms.
By employing supervised learning on the
latent features, our system achieves an ac-
curacy of 74.5% in a ?fill-in-the-blank?
task. The improvement over the current
state-of-the-art is statistically significant.
We also formalize the notion of subtlety
through its relation to semantic space di-
mensionality. Using this formalization
and our learning models, several of our
intuitions about subtlety, dimensionality,
and context are quantified and empirically
tested.
1 Introduction
Lexical choice is the process of selecting content
words in language generation. Consciously or
not, people encounter the task of lexical choice
on a daily basis ? when speaking, writing, and
perhaps even in inner monologues. Its applica-
tion also extends to various domains of natural
language processing, including Natural Language
Generation (NLG, Inkpen and Hirst 2006), writ-
ers? assistant systems (Inkpen, 2007), and sec-
ond language (L2) teaching and learning (Ouyang
et al, 2009).
In the context of near-synonymy, the process
of lexical choice becomes profoundly more com-
plicated. This is partly because of the subtle nu-
ances among near-synonyms, which can arguably
differ along an infinite number of dimensions.
Each dimension of variation carries differences in
style, connotation, or even truth conditions into
the discourse in question (Cruse, 1986), all mak-
ing the seemingly intuitive problem of ?choosing
the right word for the right context? far from triv-
ial even for native speakers of a language. In
a widely-adopted ?fill-in-the-blank? task, where
the goal was to guess missing words (from a set
of near-synonyms) in English sentences, two hu-
man judges achieved an accuracy of about 80%
(Inkpen, 2007). The current state-of-the-art accu-
racy for an automated system is 69.9% (Islam and
Inkpen, 2010).
When the goal is to make plausible or even
elegant lexical choices that best suit the con-
text, the representation of that context becomes a
key issue. We approach this problem in the la-
tent semantic space, where transformed local co-
occurrence data is capable of implicitly inducing
global knowledge (Landauer and Dumais, 1997).
A latent semantic space is constructed by reduc-
ing the dimensionality of co-occurring linguistic
units ? typically words and documents as in La-
tent Semantic Analysis (LSA). We refer to this
level of association (LoA) as document LoA here-
after. Although document LoA can benefit topical
level classification (e.g., as in document retrieval,
Deerwester et al 1990), it is not necessarily suit-
able for lexical-level tasks which might require in-
formation on a more fine-grained level (Edmonds
and Hirst, 2002). Our experimental results show
1182
noticeable improvement when the co-occurrence
matrix is built on a lexical LoA between words
within a given context window.
One intuitive explanation for this improvement
is that the lexical-level co-occurrence might have
helped recover the high-dimensional subtle nu-
ances between near-synonyms. This conjecture
is, however, as imprecise as it is intuitive. The
notion of subtlety has mostly been used qualita-
tively in the literature to describe the level of dif-
ficulty involved in near-synonym lexical choice.
Hence, we endeavor to formalize the concept of
subtlety computationally by using our observa-
tions regarding the relationship between ?subtle?
concepts and their lexical co-occurrence patterns.
We introduce related work on near-synonymy,
lexical choice, and latent semantic space models
in the next section. Section 3 elaborates on lexical
and contextual representations in latent semantic
space. In Section 4, we formulate near-synonym
lexical choice as a learning problem and report our
system performance. Section 5 formalizes the no-
tion of subtlety and its relation to dimensionality
and context. Conclusions and future work are pre-
sented in Section 6.
2 Related Work
2.1 Near-Synonymy and Nuances
Near-synonymy is a concept better explained by
intuition than by definition ? which it does not
seem to have in the existing literature. We thus
borrow Table 1 from Edmonds and Hirst (2002) to
illustrate some basic ideas about near-synonymy.
Cruse (1986) compared the notion of plesionymy
to cognitive synonymy in terms of mutual entail-
ment and semantic traits, which, to the best of our
knowledge, is possibly the closest to a textbook
account of near-synonymy.
There has been a substantial amount of inter-
est in characterizing the nuances between near-
synonyms for a computation-friendly representa-
tion of near-synonymy. DiMarco et al (1993)
discovered 38 dimensions for differentiating near-
synonyms from dictionary usage notes and cat-
egorized them into semantic and stylistic varia-
tions. Stede (1993) focused on the latter and fur-
ther decomposed them into seven scalable sub-
Table 1: Examples of near-synonyms and dimen-
sion of variations (Edmonds and Hirst, 2002).
Types of variation Examples
Continuous, intermittent seep:drip
Emphasis enemy:foe
Denotational, indirect error:mistake
Denotational, fuzzy woods:forest
Stylistic, formality pissed:drunk:inebriated
Stylistic, force ruin:annihilate
Expressed attitude skinny:thin:slim:slender
Emotive daddy:dad:father
Collocational task:job
Selectional pass away:die
Sub-categorization give:donate
categories. By organizing near-synonym vari-
ations into a tree structure, Inkpen and Hirst
(2006) combined stylistic and attitudinal varia-
tion into one class parallel to denotational differ-
ences. They also incorporated this knowledge of
near-synonyms into a knowledge base and demon-
strated its application in an NLG system.
2.2 Lexical Choice Evaluation
Due to their symbolic nature, many of the early
studies were only able to provide ?demo runs? in
NLG systems rather than any empirical evalua-
tion. The study of near-synonym lexical choice
had remained largely qualitative until a ?fill-in-
the-blank? (FITB) task was introduced by Ed-
monds (1997). The task is based on sentences col-
lected from the 1987 Wall Street Journal (WSJ)
that contain any of a given set of near-synonyms.
Each occurrence of the near-synonyms is removed
from the sentence to create a ?lexical gap?, and the
goal is to guess which one of the near-synonyms is
the missing word. Presuming that the 1987 WSJ
authors have made high-quality lexical choices,
the FITB test provides a fairly objective bench-
mark for empirical evaluation for near-synonym
lexical choice. The same idea can be applied to
virtually any corpus to provide a fair amount of
gold-standard data at relatively low cost for lexi-
cal choice evaluation.
The FITB task has since been frequently
adopted for evaluating the quality of lexical choice
systems on a standard dataset of seven near-
synonym sets (as shown in Table 2). Edmonds
1183
(1997) constructed a second-order lexical co-
occurrence network on a training corpus (the 1989
WSJ). He measured the word-word distance us-
ing t-score inversely weighted by both distance
and order of co-occurrence in the network. For
a sentence in the test data (generated from the
1987 WSJ), the candidate near-synonym minimiz-
ing the sum of its distance from all other words in
the sentence (word-context distance) was consid-
ered the correct answer. Average accuracy on the
standard seven near-synonym sets was 55.7%.
Inkpen (2007) modeled word-word distance
using Pointwise Mutual Information (PMI) ap-
proximated by word counts from querying the
Waterloo Multitext System (Clarke et al, 1998).
Word-context distance was the sum of PMI scores
between a candidate and its neighboring words
within a window-size of 10. An unsuper-
vised model using word-context distance directly
achieved an average accuracy of 66.0%, while a
supervised method with lexical features added to
the word-context distance further increased the
accuracy to 69.2%.
Islam and Inkpen (2010) developed a system
which completed a test sentence with possible
candidates one at a time. The candidate gener-
ating the most probable sentence (measured by
a 5-gram language model) was proposed as the
correct answer. N-gram counts were collected
from Google Web1T Corpus and smoothed with
missing counts, yielding an average accuracy of
69.9%.
2.3 Lexical Choice Outside the
Near-synonymy Domain
The problem of lexical choice also comes in many
flavors outside the near-synonymy domain. Reiter
and Sripada (2002) attributed the variation in lexi-
cal choice to cognitive and vocabulary differences
among individuals. In their meteorology domain
data, for example, the term by evening was inter-
preted as before 00:00 by some forecasters but
before 18:00 by others. They claimed that NLG
systems might have to include redundancy in their
output to tolerate cognitive differences among in-
dividuals.
2.4 Latent Semantic Space Models and LoA
LSA has been widely applied in various fields
since its introduction by Landauer and Dumais
(1997). In their study, LSA was conducted on
document LoA on encyclopedic articles and the
latent space vectors were used for solving TOEFL
synonym questions. Rapp (2008) used LSA
on lexical LoA for the same task and achieved
92.50% in accuracy in contrast to 64.38% given
by Landauer and Dumais (1997). This work con-
firmed our early postulation that document LoA
might not be tailored for lexical level tasks, which
might require lower LoAs for more fine-grained
co-occurrence knowledge. Note, however, that
confounding factors might also have led to the dif-
ference in performance, since the two studies used
different weighting schemes and different corpora
for the co-occurrence model1. In Section 3.2 we
will compare models on the two LoAs in a more
controlled setting to show their difference in the
lexical choice task.
3 Representing Words and Contexts in
Latent Semantic Space
We first formalize the FITB task to facili-
tate later discussions. A test sentence t =
{w1, . . . ,w j?1,si,w j+1, . . . ,wm} contains a near-
synonym si which belongs to a set of synonyms
S = {s1, . . . ,sn},1 ? i ? n. A FITB test case is
created by removing si from t, and the context (the
incomplete sentence) c = t?{si} is presented to
subjects with a set of possible choices S to guess
which of the near-synonyms in S is the missing
word.
3.1 Constructing the Latent Space
Representation
The first step in LSA is to build a co-occurrence
matrix M between words and documents, which is
further decomposed by Singular Value Decompo-
sition (SVD) according to the following equation:
Mv?d = Uv?k?k?kV Tk?d
1The former used Groliers Academic American Encyclo-
pedia with weights divided by word entropy, while the latter
used the British National Corpus with weights multiplied by
word entropy.
1184
Here, subscripts denote matrix dimensions, U , ?,
and V together create a decomposition of M, v and
d are the number of word types and documents,
respectively, and k is the number of dimensions
for the latent semantic space. A word w is repre-
sented by the row in U corresponding to the row
for w in M. For a context c, we construct a vector c
of length v with zeros and ones, each correspond-
ing to the presence or absence of a word wi with
respect to c, i.e.,
ci =
{ 1 if wi ? c
0 otherwise
We then take this lexical space vector cv?1 as a
pseudo-document and transform it into a latent se-
mantic space vector c?:
c? = ??1UT c (1)
An important observation is that this represen-
tation is equivalent to a weighted centroid of the
context word vectors: when c is multiplied by
??1UT in Equation (1), the product is essentially
a weighted sum of the rows in U corresponding to
the context words. Consequently, simple modifi-
cations on the weighting can yield other interest-
ing representations of context. Consider, for ex-
ample, the weighting vector wk?1 = (?1, ? ? ? ,?k)T
with
?i = 1|2(pgap? i)?1|
where pgap is the position of the ?gap? in the test
sentence. Multiplying w before ??1 in Equation
(1) is equivalent to giving the centroid gradient-
decaying weights with respect to the distance be-
tween a context word and the near-synonym. This
is a form of a Hyperspace Analogue to Language
(HAL) model, which is sensitive to word order, in
contrast to a bag-of-words model.
3.2 Dimensionality and Level of Association
The number of dimensions k is an important
choice to make in latent semantic space mod-
els. Due to the lack of any principled guideline
for doing otherwise, we conducted a brute force
grid search for a proper k value for each LoA, on
the basis of the performance of the unsupervised
model (Section 4.1 below).
Figure 1: FITB Performance on different LoAs as
a function of the latent space dimensionality.
In Figure 1, performance on FITB using this
unsupervised model is plotted against k for doc-
ument and lexical LoAs. Document LoA is very
limited in the available number of dimensions2;
higher dimensional knowledge is simply unavail-
able from this level of co-occurrence. In contrast,
lexical LoA stands out around k = 550 and peaks
around k = 700. Although the advantage of lexi-
cal LoA in the unsupervised setting is not signif-
icant, later we show that lexical LoA nonetheless
makes higher-dimensional information available
for other learning methods.
Note that the scale on the y-axis is stretched to
magnify the trends. On a zero-to-one scale, the
performance of these unsupervised methods is al-
most indistinguishable, indicating that the unsu-
pervised model is not capable of using the high-
dimensional information made available by lexi-
cal LoA. We will elaborate on this point in Section
5.2.
2The dimensions for document and lexical LoAs on our
development corpus are 55,938?500 and 55,938?55,938,
respectively. The difference is measured between v? d and
v? v (Section 3.1).
1185
4 Learning in the Latent Semantic Space
4.1 Unsupervised Vector Space Model
When measuring distance between vectors, LSA
usually adopts regular vector space model dis-
tance functions such as cosine similarity. With the
context being a centroid of words (Section 3.1),
the FITB task then becomes a k-nearest neighbor
problem in the latent space with k = 1 to choose
the best near-synonym for the context:
s? = argmax
si
cos(UrowId(v(si),M), c?)
where v(si) is the corresponding row for near-
synonym si in M, and rowId(v,M) gives the row
number of a vector v in a matrix M containing v
as a row.
In a model with a cosine similarity distance
function, it is detrimental to use ??1 to weight the
context centroid c?. This is because elements in ?
are the singular values of the co-occurrence matrix
along its diagonal, and the amplitude of a singular
value (intuitively) corresponds to the significance
of a dimension in the latent space; when the in-
verted matrix is used to weight the centroid, it will
?misrepresent? the context by giving more weight
to less-significantly co-occurring dimensions and
thus sabotage performance. We thus use ? instead
of ??1 in our experiments. As shown in Figure
1, the best unsupervised performance on the stan-
dard FITB dataset is 49.6%, achieved on lexical
LoA at k = 800.
4.2 Supervised Learning on the Latent
Semantic Space Features
In traditional latent space models, the latent space
vectors have almost invariantly been used in the
unsupervised setting discussed above. Although
the number of dimensions has been reduced in the
latent semantic space, the inter-relations between
the high-dimension data points may still be com-
plex and non-linear; such problems lend them-
selves naturally to supervised learning.
We therefore formulate the near-synonym lex-
ical choice problem as a supervised classification
problem with latent semantic space features. For
a test sentence in the FITB task, for example, the
context is represented as a latent semantic space
vector as discussed in Section 3.1, which is then
paired with the correct answer (the near-synonym
removed from the sentence) to form one training
case.
We choose Support Vector Machines (SVMs) as
our learning algorithm for their widely acclaimed
classification performance on many tasks as well
as their noticeably better performance on the lex-
ical choice task in our pilot study. Table 2 lists
the supervised model performance on the FITB
task together with results reported by other related
studies. The model is trained on the 1989 WSJ
and tested on the 1987 WSJ to ensure maximal
comparability with other results. The optimal k
value is 415. Context window size3 around the
gap in a test sentence also affects the model per-
formance. In addition to using the words in the
original sentence, we also experiment with enlarg-
ing the context window to neighboring sentences
and shrinking it to a window frame of n words
on each side of the gap. Interestingly, when mak-
ing the lexical choice, the model tends to favor
more-local information ? a window frame of size
5 gives the best accuracy of 74.5% on the test.
Based on binomial exact test4 with a 95% confi-
dence interval, our result outperforms the current
state-of-the-art with statistical significance.
5 Formalizing Subtlety in the Latent
Semantic Space
In this section, we formalize the notion of sub-
tlety through its relation to dimensionality, and
use the formalization to provide empirical support
for some of the common intuitions about subtlety
and its complexity with respect to dimensionality
and size of context.
5.1 Characterizing Subtlety Using
Collocating Differentiator of Subtlety
In language generation, subtlety can be viewed as
a subordinate semantic trait in a linguistic realiza-
3Note that the context window in this paragraph is im-
plemented on FITB test cases, which is different from the
context size we compare in Section 5.3 for building co-
occurrence matrix.
4The binomial nature of the outcome of an FITB test case
(right or wrong) makes binomial exact test a more suitable
significance test than the t-test used by Inkpen (2007).
1186
Table 2: Supervised performance on the seven standard near-synonym sets in the FITB task. 95%
Confidence based on Binomial Exact Test.
Near-synonyms
Co-occur. SVMs 5-gram SVMs on
network & PMI language model latent vectors
(Edmonds, 1997) (Inkpen, 2007) (Islam and Inkpen, 2010) (Section 4.2)
difficult, hard, tough 47.9% 57.3% 63.2% 61.7%
error, mistake, oversight 48.9% 70.8% 78.7% 82.5%
job, task, duty 68.9% 86.7% 78.2% 82.4%
responsibility, burden, 45.3% 66.7% 72.2% 63.5%
obligation, commitment
material, stuff, substance 64.6% 71.0% 70.4% 78.5%
give, provide, offer 48.6% 56.1% 55.8% 75.4%
settle, resolve 65.9% 75.8% 70.8% 77.9%
Average 55.7% 69.2% 69.9% 74.5%
Data size 29,835 31,116 31,116 30,300
95% confidence 55.1?56.3% 68.7?69.7% 69.3?70.4% 74.0?75.0%
tion of an intention5. A key observation regard-
ing subtlety is that it is non-trivial to characterize
subtle differences between two linguistic units by
their collocating linguistic units. More interest-
ingly, the difficulty in such characterization can
be approximated by the difficulty in finding a third
linguistic unit satisfying the following constraints:
1. The unit must collocate closely with at least
one of the two linguistic units under differ-
entiation;
2. The unit must be characteristic of the differ-
ence between the pair.
Such approximation is meaningful in that it trans-
forms the abstract characterization into a concrete
task of finding this third linguistic unit. For ex-
ample, suppose we want to find out whether the
difference between glass and mug is subtle. The
approximation boils the answer down to the dif-
ficulty of finding a third word satisfying the two
constraints, and we may immediately conclude
that the difference between the pair is not subtle
since it is relatively easy to find wine as the quali-
fying third word, which 1) collocates closely with
glass and 2) characterizes the difference between
5The same principle applies when we replace ?genera-
tion? with ?understanding? and ?an intention? with ?a cogni-
tion?.
the pair by instantiating one of their major differ-
ences ? the purpose of use. The same reasoning
applies to concluding non-subtlety for word pairs
such as pen and pencil with sharpener, weather
and climate with forecast, watch and clock with
wrist, etc.
In contrast, for the pair forest and woods, it
might be easy to find words satisfying one but not
both constraints. Consequently, the lack of such
qualifying words ? or at least the relative diffi-
culty for finding one ? makes the difference be-
tween this pair more subtle than in the previous
examples.
We call a linguistic unit satisfying both con-
straints a collocating differentiator of subtlety
(CDS). Notably, the second constraint puts an im-
portant difference between CDSs and the conven-
tional sense of collocation. On the lexical level,
CDSs are not merely words that collocate more
with one word in a pair than with the other; they
have to be characteristic of the differences be-
tween the pair. In the example of forest and
woods, one can easily find a word exclusively col-
locating with one but not the other ? such as na-
tional forest but not *national woods; however,
unlike the CDSs in the previous examples, the
word national does not characterize any of the dif-
ferences between the pair in size, primitiveness,
1187
proximity to civilization, or wildness (Edmonds
and Hirst, 2002), and consequently fails to satisfy
the second constraint.
5.2 Relating Subtlety to Latent Space
Dimensionality6
As mentioned in Section 4.1, elements of a latent
space vector are in descending order in terms of
co-occurrence significance, i.e., the information
within the first few dimensions is obtained from
more closely collocating linguistic units. From
the two constraints in the previous section, it fol-
lows that it should be relatively easier to find a
CDS for words that can be well distinguished in a
lower-dimensional sub-space of the latent seman-
tic space, and the difference among such words
should not be considered subtle.
We thus claim that co-occurrence-based infor-
mation capable of characterizing subtle differ-
ences must then reside in higher dimensions in
the latent space vectors. Furthermore, our intu-
ition on the complexity of subtlety can also be
empirically tested by comparing the performance
of supervised and unsupervised models at differ-
ent k values. One of the differences between the
two types of models is that supervised models are
better at unraveling the convoluted inter-relations
between high-dimensional data points. Under this
assumption, if we hypothesize that subtlety is a
certain form of complex, high-dimensional rela-
tion between semantic elements, then the differ-
ence in performance between the supervised and
unsupervised model should increase as the former
recovers subtle information in higher dimensions.
As shown in Figure 2, performance of both
models is positively correlated to the number of
dimensions in the latent semantic space (with cor-
relation coefficient ? = 0.95 for supervised model
and ? = 0.81 for unsupervised model). This sug-
gests that the lexical choice process is indeed
?picking up? implicit information about subtlety
in the higher dimensions of the latent vectors.
Meanwhile, the difference between the perfor-
mance of the two models correlates strongly to k
with ? = 0.95. Significance tests on the ?differ-
6In order to keep the test data (1987 WSJ) unseen before
producing the results in Table 2, models in this section were
trained on The Brown Corpus and tested on 1988?89 WSJ.
Figure 2: Supervised performance increasing fur-
ther from unsupervised performance in higher di-
mensions.
ence of difference?7 between their performances
further reveal increasing difference in growth rate
of their performance. Significance is witnessed in
both the F-test and the paired t-test,8 indicating
that the subtlety-related information in the higher
dimensions exhibits complex clustering patterns
that are better recognized by SVMs but beyond
the capability of the KNN model.
5.3 Subtlety and the Level of Context
Our previous models on lexical LoA associated
words within the same sentence to build the co-
occurrence matrix. Lexical LoA also allows us
to associate words that co-occur in different lev-
els of context (LoC) such as paragraphs or docu-
ments. This gives an approximate measurement
of how much context a lexical LoA model uses
for word co-occurrence. Intuitively, by looking at
more context, higher LoC models should be better
at differentiating more subtle differences.
We compare the performance of models with
different LoCs in Figure 3. The sentence LoC
model constantly out-performs the paragraph LoC
model after k = 500, indicating that, by inter-
model comparison, larger LoC models do not
necessarily perform better on higher dimensions.
However, there is a noticeable difference in the
optimal dimensionality for the model perfor-
mances. Sentence LoC performance peaks around
7The italicized difference is used in its mathematical
sense as the discrete counterpart of derivative.
8F-test: f (1,16) = 9.13, p < 0.01. Paired t-test: t(8) =
4.16 with two-tailed p = 0.0031. Both conducted on 10 data
points at k = 50 to 500 with a step of 50.
1188
Figure 3: LoC in correlation to latent space di-
mensionality for optimal model performance.
k = 700 ? much lower than that of paragraph
LoC which is around k = 1,100. Such differ-
ence may suggest that, by intra-model compari-
son, each model may have its own ?comfort zone?
for the degree of subtlety it differentiates; models
on larger LoC are better at differentiating between
more subtle nuances, which is in accordance with
our intuition.
One possible explanation for sentence LoC
models outperforming paragraph LoC models is
that, although the high-dimensional elements are
weighed down by ? due to their insignificance in
the latent space, their contribution to the output
of distance function is larger in paragraph LoC
models because the vectors are much denser than
that in the sentence LoC model; since the unsuper-
vised method is incapable of recognizing the clus-
tering patterns well in high-dimensional space,
the ?amplified? subtlety information is eventually
taken as noise by the KNN model. An interesting
extension to this discussion is to see whether a su-
pervised model can consistently perform better on
higher LoC in all dimensions.
6 Conclusions and Future Work
We propose a latent semantic space representa-
tion of near-synonyms and their contexts, which
allows a thorough investigation of several aspects
of the near-synonym lexical choice problem. By
employing supervised learning on the latent space
features, we achieve an accuracy of 74.5% on the
?fill-in-the-blank? task, outperforming the current
state-of-the-art with statistical significance.
In addition, we formalize the notion of subtlety
by relating it to the dimensionality of the latent se-
mantic space. Our empirical analysis suggests that
subtle differences between near-synonyms reside
in higher dimensions in the latent semantic space
in complex clustering patterns, and that the degree
of subtlety correlates to the level of context for co-
occurrence. Both conclusions are consistent with
our intuition.
As future work, we will make better use of the
easy customization of the context representation
to compare HAL and other models with bag-of-
words models. The correlation between subtlety
and dimensionality may lead to many interesting
tasks, such as measuring the degree of subtlety for
individual near-synonyms or near-synonym sets.
With regard to context representation, it is also
intriguing to explore other dimensionality reduc-
tion methods (such as Locality Sensitive Hashing
or Random Indexing) and to compare them to the
SVD-based model.
Acknowledgment
This study is supported by the Natural Sciences
and Engineering Research Council of Canada
(NSERC). We greatly appreciate the valuable sug-
gestions and feedback from all of our anonymous
reviewers and from our colleagues Julian Brooke,
Frank Rudzicz, and George Dahl.
1189
References
Charles L. A. Clarke, Gordon Cormack, and
Christopher Palmer. An overview of MultiText.
ACM SIGIR Forum, 32(2):14?15, 1998.
D. A. Cruse. Lexical Semantics. Cambridge Uni-
versity Press, 1986.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. In-
dexing by latent semantic analysis. Journal of
the American Society for Information Science,
41(6):391?407, 1990.
Chrysanne DiMarco, Graeme Hirst, and Manfred
Stede. The semantic and stylistic differentiation
of synonyms and near-synonyms. AAAI Spring
Symposium on Building Lexicons for Machine
Translation, pages 114?121, 1993.
Philip Edmonds. Choosing the word most typi-
cal in context using a lexical co-occurrence net-
work. In Proceedings of the 35th annual meet-
ing of the Association for Computational Lin-
guistics and Eighth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 507?509, 1997.
Philip Edmonds and Graeme Hirst. Near-
synonymy and lexical choice. Computational
Linguistics, 28(2):105?144, 2002.
Diana Inkpen. A statistical model for near-
synonym choice. ACM Transactions on Speech
and Language Processing, 4(1):1?17, 2007.
Diana Inkpen and Graeme Hirst. Building and us-
ing a lexical knowledge-base of near-synonym
differences. Computational Linguistics, 32(2):
223?262, 2006.
Aminul Islam and Diana Inkpen. Near-synonym
choice using a 5-gram language model. Re-
search in Computing Sciences, 46:41?52, 2010.
Thomas Landauer and Susan Dumais. A solution
to Plato?s problem: the latent semantic analy-
sis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review,
104(2):211?240, 1997.
Shixiao Ouyang, Helena Hong Gao, and
Soo Ngee Koh. Developing a computer-
facilitated tool for acquiring near-synonyms
in Chinese and English. In Proceedings
of the Eighth International Conference on
Computational Semantics, pages 316?319,
2009.
Reinhard Rapp. The automatic generation of the-
sauri of related words for English, French, Ger-
man, and Russian. International Journal of
Speech Technology, 11(3):147?156, 2008.
Ehud Reiter and Somayajulu Sripada. Human
variation and lexical choice. Computational
Linguistics, 28(4):545?553, 2002.
Manfred Stede. Lexical choice criteria in lan-
guage generation. In Proceedings of the sixth
conference of the European Chapter of the As-
sociation for Computational Linguistics, pages
454?459, 1993.
1190
Coling 2010: Poster Volume, pages 90?98,
Beijing, August 2010
Automatic Acquisition of Lexical Formality
Julian Brooke, Tong Wang, and Graeme Hirst
Department of Computer Science
University of Toronto
{jbrooke,tong,gh}@cs.toronto.edu
Abstract
There has been relatively little work fo-
cused on determining the formality level
of individual lexical items. This study
applies information from large mixed-
genre corpora, demonstrating that signif-
icant improvement is possible over simple
word-length metrics, particularly when
multiple sources of information, i.e. word
length, word counts, and word associ-
ation, are integrated. Our best hybrid
system reaches 86% accuracy on an En-
glish near-synonym formality identifica-
tion task, and near perfect accuracy when
comparing words with extreme formality
differences. We also test our word as-
sociation method in Chinese, a language
where word length is not an appropriate
metric for formality.
1 Introduction
The derivation of lexical resources for use in
computational applications has been focused pri-
marily on the denotational relationships among
words, e.g. the synonym and hyponym relation-
ships encapsulated in WordNet (Fellbaum, 1998).
Largely missing from popular lexical resources
such as WordNet and the General Inquirer (Stone
et al, 1966) is stylistic information; there are,
for instance, no resources which provide com-
prehensive information about the formality level
of words, which relates to the appropriateness
of a word in a given context. Consider, for
example, the problem of choice among near-
synonyms: there are only minor denotational dif-
ferences among synonyms such as get, acquire,
obtain, and snag, but it is difficult to construct a
situation where any choice would be equally suit-
able. The key difference between these words is
their formality, with acquire the most formal and
snag the most informal.
In this work, we conceive of formality as
a continuous property. This approach is in-
spired by resources such as Choose The Right
Word (Hayakawa, 1994), in which differences be-
tween synonyms are generally described in rela-
tive rather than absolute terms, as well as linguis-
tic literature in which the quantification of stylis-
tic differences among genres is framed in terms of
dimensions rather than discrete properties (Biber,
1995). We begin by defining the formality score
for a word as a real number value in the range 1
to ?1, with 1 representing an extremely formal
word, and ?1 an extremely informal word. A
formality lexicon, then, gives a FS score to every
word within its coverage.
The core of our approach to the problem of
classifying lexical formality is the automated cre-
ation of formality lexicons from large corpora. In
this paper, we focus on the somewhat low-level
task of identifying the relative formality of word
pairs; we believe, however, that a better under-
standing of lexical formality is relevant to a num-
ber of problems in computational linguistics, in-
cluding sub-fields such as text generation, error
correction of (ESL) writing, machine translation,
text classification, text simplification, word-sense
disambiguation, and sentiment analysis. One con-
clusion of our research is that formality variation
is omnipresent in natural corpora, but it does not
follow that the identification of these differences
on the lexical level is a trivial one; nevertheless,
90
we are able to make significant progress using the
methods presented here, in particular the applica-
tion of latent semantic analysis to blog corpora.
2 Related Work
As far as we are aware, there are only a few
lines of research explicitly focused on the ques-
tion of linguistic formality. In linguistics proper,
the study of register and genre usually involves
a number of dimensions or clines, sometimes
explicitly identified as formality (Leckie-Tarry,
1995; Carter, 1998), or decomposed into notions
such as informational versus interpersonal con-
tent (Biber, 1995). Heyligen and Dewaele (1998)
provide a part-of-speech based quantification of
textual contextuality (which they argue is funda-
mental to the notion of formality); their metric
has been used, for instance, in a computational
investigation of the formality of online encyclo-
pedias (Emigh and Herring, 2005). In this kind
of quantification, however, there is little, if any,
focus on individual elements of the lexicon. In
computational linguistics, formality has received
attention in the context of text generation (Hovy,
1990); of particular note relevant to our research
is the work of Inkpen and Hirst (2006), who de-
rive boolean formality tags from Choose the Right
Word (Hayakawa, 1994). Like us, their focus was
improved word choice, though the approach was
much broader, also including dimensions such as
polarity. An intriguing example of formality rel-
evant to text classification is the use of infor-
mal language (slang) to help distinguish true news
from satire (Burfoot and Baldwin, 2009).
Our approach to this task is inspired and in-
formed by automatic lexical acquisition research
within the field of sentiment analysis (Turney
and Littman, 2003; Esuli and Sebastiani, 2006;
Taboada and Voll, 2006; Rao and Ravichandra,
2009). Turney and Littman (2003) apply latent
semantic analysis (LSA) (Landauer and Dumais,
1997) and pointwise mutual information (PMI) to
derive semantic orientation ratings for words us-
ing large corpora; like us, they found that LSA
was a powerful technique for deriving this lexical
information. The lexical database SentiWordNet
(Esuli and Sebastiani, 2006) provides 0?1 rank-
ings for positive, negative, and neutral polarity,
derived automatically using relationships between
words in WordNet (Fellbaum, 1998). Unfortu-
nately, WordNet synsets tend to cut across the for-
mal/informal distinction, and so the resource is
not obviously useful for our task.
The work presented here builds directly on a pi-
lot study (Brooke et al, 2010), the focus of which
was the construction of formality score (FS) lex-
icons. In that work, we employed less sophis-
ticated forms of some of the methods used here
in a relatively small dataset (the Brown Corpus),
providing a proof of concept, but with poor cov-
erage, and with no attempt to combine the meth-
ods to maximize performance. However, the small
dataset alowed us to do a thorough test of certain
options associated with our task. In particular we
found that using a similarity metric based on LSA
gave good performance across our test sets, es-
pecially when the term-document matrix was bi-
nary (unweighted), the k-value used for LSA was
small, and the method used to derive a formality
score was cosine similarity to our seed terms. A
metric using total word counts in corpora with di-
vergent formality also showed promise, with both
methods performing above our word-length base-
line for words within their coverage. PMI, by
comparison, proved less effective, and we do not
pursue it further here.
3 Data and Resources
3.1 Word Lists
All the word lists discussed here are publicly
available.1 We begin with two, one formal and
one informal, that we use both as seeds for our
lexicon construction methods and as test sets for
evaluation (our gold standard). We assume that
all slang terms are by their very nature informal
and so our 138 informal seeds were taken primar-
ily from an online slang dictionary2 (e.g. wuss,
grubby) and also include some contractions and
interjections (e.g. cuz, yikes). The 105 formal
seeds were selected from a list of discourse mark-
ers (e.g. moreover, hence) and adverbs from a sen-
timent lexicon (e.g. preposterously, inscrutably);
these sources were chosen to avoid words with
1 http://www.cs.toronto.edu/?jbrooke/FormalityLists.zip
2 http://onlineslangdictionary.com/
91
overt topic, and to ensure that there was some
balance of sentiment across formal and informal
seed sets. Part of speech, however, is not balanced
across our seed sets.
Another test set we use to evaluate our methods
is a collection of 399 pairs of near-synonyms from
Choose the Right Word (CTRW), a manual for as-
sisting writers with synonym word choice; each
pair was either explicitly or implicitly compared
for formality in the book. Implicit comparison in-
cluded statements such as this is the most formal
of these words; in those cases, and more gener-
ally, we avoided words appearing in more than
one comparison (there are no duplicate words in
our CTRW set), as well as multiword expressions
and words whose formality is strongly ambigu-
ous (i.e. word-sense dependent). An example of
this last phenomenon is the word cool, which is
used colloquially in the sense of good but more
formally as in the sense of cold. Partly as a re-
sult of this polysemy, which is clearly more com-
mon among informal words, our pairs are biased
toward the formal end of the spectrum; although
there are some informal comparisons, e.g. belly-
ache/whine, wisecrack/joke, more typical pairs
include determine/ascertain and hefty/ponderous.
Despite this imbalance, one obvious advantage
of using near-synonyms in our evaluation is that
factors other than linguistic formality (e.g. topic,
opinion) are less likely to influence performance.
In general, the CTRW allows for a more objective,
fine-grained evaluation of our methods, and is ori-
ented towards our primary interest, near-synonym
word choice.
To test the performance of our unsupervised
method beyond English, one of the authors (a na-
tive speaker of Mandarin Chinese) created two
sets of Chinese two-character words, one formal,
one informal, based on but not limited to the
words in the English sets. The Chinese seeds in-
clude 49 formal seeds and 43 informal seeds.
3.2 Corpora
Our corpora fall generally into three categories:
formal (written) copora, informal (spoken) cor-
pora, and mixed corpora. The Brown Corpus
(Francis and Kuc?era, 1982), our development cor-
pus, is used here both as a formal and mixed cor-
pus. Although extremely small by modern cor-
pus standards (only 1 million words), the Brown
Corpus has the advantage of being compiled ex-
plicitly to represent a range of American English,
though it is all of the published, written variety.
The Switchboard (SW) Corpus is a collection of
American telephone conversations (Godfrey et al,
1992), which contains roughly 2400 conversations
with over 2.6 million word tokens; we use it as an
informal counterpart to the Brown Corpus. Like
the Brown Corpus, The British National Corpus
(Burnard, 2000) is a manually-constructed mixed-
genre corpus; it is, however, much larger (roughly
100 million words). It contains a written portion
(90%), which we use as a formal corpus, and a
spontaneous spoken portion (4.3%), which we use
as an informal corpus. Our other mixed corpora
are two blog collections available to us: the first,
which we call our development blog corpus (Dev-
Blog) contains a total of over 900,000 English
blogs, with 216 million tokens.3 The second is the
?first tier? English blogs included in the publicly
available ICSWM 2009 Spinn3r Dataset (Burton
et al, 2009), a total of about 1.3 billion word to-
kens in 7.5 million documents. For our investiga-
tions in Chinese, we use the Chinese portion of the
ICSWM blogs, approximately 25.4 million char-
acter tokens in 86,000 documents.
4 Methods
4.1 Simple Formality Measures
The simplest kind of formality measure is based
on word length, which is often used directly as
an indicator of formality for applications such as
genre classification (Karlgren and Cutting, 1994).
Here, we use logarithmic scaling to derive a FS
score based on word length. Given a maximum
word length L4 and a word w of length l, the for-
mality score function, FS(w), is given by:
FS(w) =?1+2 log llogL
3These blogs were gathered by the University of Toronto
Blogscope project (www.blogscope.net) over a week in May
2008.
4We use an upper bound of 28 characters, which is
the length of antidisestablishmentarianism, the prototypical
longest word in English; this value of L provides an appropri-
ate formality/informality threshold, between 5- and 6-letter
words
92
For hyphenated terms, the length of each compo-
nent is averaged. Though this metric works rela-
tively well for English, we note that it is problem-
atic in a language with significant word aggluti-
nation (e.g. German) or without an alphabet (e.g.
Chinese, see below).
Another straightforward method is the assump-
tion that Latinate prefixes and suffixes are indica-
tors of formality in English (Kessler et al, 1997),
i.e. informal words will not have Latinate affixes
such as -ation and intra-. Here, we simply assign
words that appear to have such a prefix or suffix
an FS of 1, and all other words an FS of ?1.
Our frequency methods derive FS from word
counts in corpora. Our first, naive approach as-
sumes a single corpus, where either formal words
are common and informal words are rare, or vice
versa. To smooth out the Zipfian distribution, we
use the frequency rank of words as exponentials;
for a corpus with R frequency ranks, the FS for a
word of rank r under the formal is rare assumption
is given by:
FS(w) =?1+2 e
(r?1)
e(R?1)
Under the informal is rare assumption:
FS(w) = 1?2 e
(r?1)
e(R?1)
We have previously shown that these methods are
not particularly effective on their own (Brooke et
al., 2010), but we note that they provide useful
information for a hybrid system.
A more sophisticated method is to use two cor-
pora that are known to vary with respect to for-
mality and use the relative appearance of words in
each corpus as the metric. If word appears n times
in a (relatively) formal corpus and m times in an
informal corpus (and one of m, n is not zero), we
derive:
FS(w) =?1+2 nm?N +n
Here, N is the ratio of the size (in tokens) of the
informal corpus (IC) to the formal corpus (FC).
We need the constant N so that an imbalance in
the size of the corpora does not result in an equiv-
alently skewed distribution of FS.
4.2 Latent Semantic Analysis
Next, we turn to LSA, a technique for extracting
information from a large corpus of texts by (dras-
tically) reducing the dimensionality of a term?
document matrix, i.e. a matrix where the row vec-
tors correspond to the appearance or (weighted)
frequency of words in a set of texts. In essence,
LSA simplifies the variation of words across a col-
lection of texts, exploiting document?document
correlation to produce information about the k
most important dimensions of variation (k < to-
tal number of documents), which are generally
thought to represent semantic concepts, i.e. topic.
The mathematical basis for this transformation is
singular value decomposition5; for the details of
the matrix transformations, we refer the reader to
the discussion of Turney and Littman (2003). The
factor k, the number of columns in the compacted
matrix, is an important variable in any application
of LSA, one is generally determined by trial and
error (Turney and Littman, 2003).
LSA is computationally intensive; in order to
apply it to extremely large blog corpora, we need
to filter the documents and terms before build-
ing our term?document matrix. We adopt the
following strategy: to limit the number of docu-
ments in our term?document matrix, we first re-
move documents less than 100 tokens in length,
with the rationale that these documents provide
less co-occurrence information. Second, we re-
move documents that either do not contain any
target words (i.e. one of our seeds or CTRW test
words), or contain only target words which are
among the most common 20 in the corpus; these
documents are less likely to provide us with use-
ful information, and the very common target terms
will be well represented regardless. We further
shrink the set of terms by removing all hapax
legomena; a single appearance in a corpus is not
enough to provide reliable co-occurrence informa-
tion, and roughly half the words in our blog cor-
pora appear only once. Finally, we remove sym-
bols and all words which are not entirely lower
5We use the implementation included in Matlab; we take
the rows of the decomposed U matrix weighted by the sin-
gular values in ? for our word vectors. Using no weights
or ??1 generally resulted in worse performance, particularly
with the CTRW sets.
93
case; we are not interested, for instance, in num-
bers, acronyms, and proper nouns. We can esti-
mate the effect this filtering has on performance
by testing it both ways in a development corpus.
Once a k-dimensional vector for each relevant
word is derived using LSA, a standard method is
to use the cosine of the angle between a word vec-
tor and the vectors of seed words to identify how
similar the distribution of the word is to the distri-
bution of the seeds. To begin, each formal seed is
assigned a FS value of 1, each informal seed a FS
value of ?1, and then a raw seed similarity score
(FS?) is calculated for each word w:
FS?(w) = ?
s?S,s6=w
Ws?FS(s)? cos(?(w,s))
S is the set of all seeds. Note that seed terms are
excluded from their own FS calculation, this is
equivalent to leave-one-out cross-validation. Ws
is a weight that depends on whether s is a formal
or informal seed, Wi (for informal seeds) is calcu-
lated as:
Wi = ? f?F FS( f )|?i?I FS(i)|+? f?F FS( f )
and Wf (for formal seeds) is:
Wf = |?i?I FS(i)||?i?I FS(i)|+? f?F FS( f )
Here, I is the set of all informal seeds, and F is the
set of all formal seeds. These weights have the ef-
fect of countering any imbalance in the seed set,
as formal and informal seeds ultimately have the
same (potential) influence on each word, regard-
less of their count. This weighting is necessary for
the iterative extension of this method discussed in
the next section.
We calculate the final FS score as follows:
FS(w) = FS
?(w)?FS?(r)
Nw
The word r is a reference term, a common func-
tion word that has no formality.6 This has the ef-
fect of countering any (moderate) bias that might
6The particular choice of this word is relatively unimpor-
tant; common function words all have essentially the same
LSA vectors because they appear at least once in nearly ev-
ery document of any size. For English, we chose r = and,
and for Chinese, r = yinwei (because); there does not seem
to be an obvious two-character, formality-neutral equivalent
to and in Chinese.
exist in the corpus; in the Brown Corpus, for in-
stance, function words have positive formality be-
fore this step, simply because formal words oc-
curred more often in the corpus. Nw is a normal-
ization factor, either
Nw = maxwi?I? |FS
?(wi)?FS?(r)|
for all wi ? I? or
Nw = maxw f?F ? |FS
?(w f )?FS?(r)|
for all w f ? F ?. I? contains all words w such that
FS?(w)?FS?(r) < 0, and F ? contains all words w
such that FS?(w)?FS?(r) > 0. This ensures that
the resulting lexicon has terms exactly in the range
1 to?1, with the reference word r at the midpoint.
We also tested the LSA method in Chinese.
The only major relevant difference between Chi-
nese and English is word segmentation: Chinese
does not have spaces between words. To sidestep
this problem, we simply included all character bi-
grams found in our corpus. The drawback of this
approach in the inclusion of a huge number of
nonsense ?words? (1.3 million terms in just 86,000
documents), however we are at least certain to
identify all instances of our seeds.
4.3 Hybrid Methods
There are a number of ways to leverage the infor-
mation we derive from our basic methods. One
intriguing option is to use the basic FS measures
as the starting point for an iterative process using
the LSA cosine similarity. Under this paradigm,
all words in the starting FS lexicon are potential
seed words; we choose a cutoff value for inclu-
sion in the seed word set (e.g. words which have
at least .5 or ?.5 FS), and then carry out the co-
sine calculations, as above, to derive new FS val-
ues (a new FS lexicon). We can repeat this process
as many times as required, with the idea that the
connections between various words (as reflected
in their LSA-derived vectors) will cause the sys-
tem to converge towards the true FS values.
A simple hybrid method that combines the two
word count models uses the ratio of word counts
in two corpora to define the center of the FS spec-
trum, but single corpus methods to define the ex-
tremes. Formally, if m and n (word counts for the
94
informal corpus IC and formal corpus FC, respec-
tively) are both non-zero, then FS is given by:
FS(w) =?0.5+ nm?N +n
However, if n is zero, FS is given by:
FS(w) =?1+0.5 e
?rIC?1
e?RIC?1
where rIC is the frequency rank of the word in IC,
and RIC is the total number of ranks in IC. If m is
zero, FS is given by:
FS(w) = 1?0.5 e
?rFC?1
e?RFC?1
where i is the rank of the word in IC, and RIC is the
total number of frequency ranks in IC). This func-
tion is undefined in the case where m and n are
both zero. Intuitively, this is a kind of backoff, re-
lying on the idea that words of extreme formality
are rare even in a corpus of corresponding formal-
ity, whereas words in the core vocabulary (Carter,
1998), which are only moderately formal, will ap-
pear in all kinds of corpora, and thus are amenable
to the ratio method.
Finally, we explore a number of ways to com-
bine lexicons directly. The motivation for this
is that the lexicons have different strengths and
weaknesses, representing partially independent
information. An obvious method is an averag-
ing or other linear combination of the scores, but
we also investigate vote-based methods (requiring
agreement among n dictionaries). Beyond these
simple options, we test support vector machines
and naive Bayes classification using the WEKA
software suite (Witten and Frank, 2005), applying
10-fold cross-validation using default WEKA set-
tings for each classifier. The features here are task
dependent (see Section 5); for the pairwise task,
we use the difference between the FS value of the
words in each lexicon, rather than their individ-
ual scores. Finally, we can use the weights from
the SVM model of the CTRW (pairwise) task to
interpolate an optimal formality lexicon.
5 Evaluation
We evaluate our methods using the gold standard
judgments from the seed sets and CTRW word
pairs. To differentiate the two, we continue to use
the term seed for the former; in this context, how-
ever, these ?seed sets? are being viewed as a test
set (recall that our LSA method is equivalent to
leave-one-out cross-validation).
We derive the following measures: first, the
coverage (Cov.) is the percentage of words in the
set that are covered under the method. The class-
based accuracy (C-Acc.) of our seed sets is the
percentage of covered words which are correctly
classified as formal (FS > 0) or informal (FS <
0). The pair-based accuracy (P-Acc.) is the result
of exhaustively pairing words in the two seed sets
and testing their relative formality; that is, for all
wi ? I and w f ? F , the percentage of wi/w f pairs
where FS(wi) < FS(w f ). For the CTRW pairs
there are only two metrics, the coverage and the
pair-based accuracy; since the CTRW pairs repre-
sent relative formality of varying degrees, it is not
possible to calculate a class-based accuracy.
The first section of Table 1 provides the re-
sults for the basic methods in various corpora.
The word length (1) and morphology-based (2)
methods provide good coverage, but poor accu-
racy, while the word count ratio methods (3?4) are
fairly accurate, but suffer from low coverage. The
LSA results in Table 1 are the best for each corpus
across the k values we tested. When both cover-
age and accuracy are considered, there is a clear
benefit associated with increasing the amount of
data, though the difference between the Dev-Blog
and ICWSM suggests diminishing returns. The
performance of the filtered Dev-Blog is actually
slightly better than the unfiltered versions (though
there is a drop in coverage), suggesting that filter-
ing is a good strategy.
In our previous work (Brooke et al, 2010), we
noted that CTRW set performance in the Brown
dropped for k > 3, while performance on the seed
set was mostly steady as k increased. Figure 1
shows the pairwise performance of each test set
for various corpora across various k. The results
here are similar; all three corpora reach a CTRW
maximum at a relatively low k values (though
higher than Brown Corpus); however the seed set
performance in each corpus continues to improve
(though marginally) as k increases, while CTRW
performance drops. An explanation for this is that
95
Table 1: Seed coverage, class-based accuracy, pairwise accuracy, CTRW coverage, and pairwise accu-
racy for various FS lexicons and hybrid methods (%).
Seed set CTRW set
Method Cov. C-Acc. P-Acc. Cov. P-Acc.
Simple
(1) Word length 100 86.4 91.8 100 63.7
(2) Latinate affix 100 74.5 46.3 100 32.6
(3) Word count ratio, Brown and Switchboard 38.0 81.5 85.7 36.0 78.2
(4) Word count ratio, BNC Written vs. Spoken 60.9 89.2 97.3 38.8 74.3
(5) LSA (k=3), Brown 51.0 87.1 94.2 59.6 73.9
(6) LSA (k=10), BNC 94.7 83.0 98.3 96.5 69.4
(7) LSA (k=20), Dev-Blog 100 91.4 96.8 99.0 80.5
(8) LSA (k=20), Dev-Blog, filtered 99.0 92.1 97.0 97.7 80.5
(9) LSA (k=20), ICWSM, filtered 100 93.0 98.4 99.7 81.9
Hybrid
(10) BNC ratio with backoff (4) 97.1 78.8 75.7 97.0 78.8
(11) Combined ratio with backoff (3 + 4) 97.1 79.2 79.9 97.5 79.9
(12) BNC weighted average (10,6), ratio 2:1 97.1 83.5 90.0 97.0 83.2
(13) Blog weighted average (9,7), ratio 4:1 100 93.8 98.5 99.7 83.4
(14) Voting, 3 agree (1, 6, 7, 9, 11) 92.6 99.1 99.9 87.0 91.6
(15) Voting, 2 agree (1, 11, 13) 86.8 99.1 100 81.5 96.9
(16) Voting, 2 agree (1, 12, 13) 87.7 98.6 100 82.7 97.3
(17) SVM classifier (1, 2, 6, 7, 9, 11) 100 97.9 99.9 100 84.2
(18) Naive Bayes classifier (1, 2, 6, 7, 9, 11) 100 97.5 99.8 100 83.9
(19) SVM (Seed, class) weighted (1, 2, 6, 7, 9, 11) 100 98.4 99.8 100 80.5
(20) SVM (CTRW) weighted (1, 6, 7, 9, 11) 100 93.0 99.0 100 86.0
(21) Average (1, 6, 7, 9, 11) 100 95.9 99.5 100 84.5
Figure 1: Seed and CTRW pairwise accuracy,
LSA method for large corpora k, 10? k ? 200.
the seed terms represent extreme examples of for-
mality; thus there are numerous semantic dimen-
sions to distinguish them. However, the CTRW
set includes near-synonyms, many with only rel-
atively subtle differences in formality; for these
pairs, it is important to focus on the core di-
mensions relevant to formality, which are among
the first discovered in a factor analysis of mixed-
register texts (Biber, 1995).
With regards to hybrid methods, we first briefly
summarize our testing with the iterative model,
which included extensive experiments using ba-
sic lexicons and the LSA vectors derived from
the Brown Corpus, and some targeted testing with
the blog corpora (iteration on these corpora is
extraordinarily time-consuming). In general, we
found only that there were only small, inconsis-
tent benefits to be gained from the iterative ap-
96
proach. More generally, the intuition behind the
iterative method, i.e. that performance would in-
crease with an drastic increase in the number of
seeds, was found to be flawed: in other testing,
we found that we could randomly remove most
of the seeds without negatively affecting perfor-
mance. Even at relatively high k values, it seems
that a few seeds are enough to calibrate the model.
The ratio (with backoff) hybrid built from the
BNC (10) provides CTRW performance that is
comparable the best LSA models, though perfor-
mance in the seed sets is somewhat poor; supple-
menting with word counts from the Brown Cor-
pus and Switchboard Corpus provides a small im-
provement (11). The weighed hybrid dictionar-
ies in (12,13) demonstrate that it is possible to ef-
fectively combine lexicons built using two differ-
ent methods on the same corpus (12) or the same
method on different corpora (13); the former, in
particular, provides an impressive boost to CTRW
accuracy, indicating that word count and word as-
sociation methods are partially independent.
The remainder of Table 1 shows the best re-
sults using voting, averaging, and weighting. The
voting results (14?16) indicate that it is possible
to sacrifice some coverage for very high accu-
racy in both sets, including a near-perfect score
in the seed sets and significant gains in CTRW
performance. In general, the best accuracy with-
out a significant loss of coverage came from 2
of 3 voting (15?16), using dictionaries that rep-
resented our three basic sources of information
(word length, word count, and word associa-
tion). The machine learning hybrids (17?18) also
demonstrate a marked improvement over any sin-
gle lexicon, though it is important to note that
each accuracy score here reflects a different task-
specific model. Hybrid FS lexicons built with the
weights learned by the SVM models (19?20) pro-
vide superior performance on the task correspond-
ing to the model used, though the simple averag-
ing of the best dictionaries (21) also provides good
performance across all evaluation metrics.
Finally, the LSA results for Chinese are mod-
est but promising, given the relatively small scale
of our experiments: we saw a pairwise accuracy of
82.2%, with 79.3% class-based accuracy (k = 10).
We believe that the main reason for the generally
lower performance in Chinese (as compared to
English) is the modest size of the corpus, though
our simplistic character bigram term extraction
technique may also play a role. As mentioned,
smaller seed sets do not seem to be an issue. Inter-
estingly, the class-based accuracy is 10.8% lower
if no reference word is used to calibrate the divide
between formal and informal, suggesting a rather
biased corpus (towards informality); in English,
by comparison, the reference-word normalization
had a slightly negative effect on the LSA results,
though the effect mostly disappeared after hy-
bridization. The obvious next step is to integrate a
Chinese word segmenter, and use a larger corpus.
We could also try word count methods, though
finding appropriate (balanced) resouces similar to
the BNC might be a challenge; (mixed) blog cor-
pora, on the other hand, are easily collected.
6 Conclusion
In this work, we have experimented with a number
of different methods and source corpora for deter-
mining the formality level of lexical items, with
the implicit goal of distinguishing the formality of
near-synonym pairs. Our methods show marked
improvement over simple word-length metrics;
when multiple sources of information, i.e. word
length, word counts, and word association, are in-
tegrated, we are able to reach over 85% perfor-
mance on the near-synonym task, and close to
100% accuracy when comparing words with ex-
treme formality differences; our voting methods
show that even higher precision is possible. We
have also demonstrated that our LSA word associ-
ation method can be applied to a language where
word length is not an appropriate metric of for-
mality, though the results here are preliminary.
Other potential future work includes addressing a
wider range of phenomena, for instance assign-
ing formality scores to morphological elements,
syntactic cues, and multi-word expressions, and
demonstrating that a formality lexicon can be use-
fully applied to other NLP tasks.
Acknowledgements
This work was supported by Natural Sciences and
Engineering Research Council of Canada. Thanks
to Paul Cook for his ICWSM corpus API.
97
References
Biber, Douglas. 1995. Dimensions of Register Vari-
ation: A cross-linguistic comparison. Cambridge
University Press.
Brooke, Julian, Tong Wang, and Graeme Hirst. 2010.
Inducing lexicons of formality from corpora. In
Proceedings of the Language Resources and Eval-
uation Conference (LREC ?10), Workshop on Meth-
ods for the automatic acquisition of Language Re-
sources and their evaluation methods.
Burfoot, Clint and Timothy Baldwin. 2009. Auto-
matic satire detection: Are you having a laugh? In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computationsl
Linguistics and the 4th International Joint Confer-
ence on Nautral Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP ?09), Short Papers, Singapore.
Burnard, Lou. 2000. User reference guide for British
National Corpus. Technical report, Oxford Univer-
sity.
Burton, Kevin, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 Spinn3r Dataset. In Proceedings
of the Third Annual Conference on Weblogs and So-
cial Media (ICWSM 2009), San Jose, CA.
Carter, Ronald. 1998. Vocabulary: applied linguistic
perspectives. Routledge, London.
Emigh, William and Susan C. Herring. 2005. Col-
laborative authoring on the web: A genre analysis
of online encyclopedias. In Proceedings of the 38th
Annual Hawaii International Conference on System
Sciences (HICSS ?05).
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tion Conference on Language Resources and Eval-
uation(LREC), Genova, Italy.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press.
Francis, Nelson and Henry Kuc?era. 1982. Frequency
Analysis of English Usage: Lexicon and Grammar.
Houghton Mifflin, Boston.
Godfrey, J.J., E.C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research
and development. IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
1:517?520.
Hayakawa, S.I., editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised
by Eugene Ehrlich.
Heylighen, Francis and Jean-Marc Dewaele. 2002.
Variation in the contextuality of language: An em-
pirical measure. Foundations of Science, 7(3):293?
340.
Hovy, Eduard H. 1990. Pragmatics and natural lan-
guage generation. Artificial Intelligence, 43:153?
197.
Inkpen, Diana and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym
differences. Computational Linguistics, 32(2):223?
262.
Karlgren, Jussi and Douglas Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics, pages 1071?
1075.
Kessler, Brett, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, pages
32?38.
Landauer, Thomas K. and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic anal-
ysis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104:211?240.
Leckie-Tarry, Helen. 1995. Language Context: a
functional linguistic theory of register. Pinter.
Rao, Delip and Deepak Ravichandra. 2009. Semi-
supervised polarity lexicon induction. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
gusitics, Athens, Greece.
Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Taboada, Maite and Kimberly Voll. 2006. Methods
for creating semantic orientation dictionaries. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), Gen-
ova, Italy.
Turney, Peter and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco.
98
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 753?761, Dublin, Ireland, August 23-29 2014.
Unsupervised Multiword Segmentation of Large Corpora using
Prediction-Driven Decomposition of n-grams
Julian Brooke
*?
Vivian Tsang
?
Graeme Hirst
*
Fraser Shein
*?
*
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
gh@cs.toronto.edu
?
Quillsoft Ltd.
Toronto, Canada
vtsang@quillsoft.ca
fshein@quillsoft.ca
Abstract
We present a new, efficient unsupervised approach to the segmentation of corpora into multiword
units. Our method involves initial decomposition of common n-grams into segments which max-
imize within-segment predictability of words, and then further refinement of these segments into
a multiword lexicon. Evaluating in four large, distinct corpora, we show that this method cre-
ates segments which correspond well to known multiword expressions; our model is particularly
strong with regards to longer (3+ word) multiword units, which are often ignored or minimized
in relevant work.
1 Introduction
Identification of multiword units in language is an active but increasingly fragmented area of research, a
problem which can limit the ability of others to make use of units beyond the level of the word as input
to other applications. General research on word association metrics (Church and Hanks, 1990; Smadja,
1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010), though increasingly comprehensive in
its scope, has mostly failed to identify a single best choice, leading some to argue that the variety of
multiword phenomena must be tackled individually. For instance, there is a body of research focusing
specifically on collocations that are (to some degree) non-compositional, i.e. multiword expressions (Sag
et al., 2002; Baldwin and Kim, 2010), with individual projects often limited to a particular set of syntactic
patterns, e.g. verb-noun combinations (Fazly et al., 2009). A major issue with approaches involving
statistical association is that they rarely address expressions larger than 2 words (Heid, 2007); in corpus
linguistics, larger sequences referred to as lexical bundles are extracted using an n-gram frequency cutoff
(Biber et al., 2004), but the frequency threshold is typically set very high so that only a very limited set
is extracted. Another drawback, common to almost all these methods, is that they rarely offer an explicit
segmentation of a text into multiword units, which would be preferable for downstream uses such as
probabilistic distributional semantics. An exception is the Bayesian approach of Newman et al. (2012),
but their method does not scale well (see Section 2). Our own long-term motivation is to identify a wide
variety of multiword units for assisting language learning, since correct use of collocations is known to
pose a particular challenge to learners (Chen and Baker, 2010).
Here, we present a multiword unit segmenter
1
with the following key features:
? It is entirely unsupervised.
? It offers both segmentation of the input corpus and a lexicon which can be used to segment new
corpora.
? It is scalable to very large corpora, and works for a variety of corpora.
? It is language independent.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The software is available at http:/www.cs.toronto.edu/
?
jbrooke/ngram decomp seg.py .
753
? It does not inherently limit possible units with respect to part-of-speech or length.
? It has a bare minimum of parameters, and can be used off-the-shelf: in particular, it does not require
the choice of an arbitrary cutoff for some uninterpretable statistical metric.
? It does, however, include a parameter fixing the minimum number of times that a valid multiword
unit will appear in the corpus, which ensures sufficient usage examples for relevant applications.
Our method involves three major steps: extraction of common n-grams, initial segmentation of the
corpus, and a refinement of the resulting lexicon (and, by extension, the initial segmentation). The
latter two steps are carried out using a simple but novel heuristic based on maximizing word prediction
within multiword segments. Importantly, our method requires just a few iterations through the corpus,
and in practice these iterations can be parallelized. Evaluating with an existing set of multiword units
from WordNet in four large corpora from distinct genres, we show that our initial segmentation offers
extremely good subsumption of known collocations, and after lexicon refinement the model offers a
good trade-off between subsumption and exact matches. We also evaluate a sample of our multiword
vocabulary using crowdsourcing, and offer a qualitative analysis.
2 Related Work
In computational linguistics, there is a large body of research that proposes and/or evaluates lexical as-
sociation measures for the creation of multiword lexicons (Church and Hanks, 1990; Smadja, 1993;
Schone and Jurafsky, 2001; Evert, 2004): there are many more measures than can be addressed here?
work by Pecina (2010) considered 82 variations?but popular choices include the t-test, log likelihood,
and pointwise mutual information (PMI). In order to build lexicons using these methods, particular syn-
tactic patterns and thresholds for the metrics are typically chosen. Many of the statistical metrics do not
generalize at all beyond two words, but PMI (Church and Hanks, 1990), the log ratio of the joint proba-
bility to the product of the marginal probabilities, is a prominent exception. Other measures specifically
designed to address collocations of larger than two words include the c-value (Frantzi et al., 2000), a
metric designed for term extraction which weights term frequency by the log length of the n-gram while
penalizing n-grams that appear in frequent larger ones, and mutual expectation (Dias et al., 1999), which
produces a normalized statistic that reflects how much a candidate phrase resists the omission of any
particular word. Another approach is to simply to combine known n? 1 collocations to form n-length
collocations (Seretan, 2011), but this is based on the assumption that all longer collocations are built up
from shorter ones?idioms, for instance, do not usually work in that way.
An approach used in corpus linguistics which does handle naturally longer sequences is the study
of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency
threshold. This includes larger phrasal chunks that would be missed by traditional collocation extraction,
and so research in this area has tended to focus on how particular phrases (e.g. if you look at) are indi-
ciative of particular genres (e.g. university lectures). In order to get very reliable phrases, the threshold
is typically set high enough (Biber et al. use 40 occurrences in 1 million words) to filter out the vast
majority of expressions in the process.
With respect to the features of our model, the work closest to ours is probably that of Newman et al.
(2012). Like us, they offer an unsupervised solution, in their case a generative Dirichlet Process model
which jointly creates a segmentation of the corpus and a multiword term vocabulary. Their method,
however, requires full Gibbs sampling with thousands of iterations through the corpus (Newman et al.
report using 5000), an approach which is simply not tractable for the large corpora that we address in
this paper (which are roughly 1000 times larger than theirs). Though the model is general, their focus is
limited to term extraction, and for larger terms they compare only with the c-value approach of Frantzi
et al. (2000). Other closely related work includes general tools available for creating multiword lexicons
using association measures or otherwise exploring the collocational behavior of words (Kilgarriff and
Tugwell, 2001; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Pedersen et al., 2011). Other related
but distinct tasks include syntactic chunking (Abney, 1991) and word segmentation for Asian languages,
in particular Chinese (Emerson, 2005).
754
3 Method
3.1 Prediction-based segmentation
Our full method consists of multiple independent steps, but it is based on one central and relatively simple
idea that we will introduce first. Given a sequence of words, w
1
. . .w
n
, and statistics (i.e. n-gram counts)
about the use of these words in a corpus, we first define p(w
i
|w
j,k
) as the conditional probability of some
word w
i
appearing with some contextual subsequence w
j
. . .w
i?1
,w
i+1
. . .w
k
,1 ? j ? i ? k ? n. In the
case i = j = k, this is simply the marginal probability, p(w
i
). We then define the word predictability
of some w
i
in the context w
1,n
as the log of the maximal conditional probability of the word across all
possible choices of j and k:
pred(w
i
,w
1,n
) = max
j,k
log p(w
i
|w
j,k
)
We can define predictability for the entire sequence then as:
pred(w
1,n
) =
n
?
i=1
pred(w
i
,w
1,n
)
Now we consider the case where we have a set of possible segmentations S of the sequence, where
each segmentation s ? S can be viewed as a (possibly empty) set of segment boundaries ?s
0
,s
1
, . . . ,s
m
?.
Among the available options, our optimal segmentation is:
argmax
s?S
m?1
?
i=0
pred(w
s
i
,s
i+1
?1
)
That is, we will prefer the segmentation which maximizes the overall predictability of each word in the
sequence, under the restriction that we only predict words using the context within their segments. This
reflects our basic assumption that words within a good segment, i.e. a multiword unit, are (much) more
predictive of each other than words outside a unit. Note that if our probabilities are calculated from the
full set of n-gram counts for the corpus being segmented and the set of possible segmentations S is not
constrained, a segmentation with a smaller number of breaks will generally be preferred over one with
more breaks. However, in practice we will be greatly constraining S and also using probabilities based
on only a subset of all the information in the corpus.
3.2 Extraction of n-grams
In order to carry out a segmentation of the corpus using this method, we first need to extract statistics
in the form of n-gram counts. Given a minimum occurrence threshold, this can be done efficiently even
for large corpora in an iterative fashion until all n-grams have been extracted. For all our experiments
here, we limit ourselves to n-grams that appear at least once in 10 million tokens, and we did not collect
n-grams for n > 10 (which are almost always the result of duplication of texts in the corpus). For the pur-
poses of calculating conditional probabilities given surrounding context in our predictive segmentation,
we collected both standard n-grams as well as (for n? 3) skip n-grams with a missing word (e.g. basic *
processes where the asterisk indicates that any word could appear in that slot). Here we use lower-cased
unlemmatized tokens, excluding punctuation, though for languages with more inflectional morphology
than English, lemmatization would be advised.
3.3 Initial segmentation
Given these n-gram statistics, our initial segmentation proceeds as follows: For each sentence in the
corpus, we identify all maximum length n-grams in the sentence, i.e. all those n-grams for n ? 2 where
there is no larger n-gram which contains them while still being above our threshold of occurrence. These
n-grams represent the upper bound of our segmentation: we will never break into segments larger than
these. However, there are many overlaps among these n-grams (in fact, with a low threshold the vast
majority of n-grams overlap with at least one other), and for proper segmentation we need to resolve
755
Figure 1: Three-step procedure for n-gram decomposition into multiword units. a) shows the maximal
n-grams identified in the sentence, b) is the segmentation after the initial pass of the corpora, and c)
shows further decomposition of segments after a pass through the lexicon resulting from b).
all overlaps between these maximal n-grams by inserting at least one break. For this we apply our
prediction-based decomposition technique. In our discussion in Section 3.1, we did not consider how
the possible segmentations were selected, but now we can be explicit: the set S consists of all possible
segmentations which minimally resolve all n-gram overlaps. By minimally resolve, we mean that the
removal of any breakpoint from our set would result in an unresolved overlap: in short, there are no
extraneous breaks, and therefore no cases where a possible set of breaks is a subset of another possible
set. Figure 1a shows a real example: if we just consider the last three maximal n-grams, there are two
possible minimal breaks: a single break between in and basic or two breaks, one between roles and in
and one between basic and cellular.
Rather than optimizing over all possible breaks over the whole sentence, which is computationally
problematic, we simplify the algorithm somewhat by moving sequentially through each n-gram over-
lap in the sentence, taking any previous breaks as given while considering only the minimum breaks
necessary to resolve any overlaps that directly influence the segmentation of the two overlapping spans
under consideration, which is to say any other overlapping spans which contain at least one word also
contained in at least one of overlapping spans under consideration. For example, in Figure 1a we first
deal independently with each of the first two overlaps (the spans modified with glucose and are enriched
in lipid rafts, and then we consider the final two overlaps together: The result is shown in Figure 1b. In
development, we tested including more context (i.e. considering second-order influence) and found no
benefit. Since we do not consider breaks other than those required to resolve overlapping n-grams, these
segments tend to be long. This is by design; our intention is that these segments will subsume as many
multiword units as possible, and therefore will be amenable to refinement by further decomposition in
the next step.
3.4 Lexicon decomposition
Based on the initial segmentation of the entire corpus, we extract a tentative lexicon, with corresponding
counts. Then, in order from longest to shortest, we consider decomposition of each entry. First, using
our prediction-based decomposition method, we find the best decomposition of the entry into two parts;
note that we only need to consider one break per lexicon entry, since breaks in the (smaller) parts will
be considered independently later in the process. If the count in our lexicon is below the occurrence
threshold, we always carry out this split, which means we remove the entry from the lexicon and (after
all n-grams of that length have been processed, so as to avoid ordering effects) add its counts to the
counts of n-grams of its best decomposition. If the count is above the threshold, we preserve the full
entry (for entries of length 3 or greater) only if the following inequality is true for each subsegment w
j,k
in the full entry w
1,n
:
k
?
i= j
pred(w
i
,w
1,n
)? pred(w
j,k
) > log p(w
j,k
)? log p(w
1,n
)
756
That is, the ratio (expressed as a difference of logarithms) between the count of the segment and the full
unsegmented entry (in our preliminary lexicon) is lower than the ratio of the predictability (as defined
in our discussion of prediction-based decomposition) of the words in the segment with the context of
the full entry to the predictability of words with only the context included within the segment (which
is just pred(w
j,k
)). In other words, we preserve only longer multiword sequences in our lexicon when
any decrease in the probability of the full entry relative to its smaller components
2
is fully offset by an
increase in the conditional probability of the individual words of that segment when the larger context
from the full segment is available. For example, after we have decided on a potential break in the phrase
basic | cellular process from our example in Figure 1, we compare the (marginal) probability ?lost? by
including basic in a larger phrase, i.e. the ratio of counts of basic to basic cellular process in our lexicon),
to the (conditional) probability ?gained? by how much more predictable the segment is in this context;
when the segment in question is a single word, as in this case, this is simply p(basic|cellular process)/
p(basic), and we break only when there is more gain than loss. This restriction could be parameterized
for more fine-grained control of the trade-off between larger and smaller segments in specific cases, but
in the interest of avoiding nuisance parameters we just use it directly. Once we have decomposed all
eligible entries to create a final lexicon, we apply these same decompositions to the segments in our
initial segmentation to produce a final segmentation (see Figure 1c).
4 Evaluation
Multiword lexicons are typically evaluated in one of two ways: direct comparison to an existing lexicon,
or precision of the top n candidates offered by the model. There are problems with both these meth-
ods, since there are no resources that offer a truly comprehensive treatment of multiword units, defined
broadly, and the top n candidates from a model for small n may not be a particularly representative sam-
ple: in particular, they might not include more common terms, which should be given more weight when
one is considering downstream applications. Given the dual output of our model, evaluation using seg-
mentation is another option, except that creating full gold standard segmentations would be a particularly
difficult annotation task, since our notion of multiword unit is a broad one.
In light of this, we evaluate by taking the best from these various approaches. Given an existing mul-
tiword lexicon, we can evaluate not by comparing our lexicon to it directly, but rather by looking at the
extent to which our segmentation preserves these known multiword units. There are several major ad-
vantages to this approach: first, it does not require a full lexicon or gold standard segmentation; second,
common units are automatically given more weight in the evaluation; third, we can use it for evaluation
in very large corpora. Our two main metrics are subsumption (Sub), namely the percentage of multiword
tokens that are fully contained with a segment, and exact matches (Exact), the percentage of multiword
tokens which correspond exactly to a segment. Exact matches would seem to be preferable to subsump-
tion, but in practice this is not necessarily the case, since our method often identifies valid compound
terms and larger constructions than our reference lexicon contains; for example, WordNet only contains
the expression a lot, but when appearing as part of a noun phrase our model typically segments this to a
lot of, which, in our opinion, is a preferable segmentation. To quantify overall performance, we calculate
a harmonic mean (Mean) of the two metrics. We also looked specifically at performance for terms of 3
or more words (Mean 3+), which are less studied and more relevant to our interests.
Our second evaluation focuses on the quality of these longer terms with a post hoc annotation of
output from our model and the best alternatives. We randomly extracted pairs of segments of three words
or more where our model mostly but not entirely overlapped with an alternative model (750 examples
per corpus per method), and asked CrowdFlower workers to choose which output seemed to be a better
multiword unit in the context; they were shown the entire sentence with the relevant span underlined,
and then the two individual chunks separately. To ensure quality, we used our multiword lexicon to
2
This probability is based on the respective counts in our preliminary lexicon at this step in the process, not the original n-
gram probability. One key advantage to doing the initial segmentation first is that words that appear consistently in larger units,
an extreme example is the bigram vector machine in the term support vector machine, already have low or zero probability, and
will not appear in the lexicon or be good candidate segments for decomposition. This rather intuitively accomplishes what the
c-value metric is modeling by applying negative weights to candidates appearing in larger n-grams.
757
create gold standard examples (comparing known multiword units to purposely bad segmentations which
overlapped with them), and used them to test and filter out unreliable workers: for inclusion in our final
set, we required a minimum 90% performance on the test questions. We also limited each contributor to
only 250 judgments, so that our results reflected a variety of opinions.
We considered a number of alternatives to our approach, though we limited the comparison to methods
which could predict segments greater than 2 words, those that were computationally feasible for large
corpora, and those which segment into single words only as a last resort: approaches which prefer single
words cannot do well under our evaluation because we have no negative examples, only positive ones.
The majority of our alternatives involve ranking all potential n-grams (not just the maximal) with n? 2
and then greedily segmenting them: big-n prefers longer n-grams (with a backoff to counts); c-value is
used for term extraction (Frantzi et al., 2000) and was also compared to by Newman et al. (2012); ME
refers to the Mutual Expectation metric (Dias et al., 1999); and PMI uses a standard extension of PMI to
more than 2 words. We also tested standard (pairwise) PMI as a metric for recursively joining contiguous
units (starting with words) into larger units until no larger units can be formed (PMI join), and a version
of our decomposition algorithm which selects the minimal breaks which maximize total word count
across segments rather than total word predictability (count decomp); the fact that traditional association
metrics are not defined for single words prevents us from using them as alternatives to predictability in
our decomposition approach. Finally, we also include an oracle which chooses the correct n-grams when
they are available for segmentation, but which still fails for units that are below our threshold.
We evaluated our model in four large English corpora: news articles from the Gigaword corpus (Graff
and Cieri, 2003) (4.9 billion tokens), out-of-copyright texts from the Gutenberg Project
3
(1.7 billion
tokens), a collection of abstracts from PubMed (2.2 billion tokens)
4
, and blogs from the ICWSM 2009
social media corpus (Burton et al., 2009) (1.1 million tokens). Our main comparison lexicon is WordNet
3.0, which contains a good variety of expressions appropriate to the various genres, but we also included
multiword terms from the Specialist Lexicon
5
for better coverage of the biomedical domain. One issue
with our evaluation is that it assumes all tokens are true instances of the multiword unit in question; we
carried out a manual inspection of multiword tokens identified by string match in our development sets
(5000 sentences set aside from each of the abstract and blog corpora), and excluded from the evaluation
a small set of idiomatic expressions (e.g. on it, do in) whose literal, non-MWE usage is too common for
the expression to be used reliably for evaluation; otherwise, we were satisfied that the vast majority of
multiword tokens were true matches. When one multiword token appeared within another, we ignored
the smaller of the two; when two overlapped in the text, we ignored both.
5 Results
All the results for the main evaluation are shown in Table 1. First, we observe that our initial segmentation
always provides the highest subsumption, and our final lexicon always provides the highest harmonic
mean, with a modest drop in subsumption but a huge increase in exact matches. The alternative models
fall roughly into two categories: those which have reasonably high subsumption, but few exact matches
(PMI rank seems to be the best of these) and those that have many exact matches (sometimes better
than either of our models) but are almost completely ineffective for identifying multiword units of length
greater than 2 (ME rank and c-value, with ME offering more exact matches): the latter phenomenon is
attributable to the predominance of two-word multiword tokens in our evaluation, which means a model
can do reasonably well by guessing mostly two-word units. For the corpora with more multiword units
of greater length, i.e. the PubMed abstracts and the Gutenberg corpus, our method also provides the most
exact matches. Our best results come in the PubMed corpus, probably because the texts are the most
uniform, though results are satisfactory in all four corpora tested here, which represent a considerable
range of genres.
3
http://www.gutenberg.org . Here we use the English texts from the 2010 image, with headers and footers filtered out using
some simple heuristics.
4
http://www.ncbi.nlm.nih.gov/pubmed/
5
http://www.nlm.nih.gov/research/umls/new users/online learning/LEX 001.htm
758
Table 1: Performance in segmenting multiword units of various segmentation methods in 4 large corpora.
Sub. = Subsumption (%); Exact = Exact Match (%); Mean = Harmonic mean of Sub and Exact; Mean 3+
= Harmonic mean of Sub and Exact for multiword tokens of at length 3 or more. Bold is best in column
for corpus, excluding the oracle.
Method
Gigaword news articles Gutenberg texts
Sub Exact Mean Mean 3+ Sub Exact Mean Mean 3+
Oracle 97.1 97.1 97.1 95.5 97.0 97.0 97.0 97.8
big-n rank 88.7 28.8 43.5 51.4 84.9 30.1 44.4 57.5
c-value rank 69.1 66.1 67.6 23.3 58.6 57.7 58.2 12.6
ME rank 75.3 70.0 72.6 14.4 63.2 61.0 62.1 10.9
PMI rank 90.8 30.0 45.1 53.5 86.9 32.8 47.7 61.2
PMI join 83.1 32.8 47.0 43.7 77.7 32.6 46.0 45.5
Count decomp 75.9 31.3 44.3 47.1 69.2 31.5 43.3 54.2
Prediction decomp, initial 92.2 36.4 52.2 64.4 89.3 38.7 54.0 71.6
Prediction decomp, final 85.6 66.4 75.2 63.8 78.9 62.8 70.0 61.6
Method
PubMed abstracts ICWSM blogs
Sub Exact Mean Mean 3+ Sub Exact Mean Mean 3+
Oracle 91.9 91.9 91.9 84.0 96.5 96.5 96.5 99.4
big-n rank 82.2 40.1 53.9 55.5 86.1 33.3 48.0 60.8
c-value rank 63.2 62.3 62.7 21.7 64.3 62.4 63.3 14.6
ME rank 68.5 65.8 67.1 9.1 69.7 66.2 67.9 11.7
PMI rank 87.0 41.4 56.1 58.3 88.4 35.7 50.8 63.4
PMI join 79.8 39.7 53.0 46.8 80.3 35.4 49.1 47.0
Count decomp 71.0 38.4 49.9 50.4 71.5 33.5 45.6 53.9
Prediction decomp, initial 88.6 50.3 64.1 67.2 90.5 40.3 55.8 70.9
Prediction decomp, final 85.2 73.4 78.8 69.5 83.2 64.9 72.9 66.9
Table 2: CrowdFlower pairwise preference evaluation, our full model versus a selection of alternatives
Comparison Preference for Prediction decomp, final
Prediction decomp, final vs. ME 57.9%
Prediction decomp, final vs. Multi PMI 71.0%
Prediction decomp, final vs. Prediction decomp, initial 70.5%
For our crowdsourced evaluation, we compared our final model to the best models of each of the two
major types from the first round, namely Mutual Expectation and PMI rank, as well as our initial seg-
mentation. The results are given in Table 2. Our full model is consistently preferred over the alternatives.
This is not surprising in the case of the high-subsumption, low-accuracy models, since the resulting seg-
ments often have extraneous words included: an example is in spite of my, which our model correctly
segmented to just in spite of. Given that the ME ranking rarely produces units larger than 2 words, how-
ever, we might have predicted that when it does it would be more precise than our model, but in fact
our model was somewhat preferred (a chi-square test confirmed that this result was statistically different
from chance, p < 0.001). An example of an instance where our model offered a better segmentation is
call for an end to as compared to for an end to from the ME model, though there are also many instances
where the ME segmentation is more sensible, e.g. what difference does it make as compared to difference
does it make from our model.
Looking closer at the output and vocabulary of our model across the various genres, we see a wide
range of multiword phenomena: in the medical abstracts, for instance, there is a lot of medical jargon (e.g.
daily caloric intake) but also other larger connective phrases and formulaic language (e.g. an alternative
explanation for, readily distinguished from). The blogs also have (very different) formulaic language of
759
the sort studied using lexical bundles (e.g. all I can say is that, where else can you) and lots of idiomatic
language (e.g. reinventing the wheel, look on the bright side). The idioms from the Gutenberg, not
surprisingly, tend to be less clich?ed and more evocative (e.g. ghost of a smile); there are rather stodgy
expressions like far be it from me and conjunctions we would not see in the other corpora (e.g. rocks
and shoals, masters and mistresses). By contrast, many of the larger expressions in the news articles are
from sports and finance (e.g. investor demand for, tied the game with), with many that would be filtered
out using the simple grammatical filters often applied in this space. However, for bigrams in particular,
some additional syntactic filtering is clearly warranted.
6 Conclusion
We have presented an efficient but effective method for segmenting a corpus into multiword collocational
units, with a particular focus on units of length greater than two. Our evaluation indicates that this
method results in high-quality segments that capture a variety of multiword phenomena, and is better
in this regard than alternatives based on relevant association measures. This result is consistent across
corpora, though we do particularly well with highly stereotyped language such as seen in the biomedical
domain.
Future work on improving the model will likely focus on extensions related to syntax, for instance
bootstrapped POS filtering and discounting of predictability that can be attributed solely to syntactic
patterns. Our method could also be adapted to decompose full syntactic trees rather than sequences of
words, offering tractable alternatives to Bayesian approaches that identify recurring tree fragments (Cohn
et al., 2009); this would allow us, for instance, to correctly identify constructions with long-distance
dependencies or other kinds of variation where relying on the surface form is insufficient (Seretan, 2011).
With regards to applications, we will be investigating how to help learners notice these chunks when
reading and then use them appropriately in their own writing; this work will eventually intersect with
the well-established areas of grammatical error correction (Leacock et al., 2014) and automated essay
scoring (Shermis and Burstein, 2003). As part of this, we will be building distributional lexical repre-
sentations of these multiword units, which is why our emphasis here was on a highly scalable method.
Part of our interest is of course in capturing the semantics of idiomatic phrases, but we note that even
in the case when a multiword unit is semantically compositional, it might provide de facto word sense
disambiguation or be stylistically distinct from its components, i.e. be very specific to a particular genre
or sub-genre. Therefore, provided we have enough examples to get reliable distributional statistics, these
larger units are likely to provide useful information for various downstream applications.
Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada and the
MITACS Elevate program. Thanks to our reviewers and also Tong Wang and David Jacob for their input.
References
Steven Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-
Based Parsing, pages 257?278. Kluwer Academic Publishers.
Vitor De Araujo, Carlos Ramisch, and Aline Villavicencio. 2011. Fast and flexible MWE candidate generation
with the mwetoolkit. In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Timothy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, FL.
Douglas Biber, Susan Conrad, and Viviana Cortes. 2004. If you look at. . .: Lexical bundles in university teaching
and textbooks. Applied Linguistics, 25:371?405.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Media (ICWSM ?09), San Jose, CA.
760
Yu-Hua Chen and Paul Baker. 2010. Lexical bundles in L1 and L2 academic writing. Language Learning &
Technology, 14(2):30?49.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution gram-
mars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics (NAACL ?09).
Ga?el Dias, Sylvie Guillor?e, and Jos?e Gabriel Pereira Lopes. 1999. Language independent automatic acquisition
of rigid multiword units from unrestricted text corpora. In Proceedings of Conf?erence Traitement Automatique
des Langues Naturelles (TALN) 1999.
Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Processing.
Stefan Evert. 2004. The statistics of word cooccurences?word pairs and collocatoins. Ph.D. thesis, University of
Stuttgart.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identification of idiomatic
expressions. Computational Linguistics, 35(1):61?103.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima. 2000. Automatic recognition of multi-word terms: the
c-value/nc-value method. International Journal on Digital Libraries, 3:115?130.
David Graff and Christopher Cieri. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA.
Ulrich Heid. 2007. Compuational linguistic aspects of phraseology. In Harald Burger, Dmitrij Dobrovol?skij,
Peter K?uhn, and Neal R. Norrick, editors, Phraseology. An international handbook. Mouton de Gruyter, Berlin.
Adam Kilgarriff and David Tugwell. 2001. Word sketch: Extraction and display of significant collocations for
lexicography. In Proceedings of the ACL Workshop on Collocation: Computational Extraction, Analysis and
Exploitation.
Nidhi Kulkarni and Mark Finlayson. 2011. jMWE: A Java toolkit for detecting multi-word expressions. In
Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2014. Automated Grammatical Error
Detection for Language Learners (2nd Edition). Morgan & Claypool.
David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin. 2012. Bayesian text segmentation for
index term identification and keyphrase extraction. In Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12).
Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evalua-
tion, 44:137?158.
Ted Pedersen, Satanjeev Banerjee, Bridget McInnes, Saiyam Kohli, Mahesh Joshi, and Ying Liu. 2011. The
Ngram statistics package (text::nsp) : A flexible tool for identifying ngrams, collocations, and word associations.
In Proceedings of the Multiword Expression Workshop at ACL 2011 (MWE 2011).
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proceedings of the 3rd International Conference on Intelligent Text Processing
and Computational Linguistics (CICLing ?02).
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free induction of multiword unit dictionary headwords a
solved problem? In Proceedings of Empirical Methods in Natural Language Processing (EMNLP ?01).
Violeta Seretan. 2011. Syntax-Based Collocation Extraction. Springer.
Mark D. Shermis and Jill Burstein, editors. 2003. Automated Essay Scoring: A Cross-Disciplinary Approach.
Lawrence Erlbaum Associates, Mahwah, NJ.
Frank Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, pages 143?177.
761
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 940?949, Dublin, Ireland, August 23-29 2014.
The Impact of Deep Hierarchical Discourse Structures
in the Evaluation of Text Coherence
Vanessa Wei Feng
1
, Ziheng Lin
2
, and Graeme Hirst
1
1
Department of Computer Science
University of Toronto
{weifeng, gh}@cs.toronto.edu
2
Singapore Press Holdings
linziheng@gmail.com
Abstract
Previous work by Lin et al. (2011) demonstrated the effectiveness of using discourse relations
for evaluating text coherence. However, their work was based on discourse relations annotated
in accordance with the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which encodes
only very shallow discourse structures; therefore, they cannot capture long-distance discourse
dependencies. In this paper, we study the impact of deep discourse structures for the task of co-
herence evaluation, using two approaches: (1) We compare a model with features derived from
discourse relations in the style of Rhetorical Structure Theory (RST) (Mann and Thompson,
1988), which annotate the full hierarchical discourse structure, against our re-implementation of
Lin et al.?s model; (2) We compare a model encoded using only shallow RST-style discourse
relations, against the one encoded using the complete set of RST-style discourse relations. With
an evaluation on two tasks, we show that deep discourse structures are truly useful for better dif-
ferentiation of text coherence, and in general, RST-style encoding is more powerful than PDTB-
style encoding in these settings.
1 Introduction
In a well-written text, utterances are not simply presented in an arbitrary order; rather, they are presented
in a logical and coherent form, so that the readers can easily interpret the meaning that the writer wishes
to present. Therefore, coherence is one of the most essential aspects of text quality. Given its importance,
the automatic evaluation of text coherence is one of the crucial components of many NLP applications.
A particularly popular model for the evaluation of text coherence is the entity-based local coherence
model of Barzilay and Lapata (B&L) (2005; 2008), which extracts mentions of entities in the text, and
models local coherence by the transitions, from one sentence to the next, in the grammatical role of each
mention. Since the initial publication of this model, a number of extensions have been proposed, the
majority of which are focused on enriching the original feature set. However, these enriched feature
sets are usually application-specific, i.e., it requires a certain expertise and intuition to conceive good
features.
In contrast, we seek insights of better feature encoding from a more general problem: discourse parsing
(to be introduced in Section 2). Discourse parsing aims to identify the discourse relations held among
various discourse units in the text. Therefore, one can expect that discourse parsing provides useful
information to the evaluation of text coherence, because, essentially, the existence and the distribution of
discourse relations are the basis of the coherence in a text.
In fact, there is already evidence showing that discourse relations can help better capture text coher-
ence. Lin et al. (2011) use a PDTB-style discourse parser (to be introduced in Section 2.1) to identify
discourse relations in the text, and they represent a text by entities and their associated discourse roles
in each sentence. In their experiments, using discourse roles alone, their model performs very simi-
lar or even better than B&L?s model. Combining their discourse role features with B&L?s entity-based
transition features further improves the performance.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
940
S1
: The dollar finished lower yesterday, after tracking another rollercoaster session on Wall Street.
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions]C
2.1
, [and traders
appeared content to let the dollar languish in a narrow range until tomorrow, when the preliminary
report on third-quarter U.S. gross national product is released.]C
2.2
S
3
: But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight and inspired market participants to bid the U.S. unit lower.
Three discourse relations are presented in the text above:
1. Implicit EntRel between S
1
as Arg1, and S
2
as Arg2.
2. Explicit Conjunction within S
2
: C
2.1
as Arg1, C
2.2
as Arg2, with and as the connective.
3. Explicit Contrast between S
2
as Arg1 and S
3
as Arg2, with but as the connective.
Figure 1: An example text fragment composed of three sentences, and its PDTB-style discourse relations.
However, PDTB-style discourse relations encode only very shallow discourse structures, i.e., the re-
lations are mostly local, e.g., within a single sentence or between two adjacent sentences. Therefore,
in general, features derived from PDTB-style discourse relations cannot capture long discourse depen-
dency, and thus the resulting model is still limited to being a local model. Nonetheless, long-distance
discourse dependency could be quite useful for capturing text coherence from a global point of view.
Therefore, in this paper, we study the effect of deep hierarchical discourse structure in the evalua-
tion of text coherence, by adopting two approaches to perform a direct comparison between models that
incorporate deep hierarchical discourse structures and models with shallow structures. To evaluate our
models, we conduct experiments on two datasets, each of which resembles a real sub-task in the evalu-
ation of text coherence: sentence ordering and essay scoring. On both tasks, the model derived from
deep discourse structures is shown to be more powerful than the model derived from shallow discourse
structures. Moreover, for sentence ordering, combining our model with entity-based transition features
achieves the best performance. However, for essay scoring, the combination is detrimental.
2 Discourse parsing
Discourse parsing is the problem of identifying the discourse structure within a text, by recognizing the
specific type of its discourse relations, such as Contrast, Explanation, and Causal relations. Although
discourse parsing is still relatively less well-studied, a number of theories have been proposed to capture
different rhetorical characteristics or to serve different applications.
Currently, the two main directions in the study of discourse parsing are PDTB-style and RST-style
parsing. These two directions are based on distinct theoretical frameworks, and each can be potentially
useful for particular kinds of downstream applications. As will be discussed shortly, the major difference
between PDTB- and RST-style discourse parsing is the notion of deep hierarchical discourse structure,
which, according to our hypothesis, can be very useful for recognizing text coherence.
2.1 PDTB-style Discourse Parsing
The Penn Discourse Treebank (PDTB), developed by Prasad et al. (2008), is currently the largest
discourse-annotated corpus, consisting of 2159 Wall Street Journal articles. The annotation in PDTB
adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because)
is treated as a predicate that takes two text spans as its arguments. The argument that the discourse con-
nective structurally attaches to is called Arg2, and the other argument is called Arg1. In PDTB, relations
are further categorized into explicit and implicit relations: a relation is explicit if there is an explicit dis-
course connective presented in the text; otherwise, it is implicit. PDTB relations focus more on locality
and adjacency: explicit relations seldom connect text units beyond local context; for implicit relations,
941
S1
: [The dollar finished lower yesterday,]e
1
[after tracking another rollercoaster session on Wall
Street.]e
2
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions,]e
3
[and traders
appeared content to let the dollar languish in a narrow range until tomorrow,]e
4
[when the preliminary
report on third-quarter U.S. gross national product is released.]e
5
S
3
: [But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight]e
6
[and inspired market participants to bid the U.S. unit lower.]e
7
Condition
(e1-e7)
(e1) (e2)
(e1-e2) (e3-e7)
(e4-e5)
(e4) (e5)
Background
Temporal
List Cause
(e6) (e7)
(e6-e7)(e3-e5)
(e3)
Contrast
Figure 2: An example text fragment composed of seven EDUs, and its RST discourse tree representation.
only adjacent sentences within paragraphs are examined for the existence of implicit relations.
The PDTB-style discourse parsing is thus the type of framework in accordance with the PDTB, which
extracts the discourse relations in a text, by identifying the presence of discourse connectives, the asso-
ciated discourse arguments, and the specific types of the relations. An example text fragment is shown
in Figure 1, consisting of three sentences, S
1
, S
2
, and S
3
. A sentence may further contain clauses, e.g.,
C
2.1
and C
2.2
in S
2
. The three PDTB-style discourse relations in this text are explained below the text.
2.2 RST-style Discourse Parsing
RST-style discourse parsing follows the theoretical framework of Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988). In the framework of RST, a coherent text can be represented as a discourse
tree whose leaves are non-overlapping text spans called elementary discourse units (EDUs); these are the
minimal text units of discourse trees. Adjacent nodes can be related through particular discourse relations
to form a discourse subtree, which can then be related to other adjacent nodes in the tree structure.
RST-style discourse relations can be categorized into two types: mononuclear and multi-nuclear. In
mononuclear relations, one of the text spans, the nucleus, is more salient than the other, the satellite,
while in multi-nuclear relations, all text spans are equally important for interpretation.
Consider Figure 2, in which the same example as in Figure 1 is chunked into seven EDUs (e
1
-e
7
),
segmented by square brackets. Its discourse tree representation is shown below in the figure, following
the notational convention of RST. The two EDUs e
1
and e
2
are related by a mononuclear relation Tem-
poral, where e
1
is the more salient span; e
4
and e
5
are related by Condition, with e
4
as the nucleus; and
e
6
and e
7
are related by Cause, with e
7
as the nucleus. Then, the spans (e
3
-e
5
) and (e
6
-e
7
) are related by
Contrast to form a higher-level discourse structure, and so on. Finally, a Background relation merges the
span (e
1
-e
2
) and (e
3
-e
7
) on the top level of the tree.
As can be seen, thanks to the tree-structured representation of RST, compared to PDTB-style repre-
sentation, we have a full hierarchy of discourse relations in the text: discourse relations exist not only in
a local context, but also on higher text levels, such as between S
1
and the concatenation of S
2
and S
3
.
3 Entity-based Local Coherence Model
The entity-based local coherence model was initially developed by Barzilay and Lapata (B&L) (2005;
2008). The fundamental assumption of this model is that a document makes repeated reference to ele-
ments of a set of entities that are central to its topic.
For a document d, an entity grid is constructed, in which the columns represent the entities referred
942
S1
: [The dollar]
S
finished lower [yesterday]
X
, after tracking [another rollercoaster session]
O
on
[Wall Street]
X
.
S
2
: [Concern]
S
about [the volatile U.S. stock market]
X
had faded in [recent sessions]
X
, and
[traders]
S
appeared content to let [the dollar]
S
languish in [a narrow range]
X
until [tomorrow]
X
,
when [the preliminary report]
S
on [third-quarter U.S. gross national product]
X
is released.
S
3
: But [seesaw gyrations]
S
in [the Dow Jones Industrial Average]
X
[yesterday]
X
put [Wall
Street]
O
back in [the spotlight]
X
and inspired [market participants]
O
to bid [the U.S. unit]
S
lower.
d
o
l
l
a
r
y
e
s
t
e
r
d
a
y
s
e
s
s
i
o
n
W
a
l
l
S
t
r
e
e
t
c
o
n
c
e
r
n
m
a
r
k
e
t
s
e
s
s
i
o
n
s
t
r
a
d
e
r
s
r
a
n
g
e
t
o
m
o
r
r
o
w
r
e
p
o
r
t
G
N
P
g
y
r
a
t
i
o
n
s
D
J
I
A
s
p
o
t
l
i
g
h
t
p
a
r
t
i
c
i
p
a
n
t
s
S
1
S X O X - - - - - - - - - - - -
S
2
S - - - S X S X X X S X - - - -
S
3
S X - O - - - - - - - - S X X O
Table 1: The entity grid for the example text with three sentences and eighteen entities. Grid cells
correspond to grammatical roles: subjects (S), objects (O), or neither (X).
to in d, and rows represent the sentences. Each cell corresponds to the grammatical role of an entity in
the corresponding sentence: subject (S), object (O), neither (X), or nothing (?), and an entity is defined
as a class of coreferent noun phrases. If the entity serves in multiple roles in a single sentence, then
we resolve its grammatical role following the priority order: S  O  X  ?. Consider the text in our
previous examples; its entity grid is shown in Table 1, and the entities are highlighted in boldface in the
text above
1
. A local transition is defined as a sequence {S,O,X,?}
n
, representing the occurrence and
grammatical roles of an entity in n adjacent sentences. Such transition sequences can be extracted from
the entity grid as continuous subsequences in each column. For example, the entity dollar in Table 1
has a bigram transition {S,S} from sentence 1 to 2. The entity grid is then encoded as a feature vector
?(d) = (p
1
(d), p
2
(d), . . . , p
m
(d)), where p
t
(d) is the normalized frequency of the transition t in the
entity grid, and m is the number of transitions with length no more than a predefined length k. p
t
(d) is
computed as the number of occurrences of t in the entity grid of document d, divided by the total number
of transitions of the same length. Moreover, entities are differentiated by their salience ? an entity is
deemed to be salient if it occurs at least l times in the text, and non-salient otherwise ? and transitions
are computed separately for salient and non-salient entities.
3.1 Extension: Lin et al.?s Discourse Role Matrix
As mentioned previously, most extensions to B&L?s entity-based local coherence model focus on enrich-
ing the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner
and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and
Hirst (2012a)?s extension from the perspective of improving the learning procedure.
Among various extensions to B&L?s entity-based local coherence model, the one most related to ours
is Lin et al. (2011)?s work on encoding a text as a set of entities with their associated discourse roles. Lin
et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using
such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem,
they expand the relation sequence into a discourse role matrix, as shown in Table 2. Columns correspond
to the entities in the text and rows represent the contiguous sentences. Each cell
?
E
i
,S
j
?
corresponds to
the set of discourse roles that the entity E
i
serves as in sentence S
j
. For example, the entity yesterday
from S
3
takes part in Arg2 of the last relation, so the cell ?yesterday,S
3
? contains the role Contrast.Arg2.
1
Text elements are considered to be a single entity with multiple mentions if they refer to the same object or concept in the
world, even if they have different textual realizations; e.g., dollar in S
1
and U.S. unit in S
3
refer to the same entity.
943
dollar yesterday session Wall Street concern market
S
1
EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 nil nil
S
2
EntRel.Arg2
nil nil nil
EntRel.Arg2 EntRel.Arg2
Conj.Arg2 Conj.Arg1 Conj.Arg1
Contrast.Arg1 Contrast.Arg1 Contrast.Arg1
S
3
Contrast.Arg2 Contrast.Arg2 nil Contrast.Arg2 nil nil
Table 2: A fragment of Lin et al.?s PDTB-style discourse role matrix for the example text with the first
six entities across three sentences.
An entry may be empty (with a symbol nil, as in ?yesterday,S
2
?) or contain multiple discourse roles (as in
?dollar,S
2
?). Next, the frequencies of the discourse role transitions of lengths 2 and 3, e.g., EntRel.Arg1
? Conjunction.Arg2 and EntRel.Arg1? nil? Contrast.Arg2, are calculated with respect to the matrix.
For example, the frequency of EntRel.Arg1? Conjunction.Arg2 is 1/24 = 0.042 in Table 2.
4 Methodology
As discussed in Section 1, the main objective of our work is to study the impact of deep hierarchical dis-
course structures in the evaluation of text coherence. In order to conduct a direct comparison between a
model with features derived from deep hierarchical discourse relations and a model with features derived
from shallow discourse relations only, we adopt two separate approaches: (1) We implement a model
with features derived from RST-style discourse relations, and compare it against a model with features
derived from PDTB-style relations. (2) In the framework of RST-style discourse parsing, we deprive the
model of any information from higher-level discourse relations and compare its performance against the
model that uses the complete set of discourse relations. Moreover, as a baseline, we also re-implemented
B&L?s entity-based local coherence model, and we will study the effect of incorporating one of our dis-
course feature sets into this baseline model. Therefore, we have four ways to encode discourse relation
features, namely, entity-based, PDTB-style, full RST-style, and shallow RST-style.
4.1 Entity-based Feature Encoding
In entity-based feature encoding, our goal is to formulate a text into an entity grid, such as the one shown
in Table 1, from which we extract entity-based local transitions. In our re-implementation of B&L, we
use the same parameter settings as B&L?s original model, i.e., the optimal transition length k = 3 and the
salience threshold l = 2. However, when extracting entities in each sentence, e.g., dollar, yesterday, etc.,
we do not perform coreference resolution; rather, for better coverage, we follow the suggestion of Elsner
and Charniak (2011) and extract all nouns (including non-head nouns) as entities. We use the Stanford
dependency parser (de Marneffe et al., 2006) to extract nouns and their grammatical roles. This strategy
of entity extraction also applies to the other three feature encoding methods to be described below.
4.2 PDTB-style Feature Encoding
To encode PDTB-style discourse relations into the model, we parse the texts using an end-to-end PDTB-
style discourse parser
2
developed by Lin et al. (2014). The F
1
score of this parser is around 85% for rec-
ognizing explicit relations and around 40% for recognizing implicit relations. A text is thus represented
by a discourse role matrix in the same way as shown in Table 2. Most parameters in our PDTB-style fea-
ture encoding follow those of Lin et al. (2011): each entity is associated with the fully-fledged discourse
roles, i.e., with type and argument information included; the maximum length of discourse role transi-
tions is 3; and transitions are generated separately for salient and non-salient entities with a threshold set
at 2. However, compared to Lin et al.?s model, there are two differences in our re-implementation, and
evaluated on a held-out development set, these modifications are shown to be effective in improving the
performance.
2
http://wing.comp.nus.edu.sg/
?
linzihen/parser/
944
dollar yesterday session Wall Street concern market
S
1
Background.N Background.N Temporal.S Temporal.S nil nil
Temporal.N Temporal.N
List.N List.N List.N
S
2
Condition.N nil nil nil Contrast.S Contrast.S
Contrast.S
Contrast.N
S
3
Background.N Cause.S nil Cause.S nil nil
Cause.N
Table 3: A fragment of the full RST-style discourse role matrix for the example text with the first six
entities across three sentences.
First, we differentiate between intra- and multi-sentential discourse relations, which is motivated by a
finding in the field of RST-style discourse parsing ? distributions of various discourse relation types are
quite distinct between intra-sentential and multi-sentential instances (Feng and Hirst, 2012b; Joty et al.,
2012) ? and we assume that a similar phenomenon exists for PDTB-style discourse relations. Therefore,
we assign two sets of discourse roles to each entity: intra-sentential and multi-sentential roles, which are
the roles that the entity plays in the corresponding intra- and multi-sentential relations.
Second, instead of Level-1 PDTB discourse relations (6 in total), we use Level-2 relations (18 in total)
in feature encoding, so that richer information can be captured in the model, resulting in 18? 2 = 36
different discourse roles with argument attached. We then generate four separate set of features for the
combination of intra-/multi-sentential discourse relation roles, and salient/non-salient entities, among
which transitions consisting of only nil symbols are excluded. Therefore, the total number of features in
PDTB-style encoding is 4? (36
2
+36
3
?2)? 192K.
4.3 Full RST-style Feature Encoding
For RST-style feature encoding, we parse the texts using an end-to-end RST-style discourse parser de-
veloped by Feng and Hirst (2014), which produces a discourse tree representation for each text, such as
the one shown in Figure 2. For relation labeling, the overall accuracy of this discourse parser is 58%,
evaluated on the RST-DT.
We encode the RST-style discourse relations in a similar fashion to PDTB-style encoding. However,
since the definition of discourse roles depends on the particular discourse framework, here, we adapt Lin
et al.?s PDTB-style encoding by replacing the PDTB-style discourse relations with RST-style discourse
relations, and the argument information (Arg1 or Arg2) by the nuclearity information (nucleus or the
satellite) in an RST-style discourse relation. More importantly, in order to reflect the hierarchical struc-
ture in an RST-style discourse parse tree, when extracting the set of discourse relations that an entity
participates in, we find all those discourse relations that the entity appears in the main EDUs of each
relation
3
and represent the role of the entity in each of these discourse relations. In this way, we can
encode long-distance discourse relations for the most relevant entities. For example, considering the
RST-style discourse tree representation in Figure 2, we encode the Background relation for the entities
dollar and yesterday in S
1
, as well as the entity dollar in S
3
, but not for the remaining entities in the text,
even though the Background relation covers the whole text. The corresponding full RST-style discourse
role matrix for the example text is shown in Table 3.
As in PDTB-style feature encoding, we differentiate between intra- and multi-sentential discourse
relations; we use 17 coarse-grained classes of RST-style relations in feature encoding; the optimal transi-
3
The main EDUs of a discourse relation are the EDUs obtained by traversing the discourse subtree in which the relation of
interest constitutes the root node, following the nucleus branches down to the leaves. For instance, for the RST discourse tree
in Figure 2, the main EDUs of the Background relation on the top level are {e
1
,e
7
}, and the main EDUs of the List relation
among (e
3
-e
5
) are {e
3
,e
4
}.
945
tion length k is 3; and the salience threshold l is 2. The total number of features in RST-style encoding is
therefore 4?(34
2
+34
3
?2)? 162K, which is roughly the same as that in PDTB-style feature encoding.
4.4 Shallow RST-style Feature Encoding
Shallow RST-style encoding is almost identical to full RST-style encoding, as introduced in Section
4.3, except that, when we derive discourse roles, we consider shallow discourse relations only. To be
consistent with the majority of PDTB-style discourse relations, we define shallow discourse relations as
those relations which hold between text spans of the same sentence, or between two adjacent sentences.
For example, in Figure 2, the Background relation between (e
1
-e
2
) and (e
3
-e
7
) is not a shallow discourse
relation (it holds between a single sentence and the concatenation of two sentences), and thus will be
excluded from shallow RST-style feature encoding.
5 Experiments
To evaluate our proposed model with deep discourse structures encoded, we conduct two series of exper-
iments on two different datasets, each of which simulates a sub-task in the evaluation of text coherence,
i.e., sentence ordering and essay scoring. Since text coherence is a matter of degree rather than a bi-
nary classification, in both evaluation tasks we formulate the problem as a pairwise preference ranking
problem. Specifically, given a set of texts with different degrees of coherence, we train a ranker which
learns to prefer a more coherent text over a less coherent counterpart. Accuracy is therefore measured
as the fraction of correct pairwise rankings as recognized by the ranker. In our experiments, we use the
SVM
light
package
4
(Joachims, 1999) with the ranking configuration, and all parameters are set to their
default values.
5.1 Sentence Ordering
The task of sentence ordering, which has been extensively studied in previous work, attempts to simulate
the situation where, given a predefined set of information-bearing items, we need to determine the best
order in which the items should be presented. As argued by Barzilay and Lapata (2005), sentence order-
ing is an essential step in many content-generation components, such as multi-document summarization.
In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which
the minimum length of a text is 20 sentences, and the average length is 41 sentences. For each text, we
create 20 random permutations by shuffling the original order of the sentences. In total, we have 735
source documents and 735?20 = 14,700 permutations. Because the RST-style discourse parser we use
is trained on a fraction of the WSJ corpus, we remove the training texts from our dataset, to guarantee
that the discourse parser will not perform exceptionally well on some particular texts. However, since
the PDTB-style discourse parser we use is trained on almost the entire WSJ corpus, we cannot do the
same for the PDTB-style parser.
In this experiment, our learning instances are pairwise ranking preferences between a source text and
one of its permutations, where the source text is always considered more coherent than its permutations.
Therefore, we have 735?20 = 14,700 total pairwise rankings, and we conduct 5-fold cross-validation on
five disjoint subsets. In each fold, one-fifth of the rankings are used for testing, and the rest for training.
5.2 Essay Scoring
The second task is essay scoring, and we use a subset of International Corpus of Learner English (ICLE)
(Granger et al., 2009). The dataset consists of 1,003 essays about 34 distinct topics, written by university
undergraduates speaking 14 native languages who are learners of English as a Foreign Language. Each
essay has been annotated with an organization score from 1 to 4 at half-point increments by Persing et
al. (2010). We use these organization scores to approximate the degrees of coherence in the essays. The
average length of the essays is 32 sentences, and the average organization score is 3.05, with a standard
deviation of 0.59.
4
http://svmlight.joachis.org/
946
Model sentence ordering essay scoring
No discourse structure Entity 95.1 66.4
Shallow discourse structures
PDTB 97.2 82.2
PDTB&Entity 97.3 83.3
Shallow RST 98.5 87.2
Shallow RST&Entity 98.8 87.2
Deep discourse structures
Full RST 99.1 88.3
Full RST&Entity 99.3 87.7
Table 4: Accuracy (%) of various models on the two evaluation tasks: sentence ordering and essay
scoring. For sentence ordering, accuracy difference is significant with p < .01 for all pairs of models
except between PDTB and PDTB&Entity. For essay scoring, accuracy difference is significant with
p < .01 for all pairs of models except between shallow RST and shallow RST&Entity. Significance is
determined with the Wilcoxon signed-rank test.
In this experiment, our learning instances are pairwise ranking preferences between a pair of essays
on the same topic written by students speaking the same native language, excluding pairs with the same
organization score. In total, we have 22,362 pairwise rankings. Similarly, we conduct 5-fold cross-
validations on these rankings.
In fact, the two datasets used in the two evaluation tasks reflect different characteristics by themselves.
The WSJ dataset, although somewhat artificial due to the permuting procedure, is representative of texts
with well-formed syntax. By contrast, the ICLE dataset, although not artificial, contains occasional
syntactic errors, because the texts are written by non-native English speakers. Therefore, using these two
distinct datasets allows us to evaluate our models in tasks where different challenges may be expected.
6 Results
In this section, we demonstrate the performance of our models with discourse roles encoded in one of
the three ways: PDTB-style, full RST-style or shallow RST-style, and compare against their combination
with our re-implemented B&L?s entity-based local transition features. The evaluation is conducted on
the two tasks, sentence ordering and essay scoring, and the accuracy is reported as the fraction of correct
pairwise rankings averaged over 5-fold cross-validation.
The performance of various models is shown in Table 4. The first section of the table shows the
results of our re-implementation of B&L?s entity-based local coherence model, representing the effect
with no discourse structure encoded. The second section shows the results of four models with shallow
discourse structures encoded, including the two basic models, PDTB-style and shallow RST-style feature
encoding, and their combination with the entity-based feature encoding. The last section shows the
results of our models with deep discourse structures encoded, including the RST-style feature encoding
and its combination with the entity-base feature encoding. With respect to the performance, we observe
a number of consistent patterns across both evaluation tasks.
First, with no discourse structure encoded, the entity-based model (the first row) performs the worst
among all models, suggesting that discourse structures are truly important and can capture coherence in
a more sophisticated way than pure grammatical roles. Moreover, the performance gap is particularly
large for essay scoring, which is probably due to the fact that, as argued by Persing et al. (2010), the
organization score, which we use to approximate the degrees of coherence, is not equivalent to text
coherence. Organization relates more to the logical development in the texts, while coherence is about
lexical and semantic continuity; but discourse relations can capture the logical relations at least to some
extent.
Secondly, with deep discourse structures encoded, the RST-style model in the third section signif-
icantly outperforms (p < .01) the models with shallow discourse structures, i.e., the PDTB-style and
947
shallow RST-style models in the middle section, confirming our intuition that deep discourse structures
are more powerful than shallow structures. This is also the case when entity-based features are included.
Finally, considering the models in the middle section of the table, we can gain more insight into the
difference between PDTB-style and RST-style encoding. As can be seen, even without information from
the more powerful deep hierarchical discourse structures, shallow RST-style encoding still significantly
outperforms PDTB-style encoding on both tasks (p < .01). This is primarily due to the fact that the
discourse relations discovered by RST-style parsing have wider coverage of the text
5
, and thus induce
richer information about the text. Therefore, because of its ability to annotate deep discourse structures
and its better coverage of discourse relations, RST-style discourse parsing is generally more powerful
than PDTB-style parsing, as far as coherence evaluation is concerned.
However, with respect to combining full RST-style features with entity features, we have contradictory
results on the two tasks: for sentence ordering, the combination is significantly better than each single
model, while for essay scoring, the combination is worse than using RST-style features alone. This is
probably related to the previously discussed issue of using entity-based features for essay scoring, due to
the subtle difference between coherence and organization.
7 Conclusion and Future Work
In this paper, we have studied the impact of deep discourse structures in the evaluation of text coher-
ence by two approaches. In the first approach, we implemented a model with discourse role features
derived from RST-style discourse parsing, which represents deep discourse structures, and compared it
against our re-implemented Lin et al. (2011)?s model derived from PDTB-style parsing, with no deep
discourse structures annotated. In the second approach, we compared our complete RST-style model
against a model with shallow RST-style encoding. Evaluated on the two tasks, sentence ordering and
essay scoring, deep discourse structures are shown to be effective for better differentiation of text coher-
ence. Moreover, we showed that, even without deep discourse structures, shallow RST-style encoding is
more powerful than PDTB-style encoding, because it has better coverage of discourse relations in texts.
Finally, combining discourse relations with entity-based features is shown to have an inconsistent effect
on the two evaluation tasks, which is probably due to the different nature of the two tasks.
In our future work, we wish to explore the effect of automatic discourse parsers in our methodology.
As discussed previously, the PDTB- and RST-style discourse parsers used in our experiments are far from
perfect. Therefore, it is possible that using automatically extracted discourse relations creates some bias
to the training procedure; it is also possible that what our model actually learns is the distribution over
those discourse relations which automatic discourse parsers are mostly confident with, and thus errors (if
any) made on other relations do not matter. One potential way to verify these two possibilities is to study
the effect of each particular type of discourse relation to the resulting model, and we leave it for future
exploration.
Acknowledgements
We thank the reviewers for their valuable advice and comments. This work was financially supported by
the Natural Sciences and Engineering Research Council of Canada and by the University of Toronto.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings
of the 42rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 141?148.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-based local coherence modelling using topological fields.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages
186?195.
5
The entire text is covered by the annotation produced by RST-style discourse parsing, while this is generally not true for
PDTB-style discourse parsing.
948
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources
and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 125?129.
Vanessa Wei Feng and Graeme Hirst. 2012a. Extending the entity-based coherence model with multiple ranks. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2012), pages 315?324, Avignon, France.
Vanessa Wei Feng and Graeme Hirst. 2012b. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 60?68,
Jeju, Korea.
Vanessa Wei Feng and Graeme Hirst. 2014. A linear-time bottom-up discourse parser with constraints and post-
editing. In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL
2014), Baltimore, USA, June.
Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related
entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 2007),
pages 139?142.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner
English (Version 2). Presses universitaires de Louvain.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector Learning, chapter 11, pages 169?184. MIT Press, Cam-
bridge, MA.
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, pages 904?915.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL 2011), Portland, Oregon, USA, June.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 2:151?184.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Isaac Persing, Alan Davis, and Vincent Ng. 2010. Modeling organization in student essays. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239, Cambridge, MA,
October. Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008).
949
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2172?2183, Dublin, Ireland, August 23-29 2014.
Supervised Ranking of Co-occurrence Profiles for Acquisition of
Continuous Lexical Attributes
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Certain common lexical attributes such as polarity and formality are continuous, creating chal-
lenges for accurate lexicon creation. Here we present a general method for automatically placing
words on these spectra, using co-occurrence profiles, counts of co-occurring words within a large
corpus, as a feature vector to a supervised ranking algorithm. With regards to both polarity and
formality, we show this method consistently outperforms commonly-used alternatives, both with
respect to the intrinsic quality of the lexicon and also when these newly-built lexicons are used
in downstream tasks.
1 Introduction
Lexicon acquisition represents one key way that the information in large corpora and other resources can
be leveraged in various NLP tasks, particularly when the range of lexical items involved in a particular
phenomenon is much more diverse than can typically be captured in manually-built resources. Another
property of the lexicon which might limit a manual approach is the fact that certain attributes are not
discrete, instead falling on a continuous spectrum; although there are manually-built dictionaries which
contain fine-grained judgments of spectra?an example is the MRC psychological database (Coltheart,
1980)?these tend to be very low in coverage, reflecting the difficulty in collecting this information.
Within computational linguistics, the continuous lexical attribute that has received the most attention is
undoubtedly the positive-negative spectrum, otherwise known as semantic orientation (SO) or polarity.
Much of the work focused on acquisition of this attribute at the lexical level has involved simplification
to a binary (positive-negative) or ternary (positive-neutral-negative) distinction (Hatzivassiloglou and
McKeown, 1997; Takamura et al., 2005; Kaji and Kitsuregawa, 2007; Rao and Ravichandra, 2009;
Mohammad et al., 2009; Volkova et al., 2013) but other work explicitly offers a continuous quantification
(Turney, 2002; Turney and Littman, 2003; Baccianella et al., 2010). Another spectrum with a prominent
role in the lexicon is formality (Brooke et al., 2010; Lahiri et al., 2011), which includes colloquial words
at one end, socially-distancing words at the other, and common vocabulary in the middle. In this paper,
we will focus on these two spectra; the method presented, however, is intended to be general, and as such
could be easily applied to other spectra such as those in the MRC database, e.g. abstractness (Turney et
al., 2011), and other kinds of variation captured in, for instance, Osgood?s semantic differential (Osgood
et al., 1957).
The typical approach to this problem involves semi-supervised methods using vector space and/or
graph representations and a set of seed terms. Our method is novel in that it uses fully supervised
SVM ranking of co-occurrence profiles, i.e. normalized counts of instances of binary text co-occurrence
between the target word and a large set of profiling words, selected on the basis of their frequency,
in a publicly-available blog corpus. The seed terms from earlier methods are now viewed as training
examples for building a supervised model that can connect the distributions of co-occurring words in this
wider vocabulary to relative locations on a continuous spectrum. This approach depends somewhat upon
improved manual lexical resources available for these tasks, such as the SO-CAL dictionary (Taboada
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2172
et al., 2011) in the case of polarity, but we limit our (word) training set size in order to show it will
work in resource-scarce situations, such as languages other than English. Our method is straightforward,
practical, and offers essentially full coverage, including words and lexicogrammtical patterns that are
simply not accessible by the many popular methods that are primarily based on WordNet.
To evaluate, we compare our method with popular alternatives in both polarity and formality, with
particular emphasis on other methods based on corpus co-occurrence that have also been shown to be
generalizable across various spectra, i.e. LSA and PMI. For both spectra of interest here, we evaluate both
intrinsically using pairwise comparisons from manually-built lexical resources, and also extrinsically in
downstream tasks such as text-level polarity classification and sentence-level formality judgments. We
show our method is consistently superior across our various evaluations. We also show that not only are
co-occurrence profiles a good source of information for supervised ranking, but that a focus on ranking
rather than regression in this space appears to be fundamental to the success of a supervised approach to
lexical spectra.
2 Related Work
Viewed primarily as a categorical task, the creation or expansion of lexical resources for sentiment anal-
ysis is a commonly-addressed problem. In addition to SentiWordNet (Baccianella et al., 2010), which
we will compare to directly to here, there are numerous mostly semi-supervised approaches based on
exploiting the glosses and/or the graph structure of WordNet to determine whether a word is positive
or negative (Kamps et al., 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Takamura et al., 2005; An-
dreevskaia and Bergler, 2006; Rao and Ravichandra, 2009; Hassan and Radev, 2010), or taking advan-
tage of some other lexicographic resources (Mohammad et al., 2009; Klebanov et al., 2013). The earliest
corpus-based approach was that of Hatzivassiloglou and McKeown (1997) who used local syntactic in-
formation, i.e. conjunctions, to make connections between adjectives; other work that makes use of local
patterns in a corpus includes that of Kaji and Kitsuregawa (2007) and Kanayama and Nasukawa (2006).
Turney (2002) built a continuous polarity lexicon using PMI based on Internet hit counts as a useful
measure of relatedness between seeds, and Turney and Littman (2003) compared this approach with
LSA, which uses general patterns of co-occurrence based on dimensionality reduction. Velikovich et al.
(2010) combined web-scale corpora with a graph-based approach, assigning polarity scores to n-grams
on the basis of the maximum weighed path from an n-gram to the seed terms, using a small (6-word)
context around the word. Like us, Volkova et al. (2013) use social media, iteratively labeling tweets
and words for subjectivity and polarity. Fully-supervised approaches to polarity lexicon acquisition are
rare, but one example is the work of Chetviorkin and Loukachevitch (2012), who classify words as being
sentiment-relevant in Russian using a small set of statistical features, including ratios across disparate
corpora.
Our interest in the continuous aspect of polarity overlaps with work on deriving the semantic intensity
of lexical items from corpora (Sheinman and Tokunaga, 2009); in this task, small sets of synonyms are
ranked according to their intensity, including (but not limited to) polarity. De Melo and Bansal (2013)
use a Mixed Integer Linear Programing algorithm to combine information from multiple pairs into a
single coherent ranking. As with some of the work in polarity, the focus is on adjectives and local
patterns which explicitly distinguish degrees of intensity e.g. not only x but also y, which limits its range
of application; it would not, for instance, be useful for formality or other more pragmatic variations.
Beyond the our work in LSA-based formality lexicon creation (Brooke et al., 2010) and the sentence-
level formality annotation of Lahiri et al. (2011), which we discuss later in more detail, there is a rela-
tively small amount of computational research that directly addresses formality. At lexical level, Li and
Yarowsky (2008) identify formal and informal synonyms in Chinese. Heylighen and Dewaele (2002) and
Li et al. (2013) both offer text-level quantifications of formality; the former is based on POS frequency,
while the latter is based on the Coh-Metrix textual metrics. Using these kinds of metrics, formality
has been evaluated in social media (Mosquera and Moreda, 2012). A supervised text classification ap-
proach to formality is offered by Sheika and Inkpen (2012). Lexical formality is obviously related to
lexicon-based readability (Kidwell et al., 2009) and lexical simplification (Carroll et al., 1999), and is
2173
also relevant to the recent interest in identifying social relationships (Peterson et al., 2011) and shows of
politeness (Danescu-Niculescu-Mizil et al., 2013).
3 Method
Our approach to lexicon acquisition falls into the general category of corpus-based techniques. For both
attributes addressed in this paper, we use the same corpus, the 2009 ICWSM Spinn3r dataset (Burton et
al., 2009), a publicly-available blog corpus which we also used in our earlier work on lexical formality
(Brooke et al., 2010). Blogs are a good resource for broad lexical acquisition because they are very
broad in style and content, and are available in essentially unlimited amounts. We use the English Tier 1
(high-quality) blogs that have at least 100 word types, excluding duplicate texts; after this filtering, our
dataset contains a total of about 2.4 million blogs.
To build a lexicon for any continuous attribute of interest, we begin by creating co-occurrence profiles
as follows: First, we select a document frequency range min-df, max-df that determines a set of profile
words P in a corpus S, where for each p ? P, the document frequency df
S
p
of p in S is limited to be
min-df < df
S
p
< max-df ; that is, each profile word appears in more than min-df documents, but fewer
than max-df documents in our corpus. Then, given a sample size n and a target word w that we wish to
profile, we sample a set of n texts from S which contain the target word (or all the documents where the
word appears, if it appears fewer than n times), and count the document frequency of each profile word
p in this subcorpus, T
w
. We ignore the term frequencies within individual documents because a binary
representation is known to be preferred for stylistic dimensions like formality (Brooke et al., 2010),
and this seems to be also somewhat true in the domain of polarity, where better results can be obtained
when multiple instances of a polar word are discounted (Taboada et al., 2011). To avoid overfitting our
statistical model, we do not count a word as appearing with itself. Once we have sub-corpus document
frequences df
T
w
p
for each p, for each profile word p we define the element of our co-occurrence profile
vector v
p
as
v
p
=
df
T
w
p
?
q?P
df
T
w
q
That is, we normalize each count by the sum across all counts, such that the L1 norm of v is 1. For our
applications here, the dimension of the co-occurrence profile vector is typically in the tens of thousands,
but to illustrate the creation of this vector, suppose we choose an extremely narrow document frequency
band min-df, max-df, such there were only three co-occurrence profile words: p
1
,p
2
,p
3
. For some word
w, we sample n instances of texts from our corpus which contain w, and find that p
1
appears in 10
of these texts, p
2
in 40 of them, and p
3
in 50 of them. The resulting co-occurrence profile vector is
v = ?0.1,0.4,0.5?. This profile could be viewed as a distributional vector space representation of the
word (Turney and Pantel, 2010), or as an estimate of the probability of each p occurring with w; without
any further manipulation, however, we will use it directly as a feature vector for our supervised ranking.
In order to proceed with a supervised approach, we need a ranking of a set of words relevant to
the lexical attribute that we wish to acquire; this ranking is specific to the attribute in question, so we
discuss this in later sections. Given such a ranking (which, we note, may be partial), we apply SVM
rank
(Joachims, 2002), which is part of the SVM
light
set of SVM-based machine learning tools. SVM
rank
was developed for ranking web page results, and, to our knowledge, has not been applied in this space.
SVM
rank
uses an algorithm which optimizes the Kendall?s ? (Kendall, 1955) between a correct ranking r
a
and the automatically-generated ranking r
b
. The simplest version of ? is based on the number of pairwise
rankings which are in concord (C), i.e. both rankings rank the pair relative to each other and the pairwise
rankings are the same, or in discord (D), i.e. both rankings rank the pair relative to each other but the
rankings offered are contradictory. ? is defined as:
?(r
a
,r
b
) =
C?D
C +D
2174
In practice, this is accomplished in SVM
rank
by modifying the original SVM algorithm to use as feature
vectors the difference between ranked input vectors, rather than the input vectors directly. In the context
of this feature space, this means that the model is trained on vectors which represent the differences in the
co-occurrence profiles of ranked words; if the word with co-occurrence profile u is ranked higher than
a word with co-occurrence profile v in our annotation, then SVM
rank
will try to find a weight vector w
such that (u?v) ?w > 0, where w is constrained to be a sum of co-occurrence profile differences (i.e. the
support vectors). Like standard SVM, ranking SVM uses a C parameter which represents the trade-off
between margin size and classification errors, though the interpretation of the margin in ranking SVM
is less clear. The output of the classification step of SVM
rank
is a number for each word which can be
used directly to rank words, or which can be normalized across words into a scale. If the input rankings
also have a continuous numerical representation (which is true in our case for polarity), then this ranking
approach can be compared directly to a standard regression which is not directly sensitive to rankings; to
maximize comparability, we use the regression function included in SVM
light
for this purpose. For both,
we used a linear kernel.
There is a small number of parameters that need to be set: the sample size n, the frequency range
min-df, max-df, and the SVM C parameter. For each of the two lexical attributes of interest, we carried
out independent tuning of these parameters using 5-fold crossvalidation in the training set, carrying out a
grid search at powers of 10. We will discuss the values of parameters with respect to specific experiments
later, but we mention here that a higher-than-default C, which corresponds to more emphasis on avoiding
error rather than maximizing the margin, gave better results for both ranking and correlation, though with
diminishing returns. The role of n is primarily to make the method (much) more tractable, but we suspect
it might be beneficial to the training of the model for the profiles to be based on a uniform number of
examples across word types.
Before we move on to the experimental evaluation, we highlight some intrinsic advantages of this
model, independent of performance. As a technique based on large corpus co-occurrence, it has the im-
portant property that it can go beyond the limited vocabulary offered by, for instance, WordNet. Since
we rely only on co-occurrence, we are not at all limited to individual words (or specific types of words):
we could just as easily derive attribute values for n-grams, collocations, or full lexico-grammatical con-
structions (for instance, distinguishing high as related to price from high as related to quality); though
our interest here is in general lexical properties, there is no reason this approach could not be used
for domain-specific applications, for any lexical units that appear often enough to obtain a reliable co-
occurrence profile. Unlike many graph-based techniques, new vocabulary can be classified directly with-
out perturbing the model, potentially in an online fashion if the corpus is properly indexed (which, we
note, is by far the most time-consuming step of our method). Though some lemmatization may be re-
quired for highly inflectional languages, the method extends easily to any language for which blog data
is likely to be available in sufficient quantities. Our approach is more straightforward than most other
methods based on co-occurrence, which means fewer arbitrary choices and nuisance variables (such as
the dimensionality k or feature weighting typically used in dimensionality-reduction approaches such as
LSA); the parameters that we have are fairly well-behaved. Unlike methods which rely only on examples
from the extremes of a spectrum to derive a quantification of it, our method naturally integrates examples
from the middle of the spectrum (e.g. neutral examples in the case of polarity), but does not inherently
require fine-grained quantification of the entire spectrum; in fact, pairwise examples alone could be used
for training.
4 Polarity experiments
4.1 Word-level Evaluation
We first consider whether our model can be used to build a lexicon which reflects the polarity spectrum.
Our training set of words is taken from the SO-CAL dictionary (Taboada et al., 2011), which has man-
ually assigned SO (polarity) values for words at integer intervals in the range +5 to ?5. The entire
dictionary contains about 5000 words, but we do not use the entire set: first, we restrict our investigation
here to adjectives, which allows us to sidestep inflection issues (we do not consider comparative adjec-
2175
tives), and we randomly select only 50 words from each of the 11 possible SO ratings in the dictionary
(for a total of 550 words), so as to mimic a (relatively) low-resource situation as we might find working
in other languages, and to make it possible to keep the counts equal across SO ratings. Note that the SO-
CAL dictionary does not contain neutral words (words not in the dictionary are assumed to be neutral),
but we used a set of about 200 hand-marked neutral adjectives that had been excluded from the lexicon
during its creation from the words in a set of Epinions product reviews, and which were used for the
original dictionary evaluation by Taboada et al. (2011).
After training our model, we evaluate in two test sets. The first test set is the rest of the SO-CAL
dictionary, excluding words in the training set as well as those not given a rating by SentiWordNet (see
below). Note that this set is not balanced across SO values, since there are many more weakly positive
(SO 1 to 3) or weakly negative (SO ?1 to ?3) words than more-extreme or neutral words; we would
argue, though, that this reflects the actual situation in subjective corpora such as product reviews. To test
whether we might be overfitting to the product reviews domain, we also test using annotations from the
MPQA (Subjectivity) lexicon (Wilson et al., 2005), which was built primarily from news texts.
1
For this,
we again include only words that are in SentiWordNet. The MPQA lexicon uses a very different tagging
schema than the SO-CAL dictionary, with 3 polarity categories (positive, negative, and neutral) as well
as two degrees of subjectivity, weak or strong. Strong or weak subjectivity is defined as how reliable
an indicator of subjectivity the word is, which does not directly correspond to the rationale used for
the SO-CAL dictionary (which is closer to the notion of force or intensity); the results in Taboada et al.
(2011) and our own examination of the lexicon suggest, however, that there is some correlation.
2
Despite
this uncertainty, we combined the MPQA tags to form a polarity spectrum: strongly subjective negative,
weakly subjective negative, neutral, weakly subjective positive, strongly subjective positive. Given a
ranking by our SVM ranker, we evaluate overall pairwise accuracy by considering all possible pairings
of words across different ratings within the SO-CAL or MPQA test sets, and count the percentage of
those where the ordering of the pair with respect to the polarity spectrum is correctly predicted by the
ranking. For a more detailed breakdown, we divide these pairwise comparisons into 3 categories: polarity
(pairs which involve one positive and one negative word), neutrality (pairs which have one neutral word),
and intensity (pairs which have two words with the same polarity). Note that much work in bootstrapping
lexicons for sentiment analysis uses precision and recall, but this is not the most appropriate evaluation
metric in this case because our method can assign a rank (and, eventually, an SO value) to any word in the
2 million word vocabulary of our corpus.
3
Here, we are interested only in reliability of these rankings.
During parameter tuning in the development phase, we found that min-df = 10
3
, max-df = 10
5
was a
good choice: in other words, our profile words are words that appear less than once in 24 texts, but more
than once in 2400 texts. In the ICWSM, there are 30,852 words that fall into this category, so that is the
length of our feature vector. Based on results in the development set, we take n = 1000 as our default;
larger samples provided no appreciable benefit and were even slightly worse in some cases. For the SVM
C parameter, we used 100. We also test using SVM correlation, using the same parameters.
In addition to these variations on our co-occurrence profile technique, we also compare with three
independent alternatives. The first is SentiWordNet 3.0 (Baccianella et al., 2010), which uses a random
walk method in WordNet to derive positive, negative, and neutral values (which sum to 1) for each synset
in WordNet. We follow Taboada et al. (2011) in converting this to a single spectrum for each word by
subtracting the negative score from the positive score, and averaging the result across senses for each
1
There are of course other popular manually-built lexicons, for instance the General Inquirer (Stone et al., 1966), but they
tend to have only binary annotations.
2
One example of where these two dictionaries differ is the word nervous, which is tagged as a strongly subjective negative
word in the MPQA, but has only a?1 score in the SO-CAL dictionary, since it does not describe a particularly intense negative
emotion. An example of a ?5 word is horrendous, which is also a strongly subjective negative word in the MPQA. Instances
of discord where the SO-CAL dictionary is clearly stronger are rarer, but an example is comprehensive, which has an SO of 3
in SO-CAL, but is weakly subjective in the MPQA, probably because of its common descriptive uses, such as in the context of
insurance and (in the UK) education.
3
We have not yet build such a lexicon, but, to facilitate comparison, but we are making available raw scores for all the
adjectives already contained in at least one of the SO-CAL, MPQA, and SentiWordNet lists (excluding the 550 training words),
as well as lists of specific words used for training and testing. These resources can be found at http://www.cs.toronto.edu/
?
jbrooke/rankingpolarity.zip .
2176
Table 1: Results of polarity experiments. Left side of table shows pairwise accuracy (%) for various
sentiment lexicon ranking methods in SO-CAL and MPQA test sets. Pol. = Pairs with different polarity;
Neu. = Pairs with at least one neutral word; Int. = Pairs with the same polarity, but different intensi-
ties. Right side of table shows text polarity classification accuracy (%) in Epinions Corpus for various
adjective lexicons. Bold is best in column.
SO-CAL words MPQA words Epinions texts
Method Pol. Neu. Int. All Pol. Neu. Int. All Acc.
SentiWordNet 82.3 72.3 57.4 72.1 82.8 72.0 49.9 72.0 65.8
LSA 83.5 70.6 63.0 74.5 82.5 70.2 64.0 75.8 66.3
PMI 86.3 73.6 65.8 77.3 84.5 73.4 61.6 76.9 68.0
Profile regression 80.2 67.6 59.6 71.2 77.6 69.0 74.7 75.8 60.3
Profile ranking 88.6 75.7 67.5 79.4 87.5 74.0 56.5 77.0 71.8
word (in that paper, they also considered using only the most common sense but found the results to be
indistinguishable). The second alternative uses the semi-supervised LSA-based method of Turney and
Littman (2003). For the first step, singular value decomposition, we use a binary term-document matrix
with the same ICWSM texts as our supervised model, with k = 500 (a fairly standard choice). In the
second step, which involves calculating the cosine similarity with a set of seed terms using the LSA
vectors and then taking the difference, the positive and negative seeds are just the training instances for
our supervised model (neutral terms are discarded). Our third comparison is the PMI approach of Turney
(2002), which is still popular: for instance, PMI was used to built a Twitter sentiment lexicon in the
winning entry in a recent shared task (Mohammad et al., 2013). Because they have access to the same
corpus and even the same example words as our method, the LSA and PMI alternatives are most directly
comparable to ours.
The results for the word-level polarity experiments are shown in the left side of Table 1. In the SO-
CAL test set, the results are clear: our SVM ranking method is preferred over alternatives, across all the
different categories of pairwise comparison. The relative difficulty of each pair type reflects the average
distance between relevant pairs on the spectrum, as expected. Surprisingly, the correlation method,
despite using the same feature input as the ranking method, is the worst performing method here, though
SentiWordNet is only marginally better, while LSA falls roughly in the middle of the range, and PMI is
the strongest competitor. One potential criticism is that a ranking method is likely to have an advantage
when evaluating by rank. This is true, but we think that relative rank among words is fundamental to
the notion of a spectrum, whereas the bucketing of words into evenly spaced integer ratings is just an
annotation convenience. That said, our output ranking is perhaps too fine-grained in comparison to our
input (offering a full ranking for all words), and it would be desirable if our ranking algorithm allowed
us to encourage some words to be ranked the same.
Although SVM ranking is also the best method on the MPQA test set, the results are marginal as
compared to the SO-CAL test. Part of this could be a moderate amount of domain overfitting, or perhaps
the ranking method is better at fine-grained scales relative to the other methods. However, the most
obvious difference between the test sets appears relative to the intensity comparison, where the profile
ranking performance is relatively poor. This is likely attributable to the differences between the two
kinds of annotations: the SVM ranking method learns the SO-CAL intensity scale fairly well, but this
actually becomes a handicap when degree of subjectivity and not force is the deciding factor; on the other
hand, corpus-based models which did relatively poorly in all the other evaluations (profile regression,
LSA) actually do somewhat better in MPQA intensity than their most comparable alternatives (profile
ranking, PMI) to a degree that is in fact proportional to their relative inferiority elsewhere, suggesting
that sensitivity to degree of subjectivity might be interfering with acquisition of the SO-CAL polarity
spectrum. Interestingly, the value provided by SentiWordNet does not seem to correspond well to either
of these interpretations of intensity, since it does rather poorly with respect to both.
2177
4.2 Text-level Evaluation
The most common use of polarity lexicons is the task of text polarity classification (Turney, 2002),
identifying whether an opinionated text is positive or negative. In this section, we convert the initial
output of the models to polarity lexicons with an appropriate scale so that we can use the SO-CAL
software (Taboada et al., 2011) to carry out text sentiment analysis with our alternative adjective lexicons
rather than its original, manually-built one. SO-CAL is an unsupervised lexical sentiment analysis system
with a number of built-in features, e.g. handling of negation and intensification, that improve the accuracy
of the model, particularly when using a fine-grained, high-precision lexicon. Taboada et al. evaluate
across 4 corpora of balanced product reviews. For our evaluation, we use one of those corpora, a set of
400 product reviews from Epinions, with 50 balanced texts from each of 8 product categories (movies,
books, cars, computers, cookware, hotels, phones, and music). Taboada et al. call this corpus Epinions
2, to distinguish it from the Epinions corpus that the SO-CAL dictionary was built from. They report an
accuracy of 80% using SO-CAL with all word types and features enabled.
Our interest here is to test the influence of lexicon quality on polarity detection. Coverage, though
of course important, is actually a potential source of noise: Low coverage can naturally result in low
performance, but Taboada et al. point out that high coverage can also cause problems, when many of the
rarer words added to the lexicon, even when human-tagged, are not relevant to the primary sentiment of
the text, but rather irrelevant aspects like (in the movie or book domain) character descriptions. Steps
can be taken to mitigate this by identifying relevant sentiment (Scheible and Sch?utze, 2013), but here
we sidestep this problem by forcing our lexicons to have exactly the same coverage by limiting them to
words that appear in the static SentiWordNet lexicon. Again, we also consider only adjectives here.
To build the lexicon for this evaluation we used a different training set: it is not possible to take 50
samples from each SO rating in the SO-CAL adjective lexicon and not have training words that also
appear in the corpus, which we explicitly wanted to avoid.
4
Instead, we train using all the adjectives in
the SO-CAL dictionary that either don?t appear in the Epinions corpus or don?t appear in SentiWordNet
(since we are limiting our output lexicon to SentiWordNet words). This results in a much larger training
set than in the word-level evaluation (about 1500 words), but they are distributed unevenly across SO
ratings. Relative to the word-level evaluation, this is closer to the situation if we were using the entire
SO-CAL dictionary to expand the lexicon. We use the same set of training words as seeds for LSA.
To convert SentiWordNet to a SO-CAL-compatible dictionary, we simply multiply the raw score,
guaranteed to be between ?1 and +1, by 5, creating a range of ?5 to +5. For the raw scores for our
other three options (LSA, profile regression, and profile ranking), we linearly scale the raw score so that
the mean within the lexicon is 0, and a SO +5 word is at the third standard deviation away from the
mean; we choose a rather severe scaling so that there are only a handful of words in the lexicon whose
absolute value is over 5, which SO-CAL is not designed for.
The results of this evaluation are shown on right side of Table 1. Profile ranking is once again dom-
inant, almost 4 percentage points better than the second-best option. The ordering of the lexicons here
is exactly the same as we saw for the word-level evaluation in SO-CAL, though SentiWordNet does
somewhat better than would be expected from those scores. Again, regression does quite poorly despite
having access to the same feature vectors as the ranking method. We note that our results here are also
markedly better than all the other automatic lexicons compared by Taboada et al., namely a PMI-derived
lexicon based on Google counts (Taboada et al., 2006), and a binary lexicon built by expanding entries
in a thesaurus (Mohammad et al., 2009), and are even a bit better than using the human-tagged (binary)
annotations from the General Inquirer (Stone et al., 1966), though we are still quite a long way from what
is possible with the full manual SO-CAL dictionary. Since the quality of the lexicon is directly reflected
in our polarity classification scores here, it is not surprising that our gold-standard lexicon is superior;
in this context, it should be viewed as an upper bound. Nevertheless, we have strong evidence here that
our co-occurrence profile ranking method is a step in the right direction relative to other methods for
automatically building lexicons.
4
For all of our evaluations in this paper we were careful never to use the score of a word which appeared in our training set;
the drawback of this is that our training set size is not constant.
2178
5 Formality Experiments
5.1 Word-level Evaluation
Though the lexicon is perhaps more fundamental to distinctions of polarity than is the case for formality,
nevertheless formality is strongly expressed through word choice; for instance, in English using the
word dude to address a socially-powerful stranger would generally be unacceptable, and it would be
very strange to address a good friend as sir, except as a joke. These are not isolated examples: a huge
portion of the vocabulary is marked to some degree in this fashion, and requires special attention when
moving across text genres or social situations. Word length (in English, at least) and word frequency
can be used as a simple proxy (longer, rarer words are, on average, more formal), but the example above
belies this approach: sir is a shorter word than dude, and it is not immediately obvious that it would
appear less in, for instance, a news corpus, than dude.
In previous work (Brooke et al., 2010), we used the LSA co-occurrence method of Turney and Littman
(2003) discussed in the previous section to derive a formality lexicon using the ICWSM (which was the
best among various corpora tested, including the BNC). For testing, we used a set of 399 synonym
pairs that were pulled from a writing manual focused on word choice, Choose the Right Word (CTRW)
(Hayakawa, 1994), where the author explicitly compared words for their formality, showing that co-
occurrence was a better approach to identifying lexical formality than proxies related to word length or
word frequency. We note that many of the distinctions in the CTRW set are quite subtle, for example
determine vs. ascertain, both of which seem at least somewhat formal, though ascertain was judged by
the expert to be the more formal of the two. In this section we will build a formality ranking using our
profile ranking method, and show that it is better than the LSA method in the CTRW dataset. Here, we
follow our earlier work in using a much smaller k value (20) than is typical for topical uses of LSA,
which we found was better for this dataset, since major stylistic differences seem to be mostly captured
in the first few dimensions after dimensionality reduction, a result which is consistent with the work of
Biber (1988) looking at differences across registers.
Unlike for polarity, there is no resource available that offers a full scale of formality for a large number
of words, and the set used in our initial work on formality has only extreme, handpicked words. In more
recent work (Brooke and Hirst, 2013), we used a larger set of words (900) that included a variety of
different styles that had been tagged by a group of 5 annotators. In that work, we did not use the
term ?formality,? but one of our styles, colloquial, corresponds to the informal end of the spectrum,
and two other styles, objective and literary, can both be viewed as social-distancing language.
5
The
words tagged by annotators as belonging to neither of these categories will serve as the middle of this
spectrum. Compared to our polarity lexicon, our training set therefore is much more coarse-grained (with
only 3 rankings, as compared to 11 for polarity), but the pairwise relationships can be used to build a
fine-grained scale. As before, we remove all words that overlap with our test set.
With respect to parameters, we use the same n as in the polarity experiments, but in development we
saw better performance from a higher C (10,000) and a lower bottom bound on the document frequency
range, df-min = 10
2
. The latter might reflect the fact that rare vocabulary has a strong tendency to be
associated with extreme formality, though there is also a limit to this, since if a word is so rare that it
hardly ever co-occurs with anything, it cannot possibly be useful for training no matter how good an
example of extreme formality it is. The results using these settings are shown in the left side of Table 2.
Again, our profiling method is clearly better than lexicon-based alternatives. LSA outperforms PMI in
the word-level task, a result which is consistent with our other work in stylistic lexical induction (Brooke
and Hirst, 2013),
6
and all the co-occurrence methods are well above the word-length baseline.
Figure 1 contains a more detailed analysis of the influence of individual document frequency bands: we
5
The objective dimension corresponds roughly to the style of a technical document, while the literary dimension involves
flowery, even archaic language that suggests a literary sophistication. Contrast so with synonyms therefore (objective) and thus
(literary), which are both more formal, but in different ways.
6
We suspect this is due to the fact that LSA vectors encode information about word frequency: even when the vector norm is
controlled for, we have found that the LSA vectors of high- and low-frequency words have consistently different distributions,
which may help in identifying extremely low frequency, highly formal words appearing in the CTRW dataset; by contrast, PMI
and other probability-based approaches seem to behave more erratically when presented with low-frequency items.
2179
Table 2: Results of the formality experiments. The second column shows pairwise accuracy of different
models identifying the more formal of two synonyms in CTRW test set. The third column shows average
correlation with two human 5-point Likert-scale formality annotations of the 500 sentence test set.
Method CTRW words Sentence Evaluation
Word length 63.7 0.36
LSA 78.7 0.49
PMI 72.2 0.52
Profile ranking 86.5 0.55
Figure 1: Pairwise accuracy in the CTRW test set for various frequency bands. Dotted line represents
performance using best parameters from development phase, i.e. max-df = 10
2
, max-df = 10
5
.
built models for each of our frequency bands (ranges between two consecutive powers of ten), and tested
them in the CTRW corpus. The flat dotted line represents the larger band we used based on development
performance, 10
2
?10
5
. We see that accuracy peaks at 10
3
to 10
4
df band, at a value (87.3%) which is
higher than we saw with the larger band chosen based on the development set. The words in this band
are fairly uncommon, appearing less than once in 240 texts, but greater than once in 2400 texts; still, as
a group they provide enough evidence to make a strong determination of formalty.
5.2 Sentence-level Evaluation
Lahiri and Lu (2011) report on the creation of 5-point Likert scale annotations of sentence-level formality,
with two ratings for each of 500 sentences taken from separate texts in a diverse corpus which includes
news, blogs, forums, and academic papers (Lahiri et al., 2011). In this section, we use this annotation
to carry out an extrinsic evaluation of our lexical formality ratings. As far as we are aware, this is
the first use of this annotation for evaluating metrics of formality. We extract all lexical words (verbs,
adjectives, adverbs, and nouns, though we omit proper nouns) from the sentences and use the 3-way
formality annotation with these words removed to create LSA, PMI, and profile ranking models, which
are then used to create a formality lexicon for these words, using the same method we used to create the
SO lexicon. Given a lexicon, we averaged the formality score across each sentence (ignoring duplicate
items) to get a formality score for each sentence. We calculate Pearson?s correlation coefficient between
our score and each of the two annotators, and then average the result. For comparison, the correlation
between the two human annotators is 0.60.
The results in Table 2 indicate that the preference for the profile ranking method seen in the CTRW set
extends directly to sentence-level formality ranking, and the level of correlation reached by the profile
ranking method approaches correlation between humans. This supports our claim that lexical choice
is very important to formality: our results here indicate that humans with access to other indicators of
formality (for instance, use or avoidance of particular syntactic constructions) agree only slightly more
with each other than our lexicon-only model does with them.
2180
6 Conclusion
In this paper we have presented a novel approach to determining where a word lies on a spectrum, using
just counts of words that it tends to appear with and an SVM ranking algorithm, both of which are key
components to its success. We have shown that it can be applied to at least two continuous attributes of
interest in computational linguistics, namely polarity and formality, and that the benefits of this method
relative to established alternatives are visible not just in direct lexicon evaluation, but also in the NLP
tasks where these lexicons can be used. Even with a relatively small set of words to train with, we
see little sign of overfitting, and although we have focused on a small set of words here, our method is
efficient enough that it could easily be applied to a much larger set of lexicogrammatical units, though
we will also have to derive ways to filter out unreliable assignments to reduce overall noise. Other
future work will involve looking at other spectra, other languages, other supervised ranking models, and
improving our performance generally by being more selective of profile words or training examples or
by refining our rankings by including other sources of information such as WordNet.
Acknowledgements
This work was supported by the Natural Sciences and Engineering Research Council of Canada and the
MITACS Elevate program. Thanks to Shibamouli Lahiri and Maite Taboada for the use of their resources
and their feedback.
References
Alina Andreevskaia and Sabine Bergler. 2006. Semantic tag extraction from WordNet glosses. In Proceedings of
5th International Conference on Language Resources and Evaluation (LREC ?06).
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion mining. In Proceedings of the 7th Conference on International Lan-
guage Resources and Evaluation (LREC?10), Valletta, Malta, May.
Douglas Biber. 1988. Variation Across Speech and Writing. Cambridge University Press.
Julian Brooke and Graeme Hirst. 2013. Hybrid models for lexical acquisition of correlated styles. In Proceedings
of the 6th International Joint Conference on Natural Language Processing (IJCNLP ?13).
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Automatic acquisition of lexical formality. In Proceedings
of the 23rd International Conference on Computational Linguistics (COLING ?10), Beijing.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Media (ICWSM ?09), San Jose, CA.
John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying
English text for language impaired readers. In Proceedings of the 9th Conference of the European Chapter of
the Association for Computational Linguistics (EACL?99).
Ilia Chetviorkin and Natalia Loukachevitch. 2012. Extraction of Russian sentiment lexicon for product meta-
domain. In Proceedings of the 24th International Conference on Computational Linguistics (COLING ?12).
Max Coltheart. 1980. MRC Psycholinguistic Database User Manual: Version 1. Birkbeck College.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013.
A computational approach to politeness with application to social factors. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (ACL ?13).
Gerard de Melo and Mohit Bansal. 2013. Good, great, excellent: Global inference of semantic intensities. Trans-
actions of the Association for Computational Linguistics, 1.
Ahmed Hassan and Dragomir Radev. 2010. Identifying text polarity using random walks. In Proceedings of the
48th Annual Meeting of the Association for Computational Linguistics, (ACL ?10).
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In
Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Confer-
ence of the European Chapter of the Association for Computational Linguistics (ACL/EACL ?97).
2181
S.I. Hayakawa, editor. 1994. Choose the Right Word. HarperCollins Publishers, second edition. Revised by
Eugene Ehrlich.
Francis Heylighen and Jean-Marc Dewaele. 2002. Variation in the contextuality of language: An empirical
measure. Foundations of Science, 7(3):293?340.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ?04).
Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In 9th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD ?02).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning (EMNLP-CoNLL ?07).
Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using WordNet to measure semantic
orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and
Evaluation (LREC ?04).
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing
(EMNLP ?06).
Maurice Kendall. 1955. Rank Correlation Methods. Hafner.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-Thompson. 2009. Statistical estimation of word acquisition with
application to readability prediction. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP?09).
Soo Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
International Conference on Computational Linguistics (COLING ?04).
Beata Beigman Klebanov, Nitin Madnani, and Jill Burstein. 2013. Using pivot-based paraphrasing and sentiment
profiles to improve a subjectivity lexicon for essay data. Transactions of the Association for Computational
Linguistics, 1.
Shibamouli Lahiri and Xiaofei Lu. 2011. Inter-rater agreement on sentence formality. http://arxiv.org/abs/1109.
0069 .
Shibamouli Lahiri, Prasenjit Mitra, and Xiaofei Lu. 2011. Informality judgment at sentence level and experiments
with formality score. In Proceedings of the 12th International Conference on Computational Linguistics and
Intelligent Text Processing (CICLing?11).
Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal Chinese
phrases from web corpora. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing (EMNLP ?08).
Haiying Li, Arthur. C. Graesser, and Zhiqiang Cai. 2013. Comparing two measures of formality. In Proceedings
of the Twenty-sixth International Florida Artificial Intelligence Research Society Conference.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009. Generating high-coverage semantic orientation lexicons
from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP ?09).
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-the-art
in sentiment analysis of tweets. In Proceedings of the Second Joint Conference on Lexical and Computational
Semantics (SEMSTAR ?13).
Alejandro Mosquera and Paloma Moreda. 2012. A qualitative analysis of informality levels in web 2.0 texts: The
Facebook case study. In Proceedings of the LREC workshop:@NLP can u tag #user generated content, pages
23?29.
Charles E. Osgood, George Suci, and Percy Tannenbaum. 1957. The measurement of meaning. University of
Illinois Press, Urbana, IL.
2182
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011. Email formality in the workplace: A case study on the Enron
corpus. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ?11),
Portland, Oregon.
Delip Rao and Deepak Ravichandra. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Lingusitics.
Christian Scheible and Hinrich Sch?utze. 2013. Sentiment relevance. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (ACL ?13).
Fadi Abu Sheika and Diana Inkpen. 2012. Learning to classify documents according to formal and informal style.
Linguistic Issues in Language Technology, 8.
Vera Sheinman and Takenobu Tokunaga. 2009. Adjscales: Visualizing differences between adjectives for language
learners. IEICE Transactions on Information and Systems, 92(8):1542?1550.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilivie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press.
Maite Taboada, Caroline Anthony, and Kimberly Voll. 2006. Methods for creating semantic orientation dictionar-
ies. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC ?06).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexicon-based methods
for sentiment analysis. Computational Linguistics, 37(2):267?307.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL
?05).
Peter D. Turney and Michael Littman. 2003. Measuring praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Systems, 21:315?346.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP ?11).
Peter D. Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL
?02).
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics (HLT/NAACL ?10).
Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring sentiment in social media: Bootstrap-
ping subjectivity clues from multilingual Twitter streams. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (ACL ?13).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sen-
timent analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP ?05).
2183
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1003?1011,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Refining the Notions of Depth and Density in WordNet-based
Semantic Similarity Measures
Tong Wang
Department of Computer Science
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We re-investigate the rationale for and the ef-
fectiveness of adopting the notions of depth
and density in WordNet-based semantic sim-
ilarity measures. We show that the intuition
for including these notions in WordNet-based
similarity measures does not always stand up
to empirical examination. In particular, the
traditional definitions of depth and density
as ordinal integer values in the hierarchical
structure of WordNet does not always corre-
late with human judgment of lexical semantic
similarity, which imposes strong limitations
on their contribution to an accurate similarity
measure. We thus propose several novel defi-
nitions of depth and density, which yield sig-
nificant improvement in degree of correlation
with similarity. When used in WordNet-based
semantic similarity measures, the new defini-
tions consistently improve performance on a
task of correlating with human judgment.
1 Introduction
Semantic similarity measures are widely used in
natural language processing for measuring distance
between meanings of words. There are currently
two mainstream approaches to deriving such mea-
sures, i.e., distributional and lexical resource-based
approaches. The former usually explores the co-
occurrence patterns of words in large collections
of texts such as text corpora (Lin, 1998) or the
Web (Turney, 2001). The latter takes advantage of
mostly handcrafted information, such as dictionar-
ies (Chodorow et al, 1985; Kozima and Ito, 1997)
or thesauri (Jarmasz and Szpakowicz, 2003).
Another important resource in the latter stream is
semantic taxonomies such as WordNet (Fellbaum,
1998). Despite their high cost of compilation and
limited availability across languages, semantic tax-
onomies have been widely used in similarity mea-
sures, and one of the main reasons behind this is that
the often complex notion of lexical semantic simi-
larity can be approximated with ease by the distance
between words (represented as nodes) in their hier-
archical structures, and this approximation appeals
much to our intuition. Even methods as simple as
?hop counts? between nodes (e.g., that of Rada et al
1989 on the English WordNet) can take us a long
way. Meanwhile, taxonomy-based methods have
been constantly refined by incorporating various
structural features such as depth (Sussna, 1993; Wu
and Palmer, 1994), density (Sussna, 1993), type of
connection (Hirst and St-Onge, 1998; Sussna, 1993),
word class (sense) frequency estimates (Resnik,
1999), or a combination these features (Jiang and
Conrath, 1997). Most of these algorithms are fairly
self-contained and easy to implement, with off-the-
shelf toolkits such as that of Pedersen et al (2004).
With the existing literature focusing on carefully
weighting these features to construct a better seman-
tic similarity measure, however, the rationale for
adopting these features in calculating semantic sim-
ilarity remains largely intuitive. To the best of our
knowledge, there is no empirical study directly in-
vestigating the effectiveness of adopting structural
features such as depth and density. This serves as
the major motivation for this study.
The paper is organized as follows. In Section
2 we review the basic rationale for adopting depth
1003
and density in WordNet-based similarity measures
as well as existing literature on constructing such
measures. In Section 3, we show the limitations of
the current definitions of depth and density as well as
possible explanations for these limitations.1 We then
propose new definitions to avoid such limitations in
Section 4. The effectiveness of the new definitions
is evaluated by applying them in semantic similar-
ity measures in Section 5 and conclusions made in
Section 6.
2 Related Work
The following are the current definitions of depth
and density which we aim at improving. Given a
node/concept c in WordNet, depth refers to the num-
ber of nodes between c and the root of WordNet,
(i.e., the root has depth zero, its hyponyms depth
one, and so on). There are more variations in the
definition of density, but it is usually defined as the
number of edges leaving c (i.e., its number of child
nodes) or leaving its parent node(s) (i.e., its number
of sibling nodes). We choose to use the latter since
it is used by most of the existing literature.
2.1 The Rationale for Depth and Density
The rationale for using the notions of depth and den-
sity in WordNet-based semantic similarity measures
is based on the following assumption:
Assumption 1 Everything else being equal, two
nodes are semantically closer if (a) they reside
deeper in the WordNet hierarchy, or (b) they are
more densely connected locally.
This is the working assumption for virtually all
WordNet-based semantic similarity studies using
depth and/or density. For depth, the intuition is
that adjacent nodes deep down the hierarchy are
likely to be conceptually close, since the differen-
tiation is based on finer details (Jiang and Conrath,
1997). Sussna (1993) termed the use of depth as
depth-relative scaling, claiming that ?only-siblings
deep in a tree are more closely related than only-
siblings higher in the tree?. Richardson and Smeaton
(1995) gave an hypothetical example illustrating
this ?only-siblings? situation, where plant?animal
1Since the works we review in this section have different
definitions of depth and density, we defer our formal definitions
to Section 3.
are the only two nodes under living things, and
wolfhound?foxhound under hound. They claimed
the reason that the former pair can be regarded as
conceptually farther apart compared to the latter is
related to the difference in depth.
As for the relation between density and similar-
ity, the intuition is that if the overall semantic mass
for a given node is constant (Jiang and Conrath,
1997), then the more neighboring nodes there are in
a locally connected subnetwork, the closer its mem-
bers are to each other. For example, animal, per-
son, and plant are more strongly connected with life
form than aerobe and plankton because the first three
words all have high density in their local network
structures (Richardson and Smeaton, 1995). Note
that the notion of density here is not to be con-
fused with the conceptual density used by Agirre
and Rigau (1996), which is essentially a semantic
similarity measure by itself.
In general, both observations on depth and density
conform to intuition and are supported qualitatively
by several existing studies. The main objective of
this study is to empirically examine the validity of
this assumption.
2.2 Semantic Similarity Measures Using Depth
and/or Density
One of the first examples of using depth and den-
sity in WordNet-based similarity measures is that of
Sussna (1993). The weight on an edge between two
nodes c1 and c2 with relation r in WordNet is given
as:
w(c1,c2) = w(c1?r c2)+w(c2?r c1)2d
where d is the depth of the deeper of the two nodes.
As depth increases, weight decreases and similarity
in turn increases, conforming to Assumption 1. The
edge weight was further defined as
w(c1?r c2) = maxr? maxr?minrnr(c1)
where nr(X) is ?the number of relations of type r
leaving node X?, which is essentially an implicit
form of density, and maxr and minr are the maxi-
mum and minimum of nr, respectively. Note that
this formulation of density contradicts Assumption
1004
1 since it is proportional to edge weight (left-hand-
side) and thus negatively correlated to similarity.
Wu and Palmer (1994) proposed a concept simi-
larity measure between two concepts c1 and c2 as:
sim(c1,c2) = 2 ?dep(c)len(c1,c)+ len(c2,c)+2 ?dep(c)(1)
where c is the lowest common subsumer (LCS) of c1
and c2, and len(?, ?) is the number of edges between
two nodes. The rationale is to adjust ?hop count?
(the first two terms in the denominator) with the
depth of LCS: similarity between nodes with same-
level LCS is in negative proportion to hop counts,
while given the same hop count, a ?deeper? LCS
pulls the similarity score closer to 1.
Jiang and Conrath (1997) proposed a hybrid
method incorporating depth and density information
into an information-content-based model (Resnik,
1999):
w(c, p) =(dep(p)+1dep(p) )
?
? [?+(1??) E?den(p) ]
? [IC(c)? IC(p)]T (c, p) (2)
Here, p and c are parent and child nodes in Word-
Net, dep(?) and den(?) denote the depth and den-
sity of a node, respectively, E? is the average density
over the entire network of WordNet, and ? and ? are
two parameters controlling the contribution of depth
and density values to the similarity score. IC(?) is
the information content of a node based on proba-
bility estimates of word classes from a small sense-
tagged corpus (Resnik, 1999), and T (c, p) is a link-
type factor differentiating different types of relations
between c and p.
3 Limitations on the Current Definitions of
Depth and Density
To what extent do the notions of depth and density
help towards an accurate semantic similarity mea-
sure? Our empirical investigation below suggests
that more often than not, they fail our intuition.
A direct assessment of the effectiveness of us-
ing depth and density is to examine their correla-
tion with similarity. Empirical results in this section
Figure 1: Correlation between depth and similarity.
are achieved by the following experimental setting.
Depth is defined as the number of edges between the
root of the hierarchy and the lowest common sub-
sumer (LCS) of two nodes under comparison, and
density as the number of siblings of the LCS.2 Sim-
ilarity is measured by human judgment on similar-
ity between word pairs. Commonly used data sets
for such judgments include that of Rubenstein and
Goodenough (1965), Miller and Charles (1991), and
Finkelstein et al (2001) (denoted RG, MC, and FG,
respectively). RG is a collection of similarity ratings
of 65 word pairs averaged over judgments from 51
human subjects on a scale of 0 to 4 (from least to
most similar). MC is a subset of 30 pairs out of the
RG data set. These pairs were chosen to have evenly
distributed similarity ratings in the original data set,
and similarity judgment was elicited from 38 human
judges with the same instruction as used for RG. FG
is a much larger set consisting of 353 word pairs,
and the rating scale is from 0 to 10. We combine the
RG and FG data sets in order to maximize data size.
Human ratings r on individual sets are normalized to
rn on 0 to 1 scale by the following formula:
rn = r? rminrmax? rmin
where rmax and rmin are the maximum and minimum
of the original ratings, respectively. Correlation is
evaluated using Spearman?s ?.
2We also tried several other variants of these definitions,
e.g., using the maximum or minimum depth of the two nodes
instead of the LCS. With respect to statistical significance tests,
these variants all gave the same results as our primary definition.
1005
Figure 2: Histogram of depth of WordNet noun synsets.
3.1 Depth
The distribution of similarity of the combined data
set over depth is plotted in Figure 1. For depth val-
ues under 5, similarity scores are fairly evenly dis-
tributed over depth, showing no statistical signifi-
cance in correlation. For depth 5 and above, the
shape of distribution resembles an upper-triangle,
suggesting that (1) correlation with similarity be-
comes stronger in this range of depth value, and (2)
data points with higher depth values tend to have
higher similarity scores, but the reverse of the claim
does not hold, i.e., word pairs with ?shallower? LCS
can also be judged quite similar by humans.
There are many more data points with lower depth
values than with higher depth values in the com-
bined data set. In order to have a fair comparison of
statistical significance tests on the two value ranges
for depth, we randomly sample an equal number
(100) of data points from each value range, and the
correlation coefficient between depth and similarity
is averaged over 100 of such samplings. Correla-
tion coefficients for depth value under 5 versus 5 and
above are ? = 0.0881, p ? 0.1 and ? = 0.3779, p <
0.0001, respectively, showing an apparent difference
in degree of correlation.
Two interesting observations can be made from
these results. Firstly, the notion of depth is relative
to the distribution of number of nodes over depth
value. For example, depth 20 by itself is virtually
meaningless since it might be quite high if the ma-
jority of nodes in WordNet are of depth 10 or less,
or quite low if the majority depth value are 50 or
more. According to the histogram of depth values
in WordNet (Figure 2), the distribution of number of
nodes over depth value approximately conforms to a
normal distribution N (8,2). It is visually quite no-
ticeable that the actual quantity denoting how deep a
node resides in WordNet is conflated at depth values
below 5 or above 14. In other words, the distribution
makes it rather inaccurate to say, for instance, that a
node of depth 4 is twice as deep as a node of depth 2.
This might explain the low degree of correlation be-
tween similarity and depth under 5 in Figure 1 (man-
ifested by the long, vertical stripes across the entire
range of similarity scores (0 to 1) for depth 4 and
under), and also how the correlation increases with
depth value. Unfortunately, we do not have enough
data for depth above 14 to draw any conclusion on
this higher end of the depth spectrum.
Secondly, even on the range of depth values with
higher correlation with similarity, there is no defini-
tive sufficient and necessary relation between depth
and similarity (hence the upper triangle instead of
a sloped line or band). Particularly, semantically
more similar words are not necessarily deeper in the
WordNet hierarchy. Data analysis reveals that the
LCS of highly similar words can be quite close to
the hierarchical root. Examples include coast?shore,
which is judged to be very similar by humans (9 on
a scale of 0?10 in both data sets). The latter is a hy-
pernym of the former and thus the LCS of the pair,
yet it is only four levels below the root node entity
(via geological formation, object, and physical en-
tity). Another situation is when the human judges
confused relatedness with similarity, and WordNet
fails to capture the relatedness with its hierarchical
structure of lexical semantics: the pair software?
computer can only be related by the root node en-
tity as their LCS, although the pair is judged quite
?similar? by humans (8.5 on 0 to 10 scale).
The only conclusive claim that can be made here
is that word pairs with deeper LCS?s tend to be more
similar. However, since only word forms (rather
than senses) are available in these psycho-linguistic
experiments, the one similarity rating given by hu-
man judges sometimes fails to cover multiple senses
for polysemous words. In the pair stock?jaguar of
the FG set, for example, one sense of stock (live-
stock, stock, farm animal: any animals kept for use
or profit) is closely connected to jaguar through a
depth-10 LCS (placental, placental mammal, eu-
therian, eutherian mammal). However, the pair re-
ceived a low similarity rating (0.92 on 0?10), prob-
1006
Figure 3: Correlation between density and similarity.
MC RG FG
dep 0.7056*** 0.6909*** 0.3701***
den 0.2268 0.2660* 0.1023
Table 1: Correlation between depth/density and similar-
ity on individual data sets. Number of asterisks indicates
different confidence intervals (?*? for p < 0.05, ?***? for
p < 0.0001).
ably because judges associated the word form stock
with its financial sense, especially when there was
an abundant presence of pairs indicating this particu-
lar sense of the word (e.g., stock?market, company?
stock).
3.2 Density
Comparing to depth, density exhibits much lower
correlation with similarity (Figure 3-a and 3-b). We
conducted correlation experiments between density
and similarity with the same setting as for depth and
similarity above. Data points with extremely high
density values (up to over 400) are mostly idiosyn-
cratic to the densely connected regions in WordNet
and are numerically quite harmful. We thus ex-
cluded outliers with density values above 100 in the
experiment.
Evaluation on the combined data set shows no
correlation between density and similarity. To con-
firm the result, we break the experiments down to the
three individual data sets, and the results are listed in
Table 1. The correlation coefficient between density
and similarity ranges from 0.10 to 0.27 There is no
statistical significance of correlation on two of the
three data sets (MC and FG), and the significance
on RG is close to marginal with p = 0.0366.
Data analysis suggests that density values are of-
ten biased by particular fine-grainedness of local
structures in WordNet. Qualitatively, Richardson
and Smeaton (1995) previously observed that ?the
irregular densities of links between concepts results
in unexpected conceptual distance measures?. Em-
pirically, on the one hand, more than 90% of Word-
Net nodes have density values less than or equal to
3. This means that for 90% of the LCS?s, there are
only three integer values for density to distinguish
the varying degrees of similarity. In other words,
such a range might be too narrow to have any real
distinguishing power over similarity. On the other
hand, there are outliers with extreme density values
particular to the perhaps overly fine-grained subcat-
egorization of some WordNet concepts, and these
nodes can be LCS?s of word pairs of drastically dif-
ferent similarity. The node person, individual, for
example, can be the LCS of similar pairs such as
man?woman, as well as quite dissimilar ones such
as boy?sage, where the large density value does not
necessarily indicate high degree of similarity.
Another crucial limitation of the definition of den-
sity is the information loss on specificity. In the ex-
isting literature, density is often adopted as a proxy
for the degree of specificity of a concept, i.e., nodes
in densely connected regions in WordNet are taken
to be more specific and thus closer to each other.
This information of a given node should be inher-
ited by its hierarchical descendants, since specificity
should monotonically increase as one descends the
hierarchy. For example, the node piano has a den-
sity value of 15 under the node percussion instru-
ment. However, the density value of its hyponyms
Grand piano, upright piano, and mechanical piano,
is only 3. Due to the particular structure of this sub-
network in WordNet, the grand?upright pair might
be incorrectly regarded as less specific (and thus less
similar) than, say, between piano?gong, both as per-
cussion instruments.
4 New Definitions of Depth and Density
In this section, we formalize new definitions of
depth and density to correct for their current limi-
1007
MC RG FG
depu 0.7201*** 0.6798*** 0.3751***
denu 0.2268 0.2660* 0.1019
deni 0.7338*** 0.6751*** 0.3445***
Table 2: Correlation between new definitions of
depth/density and similarity.
tations discussed in Section 3.
4.1 Depth
The major problem with the current definition of
depth is its failure to take into account the uneven
distribution of number of nodes over the depth value.
As seen in previous examples, the distribution is
rather ?flat? on both ends of depth value, which does
not preserve the linearity of using the ordinal values
of depth and thus introduces much inaccuracy.
To avoid this problem, we ?re-curve? depth value
to the cumulative distribution. Specifically, if we
take the histogram distribution of depth value in Fig-
ure 2 as a probability density function, our approach
is to project cardinal depth values onto its cumula-
tive distribution function. The new depth is denoted
depu and is defined as:
depu(c) = ?c??WN |{c
? : dep(c?)? dep(c)}|
|WN|
Here, dep(?) is the original depth value, and WN is
the set of all nodes in WordNet. The resulting depth
values not only reflect the flat ends, but also preserve
linearity for the depth value range in the middle. In
comparison with Table 1), correlation between depu
and similarity increases over the original depth val-
ues on two of the three data sets (first row in Table
2 and decreases on the RG set. Later, in Section 5,
we show how these marginal improvements translate
into better similarity measures with statistical signif-
icance.
4.2 Density
In theory, a procedure analogous to the above cumu-
lative definition can also be applied to density, i.e.,
by projecting the original values onto the cumula-
tive distribution function. However, due to the Zip-
fian nature of density?s histogram distribution (Fig-
ure 4, in contrast to Gaussian for depth in Figure
2), this is essentially to collapse most density values
Figure 4: Histogram of density in WordNet.
into a very small number of discrete values (which
correspond to the original density of 1 to 3). Ex-
periments show that it does not help in improving
correlation with similarity scores (second row in Ta-
ble 2 for denu): correlation remains the same on MC
and RG, and decreases slightly on FG.
We therefore resort to addressing the issue of in-
formation loss on specificity by inheritance. Intu-
itively, the idea is to ensure that a node be assigned
no less density mass than its parent node(s). In the
?piano? example (Section 3.2), the concept piano is
highly specific due to its large number of siblings
under the parent node percussion instruments. Con-
sequently, the density of its child nodes upright pi-
ano and grand piano should inherit its specificity on
top of their own.
Formally, we redefine density recursively as fol-
lows:
deni(r) = 0
deni(c) = ?h?hyper(c) deni(h)|hyper(c)| +den(c)
where r is the root of WordNet hierarchy (with no
hypernym), and hyper(?) is the set of hypernyms of a
given concept. The first term is the inheritance part,
normalized over all hypernyms of c in case of mul-
tiple inheritance, and the second term is the original
value of density.
The resulting density values correlate signifi-
cantly better with similarity. As shown in row 3
in Table 2, the correlation coefficients are about
tripled on all three data sets with the new density
definition deni, and the significance of correlation
is greatly improved as well (from non-correlating or
1008
marginally correlating to strongly significantly cor-
relating on all three data sets).
5 Using the New Definitions in Semantic
Similarity Measures
In this section, we test the effectiveness of the new
definitions of depth and density by using them in
WordNet-based semantic similarity measures. The
two similarity measures we experiment with are that
of Wu and Palmer (1994) and Jiang and Conrath
(1997). The first one used depth only, and the second
one used both depth and density.
The task is to correlate the similarity measures
with human judgment on similarity between word
pairs. We use the same three data sets as in Section
3. despite the fact that MC is a subset of RG data
set, we include both in order to compare with exist-
ing studies.
Correlation coefficient is calculated using Spear-
man?s ?, although results reported by some earlier
studies used parametric tests such as the Pearson
Correlation Coefficient. The reason for our choice
is that the similarity scores of the word pairs in
these data sets do not necessarily conform to nor-
mal distributions. Rather, we are interested in testing
whether the artificial algorithms would give higher
scores to pairs that are regarded closer in meaning
by human judges. A non-parametric test suits better
for this scenario. And this partly explains why our
re-implementations of the models have lower corre-
lation coefficients than in the original studies.
Note that there are other WordNet-based similar-
ity measures using depth and/or density that we opt
to omit for various reasons. Some of them were not
designed for the particular task at hand (e.g., that of
Sussna, 1993, which gives very poor correlation in
this task). Others use depth of the entire WordNet
hierarchy instead of individual nodes as a scaling
factor (e.g., that of Leacock and Chodorow, 1998),
which is unsuitable for illustrating the improvement
brought about by the new depth and density defini-
tions.
Parameterization of the weighting of depth and
density is a common practice to control their indi-
vidual contribution to the final similarity score (e.g.,
? and ? in Equation (2)). Jiang and Conrath already
had separate weights in their original study. In or-
Best Average
MC RB GR MC RB GR
dep 0.7671 0.7824 0.3773 0.7612 0.7686 0.3660
depu 0.7824 0.7912 0.3946 0.7798 0.7810 0.3787
Table 3: Correlation between human judgment and simi-
larity score by Wu and Palmer (1994) using two versions
of depth.
Best AverageMC RB GR MC RB GR
dep,den 0.7875 0.8111 0.3720 0.7689 0.7990 0.3583depu, den 0.8009 0.8181 0.3804 0.7885 0.8032 0.3669dep,deni 0.7882 0.8199 0.3803 0.7863 0.8102 0.3689depu,deni 0.8065 0.8202 0.3818 0.8189 0.8194 0.3715
Table 4: Correlation between human judgment and sim-
ilarity score by Jiang and Conrath (1997) using different
definitions of depth and density.
der to parameterize depth used by Wu and Palmer in
their similarity measure, we also modify Equation
(1) as follows:
sim(c1,c2) = 2 ?dep
?(c)
len(c1,c)+ len(c2,c)+2 ?dep?(c)
where depth is raised to the power of ? to vary its
contribution to the similarity score.
For a number of combinations of the weighting
parameters, we report both the best performance
and the averaged performance over all the param-
eter combinations. The latter number is meaningful
in that it is a good indication of numerical stability of
the parameterization. In addition, parameterization
is able to generate multiple correlation coefficients,
on which statistical tests can be run in order to show
the significance of improvement. We use the range
from 0 to 5 with step 1 for ? and from 0 to 1 with
step 0.1 for ?.
Table 3 and 4 list the experiment results. In both
models, the cumulative definition of depth depu con-
sistently improve the performance of the similarity
measures. In the Jiang and Conrath (1997) model,
where density is applicable, the inheritance-based
definition of density deni also results in better cor-
relation with human judgments. The optimal result
is achieved when combining the new definitions of
depth and density (row 4 in Table 4). For average
performance, the improvement of all the new def-
initions over the original definitions is statistically
1009
significant on all three data sets according to paired
t-test.
6 Conclusions
This study explored effective uses of depth and/or
density in WordNet-based similarity measures. We
started by examining how well these two structural
features correlate with human judgment on word
pair similarities. This direct comparison showed that
depth correlates with similarity only on certain value
ranges, while density does not correlate with human
judgment at all.
Further investigation revealed that the problem for
depth lies in the simplistic representation as its ordi-
nal integer values. The linearity in this representa-
tion fails to take into account the conflated quantity
of depth in the two extreme ends of the depth spec-
trum. For density, a prominent issue is the informa-
tion loss on specificity of WordNet concepts, which
gives an inaccurate density value that is biased by
the idiosyncratic constructions in densely connected
regions in the hierarchy.
We then proposed new definitions of depth and
density to address these issues. For depth, linear-
ity in different value ranges is realistically reflected
by projecting the depth value to its cumulative dis-
tribution function. The loss of specificity informa-
tion in density, on the other hand, is corrected by
allowing concepts to inherit specificity information
from their parent nodes. The new definitions show
significant improvement in correlation of semantic
similarity given by human judges. In addition, when
used in existing WordNet-based similarity measures,
they consistently improve performance and numeri-
cal stability of the parameterization of the two fea-
tures.
The notions of depth and density pertain to any
hierarchical structure like WordNet, which suggests
various extensions of this work. A natural next step
of the current work is to apply the same idea to se-
mantic taxonomies in languages other than English
with available similarity judgments are also avail-
able. Extrinsic tasks using WordNet-based semantic
similarity can potentially benefit from these refined
notions of depth and density as well.
Acknowledgments
This study was inspired by lectures given by Profes-
sor Gerald Penn of the University of Toronto, and
was financially supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Eneko Agirre and German Rigau. Word sense dis-
ambiguation using conceptual density. In Pro-
ceedings of the 16th Conference on Computa-
tional Linguistics, pages 16?22. Association for
Computational Linguistics, 1996.
Martin Chodorow, Roy Byrd, and George Heidorn.
Extracting semantic hierarchies from a large on-
line dictionary. In Proceedings of the 23rd An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 299?304, Chicago, Illi-
nois, USA, 1985.
Christiane Fellbaum. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, MA, 1998.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. Placing search in context: The con-
cept revisited. In Proceedings of the 10th Inter-
national Conference on World Wide Web, pages
406?414. ACM, 2001.
Graeme Hirst and David St-Onge. Lexical chains
as representations of context for the detection and
correction of malapropisms. In Christiane Fell-
baum, editor, WordNet: An Electronic Lexical
Database, pages 305?332. 1998.
Mario Jarmasz and Stan Szpakowicz. Roget?s the-
saurus and semantic similarity. In Proceedings
of International Conference on Recent Advances
in Natural Language Processing, pages 212?219,
Borovets, Bulgaria, 2003.
Jay Jiang and David Conrath. Semantic similarity
based on corpus statistics and lexical taxonomy.
Proceedings of International Conference on Re-
search in Computational Linguistics, 33, 1997.
Hideki Kozima and Akira Ito. Context-sensitive
measurement of word distance by adaptive scal-
ing of a semantic space. Recent Advances in Nat-
ural Language Processing: Selected Papers from
RANLP, 95:111?124, 1997.
1010
Claudia Leacock and Martin Chodorow. Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal database, 49(2):265?283, 1998.
Dekang Lin. Automatic retrieval and clustering of
similar words. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics,
pages 768?774, Montreal, Canada, 1998.
Goerge Miller and Walter Charles. Contextual cor-
relates of semantic similarity. Language and Cog-
nitive Processes, 6(1):1?28, 1991.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at Human Language Technologies - North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 38?41. Association for Com-
putational Linguistics, 2004.
Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria
Blettner. Development and application of a metric
on semantic nets. IEEE Transactions on Systems,
Man and Cybernetics, 19(1):17?30, 1989.
Philip Resnik. Semantic similarity in a taxon-
omy: an information-based measure and its ap-
plication to problems of ambiguity in natural lan-
guage. Journal of Artificial Intelligence Research,
11(11):95?130, 1999.
R. Richardson and A.F. Smeaton. Using WordNet
in a knowledge-based approach to information re-
trieval. In Proceedings of the BCS-IRSG Collo-
quium, Crewe. Citeseer, 1995.
Herbert Rubenstein and John Goodenough. Contex-
tual correlates of synonymy. Communications of
the ACM, 8(10):627?633, 1965.
Michael Sussna. Word sense disambiguation for
free-text indexing using a massive semantic net-
work. In Proceedings of the second international
conference on Information and knowledge man-
agement, pages 67?74. ACM, 1993.
Peter Turney. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. Proceedings of the
Twelfth European Conference on Machine Learn-
ing, pages 491?502, 2001.
Zhibiao Wu and Martha Palmer. Verb semantics
and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Compu-
tational Linguistics, pages 133?138. Association
for Computational Linguistics, 1994.
1011
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1255?1265, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Resolving ?This-issue? Anaphora
Varada Kolhatkar
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
varada@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
We annotate and resolve a particular
case of abstract anaphora, namely, this-
issue anaphora. We propose a candidate
ranking model for this-issue anaphora
resolution that explores different issue-
specific and general abstract-anaphora
features. The model is not restricted
to nominal or verbal antecedents; rather,
it is able to identify antecedents that
are arbitrary spans of text. Our re-
sults show that (a) the model outperforms
the strong adjacent-sentence baseline;
(b) general abstract-anaphora features,
as distinguished from issue-specific fea-
tures, play a crucial role in this-issue
anaphora resolution, suggesting that our
approach can be generalized for other
NPs such as this problem and this debate;
and (c) it is possible to reduce the search
space in order to improve performance.
1 Introduction
Anaphora in which the anaphoric expression refers
to an abstract object such as a proposition, a prop-
erty, or a fact is known as abstract object anaphora.
This is seen in the following examples.
(1) [Be careful what you wish... because wishes
sometimes come true.]i [That]i is what the
Semiconductor Industry Association, which rep-
resents U.S. manufacturers, has been learning.
(from Asher (1993))
(2) This prospective study suggested [that oral
carvedilol is more effective than oral meto-
prolol in the prevention of AF after on-pump
CABG]i. It is well tolerated when started before
and continued after the surgery. However, further
prospective studies are needed to clarify [this is-
sue]i.
(3) In principle, he said, airlines should be allowed
[to sell standing-room-only tickets for adults]i
? as long as [this decision]i was approved by
their marketing departments.
These examples highlight a difficulty not found with
nominal anaphora. First, the anaphors refer to ab-
stract concepts that can be expressed with differ-
ent syntactic shapes which are usually not nominals.
The anaphor That in (1) refers to the proposition in
the previous utterance, whereas the anaphor this is-
sue in (2) refers to a clause from the previous text.
In (3), the anaphoric expression this decision refers
to a verb phrase from the same sentence. Second,
the antecedents do not always have precisely defined
boundaries. In (2), for example, the whole sentence
containing the marked clause could also be thought
to be the correct antecedent. Third, the actual refer-
ents are not always the precise textual antecedents.
The actual referent in (2), the issue to be clarified,
is whether oral carvedilol is more effective than oral
metoprolol in the prevention of AF after on-pump
CABG or not, a variant of the antecedent text.
Generally, abstract anaphora, as distinguished
from nominal anaphora, is signalled in English by
pronouns this, that, and it (Mu?ller, 2008). But in
abstract anaphora, English prefers demonstratives
to personal pronouns and definite articles (Pas-
sonneau, 1989; Navarretta, 2011).1 Demonstra-
1This is not to say that personal pronouns and definite arti-
cles do not occur in abstract anaphora, but they are not common.
1255
tives can be used in isolation (That in (1)) or with
nouns (e.g., this issue in (2)). The latter follows
the pattern demonstrative {modifier}* noun. The
demonstrative acts as a determiner and the noun fol-
lowing the demonstrative imposes selectional con-
straints for the antecedent, as in examples (2) and
(3). Francis (1994) calls such nouns label nouns,
which ?serve to encapsulate or package a stretch
of discourse?. Schmid (2000) refers to them as
shell nouns, a metaphoric term which reflects differ-
ent functions of these nouns such as encapsulation,
pointing, and signalling.
Demonstrative nouns, along with pronouns like
both and either, are referred to as sortal anaphors
(Castan?o et al 2002; Lin and Liang, 2004; Torii
and Vijay-Shanker, 2007). Castan?o et alobserved
that sortal anaphors are prevalent in the biomedi-
cal literature. They noted that among 100 distinct
anaphors derived from a corpus of 70 Medline ab-
stracts, 60% were sortal anaphors. But how often
do demonstrative nouns refer to abstract objects?
We observed that from a corpus of 74,000 randomly
chosen Medline2 abstracts, of the first 150 most fre-
quently occurring distinct demonstrative nouns (fre-
quency > 30), 51.3% were abstract, 41.3% were
concrete, and 7.3% were discourse deictic. This
shows that abstract anaphora resolution is an impor-
tant component of general anaphora resolution in the
biomedical domain. However, automatic resolution
of this type of anaphora has not attracted much atten-
tion and the previous work for this task is limited.
The present work is a step towards resolving ab-
stract anaphora in written text. In particular, we
choose the interesting abstract concept issue and
demonstrate the complexities of resolving this-issue
anaphora manually as well as automatically in the
Medline domain. We present our algorithm, results,
and error analysis for this-issue anaphora resolution.
The abstract concept issue was chosen for the fol-
lowing reasons. First, it occurs frequently in all
kinds of text from newspaper articles to novels to
scientific articles. There are 13,489 issue anaphora
instances in the New York Times corpus and 1,116
instances in 65,000 Medline abstracts. Second, it is
abstract enough that it can take several syntactic and
2http://www.nlm.nih.gov/bsd/pmresources.
html
semantic forms, which makes the problem interest-
ing and non-trivial. Third, issue referents in scien-
tific literature generally lie in the previous sentence
or two, which makes the problem tractable. Fourth,
issues in Medline abstracts are generally associated
with clinical problems in the medical domain and
spell out the motivation of the research presented in
the article. So extraction of this information would
be useful in any biomedical information retrieval
system.
2 Related Work
Anaphora resolution has been extensively studied
in computational linguistics (Hirst, 1981; Mitkov,
2002; Poesio et al 2011). But CL research has
mostly focused on nominal anaphora resolution
(e.g., resolving multiple ambiguous mentions of a
single entity representing a person, a location, or an
organization) mainly for two reasons. First, nominal
anaphora is the most frequently occurring anaphora
in most domains, and second, there is a substantial
amount of annotated data available for this kind of
anaphora.
Besides pronominal anaphora, some work has
been done on complement anaphora (Modjeska,
2003) (e.g., British and other European steelmak-
ers). There is also some research on resolving sor-
tal anaphora in the medical domain using domain
knowledge (Castan?o et al 2002; Lin and Liang,
2004; Torii and Vijay-Shanker, 2007). But all these
approaches focus only on the anaphors with nominal
antecedents.
By contrast, the area of abstract object anaphora
remains relatively unexplored mainly because the
standard anaphora resolution features such as agree-
ment and apposition cannot be applied to abstract
anaphora resolution. Asher (1993) built a theoreti-
cal framework to resolve abstract anaphora. He di-
vided discourse abstract anaphora into three broad
categories: event anaphora, proposition anaphora,
and fact anaphora, and discussed how abstract en-
tities can be resolved using discourse representa-
tion theory. Chen et al(2011) focused on a sub-
set of event anaphora and resolved event corefer-
ence chains in terms of the representative verbs of
the events from the OntoNotes corpus. Our task dif-
fers from their work as follows. Chen et almainly
1256
focus on events and actions and use verbs as a proxy
for the non-nominal antecedents. But this-issue an-
tecedents cannot usually be represented by a verb.
Our work is not restricted to a particular syntactic
type of the antecedent; rather we provide the flexibil-
ity of marking arbitrary spans of text as antecedents.
There are also some prominent approaches to ab-
stract anaphora resolution in the spoken dialogue
domain (Eckert and Strube, 2000; Byron, 2004;
Mu?ller, 2008). These approaches go beyond nom-
inal antecedents; however, they are restricted to spo-
ken dialogues in specific domains and need serious
adaptation if one wants to apply them to arbitrary
text.
In addition to research on resolution, there is
also some work on effective annotation of abstract
anaphora (Strube and Mu?ller, 2003; Botley, 2006;
Poesio and Artstein, 2008; Dipper and Zinsmeister,
2011). However, to the best of our knowledge, there
is currently no English corpus annotated for issue
anaphora antecedents.
3 Data and Annotation
To create an initial annotated dataset, we collected
188 this {modifier}* issue instances along with the
surrounding context from Medline abstracts.3 Five
instances were discarded as they had an unrelated
(publication related) sense. Among the remaining
183 instances, 132 instances were independently an-
notated by two annotators, a domain expert and a
non-expert, and the remaining 51 instances were an-
notated only by the domain expert. We use the for-
mer instances for training and the latter instances
(unseen by the developer) for testing. The anno-
tator?s task was to mark arbitrary text segments
as antecedents (without concern for their linguistic
types). To make the task tractable, we assumed that
an antecedent does not span multiple sentences but
lies in a single sentence (since we are dealing with
singular this-issue anaphors) and that it is a continu-
ous span of text.
3Although our dataset is rather small, its size is similar to
other available abstract anaphora corpora in English: 154 in-
stances in Eckert and Strube (2000), 69 instances in Byron
(2003), 462 instances annotated by only one annotator in Botley
(2006), and 455 instances restricted to those which have only
nominal or clausal antecedents in Poesio and Artstein (2008).
r11 r12 r13 r14 r15
r21 r22 r23 r24 r25
Annotator 1
Annotator 2
r16 r17 r18 r19
r26 r27 r28 r29 r2,10
id2
Intersections 1 2 3 4 5 6 7 8 9 10 11 12 13 14
id3 id4 id5
id1 id2 id3 id4 id5
Figure 1: Example of annotated data. Bold segments
denote the marked antecedents for the corresponding
anaphor ids. rh j is the jth section identified by the an-
notator h.
3.1 Inter-annotator Agreement
This kind of annotation ? identifying and marking
arbitrary units of text that are not necessarily con-
stituents ? requires a non-trivial variant of the usual
inter-annotator agreement measures. We use Krip-
pendorff?s reliability coefficient for unitizing (?u)
(Krippendorff, 1995) which has not often been used
or described in CL. In our context, unitizing means
marking the spans of the text that serve as the an-
tecedent for the given anaphors within the given text.
The coefficient ?u assumes that the annotated sec-
tions do not overlap in a single annotator?s output
and our data satisfies this criterion.4 The general
form of coefficient ?u is:
?u = 1? u
Do
uDe
(1)
where uDo and uDe are observed and expected dis-
agreements respectively. Both disagreement quanti-
ties express the average squared differences between
the mismatching pairs of values assigned by anno-
tators to given units of analysis. ?u = 1 indicates
perfect reliability and ?u = 0 indicates the absence
of reliability. When ?u < 0, the disagreement is sys-
tematic. Annotated data with reliability of ?u? 0.80
is considered reliable (Krippendorff, 2004).
Krippendorff?s ?u is non-trivial, and explaining it
in detail would take too much space, but the general
idea, in our context, is as follows. The annotators
mark the antecedents corresponding to each anaphor
in their respective copies of the text, as shown in Fig-
ure 1. The marked antecedents are mutually exclu-
sive sections r; we denote the jth section identified
4If antecedents overlap with each other in a single annota-
tor?s output (which is a rare event) we construct data that satis-
fies the non-overlap criterion by creating different copies of the
same text corresponding to each anaphor instance.
1257
Antecedent type Distribution Example
clause 37.9% There is a controversial debate (SBAR whether back school program might improve
quality of life in back pain patients). This study aimed to address this issue.
sentence 26.5% (S Reduced serotonin function and abnormalities in the hypothalamic-pituitary-adrenal
axis are thought to play a role in the aetiology of major depression.) We sought to
examine this issue in the elderly ...
mixed 18.2% (S (PP Given these data) (, ,) (NP decreasing HTD to < or = 5 years) (VP may have
a detrimental effect on patients with locally advanced prostate cancer) (. .)) Only a
randomized trial will conclusively clarify this issue.
nominalization 17.4% As (NP the influence of estrogen alone on breast cancer detection) is not established,
we examined this issue in the Women?s Health Initiative trial...
Table 1: Antecedent types. In examples, the antecedent type is in bold and the marked antecedent is in italics.
by the annotator h by rh j. In Figure 1, annotators 1
and 2 have reached different conclusions by identi-
fying 9 and 10 sections respectively in their copies
of the text. Annotator 1 has not marked any an-
tecedent for the anaphor with id = 1, while annotator
2 has marked r21 for the same anaphor. Both anno-
tators have marked exactly the same antecedent for
the anaphor with id = 4. The difference between two
annotated sections is defined in terms of the square
of the distance between the non-overlapping parts of
the sections. The distance is 0 when the sections are
unmarked by both annotators or are marked and ex-
actly same, and is the summation of the squares of
the unmatched parts if they are different. The coeffi-
cient is computed using intersections of the marked
sections. In Figure 1, annotators 1 and 2 have a to-
tal of 14 intersections. The observed disagreement
uDo is the weighted sum of the differences between
all mismatching intersections of sections marked by
the annotators, and the expected disagreement is the
summation of all possible differences of pairwise
combinations of all sections of all annotators nor-
malized by the length of the text (in terms of the
number of tokens) and the number of pairwise com-
binations of annotators.
For our data, the inter-annotator agreement was
?u = 0.86 (uDo = 0.81 and uDe = 5.81) despite the
fact that the annotators differed in their domain ex-
pertise, which suggests that abstract concepts such
as issue can be annotated reliably.
3.2 Corpus Statistics
A gold standard corpus was created by resolving the
cases where the annotators disagreed. Among 132
training instances, the annotators could not resolve
6 instances and we broke the tie by writing to the
authors of the articles and using their response to
resolve the disagreement. In the gold standard cor-
pus, 95.5% of the antecedents were in the current or
previous sentence and 99.2% were in the current or
previous two sentences. Only one antecedent was
found more than two sentences back and it was six
sentences back. One instance was a cataphor, but
the antecedent occurred in the same sentence as the
anaphor. This suggests that for an automatic this-
issue resolution system, it would be reasonable to
consider only the previous two sentences along with
the sentence containing the anaphor.
The distribution of the different linguistic forms
that an antecedent of this-issue can take in our data
set is shown in Table 1. The majority of antecedents
are clauses or whole sentences. A number of an-
tecedents are noun phrases, but these are gener-
ally nominalizations that refer to abstract concepts
(e.g., the influence of estrogen alone on breast can-
cer detection). Some antecedents are not even well-
defined syntactic constituents5 but are combinations
of several well-defined constituents. We denote the
type of such antecedents as mixed. In the corpus,
18.2% of the antecedents are of this type, suggest-
ing that it is not sufficient to restrict the antecedent
search space to well-defined syntactic constituents.6
In our data, we did not find anaphoric chains for
any of the this-issue anaphor instances, which indi-
cates that the antecedents of this-issue anaphors are
5We refer to every syntactic constituent identified by the
parser as a well-defined syntactic constituent.
6Indeed, many of mixed type antecedents (nearly three-
quarters of them) are the result of parser attachment errors, but
many are not.
1258
in the reader?s local memory and not in the global
memory. This observation supports the THIS-NPs
hypothesis (Gundel et al 1993; Poesio and Mod-
jeska, 2002) that this-NPs are used to refer to enti-
ties which are active albeit not in focus, i.e., they are
not the center of the previous utterance.
4 Resolution Algorithm
4.1 Candidate Extraction
For correct resolution, the set of extracted candidates
must contain the correct antecedent in the first place.
The problem of candidate extraction is non-trivial in
abstract anaphora resolution because the antecedents
are of many different types of syntactic constituents
such as clauses, sentences, and nominalizations.
Drawing on our observation that the mixed type an-
tecedents are generally a combination of different
well-defined syntactic constituents, we extract the
set of candidate antecedents as follows. First, we
create a set of candidate sentences which contains
the sentence containing the this-issue anaphor and
the two preceding sentences. Then, we parse every
candidate sentence with the Stanford Parser7. Ini-
tially, the set of candidate constituents contains a
list of well-defined syntactic constituents. We re-
quire that the node type of these constituents be in
the set {S, SBAR, NP, SQ, SBARQ, S+V}. This
set was empirically derived from our data. To each
constituent, there is associated a set of mixed type
constituents. These are created by concatenating the
original constituent with its sister constituents. For
example, in (4), the set of well-defined eligible can-
didate constituents is {NP, NP1} and so NP1 PP1 is
a mixed type candidate.
(4) NP
NP1 PP1 PP2
The set of candidate constituents is updated with
the extracted mixed type constituents. Extracting
mixed type candidate constituents not only deals
with mixed type instances as shown in Table 1, but
as a side effect it also corrects some attachment er-
rors made by the parser. Finally, the constituents
7http://nlp.stanford.edu/software/
lex-parser.shtml
having a number of leaves (words) less than a thresh-
old8 are discarded to give the final set of candidate
constituents.
4.2 Features
We explored the effect of including 43 automati-
cally extracted features (12 feature classes), which
are summarized in Table 2. The features can also be
broadly divided into two groups: issue-specific fea-
tures and general abstract-anaphora features. Issue-
specific features are based on our common-sense
knowledge of the concept of issue and the different
semantic forms it can take; e.g., controversy (X is
controversial), hypothesis (It has been hypothesized
X), or lack of knowledge (X is unknown), where X
is the issue. In our data, we observed certain syn-
tactic patterns of issues such as whether X or not
and that X and the IP feature class encodes this in-
formation. Other issue-specific features are IVERB
and IHEAD. The feature IVERB checks whether
the governing verb of the candidate is an issue
verb (e.g., speculate, hypothesize, argue, debate),
whereas IHEAD checks whether the candidate head
in the dependency tree is an issue word (e.g., contro-
versy, uncertain, unknown). The general abstract-
anaphora resolution features do not make use of
the semantic properties of the word issue. Some
of these features are derived empirically from the
training data (e.g., ST, L, D). The EL feature is bor-
rowed from Mu?ller (2008) and encodes the embed-
ding level of the candidate within the candidate sen-
tence. The MC feature tries to capture the idea of the
THIS-NPs hypothesis (Gundel et al 1993; Poesio
and Modjeska, 2002) that the antecedents of this-
NP anaphors are not the center of the previous utter-
ance. The general abstract-anaphora features in the
SR feature class capture the semantic role of the can-
didate in the candidate sentence. We used the Illinois
Semantic Role Labeler9 for SR features. The gen-
eral abstract-anaphora features also contain a few
lexical features (e.g., M, SC). But these features are
independent of the semantic properties of the word
issue. The general abstract-anaphora resolution fea-
tures also contain dependency-tree features, lexical-
8The threshold 5 was empirically derived. Antecedents in
our training data had on average 17 words.
9http://cogcomp.cs.illinois.edu/page/
software_view/SRL
1259
ISSUE PATTERN (IP)
ISWHETHER 1 iff the candidate follows the pattern SBAR? (IN whether) (S ...)
ISTHAT 1 iff the candidate follows the pattern SBAR? (IN that) (S ...)
ISIF 1 iff the candidate follows the pattern SBAR? (IN iff) (S ...)
ISQUESTION 1 iff the candidate node is SBARQ or SQ
SYNTACTIC TYPE (ST)
ISNP 1 iff the candidate node is of type NP
ISS 1 iff the candidate node is a sentence node
ISSBAR 1 iff the candidate node is an SBAR node
ISSQ 1 iff the candidate node is an SQ or SBARQ node
MIXED 1 iff the candidate node is of type mixed
EMBEDDING LEVEL (EL) (Mu?ller, 2008)
TLEMBEDDING level of embedding of the given candidate in its top clause (the root node of the syntactic tree)
ILEMBEDDING level of embedding of the given candidate in its immediate clause (the closest parent of type S or SBAR)
MAIN CLAUSE (MC)
MCLAUSE 1 iff the candidate is in the main clause
DISTANCE (D)
ISSAME 1 iff the candidate is in the same sentence as anaphor
SADJA 1 iff the candidate is in the adjacent sentence
ISREM 1 iff the candidate occurs 2 or more sentences before the anaphor
POSITION 1 iff the antecedent occurs before anaphor
SEMANTIC ROLE LABELLING (SR)
IVERB 1 iff the governing verb of the given candidate is an issue verb
ISA0 1 iff the candidate is the agent of the governing verb
ISA1 1 iff the candidate is the patient of the governing verb
ISA2 1 iff the candidate is the instrument of the governing verb
ISAM 1 iff the candidate plays the role of modiffication
ISNOR 1 iff the candidate plays no well-defined semantic role in the sentence
DEPENDENCY TREE (DT)
IHEAD 1 iff the candidate head in the dependency tree is an issue word (e.g., controversial, unknown)
ISSUBJ 1 iff the dependency relation of the candidate to its head is of type nominal, controlling or clausal subject
ISOBJ 1 iff the dependency relation of the candidate to its head is of type direct object or preposition obj
ISDEP 1 iff the dependency relation of the candidate to its head is of type dependent
ISROOT 1 iff the candidate is the root of the dependency tree
ISPREP 1 iff the dependency relation of the candidate to its head is of type preposition
ISCONT 1 iff the dependency relation of the candidate to its head is of type continuation
ISCOMP 1 iff the dependency relation of the candidate to its head is of type clausal or adjectival complement
ISSENT 1 iff candidate?s head is the root node
PRESENCE OF MODALS (M)
MODAL 1 iff the given candidate contains a modal verb
PRESENCE OF SUBORDINATING CONJUNCTION (SC)
ISCONT 1 iff the candidate starts with a contrastive subordinating conjunction (e.g., however, but, yet)
ISCAUSE 1 iff the candidate starts with a causal subordinating conjunction (e.g., because, as, since)
ISCOND 1 iff the candidate starts with a conditional subordinating conjunction (e.g., if, that, whether or not)
LEXICAL OVERLAP (LO)
TOS normalized ratio of the overlapping words in candidate and the title of the article
AOS normalized ratio of the overlapping words in candidate and the anaphor sentence
DWS proportion of domain-specific words in the candidate
CONTEXT (C)
ISPPREP 1 iff the preceding word of the candidate is a preposition
ISFPREP 1 iff the following word of the candidate is a preposition
ISPPUNCT 1 iff the preceding word of the candidate is a punctuation
ISFPUNCT 1 iff the following word of the candidate is a punctuation
LENGTH (L)
LEN length of the candidate in words
Table 2: Feature sets for this-issue resolution. All features are extracted automatically.
1260
overlap features, and context features.
4.3 Candidate Ranking Model
Given an anaphor ai and a set of candidate
antecedents C = {C1,C2, ...,Ck}, the problem of
anaphora resolution is to choose the best candidate
antecedent for ai. We follow the candidate-ranking
model proposed by Denis and Baldridge (2008).
The advantage of the candidate-ranking model over
the mention-pair model is that it overcomes the
strong independence assumption made in mention-
pair models and evaluates how good a candidate is
relative to all other candidates.
We train our model as follows. If the anaphor
is a this-issue anaphor, the set C is extracted us-
ing the candidate extraction algorithm from Section
4.1. Then a corresponding set of feature vectors,
C f = {C f 1,C f 2, ...,C f k}, is created using the features
in Table 2. The training instances are created as de-
scribed by Soon et al(2001). Note that the instance
creation is simpler than for general coreference res-
olution because of the absence of anaphoric chains
in our data. For every anaphor ai and eligible can-
didates C f = {C f 1,C f 2, ...,C f k}, we create training
examples (ai,C f i, label),?C f i ? C f . The label is 1
if Ci is the true antecedent of the anaphor ai, oth-
erwise the label is ?1. The examples with label 1
get the rank of 1, while other examples get the rank
of 2. We use SVMrank (Joachims, 2002) for train-
ing the candidate-ranking model. During testing, the
trained model is used to rank the candidates of each
test instance of this-issue anaphor.
5 Evaluation
In this section we present the evaluation of each
component of our resolution system.
5.1 Evaluation of Candidate Extraction
The set of candidate antecedents extracted by the
method from Section 4.1 contained the correct an-
tecedent 92% of the time. Each anaphor had, on
average, 23.80 candidates, of which only 5.19 can-
didates were nominal type. The accuracy dropped
to 84% when we did not extract mixed type candi-
dates. The error analysis of the 8% of the instances
where we failed to extract the correct antecedent re-
vealed that most of these errors were parsing errors
which could not be corrected by our candidate ex-
traction method.10 In these cases, the parts of the
antecedent had been placed in completely different
branches of the parse tree. For example, in (5), the
correct antecedent is a combination of the NP from
the S? V P? NP? PP? NP branch and the PP
from S?V P? PP branch. In such a case, concate-
nating sister constituents does not help.
(5) The data from this pilot study (VP (VBP provide)
(NP (NP no evidence) (PP (IN for) (NP a dif-
ference in hemodynamic effects between pulse
HVHF and CPFA))) (PP in patients with sep-
tic shock already receiving CRRT)). A larger
sample size is needed to adequately explore this
issue.
5.2 Evaluation of this-issue Resolution
We propose two metrics for abstract anaphora eval-
uation. The simplest metric is the percentage of an-
tecedents on which the system and the annotated
gold data agree. We denote this metric as EXACT-
M (Exact Match) and compute it as the ratio of
number of correctly identified antecedents to the to-
tal number of marked antecedents. This metric is
a good indicator of a system?s performance; how-
ever, it is a rather strict evaluation because, as we
noted in section 1, issues generally have no precise
boundaries in the text. So we propose another met-
ric called RLL, which is similar to the ROUGE-L
metric (Lin, 2004) used for the evaluation of auto-
matic summarization. Let the marked antecedents
of the gold corpus for k anaphor instances be G =
?g1,g2, ...,gk? and the system-annotated antecedents
be A = ?a1,a2, ...,ak?. Let the number of words in
G and A be m and n respectively. Let LCS(gi,ai)
be the the number of words in the longest common
subsequence of gi and ai. Then the precision (PRLL)
and recall (RRLL) over the whole data set are com-
puted as shown in equations (2) and (3). PRLL is
the total number of word overlaps between the gold
and system-annotated antecedents normalized by the
number of words in system-annotated antecedents
and RRLL is the total number of such word overlaps
normalized by the number of words in the gold an-
tecedents. If the system picks too much text for an-
tecedents, RRLL is high but PRLL is low. The F-score,
10Extracting candidate constituents from the dependency
trees did not add any new candidates to the set of candidates.
1261
5-fold Cross-Validation Test
PRLL RRLL FRLL EX-M PRLL RRLL FRLL EX-M
1 Adjacent sentence 66.47 86.16 74.93 22.93 61.73 87.69 72.46 24.00
2 Random 50.71 32.84 39.63 8.40 43.75 35.00 38.89 15.69
3 {IP, D, C, LO, EL, M, MC, L, SC, SR, DT} 79.37 83.66 81.11 59.80 71.89 85.74 78.20 58.82
4 {IP, D, C, LO, M, MC, L, SC, DT} 78.71 83.86 81.14 59.89 70.64 88.09 78.40 54.90
5 {IP, D, C, EL, L, SC, SR, DT} 77.95 83.06 80.33 57.41 72.03 84.85 77.92 60.78
6 {IP, D, EL, MC, L, SR, DT} 80.00 84.75 82.24 59.91 68.88 85.29 76.22 56.86
7 {IP, D, M, L, SR} 73.42 83.16 77.90 52.31 70.74 91.03 79.61 50.98
8 {D, C, LO, L, SC, SR, DT} 79.15 85.28 82.04 56.07 67.39 86.32 75.69 52.94
9 issue-specific features 74.66 45.70 56.57 41.42 64.20 45.88 53.52 41.38
10 non-issue features 76.39 79.39 77.82 51.48 71.19 83.24 76.75 58.82
11 All 78.22 82.92 80.41 56.75 71.28 83.24 76.80 56.86
12 Oracle candidate extractor + row 3 79.63 82.26 80.70 58.32 74.65 87.06 80.38 64.71
13 Oracle candidate sentence extractor + row 3 86.67 92.12 89.25 63.72 79.71 91.49 85.20 62.00
Table 3: this-issue resolution results with SVMrank. All means evaluation using all features. Issue-specific features =
{IP, IVERB, IHEAD}. EX-M is EXACT-M.
FRLL, combines these two scores.
PRLL =
1
n
k
?
i=1
LCS(gi,ai) (2)
RRLL =
1
m
k
?
i=1
LCS(gi,ai) (3)
FRLL =
2?PRLL?RRLL
PRLL +RRLL
(4)
The lower bound of FRLL is 0, where no true an-
tecedent has any common substring with the pre-
dicted antecedents and the upper bound is 1, where
all the predicted and true antecedents are exactly the
same. In our results we represent these scores in
terms of percentage.
There are no implemented systems that resolve is-
sue anaphora or abstract anaphora signalled by label
nouns in arbitrary text to use as a comparison. So
we compare our results against two baselines: ad-
jacent sentence and random. The adjacent sentence
baseline chooses the previous sentence as the correct
antecedent. This is a high baseline because in our
data 84.1% of the antecedents lie within the adjacent
sentence. The random baseline chooses a candidate
drawn from a uniform random distribution over the
set of candidates.11
11Note that our FRLL scores for both baselines are rather high
because candidates often have considerable overlap with one
another; hence a wrong choice may still have a high FRLL score.
We carried out two sets of systematic experi-
ments in which we considered all combinations of
our twelve feature classes. The first set consists of
5-fold cross-validation experiments on our training
data. The second set evaluates how well the model
built on the training data works on the unseen test
data.
Table 3 gives results of our system. The first two
rows are the baseline results. Rows 3 to 8 give re-
sults for some of the best performing feature sets.
All systems based on our features beat both base-
lines on F-scores and EXACT-M. The empirically
derived feature sets IP (issue patterns) and D (dis-
tance) appeared in almost all best feature set com-
binations. Removing D resulted in a 6 percentage
points drop in FRLL and a 4 percentage points drop
in EXACT-M scores. Surprisingly, feature set ST
(syntactic type) was not included in most of the best
performing set of feature sets. The combination of
syntactic and semantic feature sets {IP, D, EL, MC,
L, SR, DT} gave the best FRLL and EXACT-M scores
for the cross-validation experiments. For the test-
data experiments, the combination of semantic and
lexical features {D, C, LO, L, SC, SR, DT} gave
the best FRLL results, whereas syntactic, discourse,
and semantic features {IP, D, C, EL, L, SC, SR,
DT} gave the best EXACT-M results. Overall, row
3 of the table gives reasonable results for both cross-
validation and test-data experiments with no statisti-
cally significant difference to the corresponding best
1262
EXACT-M scores in rows 6 and 5 respectively.12
To pinpoint the errors made by our system, we
carried out three experiments. In the first experi-
ment, we examined the contribution of issue-specific
features versus non-issue features (rows 9 and 10).
Interestingly, when we used only non-issue features,
the performance dropped only slightly. The FRLL re-
sults from using only issue-specific features were
below baseline, suggesting that the more general
features associated with abstract anaphora play a
crucial role in resolving this-issue anaphora.
In the second experiment, we determined the er-
ror caused by the candidate extractor component of
our system. Row 12 of the table gives the result
when an oracle candidate extractor was used to add
the correct antecedent in the set of candidates when-
ever our candidate extractor failed. This did not
affect cross-validation results by much because of
the rarity of such instances. However, in the test-
data experiment, the EXACT-M improvements that
resulted were statistically significant. This shows
that our resolution algorithm was able to identify an-
tecedents that were arbitrary spans of text.
In the last experiment, we examined the effect of
the reduction of the candidate search space. We as-
sumed an oracle candidate sentence extractor (Row
13) which knows the exact candidate sentence in
which the antecedent lies. We can see that both
RLL and EXACT-M scores markedly improved in
this setting. In response to these results, we trained
a decision-tree classifier to identify the correct an-
tecedent sentence with simple location and length
features and achieved 95% accuracy in identifying
the correct candidate sentence.
6 Discussion and Conclusions
We have demonstrated the possibility of resolv-
ing complex abstract anaphora, namely, this-issue
anaphora having arbitrary antecedents. The work
takes the annotation work of Botley (2006) and Dip-
per and Zinsmeister (2011) to the next level by re-
solving this-issue anaphora automatically. We pro-
posed a set of 43 automatically extracted features
that can be used for resolving abstract anaphora.
12We performed a simple one-tailed, k-fold cross-validated
paired t-test at significance level p = 0.05 to determine whether
the difference between the EXACT-M scores of two feature
classes is statistically significant.
Our results show that general abstract-anaphora
resolution features (i.e., other than issue-specific
features) play a crucial role in resolving this-issue
anaphora. This is encouraging, as it suggests that
the approach could be generalized for other NPs ?
especially NPs having similar semantic constraints
such as this problem, this decision, and this conflict.
The results also show that reduction of search
space markedly improves the resolution perfor-
mance, suggesting that a two-stage process that first
identifies the broad region of the antecedent and then
pinpoints the exact antecedent might work better
than the current single-stage approach. The rationale
behind this two-stage process is twofold. First, the
search space of abstract anaphora is large and noisy
compared to nominal anaphora.13 And second, it is
possible to reduce the search space and accurately
identify the broad region of the antecedents using
simple features such as the location of the anaphor
in the anaphor sentence (e.g., if the anaphor occurs
at the beginning of the sentence, the antecedent is
most likely present in the previous sentence).
We chose scientific articles over general text be-
cause in the former domain the actual referents are
seldom discourse deictic (i.e., not present in the
text). In the news domain, for instance, which we
have also examined and are presently annotating, a
large percentage of this-issue antecedents lie out-
side the text. For example, newspaper articles often
quote sentences of others who talk about the issues
in their own world, as shown in example (6).
(6) As surprising and encouraging to organizers of
the movement are the Wall Street names added
to their roster. Prominent among them is Paul
Singer, a hedge fund manager who is straight
and chairman of the conservative Manhattan
Institute. He has donated more than $8 million
to various same-sex marriage efforts, in states
including California, Maine, New Hampshire,
New Jersey, New York and Oregon, much of it
since 2007.
?It?s become something that gradually peo-
13If we consider all well-defined syntactic constituents of a
sentence as issue candidates, in our data, a sentence has on av-
erage 43.61 candidates. Combinations of several well-defined
syntactic constituents only add to this number. Hence if we
consider the antecedent candidates from the previous 2 or 3 sen-
tences, the search space can become quite large and noisy.
1263
ple like myself weren?t afraid to fund, weren?t
afraid to speak out on,? Mr. Singer said in an in-
terview. ?I?m somebody who is philosophically
very conservative, and on this issue I thought
that this really was important on the basis of
liberty and actual family stability.?
In such a case, the antecedent of this issue is not
always in the text of the newspaper article itself, but
must be inferred from the context of the quotation
and the world of the speaker quoted. That said, we
do not use any domain-specific information in our
this-issue resolution model. Our features are solely
based on distance, syntactic structure, and semantic
and lexical properties of the candidate antecedents
which could be extracted for text in any domain.
Issue anaphora can also be signalled by demon-
stratives other than this. However, for our initial
study, we chose this issue for two reasons. First, in
our corpus as well as in other general corpora such
as the New York Times corpus, issue occurs much
more frequently with this than other demonstratives.
Second, we did not want to increase the complexity
of the problem by including the plural issues.
Our approach needs further development to make
it useful. Our broad goal is to resolve abstract
anaphora signalled by label nouns in all kinds of
text. At present, the major obstacle is that there
is very little annotated data available that could be
used to train an abstract anaphora resolution sys-
tem. And the understanding of abstract anaphora
itself is still at an early stage; it would be prema-
ture to think about unsupervised approaches. In this
work, we studied the narrow problem of resolution
of this-issue anaphora in the medical domain to get
a good grasp of the general abstract-anaphora reso-
lution problem.
A number of extensions are planned for this work.
First, we will extend the work to resolve other ab-
stract anaphors (e.g., this decision, this problem).
Second, we will experiment with a two-stage reso-
lution approach. Third, we would like to explore the
effect of including serious discourse structure fea-
tures in our model. (The feature sets SC and C en-
code only shallow discourse information.) Finally,
during annotation, we noted a number of issue pat-
terns (e.g., An open question is X, X is under debate);
a possible extension is extracting issues and prob-
lems from text using these patterns as seed patterns.
7 Acknowledgements
We thank Dr. Brian Budgell from the Canadian
Memorial Chiropractic College for annotating our
data and for helpful discussions. We also thank
the anonymous reviewers for their detailed and con-
structive comments. This research was financially
supported by the Natural Sciences and Engineering
Research Council of Canada and by the University
of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Philip Simon Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical Report, University of Rochester.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature. In
Proceedings of the International Symposium on Refer-
ence Resolution for NLP, Alicante, Spain, June.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A unified event coreference resolution by inte-
grating multiple resolvers. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, Chiang Mai, Thailand, November.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Stefanie Dipper and Heike Zinsmeister. 2011. Annotat-
ing abstract anaphora. Language Resources and Eval-
uation, 69:1?16.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51?89.
Gill Francis. 1994. Labelling discourse: an aspect
of nominal group lexical cohesion. In Malcolm
Coulthard, editor, Advances in written text analysis,
pages 83?101, London. Routledge.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
1264
pressions in discourse. Language, 69(2):274?307,
June.
Graeme Hirst. 1981. Anaphora in Natural Language Un-
derstanding: A Survey, volume 119 of Lecture Notes
in Computer Science. Springer.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Klaus Krippendorff. 1995. On the reliability of unitizing
contiguous data. Sociological Methodology, 25:47?
76.
Klaus Krippendorff. 2004. Content Analysis: An In-
troduction to Its Methodology. Sage, Thousand Oaks,
CA, second edition.
Yu-Hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature. In
Proceedings of ROCLING XVI: Conference on Com-
putational Linguistics and Speech Processing, Taiwan,
September.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Ruslan Mitkov. 2002. Anaphora Resolution. Longman.
Natalia N. Modjeska. 2003. Resolving Other-Anaphora.
Ph.D. thesis, School of Informatics, University of Ed-
inburgh.
Christoph Mu?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universita?t Tu?bingen.
Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse phe-
nomena, Go?ttingen, Germany, February.
Rebecca Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of
the Association for Computational Linguistics, pages
51?59, Vancouver, British Columbia, Canada, June.
Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings of
the Sixth International Conference on Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May.
Massimo Poesio and Natalia N. Modjeska. 2002.
The THIS-NPs hypothesis: A corpus-based investiga-
tion. In Proceedings of the 4th Discourse Anaphora
and Anaphor Resolution Conference (DAARC 2002),
pages 157?162, Lisbon, Portugal, September.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics. De Gruyter Mouton, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Manabu Torii and K. Vijay-Shanker. 2007. Sortal
anaphora resolution in Medline abstracts. Computa-
tional Intelligence, 23(1):15?27.
1265
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 300?310,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Interpreting Anaphoric Shell Nouns using Antecedents of
Cataphoric Shell Nouns as Training Data
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Germanistik
Universita?t Hamburg
heike.zinsmeister@uni-hamburg.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Interpreting anaphoric shell nouns
(ASNs) such as this issue and this fact
is essential to understanding virtually
any substantial natural language text.
One obstacle in developing methods
for automatically interpreting ASNs is
the lack of annotated data. We tackle
this challenge by exploiting cataphoric
shell nouns (CSNs) whose construction
makes them particularly easy to interpret
(e.g., the fact that X). We propose an ap-
proach that uses automatically extracted
antecedents of CSNs as training data to
interpret ASNs. We achieve precisions
in the range of 0.35 (baseline = 0.21) to
0.72 (baseline = 0.44), depending upon
the shell noun.
1 Introduction
Anaphors such as this fact and this issue encapsulate
complex abstract entities such as propositions, facts,
and events. An example is shown below.
(1) Here is another bit of advice: Environmental
Defense, a national advocacy group, notes that
?Mowing the lawn with a gas mower produces
as much pollution in half an hour as driving a
car 172 miles.? This fact may help to explain
the recent surge in the sales of the good old-
fashioned push mowers or the battery-powered
mowers.
Here, the anaphor this fact is interpreted with the
help of the clausal antecedent marked in bold. The
antecedent here is complex because it involves a
number of entities and events (e.g., mowing the
lawn, a gas mower) and relationships between them,
and is abstract because the antecedent itself is not a
purely physical entity.
The distinguishing property of these anaphors is
that they contain semantically rich abstract nouns
(e.g., fact in (1)) which characterize and label their
corresponding antecedents. Linguists and philoso-
phers have studied such abstract nouns for decades
(Vendler, 1968; Halliday and Hasan, 1976; Francis,
1986; Ivanic, 1991; Asher, 1993). Our work is in-
spired by one such study, namely that of Schmid
(2000). Following Schmid, we refer to these abstract
nouns as shell nouns, as they serve as conceptual
shells for complex chunks of information. Accord-
ingly, we refer to the anaphoric occurrences of shell
nouns (e.g., this fact in (1)) as anaphoric shell nouns
(ASNs).
An important reason for studying ASNs is their
ubiquity in all kinds of text. Schmid (2000) ob-
served that shell nouns such as fact, idea, point, and
problem were among the 100 most frequently oc-
curring nouns in a corpus of 225 million words of
British English. Moreover, ASNs can play several
roles in organizing a discourse such as encapsulation
of complex information, cohesion, and topic bound-
ary marking. So correct interpretation of ASNs can
be an important step for correct interpretation of a
discourse, and in a number of NLP applications such
as text summarization, information extraction, and
non-factoid question answering.
Despite their importance, ASNs have not received
much attention in Computational Linguistics, and
research in this field remains in its earliest stages. At
300
present, the major obstacle is that there is very little
annotated data available that could be used to train a
supervised machine learning system for robustly in-
terpreting these anaphors, and manual annotation is
an expensive and time-consuming task.
We tackle this challenge by exploiting a category
of examples, as shown in (2), whose construction is
particularly easy to interpret.
(2) Congress has focused almost solely on the fact
that special education is expensive ? and that
it takes away money from regular education.
Here, in contrast with (1), the fact is not anaphoric
in the traditional sense, but is an easy case of a
forward-looking anaphor ? a cataphor. While the
resolution process of this fact in (1) is quite chal-
lenging as it requires the use of semantics and world
knowledge, it is fairly easy to interpret the fact in
(2) based on the syntactic structure alone. We refer
to these easy-to-interpret cataphoric occurrences of
shell nouns as cataphoric shell nouns (CSNs). The
interpretation of both ASNs and CSNs will be re-
ferred to as antecedent.1 The antecedent of the fact
in (2) is given in the post-nominal that clause. We
use the term shell concept to refer to the general no-
tion of a shell noun, i.e., the semantic type of the
antecedent. For example, the notion of an issue is an
important problem which requires a solution.
In this work, we propose an approach to interpret
ASNs that exploits unlabelled but easy-to-interpret
CSN examples to extract characteristic features as-
sociated with the antecedent of different shell con-
cepts. We evaluate our approach using crowdsourc-
ing. Our results show that these unlabelled CSN ex-
amples provide useful linguistic properties that help
in interpreting ASNs.
2 Related work
The resolution of anaphors to non-nominal an-
tecedents has been well analyzed taking discourse
structure and semantic types into account (Web-
ber, 1991; Passonneau, 1989; Asher, 1993). Most
work in machine anaphora resolution, however,
is restricted to anaphora that involve nominal an-
tecedents only (Poesio et al, 2011).
1Sadly, the more-logical term for the interpretation of a
CSN, succedent, does not actually exist.
There are some notable exceptions which have
tackled the challenge of interpreting non-nominal
antecedents (Eckert and Strube, 2000; Strube and
Mu?ller, 2003; Byron, 2004; Mu?ller, 2008). These
approaches are limited as they either rely heavily
on domain-specific syntactic and semantic annota-
tion or prepossessing, or mark only verbal proxies
for non-nominal antecedents.
Recently, Kolhatkar and Hirst (2012) presented
a machine-learning based resolution system for this
issue anaphora, identifying full syntactic phrases as
antecedents. Although they achieved promising re-
sults, their approach was limited in two respects.
First, it focused on only one type of shell noun
anaphora (issue anaphora). Second, their training
data was restricted to MEDLINE abstracts in which
this issue is used in a rather systematic way. Further-
more, their work is based on manually labelled ASN
antecedents, whereas we use automatically identi-
fied CSN antecedents, which we interpret as explic-
itly expressed antecedents in comparison to the more
implicitly expressed ASN antecedents.
Using explicitly expressed structure in the text
to identify implicit structure is not new. The same
idea has been applied before in computational lin-
guistics. Marcu and Echihabi (2002) identified im-
plicit discourse relations using explicit ones. Mark-
ert and Nissim (2005) used Hearst?s (1992) explicit
patterns to learn lexical semantic relations for NP-
coreference and other-anaphora resolution from the
web. Although our work focuses on a different topic,
the methodology is in the same vein.
3 Hypothesis of this work
The hypothesis of this work is that CSN antecedents
and ASN antecedents share some linguistic proper-
ties and hence linguistic knowledge encoded in CSN
antecedents will help in interpreting ASNs. Accord-
ingly, we examine which features present in CSN
antecedents are relevant in interpreting ASNs.
The motivation and intuition behind this hypoth-
esis is as follows. The antecedents of both ASNs
and CSNs represent the corresponding shell con-
cept. So are there any characteristic features asso-
ciated with this shell concept? Do speakers of En-
glish follow certain patterns of syntactic shape or
words, for instance, when they state facts, decisions,
301
The CSN corpus  (211,722 instances) The ASN corpus  (2,323 instances) 
Training 
Training data 
CSN antecedent models  
CSN antecedent extractor Ranked ASN antecedent candidates 
Crowdsourcing evaluation 
Testing 
Figure 1: Overview of our approach
or issues? There is an abundance of data for CSN
antecedents and if we are able to capture particu-
lar linguistic characteristic features associated with
a shell concept using this data, we can use this in-
formation to interpret ASNs. For instance, exam-
ple (2) demonstrates characteristic properties of an-
tecedents of the shell noun fact including that (a)
they are propositions and are generally expressed
with clauses or sentences rather than noun phrases,
and (b) they are generally expressed in the present
tense. Observe that these properties also hold for
the antecedent of this fact in example (1).
We test our hypothesis by building machine learn-
ing models that are trained on automatically ex-
tracted CSN antecedents and then applying these
models to recover ASN antecedents. Figure 1 shows
an overview of our methodology.
4 Background
Formal definition Shell-nounhood is a functional
notion; it is defined by the use of an abstract noun
rather than the inherent properties of the noun itself
(Schmid, 2000). An abstract noun is a shell noun
when the speaker decides to use it as a shell noun.
Shell noun categorization Schmid (2000) gives
a list of 670 English nouns which are frequently
used as shell nouns. He divides them into six
broad semantic classes: factual, linguistic, mental,
modal, eventive, and circumstantial. Table 1 shows
this classification, along with example shell nouns
for each category. For this work, we selected six
frequently occurring shell nouns covering four of
Schmid?s six classes: fact and reason from factual,
issue and decision from mental, question from lin-
guistic, and possibility from modal. These shell
nouns tend to have antecedents that lie within a sin-
gle sentence. We excluded eventive and circumstan-
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question
mental ideas issue, decision
modal judgements possibility
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmid?s classification of shell nouns. The
nouns given in the Example column tend to occur fre-
quently with the respective class. The shell nouns used in
this work are shown in boldface.
Pattern Example
N-to Several people at the group said the deci-
sion to write the letters was not controver-
sial internally.
N-be-to The principal reason is to create a repre-
sentative government rather than to select
the most talented person.
N-that Mr. Shoval left open the possibility that
Israel would move into other West Bank
cities.
N-be-that The simple and reassuring fact is that a
future generation of leaders is seeking new
challenges during challenging times.
N-wh There is now some question whether the
country was ever really in a recession.
N-be-wh Of course, the central, and probably in-
soluble, issue is whether animal testing is
cruel.
Table 2: Easy-to-interpret CSN patterns given by Schmid
(2000). In the Example column, the patterns are marked
in boldface and the antecedents are marked in italics.
tial classes because the shell nouns in these classes
tend to have rather unclear and long antecedents.2
Shell noun patterns Schmid (2000) also provides
a number of lexico-grammatical patterns for shell
nouns. In Section 1, we noted two such patterns:
this-N (this fact in example (1)) and N-that (fact that
in example (2)). We also noted that CSNs with pat-
tern N-that are fairly easy to interpret compared to
the ASN pattern this-N. Table 2 shows some other
easy-to-interpret CSN patterns given by Schmid.
Generally, for all these patterns, the antecedent is
2These observations are based on an exploratory pilot anno-
tation we carried out on sample data of 150 ASN instances.
302
quite easy to extract with a few predefined rules.
Shell antecedent properties Antecedents of
CSNs and ASNs share some properties while they
are distinguished by others. The distinguishing
property is that CSNs, by their construction, have
their antecedents in the same sentence, as shown
in example (2). On the other hand, ASNs can
have long-distance as well as short-distance an-
tecedents.3 The common properties are as follows.
First, antecedents of both ASNs and CSNs represent
the corresponding shell concept, e.g., the notion
of a fact or an issue. Second, in both cases, the
antecedents are complex abstract entities, which
involve a number of entities and relationships
between them. Finally, in both cases, there is no
one-to-one correspondence between the syntactic
type of an antecedent and semantic type of its
referent (Webber, 1991). For instance, a semantic
type such as fact can be expressed with different
syntactic shapes such as a clause, a verb phrase, or
a complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
5 Training phase
As shown in Figure 1, the goal of the training phase
is to build training data from CSNs and their an-
tecedents and train models which can be used for
resolving ASNs.
5.1 The CSN corpus
We automatically constructed a corpus, a subset of
the New York times (NYT) corpus4, which contains
211,722 sentences following CSN patterns from Ta-
ble 2. We considered part-of-speech information5
while looking for the patterns. For instance, in-
stead of the pattern N-that, we actually looked for
{shell noun NN that IN}.
3In our annotated sample data, we observed ASN an-
tecedents as close as the same sentence and as far as 7 sentences
away from the anaphor.
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
5http://nlp.stanford.edu/software/tagger.
shtml
5.2 Antecedent extractor for CSNs
The goal of the antecedent extractor is to create auto-
matically labelled CSN antecedent data. Recall that
antecedents of CSNs can be extracted using simple
predefined rules that are based on the syntactic struc-
ture alone. For instance, the antecedent extraction
rule for example (2) would be: if the example fol-
lows the pattern fact-that, extract the post-nominal
that clause as the antecedent. To come up with a list
of such extraction rules, we systematically analyzed
a sample of examples (about 20 examples) of each
pattern for each shell noun. Table 3 summarizes the
resulting antecedent extraction rules.
The actual antecedent extraction works as fol-
lows. First, we parsed the examples from the CSN
corpus using the Stanford parser.6 Then for each ex-
ample, we applied rules from Table 3 depending on
the shell noun and the pattern it follows to extract
an appropriate syntactic constituent as the CSN an-
tecedent. For instance, for the noun fact following
the N-that pattern, as in example (2), we first looked
for the NP constituent containing the shell noun fact,
and then extracted the sentential constituent follow-
ing the NP constituent as the CSN antecedent. Al-
though, in most of the cases, the antecedent is given
in the post-nominal wh, that, or infinitive clauses,
sometimes it is not present in the immediately fol-
lowing clause but is given only as a predicate, as
shown in (3).
(3) The primary reason that the archdiocese cannot
pay teachers more is that its students cannot af-
ford higher tuition.
In such cases, we looked for the pattern (VP (VB
be verb) X) in the right sibling of the NP contain-
ing the pattern shell noun-that and extracted X as
the CSN antecedent.
Two contradictory goals need to be achieved
while extracting antecedents of CSNs. The first re-
quires only considering CSNs with high-confidence
patterns, whereas the second requires considering as
many patterns as possible to allow a wide variety of
antecedent examples with different linguistic prop-
erties (e.g., syntactic shape). Our antecedent extrac-
tor tries to find a balance between the two goals.
6http://nlp.stanford.edu/software/lex-parser.
shtml
303
fact reason issue decision question possibility
N-to ? ? ? inf clause predicate inf clause
N-be-to ? inf clause inf clause inf clause inf /wh clause inf clause
N-that that clause predicate predicate ? predicate that clause
N-be-that that clause that clause that clause that clause that clause that clause
N-wh ? predicate wh clause wh clause wh clause ?
N-be-wh wh clause wh clause wh clause wh clause wh clause wh clause
Table 3: Content extraction patterns for CSNs. Patterns in boldface are the prominent patterns for the respective shell
noun. inf clause = infinitive clause. Discarded patterns are denoted by ?.
To address the first goal, we filter examples fol-
lowing noisy patterns, i.e., the patterns that do not
unambiguously encode antecedents of that CSN. For
instance, the pattern N-to is a highly preferred pat-
tern for decision, as shown in (4). The antecedent
extraction rule here is relatively simple: if the exam-
ple follows the pattern decision-to, extract the post-
nominal infinitive clause as the correct antecedent.
(4) President Jacques Chirac?s arrogant decision to
defy the world and go ahead with two nuclear
bomb tests in Polynesia deserves contempt.
But the same pattern is noisy for reason. In (5), for
example, the actual reason is not given anywhere in
the sentence. So we discard the examples following
the pattern N-to for reason.
(5) Investors have had reason to worry about stocks.
We also discard examples with negative determiners,
as in (6), because in such cases, the extraction rules
do not precisely give the antecedent of the given
CSN.
(6) He was careful to repeat anew that he had made
no decision to go to war.
For the N-wh pattern, we exclude certain wh
words for certain nouns. For example, we exclude
the wh word which for question as the Penn Tree-
bank tagset7 does not distinguish between which as a
relative pronoun and as a question. We are interested
in the latter but not the former. Other discarded wh
words include which and when for fact; all wh words
except when and why for reason, all wh words except
how and whether for issue; which, whom, when, and
why for decision; which and when for question; and
all wh words for possibility.
7The Stanford tagger we employ uses the Penn Treebank
tagset (Marcus et al, 1993).
To address the second goal of allowing a wide
variety of antecedent examples, we try to include
as many patterns as possible for each shell noun,
as shown in Table 3. For instance, the patterns
question-to and question-be-to will have infinitive
clauses as antecedents (marked as VP or S+VP
by the parser), whereas for the examples follow-
ing patterns question-wh and question-be-wh the
antecedent will be in the wh clauses (marked as
SBAR). For the pattern question-that, the antecedent
will be in the predicate (similar to example (3)),
which can be a prepositional phrase, a noun phrase
or a clause.
5.3 Models for CSN antecedents
The antecedent extractor gives labels for each in-
stance in the CSN corpus. Using this labelled data,
we train machine learning ranking models for dif-
ferent shell concepts that capture the characteristic
features associated with that shell concept. The fol-
lowing sections describe each step of our ranking
models in detail.
5.3.1 Candidate extraction
The first step is to extract the set of eligible an-
tecedent candidates C = {C1,C2, ...,Ck} for the CSN
instance ai. To train a machine learning model we
need positive and negative examples. We already
have positive examples for antecedent candidates ?
the true antecedents given by the antecedent extrac-
tor. But we also need negative examples of an-
tecedent candidates. By their construction, CSNs
have their antecedents in the same sentence. So
we extract all syntactic constituents of this sentence,
given by the Stanford parser. All the syntactic con-
stituents, except the true antecedent, are considered
as negative examples. With this candidate extraction
method, we end up with many more negative exam-
304
ples than positive examples, but that is exactly what
we expect with ASN antecedent candidates, i.e., the
test data on which we will be applying our models.
5.3.2 Features
Although our problem is similar to anaphora res-
olution, we cannot make use of the usual anaphora
or coreference resolution features such as agreement
or string matching (Soon et al, 2001) because of the
nature of ASN and CSN antecedents. We came up
with a set of features based on the properties that
were common in both ASN and CSN antecedents,
according to our judgement.
Syntactic type of the candidate (S) We observed
that each shell noun prefers specific CSN patterns
and each pattern involves a particular syntactic type.
For instance, decision prefers the pattern N-to and
consequently realizes as its antecedents more verb
phrases than, for example, noun phrases. We employ
two versions of syntactic type: fine-grained syntac-
tic type given by the Stanford parser (e.g., NP-TMP,
RRC) and coarse-grained syntactic type (e.g., NP,
VP, S, PP) in which we consider ten basic syntactic
categories and map all fine-grained syntactic types
to these categories.
Context features (C) Context features allow our
models to learn about the contextual clues that signal
the antecedent. This class contains two features: (a)
coarse-grained syntactic type of left and right sib-
lings of the candidate, and (b) part-of-speech tag of
the preceding and following words of the candidate.
Embedding level features (E) These features
(Mu?ller, 2008) encode the embedding level of the
candidate within its sentence. We consider two em-
bedding level features: top embedding level and im-
mediate embedding level. Top embedding level is
the level of embedding of the given candidate with
respect to its top clause (the root node), and immedi-
ate embedding level is the level of embedding with
respect to its immediate clause (the closest ancestor
of type S or SBAR). The intuition behind this fea-
ture is that if the candidate is deep in the parse tree,
it is possibly not salient enough to be an antecedent.
As we consider all syntactic constituents as poten-
tial candidates, there are many that clearly cannot be
antecedents. This feature will allow us to get rid of
this noise.
Subordinating conjunctions (SC) As we can see
in Table 2, subordinating conjunctions are common
with CSN and ASN antecedents. Vendler (1968)
points out that the shell noun fact prefers a that-
clause, and question and issue prefer a wh-question
clause. We observed that the pattern because X is
common with reason. The subordinating conjunc-
tion feature encodes these preferences for different
shell nouns. The feature checks whether the candi-
date follows the pattern SBAR ? (IN sconj) (S ...),
where sconj is a subordinating conjunction.
Verb features (V) A prominent property of CSN
and ASN antecedents is that they tend to contain
verbs. All examples from Table 2, for example, con-
tain verbs. Moreover, certain shell nouns have tense
and aspect preferences. For instance, for shell noun
fact, lexical verbs in past and present tenses predom-
inate (Schmid, 2000), whereas modal forms are ex-
tremely common for possibility. We use three verb
features that capture this idea: (a) presence of verbs
in general, (b) whether the main verb is finite or non-
finite, and (c) presence of modals.
Length features (L) The intuition behind these
features is that CSN and ASN antecedents tend to be
long, especially for nouns such as fact. We consider
two length features: (a) length of the candidate in
words, and (b) relative length of the candidate with
respect to the sentence containing the antecedent.
Lexical features (LX) Our extractor gives us a
large number of antecedent examples for each shell
noun. A natural question is whether certain words
tend to occur more frequently in the antecedent than
non-antecedent parts of the sentence. To deal with
this question, we extracted all antecedent unigrams
(i.e., unigrams occurring in antecedent part of the
sentence) and non-antecedent unigrams (i.e., uni-
grams occurring in non-antecedent parts of the sen-
tence) for each shell noun. Then for all antecedent
unigrams for a particular shell noun, we computed
term goodness in terms of information gain (Yang
and Pedersen, 1997) and considered the first 50
highly ranked unigrams as the lexical features for
that noun. Note that, in contrast with the other fea-
tures, these lexical features are tailored for each shell
noun and are extracted a priori.
305
5.3.3 Candidate ranking models
Now that we have the set of candidate antecedents
and a set of features, we are ready to train CSN an-
tecedent models. We follow the candidate-ranking
models proposed by Denis and Baldridge (2008) be-
cause they allow us to evaluate how good an an-
tecedent candidate is relative to all other candidates.
For every shell noun, we gather automatically ex-
tracted antecedent data given by the extractor for all
instances of that shell noun. Then for each instance
in this data, we extract the set C as explained in
Section 5.3.1. For each candidate Ci ? C, we ex-
tract a feature vector to create a corresponding set of
feature vectors, C f = {C f 1,C f 2, ...,C f k}. For every
CSN ai and a set of feature vectors corresponding
to its eligible candidates C f = {C f 1,C f 2, ...,C f k},
we create training examples (ai,C f i,rank),?C f i ?
C f . The rank is 1 if Ci is same as the true an-
tecedent, i.e., the automatically extracted antecedent
for that CSN, otherwise the rank is 2. We use the
svm rank learn call of SVMrank (Joachims, 2002)
for training the candidate-ranking models.
6 Testing phase
In this phase, we use the learned candidate ranking
models to identify the antecedents of ASNs.
6.1 The ASN corpus
We started with about 450 instances for each of the
six selected shell nouns (2,700 total instances), con-
taining the pattern {this shell noun}. The instances
were extracted from the NYT. Each instance con-
tains three paragraphs from the corresponding NYT
article: the paragraph containing the ASN and two
preceding paragraphs as context. After automati-
cally removing duplicates and ASNs with a non-
abstract sense (e.g., this issue with a publication-
related sense), we were left with 2,323 instances.
6.2 Antecedent identification
Candidate extraction The search space of ASN
antecedents is quite large for two reasons: ASNs
tend to have long-distance as well as short-distance
antecedents, and there is no clear restriction on the
syntactic type of the antecedents. In the ASN cor-
pus, each sentence on average had 49.5 distinct syn-
tactic constituents given by the Stanford parser. If
we consider n preceding sentences, the sentence
containing the anaphor, and one following sentence8
as sources for antecedents, then the average num-
ber of antecedent candidates will be 49.5? (n+ 2).
This is large compared to the search space of ordi-
nary nominal anaphora. In our previous work (Kol-
hatkar et al, 2013), we have developed methods that
identify the sentence containing the antecedent of
the ASN before identifying the precise antecedent.
In brief, given a set of a fixed number of sentences
around the sentence containing an ASN, these meth-
ods reliably identify the sentence containing the an-
tecedent. In this paper, we treat these methods as a
black box.
Given the sentence containing the antecedent,
we extract all syntactic constituents given by the
Stanford parser from that sentence as potential an-
tecedent candidates as for the training phase. In
the training phase, the antecedent is always con-
tained in the set of syntactic constituents given by
the Stanford parser because the extractor obtains the
appropriate antecedent using the syntactic informa-
tion. But in the testing phase, we cannot guarantee
that the true antecedent occurs in the extracted syn-
tactic constituents due to the parser?s errors. So for
robust candidate extraction, we extract all distinct
constituents from the 30-best parses instead of only
considering the best parse, which increases the aver-
age number of candidates from 49.5 to 55.2.
Feature extraction and candidate ranking
Given the antecedent candidates, feature extraction
and candidate ranking are essentially the same as
for the training phase, except of course we do not
know the true antecedent. Once we have the feature
vectors for each antecedent candidate, the appro-
priate trained model, i.e., the model trained for the
corresponding shell noun, is invoked and the can-
didates are ranked using the svm rank classify
call of SVMrank.
7 Evaluation
We evaluate the ranked candidates of ASN instances
using crowdsourcing.
8The ASN corpus contains a few cataphoric examples that
do not follow the standard patterns of the CSNs shown in Table
2, but actually refer to an antecedent in the following sentence
(e.g., Mr. Dukakis put this question to him: X).
306
Interface We chose to use CrowdFlower9 as our
crowdsourcing interface because of its integrated
quality-control mechanism. For instance, it throws
gold questions randomly at the workers and the
workers who do not answer them correctly are not
allowed to continue.
We presented to the crowd evaluators the ASN
instances from the ASN corpus. Recall that each
ASN instance is made up of the paragraph contain-
ing the ASN and two preceding paragraphs as con-
text. We displayed the first 10 highly-ranked candi-
dates (ordered randomly) given by our testing phase
and asked the evaluators to choose the best answer
that represents the ASN antecedent. We encouraged
the evaluators to select None when they did not agree
with any of the displayed answers. We also asked
them how satisfied they were with the displayed an-
swers. We provided them with three options: unsat-
isfied, satisfied, and partially satisfied.
Our job contained 2,323 evaluation units. We
asked for 8 judgements per instance and paid 6
cents per evaluation unit. As we were interested
in the verdict of native speakers of English, we
limited the allowed demographic region to English-
speaking countries.
Results Among the 2,323 ASN instances, 96% of
them were labelled as satisfied, 3% as partially satis-
fied and 1% as unsatisfied. Only 2% of the instances
were labelled as None. As expected, evaluators were
unsatisfied or partially satisfied with the options of
these instances. These results suggest that our res-
olution models trained on automatically extracted
antecedents of CSNs bring the relevant candidates
of ASN antecedents to the top, i.e., within first 10
highly-ranked candidates. This itself is a positive re-
sult given the large search space of ASN antecedent
candidates (more than 55 candidates on average).
Among the evaluation units, more than half of the
evaluators agreed on an answer for 1,810 units. We
used these instances for further analysis.
To examine which CSN antecedent features are
relevant in identifying ASN antecedents, we carried
out ablation experiments with all feature class com-
binations. We compared the rankings given by our
ranker to the crowd?s answer using precision at n
9http://crowdflower.com/
(P@n).10 More specifically, we count the number of
instances where the crowd?s answers occur within
our ranker?s first n choices. P@n then is this count
divided by the total number of instances. Note that
P@1 is equivalent to the standard precision.
We compared our results against two baselines:
preceding sentence and chance. The preceding sen-
tence baseline chooses the previous sentence as the
correct antecedent. The chance baseline chooses a
candidate from a uniform random distribution over
the set of 10 top-ranked candidates.
The results are shown in Table 4. Although dif-
ferent feature combinations gave the best results for
different shell nouns, the features that occur fre-
quently in many best-performing combinations were
embedding level (E), lexical (LX), and subordinat-
ing conjunction (SC) features. The SC features were
particularly effective for issue and question, where
we expected patterns such as whether X.
Surprisingly, the syntactic type features (S) did
not show up very often in the best-performing fea-
ture combinations, suggesting that the ASN an-
tecedents had a greater variety of syntactic types
than what was available in our CSN training data.
The context features (C) did not appear in any of
the best-performing feature combinations. In fact,
they resulted in a sharp decline in the precision. For
instance, for question, adding the context features
to the best-performing combination {E,SC,V,L,LX}
resulted in a drop of 16 percentage points. This
result was not surprising because although the an-
tecedents of ASNs and CSNs share similar proper-
ties such as common words, we know that their con-
text is generally different.
We did not observe specific features associated
with Schmid?s semantic categories. An exception
was the E features which were particularly effective
for the factual nouns fact and reason: the results
with them alone gave high precision (0.68 for fact
and 0.72 for reason). That said, the E features were
present in most of the best-performing combinations
even for the shell nouns in other semantic categories.
10CrowdFlower gives us a unique answer for each instance,
which we take to be the crowd?s answer. During annotation, ev-
ery annotator is presented with a few gold questions randomly
and each annotator is assigned a trust score based on her per-
formance on these gold questions. The unique answer for an
instance is the answer with the highest sum of trusts.
307
fact (43,000 train and 472 test instances)
Features P@1 P@2 P@3 P@4
{E,L,LX} .70? .85 .91 .94
{E,V,L,LX} .68? .86 .92 .95
{E,SC,L,LX} .66? .83 .92 .95
PSbaseline .40 ? ? ?
reason (4,520 train and 443 test instances)
Features P@1 P@2 P@3 P@4
{E,V,L} .72? .86 .90 .93
{E,V} .72? .85 .90 .92
{E,SC,LX} .69? .84 .90 .94
PSbaseline .44 ? ? ?
issue (3,000 train and 303 test instances)
Features P@1 P@2 P@3 P@4
{SC,L} .47? .59 .71 .78
{SC,L,LX} .46? .60 .70 .81
{S,E,SC,L,LX}.40? .61 .72 .81
PSbaseline .30 ? ? ?
decision (42,332 train and 390 test instances)
Features P@1 P@2 P@3 P@4
{E,LX} .35? .53 .67 .76
{E,SC,LX} .30? .48 .65 .75
{E,SC,V,L,LX}.27 .44 .57 .69
PSbaseline .21 ? ? ?
question (9,336 train and 440 test instances)
Features P@1 P@2 P@3 P@4
{E,SC,V,L,LX} .70? .82 .87 .90
{E,SC,LX} .68? .83 .88 .91
{E,SC,V,LX} .69? .80 .87 .91
PSbaseline .25 ? ? ?
possibility (11,735 train and 278 test instances)
Features P@1 P@2 P@3 P@4
{SC,L,LX} .56? .75 .87 .92
{E,SC} .56? .76 .87 .91
{E,L,LX} .54? .76 .86 .91
PSbaseline .34 ? ? ?
Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing
feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the
column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked
with ?(two-sample ?2 test: p < 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,
and P@4, respectively.
The only previous work with which our results
could be compared is that of Kolhatkar and Hirst
(2012). The work reports precision in the range
of 0.41 to 0.61 in resolving this issue anaphora in
the Medline domain. In our case, for this issue in-
stances from the NYT corpus, we achieved precision
in the range of 0.40 to 0.47. Furthermore, we ap-
plied our models to resolve this issue instances from
Kolhatkar and Hirst?s (2012) work.11 Even with
models trained on automatically labelled CSN an-
tecedents, we achieved similar results to Kolhatkar
and Hirst?s results: P@1 of 0.45, P@2 of 0.59, P@3
of 0.65, and P@4 of 0.67. These results show the
domain robustness of our methods with respect to
the shell noun issue. Recall that Kolhatkar and Hirst
(2012) looked at only very specific cases of this is-
sue and used manually annotated data (Section 2),
as opposed to the automatically extracted CSN an-
tecedent data we use.
11We thank an anonymous reviewer for suggesting this to us.
8 Discussion and conclusion
The goal of this paper was to examine to what ex-
tent CSNs help in interpreting ASNs. Based on the
evaluators? satisfaction level and very few None re-
sponses, we conclude that our models trained on
CSN antecedents were able to bring the relevant
ASN antecedent candidates into the top 10 candi-
dates.
When we applied the models trained on CSN an-
tecedents to interpret ASNs, we achieved precision
in the range of 0.35 to 0.72. The precision results as
high as 0.72 for reason and 0.70 for fact and ques-
tion support our hypothesis that the linguistic knowl-
edge provided by CSN antecedents helps in identify-
ing the antecedents of ASNs. We observed different
behaviour for different nouns. The mental nouns is-
sue and decision in general were harder to interpret
than other shell nouns. The models trained on CSNs
achieved precisions of 0.35 for decision and 0.47 for
issue. So there is still much room for improvement.
That said, for the same nouns, the antecedents were
in the first four ranks about 76% to 81% of the times,
308
suggesting that in future research, our models can be
used as base models to reduce the large search space
of ASN antecedent candidates.
We observed a wide range of performance for dif-
ferent shell nouns. One reason is that the size of the
training data was different for different shell nouns.
After excluding the noisy examples (Section 5.2),
there were about 43,000 training examples for fact,
but only about 3,000 for issue. In addition, a par-
ticular shell concept itself can be difficult, e.g., the
very idea of what counts as an issue is more fuzzy
than what counts as a fact.
One limitation of our approach is that it only
learns the properties that are present in CSN an-
tecedents. However, ASN antecedents have addi-
tional properties which are not always captured by
CSN antecedents. For instance, for the shell noun
decision, most of the training examples were infini-
tive phrases of the form to X. But antecedents of the
ASN decision were mostly court decisions and were
expressed with full sentences.
Moreover, although the models trained on CSN
antecedents are able to encode characteristic fea-
tures associated with the general shell concept, they
are unable to distinguish between two competing
candidates both containing the characteristic fea-
tures of that shell concept. For instance, our ap-
proach will not be able to handle the constructed ex-
amples in (7).
(7) The teacher erased the solutions before John had
time to copy them out, as he had momentarily
been distracted by a band playing outside.
a) This fact infuriated him, as the teacher al-
ways erased the board quickly and John sus-
pected it was just to punish anyone who was
lost in thought, even for a moment.
b) This fact infuriated the teacher, who had al-
ready told John several times to focus on
class work.
Here, both propositions possess properties of the
shell concept fact. Understanding the context of the
anaphor itself is crucial in correctly identifying the
fact in each case, which cannot be learnt from CSN
antecedents due to their specific context patterns.
A number of extensions are planned for this work.
First, we plan to use both kinds of data, CSN and
ASN antecedent data, which will give us a basis
for developing a better performing ASN resolver.
We also plan to incorporate contextual features (e.g.,
right-frontier rule (Webber, 1991) and context rank-
ing (Eckert and Strube, 2000)). Finally, we will ex-
amine whether a model trained for one shell noun
can be generalized to other shell nouns from the
same semantic category.
Acknowledgements
We thank the anonymous reviewers for their
constructive comments. This material is based
upon work supported by the United States Air
Force and the Defense Advanced Research Projects
Agency under Contract No. FA8650-09-C-0179,
Ontario/Baden-Wu?rttemberg Faculty Research Ex-
change, and the University of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers, Dordrecht,
Netherlands.
Donna K. Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, Rochester, New
York: University of Rochester.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 660?669,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17:51?89.
Gill Francis. 1986. Anaphoric Nouns. Discourse Anal-
ysis Monographs 11, Birmingham: English Language
Research, University of Birmingham.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Pub Group.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics, pages 539?545, Nantes, France. Associa-
tion for Computational Linguistics.
Roz Ivanic. 1991. Nouns in search of a context: A study
of nouns with both open- and closed-system character-
istics. International Review of Applied Linguistics in
Language Teaching, 29:93?114.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In ACM SIGKDD Conference
309
on Knowledge Discovery and Data Mining (KDD),
pages 133?142.
Varada Kolhatkar and Graeme Hirst. 2012. Resolv-
ing ?this-issue? anaphora. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme Hirst.
2013. Annotating anaphoric shell nouns with their an-
tecedents. In Proceedings of the 7th Linguistic Anno-
tation Workshop and Interoperability with Discourse,
pages 112?121, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368?375, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Christoph Mu?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universita?t Tu?bingen.
Rebecca J. Passonneau. 1989. Getting at discourse refer-
ents. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 51?
59, Vancouver, British Columbia, Canada. Association
for Computational Linguistics.
Massimo Poesio, Simone Ponzetto, and Yannick Versley.
2011. Computational models of anaphora resolution:
A survey. Unpublished.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Topics
in English Linguistics 34. Mouton de Gruyter, Berlin.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages
168?175, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton de Gruyter, The Hague.
Bonnie Lynn Webber. 1991. Structure and ostension
in the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of the 14th International Conference on
Machine Learning, pages 412?420, Nashville, TN.
310
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 499?510,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Resolving Shell Nouns
Varada Kolhatkar
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
varada@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
Shell nouns, such as fact and problem, oc-
cur frequently in all kinds of texts. These
nouns themselves are unspecific, and can
only be interpreted together with the shell
content. We propose a general approach
to automatically identify shell content of
shell nouns. Our approach exploits lexico-
syntactic knowledge derived from the lin-
guistics literature. We evaluate the ap-
proach on a variety of shell nouns with a
variety of syntactic expectations, achiev-
ing accuracies in the range of 62% (base-
line = 33%) to 83% (baseline = 74%) on
crowd-annotated data.
1 Introduction
Shell nouns are abstract nouns, such as fact, issue,
idea, and problem, which facilitate efficiency by
avoiding repetition of long stretches of text. The
shell metaphor comes from Schmid (2000), and it
captures the various functions of these nouns in a
discourse: containment, signalling, pointing, and
encapsulating. Shell nouns themselves are unspe-
cific, and can only be interpreted together with
their shell content, i.e., the propositional content
they encapsulate in the given context. The process
of identifying this content in the given context is
referred to as shell noun resolution or interpreta-
tion. Examples (1), (2), and (3) show usages of the
shell nouns fact and issue. The shell noun phrases
are resolved to the postnominal that clause, the
complement wh clause, and the immediately pre-
ceding clause, respectively.
1,2
(1) The fact that a major label hadn?t been
at liberty to exploit and repackage the
material on CD meant that prices on the
vintage LP market were soaring.
(2) The issue that this country and Congress
must address is how to provide optimal
care for all without limiting access for
the many.
(3) Living expenses are much lower in rural
India than in New York, but this fact is
not fully captured if prices are converted
with currency exchange rates.
Observe that the relation between shell noun
phrases and their shell content is similar to
the relation of abstract anaphora (or cataphora)
(Asher, 1993) with backward- or forward-looking
abstract-object antecedents. For anaphoric shell
noun examples, the shell content precedes the
shell noun phrase, and for cataphoric shell noun
examples the shell content follows the shell noun
phrase.
3
Shell nouns as a group occur frequently in argu-
mentative texts (Schmid, 2000; Flowerdew, 2003;
Botley, 2006). They play an important role in or-
ganizing a discourse and maintaining its coher-
ence (Schmid, 2000; Flowerdew, 2003), and re-
solving them is an important component of var-
ious computational linguistics tasks that rely on
1
Note that the postnominal that-clause in (1) is not a rela-
tive clause: the fact in question is not an argument of exploit
and repackage.
2
All examples in this paper are from the New
York Times corpus (https://catalog.ldc.upenn.edu/
LDC2008T19)
3
We use the terms cataphoric shell noun and anaphoric
shell noun for lack of better alternatives.
499
discourse structure. Accordingly, identifying shell
content can be helpful in summarization, informa-
tion retrieval, and ESL learning (Flowerdew, 2003;
Hinkel, 2004).
Despite their importance in discourse, under-
standing of shell nouns from a computational lin-
guistics perspective is only in the preliminary
stage. Recently, we proposed an approach to anno-
tate and resolve anaphoric cases of six typical shell
nouns: fact, reason, issue, decision, question, and
possibility (Kolhatkar et al., 2013b). This work
drew on the observation that shell nouns following
cataphoric constructions are easy to resolve. We
manually developed rules to identify shell content
for such cases. Later, we used these cataphoric ex-
amples and their shell content as training data to
resolve harder anaphoric examples.
In this paper, we propose a general algorithm to
resolve cataphoric shell noun examples. Our long-
term goal is to build an end-to-end shell-noun res-
olution system. If we want to go beyond the six
shell nouns from our previous work, and general-
ize our approach to other shell nouns, first we need
to develop an approach to resolve cataphoric shell
noun examples. A number of challenges are asso-
ciated with this seemingly easy task. The primary
challenges is that this resolution is in many cru-
cial respects a semantic phenomenon. To obtain
the required semantic knowledge, we exploit the
properties of shell nouns and their categorization
described in the linguistics literature. We evalu-
ate our method using crowdsourcing, and demon-
strate how far one can get with simple, determin-
istic shell content extraction.
2 Related work
Shell-nounhood is a well-established concept in
linguistics (Vendler, 1968; Ivanic, 1991; Asher,
1993; Francis, 1994; Schmid, 2000, inter alia).
However, understanding of shell nouns from a
computational linguistics perspective is only in the
preliminary stage.
Shell nouns take a number of semantic argu-
ments. In this respect, they are similar to the gen-
eral class of argument-taking nominals as given
in the NomBank (Meyers et al., 2004). Simi-
larly, there is a small body of literature that ad-
dresses nominal semantic role labelling (Gerber et
al., 2009) and nominal subcategorization frames
(Preiss et al., 2007). That said, the distinguishing
property of shell nouns is that one of their seman-
tic arguments is the shell content, but the literature
in computational linguistics does not provide any
method that is able to identify the shell content.
The focus of our work is to rectify this.
Shell content represents complex and abstract
objects. So traditional linguistic and psycholin-
guistic principles used in pronominal anaphora
resolution (see the survey by Poesio et al. (2011)),
such as gender and number agreement, are not ap-
plicable in resolving shell nouns. That said, there
is a line of literature on annotating and resolving
personal and demonstrative pronouns, which typi-
cally refer to similar kinds of non-nominal abstract
entities (Passonneau, 1989; Eckert and Strube,
2000; Byron, 2003; M?ller, 2008; Hedberg et
al., 2007; Poesio and Artstein, 2008; Navarretta,
2011, inter alia). Also, there have been attempts
at annotating the shell content of anaphoric occur-
rences of shell nouns (e.g., Botley (2006), Kol-
hatkar et al. (2013a)). However, none of these
approaches attempt to annotate and resolve cat-
aphoric examples such (1) and (2).
3 Challenges
A number of challenges are associated with the
task of resolving cataphoric shell noun examples,
especially when it comes to developing a holistic
approach for a variety of shell nouns.
First, each shell noun has idiosyncrasies. Dif-
ferent shell nouns have different semantic and syn-
tactic expectations, and hence they take different
types of one or more semantic arguments: one in-
troducing the shell content, and others expressing
circumstantial information about the shell noun.
For instance, fact typically takes a single factual
clause as an argument, which is its shell content,
as we saw in example (1), whereas reason expects
two arguments: the cause and the effect, with the
content introduced in the cause, as shown in exam-
ple (4).
4
Similarly, decision takes an agent making
the decision and the shell content is represented as
an action or a proposition, as shown in (5).
5
(4) One reason [that 60 percent of New York
City public-school children read below
grade level]
effect
is [that many elementary
schools don?t have libraries]
cause
.
4
Observe that the postnominal that clause in (4) is not a
relative clause, and still it is not the shell content because it is
not the cause argument of the shell noun reason.
5
Observe that this aspect of shell nouns of taking different
numbers and kinds of complement clauses is similar to verbs
having different subcategorization frames.
500
(5) I applaud loudly the decision of
[Greenburgh]
agent
to ban animal per-
formances.
Second, the relation between a shell noun and
its content is in many crucial respects a seman-
tic phenomenon. For instance, resolving the shell
noun reason to its shell content involves identify-
ing a) that reason generally expects two semantic
arguments: cause and effect, b) that the cause ar-
gument (and not the effect argument) represents
the shell content, and c) that a particular con-
stituent in the given context represents the cause
argument.
Third, at the conceptual level, once we know
which semantic argument represents shell content,
resolving examples such as (4) seems straightfor-
ward using syntactic structure, i.e., by extracting
the complement clause. But at the implementa-
tion level, this is a non-trivial problem for two rea-
sons. The first reason is that examples contain-
ing shell nouns often follow syntactically complex
constructions, including embedded clauses, coor-
dination, and sentential complements. An auto-
matic parser is not always accurate for such ex-
amples. So the challenge is whether the avail-
able tools in computational linguistics such as syn-
tactic parsers and discourse parsers are able to
provide us with the information that is necessary
to resolve these difficult cases. The second rea-
son is that the shell content can occur in many
different constructions, such as apposition (e.g.,
parental ownership of children, a concept that
allows . . . ), postnominal and complement clause
constructions, as we saw in examples (1) and (2),
and modifier constructions (e.g., the liberal trade
policy that . . . ). Moreover, in some constructions,
the content is indefinite (e.g., A bad idea does not
harm until someone acts upon it.) or None be-
cause the example is a non-shell noun usage (e.g.,
this week?s issue of Sports Illustrated), and the
challenge is to identify such cases.
Finally, whether the postnominal clause intro-
duces the shell content or not is dependent on
the context of the shell noun phrase. The reso-
lution can be complicated by complex syntactic
constructions. For instance, when the shell noun
follows verbs such as expect, it becomes difficult
for an automatic system to identify whether the
postnominal or the complement clause is of the
verb or of the shell noun (e.g., they did not expect
the decision to reignite tension in Crown Heights
vs. no one expected the decision to call an elec-
tion). Similarly, shell noun phrases can be ob-
jects of prepositions, and whether the postnomi-
nal clause introduces the shell content or not is de-
pendent on this preposition. For instance, for the
pattern reason that, the postnominal that clause
does not generally introduce the shell content, as
we saw in (4); however, this does not hold when
the shell noun phrase containing reason follows
the preposition for, as shown in (6).
(6) Low tax rates give people an incentive to
work, for the simple reason that they get
to keep more of what they earn.
4 Linguistic framework
Linguists have studied a variety of shell nouns,
their classification, different patterns they follow,
and their semantic and syntactic properties in de-
tail (Vendler, 1968; Ivanic, 1991; Asher, 1993;
Francis, 1994; Schmid, 2000, inter alia). Schmid
points out that being a shell noun is a property of
a specific usage of the noun rather than an inher-
ent property of the word. He provides a list of 670
English nouns that tend to occur as shell nouns. A
few frequently occurring ones are: problem, no-
tion, concept, issue, fact, belief, decision, point,
idea, event, possibility, reason, trouble, question,
plan, theory, aim, and principle.
4.1 Lexico-syntactic patterns
Precisely defining the notion of shell-nounhood
is tricky. A necessary property of shell nouns is
that they are capable of taking clausal arguments,
primarily with two lexico-syntactic constructions:
Noun + postnominal clause and Noun + be + com-
plement clause (Vendler, 1968; Biber et al., 1999;
Schmid, 2000; Huddleston and Pullum, 2002).
Schmid exploits these lexico-syntactic construc-
tions to identify shell noun usages. In particular,
he provides a number of typical lexico-syntactic
patterns that are indicative of either anaphoric or
cataphoric shell noun occurrences. Table 1 shows
these patterns with examples.
Cataphoric These patterns primarily follow two
constructions.
N-be-clause In this construction, the shell
noun phrase occurs as the subject in a subject-
verb-clause construction, with the linking verb be,
and the shell content embedded as a wh clause,
that clause, or to-infinitive clause. The linking
501
Cataphoric
1 N-be-to Our plan is to hire and retain the best managers we can.
2 N-be-that The major reason is that doctors are uncomfortable with uncertainty.
3 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel.
4 N-to The decision to disconnect the ventilator came after doctors found no brain activity.
5 N-that Mr. Shoval left open the possibility that Israel would move into other West Bank cities.
6 N-wh If there ever is any doubt whether a plant is a poppy or not, break off a stem and squeeze it.
7 N-of The concept of having an outsider as Prime Minister is outdated.
Anaphoric
8 th-N Living expenses are much lower in rural India than in New York, but this fact is not fully
captured if prices are converted with currency exchange rates.
9 th-be-N People change. This is a fact.
10 Sub-be-N If the money is available, however, cutting the sales tax is a good idea.
Table 1: Lexico-grammatical patterns of shell nouns (Schmid, 2000). Shell noun phrases are underlined,
the pattern is marked in boldface, and the shell content is marked in italics.
Proportion
Noun N-be-to N-be-that N-be-wh N-to N-that N-wh N-of total
idea 7 2 - 5 23 10 53 91,277
issue - 1 5 7 14 2 71 55,088
concept 1 - - 6 12 - 79 14,301
decision - - - 80 12 1 5 55,088
plan 5 - - 72 17 - 4 67,344
policy 4 1 - 16 25 2 51 24,025
Table 2: Distribution of cataphoric patterns for six shell nouns in the New York Times corpus. Each
column shows the percentage of instances following that pattern. The last column shows the total number
of cataphoric instances of each noun in the corpus.
verb be indicates the semantic identity between the
shell noun and its content in the given context. The
construction follows the patterns in rows 1, 2, and
3 of Table 1.
N-clause This construction includes the cat-
aphoric patterns 4?7 in Table 1. For these patterns
the link between the shell noun and the content
is much less straightforward: whether the post-
nominal clause expresses the shell content or not
is dependent on the shell noun and the syntac-
tic structure under consideration. For instance,
for the shell noun fact, the shell content is em-
bedded in the postnominal that clause, as shown
in (1), but this does not hold for the shell noun
reason in example (4). The N-of pattern is dif-
ferent from other patterns: it follows the con-
struction N-prepositional phrase rather than N-
clause, and since a prepositional phrase can take
different kinds of embedded constituents such as a
noun phrase, a sentential complement, and a verb
phrase, the pattern offers flexibility in the syntactic
type of the shell content.
Anaphoric For these patterns, the link between
the shell noun and the content is created using
linguistic elements such as the, this, that, other,
same, and such. For the patterns 8 and 9 the shell
content does not typically occur in the sentence
containing the shell noun phrase. For the pattern
10, the shell content is the subject in a subject-
verb-N construction.
Pattern preferences Different shell nouns have
different pattern preferences. Table 2 shows the
distribution of cataphoric patterns for six shell
nouns in the New York Times corpus. The shell
nouns idea, issue, and concept prefer N-of pattern,
whereas plan and decision prefer the pattern N-to.
Among all instances of the shell noun decision fol-
502
Idea family
Semantic features: [mental], [conceptual]
Frame: mental; focus on propositional content of IDEA
Nouns: idea, issue, concept, point, notion, theory, . . .
Patterns: N-be-that/of, N-that/of
Plan family
Semantic features: [mental], [volitional], [manner]
Frame: mental; focus on IDEA
Nouns: decision, plan, policy, idea, . . .
Patterns: N-be-to/that, N-to/that
Trouble family
Semantic features: [eventive], [attitudinal], [manner],
[deontic]
Frame: general eventive
Nouns: problem, trouble, difficulty, dilemma, snag
Patterns: N-be-to
Problem family
Semantic features: [factual], [attitudinal], [impeding]
Frame: general factual
Nouns: problem, trouble, difficulty, point, thing, snag,
dilemma , . . .
Patterns: N-be-that/of
Thing family
Semantic features: [factual]
Frame: general factual
Nouns: fact, phenomenon, point, case, thing, business
Patterns: N-that, N-be-that
Reason family
Semantic features: [factual], [causal]
Frame: causal; attentional focus on CAUSE
Nouns: reason, cause, ground, thing
Patterns: N-be-that/why, N-that/why
Table 3: Example families from Schmid (2000). The nouns in boldface are used to evaluate this work.
lowing Schmid?s cataphoric patterns, 80% of the
instances follow the pattern N-to.
6
4.2 Categorization of shell nouns
Schmid classifies shell nouns at three levels. At
the most abstract level, he classifies shell nouns
into six semantic classes: factual, linguistic, men-
tal, modal, eventive, and circumstantial. Each se-
mantic class indicates the type of experience the
shell noun is intended to describe. For instance,
the mental class describes ideas and cognitive
states, whereas the linguistic class describes utter-
ances, linguistic acts, and products thereof.
The next level of classification includes more-
detailed semantic features. Each broad semantic
class is sub-categorized into a number of groups.
A group of an abstract class tries to capture
the semantic features associated with the fine-
grained differences between different usages of
shell nouns in that class. For instance, groups
associated with the mental class are: conceptual,
creditive, dubiative, volitional, and emotive.
The third level of classification consists of fam-
ilies. A family groups together shell nouns with
similar semantic features. Schmid provides 79 dis-
tinct families of 670 shell nouns. Each family is
named after the primary noun in that family. Table
3 shows six families: Idea, Plan, Trouble, Prob-
lem, Thing, and Reason. A shell noun can be
6
Table 2 does not include anaphoric patterns, as this pa-
per is focused on cataphoric shell noun examples. Anaphoric
patterns are common for all shell nouns: among all instances
of a shell noun, approximately 50 to 80% are anaphoric.
a member of multiple families. The nouns sub-
sumed in a family share semantic features. For
instance, all nouns in the Idea family are mental
and conceptual. They are mental because ideas
are only accessible through thoughts, and concep-
tual because they represent reflection or an appli-
cation of a concept. Each family activates a se-
mantic frame. The idea of these semantic frames is
similar to that of frames in Frame semantics (Fill-
more, 1985) and in semantics of grammar (Talmy,
2000). In particular, Schmid follows Talmy?s con-
ception of frames. A semantic frame describes
conceptual structures, its elements, and their in-
terrelationships. For instance, the Reason family
invokes the causal frame, which has cause and ef-
fect as its elements with the attentional focus on
the cause. According to Schmid, the nouns in a
family also share a number of lexico-syntactic fea-
tures. The patterns attribute in Table 3 shows pro-
totypical lexico-syntactic patterns, which attract
the members of the family. Schmid defines attrac-
tion as the degree to which a lexico-grammatical
pattern attracts a certain noun. For instance, the
patterns N-to and N-that attract the shell nouns in
the Plan family, whereas the N-that pattern attracts
the nouns in the Thing family. The pattern N-of is
restricted to a smaller group of nouns such as con-
cept, problem, and issue.
7,8
7
Schmid used the British section of COBUILD?S Bank of
English for his classification.
8
Schmid?s families could help enrich resources such as
FrameNet (Baker et al., 1998) with the shell content informa-
tion.
503
5 Resolution algorithm
With this exposition, the problem of shell noun
resolution is identifying the appropriate seman-
tic argument of the shell noun representing its
shell content. This section describes our algorithm
to resolve shell nouns following cataphoric pat-
terns. The algorithm addresses the primary chal-
lenge of idiosyncrasies of shell nouns by exploit-
ing Schmid?s semantic families (see Section 4.2).
The input of the algorithm is a shell noun instance
following a cataphoric pattern, and the output is
its shell content or None if the shell content is not
present in the given sentence. The algorithm fol-
lows three steps. First, we parse the given sentence
using the Stanford parser.
9
Second, we look for
the noun phrase (NP), where the head of the NP is
the shell noun to be resolved.
10
Finally, we extract
the appropriate shell content, if it is present in the
given sentence.
5.1 Identifying potentially anaphoric
shell-noun constructions
Before starting the actual resolution, first we iden-
tify whether the shell content occurs in the given
sentence or not. According to Schmid, the lexico-
syntactic patterns signal the position of the shell
content. For instance, if the pattern is of the form
N-be-clause, the shell content is more likely to
occur in the complement clause in the same sen-
tence. That said, although on the surface level, the
shell noun seems to follow a cataphoric pattern, it
is possible that the shell content is not given in a
postnominal or a complement clause, as shown in
(7).
(7) Just as weekend hackers flock to the golf
ball most used by PGA Tour players,
recreational skiers, and a legion of youth
league racers, gravitate to the skis worn
by Olympic champions. It is the reason
that top racers are so quick flash their skis
for the cameras in the finish area.
Here, the shell noun and its content are linked via
the pronoun it. For such constructions, the shell
noun phrase and shell content do not occur in the
same sentence. Shell content occurs in the preced-
ing discourse, typically in the preceding sentence.
9http://nlp.stanford.edu/software/
lex-parser.shtml
10
We extract the head of an NP following the heuristics
proposed by Collins (1999, p. 238).
We identify such cases, and other cases where the
shell content is not likely to occur in the postnom-
inal or complements clauses, by looking for the
patterns below in the given order, returning the
shell content when it occurs in the given sentence.
Sub-be-N This pattern corresponds to the
lexico-grammatical pattern in Figure 1(a). If this
pattern is found, there are three main possibilities
for the subject. First, if an existential there occurs
at the subject position, we move to the next pat-
tern. Second, if the subject is it (example (7)), this
or that, we return None, assuming that the con-
tent is not present in the given sentence. Finally,
if the first two conditions are not satisfied, i.e., if
the subject is neither a pronoun not an existential
there, we assume that subject contains a valid shell
content, and return it. An example is shown in (8).
Note that in such cases, unlike other patterns, the
shell content is expressed as a noun phrase.
(8) Strict liability is the biggest issue when
considering what athletes put in their bod-
ies.
Apposition Another case where shell content
does not typically occur in the postnominal or
complement clause is the case of apposition. In-
definite shell noun phrases often occur in apposi-
tion constructions, as shown in (9).
(9) The LH lineup, according to Gale, will
feature ?cab-forward? design, a concept
that particularly pleases him.
In this step, we check for this construction and re-
turn the sentential, verbal, or nominal left sibling
of the shell noun phrase.
Modifier For shell nouns such as issue, phe-
nomenon, and policy, often the shell content is
given in the modifier of the shell noun, as shown
in (10).
(10) But in the 18th century, Leipzig?s central
location in German-speaking Europe and
the liberal trade policy of the Saxon court
fostered publishing.
We deal with such cases as follows. First, we
extract the modifier phrases by concatenating the
modifier words having noun, verb, or adjective
part-of-speech tags. To exclude unlikely modi-
fier phrases as shell content (e.g., good idea, big
504
Parent
NP
Subject
VP
VB*
form of be
NP/NN
head = shell
(a) Sub-be-N pattern
Parent
NP/NN
head = shell
VP
VB*
form of be
SBAR/S
IN
that/wh
S
clause
(b) N-be-clause pattern
Parent
NP/NN
head = shell
SBAR/S
IN
that/wh
S
clause
(c) N-that/wh pattern
Figure 1: Lexico-syntactic patterns for shell nouns
issue), we extract a list of modifiers for a num-
ber of shell nouns and create a stoplist of modi-
fiers. If any of the words in the modifier phrases
is a pronoun or occurs in the stoplist, we move to
the next pattern. If the modifier phrase passes the
stoplist test, to distinguish between non-shell con-
tent and shell content modifiers, we examine the
hypernym paths of the words in the modifier
phrase in WordNet (Fellbaum, 1998). If the synset
abstraction.n.06 occurs in the path, we consider
the modifier phrase to be valid shell content, as-
suming that the shell content of shell nouns most
typically represents an abstract entity.
5.2 Resolving remaining instances
At this stage we are assuming that the shell con-
tent occurs either in the postnominal clause or the
complement clause. So we look for the patterns
below, returning the shell content when found.
N-be-clause The lexico-grammatical pattern
corresponding to the pattern N-be-clause is shown
in Figure 1(b). This is one of the more reliable
patterns for shell content extraction, as the be verb
suggests the semantic identity between the shell
noun and the complement clause. The be-verb
does not necessarily have to immediately follow
the shell noun. For instance, in example (2), the
head of the NP The issue that this country and
Congress must address is the shell noun issue, and
hence it satisfies the construction in Figure 1(b).
N-clause Finally, we look for this pattern. An
example of this pattern is shown in Figure 1(c).
This is the most common (see Table 2) and tricki-
est pattern in terms of resolution, and whether the
shell content is given in the postnominal clause or
not is dependent on the properties of the shell noun
under consideration and the syntactic construction
of the example. For instance, for the shell noun
decision, the postnominal to-infinitive clause typi-
cally represents shell content. But this did not hold
for the shell noun reason, as shown in (11).
(11) The reason to resist becoming a partici-
pant is obvious.
Here, Schmid?s semantic families come in the
picture. We wanted to examine a) the extent to
which the previous steps help in resolution, and b)
whether knowledge extracted from Schmid?s fam-
ilies add value to the resolution. So we employ
two versions of this step.
Include Schmid?s cues (+SC) This version
exploits the knowledge encoded in Schmid?s se-
mantic families (Section 4.2), and extracts post-
nominal clauses only if Schmid?s pattern cues are
satisfied. In particular, given a shell noun, we de-
termine the families in which it occurs and list all
possible patterns of these families as shell content
cues. The postnominal clause is a valid shell con-
tent only if it satisfies these cues. For instance,
the shell noun reason occurs in only one family:
Reason, with the allowed shell content patterns N-
that and N-why. Schmid?s patterns suggest that the
postnominal to-infinitive clauses are not allowed
as shell content for this shell noun, and thus this
step will return None. This version helps correctly
resolving examples such as (11) to None.
Exclude Schmid?s cues (?SC) This version
does not enforce Schmid?s cues in extracting the
postnominal clauses. For instance, the Problem
family does not include N-that/wh/to/of patterns,
but in this condition, we nonetheless allow these
patterns in extracting the shell content of the nouns
from this family.
6 Evaluation data
We claim that our algorithm is able to resolve a
variety of shell nouns. That said, creating eval-
uation data for all of Schmid?s 670 English shell
505
nouns is extremely time-consuming, and is there-
fore not pursued further in the current study. In-
stead we create a sample of representative evalua-
tion data to examine how well the algorithm works
a) on a variety of shell nouns, b) for shell nouns
within a family, c) for shell nouns across families
with completely different semantic and syntactic
expectations, and d) for a variety of shell patterns
from Table 1.
6.1 Selection of nouns
Recall that each shell noun has its idiosyncrasies.
So in order to evaluate whether our algorithm is
able to address these idiosyncrasies, the evalua-
tion data must contain a variety of shell nouns with
different semantic and syntactic expectations. To
examine a), we consider the six families shown in
Table 3. These families span three abstract cat-
egories: mental, eventive, and factual, and five
distinct groups: conceptual, volitional, factual,
causal, and attitudinal. Also, the families have
considerably different syntactic expectations. For
instance, the nouns in the Idea family can have
their content in that or of clauses occurring in N-
clause or N-be-clause constructions, whereas the
Trouble and Problem families do not allow N-
clause pattern. The shell content of the nouns in
the Plan family is generally represented with to-
infinitive clauses. To examine b) and c), we choose
three nouns from each of the first four families
from Table 3. To add diversity, we also include
two shell nouns from the Thing family and a shell
noun from the Reason family. So we selected a
total of 12 shell nouns for evaluation: idea, issue,
concept, decision, plan, policy, problem, trouble,
difficulty, reason, fact, and phenomenon.
6.2 Selection of instances
Recall that the shell content varies based on the
shell noun and the pattern it follows. Moreover,
shell nouns have pattern preferences, as shown in
Table 2. To examine d), we need shell noun exam-
ples following different patterns from Table 1. We
consider the New York Times corpus as our base
corpus, and from this corpus extract all sentences
following the lexico-grammatical patterns in Ta-
ble 1 for the twelve selected shell nouns. Then we
arbitrarily pick 100 examples for each shell noun,
making sure that the selection contains examples
of each cataphoric pattern from Table 1. These
examples consist of 70% examples of each of the
seven cataphoric patterns, and the remaining 30%
of the examples are picked randomly from the dis-
tribution of patterns for that shell noun.
6.3 Crowdsourcing annotation
We designed a crowdsourcing experiment to ob-
tain the annotated data for evaluation. We parse
each sentence using the Stanford parser, and ex-
tract all possible candidates, i.e., arguments of the
shell noun from the parser?s output. Since our ex-
amples include embedding clauses and sentential
complements, the parser is often inaccurate. For
instance, in example (12), the parser attaches only
the first clause of the coordination (that people
were misled) to the shell noun fact.
(12) The fact that people were misled and in-
formation was denied, that?s the reason
that you?d wind up suing.
To deal with such parsing errors, we consider the
30-best parses given by the parser. From these
parses, we extract a list of eligible candidates. This
list includes the arguments of the shell noun given
in the appositional clauses, modifier phrases, post-
nominal that, wh, or to-infinitive clauses, comple-
ment clauses, objects of postnominal prepositions
of the shell noun, and subject if the shell noun fol-
lows subject-be-N construction. On average, there
were three candidates per instance.
After extracting the candidates, we present the
annotators with the sentence, with the shell noun
highlighted, and the extracted candidates. We ask
the annotators to choose the option that provides
the correct interpretation of the highlighted shell
noun. We also provide them the option None of
the above, and ask them to select it if the shell con-
tent is not present in the given sentence or the shell
content is not listed in the list of candidates.
CrowdFlower We used CrowdFlower
11
as our
crowdsourcing platform, which in turn uses vari-
ous worker channels such as Amazon Mechanical
Turk
12
. CrowdFlower offers a number of features.
First, it provides a quiz mode which facilitates
filtering out spammers by requiring an annotator
to pass a certain number of test questions before
starting the real annotation. Second, during an-
notation, it randomly presents test questions with
known answers to the annotators to keep them on
their toes. Based on annotators? responses to the
test questions, each annotator is assigned a trust
11http://crowdflower.com/
12https://www.mturk.com/mturk/welcome
506
? 5 ? 4 ? 3 < 3
idea 53 67 95 5
issue 44 65 95 5
concept 40 56 96 4
decision 50 72 98 2
plan 41 55 95 5
policy 42 61 94 6
problem 52 70 100 0
trouble 44 69 99 1
difficulty 45 61 96 4
reason 48 60 93 7
fact 52 68 98 2
phenomenon 39 56 95 5
all 46 63 96 4
Table 4: Annotator agreement on shell content.
Each column shows the percentage of instances on
which at least n or fewer than n annotators agree
on a single answer.
score: an annotator performing well on the test
questions gets a high trust score. Finally, Crowd-
Flower allows the user to select the permitted de-
mographic areas and skills required.
Settings We asked for at least 5 annotations per
instance by annotators from the English-speaking
countries. The evaluation task contained a total
of 1200 instances, 100 instances per shell noun.
To maintain the annotation quality, we included
105 test questions, distributed among different an-
swers. We paid 2.5 cents per instance and the an-
notation task was completed in less than 24 hours.
Results Table 4 shows the agreement of the
crowd. In most cases, at least 3 out of 5 anno-
tators agreed on a single answer. We took this an-
swer as the gold standard in our evaluation, and
discard the instances where fewer than three anno-
tators agreed. The option None of the above was
annotated for about 30% of the cases. We include
these cases in the evaluation. In total we had 1,257
instances (1,152 instances where at least 3 annota-
tors agreed + 105 test questions).
7 Evaluation results
Baseline We evaluate our algorithm against
crowd-annotated data using a lexico-syntactic
clause (LSC) baseline. Given a sentence con-
taining a shell instance and its parse tree, this
baseline extracts the postnominal or complement
clause from the parse tree depending only upon
the lexico-syntactic pattern of the shell noun. For
instance, for the N-that and N-be-to patterns, it ex-
Nouns LSC A?SC A+SC
1 idea 74 82 83
2 issue 60 75 77
3 concept 51 67 68
4 decision 70 71 73
5 plan 51 63 62
6 policy 58 70 52
7 problem 66 69 59
8 trouble 63 68 50
9 difficulty 68 75 49
10 reason 43 53 77
11 fact 43 55 68
12 phenomenon 33 62 50
13 all 57 69 64
Table 5: Shell noun resolution results. Each col-
umn shows the percent accuracy of resolution with
the respective method. Boldface is best in row.
tracts the postnominal that clause and the comple-
ment to-infinitive clause, respectively.
13
Results Table 5 shows the evaluation results for
the LSC baseline, the algorithm without Schmid?s
cues (A?SC), and the algorithm with Schmid?s
cues (A+SC). The A?SC condition in all cases and
the A+SC condition in some cases outperform the
LSC baseline, which proves to be rather low, espe-
cially for the shell nouns with strict syntactic ex-
pectations (e.g., fact and reason). Thus we see that
our algorithm is adding value.
That said, we observe a wide range of per-
formance for different shell nouns. On the up
side, adding Schmid?s cues helps resolving the
shell nouns with strict syntactic expectations. The
A+SC results for the shell nouns idea, issue, con-
cept, decision, reason, and fact outperform the
baseline and the A?SC results. In particular, the
A+SC results for the shell nouns fact and rea-
son are markedly better than the baseline results.
These nouns have strict syntactic expectations for
the shell content clauses they take: the families
Thing and Certainty of the shell noun fact allow
only a that clause, and the Reason family of the
shell noun reason allows only that and because
clauses for the shell content. These cues help
in correctly resolving examples such as (11) to
None, where the postnominal to-infinitive clause
13
Note that we only extract subordinating clauses (e.g.,
(SBAR (IN that) (clause))) and to-infinitive clauses, and not
relative clauses.
507
describes the purpose or the goal for the reason,
but not the shell content itself.
On the down side, adding Schmid?s cues hurts
the performance of more versatile nouns, which
can take a variety of clauses. Although the A?SC
results for the shell nouns plan, policy, problem,
trouble, difficulty, and phenomenon are well above
the baseline, the A+SC results are markedly be-
low it. That is, Schmid?s cues were deleterious.
Our error analysis revealed that these nouns are
versatile in terms of the clauses they take as shell
content, and Schmid?s cues restrict these clauses
to be selected as shell content. For instance, the
shell noun problem occurs in two semantic fami-
lies with N-be-that/of and N-be-to as pattern cues
(Table 3), and postnominal clauses are not allowed
for this noun. Although these cues help in filtering
some unwanted cases, we observed a large number
of cases where the shell content is given in post-
nominal clauses, as shown in (13).
(13) I was trying to address the problem of un-
reliable testimony by experts in capital
cases.
Similarly, the Plan family does not allow the N-
of pattern. This cue works well for the shell noun
decision from the same family because often the
postnominal of clause is the agent for this shell
noun and not the shell content. However, it hurts
the performance of the shell noun policy, as N-
of is a common pattern for this shell noun (e.g.,
. . . officials in Rwanda have established a policy of
refusing to protect refugees. . . ). Other failures of
the algorithm are due to parsing errors and lack of
inclusion of context information.
8 Discussion and conclusion
In this paper, we proposed a general method to re-
solve shell nouns following cataphoric construc-
tions. This is a first step towards end-to-end shell
noun resolution. In particular, this method can
be used to create training data for any given shell
noun, which can later be used to resolve harder
anaphoric cases of that noun using the method that
we proposed earlier (Kolhatkar et al., 2013b).
The first goal of this work was to point out the
difficulties associated with the resolution of cat-
aphoric cases of shell nouns. The low resolution
results of the LSC baseline demonstrate the diffi-
culties of resolving such cases using syntax alone,
suggesting the need for incorporating more lin-
guistic knowledge in the resolution.
The second goal of this work was to examine to
what extent knowledge derived from the linguis-
tics literature helps in resolving shell nouns. We
conclude that Schmid?s pattern and clausal cues
are useful for resolving nouns with strict syntac-
tic expectations (e.g., fact, reason); however, these
cues are defeasible: they miss a number of cases in
our corpus. It is possible to improve on Schmid?s
cues using crowdsourcing annotation and by ex-
ploiting lexico-syntactic patterns associated with
different shell nouns from a variety of corpora.
One limitation of our approach is that in our res-
olution framework, we do not consider the prob-
lem of ambiguity of nouns that might not be used
as shell nouns. The occurrence of nouns with the
lexical patterns in Table 1 does not always guaran-
tee shell noun usage. For instance, in our data, we
observed a number of instances of the noun issue
with the publication sense (e.g., this week?s issue
of Sports Illustrated).
Our algorithm is able to deal with only a re-
stricted number of shell noun usage constructions,
but the shell content can be expressed in a variety
of other constructions. A robust machine learning
approach that incorporates context and deeper se-
mantics of the sentence, along with Schmid?s cues,
could mitigate this limitation.
This work opens a number of new research di-
rections. Our next planned task is clustering dif-
ferent shell nouns based on the kind of comple-
ments they take in different usages similar to verb
clustering (Merlo and Stevenson, 2000; Schulte im
Walde and Brew, 2002).
Acknowledgements
We thank the anonymous reviewers for their com-
ments. We also thank Suzanne Stevenson, Gerald
Penn, Heike Zinsmeister, Kathleen Fraser, Aida
Nematzadeh, and Ryan Kiros for their feedback.
This research was financially supported by the
Natural Sciences and Engineering Research Coun-
cil of Canada and by the University of Toronto.
References
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers, Dor-
drecht, Netherlands.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
508
ings of the 17th International Conference on Com-
putational Linguistics, volume 1 of COLING ?98,
pages 86?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, and Edward Finegan. 1999. Longman
Grammar of Spoken and Written English. Pearson
ESL, November.
Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Miriam Eckert and Michael Strube. 2000. Dialogue
acts, synchronizing units, and anaphora resolution.
Journal of Semantics, 17:51?89.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6(2):222?
254.
John Flowerdew. 2003. Signalling nouns in discourse.
English for Specific Purposes, 22(4):329?346.
Gill Francis. 1994. Labelling discourse: An aspect of
nominal group lexical cohesion. In M. Coulthard,
editor, Advances in written text analysis, pages 83?
101. Routledge, London.
Matthew Gerber, Joyce Chai, and Adam Meyers. 2009.
The role of implicit argumentation in nominal srl.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 146?154, Boulder, Colorado, June.
Association for Computational Linguistics.
Nancy Hedberg, Jeanette K. Gundel, and Ron
Zacharski. 2007. Directly and indirectly anaphoric
demonstrative and personal pronouns in newspaper
articles. In Proceedings of DAARC-2007 8th Dis-
course Anaphora and Anaphora Resolution Collo-
quium, pages 31?36.
Eli Hinkel. 2004. Teaching Academic ESL Writ-
ing: Practical Techniques in Vocabulary and Gram-
mar (ESL and Applied Linguistics Professional).
Lawrence Erlbaum, Mahwah, NJ, London.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Roz Ivanic. 1991. Nouns in search of a context: A
study of nouns with both open- and closed-system
characteristics. International Review of Applied Lin-
guistics in Language Teaching, 29:93?114.
Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013a. Annotating anaphoric shell nouns
with their antecedents. In Proceedings of the 7th
Linguistic Annotation Workshop and Interoperabil-
ity with Discourse, pages 112?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Varada Kolhatkar, Heike Zinsmeister, and Graeme
Hirst. 2013b. Interpreting anaphoric shell nouns us-
ing antecedents of cataphoric shell nouns as training
data. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 300?310, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Paola Merlo and Suzanne Stevenson. 2000. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation.
Christoph M?ller. 2008. Fully Automatic Resolution of
It, This and That in Unrestricted Multi-Party Dialog.
Ph.D. thesis, Universit?t T?bingen.
Costanza Navarretta. 2011. Antecedent and referent
types of abstract pronominal anaphora. In Proceed-
ings of the Workshop Beyond Semantics: Corpus-
based investigations of pragmatic and discourse
phenomena, G?ttingen, Germany, Feb.
Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51?59, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).
Massimo Poesio, Simone Ponzetto, and Yannick Vers-
ley. 2011. Computational models of anaphora reso-
lution: A survey. Unpublished.
Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007.
A system for large-scale acquisition of verbal, nom-
inal and adjectival subcategorization frames from
corpora. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 912?919, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
509
Hans-J?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. Mouton de Gruyter,
Berlin.
Sabine Schulte im Walde and Chris Brew. 2002. In-
ducing German semantic verb classes from purely
syntactic subcategorisation information. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 223?230,
Philadelphia, PA.
Leonard Talmy. 2000. The windowing of attention.
In Toward a Cognitive Semantics, volume 1, pages
257?309. The MIT Press.
Zeno Vendler. 1968. Adjectives and Nominalizations.
Mouton and Co., The Netherlands.
510
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315?324,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Extending the Entity-based Coherence Model with Multiple Ranks
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
We extend the original entity-based coher-
ence model (Barzilay and Lapata, 2008)
by learning from more fine-grained coher-
ence preferences in training data. We asso-
ciate multiple ranks with the set of permuta-
tions originating from the same source doc-
ument, as opposed to the original pairwise
rankings. We also study the effect of the
permutations used in training, and the effect
of the coreference component used in en-
tity extraction. With no additional manual
annotations required, our extended model
is able to outperform the original model on
two tasks: sentence ordering and summary
coherence rating.
1 Introduction
Coherence is important in a well-written docu-
ment; it helps make the text semantically mean-
ingful and interpretable. Automatic evaluation
of coherence is an essential component of vari-
ous natural language applications. Therefore, the
study of coherence models has recently become
an active research area. A particularly popular
coherence model is the entity-based local coher-
ence model of Barzilay and Lapata (B&L) (2005;
2008). This model represents local coherence
by transitions, from one sentence to the next, in
the grammatical role of references to entities. It
learns a pairwise ranking preference between al-
ternative renderings of a document based on the
probability distribution of those transitions. In
particular, B&L associated a lower rank with au-
tomatically created permutations of a source doc-
ument, and learned a model to discriminate an
original text from its permutations (see Section
3.1 below). However, coherence is matter of de-
gree rather than a binary distinction, so a model
based only on such pairwise rankings is insuffi-
ciently fine-grained and cannot capture the sub-
tle differences in coherence between the permuted
documents.
Since the first appearance of B&L?s model,
several extensions have been proposed (see Sec-
tion 2.3 below), primarily focusing on modify-
ing or enriching the original feature set by incor-
porating other document information. By con-
trast, we wish to refine the learning procedure
in a way such that the resulting model will be
able to evaluate coherence on a more fine-grained
level. Specifically, we propose a concise exten-
sion to the standard entity-based coherence model
by learning not only from the original docu-
ment and its corresponding permutations but also
from ranking preferences among the permutations
themselves.
We show that this can be done by assigning a
suitable objective score for each permutation indi-
cating its dissimilarity from the original one. We
call this a multiple-rank model since we train our
model on a multiple-rank basis, rather than tak-
ing the original pairwise ranking approach. This
extension can also be easily combined with other
extensions by incorporating their enriched feature
sets. We show that our multiple-rank model out-
performs B&L?s basic model on two tasks, sen-
tence ordering and summary coherence rating,
evaluated on the same datasets as in Barzilay and
Lapata (2008).
In sentence ordering, we experiment with
different approaches to assigning dissimilarity
scores and ranks (Section 5.1.1). We also exper-
iment with different entity extraction approaches
315
Manila Miles Island Quake Baco
1 ? ? X X ?
2 S ? O ? ?
3 X X X X X
Table 1: A fragment of an entity grid for five entities
across three sentences.
(Section 5.1.2) and different distributions of per-
mutations used in training (Section 5.1.3). We
show that these two aspects are crucial, depend-
ing on the characteristics of the dataset.
2 Entity-based Coherence Model
2.1 Document Representation
The original entity-based coherence model is
based on the assumption that a document makes
repeated reference to elements of a set of entities
that are central to its topic. For a document d, an
entity grid is constructed, in which the columns
represent the entities referred to in d, and rows
represent the sentences. Each cell corresponds
to the grammatical role of an entity in the corre-
sponding sentence: subject (S), object (O), nei-
ther (X), or nothing (?). An example fragment
of an entity grid is shown in Table 1; it shows
the representation of three sentences from a text
on a Philippine earthquake. B&L define a lo-
cal transition as a sequence {S ,O, X,?}n, repre-
senting the occurrence and grammatical roles of
an entity in n adjacent sentences. Such transi-
tion sequences can be extracted from the entity
grid as continuous subsequences in each column.
For example, the entity ?Manila? in Table 1 has
a bigram transition {S , X} from sentence 2 to 3.
The entity grid is then encoded as a feature vector
?(d) = (p1(d), p2(d), . . . , pm(d)), where pt(d) is
the probability of the transition t in the entity grid,
and m is the number of transitions with length no
more than a predefined optimal transition length
k. pt(d) is computed as the number of occurrences
of t in the entity grid of document d, divided by
the total number of transitions of the same length
in the entity grid.
For entity extraction, Barzilay and Lapata
(2008) had two conditions: Coreference+ and
Coreference?. In Coreference+, entity corefer-
ence relations in the document were resolved by
an automatic coreference resolution tool (Ng and
Cardie, 2002), whereas in Coreference?, nouns
are simply clustered by string matching.
2.2 Evaluation Tasks
Two evaluation tasks for Barzilay and Lapata
(2008)?s entity-based model are sentence order-
ing and summary coherence rating.
In sentence ordering, a set of random permu-
tations is created for each source document, and
the learning procedure is conducted on this syn-
thetic mixture of coherent and incoherent docu-
ments. Barzilay and Lapata (2008) experimented
on two datasets: news articles on the topic of
earthquakes (Earthquakes) and narratives on the
topic of aviation accidents (Accidents). A train-
ing data instance is constructed as a pair con-
sisting of a source document and one of its ran-
dom permutations, and the permuted document
is always considered to be less coherent than the
source document. The entity transition features
are then used to train a support vector machine
ranker (Joachims, 2002) to rank the source docu-
ments higher than the permutations. The model is
tested on a different set of source documents and
their permutations, and the performance is evalu-
ated as the fraction of correct pairwise rankings in
the test set.
In summary coherence rating, a similar exper-
imental framework is adopted. However, in this
task, rather than training and evaluating on a set
of synthetic data, system-generated summaries
and human-composed reference summaries from
the Document Understanding Conference (DUC
2003) were used. Human annotators were asked
to give a coherence score on a seven-point scale
for each item. The pairwise ranking preferences
between summaries generated from the same in-
put document cluster (excluding the pairs consist-
ing of two human-written summaries) are used by
a support vector machine ranker to learn a dis-
criminant function to rank each pair according to
their coherence scores.
2.3 Extended Models
Filippova and Strube (2007) applied Barzilay and
Lapata?s model on a German corpus of newspa-
per articles with manual syntactic, morphological,
and NP coreference annotations provided. They
further clustered entities by semantic relatedness
as computed by the WikiRelated! API (Strube and
Ponzetto, 2006). Though the improvement was
not significant, interestingly, a short subsection in
316
their paper described their approach to extending
pairwise rankings to longer rankings, by supply-
ing the learner with rankings of all renderings as
computed by Kendall?s ?, which is one of our
extensions considered in this paper. Although
Filippova and Strube simply discarded this idea
because it hurt accuracies when tested on their
data, we found it a promising direction for further
exploration. Cheung and Penn (2010) adapted
the standard entity-based coherence model to the
same German corpus, but replaced the original
linguistic dimension used by Barzilay and Lap-
ata (2008) ? grammatical role ? with topologi-
cal field information, and showed that for German
text, such a modification improves accuracy.
For English text, two extensions have been pro-
posed recently. Elsner and Charniak (2011) aug-
mented the original features used in the standard
entity-based coherence model with a large num-
ber of entity-specific features, and their extension
significantly outperformed the standard model
on two tasks: document discrimination (another
name for sentence ordering), and sentence inser-
tion. Lin et al(2011) adapted the entity grid rep-
resentation in the standard model into a discourse
role matrix, where additional discourse informa-
tion about the document was encoded. Their ex-
tended model significantly improved ranking ac-
curacies on the same two datasets used by Barzi-
lay and Lapata (2008) as well as on the Wall Street
Journal corpus.
However, while enriching or modifying the
original features used in the standard model is cer-
tainly a direction for refinement of the model, it
usually requires more training data or a more so-
phisticated feature representation. In this paper,
we instead modify the learning approach and pro-
pose a concise and highly adaptive extension that
can be easily combined with other extended fea-
tures or applied to different languages.
3 Experimental Design
Following Barzilay and Lapata (2008), we wish
to train a discriminative model to give the cor-
rect ranking preference between two documents
in terms of their degree of coherence. We experi-
ment on the same two tasks as in their work: sen-
tence ordering and summary coherence rating.
3.1 Sentence Ordering
In the standard entity-based model, a discrimina-
tive system is trained on the pairwise rankings be-
tween source documents and their permutations
(see Section 2.2). However, a model learned from
these pairwise rankings is not sufficiently fine-
grained, since the subtle differences between the
permutations are not learned. Our major contribu-
tion is to further differentiate among the permuta-
tions generated from the same source documents,
rather than simply treating them all as being of the
same degree of coherence.
Our fundamental assumption is that there exists
a canonical ordering for the sentences of a doc-
ument; therefore we can approximate the degree
of coherence of a document by the similarity be-
tween its actual sentence ordering and that canon-
ical sentence ordering. Practically, we automati-
cally assign an objective score for each permuta-
tion to estimate its dissimilarity from the source
document (see Section 4). By learning from all
the pairs across a source document and its per-
mutations, the effective size of the training data
is increased while no further manual annotation
is required, which is favorable in real applica-
tions when available samples with manually an-
notated coherence scores are usually limited. For
r source documents each with m random permuta-
tions, the number of training instances in the stan-
dard entity-based model is therefore r ? m, while
in our multiple-rank model learning process, it is
r ?
(
m+1
2
)
? 12 r ? m
2 > r ? m, when m > 2.
3.2 Summary Coherence Rating
Compared to the standard entity-based coherence
model, our major contribution in this task is to
show that by automatically assigning an objective
score for each machine-generated summary to es-
timate its dissimilarity from the human-generated
summary from the same input document cluster,
we are able to achieve performance competitive
with, or even superior to, that of B&L?s model
without knowing the true coherence score given
by human judges.
Evaluating our multiple-rank model in this task
is crucial, since in summary coherence rating,
the coherence violations that the reader might en-
counter in real machine-generated texts can be
more precisely approximated, while the sentence
ordering task is only partially capable of doing so.
317
4 Dissimilarity Metrics
As mentioned previously, the subtle differences
among the permutations of the same source docu-
ment can be used to refine the model learning pro-
cess. Considering an original document d and one
of its permutations, we call ? = (1, 2, . . . ,N) the
reference ordering, which is the sentence order-
ing in d, and pi = (o1, o2, . . . , oN) the test order-
ing, which is the sentence ordering in that permu-
tation, where N is the number of sentences being
rendered in both documents.
In order to approximate different degrees of co-
herence among the set of permutations which bear
the same content, we need a suitable metric to
quantify the dissimilarity between the test order-
ing pi and the reference ordering ?. Such a metric
needs to satisfy the following criteria: (1) It can be
automatically computed while being highly corre-
lated with human judgments of coherence, since
additional manual annotation is certainly undesir-
able. (2) It depends on the particular sentence
ordering in a permutation while remaining inde-
pendent of the entities within the sentences; oth-
erwise our multiple-rank model might be trained
to fit particular probability distributions of entity
transitions rather than true coherence preferences.
In our work we use three different metrics:
Kendall?s ? distance, average continuity, and edit
distance.
Kendall?s ? distance: This metric has been
widely used in evaluation of sentence ordering
(Lapata, 2003; Lapata, 2006; Bollegala et al
2006; Madnani et al 2007)1. It measures the
disagreement between two orderings ? and pi in
terms of the number of inversions of adjacent sen-
tences necessary to convert one ordering into an-
other. Kendall?s ? distance is defined as
? =
2m
N(N ? 1)
,
where m is the number of sentence inversions nec-
essary to convert ? to pi.
Average continuity (AC): Following Zhang
(2011), we use average continuity as the sec-
ond dissimilarity metric. It was first proposed
1Filippova and Strube (2007) found that their perfor-
mance dropped when using this metric for longer rankings;
but they were using data in a different language and with
manual annotations, so its effect on our datasets is worth try-
ing nonetheless.
by Bollegala et al(2006). This metric esti-
mates the quality of a particular sentence order-
ing by the number of correctly arranged contin-
uous sentences, compared to the reference order-
ing. For example, if pi = (. . . , 3, 4, 5, 7, . . . , oN),
then {3, 4, 5} is considered as continuous while
{3, 4, 5, 7} is not. Average continuity is calculated
as
AC = exp
?
??????
1
n ? 1
n?
i=2
log (Pi + ?)
?
?????? ,
where n = min(4,N) is the maximum number
of continuous sentences to be considered, and
? = 0.01. Pi is the proportion of continuous sen-
tences of length i in pi that are also continuous in
the reference ordering ?. To represent the dis-
similarity between the two orderings pi and ?, we
use its complement AC? = 1 ? AC, such that the
larger AC? is, the more dissimilar two orderings
are2.
Edit distance (ED): Edit distance is a com-
monly used metric in information theory to mea-
sure the difference between two sequences. Given
a test ordering pi, its edit distance is defined as the
minimum number of edits (i.e., insertions, dele-
tions, and substitutions) needed to transform it
into the reference ordering ?. For permutations,
the edits are essentially movements, which can
be considered as equal numbers of insertions and
deletions.
5 Experiments
5.1 Sentence Ordering
Our first set of experiments is on sentence order-
ing. Following Barzilay and Lapata (2008), we
use all transitions of length ? 3 for feature extrac-
tion. In addition, we explore three specific aspects
in our experiments: rank assignment, entity ex-
traction, and permutation generation.
5.1.1 Rank Assignment
In our multiple-rank model, pairwise rankings
between a source document and its permutations
are extended into a longer ranking with multiple
ranks. We assign a rank to a particular permuta-
tion, based on the result of applying a chosen dis-
similarity metric from Section 4 (?, AC, or ED) to
the sentence ordering in that permutation.
We experiment with two different approaches
to assigning ranks to permutations, while each
2We will refer to AC? as AC from now on.
318
source document is always assigned a zero (the
highest) rank.
In the raw option, we rank the permutations di-
rectly by their dissimilarity scores to form a full
ranking for the set of permutations generated from
the same source document.
Since a full ranking might be too sensitive to
noise in training, we also experiment with the
stratified option, in which C ranks are assigned to
the permutations generated from the same source
document. The permutation with the smallest dis-
similarity score is assigned the same (zero, the
highest) rank as the source document, and the one
with the largest score is assigned the lowest (C?1)
rank; then ranks of other permutations are uni-
formly distributed in this range according to their
raw dissimilarity scores. We experiment with 3
to 6 ranks (the case where C = 2 reduces to the
standard entity-based model).
5.1.2 Entity Extraction
Barzilay and Lapata (2008)?s best results were
achieved by employing an automatic coreference
resolution tool (Ng and Cardie, 2002) for ex-
tracting entities from a source document, and the
permutations were generated only afterwards ?
entity extraction from a permuted document de-
pends on knowing the correct sentence order and
the oracular entity information from the source
document ? since resolving coreference relations
in permuted documents is too unreliable for an au-
tomatic tool.
We implement our multiple-rank model with
full coreference resolution using Ng and Cardie?s
coreference resolution system, and entity extrac-
tion approach as described above ? the Coref-
erence+ condition. However, as argued by El-
sner and Charniak (2011), to better simulate
the real situations that human readers might en-
counter in machine-generated documents, such
oracular information should not be taken into ac-
count. Therefore we also employ two alterna-
tive approaches for entity extraction: (1) use the
same automatic coreference resolution tool on
permuted documents ? we call it the Corefer-
ence? condition; (2) use no coreference reso-
lution, i.e., group head noun clusters by simple
string matching ? B&L?s Coreference? condi-
tion.
5.1.3 Permutation Generation
The quality of the model learned depends on
the set of permutations used in training. We are
not aware of how B&L?s permutations were gen-
erated, but we assume they are generated in a per-
fectly random fashion.
However, in reality, the probabilities of seeing
documents with different degrees of coherence are
not equal. For example, in an essay scoring task,
if the target group is (near-) native speakers with
sufficient education, we should expect their essays
to be less incoherent ? most of the essays will
be coherent in most parts, with only a few minor
problems regarding discourse coherence. In such
a setting, the performance of a model trained from
permutations generated from a uniform distribu-
tion may suffer some accuracy loss.
Therefore, in addition to the set of permutations
used by Barzilay and Lapata (2008) (PSBL), we
create another set of permutations for each source
document (PSM) by assigning most of the proba-
bility mass to permutations which are mostly sim-
ilar to the original source document. Besides its
capability of better approximating real-life situ-
ations, training our model on permutations gen-
erated in this way has another benefit: in the
standard entity-based model, all permuted doc-
uments are treated as incoherent; thus there are
many more incoherent training instances than co-
herent ones (typically the proportion is 20:1). In
contrast, in our multiple-rank model, permuted
documents are assigned different ranks to fur-
ther differentiate the different degrees of coher-
ence within them. By doing so, our model will
be able to learn the characteristics of a coherent
document from those near-coherent documents as
well, and therefore the problem of lacking coher-
ent instances can be mitigated.
Our permutation generation algorithm is shown
in Algorithm 1, where ? = 0.05, ? = 5.0,
MAX NUM = 50, and K and K? are two normal-
ization factors to make p(swap num) and p(i, j)
proper probability distributions. For each source
document, we create the same number of permu-
tations as PSBL.
5.2 Summary Coherence Rating
In the summary coherence rating task, we are
dealing with a mixture of multi-document sum-
maries generated by systems and written by hu-
mans. Barzilay and Lapata (2008) did not assume
319
Algorithm 1 Permutation Generation.
Input: S 1, S 2, . . . , S N ; ? = (1, 2, . . . ,N)
Choose a number of sentence swaps
swap num with probability e???swap num/K
for i = 1? swap num do
Swap a pair of sentence (S i, S j)
with probability p(i, j) = e???|i? j|/K?
end for
Output: pi = (o1, o2, . . . , oN)
a simple binary distinction among the summaries
generated from the same input document clus-
ter; rather, they had human judges give scores for
each summary based on its degree of coherence
(see Section 3.2). Therefore, it seems that the
subtle differences among incoherent documents
(system-generated summaries in this case) have
already been learned by their model.
But we wish to see if we can replace hu-
man judgments by our computed dissimilarity
scores so that the original supervised learning is
converted into unsupervised learning and yet re-
tain competitive performance. However, given
a summary, computing its dissimilarity score is
a bit involved, due to the fact that we do not
know its correct sentence order. To tackle this
problem, we employ a simple sentence align-
ment between a system-generated summary and
a human-written summary originating from the
same input document cluster. Given a system-
generated summary Ds = (S s1, S s2, . . . , S sn) and
its corresponding human-written summary Dh =
(S h1, S h2, . . . , S hN) (here it is possible that n ,
N), we treat the sentence ordering (1, 2, . . . ,N)
in Dh as ? (the original sentence ordering), and
compute pi = (o1, o2, . . . , on) based on Ds. To
compute each oi in pi, we find the most similar
sentence S h j, j ? [1,N] in Dh by computing their
cosine similarity over all tokens in S h j and S si;
if all sentences in Dh have zero cosine similarity
with S si, we assign ?1 to oi.
Once pi is known, we can compute its ?dissimi-
larity? from ? using a chosen metric. But because
now pi is not guaranteed to be a permutation of ?
(there may be repetition or missing values, i.e.,
?1, in pi), Kendall?s ? cannot be used, and we use
only average continuity and edit distance as dis-
similarity metrics in this experiment.
The remaining experimental configuration is
the same as that of Barzilay and Lapata (2008),
with the optimal transition length set to ? 2.
6 Results
6.1 Sentence Ordering
In this task, we use the same two sets of source
documents (Earthquakes and Accidents, see Sec-
tion 3.1) as Barzilay and Lapata (2008). Each
contains 200 source documents, equally divided
between training and test sets, with up to 20 per-
mutations per document. We conduct experi-
ments on these two domains separately. For each
domain, we accompany each source document
with two different sets of permutations: the one
used by B&L (PSBL), and the one generated from
our model described in Section 5.1.3 (PSM). We
train our multiple-rank model and B&L?s standard
two-rank model on each set of permutations using
the SVMrank package (Joachims, 2006), and eval-
uate both systems on their test sets. Accuracy is
measured as the fraction of correct pairwise rank-
ings for the test set.
6.1.1 Full Coreference Resolution with
Oracular Information
In this experiment, we implement B&L?s fully-
fledged standard entity-based coherence model,
and extract entities from permuted documents us-
ing oracular information from the source docu-
ments (see Section 5.1.2).
Results are shown in Table 2. For each test sit-
uation, we list the best accuracy (in Acc columns)
for each chosen dissimilarity metric, with the cor-
responding rank assignment approach. C repre-
sents the number of ranks used in stratifying raw
scores (?N? if using raw configuration, see Sec-
tion 5.1.1 for details). Baselines are accuracies
trained using the standard entity-based coherence
model3.
Our model outperforms the standard entity-
based model on both permutation sets for both
datasets. The improvement is not significant
when trained on the permutation set PSBL, and
is achieved only with one of the three metrics;
3There are discrepancies between our reported accuracies
and those of Barzilay and Lapata (2008). The differences are
due to the fact that we use a different parser: the Stanford de-
pendency parser (de Marneffe et al 2006), and might have
extracted entities in a slightly different way than theirs, al-
though we keep other experimental configurations as close
as possible to theirs. But when comparing our model with
theirs, we always use the exact same set of features, so the
absolute accuracies do not matter.
320
Condition: Coreference+
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 3 79.5 3 82.0
AC 4 85.2 3 83.3
ED 3 86.8 6 82.2
Baseline 85.3 83.2
PSM
? 3 86.8 3 85.2*
AC 3 85.6 1 85.4*
ED N 87.9* 4 86.3*
Baseline 85.3 81.7
Table 2: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference+ op-
tion. Accuracies which are significantly better than the
baseline (p < .05) are indicated by *.
but when trained on PSM (the set of permutations
generated from our biased model), our model?s
performance significantly exceeds B&L?s4 for all
three metrics, especially as their model?s perfor-
mance drops for dataset Accidents.
From these results, we see that in the ideal sit-
uation where we extract entities and resolve their
coreference relations based on the oracular infor-
mation from the source document, our model is
effective in terms of improving ranking accura-
cies, especially when trained on our more realistic
permutation sets PSM.
6.1.2 Full Coreference Resolution without
Oracular Information
In this experiment, we apply the same auto-
matic coreference resolution tool (Ng and Cardie,
2002) on not only the source documents but also
their permutations. We want to see how removing
the oracular component in the original model af-
fects the performance of our multiple-rank model
and the standard model. Results are shown in Ta-
ble 3.
First we can see when trained on PSM, run-
ning full coreference resolution significantly hurts
performance for both models. This suggests that,
in real-life applications, where the distribution of
training instances with different degrees of co-
herence is skewed (as in the set of permutations
4Following Elsner and Charniak (2011), we use the
Wilcoxon Sign-rank test for significance.
Condition: Coreference?
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 3 71.0 3 73.3
AC 3 *76.8 3 74.5
ED 4 *77.4 6 74.4
Baseline 71.7 73.8
PSM
? 3 55.9 3 51.5
AC 4 53.9 6 49.0
ED 4 53.9 5 52.3
Baseline 49.2 53.2
Table 3: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference? op-
tion. Accuracies which are significantly better than the
baseline (p < .05) are indicated by *.
generated from our model), running full corefer-
ence resolution is not a good option, since it al-
most makes the accuracies no better than random
guessing (50%).
Moreover, considering training using PSBL,
running full coreference resolution has a different
influence for the two datasets. For Earthquakes,
our model significantly outperforms B&L?s while
the improvement is insignificant for Accidents.
This is most probably due to the different way that
entities are realized in these two datasets. As an-
alyzed by Barzilay and Lapata (2008), in dataset
Earthquakes, entities tend to be referred to by pro-
nouns in subsequent mentions, while in dataset
Accidents, literal string repetition is more com-
mon.
Given a balanced permutation distribution as
we assumed in PSBL, switching distant sentence
pairs in Accidents may result in very similar en-
tity distribution with the situation of switching
closer sentence pairs, as recognized by the auto-
matic tool. Therefore, compared to Earthquakes,
our multiple-rank model may be less powerful in
indicating the dissimilarity between the sentence
orderings in a permutation and its source docu-
ment, and therefore can improve on the baseline
only by a small margin.
6.1.3 No Coreference Resolution
In this experiment, we do not employ any coref-
erence resolution tool, and simply cluster head
321
Condition: Coreference?
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 4 82.8 N 82.0
AC 3 78.0 3 **84.2
ED N 78.2 3 *82.7
Baseline 83.7 80.1
PSM
? 3 **86.4 N **85.7
AC 4 *84.4 N **86.6
ED 5 **86.7 N **84.6
Baseline 82.6 77.5
Table 4: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference? op-
tion. Accuracies which are significantly better than the
baseline are indicated by * (p < .05) and ** (p < .01).
nouns by string matching. Results are shown in
Table 4.
Even with such a coarse approximation of
coreference resolution, our model is able to
achieve around 85% accuracy in most test cases,
except for dataset Earthquakes, training on PSBL
gives poorer performance than the standard model
by a small margin. But such inferior perfor-
mance should be expected, because as explained
above, coreference resolution is crucial to this
dataset, since entities tend to be realized through
pronouns; simple string matching introduces too
much noise into training, especially when our
model wants to train a more fine-grained discrim-
inative system than B&L?s. However, we can see
from the result of training on PSM, if the per-
mutations used in training do not involve swap-
ping sentences which are too far away, the result-
ing noise is reduced, and our model outperforms
theirs. And for dataset Accidents, our model
consistently outperforms the baseline model by a
large margin (with significance test at p < .01).
6.1.4 Conclusions for Sentence Ordering
Considering the particular dissimilarity metric
used in training, we find that edit distance usually
stands out from the other two metrics. Kendall?s ?
distance proves to be a fairly weak metric, which
is consistent with the findings of Filippova and
Strube (2007) (see Section 2.3). Figure 1 plots
the testing accuracies as a function of different
68.0 
73.0 
78.0 
83.0 
88.0 
3 4 5 6 N 
Acc
urac
y (%
) 
C 
Earthquake ED Coref+ 
Earthquake ED Coref? 
Accidents ED Coref+ 
Accidents  ED Coref? 
Accidents ? Coref- 
Figure 1: Effect of C on testing accuracies in selected
sentence ordering experimental configurations.
choices of C?s with the configurations where our
model outperforms the baseline model. In each
configuration, we choose the dissimilarity metric
which achieves the best accuracy reported in Ta-
bles 2 to 4 and the PS BL permutation set. We
can see that the dependency of accuracies on the
particular choice of C is not consistent across all
experimental configurations, which suggests that
this free parameter C needs careful tuning in dif-
ferent experimental setups.
Combining our multiple-rank model with sim-
ple string matching for entity extraction is a ro-
bust option for coherence evaluation, regardless
of the particular distribution of permutations used
in training, and it significantly outperforms the
baseline in most conditions.
6.2 Summary Coherence Rating
As explained in Section 3.2, we employ a simple
sentence alignment between a system-generated
summary and its corresponding human-written
summary to construct a test ordering pi and calcu-
late its dissimilarity between the reference order-
ing ? from the human-written summary. In this
way, we convert B&L?s supervised learning model
into a fully unsupervised model, since human an-
notations for coherence scores are not required.
We use the same dataset as Barzilay and Lap-
ata (2008), which includes multi-document sum-
maries from 16 input document clusters generated
by five systems, along with reference summaries
composed by humans.
In this experiment, we consider only average
continuity (AC) and edit distance (ED) as dissimi-
larity metrics, with raw configuration for rank as-
signment, and compare our multiple-rank model
with the standard entity-based model using ei-
ther full coreference resolution5 or no resolution
5We run the coreference resolution tool on all documents.
322
Entities Metric Same Full
Coreference+
AC 82.5 *72.6
ED 81.3 **73.0
Baseline 78.8 70.9
Coreference?
AC 76.3 72.0
ED 78.8 71.7
Baseline 80.0 72.3
Table 5: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in summary rating. Baselines are results of
standard entity-based coherence model. Accuracies
which are significantly better than the corresponding
baseline are indicated by * (p < .05) and ** (p < .01).
for entity extraction. We train both models on
the ranking preferences (144 in all) among sum-
maries originating from the same input document
cluster using the SVMrank package (Joachims,
2006), and test on two different test sets: same-
cluster test and full test. Same-cluster test is the
one used by Barzilay and Lapata (2008), in which
only the pairwise rankings (80 in all) between
summaries originating from the same input doc-
ument cluster are tested; we also experiment with
full test, in which pairwise rankings (1520 in all)
between all summary pairs excluding two human-
written summaries are tested.
Results are shown in Table 5. Coreference+
and Coreference? denote the configuration of
using full coreference resolution or no resolu-
tion separately. First, clearly for both models,
performance on full test is inferior to that on
same-cluster test, but our model is still able to
achieve performance competitive with the stan-
dard model, even if our fundamental assumption
about the existence of canonical sentence order-
ing in documents with same content may break
down on those test pairs not originating from the
same input document cluster. Secondly, for the
baseline model, using the Coreference? configu-
ration yields better accuracy in this task (80.0%
vs. 78.8% on same-cluster test, and 72.3% vs.
70.9% on full test), which is consistent with the
findings of Barzilay and Lapata (2008). But our
multiple-rank model seems to favor the Corefer-
ence+ configuration, and our best accuracy even
exceeds B&L?s best when tested on the same set:
82.5% vs. 80.0% on same-cluster test, and 73.0%
vs. 72.3% on full test.
When our model performs poorer than the
baseline (using Coreference? configuration), the
difference is not significant, which suggests that
our multiple-rank model with unsupervised score
assignment via simple cosine matching can re-
main competitive with the standard model, which
requires human annotations to obtain a more fine-
grained coherence spectrum. This observation is
consistent with Banko and Vanderwende (2004)?s
discovery that human-generated summaries look
quite extractive.
7 Conclusions
In this paper, we have extended the popular co-
herence model of Barzilay and Lapata (2008) by
adopting a multiple-rank learning approach. This
is inherently different from other extensions to
this model, in which the focus is on enriching
the set of features for entity-grid construction,
whereas we simply keep their original feature set
intact, and manipulate only their learning method-
ology. We show that this concise extension is
effective and able to outperform B&L?s standard
model in various experimental setups, especially
when experimental configurations are most suit-
able considering certain dataset properties (see
discussion in Section 6.1.4).
We experimented with two tasks: sentence or-
dering and summary coherence rating, following
B&L?s original framework. In sentence ordering,
we also explored the influence of removing the
oracular component in their original model and
dealing with permutations generated from differ-
ent distributions, showing that our model is robust
for different experimental situations. In summary
coherence rating, we further extended their model
such that their original supervised learning is con-
verted into unsupervised learning with competi-
tive or even superior performance.
Our multiple-rank learning model can be easily
adapted into other extended entity-based coher-
ence models with their enriched feature sets, and
further improvement in ranking accuracies should
be expected.
Acknowledgments
This work was financially supported by the Nat-
ural Sciences and Engineering Research Council
of Canada and by the University of Toronto.
323
References
Michele Banko and Lucy Vanderwende. 2004. Us-
ing n-grams to understand the nature of summaries.
In Proceedings of Human Language Technologies
and North American Association for Computational
Linguistics 2004: Short Papers, pages 1?4.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 42rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2006. A bottom-up approach to sen-
tence ordering for multi-document summarization.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 385?392.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 186?195.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2011),
pages 125?129.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG 2007), pages 139?142.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the 8th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2002), pages 133?142.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2006), pages
217?226.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL 2003), pages
545?552.
Mirella Lapata. 2006. Automatic evaluation of in-
formation ordering: Kendall?s tau. Computational
Linguistics, 32(4):471?484.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2011), pages 997?1006.
Nitin Madnani, Rebecca Passonneau, Necip Fazil
Ayan, John M. Conroy, Bonnie J. Dorr, Ju-
dith L. Klavans, Dianne P. O?Leary, and Judith D.
Schlesinger. 2007. Measuring variability in sen-
tence ordering for news summarization. In Pro-
ceedings of the Eleventh European Workshop on
Natural Language Generation (ENLG 2007), pages
81?88.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics (ACL 2002),
pages 104?111.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National
Conference on Artificial Intelligence, pages 1219?
1224.
Renxian Zhang. 2011. Sentence ordering driven by
local and global coherence for summary generation.
In Proceedings of the ACL 2011 Student Session,
pages 6?11.
324
Computing Lexical Contrast
Saif M. Mohammad?
National Research Council Canada
Bonnie J. Dorr??
University of Maryland
Graeme Hirst?
University of Toronto
Peter D. Turney?
National Research Council Canada
Knowing the degree of semantic contrast between words has widespread application in natural
language processing, including machine translation, information retrieval, and dialogue sys-
tems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of
many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not
classify opposites into the different kinds, however. They also do not explicitly list word pairs
that are not opposites but yet have some degree of contrast in meaning, such as warm and cold
or tropical and freezing. We propose an automatic method to identify contrasting word pairs
that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is
a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly
related. (For example, there exists the pair of opposites hot and cold such that tropical is related
to hot, and freezing is related to cold.) We will call this the contrast hypothesis.
We begin with a large crowdsourcing experiment to determine the amount of human
agreement on the concept of oppositeness and its different kinds. In the process, we flesh out
key features of different kinds of opposites. We then present an automatic and empirical measure
of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of
a Roget-like thesaurus. We show how, using four different data sets, we evaluated our approach
on two different tasks, solving ?most contrasting word? questions and distinguishing synonyms
from opposites. The results are analyzed across four parts of speech and across five different kinds
of opposites. We show that the proposed measure of lexical contrast obtains high precision and
large coverage, outperforming existing methods.
? National Research Council Canada. E-mail: saif.mohammad@nrc-cnrc.gc.ca.
?? Department of Computer Science and Institute of Advanced Computer Studies, University of Maryland.
E-mail: bonnie@umiacs.umd.edu.
? Department of Computer Science, University of Toronto. E-mail: gh@cs.toronto.edu.
? National Research Council Canada. E-mail: peter.turney@nrc-cnrc.gc.ca.
Submission received: 14 January 2010; revised submission received: 26 June 2012; accepted for publication:
16 July 2012.
doi:10.1162/COLI a 00143
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Native speakers of a language intuitively recognize different degrees of lexical contrast?
for example, most people will agree that hot and cold have a higher degree of contrast
than cold and lukewarm, and cold and lukewarm have a higher degree of contrast than
penguin and clown. Automatically determining the degree of contrast between words
has many uses, including:
 Detecting and generating paraphrases (Marton, El Kholy, and Habash
2011) (The dementors caught Sirius Black / Black could not escape the
dementors).
 Detecting certain types of contradictions (de Marneffe, Rafferty, and
Manning 2008; Voorhees 2008) (Kyoto has a predominantly wet climate /
It is mostly dry in Kyoto). This is in turn useful in effectively reranking
target language hypotheses in machine translation, and for reranking
query responses in information retrieval.
 Understanding discourse structure and improving dialogue systems.
Opposites often indicate the discourse relation of contrast (Marcu and
Echihabi 2002).
 Detecting humor (Mihalcea and Strapparava 2005). Satire and jokes tend
to have contradictions and oxymorons.
 Distinguishing near-synonyms from word pairs that are semantically
contrasting in automatically created distributional thesauri. Measures
of distributional similarity typically fail to do so.
Detecting lexical contrast is not sufficient by itself to solve most of these problems, but
it is a crucial component.
Lexicons of pairs of words that native speakers consider opposites have been
created for certain languages, but their coverage is limited. Opposites are of many kinds,
such as antipodals, complementaries, and gradable (summarized in Section 3). Existing
lexicons often do not classify opposites into the different kinds, however. Further, the
terminology is inconsistent across different sources. For example, Cruse (1986) defines
antonyms as gradable adjectives that are opposite in meaning, whereas the WordNet
antonymy link connects some verb pairs, noun pairs, and adverb pairs too. In this
article, we will follow Cruse?s terminology, and we will refer to word pairs connected
by WordNet?s antonymy link as opposites, unless referring specifically to gradable
adjectival pairs.
Manually created lexicons also do not explicitly list word pairs that are not
opposites but yet have some degree of contrast in meaning, such as warm and cold or
tropical and cold. Further, contrasting word pairs far outnumber those that are commonly
considered opposites. In our own experiments described later in this article, we find that
more than 90% of the contrasting pairs in GRE ?most contrasting word? questions are
not listed as antonyms in WordNet. We should not infer from this that WordNet or any
other lexicographic resource is a poor source for detecting opposites, but rather that
identifying the large number of contrasting word pairs requires further computation,
possibly relying on other semantic relations stored in the lexicographic resource.
Even though a number of computational approaches have been proposed for se-
mantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy?
556
Mohammad et al Computing Lexical Contrast
hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some
extent, this is because lexical contrast is not as well understood as other classical lexical?
semantic relations.
Over the years, many definitions of semantic contrast and opposites have been
proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan
1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from
each other in various respects. Cruse (1986, page 197) observes that even though people
have a robust intuition of opposites, ?the overall class is not a well-defined one.? He
points out that a defining feature of opposites is that they tend to have many common
properties, but differ saliently along one dimension of meaning. We will refer to this
semantic dimension as the dimension of opposition. For example, giant and dwarf are
both living beings; they both eat, they both walk, they are both capable of thinking, and
so on. They are most saliently different, however, along the dimension of height. Cruse
also points out that sometimes it is difficult to identify or articulate the dimension of
opposition (for example, city?farm).
Another way to define opposites is that they are word pairs with a ?binary incom-
patible relation? (Kempson 1977, page 84). That is to say that one member entails the
absence of the other, and given one member, the identity of the other member is obvious.
Thus, night and day are good examples of opposites because night is best paraphrased
by not day, rather than the negation of any other term. On the other hand, blue and yellow
make poor opposites because even though they are incompatible, they do not have an
obvious binary relation such that blue is understood to be a negation of yellow. It should
be noted that there is a relation between binary incompatibility and difference along just
one dimension of meaning.
For this article, we define opposites to be term pairs that clearly satisfy either
the property of binary incompatibility or the property of salient difference across a
dimension of meaning. Word pairs may satisfy the two properties to different degrees,
however. We will refer to all word pairs that satisfy either of the two properties to some
degree as contrasting. For example, daylight and darkness are very different along the
dimension of light, and they satisfy the binary incompatibity property to some degree,
but not as strongly as day and night. Thus we will consider both daylight and darkness as
well as day and night as semantically contrasting pairs (the former pair less so than the
latter), but only day and night as opposites. Even though there are subtle differences in
the meanings of the terms contrasting, opposite, and antonym, they have often been used
interchangeably in the literature, dictionaries, and common parlance. Thus, we list here
what we use these terms to mean in this article:
 Opposites are word pairs that have a strong binary incompatibility
relation with each other and/or are saliently different across a dimension
of meaning.
 Contrasting word pairs are word pairs that have some non-zero degree
of binary incompatibility and/or have some non-zero difference across
a dimension of meaning. Thus, all opposites are contrasting, but not all
contrasting pairs are opposites.
 Antonyms are opposites that are also gradable adjectives.1
1 We follow Cruse?s (1986) definition for antonyms. The WordNet antonymy link, however, also connects
some verb pairs, noun pairs, and adverb pairs.
557
Computational Linguistics Volume 39, Number 3
In this article, we present an automatic method to identify contrasting word pairs
that is based on the following hypothesis:
Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair of
opposites, C and D, such that A and C are strongly related and B and D are strongly
related.
For example, there exists the pair of opposites night and day such that darkness is related
to night, and daylight is related to day. We then determine the degree of contrast between
two words using this hypothesis:
Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then their
degree of contrast is proportional to their tendency to co-occur in a large corpus.
For example, consider the contrasting word pairs top?low and top?down; because top and
down occur together much more often than top and low, our method concludes that the
pair top?down has a higher degree of lexical contrast than the pair top?low. The degree
of contrast hypothesis is inspired by the idea that opposites tend to co-occur more often
than chance (Charles and Miller 1989; Fellbaum 1995). Murphy and Andrew (1993)
claim that this is because together opposites convey contrast well, which is rhetorically
useful. Thus we hypothesize that the higher the degree of contrast between two words,
the higher the tendency of people to use them together.
Because opposites are a key component of our method, we begin by first under-
standing different kinds of opposites (Section 3). Then we describe a crowdsourced
project on the annotation of opposites into different kinds (Section 4). In Section 5.1,
we examine whether opposites and other highly contrasting word pairs occur together
in text more often than randomly chosen word pairs. This experiment is crucial to
the degree of contrast hypothesis because if our assumption is true, then we should
find that highly contrasting pairs are used together much more often than randomly
chosen word pairs. Section 5.2 examines this question. Section 6 presents our method
to automatically compute the degree of contrast between word pairs by relying on
the contrast hypothesis, the degree of contrast hypothesis, seed opposites, and the
structure of a Roget-like thesaurus. (This method was first described in Mohammad,
Dorr, and Hirst [2008].) Finally we present experiments that evaluate various aspects of
the automatic method (Section 7). Following is a summary of the key research questions
addressed by this article:
(1) On the kinds of opposites:
Research questions: How good are humans at identifying different kinds
of opposites? Can certain term pairs belong to more than one kind of
opposite?
Experiment: In Sections 3 and 4, we describe how we designed a ques-
tionnaire to acquire annotations about opposites. Because the annotations
are done by crowdsourcing, and there is no control over the educational
background of the annotators, we devote extra effort to make sure that the
questions are phrased in a simple, yet clear, manner. We deploy a quality
control method that uses a word-choice question to automatically identify
and discard dubious and outlier annotations.
Findings: We find that humans agree markedly in identifying opposites;
there is significant variation in the agreement for different kinds of
558
Mohammad et al Computing Lexical Contrast
opposites, however. We find that a large number of opposing word pairs
have properties pertaining to more than one kind of opposite.
(2) On the manifestation of opposites and other highly contrasting pairs in text:
Research questions: How often do highly contrasting word pairs co-occur
in text? How strong is this tendency compared with random word pairs,
and compared with near-synonym word pairs?
Experiment: Section 5 describes how we compiled sets of highly contrast-
ing word pairs (including opposites), near-synonym pairs, and random
word pairs, and determine the tendency for pairs in each set to co-occur in
a corpus.
Findings: Highly contrasting word pairs co-occur significantly more often
than both the random word pairs set and also the near-synonyms set.
We also find that the average distributional similarity of highly contrasting
word pairs is higher than that of synonymous words. The standard de-
viations of the distributions for the high-contrast set and the synonyms
set are large, however, and so the tendency to co-occur is not sufficient to
distinguish highly contrasting word pairs from near-synonymous pairs.
(3) On an automatic method for computing lexical contrast:
Research questions: How can the contrast hypothesis and the degree
of contrast hypothesis be used to develop an automatic method for
identifying contrasting word pairs? How can we automatically generate
the list of opposites, which are needed as input for a method relying on
the contrast hypothesis?
Proposed Method: Section 6 describes an empirical method for deter-
mining the degree of contrast between two words by using the contrast
hypothesis, the degree of contrast hypothesis, the structure of a thesaurus,
and seed opposite pairs. The use of affixes to generate seed opposite pairs
is also described. (This method was first proposed in Mohammad, Dorr,
and Hirst [2008].)
(4) On the evaluation of automatic methods of contrast:
Research questions: How accurate are automatic methods at identifying
whether one word pair has a higher degree of contrast than another? What
is the accuracy of this method in detecting opposites (a notable subset of
the contrasting pairs)? How does this accuracy vary for different kinds of
opposites?2 How easy is it for automatic methods to distinguish between
opposites and synonyms? How does the proposed method perform when
compared with other automatic methods?
Experiments: We conduct three experiments (described in Sections 7.1,
7.2, and 7.3) involving three different data sets and two tasks to answer
2 Note that though linguists have classified opposites into different kinds, we know of no work doing so
for contrasts more generally. Thus this particular analysis must be restricted to opposites alone.
559
Computational Linguistics Volume 39, Number 3
these questions. We compare performance of our method with methods
proposed by Lin et al (2003) and Turney (2008). We automatically
generate a new set of 1,296 ?most contrasting word? questions to evaluate
performance of our method on five different kinds of opposites and across
four parts of speech. (The evaluation described in Section 7.1 was first
described in Mohammad, Dorr, and Hirst [2008].)
Findings: We find that the proposed measure of lexical contrast obtains
high precision and large coverage, outperforming existing methods.
Our method performs best on gradable pairs, antipodal pairs, and com-
plementary pairs, but poorly on disjoint opposite pairs. Among different
parts of speech, the method performs best on noun pairs, and relatively
worse on verb pairs.
All of the data created and compiled as part of this research are summarized in Table 18
(Section 8), and is available for download.3
2. Related Work
Charles and Miller (1989) proposed that opposites occur together in a sentence more
often than chance. This is known as the co-occurrence hypothesis. Paradis, Willners,
and Jones (2009) describe further experiments to show how canonical opposites tend
to have high textual co-occurrence. Justeson and Katz (1991) gave evidence in support
of the hypothesis using 35 prototypical opposites (from an original set of 39 opposites
compiled by Deese [1965]) and also with an additional 22 frequent opposites. They also
showed that opposites tend to occur in parallel syntactic constructions. All of these
pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb,
adjective, and adverb pairs (noun?noun, noun?verb, noun?adjective, verb?adverb, etc.)
pertaining to 18 concepts (for example, lose(v)?gain(n) and loss(n)?gain(n), where lose(v)
and loss(n) pertain to the concept of ?failing to have/maintain?). Non-opposite semanti-
cally related words also tend to occur together more often than chance, however. Thus,
separating opposites from these other classes has proven to be difficult.
Some automatic methods of lexical contrast rely on lexical patterns in text. For
example, Lin et al (2003) used patterns such as ?from X to Y ? and ?either X or Y ?
to separate opposites from distributionally similar pairs. They evaluated their method
on 80 pairs of opposites and 80 pairs of synonyms taken from the Webster?s Collegiate
Thesaurus (Kay 1988). The evaluation set of 160 word pairs was chosen such that it
included only high-frequency terms. This was necessary to increase the probability
of finding sentences in a corpus where the target pair occurred in one of the chosen
patterns. Lobanova, van der Kleij, and Spenader (2010) used a set of Dutch adjective
seed pairs to learn lexical patterns commonly containing opposites. The patterns were
in turn used to create a larger list of Dutch opposites. The method was evaluated by
comparing entries to Dutch lexical resources and by asking human judges to determine
whether an automatically found pair is indeed an opposite. Turney (2008) proposed a
supervised method for identifying synonyms, opposites, hypernyms, and other lexical-
semantic relations between word pairs. The approach learns patterns corresponding to
different relations.
3 http://www.purl.org/net/saif.mohammad/research.
560
Mohammad et al Computing Lexical Contrast
Harabagiu, Hickl, and Lacatusu (2006) detected contrasting word pairs for the
purpose of identifying contradictions by using WordNet chains?synsets connected by
the hypernymy?hyponymy links and exactly one antonymy link. Lucerto, Pinto, and
Jimen?ez-Salazar (2002) proposed detecting contrasting word pairs using the number
of tokens between two words in text and also cue words such as but, from, and and.
Unfortunately, they evaluated their method on only 18 word pairs. Neither Harabagiu,
Hickl, and Lacatusu nor Lucerto, Pinto, and Jime?nez-Salazar determined the degree of
contrast between words, and their methods have not been shown to have substantial
coverage.
Schwab, Lafourcade, and Prince (2002) created an oppositeness vector for a target
word. The closer this vector is to the context vector of the other target word, the
more opposite the two target words are. The oppositeness vectors were created by first
manually identifying possible opposites and then generating suitable vectors for each
using dictionary definitions. The approach was evaluated on only a handful of word
pairs.
There is a large amount of work on sentiment analysis and opinion mining aimed
at determining the polarity of words (Pang and Lee 2008). For example, Pang, Lee, and
Vaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping cast
their qualifying nouns positively whereas adjectives such as bad, cliched, and boring
portray the noun negatively. Many of these gradable adjectives have opposites, but
these approaches, with the exception of that of Hatzivassiloglou and McKeown (1997),
did not attempt to determine pairs of positive and negative polarity words that are
opposites. Hatzivassiloglou and McKeown proposed a supervised algorithm that uses
word usage patterns to generate a graph with adjectives as nodes. An edge between
two nodes indicates either that the two adjectives have the same or opposite polarity. A
clustering algorithm then partitions the graph into two subgraphs such that the nodes in
a subgraph have the same polarity. They used this method to create a lexicon of positive
and negative words, and argued that the method could also be used to detect opposites.
3. The Heterogeneous Nature of Opposites
Opposites, unlike synonyms, can be of different kinds. Many different classifications
have been proposed, one of which is given by Cruse (1986) (Chapters 9, 10, and 11).
It consists of complementaries (open?shut, dead?alive), antonyms (long?short, slow?
fast) (further classified into polar, overlapping, and equipollent opposites), directional
opposites (up?down, north?south) (further classified into antipodals, counterparts, and
reversives), relational opposites (husband?wife, predator?prey), indirect converses (give?
receive, buy?pay), congruence variants (huge?little, doctor?patient), and pseudo opposites
(black?white).
Various lexical relations have also received attention at the Educational Testing
Services, as analogies and ?most contrasting word? questions are part of the tests they
conduct. They classify opposites into contradictories (alive?dead, masculine?feminine),
contraries (old?young, happy-sad), reverses (attack?defend, buy?sell), directionals ( front?
back, left?right), incompatibles (happy?morbid, frank?hypocritical), asymmetric contraries
(hot?cool, dry?moist), pseudo-opposites (popular?shy, right?bad), and defectives (default?
payment, limp?walk) (Bejar, Chaffin, and Embretson 1991).
Keeping in mind the meanings and subtle distinctions between each of these kinds
of opposites is not easy even if we provide extensive training to annotators. Because
we crowdsource the annotations, and we know that Turkers prefer to spend their
time doing the task (and making money) rather than reading lengthy descriptions, we
561
Computational Linguistics Volume 39, Number 3
focused only on five kinds of opposites that we believed would be easiest to annotate,
and which still captured a majority of the opposites:
 Antipodals (top?bottom, start?finish): Antipodals are opposites in which
?one term represents an extreme in one direction along some salient axis,
while the other term denotes the corresponding extreme in the other
direction? (Cruse 1986, page 225).
 Complementaries (open?shut, dead?alive): The essential characteristic of a
pair of complementaries is that ?between them they exhaustively divide
the conceptual domain into two mutually exclusive compartments, so that
what does not fall into one of the compartments must necessarily fall into
the other? (Cruse 1986, page 198).
 Disjoint (hot?cold, like?dislike): Disjoint opposites are word pairs that
occupy non-overlapping regions in the semantic dimension such that there
are regions not covered by either term. This set of opposites includes
equipollent adjective pairs (for example, hot?cold) and stative verb pairs
(for example, like?dislike). We refer the reader to Sections 9.4 and 9.7 of
Cruse (1986) for details about these sub-kinds of opposites.
 Gradable opposites (long?short, slow?fast): are adjective-pair or
adverb-pair opposites that are gradable, that is, ?members of the pair
denote degrees of some variable property such as length, speed, weight,
accuracy, etc.? (Cruse 1986, page 204).
 Reversibles (rise?fall, enter?exit): Reversibles are opposite verb pairs such
that ?if one member denotes a change from A to B, its reversive partner
denotes a change from B to A? (Cruse 1986, page 226).
It should be noted that there is no agreed-upon number of kinds of opposites. Different
researchers have proposed various classifications that overlap to a greater or lesser
degree. It is possible that for a certain application or study one may be interested in
a kind of opposite not listed here.
4. Crowdsourcing
We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different
kinds of opposites. We broke the task into small independently solvable units called
HITs (Human Intelligence Tasks) and uploaded them on the AMT Web site.4 Each HIT
had a set of questions, all of which were to be answered by the same person (a Turker, in
AMT parlance). We created HITs for word pairs, taken from WordNet, that we expected
to have some degree of contrast in meaning.
In WordNet, words that are close in meaning are grouped together in a set called a
synset. If one of the words in a synset is an opposite of another word in a different synset,
then the two synsets are called head synsets and WordNet records the two words as
direct antonyms (Gross, Fischer, and Miller 1989)?WordNet regards the terms opposite
and antonym as synonyms. Other word pairs across the two head synsets are called
indirect antonyms. Because we follow Cruse?s definition of antonyms, which requires
4 https://www.mturk.com/mturk/welcome.
562
Mohammad et al Computing Lexical Contrast
Table 1
Target word pairs chosen for annotation. Each term was annotated about eight times.
part of speech # of word pairs
adverbs 185
adjectives 646
nouns 416
verbs 309
all 1,556
antonyms to be gradable adjectives, and because WordNet?s direct antonyms include
noun, verb, and adverb pairs too, for the rest of the article we will refer to WordNet
direct antonyms as direct opposites and WordNet indirect antonyms as indirect op-
posites. We will refer to the union of both the direct and indirect opposites simply as
WordNet opposites. Note that the WordNet opposites are highly contrasting term pairs.
We chose as target pairs all the direct or indirect opposites from WordNet that were
also listed in the Macquarie Thesaurus. This condition was a mechanism to ignore less-
frequent and obscure words, and apply our resources on words that are more common.
Additionally, as we will describe subsequently, we use the presence of the words in
the thesaurus to help generate Question 1, which we use for quality control of the
annotations. Table 1 gives a breakdown of the 1,556 pairs chosen by part of speech.
Because we do not have any control over the educational background of the
annotators, we made efforts to phrase questions about the kinds of opposites in a simple
and clear manner. Therefore we avoided definitions and long instructions in favor of
examples and short questions. We believe this strategy is beneficial even in traditional
annotation scenarios.
We created separate questionnaires (HITs) for adjectives, adverbs, nouns, and
verbs. A complete example adjective HIT with directions and questions is shown
in Figure 1. The adverb, noun, and verb questionnaires had similar questions, but
were phrased slightly differently to accommodate differences in part of speech. These
questionnaires are not shown here due to lack of space, but all four questionnaires are
available for download.5 The verb questionnaire had an additional question, shown
in Figure 2. Because nouns and verbs are not considered gradable, the corresponding
questionnaires did not have Q8 and Q9. We requested annotations from eight different
Turkers for each HIT.
4.1 The Word Choice Question: Q1
Q1 is an automatically generated word choice question that has a clear correct answer. It
helps identify outlier and malicious annotations. If this question is answered incorrectly,
then we assume that the annotator does not know the meanings of the target words, and
we ignore responses to the remaining questions. Further, as this question makes the
annotator think about the meanings of the words and about the relationship between
them, we believe it improves the responses for subsequent questions.
The options for Q1 were generated automatically. Each option is a set of four
comma-separated words. The words in the answer are close in meaning to both of the
target words. In order to create the answer option, we first generated a much larger
5 http://www.purl.org/net/saif.mohammad/research.
563
Computational Linguistics Volume 39, Number 3
Word-pair: musical ? dissonant
Q1. Which set of words is most related to the word pair musical:dissonant?
 useless, surgery, ineffectual, institution
 sequence, episode, opus, composition
 youngest, young, youthful, immature
 consequential, important, importance, heavy
Q2. Do musical and dissonant have some contrast in meaning?
 yes  no
For example, up?down, lukewarm?cold, teacher?student, attack?defend, all have at least
some degree of contrast in meaning. On the other hand, clown?down, chilly?cold,
teacher?doctor, and attack?rush DO NOT have contrasting meanings.
Q3. Some contrasting words are paired together so often that given one we naturally
think of the other. If one of the words in such a pair were replaced with another word of
almost the same meaning, it would sound odd. Are musical:dissonant such a pair?
 yes  no
Examples for ?yes?: tall?short, attack?defend, honest?dishonest, happy?sad.
Examples for ?no?: tall?stocky, attack?protect, honest?liar, happy?morbid.
Q5. Do musical and dissonant represent two ends or extremes?
 yes  no
Examples for ?yes?: top?bottom, basement?attic, always?never, all?none, start?finish.
Examples for ?no?: hot?cold (boiling refers to more warmth than hot and freezing refers
to less warmth than cold), teacher?student (there is no such thing as more or less teacher
and more or less student), always?sometimes (never is fewer times than sometimes).
Q6. If something is musical, would you assume it is not dissonant, and vice versa?
In other words, would it be unusual for something to be both musical and dissonant?
 yes  no
Examples for ?yes?: happy?sad, happy?morbid, vigilant?careless, slow?stationary.
Examples for ?no?: happy?calm, stationary?still, vigilant?careful, honest?truthful.
Q7. If something or someone could possibly be either musical or dissonant, is it
necessary that it must be either musical or dissonant? In other words, is it true that for
things that can be musical or dissonant, there is no third possible state, except perhaps
under highly unusual circumstances?
 yes  no
Examples for ?yes?: partial?impartial, true?false, mortal?immortal.
Examples for ?no?: hot?cold (an object can be at room temperature is neither hot nor cold),
tall?short (a person can be of medium or average height).
Q8. In a typical situation, if two things or two people are musical, then can one be more
musical than the other?
 yes  no
Examples for ?yes?: quick, exhausting, loving, costly.
Examples for ?no?: dead, pregnant, unique, existent.
Q9. In a typical situation, if two things or two people are dissonant, can one be more
dissonant than the other?
 yes  no
Examples for ?yes?: quick, exhausting, loving, costly, beautiful.
Examples for ?no?: dead, pregnant, unique, existent, perfect, absolute.
Figure 1
Example HIT: Adjective pairs questionnaire.
Note: Perhaps ?musical ? dissonant? might be better written as ?musical versus dissonant,? but we have
kept ??? here to show the reader exactly what the Turkers were given.
Note: Q4 is not shown here, but can be seen in the on-line version of the questionnaire. It was an exploratory
question, and it was not multiple choice. Q4?s responses have not been analyzed.
564
Mohammad et al Computing Lexical Contrast
Word-pair: enabling ? disabling
Q10. In a typical situation, do the sequence of actions disabling and then enabling bring
someone or something back to the original state, AND do the sequence of actions enabling
and disabling also bring someone or something back to the original state?
 yes, both ways: the transition back to the initial state makes much sense in both
sequences.
 yes, but only one way: the transition back to the original state makes much more sense
one way than the other way.
 none of the above
Examples for ?yes, both ways?: enter?exit, dress?undress, tie?untie, appear?disappear.
Examples for ?yes, but only one way?: live?die, create?destroy, damage?repair, kill?resurrect.
Examples for ?none of the above?: leave?exit, teach?learn, attack?defend (attacking and then
defending does not bring one back to the original state).
Figure 2
Additional question in the questionnaire for verbs.
source pool of all the words that were in the same thesaurus category as any of the two
target words. (Words in the same category are closely related.) Words that had the same
stem as either of the target words were discarded. For each of the remaining words, we
added their Lesk similarities with the two target words (Banerjee and Pedersen 2003).
The four words with the highest sum were chosen to form the answer option.
The three distractor options were randomly selected from the pool of correct an-
swers for all other word choice questions. Finally, the answer and distractor options
were presented to the Turkers in random order.
4.2 Post-Processing
The response to a HIT by a Turker is called an assignment. We obtained about 12,448
assignments in all (1,556 pairs ? 8 assignments each). About 7% of the adjective, adverb,
and noun assignments and about 13% of the verb assignments had an incorrect answer
to Q1. These assignments were discarded, leaving 1,506 target pairs with three or more
valid assignments. We will refer to this set of assignments as the master set, and all
further analysis in this article is based on this set. Table 2 gives a breakdown of the
average number of annotations for each of the target pairs in the master set.
4.3 Prevalence of Different Kinds of Contrasting Pairs
For each question pertaining to every word pair in the master set, we determined the
most frequent response by the annotators. Table 3 gives the percentage of word pairs in
Table 2
Number of word pairs and average number of annotations per word pair in the master set.
part of # of average # of
speech word pairs annotations
adverbs 182 7.80
adjectives 631 8.32
nouns 405 8.44
verbs 288 7.58
all 1,506 8.04
565
Computational Linguistics Volume 39, Number 3
Table 3
Percentage of word pairs that received a response of ?yes? for the questions in the questionnaire.
adj. = adjectives; adv. = adverbs.
% of word pairs
Question answer adj. adv. nouns verbs
Q2. Do X and Y have some contrast? yes 99.5 96.8 97.6 99.3
Q3. Are X and Y opposites? yes 91.2 68.6 65.8 88.8
Q5. Are X and Y at two ends of a dimension? yes 81.8 73.5 81.1 94.4
Q6. Does X imply not Y? yes 98.3 92.3 89.4 97.5
Q7. Are X and Y mutually exhaustive? yes 85.1 69.7 74.1 89.5
Q8. Does X represent a point on some scale? yes 78.5 77.3 ? ?
Q9. Does Y represent a point on some scale? yes 78.5 70.8 ? ?
Q10. Does X undo Y OR does Y undo X? one way ? ? ? 3.8
both ways ? ? ? 90.9
Table 4
Percentage of WordNet source pairs that are contrasting, opposite, and ?contrasting but not
opposite.?
category basis adj. adv. nouns verbs
contrasting Q2 yes 99.5 96.8 97.6 99.3
opposites Q2 yes and Q3 yes 91.2 68.6 60.2 88.9
contrasting, but not opposite Q2 yes and Q3 no 8.2 28.2 37.4 10.4
the master set that received a most frequent response of ?yes.? The first column in the
table lists the question number followed by a brief description of question. (Note that
the Turkers saw only the full forms of the questions, as shown in the example HIT.)
Observe that most of the word pairs are considered to have at least some contrast
in meaning. This is not surprising because the master set was constructed using words
connected through WordNet?s antonymy relation.6 Responses to Q3 show that not all
contrasting pairs are considered opposite, and this is especially the case for adverb
pairs and noun pairs. The rows in Table 4 show the percentage of words in the master
set that are contrasting (row 1), opposite (row 2), and contrasting but not opposite
(row 3).
Responses to Q5, Q6, Q7, Q8, and Q9 (Table 3) show the prevalence of different
kinds of relations and properties of the target pairs.
Table 5 shows the percentage of contrasting word pairs that may be classified into
the different types discussed in Section 3. Observe that rows for all categories other
than the disjoints have percentages greater than 60%. This means that a number of
contrasting word pairs can be classified into more than one kind. Complementaries
are the most common kind in case of adverbs, nouns, and verbs, whereas antipodals
are most common among adjectives. A majority of the adjective and adverb contrasting
pairs are gradable, but more than 30% of the pairs are not. Most of the verb pairs are
reversives (91.6%). Disjoint pairs are much less common than all the other categories
6 All of the direct antonyms were marked as contrasting by the Turkers. Only a few indirect antonyms
were marked as not contrasting.
566
Mohammad et al Computing Lexical Contrast
Table 5
Percentage of contrasting word pairs belonging to various subtypes. The subtype ?reversives?
applies only to verbs. The subtype ?gradable? applies only to adjectives and adverbs.
subtype basis adv. adj. nouns verbs
Antipodals Q2 yes, Q5 yes 82.3 75.9 82.5 95.1
Complementaries Q2 yes, Q7 yes 85.6 72.0 84.8 98.3
Disjoint Q2 yes, Q7 no 14.4 28.0 15.2 1.7
Gradable Q2 yes, Q8 yes, Q9 yes 69.6 66.4 - -
Reversives Q2 yes, Q10 both ways - - - 91.6
considered, and they are most prominent among adjectives (28%), and least among verb
pairs (1.7%).
4.4 Agreement
People do not always agree on linguistic classifications of terms, and one of the goals of
this work was to determine how much people agree on properties relevant to different
kinds of opposites. Table 6 lists the breakdown of agreement by target-pair part of
speech and question, where agreement is the average percentage of the number of
Turkers giving the most-frequent response to a question?the higher the number
of Turkers that vote for the majority answer, the higher is the agreement.
Observe that agreement is highest when asked whether a word pair has some
degree of contrast in meaning (Q2), and that there is a marked drop when asked if the
two words are opposites (Q3). This is true for each of the parts of speech, although the
drop is highest for verbs (94.7% to 75.2%).
For Questions 5 through 9, we see varying degrees of agreement?Q6 obtaining the
highest agreement and Q5 the lowest. There is marked difference across parts of speech
for certain questions. For example, verbs are the easiest to identify (highest agreement
for Q5, Q7, and Q8). For Q6, nouns have markedly lower agreement than all other parts
of speech?not surprising considering that the set of disjoint opposites is traditionally
associated with equipollent adjectives and stative verbs. Adverbs and adjectives have
markedly lower agreement scores for Q7 than nouns and verbs.
Table 6
Breakdown of answer agreement by target-pair part of speech and question: For every target
pair, a question is answered by about eight annotators. The majority response is chosen as the
answer. The ratio of the size of the majority and the number of annotators is indicative of the
amount of agreement. The table shows the average percentage of this ratio.
question adj. adv. nouns verbs average
Q2. Do X and Y have some contrast? 90.7 92.1 92.0 94.7 92.4
Q3. Are X and Y opposites? 79.0 80.9 76.4 75.2 77.9
Q5. Are X and Y at two ends of a dimension? 70.3 66.5 73.0 78.6 72.1
Q6. Does X imply not Y? 89.0 90.2 81.8 88.4 87.4
Q7. Are X and Y mutually exhaustive? 70.4 69.2 78.2 88.3 76.5
average (Q2, Q3, Q5, Q6, and Q7) 82.3 79.8 80.3 85.0 81.3
Q8. Does X represent a point on some scale? 77.9 71.5 ? ? 74.7
Q9. Does Y represent a point on some scale? 75.2 72.0 ? ? 73.6
Q10. Does X undo Y OR does Y undo X? ? ? ? 73.0 73.0
567
Computational Linguistics Volume 39, Number 3
5. Manifestation of Highly Contrasting Word Pairs in Text
As pointed out earlier, there is work on a small set of opposites showing that opposites
co-occur more often than chance (Charles and Miller 1989; Fellbaum 1995). Section 5.1
describes experiments on a larger scale to determine whether highly contrasting word
pairs (including opposites) occur together more often than randomly chosen word
pairs of similar frequency. The section also compares co-occurrence associations with
synonyms.
Research in distributional similarity has found that entries in distributional thesauri
tend to also contain terms that are opposite in meaning (Lin 1998; Lin et al 2003).
Section 5.2 describes experiments to determine whether highly contrasting word pairs
(including opposites) occur in similar contexts as often as randomly chosen pairs of
words with similar frequencies, and whether highly contrasting words occur in similar
contexts as often as synonyms.
5.1 Co-Occurrence
In order to compare the tendencies of highly contrasting word pairs, synonyms, and
random word pairs to co-occur in text, we created three sets of word pairs: the high-
contrast set, the synonyms set, and the control set of random word pairs. The high-contrast set
was created from a pool of direct and indirect opposites (nouns, verbs, and adjectives)
from WordNet. We discarded pairs that did not meet the following conditions: (1) both
members of the pair must be unigrams, (2) both members of the pair must occur in
the British National Corpus (BNC) (Burnard 2000), and (3) at least one member of the
pair must have a synonym in WordNet. A total of 1,358 word pairs remained, and these
form the high-contrast set.
Each of the pairs in the high-contrast set was used to create a synonym pair by
choosing a WordNet synonym of exactly one member of the pair.7 If a word has more
than one synonym, then the most frequent synonym is chosen.8 These 1,358 word pairs
form the synonyms set. Note that for each of the pairs in the high-contrast set, there is a
corresponding pair in the synonyms set, such that the two pairs have a common term.
For example, the pair agitation and calmness in the high-contrast set has a corresponding
pair agitation and ferment in the synonyms set. We will refer to the common terms
(agitation in this example) as the focus words. Because we also wanted to compare
occurrence statistics of the high-contrast set with the random pairs set, we created the
control set of random pairs by taking each of the focus words and pairing them with
another word in WordNet that has a frequency of occurrence in BNC closest to the term
contrasting with the focus word. This is to ensure that members of the pairs across the
high-contrast set and the control set have similar unigram frequencies.
We calculated the pointwise mutual information (PMI) (Church and Hanks 1990)
for each of the word pairs in the high-contrast set, the random pairs set, and the
synonyms set using unigram and co-occurrence frequencies in the BNC. If two words
occurred within a window of five adjacent words in a sentence, they were marked as
co-occurring (same window as Church and Hanks [1990] used in their seminal work on
word?word associations). Table 7 shows the average and standard deviation in each set.
7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym is
taken.
8 WordNet lists synonyms in order of decreasing frequency in the SemCor corpus.
568
Mohammad et al Computing Lexical Contrast
Table 7
Pointwise mutual information (PMI) of word pairs. High positive values imply a tendency to
co-occur in text more often than random chance.
average PMI standard deviation
high-contrast set 1.471 2.255
random pairs set 0.032 0.236
synonyms set 0.412 1.110
Observe that the high-contrast pairs have a much higher tendency to co-occur than the
random pairs control set, and also the synonyms set. The high-contrast set has a large
standard deviation, however. A two-sample t-test revealed that the high-contrast set
is significantly different from the random set (p < 0.05), and also that the high-contrast
set is significantly different from the synonyms set (p < 0.05).
On average, however, the PMI between a focus word and its contrasting term was
lower than the PMI between the focus word and 3,559 other words in the BNC. These
were often words related to the focus words, but neither contrasting nor synonymous.
Thus, even though a high tendency to co-occur is a feature of highly contrasting pairs,
it is not a sufficient condition for detecting them. We use PMI as part of our method
for determining the degree of lexical contrast (described in Section 6).
5.2 Distributional Similarity
Charles and Miller (1989) proposed that in most contexts, opposites may be inter-
changed. The meaning of the utterance will be inverted, of course, but the sentence
will remain grammatical and linguistically plausible. This came to be known as the
substitutability hypothesis. Their experiments did not support this claim, however. They
found that given a sentence with the target adjective removed, most people did not
confound the missing word with its opposite. Justeson and Katz (1991) later showed
that in sentences that contain both members of an adjectival opposite pair, the target
adjectives do indeed occur in similar syntactic structures at the phrasal level. Jones et al
(2007) show how the tendency to appear in certain textual constructions such as ?from
X to Y? and ?either X or Y? are indicative of prototypicalness of opposites. Thus, we can
formulate the distributional hypothesis of highly contrasting pairs: highly contrasting
pairs occur in similar contexts more often than non-contrasting word pairs.
We used the same sets of high-contrast pairs, synonyms, and random pairs de-
scribed in the previous section to gather empirical proof of the distributional hypothesis.
We calculated the distributional similarity between each pair in the three sets using
Lin?s (1998) measure. Table 8 shows the average and standard deviation in each set.
Observe that the high-contrast set has a much higher average distributional similarity
Table 8
Distributional similarity of word pairs. The measure proposed in Lin (1998) was used.
average distributional similarity standard deviation
opposites set 0.064 0.071
random pairs set 0.036 0.034
synonyms set 0.056 0.057
569
Computational Linguistics Volume 39, Number 3
than the random pairs control set, and interestingly it is also higher than the synonyms
set. Once again, the high-contrast set has a large standard deviation. A two-sample
t-test revealed that the high-contrast set is significantly different from both the random
set and the synonyms set with a confidence interval of 0.05. This demonstrates that
relative to other word pairs, high-contrast pairs tend to occur in similar contexts. We
also find that the synonyms set has a significantly higher distributional similarity than
the random pairs set (p< 0.05). This shows that near-synonymous word pairs also occur
in similar contexts (the distributional hypothesis of similarity). Further, a consequence
of the large standard deviations in the cases of both high-contrast pairs and synonyms
means that distributional similarity alone is not sufficient to determine whether two
words are contrasting or synonymous. An automatic method for recognizing contrast
will require additional cues. Our method uses PMI and other sources of information
described in the next section.
6. Computing Lexical Contrast
In this section, we recapitulate the automatic method for determining lexical con-
trast that we first proposed in Mohammad, Dorr, and Hirst (2008). Additional details
are provided regarding the lexical resources used (Section 6.1) and the method itself
(Section 6.2).
6.1 Lexical Resources
Our method makes use of a published thesaurus and co-occurrence information from
text. Optionally, it can use opposites listed in WordNet if available. We briefly describe
these resources here.
6.1.1 Published Thesauri. Published thesauri, such as Roget?s and Macquarie, divide the
vocabulary of a language into about a thousand categories. Words within a category are
semantically related to each other, and they tend to pertain to a coarse concept. Each
category is represented by a category number (unique ID) and a head word?a word that
best represents the meanings of the words in the category. One may also find opposites
in the same category, but this is rare. Words with more than one meaning may be found
in more than one category; these represent its coarse senses.
Within a category, the words are grouped into finer units called paragraphs. Words
in the same paragraph are closer in meaning than those in differing paragraphs. Each
paragraph has a paragraph head?a word that best represents the meaning of the words
in the paragraph. Words in a thesaurus paragraph belong to the same part of speech. A
thesaurus category may have multiple paragraphs belonging to the same part of speech.
For example, a category may have three noun paragraphs, four verb paragraphs, and
one adjective paragraph. We will take advantage of the structure of the thesaurus in
our approach.
6.1.2 WordNet. As mentioned earlier, WordNet encodes certain opposites. We found
in our experiments (Section 7, subsequently) that more than 90% of contrasting pairs
included in Graduate Record Examination (GRE) ?most contrasting word? questions
are not encoded in WordNet, however. Also, neither WordNet nor any other manually
created repository of opposites provides the degree of contrast between word pairs.
Nevertheless, we investigate the usefulness of WordNet as a source of seed opposites
for our approach.
570
Mohammad et al Computing Lexical Contrast
6.2 Proposed Measure of Lexical Contrast
Our method for determining lexical contrast has two parts: (1) determining whether
the target word pair is contrasting or not, and (2) determining the degree of contrast
between the words.
6.2.1 Detecting Whether a Target Word Pair is Contrasting. We use the contrast hypothesis
to determine whether two words are contrasting. The hypothesis is repeated here:
Contrast Hypothesis: If a pair of words, A and B, are contrasting, then there is a pair of
opposites, C and D, such that A and C are strongly related and B and D are strongly
related.
Even if a few exceptions to this hypothesis are found (we are not aware of any), the
hypothesis would remain useful for practical applications. We first determine pairs of
thesaurus categories that have at least one word in each category that are opposites of
each other. We will refer to these categories as contrasting categories and the opposite
connecting the two categories as the seed opposite. Because each thesaurus category
is a collection of closely related terms, all of the word pairs across two contrasting
categories satisfy the contrast hypothesis, and they are considered to be contrasting
word pairs. Note also that words within a thesaurus category may belong to different
parts of speech, and they may be related to the seed opposite word through any of
the many possible semantic relations. Thus a small number of seed opposites can
help identify a large number of contrasting word pairs. We determine whether two
categories are contrasting using the three methods described here, which may be used
alone or in combination with each other:
Method 1: Using word pairs generated from affix patterns.
Opposites such as hot?cold and dark?light occur frequently in text, but in terms of type-
pairs they are outnumbered by those created using affixes, such as un- (clear?unclear)
and dis- (honest?dishonest). Further, this phenomenon is observed in most languages
(Lyons 1977).
Table 9 lists 15 affix patterns that tend to generate opposites in English. They
were compiled by the first author by examining a small list of affixes for the English
language.9 These patterns were applied to all words in the thesaurus that are at least
three characters long. If the resulting term was also a valid word in the thesaurus, then
the word pair was added to the affix-generated seed set. These fifteen rules generated 2,682
word pairs when applied to the words in the Macquarie Thesaurus. Category pairs that
had these opposites were marked as contrasting. Of course, not all of the word pairs
generated through affixes are truly opposites, for example sect?insect and part?impart.
For now, such pairs are sources of error in the system. Manual analysis of these 2,682
word pairs can help determine whether this error is large or small. (We have released
the full set of word pairs.) Evaluation results (Section 7) indicate that these seed pairs
improve the overall accuracy of the system, however.
Figure 3 presents such an example pair. Observe that categories 360 and 361 have
the words cover and uncover, respectively. Affix pattern 8 from Table 9 produces seed
9 http://www.englishclub.com/vocabulary/prefixes.htm.
571
Computational Linguistics Volume 39, Number 3
Table 9
Fifteen affix patterns used to generate opposites. Here ?X? stands for any sequence of letters
common to both words w1 and w2.
affix pattern
pattern # word 1 word 2 # word pairs example pair
1 X antiX 41 clockwise?anticlockwise
2 X disX 379 interest?disinterest
3 X imX 193 possible?impossible
4 X inX 690 consistent?inconsistent
5 X malX 25 adroit?maladroit
6 X misX 142 fortune?misfortune
7 X nonX 72 aligned?nonaligned
8 X unX 833 biased?unbiased
9 lX illX 25 legal?illegal
10 rX irrX 48 regular?irregular
11 imX exX 35 implicit?explicit
12 inX exX 74 introvert?extrovert
13 upX downX 22 uphill?downhill
14 overX underX 52 overdone?underdone
15 Xless Xful 51 harmless?harmful
Total: 2,682
pair cover?uncover, and so the system concludes that the two categories have contrasting
meaning. The contrast in meaning is especially strong for the paragraphs cover and
expose because words within these paragraphs are very close in meaning to cover and
uncover, respectively. We will refer to such thesaurus paragraph pairs that have one
word each of a seed pair as prime contrasting paragraphs. We expect the words across
prime contrasting paragraphs to have a high degree of antonymy (for example, mask
and bare), whereas words across other contrasting category paragraphs may have a
smaller degree of antonymy as the meaning of these words may diverge significantly
from the meanings of the words in the prime contrasting paragraphs (for example,
white lie and disclosure).
Method 2: Using opposites from WordNet.
We compiled a list of 20,611 pairs that WordNet records as direct and indirect opposites.
(Recall discussion in Section 4 about direct and indirect opposites.) A large number of
these pairs include multiword expressions. Only 10,807 of the 20,611 pairs have both
words in the Macquarie Thesaurus?the vocabulary used for our experiments. We will
refer to them as the WordNet seed set. Category pairs that had these opposites were
marked as contrasting.
Method 3: Using word pairs in adjacent thesaurus categories.
Most published thesauri, such as Roget?s, are organized such that categories correspond-
ing to opposing concepts are placed adjacent to each other. For example, in the Macquarie
Thesaurus: category 369 is about honesty and category 370 is about dishonesty; as shown
in Figure 3, category 360 is about hiding and category 361 is about revealing. There are
a number of exceptions to this rule, and often a category may be contrasting in meaning
to several other categories. Because this was an easy enough heuristic to implement,
however, we investigated the usefulness of considering adjacent thesaurus categories
572
Mohammad et al Computing Lexical Contrast
Figure 3
Example contrasting category pair. The system identifies the pair to be contrasting through the
affix-based seed pair cover?uncover. The paragraphs of cover and expose are referred to as prime
contrasting paragraphs. Paragraph heads are shown in bold italic.
as contrasting. We will refer to this as the adjacency heuristic. Note that this method of
determining contrasting categories does not explicitly identify a seed opposite, but one
can assume the head words of these category pairs as the seed opposites.
To determine how accurate the adjacency heuristic is, the first author manually
inspected adjacent thesaurus categories in the Macquarie Thesaurus to determine which
of them were indeed contrasting. Because a category, on average, has about a hundred
words, the task was made less arduous by representing each category by just the first
ten words listed in it. This way it took only about five hours to manually determine that
209 pairs of the 811 adjacent Macquarie category pairs were contrasting. Twice, it was
found that category number X was contrasting not just to category number X+1 but also
to category number X+2: category 40 (ARISTOCRACY) has a meaning that contrasts that
of category 41 (MIDDLE CLASS) as well as category 42 (WORKING CLASS); category 542
(PAST) contrasts with category 543 (PRESENT) as well as category 544 (FUTURE). Both
these X ? (X+2) pairs are also added to the list of manually annotated contrasting
categories.
6.2.2 Computing the Degree of Contrast Between Two Words. Charles and Miller (1989)
and Fellbaum (1995) argued that opposites tend to co-occur more often than random
chance. Murphy and Andrew (1993) claimed that the greater-than-chance co-occurrence
of opposites is because together they convey contrast well, which is rhetorically useful.
We showed earlier in Section 5.1 that highly contrasting pairs (including opposites)
co-occur more often than randomly chosen pairs. All of these support the degree of
contrast hypothesis stated earlier in the introduction:
Degree of Contrast Hypothesis: If a pair of words, A and B, are contrasting, then their
degree of contrast is proportional to their tendency to co-occur in a large corpus.
573
Computational Linguistics Volume 39, Number 3
We used PMI to capture the tendency of word?word co-occurrence. We collected
these co-occurrence statistics from the Google n-gram corpus (Brants and Franz 2006),
which was created from a text collection of over one trillion words. Words that occurred
within a window of five words were considered to be co-occurring.
We expected that some features may be more accurate than others. If multiple
features give evidence towards opposing information, then it is useful for the system
to know which feature is more reliable. Therefore, we held out some data from the
evaluation data described in Section 7.1 as the development set. Experiments on the
development set showed that contrasting words may be placed in three bins corre-
sponding to the amount of reliability of the source feature: high, medium, or acceptable.
 High reliability (Class I): target words that belong to adjacent thesaurus
categories. For example, all the word pairs across categories 360 and 361,
shown in Figure 3. Examples of Class I contrasting word pairs from the
development set include graceful?ungainly, fortunate?hapless, obese?slim,
and effeminate?virile. (Note, there need not be any affix or WordNet seed
pairs across adjacent thesaurus categories for these word pairs to be
marked Class I.) As expected, if we use only those adjacent categories that
were manually identified to be contrasting (as described in Section 6.2.1,
Method 3), then the system obtains even better results than those obtained
using all adjacent thesaurus categories. (Experiments and results shown
in Section 7.1).
 Medium reliability (Class II): target words that are not Class I contrasting
pairs, but belong to one paragraph each of a prime contrasting paragraph.
For example, all the word pairs across the paragraphs of sympathetic and
indifferent. See Figure 4. Examples of Class II contrasting word pairs
from the development set include altruism?avarice, miserly?munificent,
accept?repudiate, and improper?prim.
 Acceptable reliability (Class III): target words that are not Class I or
Class II contrasting pairs, but occur across contrasting category pairs. For
example, all word pairs across categories 423 and 230 except those that
have one word each from the paragraphs of sympathetic and indifferent.
See Figure 4. Examples of Class III contrasting word pairs from the
development set include pandemonium?calm, probity?error, artifice?sincerity,
and hapless?wealthy.
Even with access to very large textual data sets, there is always a long tail of words
that occur so few times that there is not enough co-occurrence information for them.
Thus we assume that all word pairs in Class I have a higher degree of contrast than
all word pairs in Class II, and that all word pairs in Class II have a higher degree of
contrast than the pairs in Class III. If two word pairs belong to the same class, then we
calculate their tendency to co-occur with each other in text to determine which pair is
more contrasting. All experiments in the evaluation section ahead follow this method.
6.2.3 Lexicon of Contrasting Word Pairs. Using the method described in the previous
sections, we generated a lexicon of word pairs pertaining to Class I and Class II. The
lexicon has 6.3 million contrasting word pairs, about 3.5 million of which belong to
Class I and about 2.8 million to Class II. Class III pairs are even more numerous and,
given a word pair, our algorithm checked whether it is a Class III pair, but we did not
574
Mohammad et al Computing Lexical Contrast
1. nouns:
Category number: 423
Category head: KINDNESS
1. nouns:
Category number: 230
Category head: APATHY
kindness
considerateness
niceness
goodness
...
2. adjectives:
sympathetic
consolatory
caring
involved...
3. adverbs:
benevolent
beneficiently
graciously
kindheartedly...
...
apathy
acedia
depression
moppishness...
2. nouns:
nonchalance
insouciance
carelessness
casualness
3. adjectives:
...
indifferent
detached
irresponsive
uncaring...
...
Figure 4
Example contrasting category pair that has Class II and Class III contrasting pairs. The system
identifies the pair to be contrasting through the affix-based seed pair caring (second word in
paragraph 2 or category 423) and uncaring (fourth word in paragraph 3 or category 230). The
paragraphs of sympathetic and indifferent are therefore the prime contrasting paragraphs and so
all word pairs that have one word each from these two paragraphs are Class II contrasting pairs.
All other pairs formed by taking one word each from the two contrasting categories are the
Class III contrasting pairs. Paragraph heads are shown in bold italic.
create a complete set of all Class III contrasting pairs. Class I and II lexicons are available
for download and summarized in Table 18.
7. Evaluation
We evaluate our algorithm on two different tasks and four data sets. Section 7.1 de-
scribes experiments on solving existing GRE ?choose the most contrasting word? ques-
tions (a recapitulation of the evaluation reported in Mohammad, Dorr, and Hirst [2008]).
Section 7.2 describes experiments on solving newly created ?choose the most contrast-
ing word? questions specifically designed to determine performance on different kinds
of opposites. And lastly, Section 7.3 describes experiments on two different data sets
where the goal is to identify whether a given word pair is synonymous or antonymous.
7.1 Solving GRE?s ?Choose the Most Contrasting Word? Questions
The GRE is a test taken by thousands of North American graduate school applicants.
The test is administered by Educational Testing Service (ETS). The Verbal Reasoning
section of GRE is designed to test verbal skills. Until August 2011, one of its sections had
a set of questions pertaining to word-pair contrast. Each question had a target word and
four or five alternatives, or option words. The objective was to identify the alternative
which was most contrasting with respect to the target. For example, consider:
adulterate: a. renounce b. forbid c. purify d. criticize e. correct
575
Computational Linguistics Volume 39, Number 3
Here the target word is adulterate. One of the alternatives provided is correct, which
as a verb has a meaning that contrasts with that of adulterate; purify, however, has
a greater degree of contrast with adulterate than correct does and must be chosen
in order for the instance to be marked as correctly answered. ETS referred to these
questions as ?antonym questions,? where the examinees had to ?choose the word
most nearly opposite? to the target. Most of the target?answer pairs are not gradable
adjectives, however, and because most of them are not opposites either, we will refer
to these questions as ?choose the most contrasting word? questions or contrast questions
for short.
Evaluation on this data set tests whether the automatic method is able to identify
not just opposites but also those pairs that are not opposites but that have some degree
of semantic contrast. Notably, for these questions, the method must be able to identify
that one word pair has a higher degree of contrast than all others, even though that
word pair may not necessarily be an opposite.
7.1.1 Data. A Web search for large sets of contrast questions yielded two independent
sets of questions designed to prepare students for the GRE. The first set consists of
162 questions. We used this set while we were developing our lexical contrast algo-
rithm described in Section 4. Therefore, we will refer to it as the development set. The
development set helped determine which features of lexical contrast were more reliable
than others. The second set has 1,208 contrast questions. We discarded questions that
had a multiword target or alternative. After removing duplicates we were left with
790 questions, which we used as the unseen test set. This data set was used (and seen)
only after our algorithm for determining lexical contrast was frozen.
Interestingly, the data contains many instances that have the same target word used
in different senses. For example:
1. obdurate: a. meager b. unsusceptible c. right d. tender e. intelligent
2. obdurate: a. yielding b. motivated c. moribund d. azure e. hard
3. obdurate: a. transitory b. commensurate c. complaisant d. similar e. laconic
In (1), obdurate is used in the sense of HARDENED IN FEELINGS and is most contrasting
with tender. In (2), it is used in the sense of RESISTANT TO PERSUASION and is most
contrasting with yielding. In (3), it is used in the sense of PERSISTENT and is most
contrasting with transitory.
The data sets also contain questions in which one or more of the alternatives is a
near-synonym of the target word. For example:
astute: a. shrewd b. foolish c. callow d. winning e. debating
Observe that shrewd is a near-synonym of astute. The word most contrasting with astute
is foolish. A manual check of a randomly selected set of 100 test-set questions revealed
that, on average, one in four had a near-synonym as one of the alternatives.
7.1.2 Results. Table 10 presents results obtained on the development and test data
using two baselines, a re-implementation of the method described in Lin et al (2003),
and variations of our method. Some of the results are for systems that refrain from
576
Mohammad et al Computing Lexical Contrast
Table 10
Results obtained on contrast questions. The best performing system and configuration are
shown in bold.
development data test data
P R F P R F
Baselines:
a. random baseline 0.20 0.20 0.20 0.20 0.20 0.20
b. WordNet antonyms 0.23 0.23 0.23 0.23 0.23 0.23
Related work:
a. Lin et al (2003) 0.23 0.23 0.23 0.24 0.24 0.24
Our method:
a. affix-generated pairs as seeds 0.72 0.53 0.61 0.71 0.51 0.59
b. WordNet antonyms as seeds 0.79 0.52 0.63 0.72 0.49 0.58
c. both seed sets (a + b) 0.77 0.65 0.70 0.72 0.58 0.64
d. adjacency heuristic only 0.81 0.43 0.56 0.83 0.44 0.57
e. manual annotation of adjacent categories 0.88 0.41 0.56 0.87 0.41 0.55
f. affix seed set and adjacency heuristic (a + d) 0.75 0.60 0.67 0.76 0.60 0.67
g. both seed sets and adjacency heuristic (a + b + d) 0.76 0.66 0.70 0.76 0.63 0.69
h. affix seed set and annotation of adjacent 0.79 0.63 0.70 0.78 0.60 0.68
categories (a + e)
i. both seed sets and annotation of adjacent 0.79 0.66 0.72 0.77 0.63 0.69
categories (a + b + e)
attempting questions for which they do not have sufficient information. We therefore
report precision (P), recall (R), and balanced F-score (F).
P =
# of questions answered correctly
# of questions attempted
(1)
R =
# of questions answered correctly
# of questions
(2)
F =
2 ? P ? R
P+ R
(3)
Baselines. If a system randomly guesses one of the five alternatives with equal probabil-
ity (random baseline), then it obtains an accuracy of 0.2. A system that looks up the list of
WordNet antonyms (10,807 pairs) to solve the contrast questions is our second baseline.
That obtained the correct answer in only 5 instances of the development set (3.09% of
the 162 instances) and 25 instances of the test set (3.17% of the 790 instances), however.
Even if the system guesses at random for all other instances, it attains only a modest
improvement over the random baseline (see row b, under ?Baselines,? in Table 10).
Re-implementation of related work. In order to estimate how well the method of Lin
et al (2003) performs on this task, we re-implemented their method. For each closest-
antonym question, we determined frequency counts in the Google n-gram corpus for
the phrases ?from ?target word? to ?known correct answer?,? ?from ?known correct
answer? to ?target word?,? ?either ?target word? or ?known correct answer?,? and
?either ?known correct answer? or ?target word?.? We then summed up the four counts
for each contrast question. This resulted in non-zero counts for only 5 of the 162 in-
stances in the development set (3.09%), and 35 of the 790 instances in the test set (4.43%).
Thus, these patterns fail to cover a vast majority of closest-antonyms, and even if the
577
Computational Linguistics Volume 39, Number 3
system guesses at random for all other instances, it attains only a modest improvement
over the baseline (see row a, under ?Related work,? in Table 10).
Our method. Table 10 presents results obtained on the development and test data using
different combinations of the seed sets and the adjacency heuristic. The best performing
system is marked in bold. It has significantly higher precision and recall than that of the
method proposed by Lin et al (2003), with 95% confidence according to the Fisher Exact
Test (Agresti 1990).
We performed experiments on the development set first, using our method with
configurations described in rows a, b, and d. These results showed that marking
adjacent categories as contrasting has the highest precision (0.81), followed by using
WordNet seeds (0.79), followed by the use of affix rules to generate seeds (0.72). This
allowed us to determine the relative reliability of the three features as described in
Section 6.2.2. We then froze all system development and ran the remaining experiments,
including those on the test data.
Observe that all of the results shown in Table 10 are well above the random baseline
of 0.20. Using only the small set of 15 affix rules, the system performs almost as well as
when it uses 10,807 WordNet opposites. Using both the affix-generated and the Word-
Net seed sets, the system obtains markedly improved precision and coverage. Using
only the adjacency heuristic gave precision values (upwards of 0.8) with substantial
coverage (attempting more than half of the questions). Using the manually identified
contrasting adjacent thesaurus categories gave precision values just short of 0.9. The
best results were obtained using both seed sets and the contrasting adjacent thesaurus
categories (F-scores of 0.72 and 0.69 on the development and test set, respectively).
In order to determine whether our method works well with thesauri other than the
Macquarie Thesaurus, we determined performance of configurations a, b, c, d, f, and h
using the 1911 U.S. edition of the Roget?s Thesaurus, which is available freely in the public
domain.10 The results were similar to those obtained using the Macquarie Thesaurus. For
example, configuration g obtained a precision of 0.81, recall of 0.58, and F-score of 0.68
on the test set. It may be possible to obtain even better results by combining multiple
lexical resources; that is left for future work. The remainder of this article reports results
obtained with the Macquarie Thesaurus; the 1911 vocabulary is less suited for practical
use in the 21st century.
7.1.3 Discussion. These results show that our method performs well on questions de-
signed to be challenging for humans. In tasks that require higher precision, using only
the contrasting adjacent categories is best, whereas in tasks that require both precision
and coverage, the seed sets may be included. Even when both seed sets were included,
only four instances in the development set and twenty in the test set had target?answer
pairs that matched a seed opposite pair. For all remaining instances, the approach had
to generalize to determine the most contrasting word. This also shows that even the
seemingly large number of direct and indirect antonyms from WordNet (more than
10,000) are by themselves insufficient.
The comparable performance obtained using the affix rules alone suggests that even
in languages that do not have a WordNet-like resource, substantial accuracies may be
obtained. Of course, improved results when using WordNet antonyms as well suggests
that the information they provide is complementary.
10 http://www.gutenberg.org/ebooks/10681.
578
Mohammad et al Computing Lexical Contrast
Error analysis revealed that at times the system failed to identify that a category
pertaining to the target word contrasted with a category pertaining to the answer.
Additional methods to identify seed opposite pairs will help in such cases. Certain
other errors occurred because one or more alternatives other than the official answer
were also contrasting with the target. For example, one of the questions has chasten as
the target word. One of the alternatives is accept, which has some degree of contrast in
meaning to the target. Another alternative, reward, has an even higher degree of contrast
with the target, however. In this instance, the system erred by choosing accept as the
answer.
7.2 Determining Performance of Automatic Method on Different Kinds of Opposites
The previous section showed the overall performance of our method. The performance
of a method may vary significantly on different subsets of data, however. In order to
determine performance on different kinds of opposites, we generated new contrast
questions from the crowdsourced term pairs described in Section 4. Note that for solving
contrast questions with this data set, again the method must be able to identify that
one word pair has a higher degree of contrast than the other pairs; unlike the previous
section, however, here the correct answer is often an opposite of the target.
7.2.1 Generating Contrast Questions. For each word pair from the list of WordNet oppo-
sites, we chose one word randomly to be the target word, and the other as one of its
candidate options. Four other candidate options were chosen from Lin?s distributional
thesaurus (Lin 1998).11 An entry in the distributional thesaurus has a focus word and
a number of other words that are distributionally similar to the focus word. The words
are listed in decreasing order of similarity. Note that these entries include not just
near-synonymous words but also at times contrasting words because contrasting
words tend to be distributionally similar (Lin et al 2003).
For each of the target words in our contrast questions, we chose the four distribu-
tionally closest words from Lin?s thesaurus to be the distractors. If a distractor had the
same first three letters as the target word or the correct answer, then it was replaced
with another word from the distributional thesaurus. This ad hoc filtering criterion is
effective at discarding distractors that are morphological variants of the target or the
answer. For example, if the target word is adulterate, then words such as adulterated and
adulterates will not be included as distractors even if they are listed as closely similar
terms in the distributional thesaurus.
We place the four distractors and the correct answer in random order. Some of the
WordNet opposites were not listed in Lin?s thesaurus, and the corresponding question
was not generated. In all, 1,269 questions were generated. We created subsets of these
questions corresponding to the different kinds of opposites and also corresponding to
different parts of speech. Because a word pair may be classified as more than one kind
of opposite, the corresponding question may be part of more than one subset.
7.2.2 Experiments and Results. We applied our method of lexical contrast to solve the
complete set of 1,269 questions and also the various subsets. Because this test set is
11 http://webdocs.cs.ualberta.ca/?lindek/downloads.htm.
579
Computational Linguistics Volume 39, Number 3
Table 11
Percentage of contrast questions correctly answered by the automatic method, where different
question sets correspond to target?answer pairs of different kinds. The automatic method did not
use WordNet seeds for this task. The results shown for ?ALL? are micro-averages, that is, they are
the results for the master set of 1,269 contrast questions.
# instances P R F
Antipodals 1,044 0.95 0.84 0.89
Complementaries 1,042 0.95 0.83 0.89
Disjoint 228 0.81 0.59 0.69
Gradable 488 0.95 0.85 0.90
Reversives 203 0.93 0.74 0.82
ALL 1,269 0.93 0.79 0.85
created from WordNet opposites, we applied the algorithm without the use of WordNet
seeds (no WordNet information was used by the method).
Table 11 shows the precision (P), recall (R), and F-score (F) obtained by the method
on the data sets corresponding to different kinds of opposites. The column ?# instances?
shows the number of questions in each of the data sets. The performance of our method
on the complete data set is shown in the last row ALL. Observe that the F-score of
0.85 is markedly higher than the score obtained on the GRE-preparatory questions.
This is expected because the GRE questions involved vocabulary from a higher reading
level, and included carefully chosen distractors to confuse the examinee. The automatic
method obtains highest F-score on the data sets of gradable adjectives (0.90), antipodals
(0.89), and complementaries (0.89). The precisions and recalls for these opposites are
significantly higher than those of disjoint opposites. The recall for reversives is also sig-
nificantly lower than that for the gradable adjectives, antipodals, and complementaries,
but precision on reversives is quite good (0.93).
Table 12 shows the precision, recall, and F-score obtained by the method on the the
data sets corresponding to different parts of speech. Observe that performances on all
parts of speech are fairly high. The method deals with adverb pairs best (F-score of 0.89),
and the lowest performance is for verbs (F-score of 0.80). The differences in precision
values between various parts of speech are not significant. The recall obtained on the
adverbs is significantly higher than that obtained on adjectives, however, and the recall
on adjectives is significantly higher than that obtained on verbs. The difference between
the recalls on adverbs and nouns is not significant. We used the Fisher Exact Test and a
confidence interval of 95% for all significance testing reported in this section.
Table 12
Percentage of contrast questions correctly answered by the automatic method, where different
question sets correspond to different parts-of-speech.
# instances P R F
Adjectives 551 0.92 0.79 0.85
Adverbs 165 0.95 0.84 0.89
Nouns 330 0.93 0.81 0.87
Verbs 226 0.93 0.71 0.80
ALL 1,269 0.93 0.79 0.85
580
Mohammad et al Computing Lexical Contrast
7.3 Distinguishing Synonyms from Opposites
Our third evaluation follows that of Lin et al (2003) and Turney (2008). We developed
a system for automatically distinguishing synonyms from opposites, and applied it to
two data sets. The approach and experiments are described herein.
7.3.1 Data. Lin et al (2003) compiled 80 pairs of synonyms and 80 pairs of opposites
from the Webster?s Collegiate Thesaurus (Kay 1988) such that each word in a pair is also
in their list of the 50 distributionally most similar words of the other. (Distributional
similarity was calculated using the algorithm proposed by Lin et al [1998].) Turney
(2008) compiled 136 pairs of words (89 opposites and 47 synonyms) from various Web
sites for learners of English as a second language; the objective for the learners is to
identify whether the words in a pair are opposites or synonyms of each other. The
goals of this evaluation are to determine whether our automatic method can distinguish
opposites from near-synonyms, and to compare our method with the closest related
work on an evaluation task for which published results are already available.
7.3.2 Method. The core of our method is this:
1. Word pairs that occur in the same thesaurus category are close in meaning
and so are marked as synonyms.
2. Word pairs that occur in contrasting thesaurus categories or paragraphs
(as described in Section 6.2.1 above) are marked as opposites.
Even though opposites often occur in different thesaurus categories, they can sometimes
also be found in the same category, however. For example, the word ascent is listed in
the Macquarie Thesaurus categories of 49 (CLIMBING) and 694 (SLOPE), whereas the word
descent is listed in the categories 40 (ARISTOCRACY), 50 (DROPPING), 538 (PARENTAGE),
and 694 (SLOPE). Observe that ascent and descent are both listed in the same category
694 (SLOPE), which makes sense here because both words are pertinent to the concept of
slope. On the other hand, two separate clues independently inform our system that the
words are opposites of each other: (1) Category 49 has the word upwardness in the same
paragraph as ascent, and category 50 has the word downwardness in the same paragraph
as descent. The 13th affix pattern from Table 9 (upX and downX) indicates that the two
thesaurus paragraphs have contrasting meaning. Thus, ascent and descent occur in prime
contrasting thesaurus paragraphs. (2) One of the ascent categories (49) is adjacent to one
of the descent categories (50), and further this adjacent category pair has been manually
marked as contrasting.
Thus the words in a pair may be deemed both synonyms and opposites simultane-
ously by our methods of determining synonyms and opposites, respectively. Some of
the features we use to determine opposites were found to be more precise (e.g., words
listed in adjacent categories) than others (e.g., categories identified as contrasting based
on affix and WordNet seeds), however. Thus we apply the following rules as a decision
list: If one rule fires, then the subsequent rules are ignored.
1. Rule 1 (high confidence for opposites): If the words in a pair occur in
adjacent thesaurus categories, then they are marked as opposites.
2. Rule 2 (high confidence for synonyms): If both the words in a pair occur
in the same thesaurus category, then they are marked as synonyms.
581
Computational Linguistics Volume 39, Number 3
3. Rule 3 (medium confidence for opposites): If the words in a pair occur in
prime contrasting thesaurus paragraphs, as determined by an affix-based
or WordNet seed set, then they are marked as opposites.
If a word pair is not tagged as synonym or opposite: (a) the system can refrain
from attempting an answer (this will attain high precision), or (b) the system can
randomly guess the lexical relation (this will obtain 50% accuracy for the pairs), or (c) it
could mark all remaining word pairs with the predominant lexical relation in the data
(this will obtain an accuracy proportional to the skew in distribution of opposites and
synonyms). For example, if after step 3, the system finds that 70% of the marked word
pairs were tagged opposites, and 30% as synonyms, then it could mark every hitherto
untagged word pair (word pair for which it has insufficient information) as opposites.
We implemented all three variants. Note that option (b) is indeed expected to perform
poorly compared to option (c), but we include it as part of our evaluation to measure
usefulness of option (c).
7.3.3 Results and Discussion. Table 13 shows the precision (P), recall (R), and balanced
F-score (F) of various systems and baselines in identifying synonyms and opposites
from the data set described in Lin et al (2003). We will refer to this data set as LZQZ
(the first letters of the authors? last names).
If a system guesses at random (random baseline) it will obtain an accuracy of 50%.
Choosing opposites (or synonyms) as the predominant class also obtains an accuracy of
50% because the data set has an equal number of opposites and synonyms. Published
results on LZQZ (Lin et al 2003) are shown here again for convenience. The results
obtained with our system and the three variations on handling word pairs for which
it does not have enough information are shown in the last three rows. The precision of
our method in configuration (a) is significantly higher than that of Lin et al (2003), with
95% confidence according to the Fisher Exact Test (Agresti 1990). Because precision and
recall are the same for configuration (b) and (c), as well as for the methods described in
Lin et al (2003) and Turney (2011), we can also refer to these results simply as accuracy.
Table 13
Results obtained on the synonym-or-opposite questions in LZQZ. The best performing systems
are marked in bold. The difference in precision and recall of method by Lin et al (2003) and our
method in configurations (b) and (c) is not statistically significant.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.50 0.50 0.50
Related work:
a. Lin et al (2003) 0.90 0.90 0.90
b. Turney (2011) 0.82 0.82 0.82
Our method: if no information
a. refrain from guessing 0.98 0.78 0.87
b. make random guess 0.88 0.88 0.88
c. mark the predominant class? 0.87 0.87 0.87
?This data set has an equal number of opposites and synonyms. Results reported are when
choosing opposites as the predominant class.
?The system concluded that opposites were slightly more frequent than synonyms.
582
Mohammad et al Computing Lexical Contrast
Table 14
Results obtained on the synonym-or-opposite questions in TURN. The best performing systems
are marked in bold.
P R F
Baselines
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.65 0.65 0.65
Related work
a. Turney (2008) 0.75 0.75 0.75
b. Lin et al (2003) 0.35 0.35 0.35
Our method: if no information
a. refrain from guessing 0.97 0.69 0.81
b. make random guess 0.84 0.84 0.84
c. mark the predominant class? 0.90 0.90 0.90
?About 65.4% of the pairs in this data set are opposites. So this row reports baseline results when
choosing opposites as the predominant class.
?The system concluded that opposites were much more frequent than synonyms.
We found that the differences in accuracies between the method of Lin et al (2003) and
our method in configurations (b) and (c) are not statistically significant. The method by
Lin et al (2003) and our method in configuration (b) have significantly higher accuracy
than the method described in Turney (2011), however. The lexical contrast features used
in configurations (a), (b), and (c) correspond to row i in Table 10. The next subsection
presents an analysis of the usefulness of the different features listed in Table 10.
Observe that when our method refrains from guessing in case of insufficient infor-
mation, it obtains excellent precision (0.98), while still providing very good coverage
(0.78). As expected, the results obtained with (b) and (c) do not differ much from each
other because the data set has an equal number of synonyms and opposites. (Note
that the system was not privy to this information.) After step 3 of the algorithm,
however, the system had marked 65 pairs as opposites and 63 pairs as synonyms,
and so it concluded that opposites are slightly more dominant in this data set and
therefore the guess-predominant-class variant marked all previously unmarked pairs as
opposites.
It should be noted that the LZQZ data set was chosen from a list of high-frequency
terms. This was necessary to increase the probability of finding sentences in a corpus
where the target pair occurred in one of the chosen patterns proposed by Lin et al
(2003). As shown in Table 10, the Lin et al (2003) patterns have a very low coverage
otherwise. Further, the test data compiled by Lin et al only had opposites whereas the
contrast questions had many contrasting word pairs that were not opposites.
Table 14 shows results on the data set described in Turney (2008). We will refer to
this data set as TURN. The supervised baseline of always guessing the most frequent
class (in this case, opposites), will obtain an accuracy of 65.4% (P = R = F = 0.654).
Turney (2008) obtains an accuracy of 75% using a supervised method and 10-fold
cross-validation. A re-implementation of the method proposed by Lin et al (2003) as
described in Section 7.1.3 did not recognize any of the word pairs in TURN as opposites;
that is, none of the word pairs in TURN occurred in the Google n-gram corpus in
patterns used by Lin et al (2003). Thus it marked all words in TURN as synonyms.
The results obtained with our method are shown in the last three rows. The precision
and recall of our method in configurations (b) and (c) are significantly higher than those
583
Computational Linguistics Volume 39, Number 3
obtained by the methods by Turney (2008) and Lin et al (2003), with 95% confidence
according to the Fisher Exact Test (Agresti 1990).
Observe that once again our method, especially the variant that refrains from
guessing in case of insufficient information, obtains excellent precision (0.97), while
still providing good coverage (0.69). Also observe that results obtained by guessing the
predominant class (method (c)) are markedly better than those obtained by randomly
guessing in case of insufficient information (method (b)). This is because, as mentioned
earlier, the distribution of opposites and synonyms is somewhat skewed in this data
set (65.4% of the pairs are opposites). Of course, again the system was not privy to this
information, but method (a) marked 58 pairs as opposites and 39 pairs as synonyms.
Therefore, the system concluded that opposites are more dominant and method (c)
marked all previously unmarked pairs as opposites, obtaining an accuracy of 90%.
Recall that in Section 7.3.2 we described how opposite pairs may occasionally be
listed in the same thesaurus category because the category may be pertinent to both
words. For 12 of the word pairs in the Lin et al data and 3 of the word pairs in the
Turney data, both words occurred together in the same thesaurus category, and yet
the system marked them as opposites because they occurred in adjacent thesaurus
categories (Class I). For 11 of the 12 pairs from LZQZ and for all 3 of the TURN pairs,
this resulted in the correct answer. These pairs are shown in Table 15. By contrast, only
one of the term pairs in this table occurred in one of Lin?s patterns of oppositeness, and
was thus the only one correctly identified by their method as a pair of opposites.
It should also be noted that a word may have multiple meanings such that it may be
synonymous to a word in one sense and opposite to it in another sense. Such pairs are
also expected to be marked as opposites by our system. Two such pairs in the Turney
(2008) data are: fantastic?awful and terrific?terrible. The word awful can mean INSPIRING
AWE (and so close to the meaning of fantastic in some contexts), and also EXTREMELY
DISAGREEABLE (and so opposite to fantastic). The word terrific can mean FRIGHTFUL
(and so close to the meaning of terrible), and also UNUSUALLY FINE (and so opposite
to terrible). Such pairs are probably not the best synonym-or-opposite questions. Faced
with these questions, however, humans probably home in on the dominant senses of
Table 15
Pairs from LZQZ and TURN that have at least one category in common but are still marked as
opposites by our method.
LZQZ TURN
word 1 word 2 official solution word 1 word 2 official solution
amateur professional opposite fantastic awful opposite
ascent descent opposite dry wet opposite
back front opposite terrific terrible opposite
bottom top opposite
broadside salvo synonym
entrance exit opposite
heaven hell opposite
inside outside opposite
junior senior opposite
lie truth opposite
majority minority opposite
nadir zenith opposite
strength weakness opposite
584
Mohammad et al Computing Lexical Contrast
Table 16
Results for individual components as well as certain combinations of components on the
synonym-or-opposite questions in LZQZ. The best performing configuration is shown in bold.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.50 0.50 0.50
Our methods:
a. affix-generated seeds only 0.86 0.54 0.66
b. WordNet seeds only 0.88 0.65 0.75
c. both seed sets (a + b) 0.88 0.65 0.75
d. adjacency heuristic only 0.95 0.74 0.83
e. manual annotation of adjacent categories 0.98 0.74 0.84
f. affix seed set and adjacency heuristic (a + d) 0.95 0.75 0.84
g. both seed sets and adjacency heuristic (a + b + d) 0.95 0.78 0.86
h. affix seed set and annotation of adjacent categories 0.98 0.77 0.86
(a + e)
i. both seed sets and annotation of adjacent categories 0.98 0.78 0.87
(a + b + e)
?This data set has equal number of opposites and synonyms, so either class can be chosen to be
predominant. Baseline results shown here are for choosing opposites as the predominant class.
the target words to determine an answer. For example, in modern-day English terrific
is used more frequently in the sense of UNUSUALLY FINE than the sense of FRIGHTFUL,
and so most people will say that terrific and terrible are opposites (in fact that is the
solution provided with these data).
7.3.4 Analysis. We carried out additional experiments to determine how useful individ-
ual components of our method were in solving the synonym-or-opposite questions.
The results on LZQZ are shown in Table 16 and the results on TURN are shown in
Table 17. These results are for the case when the system refrains from guessing in case
of insufficient information. The rows in the tables correspond to the rows in Table 10
shown earlier that gave results on the contrast questions.
Observe that the affix-generated seeds give a marked improvement over the base-
lines, and that knowing which categories are contrasting (either from the adjacency
heuristic or manual annotation of adjacent categories) proves to be the most useful
feature. Also note that even though manual annotation and WordNet seeds eventually
lead to the best results (F = 0.87 for LZQZ and F = 0.81 for TURN), using only the
adjacency heuristic and the affix-generated seeds gives competitive results (F = 0.84 for
the Lin set and F = 0.78 for the Turney set). We are interested in developing methods
to make the approach cross-lingual, so that we can use a thesaurus from one language
(say, English) to compute lexical contrast in a resource-poor target language.
The precision of our method is very good (>0.95). Thus future work will be aimed
at improving recall. This can be achieved by developing methods to generate more seed
opposites. This is also an avenue through which some of the pattern-based approaches
(such as the methods described by Lin et al [2003] and Turney [2008]) can be incorpo-
rated into our method. For instance, we could use n-gram patterns such as ?either X or
Y? and ?from X to Y? to identify pairs of opposites that can be used as additional seeds
in our method.
585
Computational Linguistics Volume 39, Number 3
Recall can also be improved by using affix patterns in other languages to identify
contrasting thesaurus paragraphs in the target language. Thus, constructing a cross-
lingual framework in which words from one language will be connected to thesaurus
categories in another language will be useful not only in computing lexical contrast in a
resource-poor language, but also in using affix information from different languages to
improve results in the target, possibly even resource-rich, language.
8. Conclusions and Future Work
Detecting semantically contrasting word pairs has many applications in natural lan-
guage processing. In this article, we proposed a method for computing lexical contrast
that is based on the hypothesis that if a pair of words, A and B, are contrasting, then
there is a pair of opposites, C and D, such that A and C are strongly related and B and
D are strongly related?the contrast hypothesis. We used pointwise mutual information
to determine the degree of contrast between two contrasting words. The method outper-
formed others on the task of solving a large set of ?choose the most contrasting word?
questions wherein the system not only identified whether two words are contrasting
but also distinguished between pairs of contrasting words with differing degrees of
contrast. We further determined performance of the method on five different kinds of
opposites and across four parts of speech. We used our approach to solve synonym-or-
opposite questions described in Turney (2008) and Lin et al (2003).
Because opposites were central to our methodology, we designed a questionnaire to
better understand different kinds of opposites, which we crowdsourced with Amazon
Mechanical Turk. We devoted extra effort to making sure the questions are phrased
in a simple, yet clear manner. Additionally, a quality control method was developed,
using a word-choice question, to automatically identify and discard dubious and outlier
annotations. From these data, we created a data set of different kinds of opposites that
Table 17
Results for individual components as well as certain combinations of components on the
synonym-or-opposite questions in TURN. The best performing configuration is shown in bold.
P R F
Baselines:
a. random baseline 0.50 0.50 0.50
b. supervised most-frequent baseline? 0.65 0.65 0.65
Our methods:
a. affix-generated seeds only 0.92 0.54 0.68
b. WordNet seeds only 0.93 0.61 0.74
c. both seed sets (a + b) 0.93 0.61 0.74
d. adjacency heuristic only 0.94 0.60 0.74
e. manual annotation of adjacent categories 0.96 0.60 0.74
f. affix seed set and adjacency heuristic (a + d) 0.95 0.67 0.78
g. both seed sets and adjacency heuristic (a + b + d) 0.95 0.68 0.79
h. affix seeds and annotation of adjacent categories 0.97 0.68 0.80
(a + e)
i. both seed sets and annotation of adjacent categories 0.97 0.69 0.81
(a + b + e)
?About 65.4% of the pairs in this data set are opposites. So this row reports baseline results when
choosing opposites as the predominant class.
586
Mohammad et al Computing Lexical Contrast
Table 18
A summary of the data created as part of this research on lexical contrast. Available for
download at: http://www.purl.org/net/saif.mohammad/research.
Name # of items
Affix patterns that tend to generate opposites 15 rules
Contrast questions:
GRE preparatory questions:
Development set 162 questions
Test set 790 questions
Newly created questions: 1,269 questions
Data from work on types of opposites:
Crowdsourced questionnaires 4 sets (one for every pos)
Responses to questionnaires 12,448 assignments (in four files)
Lexicon of opposites generated by the
Mohammad et al method:
Class I opposites 3.5 million word pairs
Class II opposites 2.5 million word pairs
Manually identified contrasting categories
in the Macquarie Thesaurus 209 category pairs
Word-pairs used in Section 5 experiments:
WordNet opposites set 1,358 word pairs
WordNet random word pairs set 1,358 word pairs
WordNet synonyms set 1,358 word pairs
we have made available. We determined the amount of agreement among humans in
identifying lexical contrast, and also in identifying different kinds of opposites. We also
showed that a large number of opposing word pairs have properties pertaining to more
than one kind. Table 18 summarizes the data created as part of this research on lexical
contrast, all of which is available for download.
New questions that target other types of lexical contrast not addressed in this paper
may be added in the future. It may be desirable to break a complex question into two
or more simpler questions. For example, if a word pair is considered to be a certain
kind of opposite when it has both properties M and N, then it is best to have two
separate questions asking whether the word pair has properties M and N, respectively.
The crowdsourcing study can be replicated for other languages by asking the same
questions in the target language for words in the target language. Note, however, that
as of February 2012, most of the Mechanical Turk participants are native speakers of
English, certain Indian languages, and some European languages.
Our future goals include porting this approach to a cross-lingual framework to
determine lexical contrast in a resource-poor language by using a bilingual lexicon to
connect the words in that language with words in another resource-rich language. We
can then use the structure of the thesaurus from the resource-rich language as described
in this article to detect contrasting categories of terms. This is similar to the approach
described by Mohammad et al (2007), who compute semantic distance in a resource-
poor language by using a bilingual lexicon and a sense disambiguation algorithm to
connect text in the resource-poor language with a thesaurus in a different language. This
enables automatic discovery of lexical contrast in a language even if it does not have a
Roget-like thesaurus. The cross-lingual method still requires a bilingual lexicon to map
words between the target language and the language with the thesaurus, however.
587
Computational Linguistics Volume 39, Number 3
Our method used only one Roget-like published thesaurus, but even more gains
may be obtained by combining many dictionaries and thesauri using methods proposed
by Ploux and Victorri (1998) and others.
We modified our algorithm to create lexicons of words associated with positive
and negative sentiment (Mohammad, Dunne, and Dorr 2009). We also used the lexi-
cal contrast algorithm in some preliminary experiments to identify contrast between
sentences and use that information to improve cohesion in automatic summarization
(Mohammad et al 2008). Since its release, the lexicon of contrasting word pairs was used
to improve textual paraphrasing and in turn help improve machine translation (Marton,
El Kholy, and Habash 2011). We are interested in using contrasting word pairs as seeds
to identify phrases that convey contrasting meaning. These will be especially helpful
in machine translation where current systems have difficulty separating translation
hypotheses that convey the same meaning as the source sentences, and those that
do not.
Given a particular word, our method computes a single score as the degree of
contrast with another. A word may be more or less contrasting with another word when
used in different contexts (Murphy 2003), however. Just as in the lexical substitution task
(McCarthy and Navigli 2009), where a system has to find the word that can best replace
a target word in context to preserve meaning, one can imagine a lexical substitution
task to generate contradictions where the objective is to replace a given target word
with one that is contrasting so as to generate a contradiction. Our future work includes
developing a context-sensitive measure of lexical contrast that can be used for exactly
such a task.
There is considerable evidence that children are aware of lexical contrast at a
very early age (Murphy and Jones 2008). They rely on it to better understand var-
ious concepts and in order to communicate effectively. Thus we believe that com-
puter algorithms that deal with language can also obtain significant gains through the
ability to detect contrast and the ability to distinguish between differing degrees of
contrast.
Acknowledgments
We thank Tara Small, Smaranda Muresan,
and Siddharth Patwardhan for their valuable
feedback. This work was supported in part
by the National Research Council Canada;
in part by the National Science Foundation
under grant no. IIS-0705832; in part by the
Human Language Technology Center of
Excellence; and in part by the Natural
Sciences and Engineering Research Council
of Canada. Any opinions, findings, and
conclusions or recommendations expressed
in this material are those of the authors and
do not necessarily reflect the views of the
sponsors.
References
Agresti, Alan. 1990. Categorical Data Analysis.
Wiley, New York, NY.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure
of semantic relatedness. In International
Joint Conference on Artificial Intelligence,
pages 805?810, Acapulco, Mexico.
Bejar, Isaac I., Roger Chaffin, and Susan
Embretson. 1991. Cognitive and
Psychometric Analysis of Analogical Problem
Solving. Springer-Verlag, New York.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram version 1. Linguistic Data
Consortium, Philadelphia, PA.
Budanitsky, Alexander and Graeme Hirst.
2006. Evaluating WordNet-based
measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition).
Oxford University Computing Services,
Oxford.
Charles, Walter G. and George A. Miller.
1989. Contexts of antonymous adjectives.
Applied Psychology, 10:357?375.
Church, Kenneth and Patrick Hanks.
1990. Word association norms, mutual
588
Mohammad et al Computing Lexical Contrast
information and lexicography.
Computational Linguistics, 16(1):22?29.
Cruse, David A. 1986. Lexical Semantics.
Cambridge University Press, Cambridge.
Curran, James R. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, School of
Informatics, University of Edinburgh,
Edinburgh, UK.
de Marneffe, Marie-Catherine, Anna
Rafferty, and Christopher D. Manning.
2008. Finding contradictions in text. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL-08), pages 1,039?1,047, Columbus, OH.
Deese, James. 1965. The Structure of
Associations in Language and Thought.
The Johns Hopkins University Press,
Baltimore, MD.
Egan, Rose F. 1984. Survey of the history
of English synonymy. Webster?s New
Dictionary of Synonyms, pages 5a?25a.
Fellbaum, Christiane. 1995. Co-occurrence
and antonymy. International Journal of
Lexicography, 8:281?303.
Gross, Derek, Ute Fischer, and George A.
Miller. 1989. Antonymy and the
representation of adjectival meanings.
Memory and Language, 28(1):92?106.
Harabagiu, Sanda M., Andrew Hickl,
and Finley Lacatusu. 2006. Lacatusu:
Negation, contrast and contradiction in
text processing. In Proceedings of the 23rd
National Conference on Artificial Intelligence
(AAAI-06), pages 755?762, Boston, MA.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In Proceedings of
the Eighth Conference on European Chapter of
the Association for Computational Linguistics,
pages 174?181, Madrid.
Hearst, Marti. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 539?546, Nantes.
Jones, Steven, Carita Paradis, M. Lynne
Murphy, and Caroline Willners. 2007.
Googling for ?opposites?: A Web-based
study of antonym canonicity. Corpora,
2(2):129?154.
Justeson, John S. and Slava M. Katz.
1991. Co-occurrences of antonymous
adjectives and their contexts.
Computational Linguistics, 17:1?19.
Kagan, Jerome. 1984. The Nature of the Child.
Basic Books, New York.
Kay, Maire Weir, editor. 1988. Webster?s
Collegiate Thesaurus. Merriam-Webster,
Springfield, MA.
Kempson, Ruth M. 1977. Semantic Theory.
Cambridge University Press, Cambridge.
Lehrer, Adrienne and K. Lehrer. 1982.
Antonymy. Linguistics and Philosophy,
5:483?501.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 768?773,
Montreal.
Lin, Dekang, Shaojun Zhao, Lijuan Qin,
and Ming Zhou. 2003. Identifying
synonyms among distributionally
similar words. In Proceedings of the 18th
International Joint Conference on Artificial
Intelligence (IJCAI-03), pages 1,492?1,493,
Acapulco.
Lobanova, Anna, Tom van der Kleij, and
Jennifer Spenader. 2010. Defining
antonymy: A corpus-based study of
opposites by lexico-syntactic patterns.
International Journal of Lexicography,
23(1):19?53.
Lucerto, Cupertino, David Pinto, and
He?ctor Jime?nez-Salazar. 2002. An
automatic method to identify antonymy.
In Workshop on Lexical Resources and the
Web for Word Sense Disambiguation,
pages 105?111, Puebla.
Lyons, John. 1977. Semantics, volume 1.
Cambridge University Press, Cambridge.
Marcu, Daniel and Abdesammad Echihabi.
2002. An unsupervised approach
to recognizing discourse relations.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL-02), pages 368?375,
Philadelphia, PA.
Marton, Yuval, Ahmed El Kholy, and
Nizar Habash. 2011. Filtering antonymous,
trend-contrasting, and polarity-dissimilar
distributional paraphrases for improving
statistical machine translation. In
Proceedings of the Sixth Workshop on
Statistical Machine Translation,
pages 237?249, Edinburgh.
McCarthy, Diana and Roberto Navigli.
2009. The English lexical substitution
task. Language Resources And Evaluation,
43(2):139?159.
Mihalcea, Rada and Carlo Strapparava.
2005. Making computers laugh:
Investigations in automatic humor
recognition. In Proceedings of the
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing, pages 531?538, Vancouver.
Mohammad, Saif, Bonnie Dorr, and
Graeme Hirst. 2008. Computing word-pair
589
Computational Linguistics Volume 39, Number 3
antonymy. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP-2008), pages 982?991,
Waikiki, HI.
Mohammad, Saif, Bonnie J. Dorr,
Melissa Egan, Nitin Madnani, David Zajic,
and Jimmy Lin. 2008. Multiple alternative
sentence compressions and word-pair
antonymy for automatic text
summarization and recognizing textual
entailment. In Text Analysis Conference
(TAC), Gaithersburg, MD.
Mohammad, Saif, Cody Dunne, and
Bonnie Dorr. 2009. Generating
high-coverage semantic orientation
lexicons from overtly marked words
and a thesaurus. In Proceedings of
Empirical Methods in Natural Language
Processing (EMNLP-2009), pages 599?608,
Singapore.
Mohammad, Saif, Iryna Gurevych,
Graeme Hirst, and Torsten Zesch. 2007.
Cross-lingual distributional profiles of
concepts for measuring semantic distance.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL-2007),
pages 571?580, Prague.
Murphy, Gregory L. and Jane M. Andrew.
1993. The conceptual basis of antonymy
and synonymy in adjectives. Journal of
Memory and Language, 32(3):1?19.
Murphy, Lynne M. 2003. Semantic Relations
and the Lexicon: Antonymy, Synonymy, and
Other Paradigms. Cambridge University
Press, Cambridge.
Murphy, Lynne M. and Steven Jones.
2008. Antonyms in children?s and
child-directed speech. First Language,
28(4):403?430.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?:
Sentiment classification using machine
learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 79?86,
Philadelphia, PA.
Paradis, Carita, Caroline Willners,
and Steven Jones. 2009. Good and
bad opposites using textual and
experimental techniques to measure
antonym canonicity. The Mental Lexicon,
4(3):380?429.
Ploux, Sabine and Bernard Victorri. 1998.
Construction d?espaces se?mantiques a`
l?aide de dictionnaires de synonymes.
TAL, 39(1):161?182.
Schwab, Didier, Mathieu Lafourcade, and
Violaine Prince. 2002. Antonymy and
conceptual vectors. In Proceedings of
the 19th International Conference on
Computational Linguistics (COLING-02),
pages 904?910, Taipei, Taiwan.
Turney, Peter D. 2008. A uniform approach
to analogies, synonyms, antonyms, and
associations. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING-08), pages 905?912,
Manchester.
Turney, Peter D. 2011. Analogy perception
applied to seven tests of word
comprehension. Journal of Experimental
and Theoretical Artificial Intelligence -
Psychometric Artificial Intelligence,
23(3):343?362.
Voorhees, Ellen M. 2008. Contradictions and
justifications: Extensions to the textual
entailment task. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics (ACL-08), 63?71,
Columbus, OH.
590
Proceedings of NAACL-HLT 2013, pages 673?679,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Multi-Dimensional Bayesian Approach to Lexical Style
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We adapt the popular LDA topic model (Blei
et al, 2003) to the representation of stylistic
lexical information, evaluating our model on
the basis of human-interpretability at the word
and text level. We show, in particular, that this
model can be applied to the task of inducing
stylistic lexicons, and that a multi-dimensional
approach is warranted given the correlations
among stylistic dimensions.
1 Introduction
In language, stylistic variation is a reflection of var-
ious contextual factors, including the backgrounds
of and relationship between the parties involved.
Although in the context of prescriptive linguistics
(Strunk and White, 1979), style is often assumed to
be a matter of aesthetics, the stylistic intuitions of
language users are inextricably linked to the conven-
tions of register and genre (Biber and Conrad, 2009).
Intentional or not, stylistic differences play a role
in numerous NLP tasks. Examples include genre
classification (Kessler et al, 1997), author profil-
ing (Garera and Yarowsky, 2009; Rosenthal and Mc-
Keown, 2011), social relationship classification (Pe-
terson et al, 2011), sentiment analysis (Wilson et al,
2005), readability classification (Collins-Thompson
and Callan, 2005), and text generation (Hovy, 1990;
Inkpen and Hirst, 2006). Following the classic work
of Biber (1988), computational modeling of style
has often focused on textual statistics and the fre-
quency of function words and syntactic categories.
When content words are considered, they are of-
ten limited to manually-constructed lists (Argamon
et al, 2007), or used as individual features for su-
pervised classification, which can be confounded by
topic (Petrenz and Webber, 2011) or fail in the face
of lexical variety. Our interest is models that offer
broad lexical coverage of human-identifiable stylis-
tic variation.
Research most similar to ours has focused on clas-
sifying the lexicon in terms of individual aspects rel-
evant to style (e.g. formality, specificity, readability)
(Brooke et al, 2010; Pan and Yang, 2010; Kidwell
et al, 2009) and a large body of research on the in-
duction of polarity lexicons, in particular from large
corpora (Turney, 2002; Kaji and Kitsuregawa, 2007;
Velikovich et al, 2010). Our work is the first to rep-
resent multiple dimensions of style in a single statis-
tical model, adapting latent Dirichlet alocation (Blei
et al, 2003), a Bayesian ?topic? model, to our stylis-
tic purposes; as such, our approach also follows on
recent interest in the interpretability of topic-model
topics (Chang et al, 2009; Newman et al, 2011).
We show that our model can be used for acquisition
of stylistic lexicons, and we also evaluate the model
relative to theories of register variation and the ex-
pected stylistic character of particular genres.
2 Model
2.1 Linguistic foundations
In English manuals of style and other prescriptivist
texts (Fowler and Fowler, 1906; Gunning, 1952;
Follett, 1966; Strunk and White, 1979; Kane, 1983;
Hayakawa, 1994), writers are urged to pay atten-
tion to various aspects of lexical style, including el-
ements such as clarity, familiarity, readability, for-
673
mality, fanciness, colloquialness, specificity, con-
creteness, objectivity, and naturalness; these stylis-
tic categories reflect common aesthetic judgments
about language. In descriptive studies of register,
some researchers have posited a few fixed styles
(Joos, 1961) or a small, discrete set of situational
constraints which determine style and register (Crys-
tal and Davy, 1969; Halliday and Hasan, 1976); by
contrast, the applied approach of Biber (1988) and
theoretical framework of Leckie-Tarry (1995) offer a
more continuous interpretation of register variation.
In Biber?s approach, functional dimensions such
as Involved vs. Informational, Argumentative vs.
Non-argumentative, and Abstract vs. non-Abstract
are derived in an unsupervised manner from a
mixed-genre corpus, with the labels assigned de-
pending on where features (a small set of known in-
dicators of register) and genres fall on each spec-
trum. The theory of Leckie-Tarry posits a single
main cline of register with one pole (the oral pole)
reflecting a full reliance on the context of the lin-
guistic situation, and the other (the literate pole) re-
flecting a reliance on cultural knowledge. The more
specific elements of register are represented as sub-
clines which are strongly influenced by this main
cline, creating probabilistic relationships between
related dimensions (Birch, 1995).
For the present study, we have chosen 3 dimen-
sions (6 styles) which are clearly represented in the
lexicon, which are discussed often in the relevant lit-
erature, and which fit well into the Leckie-Tarry con-
ception of related subclines: colloquial vs. literary,
concrete vs. abstract, and subjective vs. objective. In
addition to a negative correlation between opposing
styles, we also expect a positive correlation between
stylistic aspects that tend toward the same main pole,
situational (i.e. colloquial, concrete, subjective) or
cultural (i.e. literary, abstract, objective). These cor-
relations can potentially interfere with accurate lex-
ical acquisition.
2.2 Implementation
Our main model is an adaption of the popular latent
Dirichlet alocation topic model (Blei et al, 2003),
with each of the 6 styles corresponding to a topic.
Briefly, latent Dirichlet alocation (LDA) is a gener-
ative Bayesian model: for each document d, a dis-
tribution of topics ?d is drawn from a Dirichlet prior
(with parameter ?). For each topic z, there is a prob-
ability distribution ?z1 corresponding to the proba-
bility of that topic generating any given word in the
vocabulary. Words in document d are generated by
first selecting a topic z randomly according to ?d ,
and then randomly selecting a word w according to
?z. An extension of LDA, the correlated topic model
(CTM) (Blei and Lafferty, 2007), supposes a more
complex representation of topics: given a matrix ?
representing the covariance between topics and ?
representing the means, for each document a topic
distribution ? (analogous to ? ) is drawn from the
logistic normal distribution. Given a corpus, good
estimates for the relevant parameters can be derived
using Bayesian inference.
For both LDA and CTM we use the original
variational Bayes implementation of Blei. Varia-
tional Bayes (VB) works by approximating the true
posterior with a simpler distribution, minimizing
the Kullback-Leibler divergence between the two
through iterative updates of specially-introduced
free variables. The mathematical and algorithmic
details are omitted here; see Blei et al (2003; 2007).
Our early investigations used an online, batch ver-
sion of LDA (Hoffman et al, 2010), which is more
appropriate for large corpora because it requires
only a single iteration over the dataset. We discov-
ered, however, that batch models were markedly in-
ferior to more traditional models for our purposes
because the influence of the initial model diminishes
too quickly; here, we need particular topics in the
model to correspond to particular styles, and we ac-
complish this by seeding the model with known in-
stances of each style (see Section 3). Specifically,
our initial ? consists of distributions where the entire
probability mass is divided amongst the seeds for
each corresponding topic, and a full iteration over
the corpus occurs before ? is updated. Typically,
LDA iterates over the corpus until a convergence re-
quirement is met, but in this case this is neither prac-
tical (due to the size of our corpus) nor necessarily
desirable; the diminishing effects of the initial seed-
ing means that the model may not stabilize, in terms
of its likelihood, until after it has shifted away from
our desired stylistic dimensions towards some other
1Some versions of LDA smooth this distribution using a
Dirichlet prior; here, though, we use the original formulation
from Blei (2003), which does not.
674
variation in the data. Therefore, we treat the optimal
number of iterations as a variable to investigate.
The model is trained on a 1 million text por-
tion of the 2009 version of the ICWSM Spinn3r
dataset (Burton et al, 2009), a corpus of blogs we
have previously used for formality lexicon induction
(Brooke et al, 2010). Since our method relies on co-
occurrence, we followed our earlier work in using
only texts with at least 100 different word types. All
words were tokenized and converted to lower-case,
with no further lemmatization. Following Hoffman
et al (2010), we initialized the ? of our models to
1/k where k is the number of topics. Otherwise we
used the default settings; when they overlap they
were identical for the LDA and CTM models.
3 Lexicon Induction
Our primary evaluation is based on the stylistic in-
duction of held-out seed words. The words were
collected from various sources by the first author
and further reviewed by the second; we are both
native speakers of English with significant experi-
ence in English linguistics. Included words had to
be clear, extreme members of their stylistic cate-
gory, with little or no ambiguity with respect to their
style. The colloquial seeds consist of English slang
terms and acronyms, e.g. cuz, gig, asshole, lol. The
literary seeds were primarily drawn from web sites
which explain difficult language in texts such as the
Bible and Lord of the Rings; examples include be-
hold, resplendent, amiss, and thine. The concrete
seeds all denote objects and actions strongly rooted
in the physical world, e.g. shove and lamppost, while
the abstract seeds all involve concepts which require
significant human psychological or cultural knowl-
edge to grasp, for instance patriotism and noncha-
lant. For our subjective seeds, we used an edited
list of strongly positive and negative terms from a
manually-constructed sentiment lexicon (Taboada et
al., 2011), e.g. gorgeous and depraved, and for our
objective set we selected words from sets of near-
synonyms where one was clearly an emotionally-
distant alternative, e.g. residence (for home), jocu-
lar (for funny) and communicable (for contagious).
We filtered initial lists to 150 of each type, remov-
ing words which did not appear in the corpus or
which occurred in multiple lists. For evaluation we
used stratified 3-fold crossvalidation, averaged over
5 different (3-way) splits of the seeds, with the same
splits used for all evaluated conditions.
Given two sets of opposing seeds, we follow our
earlier work in evaluating our performance in terms
of the number of pairings of seeds from each set
which have the expected stylistic relationship rel-
ative to each other (the guessing baseline is 0.5).
Given a word w and two opposing styles (topics) p
and n, we place w on the PN dimension according to
the ? of our trained model as follows:
PNw =
?pw??nw
?pw +?nw
The normalization is important because otherwise
more-common words would tend to have higher
PN?s, when in fact the opposite is true (rare words
tend to be more stylistically prominent). We then
calculate pairwise accuracy as the percentage of
pairs ?wp,wn? (wp ? Pseeds and wn ? Nseeds) where
PNwp >PNwn . However, this metric does not address
the case where the degree of a word in one stylistic
dimension is overestimated because of its status on
a parallel dimension. Two more-holistic alternatives
are total accuracy, the percentage of seeds for which
the highest ?tw is the topic t for which w is a seed
(guessing baseline is 0.17), and the average rank of
the correct t as ordered by ?tw (in the range 1?6,
guessing baseline is 3.5); the latter is more forgiving
of near misses.
We tested a few options which involved straight-
forward modifications to model training. Standard
LDA produces all tokens in the document, but when
dealing with style rather than topic, the number of
times a word appears is much less relevant (Brooke
et al, 2010). Our binary model assumes an LDA
that generates types, not tokens.2 A key comparison
2At the theoretical level, this move is admittedly problem-
atic, since our LDA model is thus being trained under the as-
sumption that texts with multiple instances of the same type can
be generated, when of course such texts cannot by definition ex-
ist. We might address this by moving to Bayesian models with
very different generative assumptions, e.g. the spherical topic
model (Reisinger et al, 2010), but these methods involve a sig-
nificant increase of computational complexity and we believe
that on a practical level there are no real negatives associated
with directly using a binary representation as input to LDA; in
fact, we are avoiding what appears to be a much more serious
problem, burstiness (Doyle and Elkan, 2009), i.e. the fact that
675
Model
Pairwise Accuracy (%)
Total Acc. (%) Avg. Rank
Lit/Col Abs/Con Obj/Sub All
guessing baseline 50.0 50.0 50.0 50.0 16.6. 3.50
basic LDA (iter 2) 94.3 98.8 93.0 95.4 55.0 1.79
binary LDA (iter 2) 96.2 98.9 93.5 96.2 57.7 1.74
combo binary LDA (iter 1) 95.4 99.2 93.3 96.0 53.1 1.86
binary CTM (iter 1) 96.3 99.0 89.6 95.0 53.0 1.87
Table 1: Model performance in lexical induction of seeds. Bold indicates best in column.
here is with a combined LDA model (combo), an
amalgamation of three independently trained 2-topic
models, one for each dimension; this tests our key
hypothesis that training dimensions of style together
is beneficial. Finally, we test against the correlated
topic model (CTM), which offers an explicit repre-
sentation of style correlation, but which has done
poorly with respect to interpretability, despite offer-
ing better perplexity (Chang et al, 2009).
The results of the lexicon induction evaluation
are in Table 1. Since the number of optimal iter-
ations varies, we report the result from the best of
the first five iterations, as measured by total accu-
racy; the best iteration is shown in parenthesis. In
general, all the results are high enough?we are re-
liably above 90% for the pairwise task, and above
50% for the 6-way task?for us to conclude with
some confidence that our model is capturing a sig-
nificant amount of stylistic variation. As predicted,
using words as boolean features had a net positive
gain, consistent across all of our metrics, though this
effect was not as marked as we have seen previously.
The model with independent training of each dimen-
sion (combo) did noticeably worse, supporting our
conclusion that a multidimensional approach is war-
ranted here. Particularly striking is the much larger
drop in overall accuracy as compared to pairwise ac-
curacy, which suggests that the combo model is cap-
turing the general trends but not distinguishing cor-
related styles as well. However, the most complex
model, the CTM, actually does slightly worse than
the combo, which was contrary to our expectations
but nonetheless consistent with previous work on the
interpretability of topic models. The performance of
the full LDA models benefited from a second itera-
traditional LDA is influenced too much by multiple instances of
the same word.
tion, but this was not true of combo LDA or CTM,
and the performance of all models dropped after the
second iteration.
An analysis of individual errors reveals, unsur-
prisingly, that most of the errors occur across styles
on the same pole; by far the largest single com-
mon misclassification is objective words to abstract.
Of the words that consistently show this misclas-
sification across the runs, many of them, e.g. ani-
mate, aperture, encircle, and constrain are clearly
errors (if anything, these words tend towards con-
creteness), but in other cases the word in question
is arguably also fairly abstract, e.g. categorize and
predominant, and might not be labeled an error at
all. Other signs that our model might be doing bet-
ter than our total accuracy metric gives it credit for:
many of the subjective words that are consistently
mislabeled as literary have an exaggerated, literary
feel, e.g. jubilant, grievous, and malevolent.
4 Text-level Analysis
Our secondary analysis involved evaluating the ? ?s
of our best configuration (based on average pairwise
and total accuracy) on other texts. After training,
we carried out inference on the BNC corpus, aver-
aging the resulting ? ?s to see which styles are asso-
ciated with which genres. Appearances of the seed
terms for each model were disregarded during this
process; only the induced part of the lexicon was
used. The average differences relative to the mean
across the various stylistic dimensions (as measured
by the probabilities in ? ) are given for a selection of
genres in Table 2.
The most obvious pattern in table 2 is the domi-
nance of the medium: all written genres are positive
for our styles on the ?cultural? pole and negative for
styles on the ?situational? pole and the opposite is
676
Genre
Styles
Literary Abstract Objective Colloquial Concrete Subjective
News +0.67 +0.50 +0.43 ?0.31 ?0.72 ?0.57
Religious texts +0.38 +0.38 +0.28 ?0.27 ?0.44 ?0.32
Academic +0.18 +0.29 +0.26 ?0.20 ?0.36 ?0.18
Fiction +0.31 +0.09 +0.02 ?0.05 ?0.12 ?0.25
Meeting ?0.61 ?0.54 ?0.42 +0.35 +0.69 +0.55
Courtroom ?0.63 ?0.53 ?0.41 +0.32 +0.69 +0.57
Conversation ?0.56 ?0.63 ?0.54 +0.43 +0.80 +0.50
Table 2: Average differences from corpus mean of LDA-derived stylistic dimension probabilities for various genres in
the BNC, in hundredths.
true for spoken genres. The magnitude of this ef-
fect is more difficult to interpret: though it is clear
why fiction should sit on the boundary (since it con-
tains spoken dialogue), the appearance of news at
the written extreme is odd, though it might be due to
the fact that news blogs are the most prevalent for-
mal genre in the training corpus.
However, if we ignore magnitude and focus on the
relative ratios of the stylistic differences for styles
on the same pole, we can identify some individ-
ual stylistic effects among genres within the same
medium. Relative to the other written genres, for in-
stance, fiction is, sensibly, more literary and much
less objective, while academic texts are much more
abstract and objective; for the other two written gen-
res, the spread is more even, though relative to re-
ligious texts, news is more objective. At the sit-
uational pole, fiction also stands out, being much
more colloquial and concrete than other written gen-
res. Predictably, if we consider again the ratios
across styles, conversation is the most colloquial
genre here, though the difference is subtle.
We carried out a correlation analysis of the LDA-
reduced styles of all texts in the BNC and, con-
sistent with the genre results in Table 2, found a
strong positive correlation for all styles on the same
main pole, averaging 0.83. The average negative
correlation between opposing poles is even higher,
?0.88. This supports the Leckie-Tarry formulation.
The independence assumptions of the LDA model
did not prevent strong correlations from forming be-
tween these distinct yet clearly interrelated dimen-
sions; if anything, the correlations are stronger than
we would have predicted.
5 Conclusion
We have introduced a Bayesian model of stylistic
variation. Topic models like LDA are often evalu-
ated using information-theoretic measures, but our
emphasis has been on interpretibility: at the word
level we can use the model to induce stylistic lex-
icons which correspond to human judgement, and
at the text level we can use it distinguish genres in
expected ways. Another theme has been to offer ev-
idence that indeed a multi-dimensional approach is
strongly warranted: importantly, our results indicate
that separate unidimensional models of style are in-
ferior for identifying the core stylistic character of
each word, and in our secondary analysis we found
strong correlations among styles attributable to the
situational/cultural dichotomy. However, an off-the-
shelf model that integrates correlation among topics
did not outperform basic LDA.
One advantage of a Bayesian approach is in the
flexibility of the model: there are any number of
other interesting possible extensions at both the ?
and ? levels of the model, including alternative ap-
proaches to correlation (Li and McCallum, 2006).
Beyond Bayesian models, vector space and graphi-
cal approaches should be compared. More work is
clearly needed to improve evaluation: some of our
seeds could fall into multiple stylistic categories, so
a more detailed annotation would be useful.
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
677
References
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91?109.
Douglas Biber and Susan Conrad. 2009. Register, Genre,
and Style. Cambridge University Press.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David Birch. 1995. Introduction. In Helen Leckie-Tarry,
editor, Language and Context: A Functional Linguis-
tic Theory of Register. Pinter.
David M. Blei and John D. Lafferty. 2007. Correlated
topic models. Annals of Applied Statistics, 1(1):17?
35.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent Dirichlet alocation.
Journal of Machine Learning Research, 3:993?1022.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of Neural Information Processing Systems
(NIPS ?09).
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science Technology, 56(13):1448?1462.
David Crystal and Derek Davy. 1969. Investigating En-
glish Style. Indiana University Press.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In International Confer-
ence on Machine Learning (ICML ?09).
Wilson Follett. 1966. Modern American Usage. Hill &
Wang, New York.
H. W. Fowler and F. G. Fowler. 1906. The King?s En-
glish. Clarendon Press, Oxford, 2nd edition.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ?09), pages 710?718, Sin-
gapore.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill, New York.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman, London.
S.I. Hayakawa, editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised by
Eugene Ehrlich.
Matthew D. Hoffman, David M. Blei, and Francis R.
Bach. 2010. Online learning for latent Dirichlet al
location. In Neural Information Processing Systems
(NIPS ?10), pages 856?864.
Eduard H. Hovy. 1990. Pragmatics and natural language
generation. Artificial Intelligence, 43:153?197.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223?262.
Martin Joos. 1961. The Five Clocks. Harcourt, Brace
and World, New York.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of HTML documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL ?07).
Thomas S. Kane. 1983. The Oxford Guide to Writing.
Oxford Univeristy Press.
Brett Kessler, Geoffrey Nunberg, and Hinrich Schu?tze.
1997. Automatic detection of text genre. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL ?97), pages 32?
38, Madrid, Spain.
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word
acquisition with application to readability predic-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), pages 900?909, Singapore.
Helen Leckie-Tarry. 1995. Language and Context: A
Functional Linguistic Theory of Register. Pinter.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In Proceedings of the 23rd International Con-
ference on Machine Learning, ICML ?06, pages 577?
584.
David Newman, Edwin V. Bonilla, and Wray Buntine.
2011. Improving topic coherence with regularized
topic models. In Proceedings of Advances in Neural
Information Processing Systems (NIPS ?11).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10).
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
678
the Enron corpus. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?11), Portland, Oregon.
Philipp Petrenz and Bonnie Webber. 2011. Stable clas-
sification of text genres. Computational Linguistics,
37(2):385?393, June.
J. Reisinger, A. Waters, B. Silverthorn, and R. Mooney.
2010. Spherical topic models. In International Con-
ference on Machine Learning (ICML ?10).
Sara Rosenthal and Kathleen McKeown. 2011. Age pre-
diction in blogs: A study of style, content, and online
behavior in pre- and post-social media generations. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon.
William Strunk and E.B. White. 1979. The Elements of
Style. Macmillan, 3rd edition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?02, pages 417?424, Philadelphia, Pennsyl-
vania.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 777?785, Los An-
geles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
?05, pages 347?354.
679
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987?996,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Classifying Arguments by Scheme
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
Argumentation schemes are structures or tem-
plates for various kinds of arguments. Given
the text of an argument with premises and con-
clusion identified, we classify it as an instance
of one of five common schemes, using features
specific to each scheme. We achieve accura-
cies of 63?91% in one-against-others classifi-
cation and 80?94% in pairwise classification
(baseline = 50% in both cases).
1 Introduction
We investigate a new task in the computational anal-
ysis of arguments: the classification of arguments
by the argumentation schemes that they use. An ar-
gumentation scheme, informally, is a framework or
structure for a (possibly defeasible) argument; we
will give a more-formal definition and examples in
Section 3. Our work is motivated by the need to de-
termine the unstated (or implicitly stated) premises
that arguments written in natural language normally
draw on. Such premises are called enthymemes.
For instance, the argument in Example 1 consists
of one explicit premise (the first sentence) and a con-
clusion (the second sentence):
Example 1 [Premise:] The survival of the entire
world is at stake.
[Conclusion:] The treaties and covenants aiming
for a world free of nuclear arsenals and other con-
ventional and biological weapons of mass destruc-
tion should be adhered to scrupulously by all na-
tions.
Another premise is left implicit ? ?Adhering to
those treaties and covenants is a means of realizing
survival of the entire world?. This proposition is an
enthymeme of this argument.
Our ultimate goal is to reconstruct the en-
thymemes in an argument, because determining
these unstated assumptions is an integral part of un-
derstanding, supporting, or attacking an entire argu-
ment. Hence reconstructing enthymemes is an im-
portant problem in argument understanding. We be-
lieve that first identifying the particular argumenta-
tion scheme that an argument is using will help to
bridge the gap between stated and unstated proposi-
tions in the argument, because each argumentation
scheme is a relatively fixed ?template? for arguing.
That is, given an argument, we first classify its ar-
gumentation scheme; then we fit the stated proposi-
tions into the corresponding template; and from this
we infer the enthymemes.
In this paper, we present an argument scheme
classification system as a stage following argument
detection and proposition classification. First in Sec-
tion 2 and Section 3, we introduce the background
to our work, including related work in this field,
the two core concepts of argumentation schemes and
scheme-sets, and the Araucaria dataset. In Section 4
and Section 5 we present our classification system,
including the overall framework, data preprocessing,
feature selection, and the experimental setups. In
the remaining section, we present the essential ap-
proaches to solve the leftover problems of this paper
which we will study in our future work, and discuss
the experimental results, and potential directions for
future work.
987
2 Related work
Argumentation has not received a great deal of at-
tention in computational linguistics, although it has
been a topic of interest for many years. Cohen
(1987) presented a computational model of argu-
mentative discourse. Dick (1987; 1991a; 1991b) de-
veloped a representation for retrieval of judicial de-
cisions by the structure of their legal argument ? a
necessity for finding legal precedents independent of
their domain. However, at that time no corpus of ar-
guments was available, so Dick?s system was purely
theoretical. Recently, the Araucaria project at Uni-
versity of Dundee has developed a software tool for
manual argument analysis, with a point-and-click in-
terface for users to reconstruct and diagram an ar-
gument (Reed and Rowe, 2004; Rowe and Reed,
2008). The project also maintains an online repos-
itory, called AraucariaDB, of marked-up naturally
occurring arguments collected by annotators world-
wide, which can be used as an experimental corpus
for automatic argumentation analysis (for details see
Section 3.2).
Recent work on argument interpretation includes
that of George, Zukerman, and Nieman (2007), who
interpret constructed-example arguments (not natu-
rally occurring text) as Bayesian networks. Other
contemporary research has looked at the automatic
detection of arguments in text and the classification
of premises and conclusions. The work closest to
ours is perhaps that of Mochales and Moens (2007;
2008; 2009a; 2009b). In their early work, they fo-
cused on automatic detection of arguments in legal
texts. With each sentence represented as a vector of
shallow features, they trained a multinomial na??ve
Bayes classifier and a maximum entropy model on
the Araucaria corpus, and obtained a best average
accuracy of 73.75%. In their follow-up work, they
trained a support vector machine to further classify
each argumentative clause into a premise or a con-
clusion, with an F1 measure of 68.12% and 74.07%
respectively. In addition, their context-free grammar
for argumentation structure parsing obtained around
60% accuracy.
Our work is ?downstream? from that of Mochales
and Moens. Assuming the eventual success of their,
or others?, research program on detecting and clas-
sifying the components of an argument, we seek to
determine how the pieces fit together as an instance
of an argumentation scheme.
3 Argumentation schemes, scheme-sets,
and annotation
3.1 Definition and examples
Argumentation schemes are structures or templates
for forms of arguments. The arguments need not be
deductive or inductive; on the contrary, most argu-
mentation schemes are for presumptive or defeasible
arguments (Walton and Reed, 2002). For example,
argument from cause to effect is a commonly used
scheme in everyday arguments. A list of such argu-
mentation schemes is called a scheme-set.
It has been shown that argumentation schemes
are useful in evaluating common arguments as falla-
cious or not (van Eemeren and Grootendorst, 1992).
In order to judge the weakness of an argument, a set
of critical questions are asked according to the par-
ticular scheme that the argument is using, and the
argument is regarded as valid if it matches all the
requirements imposed by the scheme.
Walton?s set of 65 argumentation schemes (Wal-
ton et al, 2008) is one of the best-developed scheme-
sets in argumentation theory. The five schemes de-
fined in Table 1 are the most commonly used ones,
and they are the focus of the scheme classification
system that we will describe in this paper.
3.2 Araucaria dataset
One of the challenges for automatic argumentation
analysis is that suitable annotated corpora are still
very rare, in spite of work by many researchers.
In the work described here, we use the Araucaria
database1, an online repository of arguments, as our
experimental dataset. Araucaria includes approxi-
mately 660 manually annotated arguments from var-
ious sources, such as newspapers and court cases,
and keeps growing. Although Araucaria has sev-
eral limitations, such as rather small size and low
agreement among annotators2, it is nonetheless one
of the best argumentative corpora available to date.
1http://araucaria.computing.dundee.ac.uk/doku.php#
araucaria argumentation corpus
2The developers of Araucaria did not report on inter-
annotator agreement, probably because some arguments are an-
notated by only one commentator.
988
Argument from example
Premise: In this particular case, the individual a
has property F and also property G.
Conclusion: Therefore, generally, if x has prop-
erty F, then it also has property G.
Argument from cause to effect
Major premise: Generally, if A occurs, then B will
(might) occur.
Minor premise: In this case, A occurs (might oc-
cur).
Conclusion: Therefore, in this case, B will
(might) occur.
Practical reasoning
Major premise: I have a goal G.
Minor premise: Carrying out action A is a means
to realize G.
Conclusion: Therefore, I ought (practically
speaking) to carry out this action A.
Argument from consequences
Premise: If A is (is not) brought about, good (bad)
consequences will (will not) plausibly occur.
Conclusion: Therefore, A should (should not) be
brought about.
Argument from verbal classification
Individual premise: a has a particular property F.
Classification premise: For all x, if x has property
F, then x can be classified as having property
G.
Conclusion: Therefore, a has property G.
Table 1: The five most frequent schemes and their defini-
tions in Walton?s scheme-set.
Arguments in Araucaria are annotated in a XML-
based format called ?AML? (Argument Markup
Language). A typical argument (see Example 2)
consists of several AU nodes. Each AU node is a
complete argument unit, composed of a conclusion
proposition followed by optional premise proposi-
tion(s) in a linked or convergent structure. Each of
these propositions can be further defined as a hier-
archical collection of smaller AUs. INSCHEME is
the particular scheme (e.g., ?Argument from Con-
sequences?) of which the current proposition is a
member; enthymemes that have been made explicit
are annotated as ?missing = yes?.
Example 2 Example of argument markup from
Araucaria
<TEXT>If we stop the free creation of art, we will stop
the free viewing of art.</TEXT>
<AU>
<PROP identifier="C" missing="yes">
<PROPTEXT offset="-1">
The prohibition of the free creation of art should
not be brought about.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
<LA>
<AU>
<PROP identifier="A" missing="no">
<PROPTEXT offset="0">
If we stop the free creation of art, we will
stop the free viewing of art.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
</AU>
<AU>
<PROP identifier="B" missing="yes">
<PROPTEXT offset="-1">
The prohibition of free viewing of art is not
acceptable.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
</AU>
</LA>
</AU>
There are three scheme-sets used in the anno-
tations in Araucaria: Walton?s scheme-set, Katzav
and Reed?s (2004) scheme-set, and Pollock?s (1995)
scheme-set. Each of these has a different set of
schemes; and most arguments in Araucaria are
marked up according to only one of them. Our
experimental dataset is composed of only those
arguments annotated in accordance with Walton?s
scheme-set, within which the five schemes shown in
Table 1 constitute 61% of the total occurrences.
4 Methods
4.1 Overall framework
As we noted above, our ultimate goal is to recon-
struct enthymemes, the unstated premises, in an ar-
gument by taking advantage of the stated proposi-
tions; and in order to achieve this goal we need to
first determine the particular argumentation scheme
that the argument is using. This problem is de-
picted in Figure 1. Our scheme classifier is the
dashed round-cornered rectangle portion of this
989
Detecting 
argumentative text
ARGUMENTATIVE 
SEGMENT
Premise / 
conclusion 
classifier
CONCLUSION
PREMISE #1
PREMISE #2
Scheme classifier
TEXT
ARGUMENTATION 
SCHEME
Argument 
template fitter
CONSTRUCTED 
ENTHYMEME
Figure 1: Overall framework of this research.
overall framework: its input is the extracted con-
clusion and premise(s) determined by an argument
detector, followed by a premise / conclusion classi-
fier, given an unknown text as the input to the entire
system. And the portion below the dashed round-
rectangle represents our long-term goal ? to recon-
struct the implicit premise(s) in an argument, given
its argumentation scheme and its explicit conclusion
and premise(s) as input. Since argument detection
and classification are not the topic of this paper, we
assume here that the input conclusion and premise(s)
have already been retrieved, segmented, and classi-
fied, as for example by the methods of Mochales and
Moens (see Section 2 above). And the scheme tem-
plate fitter is the topic of our on-going work.
4.2 Data preprocessing
From all arguments in Araucaria, we first ex-
tract those annotated in accordance with Walton?s
scheme-set. Then we break each complex AU
node into several simple AUs where no conclusion
or premise proposition nodes have embedded AU
nodes. From these generated simple arguments, we
extract those whose scheme falls into one of the five
most frequent schemes as described in Table 1. Fur-
thermore, we remove all enthymemes that have been
inserted by the annotator and ignore any argument
with a missing conclusion, since the input to our pro-
posed classifier, as depicted in Figure 1, cannot have
any access to unstated argumentative propositions.
The resulting preprocessed dataset is composed of
393 arguments, of which 149, 106, 53, 44, and 41
respectively belong to the five schemes in the order
shown in Table 1.
4.3 Feature selection
The features used in our work fall into two cat-
egories: general features and scheme-specific fea-
tures.
4.3.1 General features
General features are applicable to arguments belong-
ing to any of the five schemes (shown in Table 2).
For the features conLoc, premLoc, gap, and
lenRat, we have two versions, differing in terms
of their basic measurement unit: sentence-based
and token-based. The final feature, type, indicates
whether the premises contribute to the conclusion
in a linked or convergent order. A linked argument
(LA) is one that has two or more inter-dependent
premise propositions, all of which are necessary to
make the conclusion valid, whereas in a conver-
gent argument (CA) exactly one premise proposi-
tion is sufficient to do so. Since it is observed that
there exists a strong correlation between type and
the particular scheme employed while arguing, we
believe type can be a good indicator of argumenta-
tion scheme. However, although this feature is avail-
able to us because it is included in the Araucaria an-
notations, its value cannot be obtained from raw text
as easily as other features mentioned above; but it is
possible that we will in the future be able to deter-
mine it automatically by taking advantage of some
scheme-independent cues such as the discourse re-
lation between the conclusion and the premises.
4.3.2 Scheme-specific features
Scheme-specific features are different for each
scheme, since each scheme has its own cue phrases
or patterns. The features for each scheme are shown
in Table 3 (for complete lists of features see Feng
(2010)). In our experiments in Section 5 below, all
these features are computed for all arguments; but
990
conLoc: the location (in token or sentence) of the
conclusion in the text.
premLoc: the location (in token or sentence) of
the first premise proposition.
conFirst: whether the conclusion appears before
the first premise proposition.
gap: the interval (in token or sentence) between
the conclusion and the first premise proposi-
tion.
lenRat: the ratio of the length (in token or sen-
tence) of the premise(s) to that of the conclu-
sion.
numPrem: the number of explicit premise propo-
sitions (PROP nodes) in the argument.
type: type of argumentation structure, i.e., linked
or convergent.
Table 2: List of general features.
the features for any particular scheme are used only
when it is the subject of a particular task. For ex-
ample, when we classify argument from example
in a one-against-others setup, we use the scheme-
specific features of that scheme for all arguments;
when we classify argument from example against
argument from cause to effect, we use the scheme-
specific features of those two schemes.
For the first three schemes (argument from ex-
ample, argument from cause to effect, and practi-
cal reasoning), the scheme-specific features are se-
lected cue phrases or patterns that are believed to be
indicative of each scheme. Since these cue phrases
and patterns have differing qualities in terms of their
precision and recall, we do not treat them all equally.
For each cue phrase or pattern, we compute ?confi-
dence?, the degree of belief that the argument of in-
terest belongs to a particular scheme, using the dis-
tribution characteristics of the cue phrase or pattern
in the corpus, as described below.
For each argument A, a vector CV = {c1, c2, c3}
is added to its feature set, where each ci indicates
the ?confidence? of the existence of the specific fea-
tures associated with each of the first three schemes,
schemei. This is defined in Equation 1:
ci =
1
N
mi?
k=1
(P (schemei|cpk) ? dik) (1)
Argument from example
8 keywords and phrases including for example,
such as, for instance, etc.; 3 punctuation cues: ?:?,
?;?, and ???.
Argument from cause to effect
22 keywords and simple cue phrases including re-
sult, related to, lead to, etc.; 10 causal and non-
causal relation patterns extracted from WordNet
(Girju, 2003).
Practical reasoning
28 keywords and phrases including want, aim, ob-
jective, etc.; 4 modal verbs: should, could, must,
and need; 4 patterns including imperatives and in-
finitives indicating the goal of the speaker.
Argument from consequences
The counts of positive and negative propositions
in the conclusion and premises, calculated from
the General Inquirer2.
Argument from verbal classification
The maximal similarity between the central word
pairs extracted from the conclusion and the
premise; the counts of copula, expletive, and neg-
ative modifier dependency relations returned by
the Stanford parser3 in the conclusion and the
premise.
2 http://www.wjh.harvard.edu/?inquirer/
3 http://nlp.stanford.edu/software/lex-parser.shtml
Table 3: List of scheme-specific features.
Here mi is the number of scheme-specific cue
phrases designed for schemei; P (schemei|cpk) is the
prior probability that the argument A actually be-
longs to schemei, given that some particular cue
phrase cpk is found in A; dik is a value indicat-
ing whether cpk is found in A; and the normaliza-
tion factor N is the number of scheme-specific cue
phrase patterns designed for schemei with at least
one support (at least one of the arguments belonging
to schemei contains that cue phrase). There are two
ways to calculate dik, Boolean and count: in Boolean
mode, dik is treated as 1 if A matches cpk; in count
mode, dik equals to the number of times A matches
cpk; and in both modes, dik is treated as 0 if cpk is
not found inA.
991
For argument from consequences, since the arguer
has an obvious preference for some particular con-
sequence, sentiment orientation can be a good in-
dicator for this scheme, which is quantified by the
counts of positive and negative propositions in the
conclusion and premise.
For argument from verbal classification, there ex-
ists a hypernymy-like relation between some pair of
propositions (entities, concepts, or actions) located
in the conclusion and the premise respectively. The
existence of such a relation is quantified by the max-
imal Jiang-Conrath Similarity (Jiang and Conrath,
1997) between the ?central word? pairs extracted
from the conclusion and the premise. We parse each
sentence of the argument with the Stanford depen-
dency parser, and a word or phrase is considered to
be a central word if it is the dependent or governor of
several particular dependency relations, which basi-
cally represents the attribute or the action of an en-
tity in a sentence, or the entity itself. For example,
if a word or phrase is the dependent of the depen-
dency relation agent, it is therefore considered as a
?central word?. In addition, an arguer tends to use
several particular syntactic structures (copula, exple-
tive, and negative modifier) when using this scheme,
which can be quantified by the counts of those spe-
cial relations in the conclusion and the premise(s).
5 Experiments
5.1 Training
We experiment with two kinds of classification: one-
against-others and pairwise. We build a pruned
C4.5 decision tree (Quinlan, 1993) for each different
classification setup, implemented by Weka Toolkit
3.65 (Hall et al, 2009).
One-against-others classification A one-against-
others classifier is constructed for each of the five
most frequent schemes, using the general features
and the scheme-specific features for the scheme of
interest. For each classifier, there are two possi-
ble outcomes: target scheme and other; 50% of the
training dataset is arguments associated with tar-
get scheme, while the rest is arguments of all the
other schemes, which are treated as other. One-
against-other classification thus tests the effective-
5http://cs.waikato.ac.nz/ml/weka
ness of each scheme?s specific features.
Pairwise classification A pairwise classifier is
constructed for each of the ten possible pairings
of the five schemes, using the general features and
the scheme-specific features of the two schemes in
the pair. For each of the ten classifiers, the train-
ing dataset is divided equally into arguments be-
longing to scheme1 and arguments belonging to
scheme2, where scheme1 and scheme2 are two dif-
ferent schemes among the five. Only features asso-
ciated with scheme1 and scheme2 are used.
5.2 Evaluation
We experiment with different combinations of gen-
eral features and scheme-specific features (discussed
in Section 4.3). To evaluate each experiment, we
use the average accuracy over 10 pools of randomly
sampled data (each with baseline at 50%6) with 10-
fold cross-validation.
6 Results
We first present the best average accuracy (BAA) of
each classification setup. Then we demonstrate the
impact of the feature type (convergent or linked ar-
gument) on BAAs for different classification setups,
since we believe type is strongly correlated with
the particular argumentation scheme and its value is
the only one directly retrieved from the annotations
of the training corpus. For more details, see Feng
(2010).
6.1 BAAs of each classification setup
target scheme BAA dik base type
example 90.6 count token yes
cause 70.4 Boolean
/ count
token no
reasoning 90.8 count sentence yes
consequences 62.9 ? sentence yes
classification 63.2 ? token yes
Table 4: Best average accuracies (BAAs) (%) of one-
against-others classification.
6We also experiment with using general features only, but
the results are consistently below or around the sampling base-
line of 50%; therefore, we do not use them as a baseline here.
992
example cause reason-
ing
conse-
quences
cause 80.6
reasoning 93.1 94.2
consequences 86.9 86.7 97.9
classification 86.0 85.6 98.3 64.2
Table 5: Best average accuracies (BAAs) (%) of pairwise
classification.
Table 4 presents the best average accuracies of
one-against-others classification for each of the five
schemes. The subsequent three columns list the
particular strategies of features incorporation under
which those BAAs are achieved (the complete set of
possible choices is given in Section 4.3.):
? dik: Boolean or count ? the strategy of com-
bining scheme-specific cue phrases or patterns
using either Boolean or count for dik.
? base: sentence or token ? the basic unit of ap-
plying location- or length-related general fea-
tures.
? type: yes or no ? whether type (convergent or
linked argument) is incorporated into the fea-
ture set.
As Table 4 shows, one-against-others classifica-
tion achieves high accuracy for argument from ex-
ample and practical reasoning: 90.6% and 90.8%.
The BAA of argument from cause to effect is only
just over 70%. However, with the last two schemes
(argument from consequences and argument from
verbal classification), accuracy is only in the low
60s; there is little improvement of our system over
the majority baseline of 50%. This is probably due
at least partly to the fact that these schemes do not
have such obvious cue phrases or patterns as the
other three schemes which therefore may require
more world knowledge encoded, and also because
the available training data for each is relatively small
(44 and 41 instances, respectively). The BAA for
each scheme is achieved with inconsistent choices
of base and dik, but the accuracies that resulted from
different choices vary only by very little.
Table 5 shows that our system is able to correctly
differentiate between most of the different scheme
pairs, with accuracies as high as 98%. It has poor
performance (64.0%) only for the pair argument
from consequences and argument from verbal clas-
sification; perhaps not coincidentally, these are the
two schemes for which performance was poorest in
the one-against-others task.
6.2 Impact of type on classification accuracy
As we can see from Table 6, for one-against-others
classifications, incorporating type into the feature
vectors improves classification accuracy in most
cases: the only exception is that the best average ac-
curacy of one-against-others classification between
argument from cause to effect and others is obtained
without involving type into the feature vector ?
but the difference is negligible, i.e., 0.5 percent-
age points with respect to the average difference.
Type also has a relatively small impact on argument
from verbal classification (2.6 points), compared to
its impact on argument from example (22.3 points),
practical reasoning (8.1 points), and argument from
consequences (7.5 points), in terms of the maximal
differences.
Similarly, for pairwise classifications, as shown
in Table 7, type has significant impact on BAAs, es-
pecially on the pairs of practical reasoning versus
argument from cause to effect (17.4 points), prac-
tical reasoning versus argument from example (22.6
points), and argument from verbal classification ver-
sus argument from example (20.2 points), in terms
of the maximal differences; but it has a relatively
small impact on argument from consequences ver-
sus argument from cause to effect (0.8 point), and
argument from verbal classification versus argument
from consequences (1.1 points), in terms of average
differences.
7 Future Work
In future work, we will look at automatically clas-
sifying type (i.e., whether an argument is linked or
convergent), as type is the only feature directly re-
trieved from annotations in the training corpus that
has a strong impact on improving classification ac-
curacies.
Automatically classifying type will not be easy,
because sometimes it is subjective to say whether a
premise is sufficient by itself to support the conclu-
sion or not, especially when the argument is about
993
target scheme BAA-t BAA-no t max diff min diff avg diff
example 90.6 71.6 22.3 10.6 14.7
cause 70.4 70.9 ?0.5 ?0.6 ?0.5
reasoning 90.8 83.2 8.1 7.5 7.7
consequences 62.9 61.9 7.5 ?0.6 4.2
classification 63.2 60.7 2.6 0.4 2.0
Table 6: Accuracy (%) with and without type in one-against-others classification. BAA-t is best average accuracy with
type, and BAA-no t is best average accuracy without type. max diff, min diff, and avg diff are maximal, minimal, and
average differences between each experimental setup with type and without type while the remaining conditions are
the same.
scheme1 scheme2 BAA-t BAA-no t max diff min diff avg diff
cause example 80.6 69.7 10.9 7.1 8.7
reasoning example 93.1 73.1 22.8 19.1 20.1
reasoning cause 94.2 80.5 17.4 8.7 13.9
consequences example 86.9 76.0 13.8 6.9 10.1
consequences cause 87.7 86.7 3.8 ?1.5 ?0.1
consequences reasoning 97.9 97.9 10.6 0.0 0.8
classification example 86.0 74.6 20.2 3.7 7.1
classification cause 85.6 76.8 9.0 3.7 7.1
classification reasoning 98.3 89.3 8.9 4.2 8.3
classification consequences 64.0 60.0 6.5 ?1.3 1.1
Table 7: Accuracy (%) with and without type in pairwise classification. Column headings have the same meanings as
in Table 6.
personal opinions or judgments. So for this task,
we will initially focus on arguments that are (or at
least seem to be) empirical or objective rather than
value-based. It will also be non-trivial to deter-
mine whether an argument is convergent or linked
? whether the premises are independent of one an-
other or not. Cue words and discourse relations be-
tween the premises and the conclusion will be one
helpful factor; for example, besides generally flags
an independent premise. And one premise may be
regarded as linked to another if either would become
an enthymeme if deleted; but determining this in the
general case, without circularity, will be difficult.
We will also work on the argument template fitter,
which is the final component in our overall frame-
work. The task of the argument template fitter is to
map each explicitly stated conclusion and premise
into the corresponding position in its scheme tem-
plate and to extract the information necessary for en-
thymeme reconstruction. Here we propose a syntax-
based approach for this stage, which is similar to
tasks in information retrieval. This can be best ex-
plained by the argument in Example 1, which uses
the particular argumentation scheme practical rea-
soning.
We want to fit the Premise and the Conclusion of
this argument into the Major premise and the Con-
clusion slots of the definition of practical reasoning
(see Table 1), and construct the following conceptual
mapping relations:
1. Survival of the entire world ?? a goal G
2. Adhering to the treaties and covenants aiming
for a world free of nuclear arsenals and other
conventional and biological weapons of mass
destruction ?? action A
Thereby we will be able to reconstruct the missing
Minor premise ? the enthymeme in this argument:
Carrying out adhering to the treaties and
covenants aiming for a world free of nuclear
arsenals and other conventional and biological
994
weapons of mass destruction is a means of real-
izing survival of the entire world.
8 Conclusion
The argumentation scheme classification system that
we have presented in this paper introduces a new
task in research on argumentation. To the best of
our knowledge, this is the first attempt to classify
argumentation schemes.
In our experiments, we have focused on the five
most frequently used schemes in Walton?s scheme-
set, and conducted two kinds of classification: in
one-against-others classification, we achieved over
90% best average accuracies for two schemes, with
other three schemes in the 60s to 70s; and in pair-
wise classification, we obtained 80% to 90% best
average accuracies for most scheme pairs. The poor
performance of our classification system on other
experimental setups is partly due to the lack of train-
ing examples or to insufficient world knowledge.
Completion of our scheme classification system
will be a step towards our ultimate goal of recon-
structing the enthymemes in an argument by the pro-
cedure depicted in Figure 1. Because of the signifi-
cance of enthymemes in reasoning and arguing, this
is crucial to the goal of understanding arguments.
But given the still-premature state of research of ar-
gumentation in computational linguistics, there are
many practical issues to deal with first, such as the
construction of richer training corpora and improve-
ment of the performance of each step in the proce-
dure.
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and by the University of Toronto. We are
grateful to Suzanne Stevenson for helpful comments
and suggestions.
References
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational Linguistics,
13(1?2):11?24.
Judith Dick. 1987. Conceptual retrieval and case law.
In Proceedings, First International Conference on Ar-
tificial Intelligence and Law, pages 106?115, Boston,
May.
Judith Dick. 1991a. A Conceptual, Case-relation Repre-
sentation of Text for Intelligent Retrieval. Ph.D. thesis,
Faculty of Library and Information Science, Univer-
sity of Toronto, April.
Judith Dick. 1991b. Representation of legal text for con-
ceptual retrieval. In Proceedings, Third International
Conference on Artificial Intelligence and Law, pages
244?252, Oxford, June.
Vanessa Wei Feng. 2010. Classifying argu-
ments by scheme. Technical report, Depart-
ment of Computer Science, University of Toronto,
November. http://ftp.cs.toronto.edu/pub/
gh/Feng-MSc-2010.pdf.
Sarah George, Ingrid Zukerman, and Michael Niemann.
2007. Inferences, suppositions and explanatory exten-
sions in argument interpretation. User Modeling and
User-Adapted Interaction, 17(5):439?474.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
ACL 2003 Workshop on Multilingual Summarization
and Question Answering, pages 76?83, Morristown,
NJ, USA. Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations Newsletter, 11(1):10?18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In International Conference Research on Com-
putational Linguistics (ROCLING X), pages 19?33.
Joel Katzav and Chris Reed. 2004. On argumentation
schemes and the natural classification of arguments.
Argumentation, 18(2):239?259.
Raquel Mochales and Marie-Francine Moens. 2008.
Study on the structure of argumentation in case law. In
Proceedings of the 2008 Conference on Legal Knowl-
edge and Information Systems, pages 11?20, Amster-
dam, The Netherlands. IOS Press.
Raquel Mochales and Marie-Francine Moens. 2009a.
Argumentation mining: the detection, classification
and structure of arguments in text. In ICAIL ?09: Pro-
ceedings of the 12th International Conference on Arti-
ficial Intelligence and Law, pages 98?107, New York,
NY, USA. ACM.
Raquel Mochales and Marie-Francine Moens. 2009b.
Automatic argumentation detection and its role in law
and the semantic web. In Proceedings of the 2009
Conference on Law, Ontologies and the Semantic Web,
pages 115?129, Amsterdam, The Netherlands. IOS
Press.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
995
of arguments in legal texts. In ICAIL ?07: Proceed-
ings of the 11th International Conference on Artificial
Intelligence and Law, pages 225?230, New York, NY,
USA. ACM.
John L. Pollock. 1995. Cognitive Carpentry: A
Blueprint for How to Build a Person. Bradford Books.
The MIT Press, May.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Machine Learning, 16(3):235?240.
Chris Reed and Glenn Rowe. 2004. Araucaria: Software
for argument analysis, diagramming and representa-
tion. International Journal of Artificial Intelligence
Tools, 14:961?980.
Glenn Rowe and Chris Reed. 2008. Argument diagram-
ming: The Araucaria project. In Knowledge Cartog-
raphy, pages 163?181. Springer London.
Frans H. van Eemeren and Rob Grootendorst. 1992.
Argumentation, Communication, and Fallacies: A
Pragma-Dialectical Perspective. Routledge.
Douglas Walton and Chris Reed. 2002. Argumenta-
tion schemes and defeasible inferences. In Workshop
on Computational Models of Natural Argument, 15th
European Conference on Artificial Intelligence, pages
11?20, Amsterdam, The Netherlands. IOS Press.
Douglas Walton, Chris Reed, and Fabrizio Macagno.
2008. Argumentation Schemes. Cambridge University
Press.
996
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60?68,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Text-level Discourse Parsing with Rich Linguistic Features
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
In this paper, we develop an RST-style text-
level discourse parser, based on the HILDA
discourse parser (Hernault et al, 2010b). We
significantly improve its tree-building step by
incorporating our own rich linguistic features.
We also analyze the difficulty of extending
traditional sentence-level discourse parsing to
text-level parsing by comparing discourse-
parsing performance under different discourse
conditions.
1 Introduction
In a well-written text, no unit of the text is com-
pletely isolated; interpretation requires understand-
ing the unit?s relation with the context. Research in
discourse parsing aims to unmask such relations in
text, which is helpful for many downstream applica-
tions such as summarization, information retrieval,
and question answering.
However, most existing discourse parsers oper-
ate on individual sentences alone, whereas discourse
parsing is more powerful for text-level analysis.
Therefore, in this work, we aim to develop a text-
level discourse parser. We follow the framework of
Rhetorical Structure Theory (Mann and Thompson,
1988) and we take the HILDA discourse parser (Her-
nault et al, 2010b) as the basis of our work, because
it is the first fully implemented text-level discourse
parser with state-of-the-art performance. We signif-
icantly improve the performance of HILDA?s tree-
building step (introduced in Section 5.1 below) by
incorporating rich linguistic features (Section 5.3).
In our experiments (Section 6), we also analyze the
difficulty with extending traditional sentence-level
discourse parsing to text-level parsing, by compar-
ing discourse parsing performance under different
discourse conditions.
2 Discourse-annotated corpora
2.1 The RST Discourse Treebank
Rhetorical Structure Theory (Mann and Thompson,
1988) is one of the most widely accepted frame-
works for discourse analysis. In the framework of
RST, a coherent text can be represented as a dis-
course tree whose leaves are non-overlapping text
spans called elementary discourse units (EDUs);
these are the minimal text units of discourse trees.
Adjacent nodes can be related through particular dis-
course relations to form a discourse subtree, which
can then be related to other adjacent nodes in the tree
structure. According to RST, there are two types of
discourse relations, hypotactic (?mononuclear?) and
paratactic (?multi-nuclear?). In mononuclear rela-
tions, one of the text spans, the nucleus, is more
salient than the other, the satellite, while in multi-
nuclear relations, all text spans are equally important
for interpretation.
The example text fragment shown in Figure 1
consists of four EDUs (e1-e4), segmented by square
brackets. Its discourse tree representation is shown
below in the figure, following the notational conven-
tion of RST. The two EDUs e1 and e2 are related by a
mononuclear relation ATTRIBUTION, where e1 is the
more salient span; the span (e1-e2) and the EDU e3
are related by a multi-nuclear relation SAME-UNIT,
where they are equally salient.
60
[Catching up with commercial competitors in retail banking
and financial services,]e1 [they argue,]e2 [will be difficult,]e3
[particularly if market conditions turn sour.]e4
(e1) (e2)
attribution
(e1-e3)
same-unit
(e3)
(e4)
condition
(e1-e4)
(e1-e2)
Figure 1: An example text fragment (wsj 0616) com-
posed of four EDUs, and its RST discourse tree repre-
sentation.
The RST Discourse Treebank (RST-DT) (Carlson
et al, 2001), is a corpus annotated in the framework
of RST. It consists of 385 documents (347 for train-
ing and 38 for testing) from the Wall Street Jour-
nal. In RST-DT, the original 24 discourse relations
defined by Mann and Thompson (1988) are further
divided into a set of 18 relation classes with 78 finer-
grained rhetorical relations in total, which provides
a high level of expressivity.
2.2 The Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) is another annotated discourse corpus. Its
text is a superset of that of RST-DT (2159 Wall
Street Journal articles). Unlike RST-DT, PDTB does
not follow the framework of RST; rather, it follows
a lexically grounded, predicate-argument approach
with a different set of predefined discourse relations,
as proposed by Webber (2004). In this framework, a
discourse connective (e.g., because) is considered to
be a predicate that takes two text spans as its argu-
ments. The argument that the discourse connective
structurally attaches to is called Arg2, and the other
argument is called Arg1 ? unlike in RST, the two
arguments are not distinguished by their saliency
for interpretation. Another important difference be-
tween PDTB and RST-DT is that in PDTB, there
does not necessarily exist a tree structure covering
the full text, i.e., PDTB-styled discourse relations
exist only in a very local contextual window. In
PDTB, relation types are organized hierarchically:
there are 4 classes, which can be further divided into
16 types and 23 subtypes.
3 Related work
Discourse parsing was first brought to prominence
by Marcu (1997). Since then, many different algo-
rithms and systems (Soricut and Marcu, 2003; Reit-
ter, 2003; LeThanh et al, 2004; Baldridge and Las-
carides, 2005; Subba and Di Eugenio, 2009; Sagae,
2009; Hernault et al, 2010b) have been proposed,
which extracted different textual information and
adopted various approaches for discourse tree build-
ing. Here we briefly review two fully implemented
text-level discourse parsers with the state-of-the-art
performance.
The HILDA discourse parser of Hernault and his
colleagues (duVerle and Prendinger, 2009; Hernault
et al, 2010b) is the first fully-implemented feature-
based discourse parser that works at the full text
level. Hernault et al extracted a variety of lexi-
cal and syntactic features from the input text, and
trained their system on RST-DT. While some of their
features were inspired by the previous work of oth-
ers, e.g., lexico-syntactic features borrowed from
Soricut and Marcu (2003), Hernault et al also pro-
posed the novel idea of discourse tree building by
using two classifiers in cascade ? a binary struc-
ture classifier to determine whether two adjacent text
units should be merged to form a new subtree, and
a multi-class classifier to determine which discourse
relation label should be assigned to the new subtree
? instead of the more-usual single multi-class clas-
sifier with the additional label NO-REL. Hernault
et al obtained 93.8% F-score for EDU segmenta-
tion, 85.0% accuracy for structure classification, and
66.8% accuracy for 18-class relation classification.
Lin et al (2009) attempted to recognize implicit
discourse relations (discourse relations which are
not signaled by explicit connectives) in PDTB by us-
ing four classes of features ? contextual features,
constituent parse features, dependency parse fea-
tures, and lexical features ? and explored their indi-
vidual influence on performance. They showed that
the production rules extracted from constituent parse
trees are the most effective features, while contex-
tual features are the weakest. Subsequently, they
fully implemented an end-to-end PDTB-style dis-
course parser (Lin et al, 2010).
Recently, Hernault et al (2010a) argued that more
effort should be focused on improving performance
61
on certain infrequent relations presented in the dis-
course corpora, since due to the imbalanced distribu-
tion of different discourse relations in both RST-DT
and PDTB, the overall accuracy score can be over-
whelmed by good performance on the small sub-
set of frequent relations, even though the algorithms
perform poorly on all other relations. However, be-
cause of infrequent relations for which we do not
have sufficient instances for training, many unseen
features occur in the test data, resulting in poor test
performance. Therefore, Hernault et al proposed
a semi-supervised method that exploits abundant,
freely-available unlabeled data as a basis for feature
vector extension to alleviate such issues.
4 Text-level discourse parsing
Not until recently has discourse parsing for full texts
been a research focus ? previously, discourse pars-
ing was only performed on the sentence level1. In
this section, we explain why we believe text-level
discourse parsing is crucial.
Unlike syntactic parsing, where we are almost
never interested in parsing above sentence level,
sentence-level parsing is not sufficient for discourse
parsing. While a sequence of local (sentence-level)
grammaticality can be considered to be global gram-
maticality, a sequence of local discourse coherence
does not necessarily form a globally coherent text.
For example, the text shown in Figure 2 contains
two sentences, each of which is coherent and sen-
sible itself. However, there is no reasonable content
transition between these two sentences, so the com-
bination of the two sentences does not make much
sense. If we attempt to represent the text as an RST
discourse tree like the one shown in Figure 1, we
find that no discourse relation can be assigned to re-
late the spans (e1-e2) and (e3-e4) and the text cannot
be represented by a valid discourse tree structure.
In order to rule out such unreasonable transitions
between sentences, we have to expand the text units
upon which discourse parsing is performed: from
sentences to paragraphs, and finally paragraphs to
1Strictly speaking, for PDTB-style discourse parsing (e.g.,
Lin et al (2009; 2010)), there is no absolute distinction between
sentence-level and text-level parsing, since in PDTB, discourse
relations are annotated at a level no higher than that of adjacent
sentences. Here we are concerned with RST-style discourse
parsing.
[No wonder he got an A for his English class,]e1 [he was
studying so hard.]e2 [He avoids eating chocolates,]e3 [since he
is really worried about gaining weight.]e4
(e1) (e2)
cause
(e1-e2)
(e3) (e4)
cause
(e3-e4)
?
Figure 2: An example of incoherent text fragment com-
posed of two sentences. The two EDUs associated with
each sentence are coherent themselves, whereas the com-
bination of the two sentences is not coherent at the sen-
tence boundary. No discourse relation can be associated
with the spans (e1-e2) and (e3-e4).
the full text.
Text-level discourse parsing imposes more con-
straints on the global coherence than sentence-level
discourse parsing. However, if, technically speak-
ing, text-level discourse parsing were no more diffi-
cult than sentence-level parsing, any sentence-level
discourse parser could be easily upgraded to a text-
level discourse parser just by applying it to full
texts. In our experiments (Section 6), we show
that when applied above the sentence level, the per-
formance of discourse parsing is consistently infe-
rior to that within individual sentences, and we will
briefly discuss what the key difficulties with extend-
ing sentence-level to text-level discourse parsing are.
5 Method
We use the HILDA discourse parser of Hernault et
al. (2010b) as the basis of our work. We refine Her-
nault et al?s original feature set by incorporating our
own features as well as some adapted from Lin et al
(2009). We choose HILDA because it is a fully im-
plemented text-level discourse parser with the best
reported performance up to now. On the other hand,
we also follow the work of Lin et al (2009), because
their features can be good supplements to those used
by HILDA, even though Lin et al?s work was based
on PDTB. More importantly, Lin et al?s strategy of
performing feature selection prior to classification
proves to be effective in reducing the total feature
dimensions, which is favorable since we wish to in-
corporate rich linguistic features into our discourse
parser.
62
5.1 Bottom-up approach and two-stage
labeling step
Following the methodology of HILDA, an input text
is first segmented into EDUs. Then, from the EDUs,
a bottom-up approach is applied to build a discourse
tree for the full text. Initially, a binary Structure clas-
sifier evaluates whether a discourse relation is likely
to hold between consecutive EDUs. The two EDUs
which are most probably connected by a discourse
relation are merged into a discourse subtree of two
EDUs. A multi-class Relation classifier evaluates
which discourse relation label should be assigned to
this new subtree. Next, the Structure classifier and
the Relation classifier are employed in cascade to re-
evaluate which relations are the most likely to hold
between adjacent spans (discourse subtrees of any
size, including atomic EDUs). This procedure is re-
peated until all spans are merged, and a discourse
tree covering the full text is therefore produced.
Since EDU boundaries are highly correlated with
the syntactic structures embedded in the sentences,
EDU segmentation is a relatively trivial step ? us-
ing machine-generated syntactic parse trees, HILDA
achieves an F-score of 93.8% for EDU segmenta-
tion. Therefore, our work is focused on the tree-
building step, i.e., the Structure and the Relation
classifiers. In our experiments, we improve the over-
all performance of these two classifiers by incorpo-
rating rich linguistic features, together with appro-
priate feature selection. We also explore how these
two classifiers perform differently under different
discourse conditions.
5.2 Instance extraction
Because HILDA adopts a bottom-up approach for
discourse tree building, errors produced on lower
levels will certainly propagate to upper levels, usu-
ally causing the final discourse tree to be very dis-
similar to the gold standard. While appropriate post-
processing may be employed to fix these errors and
help global discourse tree recovery, we feel that it
might be more effective to directly improve the raw
instance performance of the Structure and Relation
classifiers. Therefore, in our experiments, all classi-
fications are conducted and evaluated on the basis of
individual instances.
Each instance is of the form (SL,SR), which is a
pair of adjacent text spans SL (left span) and SR (right
span), extracted from the discourse tree representa-
tion in RST-DT. From each discourse tree, we ex-
tract positive instances as those pairs of text spans
that are siblings of the same parent node, and neg-
ative examples as those pairs of adjacent text spans
that are not siblings in the tree structure. In all in-
stances, both SL and SR must correspond to a con-
stituent in the discourse tree, which can be either an
atomic EDU or a concatenation of multiple consec-
utive EDUs.
5.3 Feature extraction
Given a pair of text spans (SL,SR), we extract the
following seven types of features.
HILDA?s features: We incorporate the origi-
nal features used in the HILDA discourse parser
with slight modification, which include the follow-
ing four types of features occurring in SL, SR, or
both: (1) N-gram prefixes and suffixes; (2) syntac-
tic tag prefixes and suffixes; (3) lexical heads in the
constituent parse tree; and (4) POS tag of the domi-
nating nodes.
Lin et al?s features: Following Lin et al (2009),
we extract the following three types of features: (1)
pairs of words, one from SL and one from SR, as
originally proposed by Marcu and Echihabi (2002);
(2) dependency parse features in SL, SR, or both; and
(3) syntactic production rules in SL, SR, or both.
Contextual features: For a globally coherent
text, there exist particular sequential patterns in the
local usage of different discourse relations. Given
(SL,SR), the pair of text spans of interest, contextual
features attempt to encode the discourse relations as-
signed to the preceding and the following text span
pairs. Lin et al (2009) also incorporated contextual
features in their feature set. However, their work
was based on PDTB, which has a very different an-
notation framework from RST-DT (see Section 2):
in PDTB, annotated discourse relations can form a
chain-like structure such that contextual features can
be more readily extracted. However, in RST-DT, a
full text is represented as a discourse tree structure,
so the previous and the next discourse relations are
not well-defined.
We resolve this problem as follows. Suppose SL =
(ei-e j) and SR = (e j+1-ek), where i? j < k. To find
the previous discourse relation RELprev that immedi-
63
ately precedes (SL,SR), we look for the largest span
Sprev = (eh-ei?1),h < i, such that it ends right before
SL and all its leaves belong to a single subtree which
neither SL nor SR is a part of. If SL and SR belong
to the same sentence, Sprev must also be a within-
sentence span, and it must be a cross-sentence span
if SL and SR are a cross-sentence span pair. RELprev
is then the discourse relation which covers Sprev. The
next discourse relation RELnext that immediately fol-
lows (SL,SR) is found in the analogous way.
However, when building a discourse tree using
a greedy bottom-up approach, as adopted by the
HILDA discourse parser, RELprev and RELnext are
not always available; therefore these contextual fea-
tures represent an idealized situation. In our ex-
periments we wish to explore whether incorporating
perfect contextual features can help better recognize
discourse relations, and if so, set an upper bound of
performance in more realistic situations.
Discourse production rules: Inspired by Lin et
al. (2009)?s syntactic production rules as features,
we develop another set of production rules, namely
discourse production rules, derived directly from the
tree structure representation in RST-DT.
For example, with respect to the RST discourse
tree shown in Figure 1, we extract the following
discourse production rules: ATTRIBUTION ? NO-
REL NO-REL, SAME-UNIT ? ATTRIBUTION NO-
REL, CONDITION ? SAME-UNIT NO-REL, where
NO-REL denotes a leaf node in the discourse subtree.
The intuition behind using discourse production
rules is that the discourse tree structure is able to re-
flect the relatedness of different discourse relations
? discourse relations on the lower level of the tree
can determine the relation of their direct parent to
some degree. Hernault et al (2010b) attempt to
capture such relatedness by traversing a discourse
subtree and encoding its traversal path as features,
but since they used a depth-first traversal order, the
information encoded in a node?s direct children is
too distant; whereas most useful information can be
gained from the relations covering these direct chil-
dren.
Semantic similarities: Semantic similarities are
useful for recognizing relations such as COMPARI-
SON, when there are no explicit syntactic structures
or lexical features signaling such relations.
We use two subsets of similarity features for verbs
and nouns separately. For each verb in either SL or
SR, we look up its most frequent verb class ID in
VerbNet2, and specify whether that verb class ID ap-
pears in SL, SR, or both. For nouns, we extract all
pairs of nouns from (SL,SR), and compute the aver-
age similarity among these pairs. In particular, we
use path similarity, lch similarity, wup similarity,
res similarity, jcn similarity, and lin similarity pro-
vided in the nltk.wordnet.similarity package (Bird et
al., 2009) for computing WordNet-based similarity,
and always choose the most frequent sense for each
noun.
Cue phrases: We compile a list of cue phrases,
the majority of which are connectives collected by
Knott and Dale (1994). For each cue phrase in this
list, we determine whether it appears in SL or SR. If
a cue phrase appears in a span, we also determine
whether its appearance is in the beginning, the end,
or the middle of that span.
5.4 Feature selection
If we consider all possible combinations of the fea-
tures listed in Section 5.3, the resulting data space
can be horribly high dimensional and extremely
sparse. Therefore, prior to training, we first conduct
feature selection to effectively reduce the dimension
of the data space.
We employ the same feature selection method as
Lin et al (2009). Feature selection is done for each
feature type separately. Among all features belong-
ing to the feature type to be selected, we first ex-
tract all possible features that have been seen in the
training data, e.g., when applying feature selection
for word pairs, we find all word pairs that appear
in some text span pair that have a discourse relation
between them. Then for each extracted feature, we
compute its mutual information with all 18 discourse
relation classes defined in RST-DT, and use the high-
est mutual information to evaluate the effectiveness
of that feature. All extracted features are sorted to
form a ranked list by effectiveness. After that, we
use a threshold to select the top features from that
ranked list. The total number of selected features
used in our experiments is 21,410.
2http://verbs.colorado.edu/?mpalmer/
projects/verbnet
64
6 Experiments
As discussed in Section 5.1, our research focus in
this paper is the tree-building step of the HILDA
discourse parser, which consists of two classifica-
tions: Structure and Relation classification. The bi-
nary Structure classifier decides whether a discourse
relation is likely to hold between consecutive text
spans, and the multi-class Relation classifier decides
which discourse relation label holds between these
two text spans if the Structure classifier predicts the
existence of such a relation.
Although HILDA?s bottom-up approach is aimed
at building a discourse tree for the full text, it does
not explicitly employ different strategies for within-
sentence text spans and cross-sentence text spans.
However, we believe that discourse parsing is signif-
icantly more difficult for text spans at higher levels
of the discourse tree structure. Therefore, we con-
duct the following three sub-experiments to explore
whether the two classifiers behave differently under
different discourse conditions.
Within-sentence: Trained and tested on text span
pairs belonging to the same sentence.
Cross-sentence: Trained and tested on text span
pairs belonging to different sentences.
Hybrid: Trained and tested on all text span pairs.
In particular, we split the training set and the test-
ing set following the convention of RST-DT, and
conduct Structure and Relation classification by in-
corporating our rich linguistic features, as listed in
Section 5.3 above. To rule out all confounding fac-
tors, all classifiers are trained and tested on the basis
of individual text span pairs, by assuming the dis-
course subtree structure (if any) covering each indi-
vidual text span has been already correctly identified
(no error propagation).
6.1 Structure classification
The number of training and testing instances used in
this experiment for different discourse conditions is
listed in Table 1. Instances are extracted in the man-
ner described in Section 5.2. We observe that the
distribution of positive and negative instances is ex-
tremely skewed for cross-sentence instances, while
for all conditions, the distribution is similar in the
training and the testing set.
In this experiment, classifiers are trained using
Dataset Pos # Neg # Total #
Within
Training 11,087 10,188 21,275
Testing 1,340 1,181 2,521
Cross
Training 6,646 49,467 56,113
Testing 882 6,357 7,239
Hybrid
Training 17,733 59,655 77,388
Testing 2,222 7,539 9,761
Table 1: Number of training and testing instances used in
Structure classification.
the SVMperf classifier (Joachims, 2005) with a lin-
ear kernel.
Structure classification performance for all three
discourse conditions is shown in Table 2. The
columns Full and NC (No Context) denote the per-
formance of using all features listed in Section 5.3
and all features except for contextual features re-
spectively. As discussed in Section 5.3, contex-
tual features represent an ideal situation which is
not always available in real applications; therefore,
we wish to see how they affect the overall per-
formance by comparing the performance obtained
with them and without them as features. The col-
umn HILDA lists the performance of using Hernault
et al (2010b)?s original features, and Baseline de-
notes the performance obtained by always picking
the more frequent class. Performance is measured
by four metrics: accuracy, precision, recall, and F1
score on the test set, shown in the first section in
each sub-table.
Under the within-sentence condition, we observe
that, surprisingly, incorporating contextual features
boosts the overall performance by a large margin,
even though it requires only 38 additional features.
Under the cross-sentence condition, our features re-
sult in lower accuracy and precision than HILDA?s
features. However, under this discourse condition,
the distribution of positive and negative instances
in both training and test sets is extremely skewed,
which makes it more sensible to compare the recall
and F1 scores for evaluation. In fact, our features
achieve much higher recall and F1 score despite a
much lower precision and a slightly lower accuracy.
In the second section of each sub-table, we also
list the F1 score on the training data. This allows
65
us to compare the model-fitting capacity of differ-
ent feature sets from another perspective, especially
when the training data is not sufficiently well fitted
by the model. For example, looking at the training
F1 score under the cross-sentence condition, we can
see that classification using full features and clas-
sification without contextual features both perform
significantly better on the training data than HILDA
does. At the same time, such superior performance
is not due to possible over-fitting on the training
data, because we are using significantly fewer fea-
tures (21,410 for Full and 21,372 for NC) than Her-
nault et al (2010b)?s 136,987; rather, it suggests
that using carefully selected rich linguistic features
is able to better model the problem itself.
Comparing the results obtained under the first
two conditions, we see that the binary classification
problem of whether a discourse relation is likely to
hold between two adjacent text spans is much more
difficult under the cross-sentence condition. One
major reason is that many features that are predictive
for within-sentence instances are no longer applica-
ble (e.g., Dependency parse features). In addition,
given the extremely imbalanced nature of the dataset
under this discourse condition, we might need to
employ special approaches to deal with this needle-
in-a-haystack problem. This difficulty can also be
perceived from the training performance. Compared
to the within-sentence condition, all features fit the
training data much more poorly under the cross-
sentence condition. This suggests that sophisticated
features or models in addition to our rich linguis-
tic features must be incorporated in order to fit the
problem sufficiently well. Unfortunately, this under-
fitting issue cannot be resolved by exploiting any
abundant linguistic resources for feature vector ex-
tension (e.g., Hernault et al (2010a)), because the
poor training performance is no longer caused by the
unknown features found in test vectors.
Turning to the hybrid condition, the performance
of Full features is surprisingly good, probably be-
cause we have more available training data than the
other two conditions. However, with contextual fea-
tures removed, our features perform quite similarly
to those of Hernault et al (2010b), but still with
a marginal, but nonetheless statistically significant,
improvement on recall and F1 score.
Full NC HILDA Baseline
Within-sentence
Accuracy 91.04* 85.17* 83.74 53.15
Precision 92.71* 85.36* 84.81 53.15
Recall 90.22* 87.01* 84.55 100.00
F1 91.45* 86.18* 84.68 69.41
Train F1 97.87* 96.23* 95.42 68.52
Cross-sentence
Accuracy 87.69 86.68 89.13 87.82
Precision 49.60 44.73 61.90 ?
Recall 63.95* 39.46* 28.00 0.00
F1 55.87* 41.93* 38.56 ?
Train F1 87.25* 71.93* 49.03 ?
Hybrid
Accuracy 95.64* 87.03 87.04 77.24
Precision 94.77* 74.19 79.41 ?
Recall 85.92* 65.98* 58.15 0.00
F1 89.51* 69.84* 67.13 ?
Train F1 93.15* 80.79* 72.09 ?
Table 2: Structure classification performance (in percent-
age) on text spans of within-sentence, cross-sentence, and
all level. Performance that is significantly superior to that
of HILDA (p < .01, using the Wilcoxon sign-rank test for
significance) is denoted by *.
6.2 Relation classification
The Relation classifier has 18 possible output la-
bels, which are the coarse-grained relation classes
defined in RST-DT. We do not consider nuclearity
when classifying different discourse relations, i.e.,
ATTRIBUTION[N][S] and ATTRIBUTION[S][N] are
treated as the same label. The training and test in-
stances in this experiment are from the positive sub-
set used in Structure classification.
In this experiment, classifiers are trained using
LibSVM classifier (Chang and Lin, 2011) with a lin-
ear kernel and probability estimation.
Relation classification performance under three
discourse conditions is shown in Table 3. We list
the performance achieved by Full, NC, and HILDA
features, as well as the majority baseline, which is
obtained by always picking the most frequent class
label (ELABORATION in all cases).
66
Full NC HILDA Baseline
Within-sentence
MAFS 0.490 0.485 0.446 ?
WAFS 0.763 0.762 0.740 ?
Acc (%) 78.06 78.13 76.42 31.42
TAcc (%) 99.90 99.93 99.26 33.38
Cross-sentence
MAFS 0.194 0.184 0.127 ?
WAFS 0.334 0.329 0.316 ?
Acc (%) 46.83 46.71 45.69 42.52
TAcc (%) 78.30 67.30 57.70 47.79
Hybrid
MAFS 0.440 0.428 0.379 ?
WAFS 0.607 0.604 0.588 ?
Acc (%) 65.30 65.12 64.18 35.82
TAcc (%) 99.96 99.95 90.11 38.78
Table 3: Relation classification performance on text
spans of within-sentence, cross-sentence, and all levels.
Following Hernault et al (2010a), we use Macro-
averaged F-scores (MAFS) to evaluate the perfor-
mance of each classifier. Macro-averaged F-score
is not influenced by the number of instances that
exist in each relation class, by equally weighting
the performance of each relation class3. Therefore,
the evaluation is not biased by the performance on
those prevalent classes such as ATTRIBUTION and
ELABORATION. For reasons of space, we do not
show the class-wise F-scores, but in our results,
we find that using our features consistently provides
superior performance for most class relations over
HILDA?s features, and therefore results in higher
overall MAFS under all conditions. We also list two
other metrics for performance on the test data ?
Weight-averaged F-score (WAFS), which weights
the performance of each relation class by the num-
ber of its existing instances, and the testing accuracy
(Acc) ? but these metrics are relatively more bi-
3No significance test is reported for relation classification,
because we are comparing MAFS, which equally weights the
performance of each relation. Therefore, traditional signifi-
cance tests which operate on individual instances rather than
individual relation classes are not applicable.
ased evaluation metrics in this task. Similar to Struc-
ture classification, the accuracy on the training data
(TAcc)4 is listed in the second section of each sub-
table. It demonstrates that our carefully selected rich
linguistic features are able to better fit the classifi-
cation problem, especially under the cross-sentence
condition.
Similar to our observation in Structure classifica-
tion, the performance of Relation classification for
cross-sentence instances is also much poorer than
that on within-sentence instances, which again re-
veals the difficulty of text-level discourse parsing.
7 Conclusions
In this paper, we aimed to develop an RST-style
text-level discourse parser. We chose the HILDA
discourse parser (Hernault et al, 2010b) as the ba-
sis of our work, and significantly improved its tree-
building step by incorporating our own rich linguis-
tic features, together with features suggested by Lin
et al (2009). We analyzed the difficulty of extending
traditional sentence-level discourse parsing to text-
level parsing by showing that using exactly the same
set of features, the performance of Structure and Re-
lation classification on cross-sentence instances is
consistently inferior to that on within-sentence in-
stances. We also explored the effect of contextual
features on the overall performance. We showed
that contextual features are highly effective for both
Structure and Relation classification under all dis-
course conditions. Although perfect contextual fea-
tures are available only in idealized situations, when
they are correct, together with other features, they
can almost correctly predict the tree structure and
better predict the relation labels. Therefore, an it-
erative updating approach, which progressively up-
dates the tree structure and the labeling based on the
current estimation, may push the final results toward
this idealized end.
Our future work will be to fully implement an
end-to-end discourse parser using our rich linguis-
tic features, and focus on improving performance on
cross-sentence instances.
4We use accuracy instead of MAFS as the evaluation metric
on the training data because it is the metric that the training
procedure is optimized toward.
67
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and by the University of Toronto.
References
Jason Baldridge and Alex Lascarides. 2005. Probabilis-
tic head-driven parsing for discourse structure. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, pages 96?103.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python ? Analyzing
Text with the Natural Language Toolkit. O?Reilly.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of Rhetorical Structure Theory. In Pro-
ceedings of Second SIGdial Workshop on Discourse
and Dialogue, pages 1?10.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:1?27.
David A. duVerle and Helmut Prendinger. 2009. A
novel discourse parser based on Support Vector Ma-
chine classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, Volume 2, ACL ?09,
pages 665?673, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 399?409, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010b. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1?33.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In International
Conference on Machine Learning (ICML), pages 377?
384.
Alistair Knott and Robert Dale. 1994. Using linguistic
phenomena to motivate a set of coherence relations.
Discourse Processes, 18(1).
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th International
Conference on Computational Linguistics, pages 329?
335.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, Volume 1, EMNLP ?09, pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
368?375, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Daniel Marcu. 1997. The rhetorical parsing of natu-
ral language texts. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics, pages 96?103.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
David Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature sup-
port vector models. LDV Forum, 18(1/2):38?52.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies, pages 81?84.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical informa-
tion. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, Volume 1, pages 149?156.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic informa-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 566?574.
Bonnie Webber. 2004. D-LTAG: Extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751?779.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511?521,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Linear-Time Bottom-Up Discourse Parser
with Constraints and Post-Editing
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, Canada
gh@cs.toronto.edu
Abstract
Text-level discourse parsing remains a
challenge. The current state-of-the-art
overall accuracy in relation assignment is
55.73%, achieved by Joty et al (2013).
However, their model has a high order of
time complexity, and thus cannot be ap-
plied in practice. In this work, we develop
a much faster model whose time complex-
ity is linear in the number of sentences.
Our model adopts a greedy bottom-up ap-
proach, with two linear-chain CRFs ap-
plied in cascade as local classifiers. To en-
hance the accuracy of the pipeline, we add
additional constraints in the Viterbi decod-
ing of the first CRF. In addition to effi-
ciency, our parser also significantly out-
performs the state of the art. Moreover,
our novel approach of post-editing, which
modifies a fully-built tree by considering
information from constituents on upper
levels, can further improve the accuracy.
1 Introduction
Discourse parsing is the task of identifying the
presence and the type of the discourse relations
between discourse units. While research in dis-
course parsing can be partitioned into several di-
rections according to different theories and frame-
works, Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is probably the most am-
bitious one, because it aims to identify not only
the discourse relations in a small local context, but
also the hierarchical tree structure for the full text:
from the relations relating the smallest discourse
units (called elementary discourse units, EDUs),
to the ones connecting paragraphs.
For example, Figure 1 shows a text fragment
consisting of two sentences with four EDUs in
total (e
1
-e
4
). Its discourse tree representation is
shown below the text, following the notation con-
vention of RST: the two EDUs e
1
and e
2
are re-
lated by a mononuclear relation CONSEQUENCE,
where e
2
is the more salient span (called nucleus,
and e
1
is called satellite); e
3
and e
4
are related by
another mononuclear relation CIRCUMSTANCE,
with e
4
as the nucleus; the two spans e
1:2
and e
3:4
are further related by a multi-nuclear relation SE-
QUENCE, with both spans as the nucleus.
Conventionally, there are two major sub-tasks
related to text-level discourse parsing: (1) EDU
segmentation: to segment the raw text into EDUs,
and (2) tree-building: to build a discourse tree
from EDUs, representing the discourse relations in
the text. Since the first sub-task is considered rela-
tively easy, with the state-of-art accuracy at above
90% (Joty et al, 2012), the recent research focus
is on the second sub-task, and often uses manual
EDU segmentation.
The current state-of-the-art overall accuracy of
the tree-building sub-task, evaluated on the RST
Discourse Treebank (RST-DT, to be introduced in
Section 8), is 55.73% by Joty et al (2013). How-
ever, as an optimal discourse parser, Joty et al?s
model is highly inefficient in practice, with re-
spect to both their DCRF-based local classifiers,
and their CKY-like bottom-up parsing algorithm.
DCRF (Dynamic Conditional Random Fields) is
a generalization of linear-chain CRFs, in which
each time slice contains a set of state variables
and edges (Sutton et al, 2007). CKY parsing is
a bottom-up parsing algorithm which searches all
possible parsing paths by dynamic programming.
Therefore, despite its superior performance, their
model is infeasible in most realistic situations.
The main objective of this work is to develop
a more efficient discourse parser, with similar or
even better performance with respect to Joty et
al.?s optimal parser, but able to produce parsing re-
sults in real time.
Our contribution is three-fold. First, with a
511
[On Aug. 1, the state tore up its controls,]e
1
[and food prices leaped]e
2
[Without buffer
stocks,]e
3
[inflation exploded.]e
4
wsj 1146
e1 e2
consequence
e1:4
e3 e4
circumstance
sequence
e1:2 e3:4
Figure 1: An example text fragment composed of
two sentences and four EDUs, with its RST dis-
course tree representation shown below.
greedy bottom-up strategy, we develop a discourse
parser with a time complexity linear in the total
number of sentences in the document. As a re-
sult of successfully avoiding the expensive non-
greedy parsing algorithms, our discourse parser is
very efficient in practice. Second, by using two
linear-chain CRFs to label a sequence of discourse
constituents, we can incorporate contextual infor-
mation in a more natural way, compared to us-
ing traditional discriminative classifiers, such as
SVMs. Specifically, in the Viterbi decoding of
the first CRF, we include additional constraints
elicited from common sense, to make more ef-
fective local decisions. Third, after a discourse
(sub)tree is fully built from bottom up, we perform
a novel post-editing process by considering infor-
mation from the constituents on upper levels. We
show that this post-editing can further improve the
overall parsing performance.
2 Related work
2.1 HILDA discourse parser
The HILDA discourse parser by Hernault et al
(2010) is the first attempt at RST-style text-level
discourse parsing. It adopts a pipeline framework,
and greedily builds the discourse tree from the bot-
tom up. In particular, starting from EDUs, at each
step of the tree-building, a binary SVM classifier
is first applied to determine which pair of adjacent
discourse constituents should be merged to form a
larger span, and another multi-class SVM classi-
fier is then applied to assign the type of discourse
relation that holds between the chosen pair.
The strength of HILDA?s greedy tree-building
strategy is its efficiency in practice. Also, the em-
ployment of SVM classifiers allows the incorpora-
tion of rich features for better data representation
(Feng and Hirst, 2012). However, HILDA?s ap-
proach also has obvious weakness: the greedy al-
gorithm may lead to poor performance due to local
optima, and more importantly, the SVM classifiers
are not well-suited for solving structural problems
due to the difficulty of taking context into account.
2.2 Joty et al?s joint model
Joty et al (2013) approach the problem of text-
level discourse parsing using a model trained by
Conditional Random Fields (CRF). Their model
has two distinct features.
First, they decomposed the problem of text-
level discourse parsing into two stages: intra-
sentential parsing to produce a discourse tree for
each sentence, followed by multi-sentential pars-
ing to combine the sentence-level discourse trees
and produce the text-level discourse tree. Specif-
ically, they employed two separate models for
intra- and multi-sentential parsing. Their choice
of two-stage parsing is well motivated for two rea-
sons: (1) it has been shown that sentence bound-
aries correlate very well with discourse bound-
aries, and (2) the scalability issue of their CRF-
based models can be overcome by this decompo-
sition.
Second, they jointly modeled the structure and
the relation for a given pair of discourse units.
For example, Figure 2 shows their intra-sentential
model, in which they use the bottom layer to rep-
resent discourse units; the middle layer of binary
nodes to predict the connection of adjacent dis-
course units; and the top layer of multi-class nodes
to predict the type of the relation between two
units. Their model assigns a probability to each
possible constituent, and a CKY-like parsing al-
gorithm finds the globally optimal discourse tree,
given the computed probabilities.
The strength of Joty et al?s model is their joint
modeling of the structure and the relation, such
that information from each aspect can interact with
the other. However, their model has a major defect
in its inefficiency, or even infeasibility, for appli-
cation in practice. The inefficiency lies in both
their DCRF-based joint model, on which infer-
ence is usually slow, and their CKY-like parsing
algorithm, whose issue is more prominent. Due to
the O(n
3
) time complexity, where n is the number
512
R2
S2
U2U1
R3
S3
U3
Rj
Sj
Uj
Rt-1
St-1
Ut-1
Relation sequence
Structure sequence
Unit sequence at level i
Figure 2: Joty et al (2013)?s intra-sentential Con-
dition Random Fields.
of input discourse units, for large documents, the
parsing simply takes too long
1
.
3 Overall work flow
Figure 3 demonstrates the overall work flow of our
discourse parser. The general idea is that, similar
to Joty et al (2013), we perform a sentence-level
parsing for each sentence first, followed by a text-
level parsing to generate a full discourse tree for
the whole document. However, in addition to effi-
ciency (to be shown in Section 6), our discourse
parser has a distinct feature, which is the post-
editing component (to be introduced in Section 5),
as outlined in dashes.
Our discourse parser works as follows. A doc-
ument D is first segmented into a list of sen-
tences. Each sentence S
i
, after being segmented
into EDUs (not shown in the figure), goes through
an intra-sentential bottom-up tree-building model
M
intra
, to form a sentence-level discourse tree T
S
i
,
with the EDUs as leaf nodes. After that, we ap-
ply the intra-sentential post-editing model P
intra
to
modify the generated tree T
S
i
to T
p
S
i
, by considering
upper-level information.
We then combine all sentence-level discourse
tree T
p
S
i
?s using our multi-sentential bottom-up
tree-building model M
multi
to generate the text-
level discourse tree T
D
. Similar to sentence-level
parsing, we also post-edit T
D
using P
multi
to pro-
duce the final discourse tree T
p
D
.
1
The largest document in the RST-DT contains over 180
sentences, i.e., n > 180 for their multi-sentential CKY pars-
ing. Intuitively, suppose the average time to compute the
probability of each constituent is 0.01 second, then in total,
the CKY-like parsing takes over 16 hours. It is possible to op-
timize Joty et al?s CKY-like parsing by replacing their CRF-
based computation for upper-level constituents with some lo-
cal computation based on the probabilities of lower-level con-
stituents. However, such optimization is beyond the scope of
this paper.
4 Bottom-up tree-building
For both intra- and multi-sentential parsing, our
bottom-up tree-building process adopts a similar
greedy pipeline framework like the HILDA dis-
course parser (discussed in Section 2.1), to guar-
antee efficiency for large documents. In partic-
ular, starting from the constituents on the bot-
tom level (EDUs for intra-sentential parsing and
sentence-level discourse trees for multi-sentential
parsing), at each step of the tree-building, we
greedily merge a pair of adjacent discourse con-
stituents such that the merged constituent has the
highest probability as predicted by our structure
model. The relation model is then applied to as-
sign the relation to the new constituent.
4.1 Linear-chain CRFs as Local models
Now we describe the local models we use to make
decisions for a given pair of adjacent discourse
constituents in the bottom-up tree-building. There
are two dimensions for our local models: (1) scope
of the model: intra- or multi-sentential, and (2)
purpose of the model: for determining structures
or relations. So we have four local models, M
struct
intra
,
M
rel
intra
, M
struct
multi
, and M
rel
multi
.
While our bottom-up tree-building shares the
greedy framework with HILDA, unlike HILDA,
our local models are implemented using CRFs.
In this way, we are able to take into account the
sequential information from contextual discourse
constituents, which cannot be naturally repre-
sented in HILDA with SVMs as local classifiers.
Therefore, our model incorporates the strengths
of both HILDA and Joty et al?s model, i.e., the
efficiency of a greedy parsing algorithm, and the
ability to incorporate sequential information with
CRFs.
As shown by Feng and Hirst (2012), for a pair
of discourse constituents of interest, the sequential
information from contextual constituents is cru-
cial for determining structures. Therefore, it is
well motivated to use Conditional Random Fields
(CRFs) (Lafferty et al, 2001), which is a discrimi-
native probabilistic graphical model, to make pre-
dictions for a sequence of constituents surround-
ing the pair of interest.
In this sense, our local models appear similar
to Joty et al?s non-greedy parsing models. How-
ever, the major distinction between our models
and theirs is that we do not jointly model the struc-
ture and the relation; rather, we use two linear-
513
DS1
Si
Sn
...
...
Mintra
Mintrai
Mintra
Pintra...
... ...
Pintra
Pintra
Pmulti
Mmulti
...
...
1ST iST nST pSnT pS iT
pST 1 DT pDT
Figure 3: The work flow of our proposed discourse parser. In the figure, M
intra
and M
multi
stand for the
intra- and multi-sentential bottom-up tree-building models, and P
intra
and P
multi
stand for the intra- and
multi-sentential post-editing models.
chain CRFs to model the structure and the relation
separately. Although joint modeling has shown to
be effective in various NLP and computer vision
applications (Sutton et al, 2007; Yang et al, 2009;
Wojek and Schiele, 2008), our choice of using two
separate models is for the following reasons:
First, it is not entirely appropriate to model the
structure and the relation at the same time. For
example, with respect to Figure 2, it is unclear
how the relation node R
j
is represented for a train-
ing instance whose structure node S
j
= 0, i.e., the
units U
j?1
and U
j
are disjoint. Assume a special
relation NO-REL is assigned for R
j
. Then, in the
tree-building process, we will have to deal with the
situations where the joint model yields conflicting
predictions: it is possible that the model predicts
S
j
= 1 and R
j
= NO-REL, or vice versa, and we
will have to decide which node to trust (and thus
in some sense, the structure and the relation is no
longer jointly modeled).
Secondly, as a joint model, it is mandatory to
use a dynamic CRF, for which exact inference is
usually intractable or slow. In contrast, for linear-
chain CRFs, efficient algorithms and implementa-
tions for exact inference exist.
4.2 Structure models
4.2.1 Intra-sentential structure model
Figure 4a shows our intra-sentential structure
model M
struct
intra
in the form of a linear-chain CRF.
Similar to Joty et al?s intra-sentential model, the
first layer of the chain is composed of discourse
constituents U
j
?s, and the second layer is com-
posed of binary nodes S
j
?s to indicate the proba-
bility of merging adjacent discourse constituents.
S2
U2U1
S3
U3
Sj
Uj
St
Ut
Structure 
sequence
A ll units in 
sentence 
at level i
(a) Intra-sentential structure model M
struct
intra
.
Sj-1
Uj-1Uj-2
Sj
Uj
Structure 
sequence
A d jacent 
units  at 
level i
Uj+ 1
Sj+ 1Sj-1
Uj-3
Sj+ 2
Uj+ 2
C1 C
2
C3
(b) Multi-sentential structure model M
struct
multi
. C
1
, C
2
, and C
3
denote the three chains for predicting U
j
and U
j+1
.
Figure 4: Local structure models.
At each step in the bottom-up tree-building pro-
cess, we generate a single sequence E, consisting
of U
1
,U
2
, . . . ,U
j
, . . . ,U
t
, which are all the current
discourse constituents in the sentence that need
to be processed. For instance, initially, we have
the sequence E
1
= {e
1
,e
2
, . . . ,e
m
}, which are the
EDUs of the sentence; after merging e
1
and e
2
on
the second level, we have E
2
= {e
1:2
,e
3
, . . . ,e
m
};
after merging e
4
and e
5
on the third level, we have
E
3
= {e
1:2
,e
3
,e
4:5
, . . . ,e
m
}, and so on.
Because the structure model is the first com-
ponent in our pipeline of local models, its accu-
racy is crucial. Therefore, to improve its accuracy,
we enforce additional commonsense constraints in
its Viterbi decoding. In particular, we disallow 1-
1 transitions between adjacent labels (a discourse
unit can be merged with at most one adjacent unit),
and we disallow all-zero sequences (at least one
514
pair must be merged).
Since the computation of E
i
does not depend
on a particular pair of constituents, we can use the
same sequence E
i
to compute structural probabili-
ties for all adjacent constituents. In contrast, Joty
et al?s computation of intra-sentential sequences
depends on the particular pair of constituents: the
sequence is composed of the pair in question, with
other EDUs in the sentence, even if those EDUs
have already been merged. Thus, different CRF
chains have to be formed for different pairs of con-
stituents. In addition to efficiency, our use of a
single CRF chain for all constituents can better
capture the sequential dependencies among con-
text, by taking into account the information from
partially built discourse constituents, rather than
bottom-level EDUs only.
4.2.2 Multi-sentential structure model
For multi-sentential parsing, where the smallest
discourse units are single sentences, as argued by
Joty et al (2013), it is not feasible to use a long
chain to represent all constituents, due to the fact
that it takes O(TM
2
) time to perform the forward-
backward exact inference on a chain with T units
and an output vocabulary size of M, thus the over-
all complexity for all possible sequences in their
model is O(M
2
n
3
)
2
.
Instead, we choose to take a sliding-window
approach to form CRF chains for a particular pair
of constituents, as shown in Figure 4b. For exam-
ple, suppose we wish to compute the structural
probability for the pairU
j?1
andU
j
, we form three
chains, each of which contains two contextual
constituents: C
1
= {U
j?3
,U
j?2
,U
j?1
,U
j
},
C
2
= {U
j?2
,U
j?1
,U
j
,U
j+1
}, and C
3
=
{U
j?1
,U
j
,U
j+1
,U
j+2
}. We then find the chain
C
t
,1 ? t ? 3, with the highest joint probability
over the entire sequence, and assign its marginal
probability P(S
t
j
= 1) to P(S
j
= 1).
Similar to M
struct
intra
, for M
struct
multi
, we also include
additional constraints in the Viterbi decoding, by
disallowing transitions between two ones, and dis-
allowing the sequence to be all zeros if it contains
all the remaining constituents in the document.
4.3 Relation models
4.3.1 Intra-sentential relation model
The intra-sentential relation model M
rel
intra
, shown
in Figure 5a, works in a similar way to M
struct
intra
, as
2
The time complexity will be reduced to O(M
2
n
2
), if we
use the same chain for all constituents as in our M
struct
intra
.
described in Section 4.2.1. The linear-chain CRF
contains a first layer of all discourse constituents
U
j
?s in the sentence on level i, and a second layer
of relation nodes R
j
?s to represent the relation be-
tween a pair of discourse constituents.
However, unlike the structure model, adjacent
relation nodes do not share discourse constituents
on the first layer. Rather, each relation node R
j
attempts to model the relation of one single con-
stituent U
j
, by taking U
j
?s left and right subtrees
U
j,L
and U
j,R
as its first-layer nodes; if U
j
is a sin-
gle EDU, then the first-layer node of R
j
is simply
U
j
, and R
j
is a special relation symbol LEAF
3
.
Since we know, a priori, that the constituents in the
chains are either leaf nodes or the ones that have
been merged by our structure model, we never
need to worry about the NO-REL issue as out-
lined in Section 4.1.
In the bottom-up tree-building process, after
merging a pair of adjacent constituents using
M
struct
intra
into a new constituent, say U
j
, we form a
chain consisting of all current constituents in the
sentence to decide the relation label for U
j
, i.e.,
the R
j
node in the chain. In fact, by perform-
ing inference on this chain, we produce predic-
tions not only for R
j
, but also for all other R nodes
in the chain, which correspond to all other con-
stituents in the sentence. Since those non-leaf con-
stituents are already labeled in previous steps in
the tree-building, we can now re-assign their rela-
tions if the model predicts differently in this step.
Therefore, this re-labeling procedure can compen-
sate for the loss of accuracy caused by our greedy
bottom-up strategy to some extent.
4.3.2 Multi-sentential relation model
Figure 5b shows our multi-sentential relation
model. Like M
rel
intra
, the first layer consists of adja-
cent discourse units, and the relation nodes on the
second layer model the relation of each constituent
separately.
Similar to M
struct
multi
introduced in Section 4.2.2,
M
rel
multi
also takes a sliding-window approach to
predict labels for constituents in a local context.
For a constituent U
j
to be predicted, we form three
chains, and use the chain with the highest joint
probability to assign or re-assign relations to con-
stituents in that chain.
3
These leaf constituents are represented using a special
feature vector is leaf = True; thus the CRF never labels
them with relations other than LEAF.
515
Relation sequence
A ll units in sentence at level i
R1
U1, RU1, L
R2
U2
Rj
Uj, RUj, L
Rt
Ut, RUt, L
(a) Intra-sentential relation model M
rel
intra
.
Relation sequence
A d jacent units  at level i
R1
Uj-2, RUj-2, L
Rj-1
Uj-1
Rj
Uj, RUj, L
Rj+ 1
Uj+ 1, RUj+ 1, L
Rj+ 2
Uj+ 2
C1 C
2
C3
(b) Multi-sentential relation model M
rel
multi
. C
1
, C
2
, and C
3
denote the three sliding windows for predictingU
j,L
andU
j,R
.
Figure 5: Local relation models.
5 Post-editing
After an intra- or multi-sentential discourse tree
is fully built, we perform a post-editing to con-
sider possible modifications to the current tree, by
considering useful information from the discourse
constituents on upper levels, which is unavailable
in the bottom-up tree-building process.
The motivation for post-editing is that, some
particular discourse relations, such as TEXTUAL-
ORGANIZATION, tend to occur on the top levels
of the discourse tree; thus, information such as the
depth of the discourse constituent can be quite in-
dicative. However, the exact depth of a discourse
constituent is usually unknown in the bottom-up
tree-building process; therefore, it might be ben-
eficial to modify the tree by including top-down
information after the tree is fully built.
The process of post-editing is shown in Algo-
rithm 1. For each input discourse tree T , which
is already fully built by bottom-up tree-building
models, we do the following:
Lines 3 ? 9: Identify the lowest level of T on
which the constituents can be modified according
to the post-editing structure component, P
struct
. To
do so, we maintain a list L to store the discourse
constituents that need to be examined. Initially, L
consists of all the bottom-level constituents in T .
At each step of the loop, we consider merging the
pair of adjacent units in L with the highest proba-
bility predicted by P
struct
. If the predicted pair is
not merged in the original tree T , then a possible
modification is located; otherwise, we merge the
pair, and proceed to the next iteration.
Lines 10 ? 12: If modifications have been pro-
posed in the previous step, we build a new tree
Algorithm 1 Post-editing algorithm.
Input: A fully built discourse tree T .
1: if |T |= 1 then
2: return T . Do nothing if it is a single
EDU.
3: L? [U
1
,U
2
, . . . ,U
t
] . The bottom-level
constituents in T .
4: while |L|> 2 do
5: i? PREDICTMERGING(L,P
struct
)
6: p? PARENT(L[i],L[i+1],T )
7: if p = NULL then
8: break
9: Replace L[i] and L[i+1] with p
10: if |L|= 2 then
11: L? [U
1
,U
2
, . . . ,U
t
]
12: T
p
? BUILDTREE(L,P
struct
,P
rel
,T )
Output: T
p
T
p
using P
struct
as the structure model, and P
rel
as the relation model, from the constituents on
which modifications are proposed. Otherwise, T
p
is built from the bottom-level constituents of T .
The upper-level information, such as the depth of
a discourse constituent, is derived from the initial
tree T .
5.1 Local models
The local models, P
{struct|rel}
{intra|multi}
, for post-editing
is almost identical to their counterparts of the
bottom-up tree-building, except that the linear-
chain CRFs in post-editing includes additional
features to represent information from constituents
on higher levels (to be introduced in Section 7).
6 Linear time complexity
Here we analyze the time complexity of each com-
ponent in our discourse parser, to quantitatively
demonstrate the time efficiency of our model. The
following analysis is focused on the bottom-up
tree-building process, but a similar analysis can be
carried out for the post-editing process. Since the
number of operations in the post-editing process is
roughly the same (1.5 times in the worst case) as
in the bottom-up tree-building, post-editing shares
the same complexity as the tree-building.
6.1 Intra-sentential parsing
Suppose the input document is segmented into
n sentences, and each sentence S
k
contains m
k
EDUs. For each sentence S
k
with m
k
EDUs, the
516
overall time complexity to perform intra-sentential
parsing is O(m
2
k
). The reason is the following. On
level i of the bottom-up tree-building, we generate
a single chain to represent the structure or relation
for all the m
k
? i constituents that are currently in
the sentence. The time complexity for performing
forward-backward inference on the single chain is
O((m
k
? i)?M
2
) =O(m
k
? i), where the constant
M is the size of the output vocabulary. Starting
from the EDUs on the bottom level, we need to
perform inference for one chain on each level dur-
ing the bottom-up tree-building, and thus the total
time complexity is ?
m
k
i=1
O(m
k
? i) = O(m
2
k
).
The total time to generate sentence-level dis-
course trees for n sentences is ?
n
k=1
O(m
2
k
). It is
fairly safe to assume that each m
k
is a constant,
in the sense that m
k
is independent of the total
number of sentences in the document. There-
fore, the total time complexity ?
n
k=1
O(m
2
k
) ? n?
O(max
1? j?n
(m
2
j
)) = n?O(1) = O(n), i.e., linear
in the total number of sentences.
6.2 Multi-sentential parsing
For multi-sentential models, M
struct
multi
and M
rel
multi
, as
shown in Figures 4b and 5b, for a pair of con-
stituents of interest, we generate multiple chains
to predict the structure or the relation.
By including a constant number k of discourse
units in each chain, and considering a constant
number l of such chains for computing each ad-
jacent pair of discourse constituents (k = 4 for
M
struct
multi
and k = 3 for M
rel
multi
; l = 3), we have an
overall time complexity of O(n). The reason is
that it takes l?O(kM
2
) =O(1) time, where l,k,M
are all constants, to perform exact inference for a
given pair of adjacent constituents, and we need
to perform such computation for all n?1 pairs of
adjacent sentences on the first level of the tree-
building. Adopting a greedy approach, on an ar-
bitrary level during the tree-building, once we de-
cide to merge a certain pair of constituents, say
U
j
and U
j+1
, we only need to recompute a small
number of chains, i.e., the chains which originally
include U
j
or U
j+1
, and inference on each chain
takes O(1). Therefore, the total time complexity
is (n?1)?O(1)+(n?1)?O(1) = O(n), where
the first term in the summation is the complexity
of computing all chains on the bottom level, and
the second term is the complexity of computing
the constant number of chains on higher levels.
We have thus showed that the time complexity
is linear in n, which is the number of sentences in
the document. In fact, under the assumption that
the number of EDUs in each sentence is indepen-
dent of n, it can be shown that the time complexity
is also linear in the total number of EDUs
4
.
7 Features
In our local models, to encode two adjacent units,
U
j
and U
j+1
, within a CRF chain, we use the fol-
lowing 10 sets of features, some of which are mod-
ified from Joty et al?s model.
Organization features: WhetherU
j
(orU
j+1
) is
the first (or last) constituent in the sentence (for
intra-sentential models) or in the document (for
multi-sentential models); whether U
j
(or U
j+1
) is
a bottom-level constituent.
Textual structure features: Whether U
j
con-
tains more sentences (or paragraphs) than U
j+1
.
N-gram features: The beginning (or end) lexi-
cal n-grams in each unit; the beginning (or end)
POS n-grams in each unit, where n ? {1,2,3}.
Dominance features: The PoS tags of the head
node and the attachment node; the lexical heads of
the head node and the attachment node; the domi-
nance relationship between the two units.
Contextual features: The feature vector of the
previous and the next constituent in the chain.
Substructure features: The root node of the left
and right discourse subtrees of each unit.
Syntactic features: whether each unit corre-
sponds to a single syntactic subtree, and if so, the
top PoS tag of the subtree; the distance of each
unit to their lowest common ancestor in the syntax
tree (intra-sentential only).
Entity transition features: The type and the
number of entity transitions across the two units.
We adopt Barzilay and Lapata (2008)?s entity-
based local coherence model to represent a doc-
ument by an entity grid, and extract local transi-
tions among entities in continuous discourse con-
stituents. We use bigram and trigram transitions
with syntactic roles attached to each entity.
4
We implicitly made an assumption that the parsing time
is dominated by the time to perform inference on CRF chains.
However, for complex features, the time required for fea-
ture computation might be dominant. Nevertheless, a care-
ful caching strategy can accelerate feature computation, since
a large number of multi-sentential chains overlap with each
other.
517
Cue phrase features: Whether a cue phrase oc-
curs in the first or last EDU of each unit. The cue
phrase list is based on the connectives collected by
Knott and Dale (1994)
Post-editing features: The depth of each unit in
the initial tree.
8 Experiments
For pre-processing, we use the Stanford CoreNLP
(Klein and Manning, 2003; de Marneffe et al,
2006; Recasens et al, 2013) to syntactically parse
the texts and extract coreference relations, and we
use Penn2Malt
5
to lexicalize syntactic trees to ex-
tract dominance features.
For local models, our structure models are
trained using MALLET (McCallum, 2002) to in-
clude constraints over transitions between adja-
cent labels, and our relation models are trained
using CRFSuite (Okazaki, 2007), which is a fast
implementation of linear-chain CRFs.
The data that we use to develop and evaluate
our discourse parser is the RST Discourse Tree-
bank (RST-DT) (Carlson et al, 2001), which is a
large corpus annotated in the framework of RST.
The RST-DT consists of 385 documents (347 for
training and 38 for testing) from the Wall Street
Journal. Following previous work on the RST-DT
(Hernault et al, 2010; Feng and Hirst, 2012; Joty
et al, 2012; Joty et al, 2013), we use 18 coarse-
grained relation classes, and with nuclearity at-
tached, we have a total set of 41 distinct relations.
Non-binary relations are converted into a cascade
of right-branching binary relations.
9 Results and Discussion
9.1 Parsing accuracy
We compare four different models using manual
EDU segmentation. In Table 1, the jCRF model
in the first row is the optimal CRF model proposed
by Joty et al (2013). gSVM
FH
in the second row
is our implementation of HILDA?s greedy parsing
algorithm using Feng and Hirst (2012)?s enhanced
feature set. The third model, gCRF, represents our
greedy CRF-based discourse parser, and the last
row, gCRF
PE
, represents our parser with the post-
editing component included.
In order to conduct a direct comparison with
Joty et al?s model, we use the same set of eval-
5
http://stp.lingfil.uu.se/
?
nivre/
research/Penn2Malt.html.
Model Span Nuc Relation
Acc MAFS
jCRF 82.5 68.4 55.7 N/A
gSVM
FH
82.8 67.1 52.0 27.4/23.3
gCRF 84.9
?
69.9
?
57.2
?
35.3/31.3
gCRF
PE
85.7
??
71.0
??
58.2
??
36.2/32.3
Human 88.7 77.7 65.8 N/A
?: significantly better than gSVM
FH
(p < .01)
?: significantly better than gCRF (p < .01)
Table 1: Performance of different models using
gold-standard EDU segmentation, evaluated us-
ing the constituent accuracy (%) for span, nucle-
arity, and relation. For relation, we also report the
macro-averaged F1-score (MAFS) for correctly
retrieved constituents (before the slash) and for
all constituents (after the slash). Statistical sig-
nificance is verified using Wilcoxon?s signed-rank
test.
uation metrics, i.e., the unlabeled and labeled pre-
cision, recall, and F-score
6
as defined by Marcu
(2000). For evaluating relations, since there is a
skewed distribution of different relation types in
the corpus, we also include the macro-averaged
F1-score (MAFS)
7
as another metric, to empha-
size the performance of infrequent relation types.
We report the MAFS separately for the correctly
retrieved constituents (i.e., the span boundary is
correct) and all constituents in the reference tree.
As demonstrated by Table 1, our greedy CRF
models perform significantly better than the other
two models. Since we do not have the actual out-
put of Joty et al?s model, we are unable to con-
duct significance testing between our models and
theirs. But in terms of overall accuracy, our gCRF
model outperforms their model by 1.5%. More-
over, with post-editing enabled, gCRF
PE
signif-
icantly (p < .01) outperforms our initial model
gCRF by another 1% in relation assignment, and
this overall accuracy of 58.2% is close to 90% of
human performance. With respect to the macro-
averaged F1-scores, adding the post-editing com-
ponent also obtains about 1% improvement.
However, the overall MAFS is still at the lower
6
For manual segmentation, precision, recall, and F-score
are the same.
7
MAFS is the F1-score averaged among all relation
classes by equally weighting each class. Therefore, we can-
not conduct significance test between different MAFS.
518
Avg Min Max
# of EDUs 61.74 4 304
# of Sentences 26.11 2 187
# of EDUs per sentence 2.36 1 10
Table 2: Characteristics of the 38 documents in the
test data.
end of 30% for all constituents. Our error anal-
ysis shows that, for two relation classes, TOPIC-
CHANGE and TEXTUAL-ORGANIZATION, our
model fails to retrieve any instance, and for
TOPIC-COMMENT and EVALUATION, our model
scores a class-wise F
1
score lower than 5%. These
four relation classes, apart from their infrequency
in the corpus, are more abstractly defined, and thus
are particularly challenging.
9.2 Parsing efficiency
We further illustrate the efficiency of our parser by
demonstrating the time consumption of different
models.
First, as shown in Table 2, the average number
of sentences in a document is 26.11, which is al-
ready too large for optimal parsing models, e.g.,
the CKY-like parsing algorithm in jCRF, let alne
the fact that the largest document contains sev-
eral hundred of EDUs and sentences. Therefore,
it should be seen that non-optimal models are re-
quired in most cases.
In Table 3, we report the parsing time
8
for the
last three models, since we do not know the time of
jCRF. Note that the parsing time excludes the time
cost for any necessary pre-processing. As can be
seen, our gCRF model is considerably faster than
gSVM
FH
, because, on one hand, feature compu-
tation is expensive in gSVM
FH
, since gSVM
FH
utilizes a rich set of features; on the other hand,
in gCRF, we are able to accelerate decoding by
multi-threading MALLET (we use four threads).
Even for the largest document with 187 sentences,
gCRF is able to produce the final tree after about
40 seconds, while jCRF would take over 16 hours
assuming each DCRF decoding takes only 0.01
second. Although enabling post-editing doubles
the time consumption, the overall time is still ac-
ceptable in practice, and the loss of efficiency can
be compensated by the improvement in accuracy.
8
Tested on a Linux system with four duo-core 3.0GHz
processors and 16G memory.
Model Parsing Time (seconds)
Avg Min Max
gSVM
FH
11.19 0.42 124.86
gCRF 5.52 0.05 40.57
gCRF
PE
10.71 0.12 84.72
Table 3: The parsing time (in seconds) for the 38
documents in the test set of RST-DT. Time cost of
any pre-processing is excluded from the analysis.
10 Conclusions
In this paper, we presented an efficient text-level
discourse parser with time complexity linear in
the total number of sentences in the document.
Our approach was to adopt a greedy bottom-
up tree-building, with two linear-chain CRFs as
local probabilistic models, and enforce reason-
able constraints in the first CRF?s Viterbi decod-
ing. While significantly outperforming the state-
of-the-art model by Joty et al (2013), our parser
is much faster in practice. In addition, we pro-
pose a novel idea of post-editing, which modifies a
fully-built discourse tree by considering informa-
tion from upper-level constituents. We show that,
although doubling the time consumption, post-
editing can further boost the parsing performance
to close to 90% of human performance.
In future work, we wish to further explore the
idea of post-editing, since currently we use only
the depth of the subtrees as upper-level informa-
tion. Moreover, we wish to study whether we can
incorporate constraints into the relation models, as
we do to the structure models. For example, it
might be helpful to train the relation models us-
ing additional criteria, such as Generalized Ex-
pectation (Mann and McCallum, 2008), to better
take into account some prior knowledge about the
relations. Last but not least, as reflected by the
low MAFS in our experiments, some particularly
difficult relation types might need specifically de-
signed features for better recognition.
Acknowledgments
We thank Professor Gerald Penn and the review-
ers for their valuable advice and comments. This
work was financially supported by the Natural
Sciences and Engineering Research Council of
Canada and by the University of Toronto.
519
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 96?103, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure The-
ory. In Proceedings of Second SIGDial Workshop
on Discourse and Dialogue (SIGDial 2001), pages
1?10.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL 2012), pages 60?68, Jeju,
Korea.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification.
Dialogue and Discourse, 1(3):1?33.
Shafiq Joty, Giuseppe Carenini, and Raymond T.
Ng. 2012. A novel discriminative framework
for sentence-level discourse analysis. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL 2012, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 486?496, Sofia, Bul-
garia, August.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), ACL 2003, pages
423?430, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alistair Knott and Robert Dale. 1994. Using linguistic
phenomena to motivate a set of coherence relations.
Discourse Processes, 18(1):35?64.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
2001, pages 282?289, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Gideon S. Mann and Andrew McCallum. 2008. Gen-
eralized Expectation Criteria for semi-supervised
learning of Conditional Random Fields. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies (ACL 2008), pages 870?878, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
William Mann and Sandra Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of
text organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained decoding for
text-level discourse parsing. In Proceedings of
COLING 2012, pages 1883?1900, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627?633, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 566?574, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. The Journal of Ma-
chine Learning Research, 8:693?723, May.
Christian Wojek and Bernt Schiele. 2008. A dynamic
conditional random field model for joint labeling of
object and scene classes. In European Conference
520
on Computer Vision (ECCV 2008), pages 733?747,
Marseille, France.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura, and Sadaoki Furui.
2009. Combining a two-step conditional random
field model and a joint source channel model for
machine transliteration. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72?75, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
521
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531?537,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Applying a Naive Bayes Similarity Measure to
Word Sense Disambiguation
Tong Wang
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
University of Toronto
gh@cs.toronto.edu
Abstract
We replace the overlap mechanism of the
Lesk algorithm with a simple, general-
purpose Naive Bayes model that mea-
sures many-to-many association between
two sets of random variables. Even with
simple probability estimates such as max-
imum likelihood, the model gains signifi-
cant improvement over the Lesk algorithm
on word sense disambiguation tasks. With
additional lexical knowledge from Word-
Net, performance is further improved to
surpass the state-of-the-art results.
1 Introduction
To disambiguate a homonymous word in a given
context, Lesk (1986) proposed a method that mea-
sured the degree of overlap between the glosses
of the target and context words. Known as the
Lesk algorithm, this simple and intuitive method
has since been extensively cited and extended in
the word sense disambiguation (WSD) commu-
nity. Nonetheless, its performance in several WSD
benchmarks is less than satisfactory (Kilgarriff
and Rosenzweig, 2000; Vasilescu et al, 2004).
Among the popular explanations is a key limita-
tion of the algorithm, that ?Lesk?s approach is very
sensitive to the exact wording of definitions, so the
absence of a certain word can radically change the
results.? (Navigli, 2009).
Compounding this problem is the fact that many
Lesk variants limited the concept of overlap to
the literal interpretation of string matching (with
their own variants such as length-sensitive match-
ing (Banerjee and Pedersen, 2002), etc.), and it
was not until recently that overlap started to take
on other forms such as tree-matching (Chen et al,
2009) and vector space models (Abdalgader and
Skabar, 2012; Raviv et al, 2012; Patwardhan and
Pedersen, 2006). To address this limitation, a
Naive Bayes model (NBM) is proposed in this
study as a novel, probabilistic treatment of over-
lap in gloss-based WSD.
2 Related Work
In the extraordinarily rich literature on WSD, we
focus our review on those closest to the topic of
Lesk and NBM. In particular, we opt for the ?sim-
plified Lesk? (Kilgarriff and Rosenzweig, 2000),
where inventory senses are assessed by gloss-
context overlap rather than gloss-gloss overlap.
This particular variant prevents proliferation of
gloss comparison on larger contexts (Mihalcea
et al, 2004) and is shown to outperform the origi-
nal Lesk algorithm (Vasilescu et al, 2004).
To the best of our knowledge, NBMs have been
employed exclusively as classifiers in WSD ?
that is, in contrast to their use as a similarity mea-
sure in this study. Gale et al (1992) used NB
classifier resembling an information retrieval sys-
tem: a WSD instance is regarded as a document d,
and candidate senses are scored in terms of ?rel-
evance? to d. When evaluated on a WSD bench-
mark (Vasilescu et al, 2004), the algorithm com-
pared favourably to Lesk variants (as expected
for a supervised method). Pedersen (2000) pro-
posed an ensemble model with multiple NB clas-
sifiers differing by context window size. Hristea
(2009) trained an unsupervised NB classifier using
the EM algorithm and empirically demonstrated
the benefits of WordNet-assisted (Fellbaum, 1998)
feature selection over local syntactic features.
Among Lesk variants, Banerjee and Pedersen
(2002) extended the gloss of both inventory senses
and the context words to include words in their re-
lated synsets in WordNet. Senses were scored by
the sum of overlaps across all relation pairs, and
the effect of individual relation pairs was evalu-
ated in a later work (Banerjee and Pedersen, 2003).
Overlap was assessed by string matching, with the
number of matching words squared so as to assign
531
higher scores to multi-word overlaps.
Breaking away from string matching, Wilks
et al (1990) measured overlap as similarity be-
tween gloss- and context-vectors, which were ag-
gregated word vectors encoding second order co-
occurrence information in glosses. An extension
by Patwardhan and Pedersen (2006) differentiated
context word senses and extended shorter glosses
with related glosses in WordNet. Patwardhan et al
(2003) measured overlap by concept similarity
(Budanitsky and Hirst, 2006) between each inven-
tory sense and the context words. Gloss overlaps
from their earlier work actually out-performed all
five similarity-based methods.
More recently, Chen et al (2009) pro-
posed a tree-matching algorithm that measured
gloss-context overlap as the weighted sum of
dependency-induced lexical distance. Abdalgader
and Skabar (2012) constructed a sentential simi-
larity measure (Li et al, 2006) using lexical simi-
larity measures (Budanitsky and Hirst, 2006), and
overlap was measured by the cosine of their re-
spective sentential vectors. A related approach
(Raviv et al, 2012) also used Wikipedia-induced
concepts to encoded sentential vectors. These sys-
tems compared favourably to existing methods in
WSD performance, although by using sense fre-
quency information, they are essentially super-
vised methods.
Distributional methods have been used in many
WSD systems in quite different flavours than the
current study. Kilgarriff and Rosenzweig (2000)
proposed a Lesk variant where each gloss word is
weighted by its idf score in relation to all glosses,
and gloss-context association was incremented by
these weights rather than binary, overlap counts.
Miller et al (2012) used distributional thesauri as a
knowledge base to increase overlaps, which were,
again, assessed by string matching.
In conclusion, the majority of Lesk variants
focused on extending the gloss to increase the
chance of overlapping, while the proposed NBM
aims to make better use of the limited lexical
knowledge available. In contrast to string match-
ing, the probabilistic nature of our model offers
a ?softer? measurement of gloss-context associa-
tion, resulting in a novel approach to unsupervised
WSD with state-of-the-art performance in more
than one WSD benchmark (Section 4).
3 Model and Task Descriptions
3.1 The Naive Bayes Model
Formally, given two sets e = {e
i
} and f = { f
j
}
each consisting of multiple random events, the
proposed model measures the probabilistic asso-
ciation p(f|e) between e and f. Under the assump-
tion of conditional independence among the events
in each set, a Naive Bayes treatment of the mea-
sure can be formulated as:
p(f|e) =
?
j
p( f
j
|{e
i
}) =
?
j
p({e
i
}| f
j
)p( f
j
)
p({e
i
})
=
?
j
[p( f
j
)
?
i
p(e
i
| f
j
)]
?
j
?
i
p(e
i
)
,
(1)
In the second expression, Bayes?s rule is applied
not only to take advantage of the conditional inde-
pendence among e
i
?s, but also to facilitate proba-
bility estimation, since p({e
i
}| f
j
) is easier to esti-
mate in the context of WSD, where sample spaces
of e and f become asymmetric (Section 3.2).
3.2 Model Application in WSD
In the context of WSD, e can be regarded as an
instance of a polysemous word w, while f repre-
sents certain lexical knowledge about the sense s
of w manifested by e.
1
WSD is thus formulated as
identifying the sense s
?
in the sense inventory S
of w s.t.:
s
?
= argmax
s?S
p(f|e) (2)
In one of their simplest forms, e
i
?s correspond
to co-occurring words in the instance of w, and
f
j
?s consist of the gloss words of sense s. Conse-
quently, p(f|e) is essentially measuring the asso-
ciation between context words of w and definition
texts of s, i.e., the gloss-context association in the
simplified Lesk algorithm (Kilgarriff and Rosen-
zweig, 2000). A major difference, however, is that
instead of using hard, overlap counts between the
two sets of words from the gloss and the context,
this probabilistic treatment can implicitly model
the distributional similarity among the elements e
i
and f
j
(and consequently between the sets e and
f) over a wider range of contexts. The result is a
?softer? proxy of association than the binary view
of overlaps in existing Lesk variants.
The foregoing discussion offers a second mo-
tivation for applying Bayes?s rule on the second
1
Think of the notations e and f mnemonically as exem-
plars and features, respectively.
532
Senses Hypernyms Hyponyms Synonyms
factory building
complex,
complex
brewery,
factory,
mill, ...
works,
industrial
plant
life form organism,
being
perennial,
crop...
flora,
plant life
Table 1: Lexical knowledge for the word plant un-
der its two meanings factory and life form.
expression in Equation (1): it is easier to estimate
p(e
i
| f
j
) than p( f
j
|e
i
), since the vocabulary for the
lexical knowledge features ( f
j
) is usually more
limited than that of the contexts (e
i
) and hence esti-
mation of the former suffices on a smaller amount
of data than that of the latter.
3.3 Incorporating Additional Lexical
Knowledge
The input of the proposed NBM is bags of words,
and thus it is straightforward to incorporate var-
ious forms of lexical knowledge (LK) for word
senses: by concatenating a tokenized knowledge
source to the existing knowledge representation f,
while the similarity measure remains unchanged.
The availability of LK largely depends on the
sense inventory used in a WSD task. WordNet
senses are often used in Senseval and SemEval
tasks, and hence senses (or synsets, and possibly
their corresponding word forms) that are seman-
tic related to the inventory senses under WordNet
relations are easily obtainable and have been ex-
ploited by many existing studies.
As pointed out by Patwardhan et al (2003),
however, ?not all of these relations are equally
helpful.? Relation pairs involving hyponyms were
shown to result in better F-measure when used
in gloss overlaps (Banerjee and Pedersen, 2003).
The authors attributed the phenomenon to the the
multitude of hyponyms compared to other rela-
tions. We further hypothesize that, beyond sheer
numbers, synonyms and hyponyms offer stronger
semantic specification that helps distinguish the
senses of a given ambiguous word, and thus are
more effective knowledge sources for WSD.
Take the word plant for example. Selected hy-
pernyms, hyponyms, and synonyms pertaining to
its two senses factory and life form are listed in
Table 1. Hypernyms can be overly general terms
(e.g., being). Although conceptually helpful for
humans in coarse-grained WSD, this generality is
likely to inflate the hypernyms? probabilistic esti-
mation. Hyponyms, on the other hand, help spec-
ify their corresponding senses with information
that is possibly missing from the often overly brief
glosses: the many technical terms as hyponyms
in Table 1 ? though rare ? are likely to occur
in the (possibly domain-specific) contexts that are
highly typical of the corresponding senses. Par-
ticularly for the NBM, the co-occurrence is likely
to result in stronger gloss-definition associations
when similar contexts appear in a WSD instance.
We also observe that some semantically related
words appear under rare senses (e.g., still as an
alcohol-manufacturing plant, and annual as a one-
year-life-cycle plant; omitted from Table 1). This
is a general phenomenon in gloss-based WSD and
is beyond the scope of the current discussion.
2
Overall, all three sources of LK may complement
each other in WSD tasks, with hyponyms particu-
larly promising in both quantity and quality com-
pared to hypernyms and synonyms.
3
3.4 Probability Estimation
A most open-ended question is how to estimate the
probabilities in Equation (1). In WSD in particu-
lar, the estimation concerns the marginal and con-
ditional probabilities of and between word tokens.
Many options are available to this end in statis-
tical machine learning (MLE, MAP, etc.), infor-
mation theory (Church and Hanks, 1990; Turney,
2001), as well as the rich body of research in lex-
ical semantic similarity Resnik, 1995; Jiang and
Conrath, 1997; Budanitsky and Hirst, 2006).
Here we choose maximum likelihood ? not
only for its simplicity, but also to demonstrate
model strength with a relatively crude probability
estimation. To avoid underflow, Equation (1) is
estimated as the following log probability:
?
i
log
c( f
j
)
c(?)
+
?
i
?
j
log
c(e
i
, f
j
)
c( f
j
)
?|f|
?
j
log
c(e
i
)
c(?)
=(1?|e|)
?
i
logc( f
j
)?|f|
?
j
logc(e
i
)
+
?
i
?
j
logc(e
i
, f
j
)+ |f|(|e|?1) logc(?),
where c(x) is the count of word x, c(?) is the corpus
2
We do, however, refer curious readers to the work of Ra-
viv et al (2012) for a novel treatment of a similar problem.
3
Note that LK expansion is a feature of our model rather
than a requirement. What type of knowledge to include is
eventually a decision made by the user based on the applica-
tion and LK availability.
533
size, c(x,y) is the joint count of x and y, and |v| is
the dimension of vector v.
Nonetheless, we do investigate how model per-
formance responds to estimation quality. Specif-
ically in WSD, a source corpus is defined as the
source of the majority of the WSD instances in a
given dataset, and a baseline corpus of a smaller
size and less resemblance to the instances is used
for all datasets. The assumption is that a source
corpus offers better estimates for the model than
the baseline corpus, and difference in model per-
formance is expected when using probability esti-
mation of different quality.
4 Evaluation
4.1 Data, Scoring, and Pre-processing
Various aspects of the model discussed in Section
3 are evaluated in the English lexical sample tasks
from Senseval-2 (Edmonds and Cotton, 2001) and
SemEval-2007 (Pradhan et al, 2007). Training
sections are used as development data and test
sections held out for final testing. Model perfor-
mance is evaluated in terms of WSD accuracy us-
ing Equation (2) as the scoring function. Accu-
racy is defined as the number of correct responses
over the number of instances. Because it is a rare
event for the NBM to produce identical scores,
4
the model always proposes a unique answer and
accuracy is thus equivalent to F-score commonly
used in existing reports.
Multiword expressions (MWEs) in the
Senseval-2 sense inventory are not explicitly
marked in the contexts. Several of the top-ranking
systems implemented their own MWE detection
algorithms (Kilgarriff and Rosenzweig, 2000;
Litkowski, 2002). Without digressing to the
details of MWE detection ? and meanwhile,
to ensure fair comparison with existing systems
? we implement two variants of the prediction
module, one completely ignorant of MWE and
defaulting to INCORRECT for all MWE-related
answers, while the other assuming perfect MWE
detection and performing regular disambiguation
algorithm on the MWE-related senses (not de-
faulting to CORRECT). All results reported for
Senseval-2 below are harmonic means of the two
outcomes.
Each inventory sense is represented by a set of
LK tokens (e.g., definition texts, synonyms, etc.)
4
This has never occurred in the hundreds of thousands of
runs in our development process.
from their corresponding WordNet synset (or in
the coarse-grained case, a concatenation of tokens
from all synsets in a sense group). The MIT-JWI
library (Finlayson, 2014) is used for accessing
WordNet. Usage examples in glosses (included by
the library by default) are removed in our experi-
ments.
5
Basic pre-processing is performed on the con-
texts and the glosses, including lower-casing, stop-
word removal, lemmatization on both datasets,
and tokenization on the Senseval-2 instances.
6
Stanford CoreNLP
7
is used for lemmatization and
tokenization. Identical procedures are applied to
all corpora used for probability estimation.
Binomial test is used for significance testing,
and with one exception explicitly noted in Sec-
tion 4.3, all differences presented are statistically
highly significant (p < 0.001).
4.2 Comparing Lexical Knowledge Sources
To study the effect of different types of LK in
WSD (Section 3.3), for each inventory sense, we
choose synonyms (syn), hypernyms (hpr), and hy-
ponyms (hpo) as extended LK in addition to its
gloss. The WSD model is evaluated with gloss-
only (glo), individual extended LK sources, and
the combination of all four sources (all). The re-
sults are listed in Table 2 together with existing re-
sults (1st and 2nd correspond to the results of the
top two unsupervised methods in each dataset).
8
By using only glosses, the proposed model
already shows statistically significant improve-
ment over the basic Lesk algorithm (92.4%
and 140.5% relative improvement in Senseval-
2 coarse- and fine-grained tracks, respectively).
9
Moreover, comparison between coarse- and fine-
grained tracks reveals interesting properties of dif-
ferent LK sources. Previous hypotheses (Section
3.3) are empirically confirmed that WSD perfor-
5
We also compared the two Lesk baselines (with and
without usage examples) on the development data but did not
observe significant differences as reported by Kilgarriff and
Rosenzweig (2000).
6
The SemEval-2007 instances are already tokenized.
7
http://nlp.stanford.edu/software/
corenlp.shtml.
8
We excluded the results of UNED (Fern?andez-Amor?os
et al, 2001) in Senseval-2 because, by using sense frequency
information that is only obtainable from sense-annotated cor-
pora, it is essentially a supervised system.
9
Comparisons are made against the simplified Lesk al-
gorithm (Kilgarriff and Rosenzweig, 2000) without usage
examples. The comparison is unavailable in SemEval2007
since we have not found existing experiments with this exact
configuration.
534
Dataset glo syn hpr hpo all 1st 2nd Lesk
Senseval-2 Coarse .475 .478 .494 .518 .523 .469 .367 .262
Senseval-2 Fine .362 .371 .326 .379 .388 .412 .293 .163
SemEval-2007 .494 .511 .507 .550 .573 .538 .521 ?
Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine- and
coarse-grained) and the SemEval-2007 dataset.
Figure 1: Model response to probability esti-
mates of different quality on the SemEval-2007
dataset. Error bars indicate confidence intervals
(p < .001), and the dashed line corresponds to the
best reported result.
mance benefits most from hyponyms and least
from hypernyms. Specifically, highly similar, fine-
grained sense candidates apparently share more
hypernyms in the fine-grained case than in the
coarse-grained case; adding to the generality of
hypernyms (both semantic and distributional), we
postulate that their probability in the NBM is uni-
formly inflated among many sense candidates, and
hence they decrease in distinguishability. Syn-
onyms might help with regard to semantic spec-
ification, though their limited quantity also limits
their benefits. These patterns on the LK types are
consistent in all three experiments.
When including all four LK sources, our model
outperforms the state-of-the-art systems with sta-
tistical significance in both coarse-grained tasks.
For the fine-grained track, it achieves 2nd place
after that of Tugwell and Kilgarriff (2001), which
used a decision list (Yarowsky, 1995) on manu-
ally selected corpora evidence for each inventory
sense, and thus is not subject to loss of distin-
guishability in the glosses as Lesk variants are.
4.3 Probability Estimation
To evaluate model response to probability esti-
mation of different quality (Section 3.4), source
corpora are chosen as the majority value of the
doc-source attribute of instances in each dataset,
namely, the British National Corpus for Senseval-
2 (94%) and the Wall Street Journal for SemEval-
2007 (86%). The Brown Corpus is shared by both
datasets as the baseline corpus. Figure 1 shows the
comparison on the SemEval-2007 dataset. Across
all experiments, higher WSD accuracy is consis-
tently witnessed using the source corpus; differ-
ences are statistically highly significant except for
hpo (which is significant with p < 0.01).
5 Conclusions and Future Work
We have proposed a general-purpose Naive Bayes
model for measuring association between two sets
of random events. The model replaced string
matching in the Lesk algorithm for word sense dis-
ambiguation with a probabilistic measure of gloss-
context overlap. The base model on average more
than doubled the accuracy of Lesk in Senseval-2
on both fine- and coarse-grained tracks. With ad-
ditional lexical knowledge, the model also outper-
formed state of the art results with statistical sig-
nificance on two coarse-grained WSD tasks.
For future work, we plan to apply the model
in other shared tasks, including open-text WSD,
so as to compare with more recent Lesk variants.
We would also like to explore how to incorpo-
rate syntactic features and employ alternative sta-
tistical methods (e.g., parametric models) to im-
prove probability estimation and inference. Other
NLP problems involving compositionality in gen-
eral might also benefit from the proposed many-
to-many similarity measure.
Acknowledgments
This study is funded by the Natural Sciences and
Engineering Research Council of Canada. We
thank Afsaneh Fazly, Navdeep Jaitly, and Varada
Kolhatkar for the many inspiring discussions, as
well as the anonymous reviewers for their con-
structive advice.
535
References
Khaled Abdalgader and Andrew Skabar. Unsupervised
similarity-based word sense disambiguation using context
vectors and sentential word importance. ACM Transac-
tions on Speech and Language Processing, 9(1):2:1?2:21,
May 2012.
Satanjeev Banerjee and Ted Pedersen. An adapted Lesk al-
gorithm for word sense disambiguation using WordNet.
In Computational Linguistics and Intelligent Text Process-
ing, pages 136?145. Springer, 2002.
Satanjeev Banerjee and Ted Pedersen. Extended gloss over-
laps as a measure of semantic relatedness. In Proceedings
of the 18th International Joint Conference on Artificial In-
telligence, volume 3, pages 805?810, 2003.
Alexander Budanitsky and Graeme Hirst. Evaluating
WordNet-based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47, 2006.
Ping Chen, Wei Ding, Chris Bowes, and David Brown. A
fully unsupervised word sense disambiguation method us-
ing dependency knowledge. In Proceedings of Human
Language Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 28?36, Stroudsburg, PA,
USA, 2009.
Kenneth Ward Church and Patrick Hanks. Word association
norms, mutual information, and lexicography. Computa-
tional Linguistics, 16(1):22?29, 1990.
Philip Edmonds and Scott Cotton. Senseval-2: Overview. In
Proceedings of the 2nd International Workshop on Eval-
uating Word Sense Disambiguation Systems, pages 1?5.
Association for Computational Linguistics, 2001.
Christiane Fellbaum. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA, 1998.
David Fern?andez-Amor?os, Julio Gonzalo, and Felisa Verdejo.
The UNED systems at Senseval-2. In The Proceedings of
the Second International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 75?78. Association
for Computational Linguistics, 2001.
Mark Alan Finlayson. Java libraries for accessing the Prince-
ton WordNet: Comparison and evaluation. In Proceed-
ings of the 7th Global Wordnet Conference, Tartu, Estonia,
2014.
William Gale, Kenneth Church, and David Yarowsky. A
method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26(5-6):415?439, 1992.
Florentina Hristea. Recent advances concerning the usage of
the Na??ve Bayes model in unsupervised word sense dis-
ambiguation. International Review on Computers & Soft-
ware, 4(1), 2009.
Jay Jiang and David Conrath. Semantic similarity based on
corpus statistics and lexical taxonomy. Proceedings of
International Conference on Research in Computational
Linguistics, 1997.
Adam Kilgarriff and Joseph Rosenzweig. Framework and
results for English Senseval. Computers and the Humani-
ties, 34(1-2):15?48, 2000.
Michael Lesk. Automatic sense disambiguation using ma-
chine readable dictionaries: how to tell a pine cone from
an ice cream cone. In Proceedings of the 5th Annual In-
ternational Conference on Systems Documentation, pages
24?26, New York, New York, USA, 1986.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?Shea, and Keeley Crockett. Sentence similarity based
on semantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?1150,
2006.
Kenneth C. Litkowski. Sense information for disambigua-
tion: Confluence of supervised and unsupervised methods.
In Proceedings of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions,
pages 47?53. Association for Computational Linguistics,
July 2002.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. PageRank on
semantic networks, with application to word sense disam-
biguation. In Proceedings of the 20th International Con-
ference on Computational Linguistics, 2004.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. Using distributional similarity for lexical ex-
pansion in knowledge-based word sense disambiguation.
In Proceedings of the 24th International Conference on
Computational Linguistics, pages 1781?1796, 2012.
Roberto Navigli. Word sense disambiguation: A survey.
ACM Computing Surveys, 41(2):10:1?10:69, 2009.
Siddharth Patwardhan and Ted Pedersen. Using WordNet-
based context vectors to estimate the semantic relatedness
of concepts. Proceedings of the EACL 2006 Workshop
Making Sense of Sense-Bringing Computational Linguis-
tics and Psycholinguistics Together, 1501:1?8, 2006.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the 4th Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 241?257, 2003.
Ted Pedersen. A simple approach to building ensembles of
Naive Bayesian classifiers for word sense disambiguation.
In Proceedings of the 1st Conference of North American
Chapter of the Association for Computational Linguistics,
pages 63?69. Association for Computational Linguistics,
2000.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. SemEval-2007 task 17: English lexical
sample, SRL and all words. In Proceedings of the 4th
International Workshop on Semantic Evaluations, pages
87?92. Association for Computational Linguistics, 2007.
Ariel Raviv, Shaul Markovitch, and Sotirios-Efstathios
Maneas. Concept-based approach to word sense disam-
biguation. In Proceedings of the 26th Conference on Arti-
ficial Intelligence, 2012.
Philip Resnik. Using information content to evaluate seman-
tic similarity in a taxonomy. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence -
Volume 1, IJCAI?95, pages 448?453, San Francisco, CA,
USA, 1995.
David Tugwell and Adam Kilgarriff. Wasp-bench: a lex-
icographic tool supporting word sense disambiguation.
In The Proceedings of the Second International Work-
shop on Evaluating Word Sense Disambiguation Systems,
pages 151?154. Association for Computational Linguis-
tics, 2001.
Peter Turney. Mining the web for synonyms: PMI-IR versus
LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning, pages 491?502, 2001.
Florentina Vasilescu, Philippe Langlais, and Guy Lapalme.
Evaluating variants of the Lesk approach for disambiguat-
ing words. In Proceedings of the 4th International Con-
ference on Language Resources and Evaluation, 2004.
536
Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDon-
ald, Tony Plate, and Brian M. Slator. Providing machine
tractable dictionary tools. Machine Translation, 5(2):99?
154, 1990.
David Yarowsky. Unsupervised word sense disambiguation
rivaling supervised methods. In Proceedings of the 33rd
annual meeting on Association for Computational Lin-
guistics, pages 189?196, 1995.
537
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 33?39,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Building Readability Lexicons with Unannotated Corpora
Julian Brooke* Vivian Tsang? David Jacob? Fraser Shein*? Graeme Hirst*
*Department of Computer Science
University of Toronto
{jbrooke,gh}@cs.toronto.edu
?Quillsoft Ltd.
Toronto, Canada
{vtsang, djacob, fshein}@quillsoft.ca
Abstract
Lexicons of word difficulty are useful for var-
ious educational applications, including read-
ability classification and text simplification. In
this work, we explore automatic creation of
these lexicons using methods which go beyond
simple term frequency, but without relying on
age-graded texts. In particular, we derive infor-
mation for each word type from the readability
of the web documents they appear in and the
words they co-occur with, linearly combining
these various features. We show the efficacy of
this approach by comparing our lexicon with an
existing coarse-grained, low-coverage resource
and a new crowdsourced annotation.
1 Introduction
With its goal of identifying documents appropriate
to readers of various proficiencies, automatic anal-
ysis of readability is typically approached as a text-
level classification task. Although at least one pop-
ular readability metric (Dale and Chall, 1995) and
a number of machine learning approaches to read-
ability rely on lexical features (Si and Callan, 2001;
Collins-Thompson and Callan, 2005; Heilman et al,
2007; Petersen and Ostendorf, 2009; Tanaka-Ishii et
al., 2010), the readability of individual lexical items
is not addressed directly in these approaches. Nev-
ertheless, information about the difficulty of individ-
ual lexical items, in addition to being useful for text
readability classification (Kidwell et al, 2009), can
be applied to other tasks, for instance lexical simpli-
fication (Carroll et al, 1999; Burstein et al, 2007).
Our interest is in providing students with educa-
tional software that is sensitive to the difficulty of
particular English expressions, providing proactive
support for those which are likely to be outside a
reader?s vocabulary. However, our existing lexical
resource is coarse-grained and lacks coverage. In
this paper, we explore the extent to which an auto-
matic approach could be used to fill in the gaps of
our lexicon. Prior approaches have generally de-
pended on some kind of age-graded corpus (Kid-
well et al, 2009; Li and Feng, 2011), but this kind
of resource is unlikely to provide the coverage that
we require; instead, our methods here are based on
statistics from a huge web corpus. We show that
frequency, an obvious proxy for difficulty, is only
the first step; in fact we can derive key information
from the documents that words appear in and the
words that they appear with, information that can be
combined to give high performance in identifying
relative difficulty. We compare our automated lexi-
con against our existing resource as well as a crowd-
sourced annotation.
2 Related Work
Simple metrics form the basis of much readability
work: most involve linear combinations of word
length, syllable count, and sentence length (Kincaid
et al, 1975; Gunning, 1952), though the popular
Dale-Chall reading score (Dale and Chall, 1995) is
based on a list of 3000 ?easy? words; a recent re-
view suggests these metrics are fairly interchange-
able (van Oosten et al, 2010). In machine-learning
classification of texts by grade level, unigrams have
been found to be reasonably effective for this task,
outperforming readability metrics (Si and Callan,
2001; Collins-Thompson and Callan, 2005). Var-
33
ious other features have been explored, including
parse (Petersen and Ostendorf, 2009) and coherence
features (Feng et al, 2009), but the consensus seems
to be that lexical features are the most consistently
useful for automatic readability classification, even
when considering non-native readers (Heilman et
al., 2007).
In the field of readability, the work of Kidwell et
al. (2009) is perhaps closest to ours. Like the above,
their goal is text readability classification, but they
proceed by first deriving an age of acquisition for
each word based on its statistical distribution in age-
annotated texts. Also similar is the work of Li and
Feng (2011), who are critical of raw frequency as an
indicator and instead identify core vocabulary based
on the common use of words across different age
groups. With respect to our goal of lowering reliance
on fine-grained annotation, the work of Tanaka-Ishii
et al (2010) is also relevant; they create a readability
system that requires only two general classes of text
(easy and difficult), other texts are ranked relative to
these two classes using regression.
Other lexical acquisition work has also informed
our approach here. For instance, our co-occurrence
method is an adaption of a technique applied in
sentiment analysis (Turney and Littman, 2003),
which has recently been shown to work for formal-
ity (Brooke et al, 2010), a dimension of stylistic
variation that seems closely related to readability.
Taboada et al (2011) validate their sentiment lex-
icon using crowdsourced judgments of the relative
polarity of pairs of words, and in fact crowd sourcing
has been applied directly to the creation of emotion
lexicons (Mohammad and Turney, 2010).
3 Resources
Our primary resource is an existing lexicon, pre-
viously built under the supervision of the one of
authors. This resource, which we will refer to
as the Difficulty lexicon, consists of 15,308 words
and expressions classified into three difficulty cate-
gories: beginner, intermediate, and advanced. Be-
ginner, which was intended to capture the vocabu-
lary of early elementary school, is an amalgamation
of various smaller sources, including the Dolch list
(Dolch, 1948). The intermediate words, which in-
clude words learned in late elementary and middle
Table 1: Examples from the Difficulty lexicon
Beginner
coat, away, arrow, lizard, afternoon, rainy,
carpet, earn, hear, chill
Intermediate
bale, campground, motto, intestine, survey,
regularly, research, conflict
Advanced
contingency, scoff, characteristic, potent, myriad,
detracted, illegitimate, overture
school, were extracted from Internet-published texts
written by students at these grade levels, and then fil-
tered manually. The advanced words began as a list
of common words that were in neither of the origi-
nal two lists, but they have also been manually fil-
tered; they are intended to reflect the vocabulary un-
derstood by the average high school student. Table
1 contains some examples from each list.
For our purposes here, we only use a subset of the
Difficulty lexicon: we filtered out inflected forms,
proper nouns, and words with non-alphabetic com-
ponents (including multiword expressions) and then
randomly selected 500 words from each level for
our test set and 300 different words for our develop-
ment/training set. Rather than trying to duplicate our
arbitrary three-way distinction by manual or crowd-
sourced means, we instead focused on the relative
difficulty of individual words: for each word in each
of the two sets, we randomly selected three compar-
ison words, one from each of the difficulty levels,
forming a set of 4500 test pairs (2700 for the de-
velopment set): 1/3 of these pairs are words from
the same difficulty level, 4/9 are from adjacent dif-
ficulty levels, and the remaining 2/9 are at opposite
ends of our difficulty spectrum.
Our crowdsourced annotation was obtained using
Crowdflower, which is an interface built on top of
Mechanical Turk. For each word pair to be com-
pared, we elicited 5 judgments from workers. Rather
than frame the question in terms of difficulty or read-
ability, which we felt was too subjective, we instead
asked which of the two words the worker thought
he or she learned first: the worker could choose ei-
ther word, or answer ?about the same time?. They
34
were instructed to choose the word they did know if
one of the two words was unknown, and ?same? if
both were unknown. For our evaluation, we took the
majority judgment as the gold standard; when there
was no majority judgment, then the words were con-
sidered ?the same?. To increase the likelihood that
our workers were native speakers of English, we
required that the responses come from the US or
Canada. Before running our main set, we ran sev-
eral smaller test runs and manually inspected them
for quality; although there were outliers, the major-
ity of the judgments seemed reasonable.
Our corpus is the ICWSM Spinn3r 2009 dataset
(Burton et al, 2009). We chose this corpus because
it was used by Brooke et al (2010) to derive a lexi-
con of formality; they found that it was more effec-
tive for these purposes than smaller mixed-register
corpora like the BNC. The ICWSM 2009, collected
over several weeks in 2008, contains about 7.5 mil-
lion blogs, or 1.3 billion tokens, including well over
a million word types (more than 200,000 of which
which appear at least 10 times). We use only the
documents which have at least 100 tokens. The cor-
pus has been tagged using the TreeTagger (Schmid,
1995).
4 Automatic Lexicon Creation
Our method for lexicon creation involves first ex-
tracting a set of relevant numerical features for each
word type. We can consider each feature as defin-
ing a lexicon on its own, which can be evaluated us-
ing our test set. Our features can be roughly broken
into three types: simple features, document readabil-
ity features, and co-occurrence features. The first of
these types does not require much explanation: it in-
cludes the length of the word, measured in terms of
letters and syllables (the latter is derived using a sim-
ple but reasonably accurate vowel-consonant heuris-
tic), and the log frequency count in our corpus.1
The second feature type involves calculating sim-
ple readability metrics for each document in our cor-
pus, and then defining the relevant feature for the
word type as the average value of the metric for all
the documents that the word appears in. For exam-
1Though it is irrelevant when evaluating the feature alone,
the log frequency was noticeably better when combining fre-
quency with other features.
ple, if Dw is the set of documents where word type
w appears and di is the ith word in a document d,
then the document word length (DWL) for w can be
defined as follows:
DWL(w) = |Dw|
?1 ?
d?Dw
?
|d|
i=0 length(di)
|d|
Other features calculated in this way include: the
document sentence length, that is the average token
length of sentences; the document type-token ratio2;
and the document lexical density, the ratio of content
words (nouns, verbs, adjectives, and adverbs) to all
words.
The co-occurence features are inspired by the
semi-supervised polarity lexicon creation method of
Turney and Littman (2003). The first step is to build
a matrix consisting of each word type and the docu-
ments it appears in; here, we use a binary representa-
tion, since the frequency with which a word appears
in a particular document does not seem directly rel-
evant to readability. We also do not remove tradi-
tional stopwords, since we believe that the use of
certain common function words can in fact be good
indicators of text readability. Once the matrix is
built, we apply latent semantic analysis (Landauer
and Dumais, 1997); we omit the mathematical de-
tails here, but the result is a dimensionality reduc-
tion such that each word is represented as a vector
of some k dimensions. Next, we select two sets of
seed words (P and N) which will represent the ends
of the spectrum which we are interested in deriving.
We derive a feature value V for each word by sum-
ming the cosine similarity of the word vector with
all the seeds:
V (w) =
?p?P cos(?(w,p))
|P|
?
?n?N cos(?(w,n))
|N|
We further normalize this to a range of 1 to
?1, centered around the core vocabulary word and.
Here, we try three possible versions of P and N: the
first, Formality, is the set of words used by Brooke
et al (2010) in their study of formality, that is, a
2We calculate this using only the first 100 words of the docu-
ment, to avoid the well-documented influence of length on TTR.
35
set of slang and other markers of oral communica-
tion as N, and a set of formal discourse markers and
adverbs as P, with about 100 of each. The second,
Childish, is a set of 10 common ?childish? concrete
words (e.g. mommy, puppy) as N, and a set of 10
common abstract words (e.g. concept, philosophy)
as P. The third, Difficulty, consists of the 300 begin-
ner words from our development set as N, and the
300 advanced words from our development set as P.
We tested several values of k for each of the seed
sets (from 20 to 500); there was only small variation
so here we just present our best results for each set
as determined by testing in the development set.
Our final lexicon is created by taking a linear
combination of the various features. We can find an
appropriate weighting of each term by taking them
from a model built using our development set. We
test two versions of this: by default, we use a linear
regression model where for training beginner words
are tagged as 0, advanced words as 1, and intermedi-
ate words as 0.5. The second model is a binary SVM
classifier; the features of the model are the differ-
ence between the respective features for each of the
two words, and the classifier predicts whether the
first or second word is more difficult. Both models
were built using WEKA (Witten and Frank, 2005),
with default settings except for feature normaliza-
tion, which must be disabled in the SVM to get use-
ful weights for the linear combination which creates
our lexicon. In practice, we would further normalize
our lexicon; here, however, this normalization is not
relevant since our evaluation is based entirely on rel-
ative judgments. We also tested a range of other ma-
chine learning algorithms available in WEKA (e.g.
decision trees and MaxEnt) but the crossvalidated
accuracy was similar to or slightly lower than using
a linear classifier.
5 Evaluation
All results are based on comparing the relative dif-
ficulty judgments made for the word pairs in our
test set (or, more often, some subset) by the various
sources. Since even the existing Difficulty lexicon is
not entirely reliable, we report agreement rather than
accuracy. Except for agreement of Crowdflower
workers, agreement is the percentage of pairs where
the sources agreed as compared to the total num-
ber of pairs. For agreement between Crowdflower
workers, we follow Taboada et al (2011) in calcu-
lating agreement across all possible pairings of each
worker for each pair. Although we considered using
a more complex metric such as Kappa, we believe
that simple pairwise agreement is in fact equally in-
terpretable when the main interest is relative agree-
ment of various methods; besides, Kappa is intended
for use with individual annotators with particular bi-
ases, an assumption which does not hold here.
To evaluate the reliability of our human-annotated
resources, we look first at the agreement within the
Crowdflower data, and between the Crowdflower
and our Difficulty lexicon, with particular attention
to within-class judgments. We then compare the
predictions of various automatically extracted fea-
tures and feature combinations with these human
judgments; since most of these involve a continuous
scale, we focus only on words which were judged to
be different.3 For the Difficulty lexicon (Diff.), the
n in this comparison is 3000, while for the Crowd-
flower (CF) judgments it is 4002.
6 Results
We expect a certain amount of noise using crowd-
sourced data, and indeed agreement among Crowd-
flower workers was not extremely high, only 56.6%
for a three-way choice; note, however, that in these
circumstances a single worker disagreeing with the
rest will drop pairwise agreement in that judgement
to 60%.4 Tellingly, average agreement was rela-
tively high (72.5%) for words on the extremes of our
difficulty spectrum, and low for words in the same
difficulty category (46.0%), which is what we would
expect. As noted by Taboada et al (2011), when
faced with a pairwise comparison task, workers tend
to avoid the ?same? option; instead, the proximity of
the words on the underlying spectrum is reflected in
disagreement. When we compare the crowdsourced
judgements directly to the Difficulty lexicon, base
3A continuous scale will nearly always predict some differ-
ence between two words. An obvious approach would be to set
a threshold within which two words will be judged the same,
but the specific values depend greatly on the scale and for sim-
plicity we do not address this problem here.
4In 87.3% of cases, at least 3 workers agreed; in 56.2% of
cases, 4 workers agreed, and in 23.1% of cases all 5 workers
agreed.
36
agreement is 63.1%. This is much higher than
chance, but lower than we would like, considering
these are two human-annotated sources. However,
it is clear that much of this disagreement is due to
?same? judgments, which are three times more com-
mon in the Difficulty lexicon-based judgments than
in the Crowdflower judgments (even when disagree-
ment is interpreted as a ?same? judgment). Pairwise
agreement of non-?same? judgments for word pairs
which are in the same category in the Difficultly lex-
icon is high enough (45.9%)5 for us to conclude that
this is not random variation, strongly suggesting that
there are important distinctions within our difficulty
categories, i.e. that it is not sufficiently fine-grained.
If we disregard all words that are judged as same in
one (or both) of the two sources, the agreement of
the resulting word pairs is 91.0%, which is reason-
ably high.
Table 2 contains the agreement when feature val-
ues or a linear combination of feature values are used
to predict the readability of the unequal pairs from
the two manual sources. First, we notice that the
Crowdflower set is obviously more difficult, proba-
bly because it contains more pairs with fairly subtle
(though noticeable) distinctions. Other clear differ-
ences between the annotations: whereas for Crowd-
flower frequency is the key indicator, this is not true
for our original annotation, which prefers the more
complex features we have introduced here. A few
features did poorly in general: syllable count ap-
pears too coarse-grained to be useful on its own,
lexical density is only just better than chance, and
type-token ratio performs at or below chance. Oth-
erwise, many of the features within our major types
give roughly the same performance individually.
When we combine features, we find that simple
and document features combine to positive effect,
but the co-occurrence features are redundant with
each other and, for the most part, the document fea-
tures. A major boost comes, however, from combin-
ing either document or co-occurrence features with
the simple features; this is especially true for our
Difficulty lexicon annotation, where the gain is 7%
to 8 percentage points. It does not seem to matter
very much whether the weights of each feature are
determined by pairwise classifier or by linear regres-
5Random agreement here is 33.3%.
Table 2: Agreement (%) of automated methods with man-
ual resources on pairwise comparison task (Diff. = Diffi-
culty lexicon, CF = Crowdflower)
Features
Resource
Diff. CF
Simple
Syllable length 62.5 54.9
Word length 68.8 62.4
Term frequency 69.2 70.7
Document
Avg. word length 74.5 66.8
Avg. sentence length 73.5 65.9
Avg. type-token ratio 47.0 50.0
Avg. lexical density 56.1 54.7
Co-occurrence
Formality 74.7 66.5
Childish 74.2 65.5
Difficulty 75.7 66.1
Linear Combinations
Simple 79.3 75.0
Document 80.1 70.8
Co-occurrence 76.0 67.0
Document+Co-occurrence 80.4 70.2
Simple+Document 87.5 79.1
Simple+Co-occurrence 86.7 78.2
All 87.6 79.5
All (SVM) 87.1 79.2
sion: this is interesting because it means we can train
a model to create a readability spectrum with only
pairwise judgments. Finally, we took all the 2500
instances where our two annotations agreed that one
word was more difficult, and tested our best model
against only those pairs. Results using this selec-
tive test set were, unsurprisingly, higher than those
of either of the annotations alone: 91.2%, which is
roughly the same as the original agreement between
the two manual annotations.
7 Discussion
Word difficulty is a vague concept, and we have ad-
mittedly sidestepped a proper definition here: in-
stead, we hope to establish a measure of reliabil-
ity in judgments of ?lexical readability? by looking
for agreement across diverse sources of informa-
tion. Our comparison of our existing resources with
37
crowdsourced judgments suggests that some consis-
tency is possible, but that granularity is, as we pre-
dicted, a serious concern, one which ultimately un-
dermines our validation to some degree. An auto-
matically derived lexicon, which can be fully con-
tinuous or as coarse-grained as needed, seems like
an ideal solution, though the much lower perfor-
mance of the automatic lexicon in predicting the
more fine-grained Crowdflower judgments indicates
that automatically-derived features are limited in
their ability to deal with subtle differences. How-
ever, a visual inspection of the spectrum created by
the automatic methods suggests that, with a judi-
cious choice of granularity, it should be sufficient for
our needs. In future work, we also intend to evalu-
ate its use for readability classification, and perhaps
expand it to include multiword expressions and syn-
tactic patterns.
Our results clearly show the benefit of combin-
ing multiple sources of information to build a model
of word difficulty. Word frequency and word length
are of course relevant, and the utility of the docu-
ment context features is not surprising, since they
are merely a novel extension of existing proxies
for readability. The co-occurrence features were
also useful, though they seem fairly redundant and
slightly inferior to document features; we posit that
these features, in addition to capturing notions of
register such as formality, may also offer seman-
tic distinctions relevant to the acquisition process.
For instance, children may have a large vocabulary
in very concrete domains such as animals, includ-
ing words (e.g. lizard) that are not particularly fre-
quent in adult corpora, while very common words in
other domains (such as the legal domain) are com-
pletely outside the range of their experience. If we
look at some of the examples which term frequency
alone does not predict, they seem to be very much
of this sort: dollhouse/emergence, skirt/industry,
magic/system. Unsupervised techniques for identi-
fying semantic variation, such as LSA, can capture
these sorts of distinctions. However, our results indi-
cate that simply looking at the readability of the texts
that these sort of words appear in (i.e. our document
features) is mostly sufficient, and less than 10% of
the pairs which are correctly ordered by these two
feature sets are different. In any case, an age-graded
corpus is definitely not required.
There are a few other benefits of using word co-
occurrence that we would like to touch on, though
we leave a full exploration for future work. First, if
we consider readability in other languages, each lan-
guage may have different properties which render
proxies such as word length much less useful (e.g.
ideographic languages like Chinese or agglutinative
languages like Turkish). However, word (or lemma)
co-occurrence, like frequency, is essentially a uni-
versal feature across languages, and thus can be di-
rectly extended to any language. Second, if we con-
sider how we would extend difficulty-lexicon cre-
ation to the context of adult second-language learn-
ers, it might be enough to adjust our seed terms to
reflect the differences in the language exposure of
this population, i.e. we would expect difficulty in ac-
quiring colloquialisms that are typically learned in
childhood but are not part of the core vocabulary of
the adult language.
8 Conclusion
In this paper, we have presented an automatic
method for the derivation of a readability lexicon re-
lying only on an unannotated word corpus. Our re-
sults show that although term frequency is a key fea-
ture, there are other, more complex features which
provide competitive results on their own as well as
combining with term frequency to improve agree-
ment with manual resources that reflect word diffi-
culty or age of acquisition. By comparing our man-
ual lexicon with a new crowdsourced annotation, we
also provide a validation of the resource, while at
the same time highlighting a known issue, the lack
of fine-grainedness. Our manual lexicon provides a
solution for this problem, albeit at the cost of some
reliability. Although our immediate interest is not
text readability classification, the information de-
rived could be applied fairly directly to this task, and
might be particularly useful in the case when anno-
tated texts are not avaliable.
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
38
References
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10).
Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee,
and Matthew Ventura. 2007. The automated text
adaptation tool. In Proceedings of the Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics (NAACL ?07), Software
Demonstrations, pages 3?4.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999. Sim-
plifying English text for language impaired readers.
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL?99), pages 269?270.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the American Society for Informa-
tion Science Technology, 56(13):1448?1462.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge, MA.
Edward William Dolch. 1948. Problems in Reading.
The Garrard Press.
Lijun Feng, Noe?mie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL ?09), pages 229?237.
Robert Gunning. 1952. The Technique of Clear Writing.
McGraw-Hill.
Michael J. Heilman, Kevyn Collins, and Jamie Callan.
2007. Combining lexical and grammatical features to
improve readability measures for first and second lan-
guage texts. In Proceedings of the Conference of the
North American Chapter of Association for Computa-
tional Linguistics (NAACL-HLT ?07).
Paul Kidwell, Guy Lebanon, and Kevyn Collins-
Thompson. 2009. Statistical estimation of word
acquisition with application to readability predic-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), pages 900?909.
J. Peter Kincaid, Robert. P. Fishburne Jr., Richard L.
Rogers, and Brad. S. Chissom. 1975. Derivation of
new readability formulas for Navy enlisted personnel.
Research Branch Report 8-75, Millington, TN: Naval
Technical Training, U. S. Naval Air Station, Memphis,
TN.
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Hanhong Li and Alex C. Feng. 2011. Age tagging
and word frequency for learners? dictionaries. In Har-
ald Baayan John Newman and Sally Rice, editors,
Corpus-based Studies in Language Use, Language
Learning, and Language Documentation. Rodopi.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using Me-
chanical Turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 26?34, Los Angeles.
Sarah E. Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter Speech and Language, 23(1):89?106.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47?50.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management (CIKM ?01), pages 574?576.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manifred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting texts by readability. Computa-
tional Linguistics, 36(2):203?227.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Philip van Oosten, Dries Tanghe, and Veronique Hoste.
2010. Towards an improved methodology for auto-
mated readability prediction. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation (LREC ?10).
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco.
39
Workshop on Computational Linguistics for Literature, pages 26?35,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Stylistic Segmentation of Poetry
with Change Curves and Extrinsic Features
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
The identification of stylistic inconsistency is a
challenging task relevant to a number of gen-
res, including literature. In this work, we
carry out stylistic segmentation of a well-known
poem, The Waste Land by T.S. Eliot, which
is traditionally analyzed in terms of numerous
voices which appear throughout the text. Our
method, adapted from work in topic segmen-
tation and plagiarism detection, predicts breaks
based on a curve of stylistic change which com-
bines information from a diverse set of features,
most notably co-occurrence in larger corpora via
reduced-dimensionality vectors. We show that
this extrinsic information is more useful than
(within-text) distributional features. We achieve
well above baseline performance on both artifi-
cial mixed-style texts and The Waste Land itself.
1 Introduction
Most work in automated stylistic analysis operates
at the level of a text, assuming that a text is stylis-
tically homogeneous. However, there are a number
of instances where that assumption is unwarranted.
One example is documents collaboratively created
by multiple authors, in which contributors may, ei-
ther inadvertently or deliberately (e.g. Wikipedia
vandalism), create text which fails to form a stylis-
tically coherent whole. Similarly, stylistic incon-
sistency might also arise when one of the ?contrib-
utors? is actually not one of the purported authors
of the work at all ? that is, in cases of plagia-
rism. More-deliberate forms of stylistic dissonance
include satire, which may first follow and then flout
the stylistic norms of a genre, and much narrative lit-
erature, in which the author may give the speech or
thought patterns of a particular character their own
style distinct from that of the narrator. In this paper,
we address this last source of heterogeneity in the
context of the well-known poem The Waste Land by
T.S. Eliot, which is often analyzed in terms of the
distinct voices that appear throughout the text.
T.S. Eliot (1888?1965), recipient of the 1948 No-
bel Prize for Literature, is among the most important
twentieth-century writers in the English language.
Though he worked in a variety of forms ? he was
a celebrated critic as well as a dramatist, receiving
a Tony Award in 1950 ? he is best remembered to-
day for his poems, of which The Waste Land (1922)
is among the most famous. The poem deals with
themes of spiritual death and rebirth. It is notable
for its disjunctive structure, its syncopated rhythms,
its wide range of literary allusions, and its incorpo-
ration of numerous other languages. The poem is di-
vided into five parts; in total it is 433 lines long, and
contains 3533 tokens, not including the headings.
A prominent debate among scholars of The Waste
Land concerns whether a single speaker?s voice pre-
dominates in the poem (Bedient, 1986), or whether
the poem should be regarded instead as dramatic
or operatic in structure, composed of about twelve
different voices independent of a single speaker
(Cooper, 1987). Eliot himself, in his notes to The
Waste Land, supports the latter view by referring to
?characters? and ?personage[s]? in the poem.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
26
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
While the stylistic contrasts between these and
other voices are apparent to many readers, Eliot
does not explicitly mark the transitions between
them. The goal of the present work is to investigate
whether computational stylistic analysis can identify
the transition between one voice and the next.
Our unsupervised approach, informed by research
in topic segmentation (Hearst, 1994) and intrinsic
plagiarism detection (Stamatatos, 2009), is based
on deriving a curve representing stylistic change,
where the local maxima represent likely transition
points. Notably, our curve represents an amalga-
mation of different stylistic metrics, including those
that incorporate external (extrinsic) knowledge, e.g.
vector representations based on larger corpus co-
occurrence, which we show to be extremely use-
ful. For development and initial testing we follow
other work on stylistic inconsistency by using arti-
ficial (mixed) poems, but the our main evaluation is
on The Waste Land itself. We believe that even when
our segmentation disagrees with expert human judg-
ment, it has the potential to inform future study of
this literary work.
2 Related work
Poetry has been the subject of extensive computa-
tional analysis since the early days of literary and
linguistic computing (e.g., Beatie 1967). Most of the
research concerned either authorship attribution or
analysis of metre, rhyme, and phonetic properties of
the texts, but some work has studied the style, struc-
ture, and content of poems with the aim of better un-
derstanding their qualities as literary texts. Among
research that, like the present paper, looks at varia-
tion with a single text, Simonton (1990) found quan-
titative changes in lexical diversity and semantic
classes of imagery across the components of Shake-
speare?s sonnets, and demonstrated correlations be-
tween some of these measures and judgments of the
?aesthetic success? of individual sonnets. Duggan
(1973) developed statistical measures of formulaic
style to determine whether the eleventh-century epic
poem Chanson de Ronald manifests primarily an
oral or a written style. Also related to our work,
although it concerned a novel rather than a poem,
is that of McKenna and Antonia (2001), who used
principal component analysis of lexical frequency
to discriminate different voices (dialogue, interior
monologue, and narrative) and different narrative
styles in sections of Ulysses by James Joyce.
More general work on identifying stylistic incon-
sistency includes that of Graham et al (2005), who
built artificial examples of style shift by concate-
nating Usenet postings by different authors. Fea-
ture sets for their neural network classifiers included
standard textual features, frequencies of function
words, punctuation and parts of speech, lexical en-
tropy, and vocabulary richness. Guthrie (2008) pre-
sented some general methods for identifying stylis-
tically anomalous segments using feature vector dis-
tance, and tested the effectiveness of his unsuper-
vised method with a number of possible stylistic
variations. He used features such as simple textual
metrics (e.g. word and sentence length), readability
measures, obscure vocabulary features, frequency
rankings of function words (which were not found
to be useful), and context analysis features from
the General Inquirer dictionary. The most effective
method ranked each segment according to the city-
block distance of its feature vector to the feature vec-
tor of the textual complement (the union of all other
segments in the text). Koppel et al (2011) used a
semi-supervised method to identify segments from
two different books of the Bible artificially mixed
into a single text. They first demonstrated that, in
this context, preferred synonym use is a key stylis-
tic feature that can serve as high-precision boot-
strap for building a supervised SVM classifier on
more general features (common words); they then
used this classifier to provide an initial prediction
for each verse and smooth the results over adjacent
segments. The method crucially relied on properties
of the King James Version translation of the text in
27
order to identify synonym preferences.
The identification of stylistic inconsistency or het-
erogeneity has received particular attention as a
component of intrinsic plagiarism detection ? the
task of ?identify[ing] potential plagiarism by analyz-
ing a document with respect to undeclared changes
in writing style? (Stein et al, 2011). A typical ap-
proach is to move a sliding window over the text
looking for areas that are outliers with respect to the
style of the rest of the text, or which differ markedly
from other regions in word or character-trigram fre-
quencies (Oberreuter et al, 2011; Kestemont et al,
2011). In particular, Stamatatos (2009) used a win-
dow that compares, using a special distance func-
tion, a character trigram feature vector at various
steps throughout the text, creating a style change
function whose maxima indicate points of interest
(potential plagarism).
Topic segmentation is a similar problem that has
been quite well-explored. A common thread in this
work is the importance of lexical cohesion, though
a large number of competing models based on this
concept have been proposed. One popular unsu-
pervised approach is to identify the points in the
text where a metric of lexical coherence is at a (lo-
cal) minimum (Hearst, 1994; Galley et al, 2003).
Malioutov and Barzilay (2006) also used a lexi-
cal coherence metric, but applied a graphical model
where segmentations are graph cuts chosen to max-
imize coherence of sentences within a segment, and
minimize coherence among sentences in different
segments. Another class of approaches is based
on a generative model of text, for instance HMMs
(Blei and Moreno, 2001) and Bayesian topic mod-
eling (Utiyama and Isahara, 2001; Eisenstein and
Barzilay, 2008); in such approaches, the goal is to
choose segment breaks that maximize the probabil-
ity of generating the text, under the assumption that
each segment has a different language model.
3 Stylistic change curves
Many popular text segmentation methods depend
crucially on a reliable textual unit (often a sentence)
which can be reliably classified or compared to oth-
ers. But, for our purposes here, a sentence is both
too small a unit ? our stylistic metrics will be more
accurate over larger spans ? and not small enough
? we do not want to limit our breaks to sentence
boundaries. Generative models, which use a bag-of-
words assumption, have a very different problem: in
their standard form, they can capture only lexical co-
hesion, which is not the (primary) focus of stylistic
analysis. In particular, we wish to segment using in-
formation that goes beyond the distribution of words
in the text being segmented. The model for stylis-
tic segmentation we propose here is related to the
TextTiling technique of Hearst (1994) and the style
change function of Stamatatos (2009), but our model
is generalized so that it applies to any numeric met-
ric (feature) that is defined over a span; importantly,
style change curves represent the change of a set of
very diverse features.
Our goal is to find the precise points in the text
where a stylistic change (a voice switch) occurs. To
do this, we calculate, for each token in the text, a
measure of stylistic change which corresponds to
the distance of feature vectors derived from a fixed-
length span on either side of that point. That is, if vi j
represents a feature vector derived from the tokens
between (inclusive) indices i and j, then the stylistic
change at point ci for a span (window) of size w is:
ci = Dist(v(i?w)(i?1),vi(i+w?1))
This function is not defined within w of the edge of
the text, and we generally ignore the possibility of
breaks within these (unreliable) spans. Possible dis-
tance metrics include cosine distance, euclidean dis-
tance, and city-block distance. In his study, Guthrie
(2008) found best results with city-block distance,
and that is what we will primarily use here. The fea-
ture vector can consist of any features that are de-
fined over a span; one important step, however, is to
normalize each feature (here, to a mean of 0 and a
standard deviation of 1), so that different scaling of
features does not result in particular features having
an undue influence on the stylistic change metric.
That is, if some feature is originally measured to be
fi in the span i to i+w?1, then its normalized ver-
sion f ?i (included in vi(i+w?1)) is:
f ?i =
fi? f
? f
The local maxima of c represent our best predic-
tions for the stylistic breaks within a text. However,
28
stylistic change curves are not well behaved; they
may contain numerous spurious local maxima if a
local maximum is defined simply as a higher value
between two lower ones. We can narrow our def-
inition, however, by requiring that the local max-
imum be maximal within some window w?. That
is, our breakpoints are those points i where, for all
points j in the span x?w?, x+w?, it is the case that
gi > g j. As it happens, w? = w/2 is a fairly good
choice for our purposes, creating spans no smaller
than the smoothed window, though w? can be low-
ered to increase breaks, or increased to limit them.
The absolute height of the curve at each local min-
imum offers a secondary way of ranking (and elim-
inating) potential breakpoints, if more precision is
required; however, in our task here the breaks are
fairly regular but often subtle, so focusing only on
the largest stylistic shifts is not necessarily desirable.
4 Features
The set of features we explore for this task falls
roughly into two categories: surface and extrinsic.
The distinction is not entirely clear cut, but we wish
to distinguish features that use the basic properties
of the words or their PoS, which have traditionally
been the focus of automated stylistic analysis, from
features which rely heavily on external lexical infor-
mation, for instance word sentiment and, in partic-
ular, vector space representations, which are more
novel for this task.
4.1 Surface Features
Word length A common textual statistic in reg-
ister and readability studies. Readability, in turn,
has been used for plagiarism detection (Stein et al,
2011), and related metrics were consistently among
the best for Guthrie (2008).
Syllable count Syllable count is reasonably good
predictor of the difficulty of a vocabulary, and is
used in some readability metrics.
Punctuation frequency The presence or absence
of punctuation such as commas, colons, semicolons
can be very good indicator of style. We also include
periods, which offer a measure of sentence length.
Line breaks Our only poetry-specific feature; we
count the number of times the end of a line appears
in the span. More or fewer line breaks (that is, longer
or shorter lines) can vary the rhythm of the text, and
thus its overall feel.
Parts of speech Lexical categories can indicate,
for instance, the degree of nominalization, which is
a key stylistic variable (Biber, 1988). We collect
statistics for the four main lexical categories (noun,
verb, adjective, adverb) as well as prepositions, de-
terminers, and proper nouns.
Pronouns We count the frequency of first-,
second-, and third-person pronouns, which can in-
dicate the interactiveness and narrative character of
a text (Biber, 1988).
Verb tense Past tense is often preferred in narra-
tives, whereas present tense can give a sense of im-
mediacy.
Type-token ratio A standard measure of lexical
diversity.
Lexical density Lexical density is the ratio of the
count of tokens of the four substantive parts of
speech to the count of all tokens.
Contextuality measure The contextuality mea-
sure of Heylighen and Dewaele (2002) is based on
PoS tags (e.g. nouns decrease contextuality, while
verbs increase it), and has been used to distin-
guish formality in collaboratively built encyclope-
dias (Emigh and Herring, 2005).
Dynamic In addition to the hand-picked features
above, we test dynamically including words and
character trigrams that are common in the text being
analyzed, particularly those not evenly distributed
throughout the text (we exclude punctuation). To
measure the latter, we define clumpiness as the
square root of the index of dispersion or variance-
to-mean ratio (Cox and Lewis, 1966) of the (text-
length) normalized differences between successive
occurrences of a feature, including (importantly) the
difference between the first index of the text and the
first occurrence of the feature as well as the last oc-
currence and the last index; the measure varies be-
tween 0 and 1, with 0 indicating perfectly even dis-
tribution. We test with the top n features based on
the ranking of the product of the feature?s frequency
29
in the text (tf ) or product of the frequency and its
clumpiness (tf-cl); this is similar to a tf-idf weight.
4.2 Extrinsic features
For those lexicons which include only lemmatized
forms, the words are lemmatized before their values
are retrieved.
Percent of words in Dale-Chall Word List A list
of 3000 basic words that is used in the Dale-Chall
Readability metric (Dale and Chall, 1995).
Average unigram count in 1T Corpus Another
metric of whether a word is commonly used. We use
the unigram counts in the 1T 5-gram Corpus (Brants
and Franz, 2006). Here and below, if a word is not
included it is given a zero.
Sentiment polarity The positive or negative
stance of a span could be viewed as a stylistic vari-
able. We test two lexicons, a hand-built lexicon for
the SO-CAL sentiment analysis system which has
shown superior performance in lexicon-based sen-
timent analysis (Taboada et al, 2011), and Senti-
WordNet (SWN), a high-coverage automatic lexicon
built from WordNet (Baccianella et al, 2010). The
polarity of each word over the span is averaged.
Sentiment extremity Both lexicons provide a
measure of the degree to which a word is positive or
negative. Instead of summing the sentiment scores,
we sum their absolute values, to get a measure of
how extreme (subjective) the span is.
Formality Average formality score, using a lex-
icon of formality (Brooke et al, 2010) built using
latent semantic analysis (LSA) (Landauer and Du-
mais, 1997).
Dynamic General Inquirer The General Inquirer
dictionary (Stone et al, 1966), which was used for
stylistic inconsistency detection by Guthrie (2008),
includes 182 content analysis tags, many of which
are relevant to style; we remove the two polarity tags
already part of the SO-CAL dictionary, and select
others dynamically using our tf-cl metric.
LSA vector features Brooke et al (2010) have
posited that, in highly diverse register/genre corpora,
the lowest dimensions of word vectors derived us-
ing LSA (or other dimensionality reduction tech-
niques) often reflect stylistic concerns; they found
that using the first 20 dimensions to build their for-
mality lexicon provided the best results in a near-
synonym evaluation. Early work by Biber (1988)
in the Brown Corpus using a related technique (fac-
tor analysis) resulted in discovery of several identi-
fiable dimensions of register. Here, we investigate
using these LSA-derived vectors directly, with each
of the first 20 dimensions corresponding to a sepa-
rate feature. We test with vectors derived from the
word-document matrix of the ICWSM 2009 blog
dataset (Burton et al, 2009) which includes 1.3 bil-
lion tokens, and also from the BNC (Burnard, 2000),
which is 100 million tokens. The length of the vector
depends greatly on the frequency of the word; since
this is being accounted for elsewhere, we normalize
each vector to the unit circle.
5 Evaluation method
5.1 Metrics
To evaluate our method we apply standard topic
segmentation metrics, comparing the segmentation
boundaries to a gold standard reference. The mea-
sure Pk, proposed by Beeferman et al (1997), uses a
probe window equal to half the average length of a
segment; the window slides over the text, and counts
the number of instances where a unit (in our case,
a token) at one edge of the window was predicted
to be in the same segment (according to the refer-
ence) as a unit at the other edge, but in fact is not; or
was predicted not to be in the same segment, but in
fact is. This count is normalized by the total number
of tests to get a score between 0 and 1, with 0 be-
ing a perfect score (the lower, the better). Pevzner
and Hearst (2002) criticize this metric because it
penalizes false positives and false negatives differ-
ently and sometimes fails to penalize false positives
altogether; their metric, WindowDiff (WD), solves
these problems by counting an error whenever there
is a difference between the number of segments in
the prediction as compared to the reference. Recent
work in topic segmentation (Eisenstein and Barzi-
lay, 2008) continues to use both metrics, so we also
present both here.
During initial testing, we noted a fairly serious
shortcoming with both these metrics: all else be-
ing equal, they will usually prefer a system which
30
predicts fewer breaks; in fact, a system that predicts
no breaks at all can score under 0.3 (a very com-
petitive result both here and in topic segmentation),
if the variation of the true segment size is reason-
ably high. This is problematic because we do not
want to be trivially ?improving? simply by moving
towards a model that is too cautious to guess any-
thing at all. We therefore use a third metric, which
we call BD (break difference), which sums all the
distances, calculated as fractions of the entire text,
between each true break and the nearest predicted
break. This metric is also flawed, because it can be
trivially made 0 (the best score) by guessing a break
everywhere. However, the relative motion of the two
kinds of metric provides insight into whether we are
simply moving along a precision/recall curve, or ac-
tually improving overall segmentation.
5.2 Baselines
We compare our method to the following baselines:
Random selection We randomly select bound-
aries, using the same number of boundaries in the
reference. We use the average over 50 runs.
Evenly spaced We put boundaries at equally
spaced points in the text, using the same number of
boundaries as the reference.
Random feature We use our stylistic change
curve method with a single feature which is created
by assigning a uniform random value to each token
and averaging across the span. Again, we use the
average score over 50 runs.
6 Experiments
6.1 Artificial poems
Our main interest is The Waste Land. It is, however,
prudent to develop our method, i.e. conduct an initial
investigation of our method, including parameters
and features, using a separate corpus. We do this by
building artificial mixed-style poems by combining
stylistically distinct poems from different authors, as
others have done with prose.
6.1.1 Setup
Our set of twelve poems used for this evaluation was
selected by one of the authors (an English literature
expert) to reflect the stylistic range and influences
of poetry at the beginning of the twentieth century,
and The Waste Land in particular. The titles were
removed, and each poem was tagged by an auto-
matic PoS tagger (Schmid, 1995). Koppel et al built
their composite version of two books of the Bible by
choosing, at each step, a random span length (from a
uniform distribution) to include from one of the two
books being mixed, and then a span from the other,
until all the text in both books had been included.
Our method is similar, except that we first randomly
select six poems to include in the particular mixed
text, and at each step we randomly select one of po-
ems, reselecting if the poem has been used up or the
remaining length is below our lower bound. For our
first experiment, we set a lower bound of 100 tokens
and an upper bound of 200 tokens for each span; al-
though this gives a higher average span length than
that of The Waste Land, our first goal is to test
whether our method works in the (ideal) condition
where the feature vectors at the breakpoint gener-
ally represent spans which are purely one poem or
another for a reasonably high w (100). We create 50
texts using this method. In addition to testing each
individual feature, we test several combinations of
features (all features, all surface features, all extrin-
sic features), and present the best results for greedy
feature removal, starting with all features (exclud-
ing dynamic ones) and choosing features to remove
which minimize the sum of the three metrics.
6.1.2 Results
The Feature Sets section of Table 1 gives the in-
dividual feature results for segmentation of the
artificially-combined poems. Using any of the fea-
tures alone is better than our baselines, though some
of the metrics (in particular type-token ratio) are
only a slight improvement. Line breaks are obvi-
ously quite useful in the context of poetry (though
the WD score is high, suggesting a precision/recall
trade-off), but so are more typical stylistic features
such as the distribution of basic lexical categories
and punctuation. The unigram count and formal-
ity score are otherwise the best two individual fea-
tures. The sentiment-based features did more mod-
estly, though the extremeness of polarity was use-
ful when paired with the coverage of SentiWord-
Net. Among the larger feature sets, the GI was the
least useful, though more effective than any of the
31
Table 1: Segmentation accuracy in artificial poems
Configuration Metrics
WD Pk BD
Baselines
Random breaks 0.532 0.465 0.465
Even spread 0.498 0.490 0.238
Random feature 0.507 0.494 0.212
Feature sets
Word length 0.418 0.405 0.185
Syllable length 0.431 0.419 0.194
Punctuation 0.412 0.401 0.183
Line breaks 0.390 0.377 0.200
Lexical category 0.414 0.402 0.177
Pronouns 0.444 0.432 0.213
Verb tense 0.444 0.433 0.202
Lexical density 0.445 0.433 0.192
Contextuality 0.462 0.450 0.202
Type-Token ratio 0.494 0.481 0.204
Dynamic (tf, n=50) 0.399 0.386 0.161
Dynamic (tf-cl, 50) 0.385 0.373 0.168
Dynamic (tf-cl, 500) 0.337 0.323 0.165
Dynamic (tf-cl, 1000) 0.344 0.333 0.199
Dale-Chall 0.483 0.471 0.202
Count in 1T 0.424 0.414 0.193
Polarity (SO-CAL) 0.466 0.487 0.209
Polarity (SWN) 0.490 0.478 0.221
Extremity (SO-CAL) 0.450 0.438 0.199
Extremity (SWN) 0.426 0.415 0.182
Formality 0.409 0.397 0.184
All LSA (ICWSM) 0.319 0.307 0.134
All LSA (BNC) 0.364 0.352 0.159
GI (tf, n=5) 0.486 0.472 0.201
GI (tf-cl, 5) 0.449 0.438 0.196
GI (tf-cl, 50) 0.384 0.373 0.164
GI (tf-cl, 100) 0.388 0.376 0.163
Combinations
Surface 0.316 0.304 0.150
Extrinsic 0.314 0.301 0.124
All 0.285 0.274 0.128
All w/o GI, dynamic 0.272 0.259 0.102
All greedy (Best) 0.253 0.242 0.099
Best, w=150 0.289 0.289 0.158
Best, w=50 0.338 0.321 0.109
Best, Diff=euclidean 0.258 0.247 0.102
Best, Diff=cosine 0.274 0.263 0.145
individual features, while dynamic word and char-
acter trigrams did better, and the ICWSM LSA vec-
tors better still; the difference in size between the
ICWSM and BNC is obviously key to the perfor-
mance difference here. In general using our tf-cl
metric was better than tf alone.
When we combine the different feature types, we
see that extrinsic features have a slight edge over the
surface features, but the two do complement each
other to some degree. Although the GI and dynamic
feature sets do well individually, they do not com-
bine well with other features in this unsupervised
setting, and our best results do not include them.
The greedy feature selector removed 4 LSA dimen-
sions, type-token ratio, prepositions, second-person
pronouns, adverbs, and verbs to get our best result.
Our choice of w to be the largest fully-reliable size
(100) seems to be a good one, as is our use of city-
block distance rather than the alternatives. Overall,
the metrics we are using for evaluation suggest that
we are roughly halfway to perfect segmentation.
6.2 The Waste Land
6.2.1 Setup
In order to evaluate our method on The Waste Land,
we first created a gold standard voice switch seg-
mentation. Our gold standard represents an amal-
gamation, by one of the authors, of several sources
of information. First, we enlisted a class of 140 un-
dergraduates in an English literature course to seg-
ment the poem into voices based on their own intu-
itions, and we created a combined student version
based on majority judgment. Second, our English
literature expert listened to the 6 readings of the
poem included on The Waste Land app (Touch Press
LLP, 2011), including two readings by T.S. Eliot,
and noted places where the reader?s voice seemed
to change; these were combined to create a reader
version. Finally, our expert amalgamated these two
versions and incorporated insights from independent
literary analysis to create a final gold standard.
We created two versions of the poem for evalua-
tion: for both versions, we removed everything but
the main body of the text (i.e. the prologue, dedi-
cation, title, and section titles), since these are not
produced by voices in the poem. The ?full? ver-
sion contains all the other text (a total of 68 voice
32
switches), but our ?abridged? version involves re-
moving all segments (and the corresponding voice
switches, when appropriate) which are 20 or fewer
tokens in length and/or which are in a language
other than English, which reduces the number of
voice switches to 28 (the token count is 3179). This
version allows us to focus on the segmentation for
which our method has a reasonable chance of suc-
ceeding and ignore the segmentation of non-English
spans, which is relatively trivial but yet potentially
confounding. We use w = 50 for the full version,
since there are almost twice as many breaks as in
the abridged version (and our artificially generated
texts).
6.2.2 Results
Our results for The Waste Land are presented in Ta-
ble 2. Notably, in this evaluation, we do not investi-
gate the usefulness of individual features or attempt
to fully optimize our solution using this text. Our
goal is to see if a general stylistic segmentation sys-
tem, developed on artificial texts, can be applied suc-
cessfully to the task of segmenting an actual stylis-
tically diverse poem. The answer is yes. Although
the task is clearly more difficult, the results for the
system are well above the baseline, particularly for
the abridged version. One thing to note is that using
the features greedily selected for the artificial sys-
tem (instead of just all features) appears to hinder,
rather than help; this suggests a supervised approach
might not be effective. The GI is too unreliable to
be useful here, whereas the dynamic word and tri-
gram features continue to do fairly well, but they do
not improve the performance of the rest of the fea-
tures combined. Once again the LSA features seem
to play a central role in this success. We manually
compared predicted with real switches and found
that there were several instances (corresponding to
very clear voices switches in the text) which were
nearly perfect. Moreover, the model did tend to pre-
dict more switches in sections with numerous real
switches, though these predictions were often fewer
than the gold standard and out of sync (because the
sampling windows never consisted of a pure style).
7 Conclusion
In this paper we have presented a system for auto-
matically segmenting stylistically inconsistent text
Table 2: Segmentation accuracy in The Waste Land
Configuration Metrics
WD Pk BD
Full text
Baselines
Random breaks 0.517 0.459 0.480
Even spread 0.559 0.498 0.245
Random feature 0.529 0.478 0.314
System (w=50)
Table 1 Best 0.458 0.401 0.264
GI 0.508 0.462 0.339
Dynamic 0.467 0.397 0.257
LSA (ICWSM) 0.462 0.399 0.280
All w/o GI 0.448 0.395 0.305
All w/o dynamic, GI 0.456 0.394 0.228
Abridged text
Baselines
Random breaks 0.524 0.478 0.448
Even spread 0.573 0.549 0.266
Random feature 0.525 0.505 0.298
System (w=100)
Table 1 Best 0.370 0.341 0.250
GI 0.510 0.492 0.353
Dynamic 0.415 0.393 0.274
LSA (ICWSM) 0.411 0.390 0.272
All w/o GI 0.379 0.354 0.241
All w/o dynamic, GI 0.345 0.311 0.208
and applied it to The Waste Land, a well-known
poem in which stylistic variation, in the form of dif-
ferent ?voices?, provides an interesting challenge to
both human and computer readers. Our unsuper-
vised model is based on a stylistic change curve de-
rived from feature vectors. Perhaps our most inter-
esting result is the usefulness of low-dimension LSA
vectors over surface features such as words and tri-
gram characters as well as other extrinsic features
such as the GI dictionary. In both The Waste Land
and our development set of artificially combined po-
ems, our method performs well above baseline. Our
system could probably benefit from the inclusion of
machine learning, but our main interest going for-
ward is the inclusion of additional features ? in par-
ticular, poetry-specific elements such as alliteration
and other more complex lexicogrammatical features.
33
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Bruce A. Beatie. 1967. Computer study of medieval Ger-
man poetry: A conference report. Computers and the
Humanities, 2(2):65?70.
Calvin Bedient. 1986. He Do the Police in Different
Voices: The Waste Land and its protagonist. Univer-
sity of Chicago Press.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models. In
In Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing (EMNLP
?97), pages 35?46.
Douglas Biber. 1988. Variation Across Speech and Writ-
ing. Cambridge University Press.
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden Markov model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, SIGIR ?01, pages 343?348.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10).
Lou Burnard. 2000. User reference guide for British
National Corpus. Technical report, Oxford University.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
David R. Cox and Peter A.W. Lewis. 1966. The Sta-
tistical Analysis of Series of Events. Monographs on
Statistics and Applied Probability. Chapman and Hall.
Edgar Dale and Jeanne Chall. 1995. Readability Re-
visited: The New Dale-Chall Readability Formula.
Brookline Books, Cambridge, MA.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?08, EMNLP ?08, pages
334?343.
William Emigh and Susan C. Herring. 2005. Collabo-
rative authoring on the web: A genre analysis of on-
line encyclopedias. In Proceedings of the 38th Annual
Hawaii International Conference on System Sciences
(HICSS ?05), Washington, DC.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL ?03), ACL ?03, pages 562?569.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?94), ACL ?94, pages 9?16.
Francis Heylighen and Jean-Marc Dewaele. 2002. Vari-
ation in the contextuality of language: An empirical
measure. Foundations of Science, 7(3):293?340.
Mike Kestemont, Kim Luyckx, and Walter Daelemans.
2011. Intrinsic plagiarism detection using character
trigram distance scores. In Proceedings of the PAN
2011 Lab: Uncovering Plagiarism, Authorship, and
Social Software Misuse.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11).
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 44th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?06), pages
25?32.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
34
Gabriel Oberreuter, Gaston L?Huillier, Sebastia?n A. R??os,
and Juan D. Vela?squez. 2011. Approaches for intrin-
sic and external plagiarism detection. In Proceedings
of the PAN 2011 Lab: Uncovering Plagiarism, Author-
ship, and Social Software Misuse.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the ACL SIGDAT Workshop, pages 47?50.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. Intrinsic plagiarism detec-
tion using character n-gram profiles. In Proceedings
of the SEPLN?09 Workshop on Uncovering Plagia-
rism, Authorship and, Social Software Misuse (PAN-
09), pages 38?46. CEUR Workshop Proceedings, vol-
ume 502.
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manifred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?01), pages
499?506.
35
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 52?57,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Automatically Assessing Whether a Text Is Cliche?d,
with Applications to Literary Analysis
Paul Cook
Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
paulcook@unimelb.edu.au
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, Canada M5S 3G4
gh@cs.toronto.edu
Abstract
Cliche?s, as trite expressions, are predom-
inantly multiword expressions, but not all
MWEs are cliche?s. We conduct a prelimi-
nary examination of the problem of determin-
ing how cliche?d a text is, taken as a whole, by
comparing it to a reference text with respect
to the proportion of more-frequent n-grams, as
measured in an external corpus. We find that
more-frequent n-grams are over-represented
in cliche?d text. We apply this finding to the
?Eumaeus? episode of James Joyce?s novel
Ulysses, which literary scholars believe to be
written in a deliberately cliche?d style.
1 Cliche?s
In the broadest sense a cliche? is a tired, overused,
unoriginal idea, whether it be in music, in the vi-
sual arts, in the plot of a novel or drama, or in the
language of literature, journalism, or rhetoric. Here,
we are interested only in cliche?s of linguistic form.
Cliche?s are overused, unoriginal expressions that ap-
pear in a context where something more novel might
have reasonably been expected, or which masquer-
ade as something more original, more novel, or more
creative than they actually are. A cliche? is a kind of
ersatz novelty or creativity that is, ipso facto, unwel-
come or deprecated by the reader. Cliche?s appear to
be intuitively recognized by readers, but are difficult
to define more formally.
Cliche?s are predominantly multiword expressions
(MWEs) and are closely related to the idea of formu-
laic language, which for Wray (2002, 2008, summa-
rized in 2009) is a psycholinguistic phenomenon: a
formula is stored and retrieved as a single prefabri-
cated unit, without deeper semantic analysis, even if
it is made up of meaningful smaller units and regard-
less of whether it is or isn?t semantically transparent.
She demonstrates that formulaic language is a het-
erogeneous phenomenon, encompassing many types
of MWEs including fixed expressions (Sag et al,
2002, e.g., whys and wherefores), semi-fixed expres-
sions (e.g., hoist with/by his own petard ?injured
by that with which he would injure others?), and
syntactically-flexible expressions (e.g., sb1 haul sb2
over the coals ?reprimand severely?, allowing also
the passive sb2 was hauled over the coals (by sb1)).
Formulaic language can exhibit any of the types of
idiomaticity required by Baldwin and Kim (2010)
for an expression to be considered an MWE, i.e.,
lexical (de rigueur), syntactic (time and again), se-
mantic (fly off the handle ?lose one?s temper?), prag-
matic (nice to see you), and statistical idiomaticity
(which many of the previous examples also exhibit).
Another theme relating formulaic language to
MWEs is that of a common or preferred (though
not necessarily invariable) way for native speakers to
express an idea, i.e., institutionalization; for exam-
ple, felicitations to someone having a birthday are
usually expressed as happy birthday or (largely in
British English) many happy returns rather than any
of the many other semantically similar possibilities
(#merry birthday; cf. merry Christmas).
However, formulaic language, including cliche?s,
goes beyond the typical view of MWEs in that it
has a cultural aspect as well as a purely linguis-
tic aspect, as it includes catchphrases and allusions
to language in popular culture, such as well-known
52
lines from songs, jokes, advertisements, books, and
movies (curiouser and curiouser from Lewis Car-
roll?s Alice?s Adventures in Wonderland; go ahead,
make my day ?I dare you to attack me or do some-
thing bad, for if you do I will take great pleasure in
defeating and punishing you? from the 1983 Clint
Eastwood movie Sudden Impact).
Furthermore, not all formulaic language is
cliche?d; a weather forecast, for example, has no pre-
tensions of being linguistically creative or original,
but it would be a mistake to think of it as cliche?d,
no matter how formulaic it might be. Conversely,
a cliche? might not be formulaic from Wray?s psy-
cholinguistic perspective ? stored and recognized
as a single unit ? even if its occurrence is at least
frequent enough in relevant contexts for it to be rec-
ognized as familiar, trite, and unoriginal.
Finally, not all MWEs are cliche?s. Verb?particle
constructions such as look up (?seek information in
a resource?) and clear out are common expressions,
but aren?t unoriginal in the sense of being tired and
over-used. Moreover, they are not attempts at cre-
ativity. On the other hand, cliche?s are typically
MWEs. Some particularly long cliche?s, however,
are more prototypical of proverbs than MWEs (e.g.,
the grass is always greener on the other side). Sin-
gle words can also be trite and over-used, although
this tends to be strongly context dependent.
This paper identifies cliche?s as an under-studied
problem closely related to many issues of interest
to the MWE community. We propose a preliminary
method for assessing the degree to which a text is
cliche?d, and then show how such a method can con-
tribute to literary analysis. Specifically, we apply
this approach to James Joyce?s novel Ulysses to of-
fer insight into the ongoing literary debate about the
use of cliche?s in this work.
2 Related work
Little research in computational linguistics has
specifically addressed cliche?s. The most relevant
work is that of Smith et al (2012) who propose a
method for identifying cliche?s in song lyrics, and
determining the extent to which a song is cliche?d.
Their method combines information about rhymes
and the df-idf of trigrams (tf-idf, but using docu-
ment frequency instead of term frequency) in song
lyrics. However, this method isn?t applicable for our
goal of determining how cliche?d an arbitrary text is
with a focus on literary analysis, because in this case
rhyming is not a typical feature of the texts. More-
over, repetition in song lyrics motivated their df-idf
score, but this is not a salient feature of the texts we
consider.
In his studies of cliche?s in Ulysses, Byrnes (2012)
has drawn attention to the concept of the cliche? den-
sity of a text, i.e., the number of cliche?s per unit
of text (e.g., 1000 words). Byrnes manually iden-
tified cliche?s in Ulysses, but given a comprehensive
cliche? lexicon, automatically measuring cliche? den-
sity appears to be a straightforward application of
MWE identification ? i.e., determining which to-
kens in a text are part of an MWE. Although much
research on identification has focused on specific
kinds of MWEs (Baldwin and Kim, 2010), whereas
cliche?s are a mix of types, simple regular expres-
sions could be used to identify many fixed and semi-
fixed cliche?s. Nevertheless, an appropriate cliche?
lexicon would be required for this approach. More-
over, because of the relationship between cliche?s
and culture, to be applicable to historical texts, such
as for the literary analysis of interest to us, a lexicon
for the appropriate time period would be required.
Techniques for MWE extraction could potentially
be used to (semi-) automatically build a cliche? lex-
icon. Much work in this area has again focused
on specific types of MWEs ? e.g., verb?particle
constructions (Baldwin, 2005) or verb?noun com-
binations (Fazly et al, 2009) ? but once more the
heterogeneity of cliche?s limits the applicability of
such approaches for extracting them. Methods based
on strength of association ? applied to n-grams
or words co-occurring through some other relation
such as syntactic dependency (see Evert, 2008, for
an overview) ? could be applied to extract a wider
range of MWEs, although here most research has
focused on two-word co-occurrences, with consid-
erably less attention paid to longer MWEs. Even
if general-purpose MWE extraction were a solved
problem, methods would still be required to distin-
guish between MWEs that are and aren?t cliche?s.
53
3 Cliche?-density of known-cliche?d text
Frequency per se is not a necessary or defining crite-
rion of formulaic language. Wray (2002) points out
that even in quite large corpora, many undoubted in-
stances of formulaic language occur infrequently or
not at all; for example, Moon (1998) found that for-
mulae such as kick the bucket and speak for your-
self! occurred zero times in her 18 million?word
representative corpus of English. Nevertheless in
a very large corpus we?d expect a formulaic ex-
pression to be more frequent than a more-creative
expression suitable in the same context. Viewing
cliche?s as a type of formulaic language, we hypoth-
esized that a highly-cliche?d text will tend to contain
more n-grams whose frequency in an external cor-
pus is medium or high than a less-cliche?d text of the
same size.
We compared a text known to contain many
cliche?s to more-standard text. As a highly-
cliche?d text we created a document consisting
solely of a sample of 1,988 cliche?s from a web-
site (clichesite.com) that collects them.1 For a
reference ?standard? text we used the written por-
tion of the British National Corpus (BNC, Burnard,
2000). But because a longer text will tend to contain
a greater proportion of low-frequency n-gram types
(as measured in an external corpus) than a shorter
text, it is therefore crucial to our analysis that we
compare equal-size texts. We down-sampled our
reference text to the same size as our highly-cliche?d
text, by randomly sampling sentences.
For each 1?5-gram type in each document (i.e.,
in the sample of cliche?s and in the sample of sen-
tences from the BNC), we counted its frequency in
an external corpus, the Web 1T 5-gram Corpus (Web
1T, Brants and Franz, 2006). Histograms for the fre-
quencies are shown in Figure 1. The x-axis is the
log of the frequency of the n-gram in the corpus,
and the y-axis is the proportion of n-grams that had
that frequency. The dark histogram is for the sam-
ple from the BNC, and the light histogram is for the
cliche?s; the area where the two histograms overlap is
medium grey. For 1-grams, the two histograms are
quite similar; hence the following observations are
1Because we don?t know the coverage of this resource, it
would not be appropriate to use it for an MWE-identification
approach to measuring cliche?-density.
not merely due to simple differences in word fre-
quency. For the 3?5-grams, the light areas show that
the cliche?s contain many more n-gram types with
medium or high frequency in Web 1T than the sam-
ple of sentences from the BNC. For each of the 3?5-
grams, the types in the sample of cliche?s are signif-
icantly more frequent than those in the BNC using
a Wilcoxon rank sum test (p  0.001). The his-
togram for the 2-grams, included for completeness,
is beginning to show the trend observed for the 3?5-
grams, but there is no significant difference in mean
frequency in this case.
This finding supports our hypothesis that cliche?d
text contains more higher-frequency n-grams than
standard text. In light of this finding, in the follow-
ing section we apply this n-gram?based analysis to
the study of cliche?s in Ulysses.
4 Assessing cliche?-density for literary
analysis
Ulysses, by James Joyce, first published in 1922, is
generally regarded as one of the greatest English-
language novels of the twentieth century. It
is divided into 18 episodes written in widely
varying styles and genres. For example, some
episodes are, or contain, long passages of stream-
of-consciousness thought of one of the characters;
another is written in catechism-style question-and-
answer form; some parts are relatively conventional.
Byrnes (2010, 2012) points out that it has long
been recognized that, intuitively, some parts of the
novel are written in deliberately formulaic, cliche?d
language, whereas some other parts use novel, cre-
ative language. However, this intuitive impression
had not previously been empirically substantiated.
Byrnes took the simple step of actually counting the
cliche?s in four episodes of the book and confirmed
the intuition. In particular, he found that the ?Eu-
maeus? episode contained many more cliche?s than
the other episodes considered. However, these re-
sults are based on a single annotator identifying the
cliche?s ? Byrnes himself ? working with an infor-
mal definition of the concept, and possibly biased
by expected outcomes. By automatically and objec-
tively measuring the extent to which ?Eumaeus? is
cliche?d, we can offer further evidence ? of a very
different type ? to this debate.
54
Figure 1: Histograms for the log frequency of n-grams in a sample of sentences from the BNC and a collection of
known cliche?s. 1?5-grams are shown from left to right, top to bottom.
We compared ?Eumaeus? to a background text
consisting of episodes 1?2 and 4?10 of Ulysses,
which are not thought to be written in a marked
style. Because formulaic language could vary over
time, we selected an external corpus from the time
period leading up to the publication of Ulysses ?
the Google Books NGram Corpus (Michel et al,
2011) for the years 1850?1910 (specifically, the
?English 2012? version of this corpus). We down-
sampled each episode, by randomly sampling sen-
tences, to the size of the smallest, to ensure that we
compared equal-size texts.
Figures 2 and 3 show histograms for the fre-
quencies in the external corpus of the 1?5-grams
in ?Eumaeus? and in the background episodes. If
?Eumaeus? is more-cliche?d than the background
episodes, then, given our results in Section 3 above,
we would expect it to contain more high-frequency
higher-order n-grams. We indeed observe this in the
histograms for the 3- and 4-grams. The differences
for each of the 3?5-grams are again significant us-
ing Wilcoxon rank sum tests (p 0.001 for 3- and
4-grams, p < 0.005 for 5-grams), although the ef-
fect is less visually striking than in the analysis in
Section 3, particularly for the 5-grams. One possi-
ble reason for this difference is that in the analysis
in Section 3 the known-cliche?d text was artificial in
the sense that it was a list of expressions, as opposed
to natural text.
We further compared the mean frequency of the
3-, 4-, and 5-grams in ?Eumaeus? to that of each in-
dividual background episode, again down-sampling
by randomly sampling sentences, to ensure that
equal-size texts are compared. In each case we find
that the mean n-gram frequency is highest in ?Eu-
maeus?. These results are consistent with Byrnes?s
finding that ?Eumaeus? is written in a cliche?d style.
5 Conclusions
Cliche?s are an under-studied problem in computa-
tional linguistics that is closely related to issues of
interest to the MWE community. In our prelimi-
nary analysis, we showed that a highly-cliche?d text
contains more higher-frequency n-gram types than a
more-standard text. We then applied this approach
to literary analysis, confirming beliefs about the use
of cliche?s in the ?Eumaeus? episode of Ulysses.
55
Figure 2: Histograms for the log frequency of n-grams in
the ?Eumaeus? episode of Ulysses and episodes known
to be non-cliche?d. 1-, and 2-grams are shown on the top
and bottom, respectively.
Acknowledgments
We thank Timothy Baldwin and Bahar Salehi for
their insightful comments on this work. This work
was supported financially by the Natural Sciences
and Engineering Research Council of Canada.
References
Timothy Baldwin. 2005. The deep lexical acquisi-
tion of English verb-particle constructions. Com-
puter Speech and Language, Special Issue on
Multiword Expressions, 19(4):398?414.
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Figure 3: Histograms for the log frequency of n-grams in
the ?Eumaeus? episode of Ulysses and episodes known
to be non-cliche?d. 3-, 4-, and 5-grams are shown on the
top, middle, and bottom, respectively.
56
Damerau, editors, Handbook of Natural Lan-
guage Processing, Second Edition, pages 267?
292. CRC Press, Boca Raton, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Robert Byrnes. 2010. A statistical analysis of
the ?Eumaeus? phrasemes in James Joyce?s
Ulysses. In Actes des 10es Journe?es inter-
nationales d?Analyse statistique des Donne?es
Textuelles / Proceedings of the 10th International
Conference on Textual Data Statistical Analysis,
pages 289?295. Rome, Italy.
Robert Byrnes. 2012. The stylometry of cliche?
density and character in James Joyce?s Ulysses.
In Actes des 11es Journe?es internationales
d?Analyse statistique des Donne?es Textuelles /
Proceedings of the 11th International Conference
on Textual Data Statistical Analysis, pages 239?
246. Lie`ge, Belgium.
Stefan Evert. 2008. Corpora and collocations. In
Corpus Linguistics. An International Handbook.
Article 58. Mouton de Gruyter, Berlin.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguis-
tics, 35(1):61?103.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, William
Brockman, The Google Books Team, Joseph P.
Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,
Jon Orwant, Steven Pinker, Martin A. Nowak, and
Erez Lieberman Aiden. 2011. Quantitative anal-
ysis of culture using millions of digitized books.
Science, 331(6014):176?182.
Rosamund Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-Based Approach.
Clarendon Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLING 2002), pages 1?15.
Alex G. Smith, Christopher X. S. Zee, and Alexan-
dra L. Uitdenbogerd. 2012. In your eyes: Iden-
tifying cliche?s in song lyrics. In Proceedings of
the Australasian Language Technology Associa-
tion Workshop 2012, pages 88?96. Dunedin, New
Zealand.
Alison Wray. 2002. Formulaic Language and the
Lexicon. Cambridge University Press.
Alison Wray. 2008. Formulaic Language: Pushing
the Boundaries. Oxford University Press.
57
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 1?8,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
A Tale of Two Cultures:
Bringing Literary Analysis and Computational Linguistics Together
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
There are cultural barriers to collaborative effort
between literary scholars and computational lin-
guists. In this work, we discuss some of these
problems in the context of our ongoing research
project, an exploration of free indirect discourse
in Virginia Woolf?s To The Lighthouse, ulti-
mately arguing that the advantages of taking
each field out of its ?comfort zone? justifies the
inherent difficulties.
1 Introduction
Within the field of English literature, there is a grow-
ing interest in applying computational techniques, as
evidenced by the growth of the Digital Humanities
(Siemens et al, 2004). At the same time, a subfield
in Computational Linguistics that addresses a range
of problems in the genre of literature is gaining mo-
mentum (Mani, 2013). Nevertheless, there are sig-
nificant barriers to true collaborative work between
literary and computational researchers. In this pa-
per, we discuss this divide, starting from the classic
rift between the two cultures of the humanities and
the sciences (Snow, 1959) and then focusing in on a
single aspect, the attitude of the two fields towards
ambiguity. Next, we introduce our ongoing collab-
orative project which is an effort to bridge this gap;
in particular, our annotation of Virginia Woolf?s To
the Lighthouse for free indirect discourse, i.e. mix-
tures of objective narration and subjective speech,
requires a careful eye to literary detail, and, while
novel, interacts in interesting ways with established
areas of Computational Linguistics.
2 Background
2.1 The ?Two Cultures? Problem
Since the publication of C. P. Snow?s influential
The Two Cultures and the Scientific Revolution
(Snow, 1959), the phrase ?the two cultures? been
used to signify the rift?perceived and generally
lamented?between scientific and humanities intel-
lectual cultures. The problem, of course, is the igno-
rance of each culture with regard to the methods and
assumptions of the other, and the resulting impos-
sibility of genuine dialogue between them, prevent-
ing them from working together to solve important
problems. Many scholars describing the recent rise
of the Digital Humanities?the area of research and
teaching concerned with the intersection of comput-
ing and humanities disciplines?have argued that it
effects a reconciliation of the two alienated spheres,
bringing scientific methodology to bear on problems
within the humanities, many of which had previ-
ously been addressed in a less-than-rigorous manner
(Hockey, 2004).
From within the discipline of English literature,
however, the application of computational meth-
ods to literary analysis has frequently been?and
continues to be?a matter of considerable contro-
versy (Hoover, 2007; Flanders, 2009). This con-
troversy arises from the perception of many tradi-
tional humanists that computational analysis, which
aims to resolve dilemmas, seeking singular truth
and hard-and-fast answers, is incompatible with the
aims of humanistic research, which is often focused
on opening up questions for debate rather than re-
solving them decisively, and often premised on the
1
idea that there are no right answers, only well- and
poorly-supported arguments. Critics have responded
to these views by arguing that the best computational
literary analysis participates in this project of open-
ing up meaning, arguing that it is not a rejection of
literary reading but rather a method for carrying it
out more efficiently and extending it to more texts
(Ramsay, 2007), and that computational modelling,
even when unsuccessful, allows for the application
of the scientific method and thus carries the poten-
tial for intellectual advancement not possible with
purely anecdotal evidence (McCarty, 2005). Despite
such counter-arguments, however, the fear remains
widespread among traditional literary scholars that
the rise of computational analysis will entail the loss
of certain sacred assumptions of humanistic inquiry.
2.2 Ambiguity Across the ?Cultures?
We argue, though, that these fears are not without
basis, particularly when one considers the very dif-
ferent approaches to the question of ambiguity in
the two specific disciplines involved in our project:
English Literature and Computational Linguistics.
Here, the rift of the two cultures remains evident.
A major focus of literary scholarship since the
early twentieth century has been the semantic mul-
tiplicity of literary language. Such scholarship has
argued that literature, distinct from other forms of
discourse, may be deliberately ambiguous or poly-
semous and that literary analysis, distinct from other
analytic schools, should thus aim not to resolve am-
biguity but to describe and explore it. This was a
central insight of the early twentieth-century school,
the New Criticism, advanced in such works as
William Empson?s Seven Types of Ambiguity (Emp-
son, 1930) and Cleanth Brooks?s The Well Wrought
Urn (Brooks, 1947), which presented ambiguity and
paradox not as faults of style but as important po-
etic devices. New Criticism laid out a method of
literary analysis centred on the explication of the
complex tensions created by ambiguity and para-
dox, without any effort to resolve them. Also in
the first half of the twentieth century, but indepen-
dently, the Russian critic Mikhail Bakhtin developed
his theory of dialogism, which valorized ?double-
voiced? or polyphonic works that introduce multi-
ple, competing perspectives?particularly voices?
that present conflicting ideologies (Bakhtin, 1981).
Bakhtin, who wrote his seminal work ?Discourse in
the Novel? under a Stalinist sentence of exile, par-
ticularly valued works that enacted the free compe-
tition of ideologically opposed voices. In a simi-
lar spirit, but independently of Bakhtin, the German
critic Erich Auerbach described the ?multi-personal
representation of consciousness?, a narrative tech-
nique in which the writer, typically the narrator of
objective facts, is pushed entirely into the back-
ground and the story proceeds by reflecting the in-
dividual consciousnesses of the characters; Auer-
bach argued that this was a defining quality of mod-
ernist (early twentieth-century) literature (Auerbach,
1953). In the second half of the twentieth century,
this critical emphasis on ambiguity and paradox de-
veloped in an extreme form into the school of de-
constructive criticism, which held a theory of the
linguistic sign according to which determinate lin-
guistic meaning is considered logically impossible.
Deconstructive literary analysis proceeds by seeking
out internal contradictions in literary texts to support
its theory of infinitely ambiguous signification.
In Computational Linguistics, by contrast, ambi-
guity is almost uniformly treated as a problem to be
solved; the focus is on disambiguation, with the as-
sumption that one true, correct interpretation exists.
In the sphere of annotation, for instance, there is
an expectation that agreement between annotators,
as measured by statistics such as kappa (Di Euge-
nio and Glass, 2004), reach levels (generally 0.67 or
higher) where disagreements can be reasonably dis-
missed as noise; the implicit assumption here is that
subjectivity is something to be minimized. The chal-
lenge of dealing with subjectivity in CL has been
noted (Alm, 2011), and indeed there are rare exam-
ples in the field where multiple interpretations have
been considered during evaluations?for instance,
work in lexical cohesion (Morris and Hirst, 2005)
and in using annotator disagreements as an indicator
that two words are of similar orientation (Taboada
et al, 2011)?but they are the exception. Work in
CL focused on literary texts tends towards aspects
of the texts which readers would not find particu-
larly ambiguous, for example identifying major nar-
rative threads (Wallace, 2012) or distinguishing au-
thor gender (Luyckx et al, 2006).
2
3 A Collaborative Research Agenda
The obvious solution to the problem of the ?two
cultures??and one that has often been proposed
(Friedlander, 2009)?is interdisciplinary collabora-
tion. But while there are many computational lin-
guists working in literary topics such as genre,
and many literary scholars performing computa-
tional analysis of literature, genuine collaboration
between the disciplines remains quite rare. Over the
past two years, we have undertaken two collabora-
tive projects?one mostly complete, one ongoing?
which aim at such genuine collaboration, and in so
doing seek to bridge the real rift between scientific
and humanities cultures.1 Each of these projects
is multi-faceted, seeking (a) to produce meaningful
research within both disciplines of Computational
Linguistics and English Literature; (b) to provide
educational experience which broadens the disci-
plinary horizons of the undergraduate students in-
volved in the projects; and (c) to provide a model
of collaborative research that will spur further such
?culture-spanning? projects.
Each of our projects was launched in the context
of a course entitled ?The Digital Text? offered by the
Department of English at the University of Toronto.
The first author, whose background is in English Lit-
erature, is instructor of the course, while the sec-
ond author, a graduate student in Computer Science,
was assigned as a teaching assistant. Working to-
gether with the third author, we have designed these
projects collaboratively.
The first project, which we call ?He Do the
Police in Different Voices?,2 was carried out in
2011?12 (Hammond, 2013). Focused on a ?multi-
personal? poem, The Waste Land (1922) by T.S.
Eliot, it encompassed each of the three aspects of
our projects outlined above; in particular, it was mo-
tivated by a research question of interest to both dis-
ciplines: could we identify the points in The Waste
Land where the style changes, where one ?voice?
gives way to another? A computational approach
1In addition, the third author was part of a separate collabo-
rative project between our departments (Le et al, 2011), though
the aim of that project was not literary analysis.
2This is a reference to Eliot?s working title for The Waste
Land, which in itself is a reference to a talented storyteller in
Our Mutual Friend by Charles Dickens; another Dickens novel
is alluded to in the title of this paper.
promised to bring added rigor as well as a degree
of objectivity to this question, which humanities
methods had proven unable to resolve in almost a
century of debate. Both because poetry is dense
in signification, and because the multiple voices in
The Waste Land are a deliberate effect achieved by
a single author rather than a disguised piecing to-
gether of the works of multiple authors, the ques-
tion provided a meaningful challenge to the com-
putational approach, an unsupervised vector-space
model which first segments by identifying points
of stylistic change (Brooke et al, 2012) and then
clusters the resulting segments together into voices
(Brooke et al, 2013).
This research project was tightly integrated into
the curriculum of ?The Digital Text?. Students were
instructed in the use of the Text Encoding Initiative
(TEI) XML guidelines,3 and each of the students
provided one annotation related to voice as part of
a marked assignment. Students also participated in
an online poll in which they indicated every instance
in which they perceived a vocal switch in the poem,
and their responses were used in the construction of
a gold standard for the evaluation of our computa-
tional approach.
Once they were complete, we developed our re-
sults into a publicly accessible website.4 This web-
site promises to encourage collaboration between
literary scholars and computational linguists by ex-
plaining the project and our results in language ac-
cessible to both, and by producing a new digital edi-
tion of the poem based on our findings. Human and
computer readings of the poem are presented side-
by-side on the website, to demonstrate that each in-
terprets the poem in different ways, but that neither
of these methods is absolutely valid. Rather, we en-
courage website visitors to decide for themselves
where they believe that the vocal switches occur,
and we provide an interactive interface for divid-
ing the poem up according to their own interpreta-
tion. In addition to serving as a model of collabora-
tion between English Literature and Computational
Linguistics?and also serving as a teaching tool for
instructors of The Waste Land at any level?the site
is thus useful to us as a source of further data.
3http://www.tei-c.org/Guidelines/
4http://www.hedothepolice.org
3
4 The ?Brown Stocking? Project
4.1 Free Indirect Discourse in To the
Lighthouse
Our second, ongoing project, ?The Brown Stock-
ing?, focuses on a literary text deliberately chosen
for its deeply ambiguous, polysemous, dialogic na-
ture: Virginia Woolf?s (1927) To the Lighthouse
(TTL). Woolf?s novel was produced at the same time
that critical theories of ambiguity and polyvocality
were being developed, and indeed was taken as a
central example by many critics. Our project takes
its title from the final chapter of Erich Auerbach?s
Mimesis, in which Auerbach presents TTL as the
representative text of modernist literature?s ?mul-
tipersonal representation of consciousness? (Auer-
bach, 1953). For Auerbach, there are two prin-
cipal distinguishing features in Woolf?s narrative
style. The first is the tendency, already noted, to ?re-
flect? incidents through the subjective perspectives
of characters rather than presenting them from the
objective viewpoint of the author; thus TTL becomes
a work in which there is more than one order and in-
terpretation. Woolf?s technique not only introduces
multiple interpretations, however, but also blurs the
transitions between individual perspectives, making
it difficult to know in many instances who is speak-
ing or thinking.
Woolf achieves this double effect?multiple sub-
jective impressions combined with obscuring of the
lines separating them from the narrator and from one
another?chiefly through the narrative technique of
free indirect discourse (also known as free indirect
style). Whereas direct discourse reports the actual
words or thoughts of a character, and indirect dis-
course summarizes the thoughts or words of a char-
acter in the words of the entity reporting them, free
indirect discourse (FID) is a mixture of narrative and
direct discourse (Abrams, 1999). As in indirect dis-
course, the narrator employs third-person pronouns,
but unlike indirect discourse, the narrator includes
words and expressions that indicate subjective or
personalized aspects clearly distinct from the narra-
tor?s style. For example, in the opening sentences of
TTL:
?Yes, of course, if it?s fine tomorrow,? said Mrs.
Ramsay. ?But you?ll have to be up with the
lark,? she added. To her son these words con-
veyed an extraordinary joy, as if it were settled,
the expedition were bound to take place, and
the wonder to which he had looked forward, for
years and years it seemed, was, after a night?s
darkness and a day?s sail, within touch.
we are presented with two spans of objective nar-
ration (said Mrs. Ramsay and she added) and two
passages of direct discourse, in which the narrator
introduces the actual words of Mrs. Ramsay (?Yes,
of course, if it?s fine tomorrow? and ?But you?ll have
to be up with the lark?). The rest of the passage is
presented in FID, mixing together the voices of the
narrator, Mrs. Ramsay, and her son James: while the
use of third-person pronouns and the past tense and
clearly indicates the voice of the narrator, phrases
such as for years and years it seemed clearly present
a subjective perspective.
In FID?s mixing of voices, an element of uncer-
tainty is inevitably present. While we can be con-
fident of the identity of the voice speaking certain
words, it remains unclear whether other words be-
long to the narrator or a character; in this case, it
is not clear whether for years and years it seemed
presents James?s actual thoughts, Mrs. Ramsay?s
summary of her son?s thoughts, the narrator?s sum-
mary of James?s thoughts, the narrator?s summary
of Mrs. Ramsay?s summary of James?s thoughts, etc.
Abrams (1999) emphasizes uncertainty as a defining
trait of FID: the term ?refers to the way, in many nar-
ratives, that the reports of what a character says and
thinks shift in pronouns, adverbs, and grammatical
mode, as we move?or sometimes hover?between
the direct narrated reproductions of these events as
they occur to the character and the indirect repre-
sentation of such events by the narrator?. FID, with
its uncertain ?hovering?, is used throughout TTL;
it is the principal technical means by which Woolf
produces ambiguity, dialogism, and polysemy in the
text. It is thus the central focus of our project.
In Literary Studies, Toolan (2008) was perhaps
the first to discuss the possibility of automatic recog-
nition of FID, but his work was limited to a very
small, very informal experiment using a few a pri-
ori features, with no implementation or quantita-
tive analysis of the results. Though we are not
aware of work in Computational Linguistics that
deals with this kind of subjectivity in literature?
FID is included in the narrative annotation schema
4
of Mani (2013), but it is not given any particular
attention within that framework?there are obvious
connections with sentence-level subjectivity analy-
sis (Wilson et al, 2005) and various other stylis-
tic tasks, including authorship profiling (Argamon
et al, 2007). Since the subjective nature of these
passages is often expressed through specific lexical
choice, it would be interesting to see if sentiment
dictionaries (Taboada et al, 2011) or other stylistic
lexical resources such as dictionaries of lexical for-
mality (Brooke et al, 2010) could be useful.
4.2 Our Approach
Our project is proceeding in four stages: an initial
round of student annotation, a second round of stu-
dent annotation, computational analysis of these an-
notations, and the development of a project website.
In the first stage, we had 160 students mark up a pas-
sage of between 100?150 words in accordance with
TEI guidelines. Students were instructed to use the
TEI said element to enclose any instance of char-
acter speech, to identify the character whose speech
is being introduced, and to classify each of these in-
stances as either direct, indirect, or free indirect dis-
course and as either spoken aloud or thought silently.
Because there are often several valid ways of inter-
preting a given passage, and because we are inter-
ested in how different students respond to the same
passage, each 100?150 word span was assigned to
three or four students. This first round of annotation
focused only on the first four chapters of TTL. Raw
average agreement of the various annotations at the
level of the word was slightly less than 70%,5 and
though we hope to do better in our second round,
levels of agreement typically required are likely to
be beyond our reach due to the nature of the task.
For example, all four sudents responsible for the
passage cited above agreed on the tagging of the first
two sentences; however, two students read the third
sentence as FID mixing the voices of the narrator
and Mrs. Ramsay, and two read it as FID mixing
the voice of the narrator and James. Though they
disagree, these are both valid interpretations of the
5Since each passage was tagged by a different set of stu-
dents, we cannot apply traditional kappa measures. Raw agree-
ment overestimates success, since unlike kappa it does not
discount random agreement, which in this case varies widely
across the different kinds of annotation.
passage.
In the second round of annotation, with 160 dif-
ferent student annotators assigned slightly longer
spans of 200?300 words, we are focusing on the
final seven chapters of TTL. We have made sev-
eral minor changes to our annotation guidelines, and
two significant changes. First, we now ask that in
every span of text which students identify as FID,
they explicitly identify the words that they regard
as clearly coming from the subjective perspective
of the character. We believe this will help students
make a valid, defensible annotation, and it may also
help with the computational analysis to follow. Sec-
ond, we are also allowing embedded tags, for in-
stances of direct or indirect discourse within spans
of FID, which were confusing to students in the ini-
tial round. For instance, students would now be able
to tag the above-cited passage of as a span of FID
mixing the narrator?s and Mrs. Ramsay?s words, in-
side of which Mrs. Ramsay introduces an indirect-
discourse rendering of her son?s thoughts. Moving
from a flat to a recursive representation will natu-
rally result in additional complexity, but we believe
it is necessary to capture what is happening in the
text.
Once this second round of tagging is complete, we
will begin our computational analysis. The aim is to
see whether we can use supervised machine learn-
ing to replicate the way that second-year students
enrolled in a rigorous English literature program re-
spond to a highly complex text such as TTL. We
are interested to see whether the subjective, messy
data of the students can be used to train a useful
model, even if it is inadequate as a gold standard.
If successful, this algorithm could be deployed on
the remaining, untagged sections of TTL (i.e. ev-
erything between the first four and last seven chap-
ters) and produce meaningful readings of the text.
It would proceed by (a) identifying passages of FID
(that is, passages in which it is unclear whether a
particular word belongs to the narrator or a char-
acter); (b) making an interpretation of that passage
(hypothesizing as to which particular voices are be-
ing mixed); and (c) judging the likely validity of
this interpretation. It would seek not only to identify
spans of vocal ambiguity, but also to describe them,
as far as possible. It would thus not aim strictly
at disambiguation?at producing a right-or-wrong
5
reading of the text?but rather at producing the best
possible interpretation. The readings thus generated
could then be reviewed by an independent expert as
a form of evaluation.
Finally, we will develop an interactive website for
the project. It will describe the background and aims
of the project, present the results from the first three
stages of the project, and also include an interface
allowing visitors to the site to annotate the text for
the same features as the students (via a Javascript in-
terface, i.e. without having to manipulate the XML
markup directly). This will provide further annota-
tion data for our project, as well as giving instruc-
tors in English Literature and Digital Humanities a
resource to use in their teaching.
5 Discussion
We believe our approach has numerous benefits on
both sides of the divide. From a research perspec-
tive, the inter-disciplinary approach forces partici-
pants from both English Literature and Computa-
tional Linguistics to reconsider some of their funda-
mental disciplinary assumptions. The project takes
humanities literary scholarship out of its ?comfort
zone? by introducing alien and unfamiliar method-
ologies such as machine learning, as well as by its
basic premise that FID?by definition, a moment
of uncertainty where the question of who is speak-
ing is unresolved?can be detected automatically.
Even though many of these problems can be linked
with classic Computational Linguistics research ar-
eas, the project likewise takes Computational Lin-
guistics out of its comfort zone by seeking not to
resolve ambiguity but rather to identify it and, as far
as possible, describe it. It presents an opportunity
for a computational approach to take into account a
primary insight of twentieth-century literary schol-
arship: that ambiguity and subjectivity are often de-
sirable, intentional qualities of literary language, not
problems to be solved. It promises literary scholar-
ship a method for extending time-consuming, labo-
rious human literary readings very rapidly to a vast
number of literary texts, the possible applications of
which are unclear at this early stage, but are surely
great.
While many current major projects in computer-
assisted literary analysis operate on a ?big-data?
model, drawing conclusions from analysis of vast
numbers of lightly annotated texts, we see advan-
tages in our own method of beginning with a few
heavily-annotated texts and working outward. Tra-
ditional literary scholars often object that ?big-data?
readings take little or no account of subjective, hu-
man responses to literary texts; likewise, they find
the broad conclusions of such projects (that the nine-
teenth century novel moves from telling to show-
ing (Heuser and Le-Khac, 2012); that Austen is
more influential than Dickens (Jockers, 2012)) dif-
ficult to test (or reconcile) with traditional literary
scholarship. The specific method we are pursuing?
taking a great number of individual human read-
ings of a complex literary text and using them as
the basis for developing a general understanding of
how FID works?promises to move literary analysis
beyond merely ?subjective? readings without, how-
ever, denying the basis of all literary reading in indi-
vidual, subjective responses. Our method indeed ap-
proaches the condition of a multi-voiced modernist
literary work like TTL, in which, as Erich Auerbach
perceived, ?overlapping, complementing, and con-
tradiction yield something that we might call a syn-
thesized cosmic view?. We too are building our syn-
thetic understanding out of the diverse, often contra-
dictory, responses of individual human readers.
Developing this project in an educational
context?basing our project on readings developed
by students as part of marked assignments for
?The Digital Text??is likewise beneficial to both
cultures. It forces humanities undergraduates
out of their comfort zone by asking them to turn
their individual close readings of the text into an
explicit, machine-readable representation (in this
case, XML). Recognizing the importance of a
sharable language for expressing literary features
in machine-readable way, we have employed the
standard TEI guidelines mark-up with as few
customizations as possible, rather than developing
our own annotation language from the ground up.
The assignment asks students, however, to reflect
critically on whether such explicit languages can
ever adequately capture the polyvalent structures
of meaning in literary texts; that is, whether there
will always necessarily be possibilities that can?t
be captured in the tag set, and whether, as such, an
algorithmic process can ever really ?read? literature
6
in a useful way. At the same time, this method
has potentially great benefits to the development
of such algorithmic readings, precisely by making
available machine-readable approximations of how
readers belonging to another ?culture??humanities
undergraduates?respond to a challenging literary
text. Such annotations would not be possible from
a pool of annotators trained in the sciences, but
could only come from students of the humanities
with a basic understanding of XML. We do not
believe, for example, workers on Amazon Mechan-
ical Turk could reliably be used for this purpose,
though it might be interesting to compare our
?studentsourcing? with traditional crowdsourcing
techniques.
Our approach also faces several important chal-
lenges. Certainly the largest is whether an algo-
rithmic criticism can be developed that could come
to terms with ambiguity. The discipline of literary
studies has long taught its students to accept what
the poet John Keats called ?negative capability, that
is, when a man is capable of being in uncertainties,
mysteries, doubts, without any irritable searching af-
ter fact and reason? (Keats, 2002). Computational
analysis may simply be too fundamentally premised
on ?irritable searching after fact and reason? to be
capable of ?existing in uncertainty? in the manner of
many human literary readers. Even if we are able to
develop a successful algorithmic method of detect-
ing FID in Woolf, this method may not prove appli-
cable to other literary texts, which may employ the
device in highly individual manners; TTL may prove
simply too complex?and employ too much FID?
to serve as a representative sample text. At a more
practical level, even trained literature students do not
produce perfect annotations: they make errors both
in XML syntax and in their literary interpretation of
TTL, a text that proves elusive even for some spe-
cialists. Since we do not want our algorithm to base
its readings on invalid student readings (for instance,
readings that attribute speech to a character clearly
not involved in the scene), we face the challenge of
weeding out bad student readings?and we will face
the same challenge once readings begin to be sub-
mitted by visitors to the website. These diverse read-
ings do, however, also present an interesting possi-
bility, which we did not originally foresee: the de-
velopment of a reader-response ?map? showing how
human readers actually interpret (and in many cases
misinterpret) complex modernist texts like TTL.
6 Conclusion
Despite the philosophical and technical chal-
lenges that face researchers in this growing multi-
disciplinary area, we are increasingly optimistic that
collaboration between computational and literary re-
searchers is not only possible, but highly desirable.
Interesting phenomena such as FID, this surprising
melding of objective and personal perspective that
is the subject of the current project, requires experts
in both fields working together to identify, annotate,
and ultimately model. Though fully resolving the
rift between our two cultures is not, perhaps, a feasi-
ble goal, we argue that even this early and tentative
collaboration has demonstrated the potential benefits
on both sides.
Acknowledgements
This work was financially supported by the So-
cial Sciences and Humanities Research Council of
Canada and the Natural Sciences and Engineering
Research Council of Canada.
References
M. H. Abrams. 1999. A Glossary of Literary Terms.
Harcourt Brace, Toronto, 7th edition.
Cecilia Ovesdotter Alm. 2011. Subjective natural lan-
guage problems: Motivations, applications, charac-
terizations, and implications. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 107?112.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features. Journal of the American Society for In-
formation Science and Technology, 7:91?109.
Erich Auerbach. 1953. Mimesis: The Representation
of Reality in Western Literature. Princeton University
Press, Princeton, NJ.
Mikhail Mikhailovich Bakhtin. 1981. Discourse in
the novel. In Michael Holquist, editor, The Dialogic
Imagination: Four Essays, pages 259?422. Austin:
Univeristy of Texas Press.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
7
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Julian Brooke, Graeme Hirst, and Adam Hammond.
2013. Clustering voices in the Waste Land. In Pro-
ceedings of the 2nd Workshop on Computational Lit-
erature for Literature (CLFL ?13), Atlanta.
Cleanth Brooks. 1947. The Well Wrought Urn. Harcourt
Brace, New York.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Computational Linguistics,
30(1):95?101, March.
T.S. Eliot. 1971. The Waste Land. In The Complete
Poems and Plays, 1909?1950, pages 37?55. Harcourt
Brace Jovanovich, New York.
William Empson. 1930. Seven Types of Ambiguity.
Chatto and Windus, London.
Julia Flanders. 2009. Data and wisdom: Electronic edit-
ing and the quantification of knowledge. Literary and
Linguistic Computing, 24(1):53?62.
Amy Friedlander. 2009. Asking questions and build-
ing a research agenda for digital scholarship. Work-
ing Together or Apart: Promoting the Next Generation
of Digital Scholarship. Report of a Workshop Cospon-
sored by the Council on Library and Information Re-
sources and The National Endowment for the Human-
ities, March.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Ryan Heuser and Long Le-Khac. 2012. A quantita-
tive literary history of 2,958 nineteenth-century British
novels: The semantic cohort method. Stanford Lit-
erary Lab Pamphlet No. 4. http://litlab.stanford.edu/
LiteraryLabPamphlet4.pdf .
Susan Hockey. 2004. The history of humanities com-
puting. In Ray Siemens, Susan Schreibman, and John
Unsworth, editors, A Companion to Digital Humani-
ties. Blackwell, Oxford.
David L. Hoover. 2007. Quantitative analysis and lit-
erary studies. In Ray Siemens and Susan Schreib-
man, editors, A Companion to Digital Literary Studies.
Blackwell, Oxford.
Matthew L. Jockers. 2012. Computing and visualiz-
ing the 19th-century literary genome. Presented at the
Digital Humanities Conference. Hamburg.
John Keats. 2002. Selected Letters. Oxford University
Press, Oxford.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
Inderjeet Mani. 2013. Computational Modeling of Nar-
rative. Morgan & Claypool.
Willard McCarty. 2005. Humanities Computing. Pal-
grave Macmillan, New York.
Jane Morris and Graeme Hirst. 2005. The subjectivity
of lexical cohesion in text. In James G. Shanahan, Yan
Qu, and Janyce M. Wiebe, editors, Computing Attitude
and Affect in Text. Springer, Dordrecht, The Nether-
lands.
Stephen Ramsay. 2007. Algorithmic criticism. In Ray
Siemens and Susan Schreibman, editors, A Companion
to Digital Literary Studies. Blackwell, Oxford.
Ray Siemens, Susan Schreibman, and John Unsworth,
editors. 2004. A Companion to Digital Humanities.
Blackwell, Oxford.
C. P. Snow. 1959. The Two Cultures and the Scientific
Revolution. Cambridge University Press, New York.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Michael Toolan. 2008. Narrative progression in the short
story: First steps in a corpus stylistic approach. Nar-
rative, 16(2):105?120.
Byron Wallace. 2012. Multiple narrative disentangle-
ment: Unraveling Infinite Jest. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1?10, Montre?al,
Canada, June. Association for Computational Linguis-
tics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT/EMNLP
?05, pages 347?354.
Virginia Woolf. 1927. To the Lighthouse. Hogarth, Lon-
don.
8
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 41?46,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Clustering voices in The Waste Land
Julian Brooke
Dept of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Dept of Computer Science
University of Toronto
gh@cs.toronto.edu
Adam Hammond
Dept of English
University of Toronto
adam.hammond@utoronto.ca
Abstract
T.S. Eliot?s modernist poem The Waste Land is
often interpreted as collection of voices which
appear multiple times throughout the text. Here,
we investigate whether we can automatically
cluster existing segmentations of the text into
coherent, expert-identified characters. We show
that clustering The Waste Land is a fairly dif-
ficult task, though we can do much better than
random baselines, particularly if we begin with
a good initial segmentation.
1 Introduction
Although literary texts are typically written by a sin-
gle author, the style of a work of literature is not nec-
essarily uniform. When a certain character speaks,
for instance, an author may shift styles to give the
character a distinct voice. Typically, voice switches
in literature are explicitly marked, either by the use
of quotation marks with or without a said quota-
tive, or, in cases of narrator switches, by a major
textual boundary (e.g. the novel Ulysses by James
Joyce). However, implicit marking is the norm in
some modernist literature: a well-known example is
the poem The Waste Land by T.S. Eliot, which is
usually analyzed in terms of voices that each appear
multiple times throughout the text. Our interest is
distinguishing these voices automatically.
One of the poem?s most distinctive voices is that
of the woman who speaks at the end of its second
section:
I can?t help it, she said, pulling a long face,
It?s them pills I took, to bring it off, she said
[158?159]
Her chatty tone and colloquial grammar and lexis
distinguish her voice from many others in the poem,
such as the formal and traditionally poetic voice of a
narrator that recurs many times in the poem:
Above the antique mantel was displayed
As though a window gave upon the sylvan scene
The change of Philomel
[97?99]
Although the stylistic contrasts between these and
other voices are clear to many readers, Eliot does
not explicitly mark the transitions, nor is it obvi-
ous when a voice has reappeared. Our previous
work focused on only the segmentation part of the
voice identification task (Brooke et al, 2012). Here,
we instead assume an initial segmentation and then
try to create clusters corresponding to segments of
the The Waste Land which are spoken by the same
voice. Of particular interest is the influence of the
initial segmentation on the success of this down-
stream task.
2 Related Work
There is a small body of work applying quantita-
tive methods to poetry: Simonton (1990) looked
at lexical and semantic diversity in Shakespearean
sonnets and correlated this with aesthetic success,
whereas Dugan (1973) developed statistics of for-
mulaic style and applied them to the Chanson de
Roland to determine whether it represents an oral
or written style. Kao and Jurafsky (2012) quantify
various aspects of poety, including style and senti-
ment, and use these features to distinguish profes-
sional and amateur writers of contemporary poetry.
41
With respect to novels, the work of McKenna and
Antonia (2001) is very relevant; they used principal
components analysis of lexical frequency to discrim-
inate different voices and narrative styles in sections
of Ulysses by James Joyce.
Clustering techniques have been applied to liter-
ature in general; for instance, Luyckx (2006) clus-
tered novels according to style, and recent work in
distinguishing two authors of sections of the Bible
(Koppel et al, 2011) relies crucially on an initial
clustering which is bootstrapped into a supervised
classifier which is applied to segments. Beyond lit-
erature, the tasks of stylistic inconsistency detec-
tion (Graham et al, 2005; Guthrie, 2008) and intrin-
sic (unsupervised) plagiarism detection (Stein et al,
2011) are very closely related to our interests here,
though in such tasks usually only two authors are
posited; more general kinds of authorship identifi-
cation (Stamatatos, 2009) may include many more
authors, though some form of supervision (i.e. train-
ing data) is usually assumed.
Our work here is built on our earlier work (Brooke
et al, 2012). Our segmentation model for The Waste
Land was based on a stylistic change curve whose
values are the distance between stylistic feature vec-
tors derived from 50 token spans on either side of
each point (spaces between tokens) in the text; the
local maxima of this curve represent likely voice
switches. Performance on The Waste Land was far
from perfect, but evaluation using standard text seg-
mentation metrics (Pevzner and Hearst, 2002) indi-
cated that it was well above various baselines.
3 Method
Our approach to voice identification in The Waste
Land consists first of identifying the boundaries of
voice spans (Brooke et al, 2012). Given a segmenta-
tion of the text, we consider each span as a data point
in a clustering problem. The elements of the vector
correspond to the best feature set from the segmen-
tation task, with the rationale that features which
were useful for detecting changes in style should
also be useful for identifying stylistic similarities.
Our features therefore include: a collection of read-
ability metrics (including word length), frequency
of punctuation, line breaks, and various parts-of-
speech, lexical density, average frequency in a large
external corpus (Brants and Franz, 2006), lexicon-
based sentiment metrics using SentiWordNet (Bac-
cianella et al, 2010), formality score (Brooke et al,
2010), and, perhaps most notably, the centroid of 20-
dimensional distributional vectors built using latent
semantic analysis (Landauer and Dumais, 1997), re-
flecting the use of words in a large web corpus (Bur-
ton et al, 2009); in previous work (Brooke et al,
2010), we established that such vectors contain use-
ful stylistic information about the English lexicon
(including rare words that appear only occasionally
in such a corpus), and indeed LSA vectors were the
single most promising feature type for segmentation.
For a more detailed discussion of the feature set, see
Brooke et al (2012). All the features are normalized
to a mean of zero and a standard deviation of 1.
For clustering, we use a slightly modified ver-
sion of the popular k-means algorithm (MacQueen,
1967). Briefly, k-means assigns points to a cluster
based on their proximity to the k cluster centroids,
which are initialized to randomly chosen points from
the data and then iteratively refined until conver-
gence, which in our case was defined as a change of
less than 0.0001 in the position of each centroid dur-
ing one iteration.1 Our version of k-means is distinct
in two ways: first, it uses a weighted centroid where
the influence of each point is based on the token
length of the underlying span, i.e. short (unreliable)
spans which fall into the range of some centroid will
have less effect on the location of the centroid than
larger spans. Second, we use a city-block (L1) dis-
tance function rather than standard Euclidean (L2)
distance function; in the segmentation task, Brooke
et al found that city-block (L1) distance was pre-
ferred, a result which is in line with other work
in stylistic inconsistency detection (Guthrie, 2008).
Though it would be interesting to see if a good k
could be estimated independently, for our purposes
here we set k to be the known number of speakers in
our gold standard.
4 Evaluation
We evaluate our clusters by comparing them to a
gold standard annotation. There are various met-
rics for extrinsic cluster evaluation; Amigo? et al
1Occasionally, there was no convergence, at which point we
halted the process arbitrarily after 100 iterations.
42
(2009) review various options and select the BCubed
precision and recall metrics (Bagga and Baldwin,
1998) as having all of a set of key desirable prop-
erties. BCubed precision is a calculation of the frac-
tion of item pairs in the same cluster which are also
in the same category, whereas BCubed recall is the
fraction of item pairs in the same category which
are also in the same cluster. The harmonic mean
of these two metrics is BCubed F-score. Typically,
the ?items? are exactly what has been clustered, but
this is problematic in our case, because we wish to
compare methods which have different segmenta-
tions and thus the vectors that are being clustered
are not directly comparable. Instead, we calculate
the BCubed measures at the level of the token; that
is, for the purposes of measuring performance we
act as if we had clustered each token individually,
instead of the spans of tokens actually used.
Our first evaluation is against a set of 20
artificially-generated ?poems? which are actually
randomly generated combinations of parts of 12 po-
ems which were chosen (by an English literature ex-
pert, one of the authors) to represent the time period
and influences of The Waste Land. The longest of
these poems is 1291 tokens and the shortest is just
90 tokens (though 10 of the 12 have at least 300 to-
kens); the average length is 501 tokens. Our method
for creating these poems is similar to that of Kop-
pel et al (2011), though generalized for multiple
authors. For each of the artificial poems, we ran-
domly selected 6 poems from the 12 source poems,
and then we concatenated 100-200 tokens (or all the
remaining tokens, if less than the number selected)
from each of these 6 poems to the new combined
poem until all the poems were exhausted or below
our minimum span length (20 tokens). This allows
us to evaluate our method in ideal circumstances, i.e.
when there are very distinct voices corresponding to
different poets, and the voice spans tend to be fairly
long.
Our gold standard annotation of The Waste Land
speakers is far more tentative. It is based on a
number of sources: our own English literature ex-
pert, relevant literary analysis (Cooper, 1987), and
also The Waste Land app (Touch Press LLP, 2011),
which includes readings of the poem by various ex-
perts, including T.S. Eliot himself. However, there
is inherently a great deal of subjectivity involved in
literary annotation and, indeed, one of the potential
benefits of our work is to find independent justifi-
cation for a particular voice annotation. Our gold
standard thus represents just one potential interpre-
tation of the poem, rather than a true, unique gold
standard. The average size of the 69 segments in
the gold standard is 50 tokens; the range, however,
is fairly wide: the longest is 373 tokens, while the
shortest consists of a single token. Our annotation
has 13 voices altogether.
We consider three segmentations: the segmen-
tation of our gold standard (Gold), the segmenta-
tion predicted by our segmentation model (Auto-
matic), and a segmentation which consists of equal-
length spans (Even), with the same number of spans
as in the gold standard. The Even segmentation
should be viewed as the baseline for segmentation,
and the Gold segmentation an ?oracle? represent-
ing an upper bound on segmentation performance.
For the automatic segmentation model, we use the
settings from Brooke et al (2012). We also com-
pare three possible clusterings for each segmenta-
tion: no clustering at all (Initial), that is, we assume
that each segment is a new voice; k-means clustering
(k-means), as outlined above; and random clustering
(Random), in which we randomly assign each voice
to a cluster. For these latter two methods, which both
have a random component, we averaged our metrics
over 50 runs. Random and Initial are here, of course,
to provide baselines for judging the effectiveness of
k-means clustering model. Finally, when using the
gold standard segmentation and k-means clustering,
we included another oracle option (Seeded): instead
of the standard k-means method of randomly choos-
ing them from the available datapoints, each cen-
troid is initialized to the longest instance of a dif-
ferent voice, essentially seeding each cluster.
5 Results
Table 1 contains the results for our first evaluation
of voice clustering, the automatically-generated po-
ems. In all the conditions, using the gold segmen-
tation far outstrips the other two options. The au-
tomatic segmentation is consistently better than the
evenly-spaced baseline, but the performance is actu-
ally worse than expected; the segmentation metrics
we used in our earlier work
43
Table 1: Clustering results for artificial poems
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.703 0.154 0.249
Initial Automatic 0.827 0.177 0.286
Initial Gold 1.000 0.319 0.465
Random Even 0.331 0.293 0.307
Random Automatic 0.352 0.311 0.327
Random Gold 0.436 0.430 0.436
k-means Even 0.462 0.409 0.430
k-means Automatic 0.532 0.479 0.499
k-means Gold 0.716 0.720 0.710
k-means Gold Seeded 0.869 0.848 0.855
Table 2: Clustering results for The Waste Land
Configuration BCubed metrics
Prec. Rec. F-score
Initial Even 0.792 0.069 0.128
Initial Automatic 0.798 0.084 0.152
Initial Gold 1.000 0.262 0.415
Random Even 0.243 0.146 0.183
Random Automatic 0.258 0.160 0.198
Random Gold 0.408 0.313 0.352
k-means Even 0.288 0.238 0.260
k-means Automatic 0.316 0.264 0.296
k-means Gold 0.430 0.502 0.461
k-means Gold Seeded 0.491 0.624 0.550
The results for The Waste Land are in Table 2.
Many of the basic patterns are the same, including
the consistent ranking of the methods; overall, how-
ever, the clustering is far less effective. This is par-
ticularly true for the gold-standard condition, which
only increases modestly between the initial and clus-
tered state; the marked increase in recall is balanced
by a major loss of precision. In fact, unlike with
the artificial text, the most promising aspect of the
clustering seems to be the fairly sizable boost to the
quality of clusters in automatic segmenting perfor-
mance. The effect of seeding is also very consistent,
nearly as effective as in the automatic case.
We also looked at the results for individual speak-
ers in The Waste Land; many of the speakers (some
of which appear only in a few lines) are very poorly
distinguished, even with the gold-standard segmen-
tation and seeding, but there are a few that cluster
quite well; the best two are in fact our examples from
Section 1,2 that is, the narrator (F-score 0.869), and
the chatty woman (F-score 0.605). The former re-
sult is particularly important, from the perspective
of literary analysis, since there are several passages
which seem to be the main narrator (and our ex-
pert annotated them as such) but which are definitely
open to interpretation.
6 Conclusion
Literature, by its very nature, involves combin-
ing existing means of expression in surprising new
ways, resisting supervised analysis methods that de-
pend on assumptions of conformity. Our unsuper-
vised approach to distinguishing voices in poetry of-
fers this necessary flexibility, and indeed seems to
work reasonably well in cases when the stylistic dif-
ferences are clear. The Waste Land, however, is a
very subtle text, and our results suggest that we are
a long way from something that would be a consid-
ered a possible human interpretation. Nevertheless,
applying quantitative methods to these kinds of texts
can, for literary scholars, bridge the gab between
abstract interpretations and the details of form and
function (McKenna and Antonia, 2001). In our own
case, this computational work is just one aspect of
a larger project in literary analysis where the ulti-
mate goal is not to mimic human behavior per se,
but rather to better understand literary phenomena
by annotation and modelling of these phenomena
(Hammond, 2013; Hammond et al, 2013).
With respect to future enhancements, improving
segmentation is obviously important; the best au-
tomated efforts so far provide only a small boost
over a baseline approach to segmentation. However,
independently of this, our experiments with gold-
standard seeding suggest that refining our approach
to clustering, e.g. a method that identifies good ini-
tial points for our centroids, may also pay dividends
in the long run. A more radical idea for future work
would be to remove the somewhat artificial delim-
2These passages are the original examples from our earlier
work (Brooke et al, 2012), selected by our expert for their dis-
tinctness, so the fact that they turned out to be the most easily
clustered is actually a result of sorts (albeit an anecdotal one),
suggesting that our clustering behavior does correspond some-
what to a human judgment of distinctness.
44
itation of the task into segmentation and clustering
phases, building a model which works iteratively
to produce segments that are sensitive to points of
stylistic change but that, at a higher level, also form
good clusters (as measured by intrinsic measures of
cluster quality).
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Enrique Amigo?, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12:461?486, August.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th Conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (ACL-COLING ?98), pages 79?85, Montreal,
Quebec, Canada.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Google Inc.
Julian Brooke, Tong Wang, and Graeme Hirst. 2010. Au-
tomatic acquisition of lexical formality. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING ?10), Beijing.
Julian Brooke, Adam Hammond, and Graeme Hirst.
2012. Unsupervised stylistic segmentation of poetry
with change curves and extrinsic features. In Proceed-
ings of the 1st Workshop on Computational Literature
for Literature (CLFL ?12), Montreal.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
John Xiros Cooper. 1987. T.S. Eliot and the politics of
voice: The argument of The Waste Land. UMI Re-
search Press, Ann Arbor, Mich.
Joseph J. Duggan. 1973. The Song of Roland: Formulaic
style and poetic craft. University of California Press.
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 2005.
Segmenting documents by stylistic character. Natural
Language Engineering, 11(4):397?415.
David Guthrie. 2008. Unsupervised Detection of
Anomalous Text. Ph.D. thesis, University of Sheffield.
Adam Hammond, Julian Brooke, and Graeme Hirst.
2013. A tale of two cultures: Bringing literary analy-
sis and computational linguistics together. In Proceed-
ings of the 2nd Workshop on Computational Literature
for Literature (CLFL ?13), Atlanta.
Adam Hammond. 2013. He do the police in different
voices: Looking for voices in The Waste Land. Sem-
inar: ?Mapping the Fictional Voice? American Com-
parative Literature Association (ACLA).
Justine Kao and Dan Jurafsky. 2012. A computational
analysis of style, sentiment, and imagery in contem-
porary poetry. In Proceedings of the 1st Workshop on
Computational Literature for Literature (CLFL ?12),
Montreal.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decompo-
sition of a document into authorial components. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?11), Port-
land, Oregon.
Thomas K. Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.
2006. Stylogenetics: Clustering-based stylistic analy-
sis of literary corpora. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC ?06), Genoa, Italy.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, pages 281?297.
C. W. F. McKenna and A. Antonia. 2001. The statistical
analysis of style: Reflections on form, meaning, and
ideology in the ?Nausicaa? episode of Ulysses. Liter-
ary and Linguistic Computing, 16(4):353?373.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36, March.
Dean Keith Simonton. 1990. Lexical choices and aes-
thetic success: A computer content analysis of 154
Shakespeare sonnets. Computers and the Humanities,
24(4):251?264.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
45
Benno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.
Intrinsic plagiarism analysis. Language Resources
and Evaluation, 45(1):63?82.
Touch Press LLP. 2011. The Waste Land
app. http://itunes.apple.com/ca/app/the-waste-land/
id427434046?mt=8 .
46
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 188?196,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Using Other Learner Corpora in the 2013 NLI Shared Task
Julian Brooke
Department of Computer Science
University of Toronto
jbrooke@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Our efforts in the 2013 NLI shared task fo-
cused on the potential benefits of external cor-
pora. We show that including training data
from multiple corpora is highly effective at ro-
bust, cross-corpus NLI (i.e. open-training task
1), particularly when some form of domain
adaptation is also applied. This method can
also be used to boost performance even when
training data from the same corpus is available
(i.e. open-training task 2). However, in the
closed-training task, despite testing a number
of new features, we did not see much improve-
ment on a simple model based on earlier work.
1 Introduction
Our participation in the 2013 NLI shared task
(Tetreault et al, 2013) follows on our recent work
exploring cross-corpus evaluation, i.e. using dis-
tinct corpora for training and testing (Brooke and
Hirst, 2011; Brooke and Hirst, 2012a; Brooke and
Hirst, 2012b), an approach that is now becoming
fairly standard alternative in relevant work (Bykh
and Meurers, 2012; Tetreault et al, 2012; Swan-
son and Charniak, 2013). Our promotion of cross-
corpus evaluation in NLI was partially motivated by
serious issues with the most popular corpus for na-
tive language identification work up to now, the In-
ternational Corpus of Learner English (Granger et
al., 2009). The new TOEFL-11 (Blanchard et al,
2013) used for this NLI shared task addresses some
of the problems with the ICLE (most glaringly, the
fact that some topics in the ICLE appeared only in
some L1 backgrounds), but, from the perspective of
topic, proficiency, and particularly genre, it is nec-
essarily limited in scope (perhaps even more so than
the ICLE); in short, it addresses only a small por-
tion of the space of learner texts. Our interest, then,
continues to be in robust models for NLI that are not
restricted to utility in a particular corpus, and in our
participation in this task we have focused our efforts
on the open-training tasks which allow the use of
corpora beyond the TOEFL-11. Since participation
in these tasks was low relative to the closed-training
task, fewer papers will address them, making our
emphasis here all the more relevant.
The models built for all of three of the tasks are
extensions of the model used in our recent work
(Brooke and Hirst, 2012b); we will discuss the as-
pects of this model common to all tasks in Section
2. Section 3 is a brief review of our methodology
and results in the closed-training task, which was fo-
cused exclusively on testing features (both new and
old); we found almost nothing that improved on our
best feature set from previous work, and most fea-
tures actually hurt performance. In Section 4, we
discuss the corpora we used for the open-training
tasks, some of which we collected and/or have not
been applied to NLI before. Our approach to the
open-training task 2 using these corpora is presented
in Section 5. In Section 6, we discuss how we used
domain adaption methods and our various external
corpora to create the (winning) model for the open-
training task 1, which did not permit usage of the
TOEFL-11; we also present some post hoc testing
(now that TOEFL-11 is no longer off limits). In Sec-
tion 7 we offer conclusions.
188
2 Basic Model
In our recent work on cross-corpus NLI (Brooke and
Hirst, 2012b), we tested a number of classifier and
feature options, and most of our choices there are
carried over to this work. In particular, we use the
Liblinear SVM 1va (one versus all) classifier (Fan et
al., 2008). Using the TOEFL-11 corpus, we briefly
tested the other options explored in that paper (in-
cluding SVM 1v1) as well as the logistic regression
classifier included in Liblinear, and found that the
SVM 1va classifier was still preferred (with our best
feature set, see below), though the differences in-
volved were marginal. Although small variations in
the choice of C parameter within the SVM model
did occasionally produce benefits (here and in our
previous work), these were not consistent, whereas
the default value of 1 showed consistently near opti-
mal results. We used a binary feature representation,
and then feature vectors were normalized to the unit
circle. With respect to feature selection, our earlier
work used a frequency cutoff of 5 for all features; we
continue to use frequency cutoffs here; other com-
mon feature selection methods (e.g. use of informa-
tion gain) were ineffective in our previous work, so
we did not explore them in detail here.
With regards to the features themselves, our ear-
lier work tested a fairly standard collection of distri-
butional features, including function words, word n-
grams (up to bigram), POS n-grams (up to trigram),
character n-grams (up to trigram), dependencies,
context-free productions, and ?mixed? POS/function
n-grams (up to trigram), i.e. n-grams with all lex-
ical words replaced with part of speech. Most of
these had appeared in previous NLI work (Koppel
et al, 2005; Wong and Dras, 2011; Wong et al,
2012), though until recently word n-grams had been
avoided because of ICLE topic bias. Our best model
used only two of these features, word n-grams and
the mixed POS/function n-grams. This was our
starting point for the present work. The Stanford
parser (Klein and Manning, 2003) was used for POS
tagging and parsing.
Obviously, the training set used varies through-
out the paper, and other differences in specific mod-
els built for each task will be mentioned as they
become relevant. For evaluation here, we primar-
ily use the test set for NLI shared task, though we
Table 1: Feature testing for closed-training task, previ-
ously investigated features; best result is in bold.
Feature Set Accuracy (%)
Word+mixed 76.8
Word+mixed+characters 72.0
Word+mixed+POS 76.6
Word+mixed+productions 77.9
Word+mixed+dependencies 78.9
Word+mixed+dep+prod 78.4
employ some other evaluation corpora, as appropri-
ate. During the preparation for the shared task, we
made our decisions regarding models for two tasks
with TOEFL-11 training according to the results in
two training/test sets (800 per language for training,
100 per language for testing) sampled from the re-
leased training data. Since our research was focused
on cross-corpus evaluation, we never created mecha-
nisms for cross-validation in our system, and in fact
it creates practical difficulties for the open-training
task 2, so we do not include cross-validated results
here.
3 Closed-training Task
Our approach to the closed-training task primarily
involved feature testing. Table 1 contains the re-
sults of testing our previously investigated features
from Brooke and Hirst (2012b) in the TOEFL-11,
pivoted around the best set (word n-grams + mixed
POS/Function n-grams) from that earlier work.
Some of the features we rejected in our previous
work also underperform here, in particular charac-
ter and POS n-grams. In fact, character n-grams had
a much more negative effect on performance here
than they had previously. Dependencies are clearly a
useful feature in the TOEFL-11, this is fully consis-
tent with out initial testing. CFG productions offer a
small benefit on top of our base feature set, but are
not useful when dependencies are also included, so
we discarded them. Thus, our feature set going for-
ward consists of word n-grams, mixed POS/function
n-grams, and dependencies.
Next, we evaluate our feature frequency cutoff us-
ing this feature set (Table 2). We used the rather high
cutoff of 5 (for all features) in the previous work be-
cause of our much larger training set. We looked at
189
Table 2: Feature frequency cutoff testing for closed-
training task; best result is in bold.
Cutoff Accuracy (%)
At least 5 occurrences 78.9
At least 3 occurrences 79.5
At least 2 occurrences 79.7
All features 80.2
higher values there, but for this task we focused on
testing lower values.
Lowering our frequency cutoff is indeed benefi-
cial, and we got our best result in the test set when
we had no feature selection at all. This was not con-
sistent with our preparatory testing, which showed
some benefit to removing hapax legomena, though
the difference was marginal. However, we did in-
clude a run with this option in our final submis-
sion, and so this last result represents our best per-
formance on the closed-training task.
We tested several other feature options that were
added to our system for this task. Inspired by Bykh
and Meurers (2012), we first considered n-grams
(up to trigrams) where at least one lexical word is
abstracted to its POS, and at least one isn?t (par-
tial abstraction). Since dependencies were found to
be a positive feature, we tried adding dependency
chains, which combine two dependencies, i.e. three
lexical words linked by two grammatical relations.
We tested productions with wild cards, e.g. S? NP
VP * matches any sentence production which starts
with NP VP. Tree Substitution grammar fragments
have been shown to be superior to CFG produc-
tions (Swanson and Charniak, 2012); we used raw
Tree Substitution Grammar (TSG) fragments for the
TOEFL-111 and tested a subset of those fragments
which involved at least two levels of the grammar
(i.e. those not already covered by n-grams or CFG
productions).
Our final feature option requires slightly more
explanation. Crossley and McNamara (2012) re-
port that metrics associated with word concreteness,
imagability, meaningfulness, and familiarity are use-
ful for NLI; the metrics they use are derived from the
MRC Psycholinguistic database (Coltheart, 1980),
1We thank Ben Swanson for letting us use his TSG frag-
ments.
Table 3: Feature testing for closed-training task, new fea-
tures; best result is in bold.
Feature Set Accuracy (%)
Best 80.2
Best+partial abstraction 79.7
Best+dependency chains 78.6
Best+wild card productions 78.8
Best+TSG fragments 78.1
Best+MRC lexicon 54.2
which assign values for each dimension to individ-
ual words. We used the scores in the MRC to get
an average score for each dimension for each text,
further normalized to the range 0?1; texts with no
words in the dictionaries were assigned the average
across the training set.
Table 3 indicates that all of these new features
were, to varying degrees, a drag on our model. The
strongly negative effect of the MRC lexicons is par-
ticularly surprising. We speculate that this might
might be due partially to problems with combining
a large number of binary features with a small num-
ber of continuous metrics directly in a single SVM.
A meta-classifier might solve this problem, but we
did not explore meta-classification for features here.
Finally, since that information was available to
us, we tested creating sub-models segregated by
topic and proficiency. The topic-segregated model
consisted of 8 SVMs, one for each topic; accu-
racy of this model was quite low, only 67.3%. The
proficiency-segregated model used two groups, high
and low/medium (there were few low texts, so we
did not think they would be sufficient by themselves
for a viable model). Results were higher, 74.9%, but
still well below the best unsegregated model.
4 External Corpora
In this section we review corpora which will be used
for the open-training tasks in the next two sections.
Including the TOEFL-11, there are at least six pub-
licly available multi-L1 learner text corpora for NLI,
with many of these corpora becoming available rel-
atively recently. Below, we introduce each corpus in
detail; a summary of the number of tokens from each
L1 background for each of the corpora is in Table 4.
190
Table 4: Number of tokens (in thousands) in external learner corpora, by L1.
L1
Corpus
Lang-8 (new) ICLE FCE ICCI ICNALE
Japanese 11694k 227k 33k 232k 199k
Chinese 7044k 552k 30k 243k 366k
Korean 5174k 0k 37k 0k 151k
French 536k 256k 61k 0k 0k
Spanish 861k 225k 83k 49k 0k
Italian 450k 251k 31k 0k 0k
German 331k 258k 29k 91k 0k
Turkish 51k 222k 22k 0k 0k
Arabic 218k 0k 0k 0k 0k
Hindi 11k 0k 0k 0k 0k
Telugu 2k 0k 0k 0k 0k
Lang-8 Lang-8 is a website where language learn-
ers write journal entries in their L2 to be corrected
by native speakers. We collected a large set of these
entries, which we?ve shown to be useful for NLI
(Brooke and Hirst, 2012b), despite the noisiness of
the corpus (for instance, some entries directly mix
L1 and L2). For this task we added more entries
written since the first version was collected (58k
on top of the existing 154k entries).2 The corpus
contains entries from all the L1 backgrounds in the
TOEFL-11, though the amounts for Hindi and par-
ticularly Telugu are small. Since many of the entries
are very short, as in our previous work we add en-
tries of the same L1 together to reach a minimum
size of 250 tokens.
ICLE Before 2011, nearly all work on NLI was
done in the International Corpus of Learner English
or ICLE (Granger et al, 2009), a collection of col-
lege student essays from 15 L1 backgrounds, 8 of
which overlap with the 11 L1s in the TOEFL-11.
Despite known issues that might cause problems
(Brooke and Hirst, 2011), it is probably the closest
match in terms of genre and writer proficiency to the
TOEFL-11.
FCE What we call the FCE corpus is a small
sample of the First Certificate in English portion
of the Cambridge Learner Corpus, which was re-
2We do not have permission to distribute the corpus directly;
however, we can offer a list of URLs together with software
which can be used to recreate the corpus.
leased for the purposes of essay scoring evaluation
(Yannakoudakis et al, 2011); 16 different L1 back-
grounds are represented, 9 of which overlap with the
TOEFL-11. Each of the texts consists of two short
answers in the form of a letter, a report, an article,
or a short story. Relative to the other corpora, the
actual amount of text in the FCE is small.
ICCI Like the ICLE and TOEFL-11, the Inter-
national Corpus of Crosslinguistic Interlanguage
(Tono et al, 2012) is also an essay corpus, though
in contrast with other corpora it is focused on young
learners, i.e. those in grade school. It includes both
descriptive and argumentative essays on a number of
topics. Only 4 of its L1s overlap with the TOEFL-
11.
ICANLE The International Corpus Network of
Asian Learners of English or ICANLE (Ishikawa,
2011) is a collection of essays from college students
in 10 Asian countries; 3 of the L1s overlap with the
TOEFL-11.3 Even more so than the TOEFL-11, this
corpus is strictly controlled for topic, it has only 2
topics (part-time jobs and smoking in restaurants).
One obvious problem with using the above cor-
pora to classify L1s in the TOEFL-11 is the lack
of Hindi and Telugu text, which we found were
the two most easily confused L1s in the closed-
3The ICANLE also contains 103K of Urdu text. Since Urdu
and Hindi are mutually intelligible, this could be a good substi-
tute for Hindi; we overlooked this possibility during our prepa-
ration for the task, unfortunately.
191
Table 5: Number of tokens (in thousands) in Indian cor-
pora, by expected L1.
L1
Indian Corpus
News Twitter Blog
Hindi 996k 146k 2089k
Telugu 998k 133k 76k
training task. We explored a few methods to get
data to fill this gap. First, we downloaded two
collections of English language Indian news arti-
cles, one from a Hindi newspaper, the Hindus-
tan Times, and one from a Telugu newspaper, the
Andhra Jyothy.4 Second, we extracted a collection
of English tweets from the WORLD twitter corpus
(Han et al, 2012) that were geolocated in the Hindi
and Telugu speaking areas; as with the Lang-8, these
were combined to create texts of at least 250 tokens.5
Our third Indian corpus consists of translations (by
Google Translate) of Hindi and Telugu blogs from
the ICWSM 2009 Spinn3r Dataset (Burton et al,
2009), which we used in other work on using L1
text for NLI (Brooke and Hirst, 2012a). The number
of tokens in each of these corpora are given in Table
5.
5 Open-training Task 2
Our approach to open-training task 2 is based on the
assumption that in many ways it is a direct extension
of the closed-training task. For example, we directly
use the best feature set from that task, with no further
testing. Based on the results in our initial testing,
we used a feature frequency cutoff of 2 during our
testing for open-training task 2; for consistency, we
continue with that cutoff in this section.
We first attempted to integrate information from
other corpora by using a meta-classifier, as was suc-
cessfully used for features by Tetreault et al (2012).
Briefly, classifiers were trained on each major exter-
nal corpus (including only the L1s in the TOEFL-
11), and then tested on the TOEFL-11 training set;
4As with the Lang-8, we cannot distribute the corpus di-
rectly but would be happy to provide URLs and scraping soft-
ware for those would like to build it themselves.
5We extracted India regions 07 and 36 for Hindi, and 02 and
25 for Telegu; We can provide a list of tweet ids for reconstruct-
ing the corpus if desired. Our thanks to Bo Han and Paul Cook
for helping us get these tweets.
TOEFL-11 training was accomplished using 10-fold
crossvalidation (by modifying the code for Liblin-
ear crossvalidation to output margins). With the
TOEFL-11 as the training set, the SVM margins
from each 1va classifier (across all L1s and all cor-
pora) were used as the feature input to the meta-
classifier (also an SVM). In addition to Liblinear,
we also outputted this meta-classification problem to
WEKA format (Witten and Frank, 2005), and tested
a number of other classifier options not available
in Liblinear (e.g. Na??ve Bayes, decision trees, ran-
dom forests). In addition to (continuous) margins,
we also tested using the classification directly. Ul-
timately, we came to the conclusion were that any
use of a meta-classifer came with a cost (a mini-
mum 2?3% drop in performance) that could not be
fully overcome with the additional information from
our external corpora. The result using SVM classi-
fiers, margin features, and an SVM meta-classifier
was 78.5%, well below the TOEFL-11?only base-
line.
The other approach to using these external cor-
pora is to add the data directly to the TOEFL-11 data
and train a single classifier. This is very straightfor-
ward; really the only variable is which corpora will
be included. However, we need to introduce, at this
point, a domain-adaptation technique from our most
recent work (Brooke and Hirst, 2012b), bias adap-
tion, which we used to greatly improve the accu-
racy of cross-corpus classification. Without getting
into the algorithmic details, bias adaption involves
changing the bias (constant) factor of a model until
the output of the model in some dataset is balanced
across classes (or otherwise fits the expected distri-
bution); it partially addresses skewed results due to
differences between training and testing corpora. In
the previous work, we used a separate development
set, but here we rely on the test set itself; since the
technique is unsupervised, we do not need to know
the classes. Table 6 shows model performance after
adding various corpora to the training set (TOEFL-
11 is always included), with and without bias adap-
tion (BA).
Many of the differences in Table 6 are modest,
but there are are few points to be made. First,
there is a small improvement using either the Lang-
8 or the ICLE as additional data. The ICCI, on the
other hand, has a clearly negative effect, perhaps be-
192
Table 6: Corpus testing for open-training task; best result
is in bold.
Training Set
Accuracy (%)
no BA with BA
TOEFL-11 only 79.7 79.2
+Lang-8 79.5 80.5
+ICLE 80.2 80.2
+FCE 79.6 79.3
+ICCI 77.3 76.7
+ICANLE 79.7 79.3
+Lang-8+ICLE 80.4 80.4
+all but ICCI 80.0 80.4
cause of the age or proficiency of the contributors to
that corpus. Bias adaption seems to help when the
(messy and highly unbalanced) Lang-8 is involved
(consistent with our previous work), but it does not
seem useful applied to other corpora, at least not in
this setting.
Our second adaptation technique involves training
data selection, which has been used, for instance in
cross-domain parsing (Plank and van Noord, 2011).
The method used here is very simple: we count the
number of times each word appears in a document in
our test data, rank the texts in our training data ac-
cording to the sum of counts (in the test data) each
word that appears in a training texts, and throw away
a certain numbers of low-ranked texts. For example,
if a training text consists solely of the two words I
agree6 and I appears in 1053 texts in the test set,
and agree appears in 325, then the value for that text
is 1378. This method simultaneously penalizes short
texts, those texts with low lexical diversity, and texts
that do not use the same words as our test set. We
use a fixed cutoff, r, which refers to the proportion
of training data that is thrown away for each L1 (al-
lowing this to work independent of L1 was not ef-
fective). We tested this on this method in tandem
with bias adaption on two corpus sets: The TOEFL-
11 and the Lang-8, and all corpora except the ICCI.
The results are in Table 7. The number in italics is
the best run that we submitted.
Again, it is difficult to come to any firm con-
clusions when the differences are this small, but
6This is not a made-up example; there is actually a text in
the TOEFL-11 corpus like this.
Table 7: Training set selection testing for open-training
task 2; best result is in bold, best submitted run is in ital-
ics.
Training Set
Accuracy (%)
no BA with BA
TOEFL-11 only 79.7 79.2
+Lang-8 79.5 80.5
+Lang-8 r = 0.1 81.4 81.6
+Lang-8 r = 0.2 80.6 81.5
+Lang-8 r = 0.3 81.0 80.6
+all but ICCI 80.0 80.4
+all but ICCI r = 0.1 81.5 82.5
+all but ICCI r = 0.2 81.0 81.6
+all but ICCI r = 0.3 80.9 81.3
our best results involve all of the corpora (except
the ICCI) and both adaptation techniques. Unfor-
tunately, our initial testing suggested r = 0.2 was
the better choice, so our official best result in this
task (81.6%) is not the best result in this table. Per-
formance clearly drops for r > 0.2. Nevertheless,
nearly all the results in the table show clear improve-
ment on our closed-training task model.
6 Open-training Task 1
The central challenge of open-training task 1 was
that the TOEFL-11 was completely off-limits, even
for testing. Therefore, a discussion of how we pre-
pared for this task is very distinct from a post hoc
analysis of the best method once we allowed our-
selves access to the TOEFL-11; we separate the two
here. We did use the feature set (and frequency cut-
off) from the closed-training (and open-training 2)
task; it was close enough to the feature set from our
earlier work (using the Lang-8, ICLE, and FCE) that
it did not seem like cheating to preserve it.
6.1 Method
Given our failure to create a meta-classifier in open-
training task 2, we did not pursue that option here,
focusing purely on adding corpora directly to a
mixed training set. The central question was which
corpora to add, and whether to use our domain-
adaptation methods. Our experience with the ICCI
in the open-training task 2 suggested that it might be
worth leaving it (or perhaps other corpora) out, but
193
Table 8: ICLE testing for Open-training task 1; best result
is in bold.
Training Set
Accuracy (%)
no BA with BA
Lang-8 47.0 57.1
Lang-8+FCE 47.9 58.2
Lang-8+ICCI 46.4 54.8
Lang-8+ICNALE 46.9 57.5
Lang-8+ICNALE+FCE 47.7 58.8
Lang-8+ICNALE+FCE r = 0.1 46.6 58.2
could we come to that conclusion independently?
Our approach involved considering each external
corpus as a test set, and seeing which other corpora
were useful when included in the training set; cor-
pora which were consistently useful would be in-
cluded in the final set. Our original exploration in-
volved looking at all of the corpora (as test sets),
but it was haphazard; here, we present results just
with the ICLE and the ICANLE, which are arguably
the two closest corpora to the TOEFL-11 in terms
of proficiency and genre. For this, we used a dif-
ferent selection of L1s, 12 for the ICLE, 7 for the
ICANLE; all of these languages appeared in at least
the Lang-8, and 2 of them (Chinese and Japanese)
appeared in all corpora. Both sets were balanced by
L1. Again, we report results with and without bias
adaption. The results for the ICLE are in Table 8.
The clearest result in Table 8 is the consistently
positive effect of bias adaption, at least 10 percent-
age points, which is line with our previous work.
Adding both ICLE and ICNALE to the Lang-8 cor-
pus gave a small boost in performance, but the effect
of the ICCI was once again negative, as was the ef-
fect of our training set selection.
The ICNALE results in Table 9 support many of
the conclusions that we reached in the ICLE (and
other sets like the FCE and ICCI, which are not in-
cluded here but gave similar results); the effect of
bias adaption is even more pronounced. Two dif-
ferences: the slightly positive effect of training data
selection and the positive effect of the ICCI, the lat-
ter of which we saw nowhere else. We speculate
that this might be due to that fact that although the
ICNALE is a college-level corpus, it is a corpus of
Table 9: ICNALE testing for open-training task 1; best
result is in bold.
Training Set
Accuracy
no BA with BA
Lang-8 37.2 59.6
Lang-8+FCE 37.9 61.3
Lang-8+ICCI 35.7 61.4
Lang-8+ICLE 37.3 61.4
Lang-8+ICLE+FCE 37.6 61.7
Lang-8+ICLE+FCE r = 0.1 37.7 61.9
Asian-language native speakers. Our theory is that
Europeans are, on average, more proficient users
of English (this is supported by, for instance, the
testing from Granger et al (2009)), and that there-
fore the European component of the low-proficiency
ICCI actually interferes with using high proficiency
as a way of distinguishing European L1s, a problem
which would obviously not extend to an Asian-L1-
only corpus. This is an interesting result, but we will
not explore it further here. In any case, it would lead
us to predict that including ICCI data would be a bad
idea for TOEFL-11 testing.
Since we did not have any way to evaluate our
Indian corpora (i.e. the news, twitter, and translated
blogs from Section 4) without using the TOEFL-11,
we instead took advantage of the option to submit
multiple runs, submitting runs which use each of the
corpora, and combining the blogs and news.
6.2 Post Hoc Analysis
With the TOEFL-11 data now visible to us, we first
ask whether our specially collected Indian corpora
can distinguish texts in the ICCI. The test set used
in Table 10 contains only Hindi and Telugu texts.
The results are quite modest (the guessing baseline
is 50%), but suggest that all three corpora contain
some information that distinguish Hindi and Telugu,
particularly if bias adaption is used.
The results for a selection of models on the full
set of TOEFL-11 languages is presented in Table
11. Since ours was the best-performing model in
this task, we include results for both the TOEFL-
11 training (including development set) and test set,
to facilitate future comparison. Again, there is little
doubt that bias adaption is of huge benefit, though
in fact our results in the Lang-8 alone, without bias
194
Table 11: 11-language testing on TOEFL-11 sets for open-training task 1; best result is in bold, best submitted run is
in italics.
Training Set
Accuracy (%)
TOEFL-11 test TOEFL-11 training
no BA with BA no BA with BA
Lang-8 39.5 53.2 37.2 48.2
Lang-8+ICCI 36.9 51.0 34.9 46.3
Lang-8+FCE+ICLE+ICNALE 44.5 55.8 44.9 53.1
Lang-8+FCE+ICLE+ICNALE+Indian news 45.2 56.5 45.5 54.9
Lang-8+FCE+ICLE+ICNALE+Indian tweets 44.9 56.4 45.1 53.4
Lang-8+FCE+ICLE+ICNALE+Indian translated blog 45.4 50.1 45.7 49.9
Lang-8+FCE+ICLE+ICNALE+News+Tweets 45.2 57.5 45.5 55.2
Lang-8+FCE+ICLE+ICNALE+News+Tweets r = 0.1 44.9 58.2 45.0 58.2
Table 10: Indian corpus testing for Open-training task 1;
best result is in bold.
Training Set
Accuracy (%)
no BA with BA
Indian news 50.0 54.0
Indian tweets 54.0 56.0
Indian blogs 51.5 56.0
adaption, would have been enough to take first place
in this task. Adding other corpora, including the In-
dian corpora but not the ICCI, did consistently im-
prove performance, as suggested by our testing in
other corpora. Although the translated blog data was
useful in distinguishing Hindi from Telugu alone, it
had an unpredictable effect in the main task, lower-
ing bias-adapted performance. Training set selection
does seem to have a small positive effect, though we
did not see this consistently in our original testing.
7 Conclusion
Our efforts in the 2013 NLI shared task focused on
the potential benefits of external corpora. We have
shown here that including training data from multi-
ple corpora is effective at creating good cross-corpus
NLI systems, particularly when domain adaptation,
i.e. bias adaption or training set selection, is also
applied; we were the highest-performing group in
open-training task 1 by a large margin. This ap-
proach can also be applied to improve performance
even when training data from the same corpus is
available, as in open-training task 2. However, in
the closed-training task, despite testing a number
of new features, we did not see much improvement
on our simple model based on earlier work. Other
teams clearly did find some ways to improve on
this straightforward approach, and we hope to see
to what extent those improvements are generalizable
across different NLI corpora.
Acknowledgements
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. Pre-
sented at the 2011 Learner Corpus Research Con-
ference. Published in Sylviane Granger, Gae?tanelle
Gilquin and Fanny Meunier, editors, (2013) Twenty
Years of Learner Corpus Research: Looking back,
Moving ahead. Corpora and Language in Use - Pro-
ceedings 1, Louvain-la-Neuve: Presses universitaires
de Louvain.
Julian Brooke and Graeme Hirst. 2012a. Measuring in-
terlanguage: Native language identification with L1-
influence metrics. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC ?12), pages 779?784, Istanbul,
Turkey.
195
Julian Brooke and Graeme Hirst. 2012b. Robust, lexical-
ized native language identification. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING ?12).
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia (ICWSM 2009), San Jose, CA.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? in-
vestigating abstraction and domain dependence. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12).
Max Coltheart. 1980. MRC Psycholinguistic Database
User Manual: Version 1. Birkbeck College.
Scott A. Crossley and Danielle S. McNamara. 2012. De-
tecting the first language of second language writers
using automated indicies of cohesion, lexical sophis-
tication, syntactic complexity and conceptual knowl-
edge. In Scott Jarvis and Scott A. Crossley, editors,
Approaching Language Transfer through Text Clas-
sification: Explorations in the Detection-based Ap-
proach. Multilingual Matters.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvain-la-Neuve.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Geolo-
cation prediction in social media data by finding loca-
tion indicative words. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING ?12).
Shin?ichiro Ishikawa, 2011. A new horizon in learner
corpus studies: The aim of the ICNALE project, pages
3?11. University of Strathclyde Press, Glasgow, UK.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by min-
ing a text for errors. In Proceedings of the 11th
ACM SIGKDD International Conference on Knowl-
edge Discovery in Data Mining (KDD ?05), pages
624?628, Chicago, Illinois, USA.
Barbara Plank and Gertjan van Noord. 2011. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1566?1576, Portland, Oregon, USA,
June.
Ben Swanson and Eugene Charniak. 2012. Native lan-
guage detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ?12), pages
193?197, Jeju, Korea.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language acqui-
sition. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (NAACL HLT ?13).
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native lan-
guage identification. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING ?12).
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
Summary report on the first shared task on native lan-
guage identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Yukio Tono, Yuji Kawaguchi, and Makoto Minegishi,
editors. 2012. Developmental and Cross-linguistic
Perspectives in Learner Corpus Research. John Ben-
jamins, Amsterdam/Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?11), pages 1600?1610, Edinburgh, Scotland, UK.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL ?12), Jeju, Korea.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 180?189, Portland, Oregon.
196
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 112?121,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Annotating Anaphoric Shell Nouns with their Antecedents
Varada Kolhatkar
Department of Computer Science
University of Toronto
varada@cs.toronto.edu
Heike Zinsmeister
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
zinsmeis@ims.stuttgart-uni.de
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
Anaphoric shell nouns such as this is-
sue and this fact conceptually encapsulate
complex pieces of information (Schmid,
2000). We examine the feasibility of anno-
tating such anaphoric nouns using crowd-
sourcing. In particular, we present our
methodology for reliably annotating an-
tecedents of such anaphoric nouns and the
challenges we faced in doing so. We also
evaluated the quality of crowd annotation
using experts. The results suggest that
most of the crowd annotations were good
enough to use as training data for resolv-
ing such anaphoric nouns.
1 Introduction
Anaphoric shell nouns (ASNs) such as this fact,
this possibility, and this issue are common in all
kinds of text. They are called shell nouns be-
cause they provide nominal conceptual shells for
complex chunks of information representing ab-
stract concepts such as fact, proposition, and event
(Schmid, 2000). An example is shown in (1).
(1) Despite decades of education and widespread course
offerings, the survival rate for out-of-hospital car-
diac arrest remains a dismal 6 percent or less
worldwide.
This fact prompted the American Heart Association
last November to simplify the steps of CPR to make
it easier for lay people to remember and to encour-
age even those who have not been formally trained
to try it when needed.
Here, the ASN this fact encapsulates the clause
marked in bold from the preceding paragraph.
ASNs play an important role in organizing a dis-
course. First, they are used metadiscursively to
talk about the current discourse. In (1), the au-
thor characterizes the information presented in the
context by referring to it as a fact ? a thing that
is indisputably the case. Second, they are used as
cohesive devices in a discourse. In (1), for exam-
ple, this fact on the one hand refers to the propo-
sition marked in bold, and on the other, faces for-
ward and serves as the starting point of the follow-
ing paragraph. Finally, as Schmid (2000) points
out, like conjunctions so and however, ASNs
may function as topic boundary markers and topic
change markers.
Despite their importance, ASNs have not re-
ceived much attention in Computational Linguis-
tics. Although there has been some effort to anno-
tate certain anaphors with similar properties, i.e.,
demonstratives and the pronoun it (Byron, 2003;
Artstein and Poesio, 2006), in contrast to ordi-
nary nominal anaphora, there are not many anno-
tated corpora available that could be used to study
ASNs. Indeed, many questions of annotation of
ASNs must still be answered. For example, the
extent to which native speakers themselves agree
on the resolution of such anaphors, i.e., on the pre-
cise antecedents, remains unclear.
An essential first step in this field of research
is therefore to clearly establish the extent of inter-
annotator agreement on antecedents of ASNs as
a measure of feasibility of the task. In this pa-
per, we describe our methodology for annotating
ASNs using crowdsourcing, a cheap and fast way
of obtaining annotation. We also describe how we
evaluated the feasibility of the task and the quality
of the annotation, and the challenges we faced in
doing so, both with regard to the task itself and the
crowdsourcing platform we use. The results sug-
gest that most of the crowd-annotations were good
enough to use as training data for ASN resolution.
112
2 Related work
There exist only few annotated corpora of
anaphora with non-nominal antecedents (Dipper
and Zinsmeister, 2011). The largest one of these,
the ARRAU corpus (Poesio and Artstein, 2008),
contains 455 anaphors pointing to non-nominal
antecedents, but only a few instances are ASNs.
Kolhatkar and Hirst (2012) annotated antecedents
of the same type as we do, but restricted their ef-
forts to the ASN this issue.1 In addition, there are
corpora annotated with event anaphora in which
verbal instances are identified as proxies for non-
nominal antecedents (Pradhan et al, 2007; Chen
et al, 2011; Lee et al, 2012).
For the task of identifying non-nominal an-
tecedents as free spans of text, there is no standard
way of reporting inter-annotator agreement. Some
studies report only observed percentage agree-
ment with results in the range of about 0.40?
0.55 (Vieira et al, 2002; Dipper and Zinsmeis-
ter, 2011). The studies differed with respect to
number of annotators, types of anaphors, and lan-
guage of the corpora. Artstein and Poesio (2006)
discuss Krippendorff?s alpha for chance-corrected
agreement. They considered antecedent strings as
bags of words and computed the degree of differ-
ence between them by different distance measures
(e.g. Jaccard, Dice). The bag-of-words approach
is rather optimistic in the sense that even two non-
overlapping strings are very likely to share at least
a few words. Kolhatkar and Hirst (2012) followed
a different approach by using Krippendorff?s uni-
tizing alpha (u?) which considers the longest com-
mon subsequence of different antecedent options
(Krippendorff, 2013). They reported high chance-
corrected u? of 0.86 for two annotators but in a
very restricted domain.
There has been some prior effort to annotate
anaphora and coreference using Games with a
Purpose as a method of crowdsourcing (Chamber-
lain et al, 2009; Hladka? et al, 2009). Another, less
time-consuming approach of crowdsourcing is us-
ing platforms such as Amazon Mechanical Turk2.
It has been shown that crowdsourced data can suc-
cessfully be used as training data for NLP tasks
(Hsueh et al, 2009).
1Another data set reported in the literature could have
been relevant for us: Botley?s (2006) corpus contained about
462 ASN instances signaled by shell nouns; but this data is
no longer available (S. Botley, p.c.).
2https://mturk.com/mturk/
Class Description Examples
factual states of affairs fact, reason
linguistic linguistic acts question, report
mental thoughts and ideas issue, decision
modal subjective judgements possibility, truth
eventive events act, reaction
circumstantial situations situation, way
Table 1: Schmids categorization of shell nouns.
The nouns in boldface are used in this research.
3 The Anaphoric Shell Noun Corpus
Our goal is to obtain annotated data for ASN an-
tecedents that could be used to train a supervised
machine learning system to resolve ASNs. For
that, we created the Anaphoric Shell Noun (ASN)
corpus.
Schmid (2000) provides a list of 670 English
nouns which are frequently used as shell nouns.
He divides them into six broad semantic classes:
factual, mental, linguistic, modal, circumstantial,
and eventive. Table 1 shows this classification,
along with example shell nouns for each category.
To begin with, we considered articles contain-
ing occurrences of these 670 shell nouns from the
New York Times (NYT) corpus (about 711,046
occurrences).3 To create a corpus of a manage-
able size for annotation, we considered first 10
highly frequent shell nouns distributed across each
of Schmid?s shell noun categories from Table 1
and extracted ASN instances by searching for the
pattern {this shell noun} in these articles.4
To examine the feasibility of the annotation, we
systematically annotated sample data ourselves,
which contained about 15 examples of each of
these 10 highly frequent shell nouns. The anno-
tation process revealed that not all ASN instances
are easy to resolve. The instances with shell nouns
from the circumstantial and eventive categories, in
particular, had very long and unclear antecedents.
So we excluded these categories in this research
and work with six shell nouns from the other four
categories: fact, reason, issue, decision, question,
and possibility. To create the ASN corpus, we
extracted about 500 instances for each of these
six shell nouns. After removing duplicates and
instances with a non-abstract sense (e.g., this is-
3http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2008T19
4Schmid (2000) provides patterns for anaphoric shell
nouns, and this-NP is the most prominent pattern among
them.
113
sue with a publication-related sense), we were left
with 2,822 ASN instances.
4 ASN Annotation Challenges
ASN antecedent annotation is a complex task, as
it involves deeply understanding the discourse and
interpreting it. Here we point out two main chal-
lenges associated with the task.
What to annotate? The question of ?what to an-
notate? as mentioned by Fort et al (2012) is not
straightforward for ASN antecedents, as the no-
tion of markables is complex compared to ordi-
nary nominal anaphora: the units on which the
annotation work should focus are heterogeneous.5
Moreover, due to this heterogeneous nature of an-
notation units, there is a huge number of mark-
ables (e.g., all syntactic constituents given by a
syntactic parse tree). So there are many options
to choose from, while only a few units are actu-
ally to be annotated. Moreover, there is no one-
to-one correspondence between the syntactic type
of an antecedent and the semantic type of its refer-
ent (Webber, 1991). For instance, a semantic type
such as fact can be expressed with different syn-
tactic shapes such as a clause, a verb phrase, or a
complex sentence. Conversely, a syntactic shape,
such as a clause, can function as several semantic
types, including fact, proposition, and event.
Lack of the notion of the right answer It is not
obvious how to define clear and detailed annota-
tion guidelines to create a gold-standard corpus
for ASN antecedent annotation due to our limited
understanding of the nature and interpretation of
such antecedents. The notion of the right answer
is not well-defined for ASN antecedents. Indeed
most people will be hard-pressed to say whether
or not to include the clause Despite decades of
education and widespread course offerings in the
antecedent of this fact in example (1). The main
challenge is to identify the conditions when two
different candidates for annotation should be con-
sidered as representing essentially the same con-
cept, which raises deep philosophical issues that
we do not propose to solve in this paper. For our
purposes, we believe, this challenge could only
be possibly tackled by the requirements of down-
stream applications of ASN resolution.
5Occasionally, ASN antecedents are non-contiguous
spans of text, but in this work, we ignore them for simplicity.
5 Annotation Methodology
Considering the difficulties of ASN annotation
discussed above, there were two main challenges
involved in the annotation process: first, to find an-
notators who can annotate data reliably with min-
imal guidelines, and second, to design simple an-
notation tasks that will elicit data useful for our
purposes. Now we discuss how we dealt with
these challenges.
Crowdsourcing We wanted to examine to what
extent non-expert native speakers of English with
minimal annotation guidelines would agree on
ASN antecedents. We explored the possibility of
using crowdsourcing, which is an effective way to
obtain annotations for natural language research
(Snow et al, 2008). In particular, we explored the
use of CrowdFlower6, a crowdsourcing platform
that in turn uses various worker channels such as
Amazon Mechanical Turk. CrowdFlower offers a
number of features.
First, it offers a number of integrated quality-
control mechanisms. For instance, it throws gold
questions randomly at the annotators, and anno-
tators who do not answer them correctly are not
allowed to continue. To further minimize spam-
mers, it also offers a training phase before the ac-
tual annotation. In this phase, every annotator is
presented with a few gold questions. Only those
annotators who get the gold questions right get ad-
mittance to do the actual annotation.
Second, CrowdFlower chooses a unique answer
for each annotation unit based on the majority vote
of the trusted annotators. For each annotator, it
assigns a trust level based on how she performs
on the gold examples. The unique answer is com-
puted by adding together the trust scores of an-
notators, and then picking the answer with the
highest sum of trusts (CrowdFlower team, p.c.).
It also assigns a confidence score (denoted as c
henceforth) for each answer, which is a normal-
ized score of the summation of the trusts. For ex-
ample, suppose annotators A, B, and C with trust
levels 0.75, 0.75, and 1.0 give answers no, yes, yes
respectively for a particular instance. Then the an-
swer yes will score 1.75 and answer no will score
0.75 and yes will be chosen as the crowd?s answer
with c = 0.7 (i.e., 1.75/(1.75 + 0.75)). We use
these confidence scores in our analysis of inter-
annotator agreement below.
6http://crowdflower.com/
114
Finally, CrowdFlower also provides detailed an-
notation results including demographic informa-
tion and trustworthiness of each annotator.
Design of the annotation tasks With the help of
well-designed gold examples, CrowdFlower can
get rid of spammers and ensures that only reliable
annotators perform the annotation task. But the
annotation task must be well-designed in the first
place to get a good quality annotation. Following
the claim in the literature that with crowdsourc-
ing platforms simple tasks do best (Madnani et al,
2010; Wang et al, 2012), we split our annotation
task into two relatively simple sequential annota-
tion tasks. First, identifying the broad region of the
antecedent, i.e., not the precise antecedent but the
region where the antecedent lies, and second, iden-
tifying the precise antecedent, given the broad re-
gion of the antecedent. Now we will discuss each
of our annotation tasks in detail.
5.1 CrowdFlower experiment 1
The first annotation task was about identifying the
broad region of ASN antecedents without actu-
ally pinpointing the precise antecedents. We de-
fined the broad region as the sentence containing
the ASN antecedent, as the shell nouns we have
chosen tend to have antecedents that lie within
a single sentence. We designed a CrowdFlower
experiment where we presented to the annotators
ASNs from the ASN corpus with three preceding
paragraphs as context. Sentences in the vicinity
of ASNs were each labelled: four sentences pre-
ceding the anaphor, the sentence containing the
anaphor, and two sentences following the anaphor.
This choice was based on our pilot annotation:
the antecedents very rarely occur more than four
sentences away from the anaphor. The annota-
tion task was to pinpoint the sentence in the pre-
sented text that contained the antecedent for the
ASN and selecting the appropriate sentence label
as the correct answer. If no labelled sentence in the
presented text contained the antecedent, we sug-
gested to the annotators to select None. If the an-
tecedent spanned more than one sentence, then we
suggested to them to select Combination. We also
provided a link to the complete article from which
the text was drawn in case the annotators wanted
to have a look at it.
Settings We asked for 8 judgements per instance
and paid 8 cents per annotation unit. Our job
contained in total 2,822 annotation units with 168
gold units. As we were interested in the ver-
dict of native speakers of English, we limited the
allowed demographic region to English-speaking
countries.
5.2 CrowdFlower experiment 2
This annotation task was about pinpointing the
exact antecedent text of the ASN instances. We
designed a CrowdFlower experiment, where we
presented to the annotators ASN instances from
the ASN corpus with highlighted ASNs and the
sentences containing the antecedents, the output
of experiment 1. One way to pinpoint the ex-
act antecedent string is to ask the annotators to
mark free spans of text within the antecedent sen-
tence, similar to Byron (2003) and Artstein and
Poesio (2006). However, CrowdFlower quality-
control mechanisms require multiple-choice an-
notation labels. So we decided to display a set
of labelled candidates to the annotators and ask
them to choose the answer that best represents
the ASN antecedent. A practical requirement of
this approach is that the number of options to be
displayed be only a handful in order to make it
a feasible task for online annotation. But as we
noted in Section 4, the number of markables for
ASN antecedents is large. If, for example, we de-
fine markables as all syntactic constituents given
by the Stanford parser7, there are on average 49.5
such candidates per sentence in the ASN corpus. It
is not practical to display all these candidates and
to ask CrowdFlower annotators to choose one an-
swer from this many options. Also, some potential
candidates are clearly not appropriate candidates
for a particular shell noun. For instance, the NP
constituent the survival rate in example (1) is not
an appropriate candidate for the shell noun fact as
generally facts are propositions. So the question is
whether it is possible to restrict this set of candi-
dates by discarding unlikely ones.
To deal with this question, we used super-
vised machine learning methods trained on easy,
non-anaphoric unlabelled examples of shell nouns
(e.g., the fact that X). In this paper, we will focus
on the annotation and will treat these methods as a
black box. In brief, the methods reduce the large
search space of ASN antecedent candidates to a
size that is manageable for crowdsourcing anno-
tation, without eliminating the most likely candi-
7http://nlp.stanford.edu/software/
lex-parser.shtml
115
dates. We displayed the 10 most-likely candidates
given by these methods. In addition, we made sure
not to display two candidates with only a negli-
gible difference. For example, given two candi-
dates, X and that X, which differ only with respect
to the introductory that, we chose to display only
the longer candidate that X.
In a controlled annotation, with detailed guide-
lines, such difficulties of selecting between minor
variations could be avoided. However, such de-
tailed annotation guidelines still have to be devel-
oped.
Settings As in experiment 1, we asked for 8
judgements per instance and paid 6 cents per anno-
tation unit. But for this experiment we considered
only 2,323 annotation units with 151 gold units,
only high-confidence units (c ? 0.5) from experi-
ment 1. This task turned out to be a suitable task
for crowdsourcing as it offered a limited number
of options to choose from, instead of asking the
annotators to mark arbitrary spans of text.
6 Agreement
Our annotation tasks pose difficulties in measur-
ing inter-annotator agreement both in terms of the
task itself and the platform used for annotation. In
this section, we describe our attempt to compute
agreement for each of our annotation tasks and the
challenges we faced in doing so.
6.1 CrowdFlower experiment 1
Recall that in this experiment, annotators identify
the sentence containing the antecedent and select
the appropriate sentence label as their answer. We
know from our pilot annotation that the distribu-
tion of such labels is skewed: most of the ASN an-
tecedents lie in the sentence preceding the anaphor
sentence. We observed the same trend in the re-
sults of this experiment. In the ASN corpus, the
crowd chose the preceding sentence 64% of the
time, the same sentence 13% of the time, and long-
distance sentences 23% of the time.8 Consider-
ing the skewed distribution of labels, if we use tra-
ditional agreement coefficients, such as Cohen?s
? (1960) or Krippendorff?s ? (2013), expected
agreement is very high, which in turn results in a
low reliability coefficient (in our case ? = 0.61)
that does not necessarily reflect the true reliability
of the annotation (Artstein and Poesio, 2008).
8This confirms Passonneau?s (1989) observation that non-
nominal antecedents tend to be close to the anaphors.
F R I D Q P all
c < .5 8 8 36 21 13 7 16
.5? c < .6 6 6 13 8 7 5 8
.6? c < .8 24 25 31 31 22 27 27
.8? c < 1. 22 23 11 14 19 25 18
c = 1. 40 38 9 26 39 36 31
Average c .83 .82 .61 .72 .80 .83 .76
Table 2: CrowdFlower confidence distribution for
CrowdFlower experiment 1. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,822. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
One way to measure the reliability of the data,
without taking chance correction into account, is
to consider the distribution of the ASN instances
with different levels of CrowdFlower confidence.
Table 2 shows the percentages of instances in dif-
ferent confidence level bands for each shell noun
as well as for all instances. For example, for the
shell noun fact, 8% of the total number of this fact
instances were annotated with c < 0.5. As we
can see, most of the instances of the shell nouns
fact, reason, question, and possibility were anno-
tated with high confidence. In addition, most of
them occurred in the band 0.8 ? c ? 1. There
are relatively few instances with low confidence
for these nouns, suggesting the feasibility of re-
liable antecedent annotation for these nouns. By
contrast, the mental nouns issue and decision had
a large number of low-confidence (c < 0.5) in-
stances, bringing in the question of reliability of
antecedent annotation of these nouns.
Given these results with different confidence
levels, the primary question is what confidence
level should be considered acceptable? For our
task, we required that at least four trusted anno-
tators out of eight annotators should agree on an
answer for it to be acceptable.9 We will talk about
acceptability later in Section 7.
6.2 CrowdFlower experiment 2
Recall that this experiment was about identifying
the precise antecedent text segment given the sen-
tence containing the antecedent. It is not clear
what the best way to measure the amount of such
9We chose this threshold after systematically examining
instances with different confidence levels.
116
Jaccard Dice
Do De ? Do De ?
A&P .53 .95 .45 .43 .94 .55
Our results .47 .96 .51 .36 .92 .61
Table 3: Agreement using Krippendorff?s ? for
CrowdFlower experiment 2. A&P = Artstein and
Poesio (2006).
agreement is. Agreement coefficients such as Co-
hen?s ? underestimate the degree of agreement for
such annotation, suggesting disagreement even be-
tween two very similar annotated units (e.g., two
text segments that differ in just a word or two).
We present the agreement results in three different
ways: Krippendorff?s ? with distance metrics Jac-
card and Dice (Artstein and Poesio, 2006), Krip-
pendorff?s unitizing alpha (Krippendorff, 2013),
and CrowdFlower confidence values.
Krippendorff?s ? using Jaccard and Dice To
compare our agreement results with previous ef-
forts to annotate such antecedents, following Art-
stein and Poesio (2006), we computed Krippen-
dorff?s ? using distance metrics Jaccard and Dice.
The general form of coefficient ? is:
? = 1? Do
De
(1)
where Do and De are observed and expected dis-
agreements respectively. ? = 1 indicates perfect
reliability and u? = 0 indicates the absence of re-
liability. When u? < 0, either the sample size
is very small or the disagreement is systematic.
Table 3 shows the agreement results. Our agree-
ment results are comparable to Artstein and Poe-
sio?s agreement results. They had 20 annotators
annotating 16 anaphor instances with segment an-
tecedents, whereas we had 8 annotators annotat-
ing 2,323 ASN instances. As Artstein and Poesio
point out, expected disagreement in case of such
antecedent annotation is close to maximal, as there
is little overlap between segment antecedents of
different anaphors and therefore ? pretty much re-
flects the observed agreement.
Krippendorff?s unitizing ? (u?) Following
Kolhatkar and Hirst (2012), we use u? for measur-
ing reliability of the ASN antecedent annotation
task. This coefficient is appropriate when the an-
notators work on the same text, identify the units
in the text that are relevant to the given research
F R I D Q P all
c < .5 11 17 32 31 14 28 21
.5? c < .6 12 12 19 23 9 19 15
.6? c < .8 36 33 34 32 30 36 33
.8? c < 1. 24 22 10 10 21 13 18
c = 1. 17 16 5 3 26 4 13
Average c .74 .71 .60 .59 .77 .62 .68
Table 4: CrowdFlower confidence distribution for
CrowdFlower experiment 2. Each column shows
the distribution in percentages for confidence of
annotating antecedents of that shell noun. The fi-
nal row shows the average confidence of the dis-
tribution. Number of ASN instances = 2,323. F
= fact, R = reason, I = issue, D = decision, Q =
question, P = possibility.
question, and then label the identified units (Krip-
pendorff, p.c.). The general form of coefficient
u? is the same as in equation 1. In our context,
the annotators work on the same text, the ASN in-
stances. We define an elementary annotation unit
(the smallest separately judged unit) to be a word
token. The annotators identify and locate ASN
antecedents for the given anaphor in terms of se-
quences of elementary annotation units.
u? incorporates the notion of distance between
strings by using a distance function which is de-
fined as the square of the distance between the
non-overlapping tokens in our case. The distance
is 0 when the annotated units are exactly the same,
and is the summation of the squares of the un-
matched parts if they are different. We compute
observed and expected disagreement as explained
by Krippendorff (2013, Section 12.4). For our
data, u? was 0.54.10 u? was lower for the men-
tal nouns issue and decision and the modal noun
possibility compared to other shell nouns.
CrowdFlower confidence results We also ex-
amined different confidence levels for ASN an-
tecedent annotation. Table 4 gives confidence re-
sults for all instances and for each noun. In con-
trast with Table 2, the instances are more evenly
distributed here. As in experiment 1, the men-
tal nouns issue and decision had many low con-
fidence instances. For the modal noun possibility,
it was easy to identify the sentence containing the
antecedent, but pinpointing the precise antecedent
10Note that u? reported here is just an approximation of
the actual agreement as in our case the annotators chose an
option from a set of predefined options instead of marking
free spans of text.
117
turned out to be difficult.
Now we discuss the nature of disagreement in
ASN annotation.
Disagreement in experiment 1 There were two
primary sources of disagreement in experiment 1.
First, the annotators had problems agreeing on the
answer None. We instructed them to choose None
when the sentence containing the antecedent was
not labelled. Nonetheless, some annotators chose
sentences that did not precisely contain the actual
antecedent but just hinted at it. Second, sometimes
it was hard to identify the precise antecedent sen-
tence as the antecedent was either present in the
blend of all labelled sentences or there were multi-
ple possible answers, as shown in example (2).
(2) Any biography of Thomas More has to answer one
fundamental question. Why? Why, out of all the
many ambitious politicians of early Tudor England,
did only one refuse to acquiesce to a simple piece
of religious and political opportunism? What was it
about More that set him apart and doomed him to a
spectacularly avoidable execution?
The innovation of Peter Ackroyd?s new biography of
More is that he places the answer to this question
outside of More himself.
Here, the author formulates the question in a num-
ber of ways and any question mentioned in the
preceding text can serve as the antecedent of the
anaphor this question.
Hard instances Low agreement can indicate
different problems: unclear guidelines, poor-
quality annotators, or difficult instances (e.g., not
well understood linguistic phenomena) (Artstein
and Poesio, 2006). We can rule out the pos-
sibility of poor-quality annotators for two rea-
sons. First, we consider 8 diverse annotators
who work independently. Second, we use Crowd-
Flower?s quality-control mechanisms and hence
allow only trustworthy annotators to annotate our
texts. Regarding instructions, we take inter-
annotator agreement as a measure for feasibility of
the task, and hence we keep the annotation instruc-
tion as simple as possible. This could be a source
of low agreement. The third possibility is hard in-
stances. Our results show that the mental nouns
issue and decision had many low-confidence in-
stances, suggesting the difficulty associated with
the interpretation of these nouns (e.g., the very
idea of what counts as an issue is fuzzy). The shell
noun decision was harder because most of its in-
stances were court-decision related articles, which
were in general hard to understand.
Different strings representing similar concepts
As noted in Section 4, the main challenge with the
ASN annotation task is that different antecedent
candidates might represent the same concept and
it is not trivial to incorporate this idea in the anno-
tation process. When five trusted annotators iden-
tify the antecedent as but X and three trusted anno-
tators identify it as merely X, since CrowdFlower
will consider these two answers to be two com-
pletely different answers, it will give the answer
but X a confidence of only about 0.6. u? or ? with
Jaccard and Dice will not consider this as a com-
plete disagreement; however, the coefficients will
register it as a difference. In other words, the dif-
ference functions used with these coefficients do
not respond to semantics, paraphrases, and other
similarities that humans might judge as inconse-
quential. One way to deal with this problem would
be clustering the options that reflect essentially the
same concepts before measuring the agreement.
Some of these problems could also be avoided by
formulating instructions for marking antecedents
so that these differences do not occur in the iden-
tified antecedents. However, crowdsourcing plat-
forms require annotation guidelines to be clear and
minimal, which makes it difficult to control the an-
notation variations.
7 Evaluation of Crowd Annotation
CrowdFlower experiment 2 resulted in 1,810 ASN
instances with c > 0.5. The question is how good
are these annotations from experts? point of view.
To examine the quality of the crowd annotation
we asked two judges A and B to evaluate the ac-
ceptability of the crowd?s answers. The judges
were highly-qualified academic editors: A, a re-
searcher in Linguistics and B, a translator with a
Ph.D. in History and Philosophy of Science. From
the crowd-annotated ASN antecedent data, we
randomly selected 300 instances, 50 instances per
shell noun. We made sure to choose instances with
borderline confidence (0.5 ? c < 0.6), medium
confidence (0.6 ? c < 0.8), and high confidence
(0.8 ? c ? 1.0). We asked the judges to rate
the acceptability of the crowd-answers based on
the extent to which they provided interpretation of
the corresponding anaphor. We gave them four
options: perfectly (the crowd?s answer is perfect
and the judge would have chosen the same an-
tecedent), reasonably (the crowd?s answer is ac-
ceptable and is close to their answer),
118
Judge B
P R I N Total
Judge A
P 171 44 11 7 233
R 12 27 7 4 50
I 2 4 6 1 13
N 1 2 0 1 4
Total 186 77 24 13 300
Table 5: Evaluation of ASN antecedent annota-
tion. P = perfectly, R = reasonably, I = implicitly,
N = not at all
implicitly (the crowd?s answer only implicitly
contains the actual antecedent), and not at all (the
crowd?s answer is not in any way related to the
actual antecedent).11 Moreover, if they did not
mark perfectly, we asked them to provide their an-
tecedent string. The two judges worked on the task
independently and they were completely unaware
of how the annotation data was collected.
Table 5 shows the confusion matrix of the rat-
ings of the two judges. Judge B was stricter than
Judge A. Given the nature of the task, it was
encouraging that most of the crowd-antecedents
were rated as perfectly by both judges (72% by
A and 62% by B). Note that perfectly is rather a
strong evaluation for ASN antecedent annotation,
considering the nature of ASN antecedents them-
selves. If we weaken the acceptability criteria and
consider the antecedents rated as reasonably to be
also acceptable antecedents, 84.6% of the total in-
stances were acceptable according to both judges.
Regarding the instances marked implicitly, most
of the times the crowd?s answer was the closest
textual string of the judges? answer. So we again
might consider instances marked implicitly as ac-
ceptable answers.
For a very few instances (only about 5%) either
of the judges marked not at all. This was a posi-
tive result and suggests success of different steps
of our annotation procedure: identifying broad re-
gion, identifying the set of most likely candidates,
and identifying precise antecedent. As we can see
in Table 5, there were 7 instances where the judge
A rated perfectly while the judge B rated not at all,
i.e., completely contradictory judgements. When
we looked at these examples, they were rather hard
and ambiguous cases. An example is shown in (3).
The whether clause marked in the preceding sen-
11Before starting the actual annotation, we carried out a
training phase with 30 instances, which gave an opportunity
to the judges to ask questions about the task.
tence is the crowd?s answer. One of our judges
rated this answer as perfectly, while the other rated
it as not at all. According to her the correct an-
tecedent is whether Catholics who vote for Mr.
Kerry would have to go to confession.
(3) Several Vatican officials said, however, that any such
talk has little meaning because the church does not
take sides in elections. But the statements by several
American bishops that Catholics who vote for Mr.
Kerry would have to go to confession have raised
the question in many corners about whether this is
an official church position.
The church has not addressed this question publicly
and, in fact, seems reluctant to be dragged into the
fight...?
There was no notable relation between the an-
notator?s rating and the confidence level: many in-
stances with borderline confidence were marked
perfectly or reasonably, suggesting that instances
with c ? 0.5 were reasonably annotated instances,
to be used as training data for ASN resolution.
8 Conclusion
In this paper, we addressed the fundamental ques-
tion about feasibility of ASN antecedent annota-
tion, which is a necessary step before developing
computational approaches to resolve ASNs. We
carried out crowdsourcing experiments to get na-
tive speaker judgements on ASN antecedents. Our
results show that among 8 diverse annotators who
worked independently with a minimal set of an-
notation instructions, usually at least 4 annotators
converged on a single ASN antecedent. The re-
sult is quite encouraging considering the nature of
such antecedents.
We asked two highly-qualified judges to in-
dependently examine the quality of a sample of
crowd-annotated ASN antecedents. According to
both judges, about 95% of the crowd-annotations
were acceptable. We plan to use this crowd-
annotated data (1,810 instances) as training data
for an ASN resolver. We also plan to distribute the
annotations at a later date.
Acknowledgements
We thank the CrowdFlower team for their respon-
siveness and Hans-Jo?rg Schmid for helpful dis-
cussions. This material is based upon work sup-
ported by the United States Air Force and the De-
fense Advanced Research Projects Agency under
Contract No. FA8650-09-C-0179, Ontario/Baden-
Wu?rttemberg Faculty Research Exchange, and the
University of Toronto.
119
References
Ron Artstein and Massimo Poesio. 2006. Identify-
ing reference to abstract objects in dialogue. In
Proceedings of the 10th Workshop on the Semantics
and Pragmatics of Dialogue, pages 56?63, Potsdam,
Germany.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Simon Philip Botley. 2006. Indirect anaphora: Testing
the limits of corpus-based linguistics. International
Journal of Corpus Linguistics, 11(1):73?112.
Donna K. Byron. 2003. Annotation of pronouns and
their antecedents: A comparison of two domains.
Technical report, University of Rochester. Computer
Science Department.
Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-
sio. 2009. Constructing an anaphorically anno-
tated corpus with non-experts: Assessing the quality
of collaborative annotations. In Proceedings of the
2009 Workshop on The People?s Web Meets NLP:
Collaboratively Constructed Semantic Resources,
pages 57?62, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolu-
tion by integrating multiple resolvers. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 102?110, Chiang
Mai, Thailand, November.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37.
Stefanie Dipper and Heike Zinsmeister. 2011. Anno-
tating abstract anaphora. Language Resources and
Evaluation, 69:1?16.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In 24th Inter-
national Conference on Computational Linguistics,
pages 895?910.
Barbora Hladka?, Jir??? M??rovsky?, and Pavel Schlesinger.
2009. Play the language: Play coreference. In Pro-
ceedings of the Association of Computational Lin-
guistics and International Joint Conference on Nat-
ural Language Processing 2009 Conference Short
Papers, pages 209?212, Suntec, Singapore, August.
Association for Computational Linguistics.
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: A study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, pages 27?35, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Varada Kolhatkar and Graeme Hirst. 2012. Resolving
?this-issue? anaphora. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1255?1265, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Klaus Krippendorff. 2013. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, third edition.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500, Jeju Island, Korea, July. Association for
Computational Linguistics.
Nitin Madnani, Jordan Boyd-Graber, and Philip
Resnik. 2010. Measuring transitivity using un-
trained annotators. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, pages
188?194, Los Angeles, June. Association for Com-
putational Linguistics.
Rebecca J. Passonneau. 1989. Getting at discourse ref-
erents. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 51?59, Vancouver, British Columbia, Canada,
June. Association for Computational Linguistics.
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proeedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May. European Language Resources As-
sociation (ELRA).
Sameer S. Pradhan, Lance A. Ramshaw, Ralph M.
Weischedel, Jessica MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in OntoNotes. In Proceedings of
the International Conference on Semantic Comput-
ing, pages 446?453, September.
Hans-Jo?rg Schmid. 2000. English Abstract Nouns As
Conceptual Shells: From Corpus to Cognition. Top-
ics in English Linguistics 34. De Gruyter Mouton,
Berlin.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254?263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Renata Vieira, Susanne Salmon-Alt, Caroline
Gasperin, Emmanuel Schang, and Gabriel Othero.
2002. Coreference and anaphoric relations of
120
demonstrative noun phrases in multilingual corpus.
In Proceedings of the 4th Discourse Anaphora and
Anaphor Resolution Conference (DAARC 2002),
pages 385?427, Lisbon, Portugal, September.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.
2012. Perspectives on crowdsourcing annotations
for natural language processing. In Language Re-
sources and Evaluation, volume in press, pages 1?
23. Springer.
Bonnie Lynn Webber. 1991. Structure and ostension in
the interpretation of discourse deixis. In Language
and Cognitive Processes, pages 107?135.
121
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17?26,
Baltimore, Maryland USA, June 27, 2014.
c
?2014 Association for Computational Linguistics
Comparison of different feature sets for identification of variants in
progressive aphasia
Kathleen C. Fraser
1
, Graeme Hirst
1
, Naida L. Graham
2
, Jed A. Meltzer
3
,
Sandra E. Black
4
, and Elizabeth Rochon
2
1
Dept. of Computer Science, University of Toronto
2
Dept. of Speech-Language Pathology, University of Toronto, & Toronto Rehabilitation Institute
3
Rotman Research Institute, Baycrest Centre, Toronto
4
LC Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre, Toronto
{kfraser,gh}@cs.toronto.edu, {naida.graham,elizabeth.rochon}@utoronto.ca
jmeltzer@research.baycrest.org, sandra.black@sunnybrook.ca
Abstract
We use computational techniques to ex-
tract a large number of different features
from the narrative speech of individuals
with primary progressive aphasia (PPA).
We examine several different types of fea-
tures, including part-of-speech, complex-
ity, context-free grammar, fluency, psy-
cholinguistic, vocabulary richness, and
acoustic, and discuss the circumstances
under which they can be extracted. We
consider the task of training a machine
learning classifier to determine whether a
participant is a control, or has the fluent or
nonfluent variant of PPA. We first evaluate
the individual feature sets on their classifi-
cation accuracy, then perform an ablation
study to determine the optimal combina-
tion of feature sets. Finally, we rank the
features in four practical scenarios: given
audio data only, given unsegmented tran-
scripts only, given segmented transcripts
only, and given both audio and segmented
transcripts. We find that psycholinguis-
tic features are highly discriminative in
most cases, and that acoustic, context-free
grammar, and part-of-speech features can
also be important in some circumstances.
1 Introduction
In some types of dementia, such as primary pro-
gressive aphasia, language deficit is a core symp-
tom, and the analysis of narrative or conversa-
tional speech is important for assessing the extent
of an individual?s language impairment. Analy-
sis of connected speech has been limited in the
past because it is time-consuming and requires ex-
pert annotation. However, studies have shown that
it is possible for machine learning classifiers to
achieve high accuracy on some diagnostic tasks
when trained on features which were automati-
cally extracted from speech transcripts.
In this paper, we summarize previous research
on the automatic analysis of speech samples from
individuals with dementia, focusing in particular
on primary progressive aphasia. We discuss in de-
tail different types of features and compare their
effectiveness in the classification task. We sug-
gest some benefits and drawbacks of these differ-
ent features. We also examine the interactions be-
tween different feature sets, and discuss the rela-
tive importance of individual features across fea-
ture sets. Because we examine a large number
of features on a relatively small data set, we em-
phasize that this work is exploratory in nature;
nonetheless, our results are consistent with, and
extend, previous work in the field.
2 Background
In recent years, there has been growing interest in
using computer techniques to automatically detect
dementia from speech and language features de-
rived from a sample of narrative speech. Some re-
searchers have explored ways to use methods such
as part-of-speech tagging, statistical parsing, and
speech signal analysis to detect disorders such as
dementia of the Alzheimer?s type (DAT) (Bucks et
al., 2000; Singh et al., 2001; Thomas et al., 2005;
Jarrold et al., 2010) and mild cognitive impairment
(MCI) (Roark et al., 2011).
Here, we focus on a type of dementia called
primary progressive aphasia (PPA). PPA is a sub-
type of frontotemporal dementia (FTD) which is
characterized by progressive language impairment
without other notable cognitive impairment. There
are three subtypes of PPA: semantic dementia
(SD), progressive nonfluent aphasia (PNFA), and
logopenic progressive aphasia (LPA). SD, some-
times called ?fluent? progressive aphasia, is typi-
cally marked by fluent but empty speech, anomia,
17
deficits in comprehension, and spared grammar
and syntax (Gorno-Tempini et al., 2011). In
contrast, PNFA is characterized by halting and
sometimes agrammatic speech, reduced syntac-
tic complexity, word-finding difficulties, and rela-
tively spared single-word comprehension (Gorno-
Tempini et al., 2011). The third subtype, LPA, is
characterized by slow speech and frequent word
finding difficulties; this subtype is not included in
the current analysis.
Although clear diagnostic criteria for PPA have
been established (Gorno-Tempini et al., 2011),
there is no one test which can provide a diagno-
sis. Classification of PPA into subtypes requires
evaluation of spoken output, as well as neuropsy-
chological assessment and brain imaging. Quali-
tative evaluation of speech often can be done accu-
rately by clinicians or researchers, but the ability
to do this evaluation can require years of training
and experience. Some researchers have performed
detailed quantitative characterization of speech in
PPA, but the precise characteristics of speech are
not yet fully understood and this process is too
time-consuming for most clinicians.
Peintner et al. (2008) conducted one of the earli-
est automatic analyses of speech from individuals
with FTD, including SD and PNFA as well as a
behavioural variant. They considered psycholin-
guistic features as well as phoneme duration fea-
tures extracted from the audio samples. Although
they were fairly successful in classifying partici-
pants according to their subtype, they did not re-
port many details regarding the specific features
which were useful or how those features might re-
flect the underlying impairment of the speakers.
Pakhomov et al. (2010a) examined FTD speech
from an information-theoretic approach. They
constructed a language model of healthy control
speech, and then calculated the perplexity and out-
of-vocabulary rate for each of the patient groups
relative to that model. In another study, Pakhomov
et al. (2010b) extracted speech and language fea-
tures from samples of FTD speech. In a principal
components analysis, they discovered four com-
ponents which accounted for most of the variance
in their data: speech length, hesitancy, empty con-
tent, and grammaticality. However, they did not
perform any classification experiments.
Fraser et al. (2013a) attempted to classify par-
ticipants as either SD patients, PNFA patients, or
healthy controls using a large number of language
SD
(N = 11)
PNFA
(N = 13)
Control
(N = 16)
Male/Female 8/3 7/6 9/7
Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2)
Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3)
Table 1: Demographic information. Numbers are
given in the form: mean (standard deviation).
features extracted from manually-transcribed tran-
scripts. They distinguished between SD and con-
trol participants with very high accuracy, and were
also successful at distinguishing between PNFA
and control participants. However, their method
did not perform as well on the task of classify-
ing SD vs. PNFA speakers. In subsequent work
(Fraser et al., 2013b), they expanded their feature
set to include acoustic features extracted directly
from the audio file.
3 Methods
3.1 Data
Twenty-four patients with PPA were recruited
through three Toronto memory clinics, and 16 age-
and education-matched healthy controls were re-
cruited through a volunteer pool. All participants
were native speakers of English, or had completed
some of their education in English. Exclusion cri-
teria included a known history of drug or alcohol
abuse and a history of neurological or major psy-
chiatric illness. Each patient was diagnosed by a
behavioural neurologist and all met current crite-
ria for PPA (Gorno-Tempini et al., 2011). Table 1
shows demographic information for each group.
To elicit a sample of narrative speech, partici-
pants were asked to tell the well-known story of
Cinderella. They were given a wordless picture
book to remind them of the story; then the book
was removed and they were asked to tell the story
in their own words. This procedure, described in
full by Saffran et al. (1989), is commonly used in
studies of connected speech in aphasia.
The narrative samples were transcribed by
trained research assistants. The transcriptions in-
clude filled pauses, repetitions, and false starts,
and were annotated with the total speech time.
Sentence boundaries were marked according to se-
mantic, syntactic, and prosodic cues.
3.2 Classification framework
Given the audio files and transcripts, we can then
calculate our features (described in detail below)
18
and use them to train a support vector machine
(SVM) classifier. We use a leave-one-out cross-
validation framework and report the average ac-
curacy (i.e. proportion of correctly classified in-
stances) across folds. We optimize the complexity
parameter and the kernel type in a nested cross-
validation loop over the training set. For compar-
ison, we also tested a na??ve Bayes classifier; how-
ever we found that the results were consistently
poorer and we do not report them here.
3.3 Features
In the following sections we will describe each of
the feature sets that we use and explain how the
features are computed, and we will discuss some
of the potential advantages and disadvantages as-
sociated with each set. In particular, we discuss
what types of data are necessary for the extraction
of these features. The data types are: unsegmented
transcripts, segmented transcripts, and audio.
3.3.1 Part-of-speech features
Different categories of words may be selectively
impaired in different types of dementia. In PPA,
individuals with SD tend to be more impaired
with respect to nouns than verbs, and may replace
nouns with pronouns or circumlocutory phrases.
In contrast, individuals with PNFA may have more
difficulty with verbs and may even demonstrate
agrammatism, which can result in the omission
of grammatical morphemes and function words.
Thus, it is often useful to compare the relative fre-
quencies with which words representing the differ-
ent parts-of-speech (POS) are produced in a sam-
ple, as in Table 2. Similar features have been re-
ported in computational studies of MCI (Roark et
al., 2011), FTD (Pakhomov et al., 2010b), and
DAT (Bucks et al., 2000). Numerous POS taggers
exist, although we use the Stanford tagger here
(Toutanova et al., 2003).
3.3.2 Complexity features
Changes in linguistic complexity may accompany
the onset of dementia, although some studies have
found a decrease in complexity (e.g. Kemper et al.
(2001)) while others have found an increase (e.g.
Le et al. (2011)).
The features in Table 3 vary in their ease of
computation. Mean word length can be calculated
from an unsegmented transcript, while mean sen-
tence length requires only sentence boundary seg-
mentation. Other measures, such as Yngve depth
Nouns # nouns / # words
Verbs # verbs / # words
Noun-verb ratio # nouns / # verbs
Noun ratio # nouns / (# nouns + # verbs)
Inflected verbs # inflected verbs / # verbs
Determiners # determiners / # words
Demonstratives # demonstratives / # words
Prepositions # prepositions / # words
Adjectives # adjectives / # words
Adverbs # adverbs / # words
Pronoun ratio # pronouns / (# nouns + # pronouns)
Function words # function words / # words
Interjections # interjections / # words
Table 2: Part-of-speech features.
Max depth maximum Yngve depth of each parse tree,
averaged over all sentences
Mean depth mean Yngve depth of each node in the
parse tree, averaged over all sentences
Total depth total sum of the Yngve depths of each node
in the parse tree, averaged over all sentences
Tree height height of each parse tree, averaged over
all sentences
MLS mean length of sentence
MLC mean length of clause
MLT mean length of T-unit
Subordinate conjunctions number of subordinate
conjunctions
Coordinate conjunctions number of coordinate con-
junctions
Subordinate:coordinate ratio ratio of number of sub-
ordinate conjunctions to number of coordinate
conjunctions
Mean word length mean length, in letters, of each
word in the sample
Table 3: Complexity features.
(Yngve, 1960), require full parses of the sentences
(we use the Stanford parser (Klein and Manning,
2003) and Lu?s Syntactic Complexity Analyzer
(Lu, 2010)).
3.3.3 CFG features
Although many of the complexity features above
are derived from parse trees, in this section we
present a set of features that take into account
the context-free grammar (CFG) labels on each
of the nodes. CFG features have been previously
used to assess the grammaticality of sentences in
an artificial error corpus (Wong and Dras, 2010)
and to distinguish human from machine transla-
tions (Chae and Nenkova, 2009). However, this
is the first time such features have been applied to
speech from participants with dementia.
In Table 4 we list a few examples of our 134
CFG features, as well as the three phrase-level fea-
tures (calculated for noun phrases, verb phrases,
and prepositional phrases).
19
NP ? NNS Noun phrases consisting of only a plural
noun
VP ? VBN PP Verb phrases consisting of a past-
participle verb and a prepositional phrase
ROOT? INTJ Trees consisting of only an interjec-
tion
Phrase type proportion Length of each phrase type
(noun phrase, verb phrase, or prepositional
phrase), divided by total narrative length
Average phrase type length Total number of words in
a phrase type, divided by the number of phrases
of that type
Phrase type rate Number of phrases of a given type,
divided by total narrative length
Table 4: CFG features.
Um Frequency of filled pause um
Uh Frequency of filled pause uh
NID Frequency of words Not In Dictionary (e.g. para-
phasias, neologisms)
Verbal rate Number of words per minute
Total words Total number of words produced
Table 5: Fluency features.
3.3.4 Fluency features
Park et al. (2011) found that listeners? judgements
of fluency were affected by a number of different
variables, and the three most discriminative fea-
tures were ?speech rate, speech productivity, and
audible struggle.? For our list of fluency features
(Table 5), we include only those features which
could be extracted from the transcripts alone (as-
suming the total speech time is given). We count
pauses filled by um and uh separately, as research
has suggested that they may indicate different cog-
nitive processes (Clark and Fox Tree, 2002).
The number of words in a sample could be eas-
ily generated using the word count feature in most
text-editing software (although we first exclude
filled pauses and NID tokens), and the verbal rate
can subsequently be calculated directly. The other
three features are easily calculated using string
matching and an electronic dictionary.
3.3.5 Psycholinguistic features
Some types of dementia are characterized by im-
pairments in semantic access. Such impairments
may be sensitive to psycholinguistic features such
as lexical frequency, familiarity, imageability, and
age of acquisition (Table 6). We use the SUBTL
frequency norms (Brysbaert and New, 2009) and
the combined Bristol and Gilhooly-Logie norms
(Stadthagen-Gonzalez and Davis, 2006; Gilhooly
and Logie, 1980) for familiarity, imageability, and
Frequency Frequency with which a word occurs in
some corpus of natural language
Familiarity Subjective rating of how familiar a word
seems
Imageability Subjective rating of how easily a word
generates an image in the mind
Age of acquisition Subjective rating of how old a per-
son is when they first learn that word
Light verbs Number of occurrences of be, have, come,
go, give, take, make, do, get, move, and put,
normalized by total number of verbs
Table 6: Psycholinguistic features.
age of acquisition (see Table 6). We compute the
average of each of these measures for all content
words, as well as for nouns and verbs separately.
Another measure that fits into this category is
the frequency of occurrence of light verbs, which
an impaired speaker may use to replace a more
specific verb. We use the same list of light verbs
as Breedin et al. (1998), given in Table 6.
One challenge associated with psycholinguis-
tic features is finding norms which provide ade-
quate coverage for the given data. Fraser et al.
(2013a) reported that the SUBTL frequency norms
had a coverage of above 90% on their data, but the
Bristol-Gilhooly-Logie norms had a coverage of
only around 30%.
3.3.6 Vocabulary richness features
Individuals experiencing semantic difficulty may
use a limited range of vocabulary. We can mea-
sure the vocabulary richness or lexical diversity
of a narrative sample using a number of different
metrics (see Table 7). Type-token ratio has been
a popular choice, perhaps due to its simplicity;
however it is sensitive to the length of the sample.
Bucks et al. (2000) were the first to apply Honor?e?s
statistic and Brun?et?s index to the study of demen-
tia, and found significant differences between in-
dividuals with DAT and healthy controls. Cov-
ington and McFall (2010) proposed a new mea-
sure called the moving-average type-token ratio
(MATTR), which is independent of text length.
This feature was later applied to aphasic speech in
a study by Fergadiotis and Wright (2011), and was
found to be one of the most unbiased indicators of
lexical diversity in impaired speakers.
The measures given in Table 7 are easily com-
puted from their respective formulae. In this work,
we lemmatize each word using NLTK (Bird et
al., 2009) before calculating the features. For
MATTR, we consider w = 10,20,30,40,50.
20
Honor
?
e?s statistic N
V
?0.165
/ where V is the number of
word types and N is the number of word tokens.
Brun
?
et?s index 100logN/(1?V
1
/V ) where V
1
is the
number of words used only once, V is the total
number of word types, and N is the number of
word tokens.
Type-token ratio (TTR) V/N where V is the num-
ber of word types and N is the number of word
tokens.
Moving-average type-token ratio (MATTR
w
) TTR
calculated over a moving window of size w,
and averaged over all windows.
Table 7: Vocabulary richness features.
3.3.7 Acoustic features
What we call acoustic features are extracted di-
rectly from the audio file. We consider the fea-
tures given in Table 8. Acoustic features have been
shown to be useful when automatically detecting
conditions such as Parkinson?s disease, in which
changes in speech are common (Little et al., 2009;
Tsanas et al., 2012). Acoustic features have also
been examined in studies of DAT (Meil?an et al.,
2014), FTD (Pakhomov et al., 2010b), and PPA
(Fraser et al., 2013b, whose software we use here).
An obvious benefit to acoustic features is that
they do not require a transcription, and can be cal-
culated immediately given an audio sample. The
corresponding drawback is that they tell us noth-
ing about the linguistic content of the narrative.
4 Experiments
We report the results of three experiments explor-
ing the discriminative power of the different fea-
tures. We first compare the classification accura-
cies using each individual feature set. We then per-
form an ablation study to determine which com-
bination of feature sets leads to the highest clas-
sification accuracy. We also look at individual
features across sets and discuss which ones are
the most discriminative, particularly in situations
where data might be limited.
4.1 Individual comparison of accuracy by set
The accuracies which result from using each fea-
ture set individually are given in Table 9. The
highest accuracy across the three tasks is achieved
in distinguishing SD participants from controls.
An accuracy of .963 can be achieved using all
the features together, or using the psycholinguis-
tic or POS features alone. This is consistent with
the semantic impairments that are observed in SD.
Total duration of speech Total length of all non-silent
segments
Phonation rate Total duration of speech / total dura-
tion of the sample (including pauses)
Mean pause duration Mean length of pauses > 0.15
ms
Short pause count # Pauses > 0.15 ms and < 0.4 ms
Long pause count # Pauses > 0.4 ms
Pause:word ratio Ratio of silent segments longer than
150 ms to non-silent segments
F
0:3
mean Mean of the fundamental frequency and the
first three formant frequencies
F
0:3
variance Variance of the fundamental frequency
and the first three formant frequencies
Mean instantaneous power Measure related to the
loudness of the signal
Mean 1st ACF Mean first autocorrelation function
Max 1st ACF Maximum first autocorrelation function
Skewness Measure of lack of symmetry, associated
with tense or creaky voice
Kurtosis Measure of the peakedness of the signal
ZCR Zero-crossing rate, can be used to distinguish
between voiced and unvoiced regions
MRPDE Mean recurrence period density entropy, a
measure of periodicity
Jitter Measure of the short-term variation in the pitch
(frequency) of a voice
Shimmer Measure of the short-term variation in the
loudness (amplitude) of a voice
Table 8: Acoustic features.
The measures of vocabulary richness do not distin-
guish between the SD and control groups, suggest-
ing it is the words themselves, and not the number
of different words being used, that is important.
In the case of PNFA participants vs. controls,
we find that the highest accuracy is achieved us-
ing all the features, and the second highest by us-
ing only acoustic features. This is not surprising,
considering that the acoustic features include mea-
sures of pausing and phonation rate, which can
detect the characteristic halting speech of PNFA.
The third best accuracy is achieved using the flu-
ency features, which also fits with this explana-
tion. However, we might have expected that the
complexity and CFG features would be more sen-
sitive to the grammatical impairments of PNFA.
Finally, the best accuracy for SD vs. PNFA
is lower than in the previous two cases, and is
achieved using only CFG features. This sug-
gests that there are some grammatical construc-
tions which occur with different frequencies in
the two groups. These differences do not appear
to be captured by the complexity features, which
could explain why Fraser et al. (2013a) did not find
syntactic differences between the SD and PNFA
groups. Interestingly, the results using CFG fea-
21
Feature set
SD vs.
controls
PNFA vs.
controls
SD vs.
PNFA
All .963 .931 .708
Acoustic .778 .862 .167
Psycholinguistic .963 .724 .708
POS .963 .690 .375
Complexity .852 .621 .667
Fluency .667 .828 .500
Vocab. richness .481 .586 .583
CFG .630 .690 .792
Table 9: Classification accuracies for each feature
set individually using a SVM classifier. Bold indi-
cates the highest accuracy for each task.
tures are actually higher than the results using all
features. This demonstrates that classifier perfor-
mance can be adversely affected by the presence
of irrelevant features, especially in small data sets.
4.2 Combining feature sets
In the previous section we examined the feature
sets individually; however, one type of feature
may complement the information contained in an-
other feature set, or it may contain redundant in-
formation. To examine the interactions between
the feature sets, we perform an ablation study.
Starting with all the features, we remove each fea-
ture set one at a time and measure the accuracy
of the classifier. The feature set whose removal
causes the smallest decrease in accuracy is then re-
moved permanently from the experiment, the rea-
soning being that the most important feature sets
will cause the greatest decrease in accuracy when
removed. In some cases, we observe that the clas-
sification accuracy actually increases when a set
is removed, which suggests that those features are
not relevant to the classification (at least in combi-
nation with the other sets). In the case of a tie, we
remove the feature set whose individual classifica-
tion accuracy on that task is lowest. The procedure
is then repeated on the remaining feature sets, con-
tinuing until only one set remains.
The results for SD vs. controls are given in Ta-
ble 10a. The best result, 1.00, is achieved by
combining the psycholinguistic and POS features.
This is unsurprising, since each of those feature
sets perform well individually. Curiously, the
same result can also be achieved by also including
the complexity, vocabulary richness, and CFG fea-
tures, but not in the intermediate stages between
those two optimal sets. We attribute this to the in-
teractions between features and the small data set.
For PNFA vs. controls, shown in Table 10b, the
(a) SD vs. controls.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .963
F A+P+POS+C+VR+CFG .963
A P+POS+C+VR+CFG 1.00
VR P+POS+C+CFG .926
CFG P+POS+C .926
C P+POS 1.00
POS P .963
(b) PNFA vs. controls.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .931
VR A+P+POS+C+F+CFG .931
C A+P+POS+F+CFG .931
POS A+P+F+CFG .931
CFG A+P+F .966
F A+P .966
P A .862
(c) SD vs. PNFA.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .708
POS A+P+C+F+VR+CFG .750
VR A+P+C+F+CFG .833
F A+P+C+CFG .833
A P+C+CFG .792
C P+CFG .917
P CFG .792
Table 10: A=acoustic, P=psycholinguistic,
POS=part-of-speech, C=complexity, F=fluency,
VR=vocabulary richness, CFG=CFG production
rule features. Bold indicates the highest accuracy
with the fewest feature sets.
best result of .966 is achieved using a combina-
tion of acoustic and psycholinguistic features. In
this case the removal of the fluency features, which
gave the second highest individual accuracy, does
not make a difference to the accuracy. This sug-
gests that the fluency features contain similar in-
formation to one of the remaining sets, presum-
ably the acoustic set.
In the case of SD vs. PNFA, we again see that
the best accuracy can be achieved by combining
two feature sets, as shown in Table 10c. Us-
ing psycholinguistic and CFG features, we can
achieve an accuracy of .917, a substantial im-
provement over the best accuracy for this task in
Table 9. In fact, in all three cases we see that us-
ing a carefully selected combination of feature sets
can result in better accuracy than using all the fea-
ture sets together or using any one set individually.
4.3 Best features for incomplete data
Up to this point, we have examined complete fea-
ture sets. We now briefly explore which individual
22
features are the most discriminative across feature
sets. We approach this as a practical consideration:
given the data that a researcher has, and limited re-
sources, what are the best features to measure? We
consider the following four scenarios:
1. Given audio files only. This scenario often
arises because it is relatively easy to record
speech, but difficult to have it transcribed.
Only acoustic features can be extracted.
2. Given basic transcriptions only (no audio).
We assume there is no sentence segmentation
and the time is not marked (e.g. as in the out-
put of automatic speech recognition). Thus,
we can measure psycholinguistic, POS, and
vocabulary measures. We can also measure
the fluency features except for verbal rate,
as well as mean word length and subordi-
nate/coordinate conjunctions from the com-
plexity set. Without sentence boundaries, we
cannot parse the transcripts.
3. Given fully segmented transcripts (no audio).
We can measure all features except for acous-
tic features.
4. Given audio and fully segmented transcripts.
We can measure all features.
For each scenario, we rank the available fea-
tures by their ?
2
value and choose the top 10 only
as input to the SVM classifier (see Manning et al.
(2008) for a complete explanation of ?
2
feature se-
lection). We only include features if ?
2
> 0, so in
cases where there are very few relevant features,
fewer than 10 features may be selected. Because
we perform cross-validation, the selected features
may vary across different folds. In the tables that
follow, we present the features ranked by the num-
ber of folds in which they appear (i.e. a feature
with the value 1.00 was selected in every fold).
Due to space constraints, only the top 10 ranked
features are shown.
The results for Scenario 1 are given in Ta-
ble 11a. For the SD vs. controls and PNFA vs.
controls, the most highly ranked features tend to
be related to fluency and rate of speech, as well
as voice quality (skewness and MRPDE). How-
ever, when distinguishing between the two patient
groups, the acoustic features are essentially use-
less. In most cases, we see that none of the acous-
tic features had a non-zero ?
2
value, and thus the
classifier could not be properly trained.
For Scenario 2 (Table 11b), the results for SD
vs. controls show that within the psycholinguistic
and POS feature sets, features relating to familiar-
ity and frequency are very important, as well as
nouns and demonstratives. In the PNFA vs. con-
trols case, we see that a number of the vocabulary
richness features are selected, which is in contrast
to the previous two experiments. However, it ap-
pears that only the MATTR feature is important
(with varying window lengths), so when we con-
sidered only full feature sets, that information was
obscured by the other, irrelevant features in that
set. The SD vs. PNFA case shows a mix of fea-
tures from the previous two cases.
For Scenario 3 (Table 11c), we add the com-
plexity and CFG features. These features do not
have a large effect in the SD vs. controls case, but
a few CFG features are selected in the PNFA vs.
controls and SD vs. PNFA cases.
In Scenario 4 (Table 11d), we consider all fea-
tures. In the SD vs. controls case this increases
the accuracy. However, for PNFA vs. controls and
SD vs. PNFA, the classification accuracy actually
decreases, relative to Scenario 3. When the num-
ber of features increases, the potential to overfit to
the training data fold also increases, and it seems
likely that that is occurring here. Nonetheless, we
expect that the features which are selected in every
fold are still highly relevant. These features are
unchanged between Scenarios 3 and 4 in the SD
vs. controls and SD vs. PNFA case, however in the
PNFA vs. controls case, the acoustic features are
now ranked more highly than some of the vocabu-
lary richness and CFG features from Scenario 3.
5 Discussion
While it may be tempting to calculate as many
features as possible and use them all in a classi-
fier, we have shown here that better results can be
achieved by choosing a small, relevant subset of
features. In particular, psycholinguistic features
such as frequency and familiarity were useful in all
three classification tasks. Acoustic features were
useful in discriminating patients from controls, but
not for discriminating between the two PPA sub-
types. We also found that MATTR was relevant
in some cases, although the other vocabulary rich-
ness features were not, and that the CFG features
were more useful than traditional measures of syn-
tactic complexity. POS features were useful only
in distinguishing between SD and controls.
One of the biggest challenges in this type
of work is the small amount of data available.
23
(a) Scenario 1: audio only.
SD vs. control, Acc: .852 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .500
1.00 skewness 1.00 long pause count .083 max 1st ACF
1.00 phonation rate 1.00 phonation rate .042 mean F3
1.00 MRPDE 1.00 short pause count
1.00 mean duration of pauses 1.00 MRPDE
.037 long pause count 1.00 mean duration of pauses
.037 mean 1st ACF .966 pause:word ratio
.037 kurtosis .793 skewness
.793 ZCR
.345 mean inst. power
.035 jitter
(b) Scenario 2: unsegmented transcripts.
SD vs. control, Acc: .926 PNFA vs. control, Acc: .621 SD vs. PNFA, Acc: .792
1.00 familiarity 1.00 MATTR 50 1.00 familiarity
1.00 noun frequency 1.00 MATTR 40 1.00 noun frequency
1.00 noun familiarity 1.00 MATTR 30 1.00 noun familiarity
1.00 frequency 1.00 frequency 1.00 MATTR 20
1.00 verb frequency 1.00 MATTR 20 .708 MATTR 10
1.00 nouns .931 total words .208 MATTR 30
1.00 demonstratives .759 light verbs .042 MATTR 50
.778 pronoun ratio .690 adjectives .042 MATTR 40
.667 noun imageability .241 age of acquisition .042 light verbs
.630 Honor?e?s statistic .241 MATTR 10 .042 verbs
(c) Scenario 3: segmented transcripts.
SD vs. control, Acc: .926 PNFA vs. control, Acc: .897 SD vs. PNFA, Acc: .792
1.00 word length 1.00 MATTR 50 1.00 WHADVP?WRB
1.00 familiarity 1.00 MATTR 40 1.00 familiarity
1.00 noun frequency 1.00 WHNP?WP 1.00 noun familiarity
1.00 noun familiarity 1.00 frequency 1.00 noun frequency
1.00 frequency 1.00 MATTR 20 1.00 MATTR 20
1.00 demonstratives 1.00 verbal rate 1.00 NP? NNS
.889 nouns .966 MATTR 30 1.00 SBAR?WHADVP S
.852 verb frequency .827 S1? INTJ .667 MATTR 10
.630 MLS .483 total words .500 NP? DT JJ NNS
.630 total Yngve depth .414 word length .458 SQ? AUX NP VP
(d) Scenario 4: segmented transcripts + audio.
SD vs. control, Acc: .963 PNFA vs. control, Acc: .793 SD vs. PNFA, Acc: .750
1.00 word length 1.00 frequency 1.00 WHADVP?WRB
1.00 familiarity 1.00 phonation rate 1.00 familiarity
1.00 noun frequency 1.00 MRPDE 1.00 noun familiarity
1.00 noun familiarity 1.00 verbal rate 1.00 noun frequency
1.00 frequency 1.00 mean duration of pauses 1.00 MATTR 20
1.00 demonstratives .897 MATTR 50 1.00 NP? NNS
.963 phonation rate .897 WHNP?WP 1.00 SBAR?WHADVP S
.741 verb frequency .897 MATTR 20 .625 MATTR 10
.593 nouns .690 MATTR 40 .500 NP? DT JJ NNS
.333 MLS .690 MATTR 30 .458 SQ? AUX NP VP
Table 11: Classification accuracies and top 10 features for four different data scenarios.
Psychological studies are typically on the or-
der of only tens to possibly hundreds of partic-
ipants, while machine learning researchers often
tackle problems with thousands to millions of data
points. We have chosen techniques appropriate for
small data sets, but acknowledging the potential
weaknesses of machine learning methods when
training data are limited, these findings must be
considered preliminary. However, we also believe
that this is a promising approach for future ap-
plications, including automated screening for lan-
guage impairment, support for clinical diagnosis,
tracking severity of symptoms over time, and eval-
uating therapeutic interventions.
Acknowledgments
This research was supported by the Natural Sciences and En-
gineering Research Council of Canada and the Canadian In-
stitutes of Health Research (grant #MOP-8277). Thanks to
Frank Rudzicz for the acoustic features software.
24
References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
language processing with Python. O?Reilly Media, Inc.
Sarah D. Breedin, Eleanor M. Saffran, and Myrna F.
Schwartz. 1998. Semantic factors in verb retrieval: An
effect of complexity. Brain and Language, 63:1?31.
Marc Brysbaert and Boris New. 2009. Moving beyond
Ku?cera and Francis: A critical evaluation of current word
frequency norms and the introduction of a new and im-
proved word frequency measure for American English.
Behavior Research Methods, 41(4):977?990.
R.S. Bucks, S. Singh, J.M. Cuerden, and G.K. Wilcock.
2000. Analysis of spontaneous, conversational speech in
dementia of Alzheimer type: Evaluation of an objective
technique for analysing lexical performance. Aphasiol-
ogy, 14(1):71?91.
Jieun Chae and Ani Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies of ma-
chine translation and human-written text. In Proceedings
of the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 139?147.
Association for Computational Linguistics.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh and
um in spontaneous speaking. Cognition, 84(1):73?111.
Michael A. Covington and Joe D. McFall. 2010. Cutting
the Gordian knot: The moving-average type?token ratio
(MATTR). Journal of Quantitative Linguistics, 17(2):94?
100.
Gerasimos Fergadiotis and Heather Harris Wright. 2011.
Lexical diversity for adults with and without apha-
sia across discourse elicitation tasks. Aphasiology,
25(11):1414?1430.
Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, Carol
Leonard, Graeme Hirst, Sandra E. Black, and Elizabeth
Rochon. 2013a. Automated classification of primary
progressive aphasia subtypes from narrative speech tran-
scripts. Cortex.
Kathleen C. Fraser, Frank Rudzicz, and Elizabeth Rochon.
2013b. Using text and acoustic features to diagnose pro-
gressive aphasia and its subtypes. In Proceedings of Inter-
speech.
K.J. Gilhooly and R.H. Logie. 1980. Age-of-acquisition, im-
agery, concreteness, familiarity, and ambiguity measures
for 1,944 words. Behavior Research Methods, 12:395?
427.
M.L. Gorno-Tempini, A.E. Hillis, S. Weintraub, A. Kertesz,
M. Mendez, S.F. Cappa, J.M. Ogar, J.D. Rohrer, S. Black,
B.F. Boeve, F. Manes, N.F. Dronkers, R. Vandenberghe,
K. Rascovsky, K. Patterson, B.L. Miller, D.S. Knopman,
J.R. Hodges, M.M. Mesulam, and M. Grossman. 2011.
Classification of primary progressive aphasia and its vari-
ants. Neurology, 76:1006?1014.
William Jarrold, Bart Peintner, Eric Yeh, Ruth Krasnow,
Harold Javitz, and Gary Swan. 2010. Language ana-
lytics for assessing brain health: Cognitive impairment,
depression and pre-symptomatic Alzheimer?s disease. In
Yiyu Yao, Ron Sun, Tomaso Poggio, Jiming Liu, Ning
Zhong, and Jimmy Huang, editors, Brain Informatics, vol-
ume 6334 of Lecture Notes in Computer Science, pages
299?307. Springer Berlin / Heidelberg.
Susan Kemper, Marilyn Thompson, and Janet Marquis.
2001. Longitudinal change in language production: Ef-
fects of aging and dementia on grammatical complex-
ity and propositional content. Psychology and Aging,
16(4):600?614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st Meeting
of the Association for Computational Linguistics, pages
423?430.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina Jokel.
2011. Longitudinal detection of dementia through lex-
ical and syntactic changes in writing: a case study of
three British novelists. Literary and Linguistic Comput-
ing, 26(4):435?461.
Max A. Little, Patrick E. McSharry, Eric J. Hunter, Jennifer
Spielman, and Lorraine O. Ramig. 2009. Suitability
of dysphonia measurements for telemonitoring of Parkin-
son?s disease. Biomedical Engineering, IEEE Transac-
tions on, 56(4):1015?1022.
Xiaofei Lu. 2010. Automatic analysis of syntactic complex-
ity in second language writing. International Journal of
Corpus Linguistics, 15(4):474?496.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich
Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Juan Jos?e G. Meil?an, Francisco Mart??nez-S?anchez, Juan
Carro, Dolores E. L?opez, Lymarie Millian-Morell, and
Jos?e M. Arana. 2014. Speech in Alzheimer?s disease:
Can temporal and acoustic parameters discriminate de-
mentia? Dementia and Geriatric Cognitive Disorders,
37(5-6):327?334.
Serguei V.S. Pakhomov, Glen E. Smith, Susan Marino, An-
gela Birnbaum, Neill Graff-Radford, Richard Caselli,
Bradley Boeve, and David D. Knopman. 2010a. A com-
puterized technique to asses language use patterns in pa-
tients with frontotemporal dementia. Journal of Neurolin-
guistics, 23:127?144.
S.V. Pakhomov, G.E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman. 2010b.
Computerized analysis of speech and language to identify
psycholinguistic correlates of frontotemporal lobar degen-
eration. Cognitive and Behavioral Neurology, 23:165?
177.
Hyejin Park, Yvonne Rogalski, Amy D. Rodriguez, Zvinka
Zlatar, Michelle Benjamin, Stacy Harnish, Jeffrey Ben-
nett, John C. Rosenbek, Bruce Crosson, and Jamie Reilly.
2011. Perceptual cues used by listeners to discriminate
fluent from nonfluent narrative discourse. Aphasiology,
25(9):998?1015.
Bart Peintner, William Jarrold, Dimitra Vergyri, Colleen
Richey, Maria Luisa Gorno Tempini, and Jennifer Ogar.
2008. Learning diagnostic models using speech and lan-
guage measures. In Engineering in Medicine and Biol-
ogy Society, 2008. EMBS 2008. 30th Annual International
Conference of the IEEE, pages 4648?4651.
Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy
Hollingshead, and Jeffery Kaye. 2011. Spoken language
derived measures for detecting mild cognitive impairment.
IEEE Transactions on Audio, Speech, and Language Pro-
cessing, 19(7):2081?2090.
25
Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.
Schwartz. 1989. The quantitative analysis of agrammatic
production: procedure and data. Brain and Language,
37:440?479.
Sameer Singh, Romola S. Bucks, and Joanne M. Cuerden.
2001. Evaluation of an objective technique for analysing
temporal variables in DAT spontaneous speech. Aphasiol-
ogy, 15(6):571?583.
Hans Stadthagen-Gonzalez and Colin J. Davis. 2006. The
Bristol norms for age of acquisition, imageability, and fa-
miliarity. Behavior Research Methods, 38(4):598?605.
Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth Rock-
wood, and Elissa Asp. 2005. Automatic detection and
rating of dementia of Alzheimer type through lexical anal-
ysis of spontaneous speech. In Proceedings of the IEEE
International Conference on Mechatronics and Automa-
tion, pages 1569?1574.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies, pages 252?259.
Athanasios Tsanas, Max A. Little, Patrick E. McSharry, Jen-
nifer Spielman, and Lorraine O. Ramig. 2012. Novel
speech signal processing algorithms for high-accuracy
classification of Parkinson?s disease. IEEE Transactions
on Biomedical Engineering, 59(5):1264?1271.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features
for sentence grammaticality classification. In Proceed-
ings of the Australasian Language Technology Association
Workshop, pages 67?75.
Victor Yngve. 1960. A model and hypothesis for language
structure. Proceedings of the American Physical Society,
104:444?466.
26
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 134?142,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Using statistical parsing to detect agrammatic aphasia
Kathleen C. Fraser
1
, Graeme Hirst
1
, Jed A. Meltzer
2
,
Jennifer E. Mack
3
, and Cynthia K. Thompson
3,4,5
1
Dept. of Computer Science, University of Toronto
2
Rotman Research Institute, Baycrest Centre, Toronto
3
Dept. of Communication Sciences and Disorders, Northwestern University
4
Dept. of Neurology, Northwestern University
4
Cognitive Neurology and Alzheimer?s Disease Center, Northwestern University
{kfraser,gh}@cs.toronto.edu, jmeltzer@research.baycrest.org
{jennifer-mack-0,ckthom}@northwestern.edu
Abstract
Agrammatic aphasia is a serious language
impairment which can occur after a stroke
or traumatic brain injury. We present
an automatic method for analyzing apha-
sic speech using surface level parse fea-
tures and context-free grammar produc-
tion rules. Examining these features in-
dividually, we show that we can uncover
many of the same characteristics of agram-
matic language that have been reported
in studies using manual analysis. When
taken together, these parse features can
be used to train a classifier to accurately
predict whether or not an individual has
aphasia. Furthermore, we find that the
parse features can lead to higher classifica-
tion accuracies than traditional measures
of syntactic complexity. Finally, we find
that a minimal amount of pre-processing
can lead to better results than using either
the raw data or highly processed data.
1 Introduction
After a stroke or head injury, individuals may
experience aphasia, an impairment in the ability
to comprehend or produce language. The type
of aphasia depends on the location of the lesion.
However, even two patients with the same type
of aphasia may experience different symptoms. A
careful analysis of narrative speech can reveal spe-
cific patterns of impairment, and help a clinician
determine whether an individual has aphasia, what
type of aphasia it is, and how the symptoms are
changing over time.
In this paper, we present an automatic method
for the analysis of one type of aphasia, agram-
matic aphasia. characterized by the omission of
function words, the omission or substitution of
morphological markers for person and number, the
absence of verb inflection, and a relative increase
in the number of nouns and decrease in the number
of verbs (Bastiaanse and Thompson, 2012). There
is often a reduction in the variety of different syn-
tactic structures used, as well as a reduction in the
complexity of those structures (Progovac, 2006).
There may also be a strong tendency to use the
canonical word order of a language, for example
subject-verb-object in English (Progovac, 2006).
Most studies of narrative speech in agrammatic
aphasia are based on manually annotated speech
transcripts. This type of analysis can provide de-
tailed and accurate information about the speech
patterns that are observed. However, it is also very
time consuming and requires trained transcribers
and annotators. Studies are necessarily limited to
a manageable size, and the level of agreement be-
tween annotators can vary.
We propose an automatic approach that uses in-
formation from statistical parsers to examine prop-
erties of narrative speech. We extract context-
free grammar (CFG) production rules as well as
phrase-level features from syntactic parses of the
speech transcripts. We show that this approach can
detect many features which have been previously
reported in the aphasia literature, and that classifi-
cation of agrammatic patients and controls can be
achieved with high accuracy.
We also examine the effects of including speech
dysfluencies in the transcripts. Dysfluencies and
non-narrative words are usually removed from the
transcripts as a pre-processing step, but we show
that by retaining some of these items, we can ac-
tually achieve a higher classification accuracy than
by using the completely clean transcripts.
Finally, we investigate whether there is any ben-
efit to using the parse features instead of more tra-
ditional measures of syntactic complexity, such as
Yngve depth or mean sentence length. We find
that the parse features convey more information
134
about the specific syntactic structures being pro-
duced (or avoided) by the agrammatic speakers,
and lead to better classification accuracies.
2 Related Work
2.1 Syntactic analysis of agrammatic
narrative speech
Much of the previous work analyzing narrative
speech in agrammatic aphasia has been performed
manually. One widely used protocol is called
Quantitative Production Analysis (QPA), devel-
oped by Saffran et al. (1989). QPA can be used to
measure morphological content, such as whether
determiners and verb inflections are produced in
obligatory contexts, as well as structural complex-
ity, such as the number of embedded clauses per
sentence. Subsequent studies have found a num-
ber of differences between normal and agrammatic
speech using QPA (Rochon et al., 2000). Another
popular protocol called the Northwestern Narra-
tive Language Analysis (NNLA) was introduced
by Thompson et al. (1995). This protocol analyzes
each utterance at five different levels, and focuses
in particular on the production of verbs and verb
argument structure.
Perhaps more analogous to our work here,
Goodglass et al. (1994) conducted a detailed ex-
amination of the syntactic constituents used by
aphasic patients and controls. In that study, utter-
ances were grouped according to how many syn-
tactic constituents they contained. They found
that agrammatic participants were more likely to
produce single-constituent utterances, especially
noun phrases, and less likely to produce subor-
dinate clauses. They also found that agrammatic
speakers sometimes produced two-constituent ut-
terances consisting of only a subject and object,
with no verb. This pattern was never observed in
control speech.
A much smaller body of work explores the use
of computational techniques to analyze agramma-
tism. Holmes and Singh (1996) analyzed conver-
sational speech from aphasic speakers and con-
trols. Their features mostly included measures
of vocabulary richness and frequency counts of
various parts-of-speech (e.g. nouns, verbs); how-
ever they also measured ?clause-like semantic unit
rate?. This feature was intended to measure the
speaker?s ability to cluster words together, al-
though it is not clear what the criteria for segment-
ing clause-like units were or whether it was done
manually or automatically. Nonetheless, it was
found to be one of the most important variables
for distinguishing between patients and controls.
MacWhinney et al. (2011) presented several ex-
amples of how researchers can use the Aphasia-
Bank
1
database and associated software tools to
conduct automatic analyses (although the tran-
scripts are first hand-coded for errors by experi-
enced speech-language pathologists). Specifically
with regards to syntax, they calculated several fre-
quency counts and ratios for different parts-of-
speech and bound morphemes. There was one
extension beyond treating each word individually:
this involved searching for pre-defined colloca-
tions such as once upon a time or happily ever af-
ter, which were found to occur more rarely in the
patient transcripts than in the control transcripts.
We present an alternative, automated method of
analysis. We do not attempt to fully replicate the
results of the manual studies, but rather provide
a complementary set of features which can indi-
cate grammatic abnormalities. Unlike previous
computational studies, we attempt to move beyond
single-word analysis and examine which patterns
of syntax might indicate agrammatism.
2.2 Using parse features to assess
grammaticality
Syntactic complexity metrics derived from parse
trees have been used by various researchers in
studies of mild cognitive impairment (Roark et al.,
2011), autism (Prud?hommeaux et al., 2011), and
child language development (Sagae et al., 2005;
Hassanali et al., 2013). Here we focus specifically
on the use of CFG production rules as features.
Using the CFG production rules from statistical
parsers as features was first proposed by Baayen
et al. (1996), who applied the features to an au-
thorship attribution task. More recently, similar
features have been widely used in native language
identification (Wong and Dras, 2011; Brooke and
Hirst, 2012; Swanson and Charniak, 2012). Per-
haps most relevant to the task at hand, CFG pro-
ductions as well as other parse outputs have proved
useful for judging the grammaticality and fluency
of sentences. For example, Wong and Dras (2010)
used CFG productions to classify sentences from
an artificial error corpus as being either grammat-
ical or ungrammatical.
Taking a different approach, Chae and Nenkova
1
http://talkbank.org/AphasiaBank/
135
Agrammatic
(N = 24)
Control
(N = 15)
Male/Female 15/9 8/7
Age (years) 58.1 (10.6) 63.3 (6.4)
Education (years) 16.3 (2.5) 16.4 (2.4)
Table 1: Demographic information. Numbers are
given in the form: mean (standard deviation).
(2009) calculated several surface features based on
the output of a parser, such as the length and rel-
ative proportion of different phrase types. They
used these features to distinguish between human
and machine translations, and to determine which
of a pair of translations was the more fluent. How-
ever, to our knowledge there has been no work us-
ing parser outputs to assess the grammaticality of
speech from individuals with post-stroke aphasia.
3 Data
3.1 Participants
This was a retrospective analysis of data col-
lected by the the Aphasia and Neurolinguistics Re-
search Laboratory at Northwestern University. All
agrammatic participants had experienced a stroke
at least 1 year prior to the narrative sample col-
lection. Demographic information for the partic-
ipants is given in Table 1. There is no significant
(p < 0.05) difference between the patient and con-
trol groups on age or level of education.
3.2 Narrative task
To obtain a narrative sample, the participants were
asked to relate the well-known fairy tale Cin-
derella. Each participant was first given a word-
less picture book of the story to look through. The
book was then removed, and the participant was
asked to tell the story in his or her own words. The
examiner did not interrupt or ask questions.
The narratives were recorded and later tran-
scribed following the NNLA protocol. The data
was segmented into utterances based on syntac-
tic and prosodic cues. Filled pauses, repetitions,
false starts, and revisional phrases (e.g. I mean)
were all placed inside parentheses. The average
length of the raw transcripts was 332 words for
agrammatic participants and 387 words for con-
trols; when the non-narrative words were excluded
the average length was 194 words for the agram-
matic group and 330 for controls.
4 Methods
4.1 Parser Features
We consider two types of features: CFG pro-
duction rules and phrase-level statistics. For the
CFG production rules, we use the Charniak parser
(Charniak, 2000) trained on Wall Street Journal
data to parse each utterance in the transcript and
then extract the set of non-lexical productions.
The total number of types of productions is large,
many of them occurring very infrequently, so we
compile a list of the 50 most frequently occurring
productions in each of the two groups (agrammatic
and controls) and use the combined set as the set
of features. The feature values can be binary (does
a particular production rule appear in the narrative
or not?) or integer (how many times does a rule oc-
cur?). The CFG non-terminal symbols follow the
Penn Treebank naming conventions.
For our phrase-level statistics, we use a subset
of the features described by Chae and Nenkova
(2009), which are related to the incidence of dif-
ferent phrase types. We consider three different
phrase types: noun phrases, verb phrases, and
prepositional phrases. These features are defined
as follows:
? Phrase type proportion: Length of each
phrase type (including embedded phrases),
divided by total narrative length.
? Average phrase length: Total number of
words in a phrase type, divided by number
of phrases of that type.
? Phrase type rate: Number of phrases of a
given type, divided by total narrative length.
Because we are judging the grammaticality of
the entire narrative, we normalize by narrative
length (rather than sentence length, as in Chae and
Nenkova?s study). These features are real-valued.
We first perform the analysis on the transcribed
data with the dysfluencies removed, labeled the
?clean? dataset. This is the version of the tran-
script that would be used in the manual NNLA
analysis. However, it is the result of human ef-
fort and expertise. To test the robustness of the
system on data that has not been annotated in this
way, we also use the ?raw? dataset, with no dys-
fluencies removed (i.e. including everything inside
the parentheses), and an ?auto-cleaned? dataset,
in which filled pauses are automatically removed
from the raw transcripts. We also use a simple al-
gorithm to remove ?stutters? and false starts, by
136
removing non-word tokens of length one or two
(e.g. C- C- Cinderella would become simply Cin-
derella). This provides a more realistic view of the
performance of our system on real data. We also
hypothesize that there may be important informa-
tion to be found in the dysfluent speech segments.
4.2 Feature weighting and selection
We assume that some production rules will be
more relevant to the classification than others,
and so we want to weight the features accord-
ingly. Using term frequency?inverse document
frequency (tf-idf) would be one possibility; how-
ever, the tf-idf weights do not take into account
any class information. Supervised term weight-
ing (STW), has been proposed by Debole and Se-
bastiani (2004) as an alternative to tf-idf for text
classification tasks. In this weighting scheme, fea-
ture weights are assigned using the same algo-
rithm that is used for feature selection. For ex-
ample, one way to select features is to rank them
by their information gain (InfoGain). In STW,
the InfoGain value for each feature is also used
to replace the idf term. This can be expressed as
W (i,d) = df(i,d)? InfoGain(i), where W (i,d) is
the weight assigned to feature i in document d,
df(i,d) is the frequency of occurrence of feature i
in document d, and InfoGain(i) is the information
gain of feature i across all the training documents.
We considered two different methods of STW:
weighting by InfoGain and weighting by gain ratio
(GainRatio). The methods were also used as fea-
ture selection, since any feature that was assigned
a weight of zero was removed from the classifi-
cation. We also consider tf-idf weights and un-
weighted features for comparison.
4.3 Syntactic complexity metrics
To compare the performance of the parse features
with more-traditional syntactic complexity met-
rics (SC metrics), we calculate the mean length of
utterance (MLU), mean length of T-unit
2
(MLT),
mean length of clause (MLC), and parse tree
height. We also calculate the mean, maximum,
and total Yngve depth, which measures the pro-
portion of left-branching to right-branching in
each parse tree (Yngve, 1960). These measures
are commonly used in studies of impaired lan-
guage (e.g. Roark et al. (2011), Prud?hommeaux et
2
A T-unit consists of a main clause and its attached de-
pendent clauses.
al. (2011), Fraser et al. (2013b)). We hypothesize
that the parse features will capture more informa-
tion about the specific impairments seen in agram-
matic aphasia; however, using the general mea-
sures of syntactic complexity may be sufficient for
the classifiers to distinguish between the groups.
4.4 Classification
To test whether the features can effectively distin-
guish between the agrammatic group and controls,
we use them to train and test a machine learn-
ing classifier. We test three different classifica-
tion algorithms: naive Bayes (NB), support vec-
tor machine (SVM), and random forests (RF). We
use a leave-one-out cross-validation framework, in
which one transcript is held out as a test set, and
the other transcripts form the training data. The
feature weights are calculated on the training set
and then applied to the test set (as a result, each
fold of training/testing may use different features
and feature weights). The SVM and RF algo-
rithms are tuned in a nested cross-validation loop.
The classifier is then tested on the held-out point.
This procedure is repeated across all data points,
and the average accuracy is reported.
A baseline classifier which assigns all data to
the largest class would achieve an accuracy of .62
on this classification task. For a more realistic
measure of performance, we also compare our re-
sults to the baseline accuracy that can be achieved
using only the length of the narrative as input.
5 Results
5.1 Features using clean transcripts
We first present the results for the clean tran-
scripts. Although different features may be se-
lected in each fold of the cross-validation, for sim-
plicity we show only the feature rankings on the
whole data set. Table 2 shows the top features as
ranked by GainRatio. The frequencies are given to
indicate the direction of the trend; they represent
the average frequency per narrative for each class
(agrammatic = AG and control = CT). Boldface
indicates the group with the higher frequency. As-
terisks are used to indicate the significance of the
difference between the groups.
When working with clinical data, careful exam-
ination of the features can be beneficial. By com-
paring features with previous findings in the liter-
ature on agrammatism, we can be confident that
we are measuring real effects and not just artifacts
137
Rule AG
freq
CT
freq
p
1 PP? IN NP 10.3 24.9
???
2 ROOT? NP 2.9 0.2
???
3 NP? DT NN POS 0.0 0.7
?
4 NP? PRP$ JJ NN 0.5 0.7
?
5 VP? TO VP 4.2 7.5
?
6 NP? NNP 5.9 6.6
7 VP? VB PP 1.1 2.9
??
8 VP? VP CC VP 1.1 3.1
??
9 NP? DT NN NN 1.0 2.7
??
10 VP? VBD VP 0.1 0.5
?
11 WHADVP?WRB 0.5 1.4
?
12 FRAG? NP . 0.7 0.0
??
13 NP? JJ NN 0.7 0.0
??
14 SBAR?WHNP S 1.7 3.1
?
15 NP? NP SBAR 1.6 2.5
16 S? NP VP 7.8 16.1
??
17 NP? PRP$ JJ NNS 0.0 0.5
?
18 NP? PRP$ NN NNS 0.0 0.6
?
19 SBAR?WHADVP S 0.4 1.2
?
20 VP? VBN PP 0.4 2.0
?
Table 2: Top 20 features ranked by GainRatio us-
ing the clean transcripts. (
?
p < 0.05,
??
p < 0.005,
???
p < 0.0005).
of the parsing algorithm. This can also poten-
tially provide an opportunity to observe features of
agrammatic speech that have not been examined in
manual analyses. We examine the top-ranked fea-
tures in Table 2 in some detail, especially as they
relate to previous work on agrammatism. In par-
ticular, the top features suggest some of the fol-
lowing features of agrammatic speech:
? Reduced number of prepositional phrases.
This is suggested by feature 1, PP? IN NP.
It is also reflected in features 7 and 20.
? Impairment in using verbs. We can see in fea-
ture 2 (ROOT ? NP) that there is a greater
number of utterances consisting of only a
noun phrase. Feature 12 is also consistent
with this pattern (FRAG ? NP .). We also
observe a reduced number of coordinated
verb phrases (VP? VP CC VP).
? Omission of grammatical morphemes and
function words. The agrammatic speakers
use fewer possessives (NP? DT NN POS).
Feature 9 indicates that the control partic-
ipants more frequently produce compound
NB SVM RF
Narrative length .62 .56 .64
Binary, no weights .87 .87 .77
Binary, tf-idf .87 .90 .85
Binary, InfoGain .82 .90 .74
Binary, GainRatio .90 .82 .79
Frequency, no weights .90 .85 .85
Frequency, tf-idf .85 .82 .77
Frequency, InfoGain .90 .90 .82
Frequncy, GainRatio .90 .92 .74
SC metrics, no weights .85 .77 .82
SC metrics, InfoGain .85 .77 .79
SC metrics, GainRatio .85 .77 .82
Table 3: Average classification accuracy using the
clean transcripts. The highest classification accu-
racy for each feature set is indicated with boldface.
nouns with a determiner (often the glass
slipper or the fairy godmother). Feature 4
also suggests some difficulty with determin-
ers, as the agrammatic participants produce
fewer nouns modified by a possessive pro-
noun and an adjective. Contrast this with fea-
ture 13, which shows agrammatic speech is
more likely to contain noun phrases contain-
ing just an adjective and a noun. For example,
in the control narratives we are more likely to
see phrases such as her godmother . . . waves
her magic wand, while in the agrammatic
narratives phrases like Cinderella had wicked
stepmother are more common.
? Reduced number of embedded clauses and
phrases. Evidence for this can be found in
the reduced number of wh-adverb phrases
(WHADVP?WRB), as well as features 14,
15, and 19.
The results of our classification experiment on
the clean data are shown in Table 3. The results
are similar for the binary and frequency features,
with the best result of .92 achieved using an SVM
classifier and frequency features, with GainRatio
weights. The best results using parse features
(.85?.92) are the same or slightly better than the
best results using SC features (.85), and both fea-
ture sets perform above baseline.
5.2 Effect of non-narrative speech
In this section we perform two additional experi-
ments, using the raw and auto-cleaned transcripts.
138
Rule AG
freq.
CT
freq.
p
1 NP? DT NN POS 0.0 0.5
?
2 PP? IN NP 12.2 26.1
???
3 SBAR?WHADVP S 0.4 1.5
?
4 VP? VBD 0.75 1.1
5 VP? TO VP 4.3 7.3
?
6 S? CC PP NP VP . 0.04 0.5
?
7 NP? PRP$ JJ NNS 0.04 0.5
?
8 VP? AUX VP 3.7 6.0
9 ROOT? FRAG 4.5 0.7
??
10 ADVP? RB 9.8 12.3
11 NP? NNP 4.4 6.2
?
12 NP? DT NN 15.0 24.1
??
13 VP? VB PP 1.2 2.8
?
14 VP? VP CC VP 1.0 2.9
?
15 WHADVP?WRB 0.6 1.5
?
16 VP? VBN PP 0.4 2.0
?
17 INTJ? UH UH 3.5 0.3
?
18 VP? VBP NP 0.5 0.0
?
19 NP? NNP NNP 1.5 0.5
??
20 S? CC ADVP NP VP . 1.3 2.3
Table 4: Top 20 features ranked by GainRatio
using the raw transcripts. Bold feature numbers
indicate rules which did not appear in Table 2.
(
?
p < 0.05,
??
p < 0.005,
???
p < 0.0005).
We discuss the differences between the selected
features in each case, and the resulting classifica-
tion accuracies.
Using the raw transcripts, we find that the rank-
ing of features is markedly different than with the
human-annotated transcripts (Table 4, bold feature
numbers). Examining these production rules more
closely, we observe some characteristics of agram-
matic speech which were not detectable in the an-
notated transcripts:
? Increased number of dysfluencies. We ob-
serve a higher number of consecutive fillers
(INTJ ? UH UH) in the agrammatic data,
as well as a higher number of consecutive
proper nouns (NP ? NNP NNP), usually
two attempts at Cinderella?s name. Feature
18 (VP? VBP NP) also appears to support
this trend, although it is not immediately ob-
vious. Most of the control participants tell
the story in the past tense, and if they do
use the present tense then the verbs are of-
ten in the third-person singular (Cinderella
finds her fairy godmother). Looking at the
data, we found that feature 18 can indicate a
verb agreement error, as in he attend the ball.
However, in almost twice as many cases it in-
dicates use of the discourse markers I mean
and you know, followed by a repaired or tar-
get noun phrase.
? Decreased connection between sentences.
Feature 6 shows a canonical NP VP sentence,
preceded by a coordinate conjunction and a
prepositional phrase. Some examples of this
from the control transcripts include, And at
the stroke of midnight . . . and And in the pro-
cess . . . . The conjunction creates a connec-
tion from one utterance to the next, and the
prepositional phrase indicates the temporal
relationship between events in the story, cre-
ating a sense of cohesion. See also the similar
pattern in feature 20, representing sentence
beginnings such as And then . . . .
However, there are some features which were
highly ranked in the clean transcripts but do not
appear in Table 4. What information are we losing
by using the raw data? One issue with using the
raw transcripts is that the inclusion of filled pauses
?splits? the counts for some features. For example,
the feature FRAG? NP . is ranked 12th using the
clean transcripts but does not appear in the top 20
when using the raw transcripts. When we examine
the transcripts, we find that the phrases that are
counted in this feature in the clean transcripts are
actually split into three features in the raw tran-
scripts: FRAG? NP ., FRAG? INTJ NP ., and
FRAG? NP INTJ ..
The classification results for the raw transcripts
are given in Table 5. The results are similar to
those for the clean transcripts, although in this
case the best accuracy (.92) is achieved in three
different configurations (all using the SVM clas-
sifier). The phrase-level features out-perform the
traditional SC measures in only half the cases.
Using the auto-cleaned transcripts, we see some
similarities with the previous cases (Table 6).
However, some of the highly ranked features
which disappeared when using the raw transcripts
are now significant again (e.g. ROOT ? NP,
FRAG ? NP .). There are also three remain-
ing features which are significant and have not yet
been discussed. Feature 9 shows an increased use
of determiners with proper nouns (e.g. the Cin-
derella), a frank grammatical error. Feature 20
139
NB SVM RF
Narrative length .51 .62 .69
Binary, no weights .87 .92 .82
Binary, tf-idf .87 .92 .72
Binary, InfoGain .85 .87 .82
Binary, GainRatio .82 .87 .85
Frequency, no weights .85 .90 .69
Frequency, tf-idf .82 .92 .90
Frequency, InfoGain .85 .74 .85
Frequncy, GainRatio .85 .74 .82
SC metrics, no weights .74 .79 .82
SC metrics, InfoGain .77 .85 .85
SC metrics, GainRatio .77 .85 .87
Table 5: Average classification accuracy using
raw transcripts. The highest classification accu-
racy for each feature set is indicated with boldface.
provides another example of a sentence fragment
with no verb. Finally, feature 19 represents an in-
creased number of sentences or clauses consist-
ing of a noun phrase followed by adjective phrase.
Looking at the transcripts, this is not generally in-
dicative of an error, but rather use of the word
okay, as in she dropped her shoe okay.
The classification results for the auto-cleaned
data, shown in Table 7, show a somewhat differ-
ent pattern from the previous experiments. The
accuracies using the parse features are generally
higher, and the best result of .97 is achieved using
the binary features and the naive Bayes classifier.
Interestingly, this data set also results in the lowest
accuracy for the syntactic complexity metrics.
5.3 Phrase-level parse features
The classifiers in Tables 3, 5, and 7 used the
phrase-level parse features as well as the CFG
productions. Although these features were cal-
culated for NPs, VPs, and PPs, the NP features
were never selected by the GainRatio ranking al-
gorithm, and did not differ significantly between
groups. The significance levels of the VP and PP
features are reported in Table 8. PP rate and pro-
portion are significantly different in all three sets
of transcripts, which is consistent with the high
ranking of PP ? IN NP in each case. VP rate
and proportion are often significant, although less
so. Notably, PP and VP length are both significant
in the clean transcripts, but not significant in the
raw transcripts and only barely significant in the
auto-cleaned transcripts.
Rule AG
freq.
CT
freq.
p
1 PP? IN NP 12.0 26.0
???
2 NP? DT NN POS 0.0 0.7
?
3 VP? VP CC VP 0.8 2.9
??
4 S? CC SBAR NP VP . 0.0 0.5
5 SBAR?WHADVP S 0.4 1.5
?
6 NP? NNP 5.6 6.7
7 VP? VBD 0.8 1.1
8 S? CC PP NP VP . 0.04 0.6
?
9 NP? DT NNP 0.6 0.0
??
10 VP? TO VP 4.6 7.5
?
11 ROOT? FRAG 3.0 0.5
???
12 ROOT? NP 2.1 0.1
?
13 VP? VBP NP 1.7 3.6
14 NP? PRP$ JJ NNS 0.04 0.5
?
15 VP? VB PP 1.1 2.8
??
16 VP? VBN PP 0.4 1.9
?
17 FRAG? NP . 0.4 0.0
?
18 NP? NNP . 2.1 0.1
19 S? NP ADJP 0.4 0.0
?
20 FRAG? CC NP . 0.7 0.07
??
Table 6: Top 10 features ranked by GainRatio
using the auto-cleaned transcripts. Bold feature
numbers indicate rules which did not appear in Ta-
ble 2. (
?
p < 0.05,
??
p < 0.005,
???
p < 0.0005).
5.4 Analysis of variance
With a multi-way ANOVA we found significant
main effects of classifier (F(2,63) = 11.6, p <
0.001) and data set (F(2,63) = 11.2, p < 0.001)
on accuracy. A Tukey post-hoc test revealed sig-
nificant differences between SVM and RF (p <
0.001) and NB and RF (p < 0.001) but not be-
tween SVM and NB. As well, we see a sig-
nificant difference between the clean and auto-
cleaned data (p < 0.001) and the raw and auto-
cleaned data (p < 0.001) but not between the raw
and clean data. There was no significant main ef-
fect of weighting scheme or feature type (binary or
frequency) on accuracy. We did not examine any
possible interactions between these variables.
6 Discussion
6.1 Transcripts
We achieved the highest classification accuracies
using the auto-cleaned transcripts. The raw tran-
scripts, while containing more information about
dysfluent events, also seemed to cause more dif-
140
NB SVM RF
Narrative length .51 .62 .64
Binary, no weights .92 .95 .90
Binary, tf-idf .92 .95 .87
Binary, InfoGain .97 .90 .85
Binary, GainRatio .97 .90 .95
Frequency, no weights .90 .95 .77
Frequency, tf-idf .87 .95 .79
Frequency, InfoGain .92 .85 .82
Frequncy, GainRatio .92 .87 .95
SC metrics, no weights .79 .77 .74
SC metrics, InfoGain .79 .74 .72
SC metrics, GainRatio .79 .74 .67
Table 7: Average classification accuracy using
auto-cleaned transcripts. The highest classifica-
tion accuracy for each feature set is indicated with
boldface.
Clean Raw Auto
PP rate
??? ??? ???
PP proportion
??? ??? ??
PP length
??
VP rate
?? ?
VP proportion
??? ? ?
VP length
??? ?
Table 8: Significance of the phrase-level features
in each of the three data sets (
?
p < 0.05,
??
p <
0.005,
???
p < 0.0005).
ficulty for the parser, which mis-labelled filled
pauses and false starts in some cases. We also
found that the insertion of filled pauses resulted
in the creation of multiple features for a single un-
derlying grammatical structure. The auto-cleaned
transcripts appeared to avoid some of those prob-
lems, while still retaining information about many
of the non-narrative speech productions that were
removed from the clean transcripts.
Some of the features from the auto-cleaned tran-
scripts appear to be associated with the discourse
level of language, such as connectives and dis-
course markers. A researcher solely interested in
studying the syntax of language might resist the
inclusion of such features, and prefer to use only
features from the human-annotated clean tran-
scripts. However, we feel that such productions
are part of the grammar of spoken language, and
merit inclusion. From a practical standpoint, our
findings are reassuring: data preparation that can
be done automatically is much more feasible in
many situations than human annotation.
6.2 Features
CFG production rules can offer a more detailed
look at specific language impairments. We were
able to observe a number of important characteris-
tics of agrammatic language as reported in previ-
ous studies: fragmented speech with a higher in-
cidence of solitary noun phrases, difficulty with
determiners and possessives, reduced number of
prepositional phrases and embedded clauses, and
(in the raw transcripts), increased use of filled
pauses and repair phrases. For this reason, we be-
lieve that they are more useful for the analysis of
disordered or otherwise atypical language than tra-
ditional measures of syntactic complexity.
In some cases an in-depth analysis may not be
required, and in such cases it may be tempting to
simply use one of the more-general syntactic com-
plexity measures. Nevertheless, even in our simple
binary classification task, we found that using the
more-specific features gave us a higher accuracy.
6.3 Future work
Because of the limited data, we consider these re-
sults to be preliminary. We hope to replicate this
study as more data become available in the fu-
ture. We also plan to examine the effect, if any,
of the specific narrative task. Furthermore, we
have shown that these methods are effective for
the analysis of agrammatic aphasia, but there are
other types of aphasia in which semantic, rather
than syntactic, processing is the primary impair-
ment. We would like to extend this work to find
features which distinguish between different types
of aphasia.
Although we included manually transcribed
data in this study, these methods will be most use-
ful if they are also effective on automatically rec-
ognized speech. Previous work on speech recog-
nition for aphasic speech reported high error rates
(Fraser et al., 2013a). Our finding that the auto-
cleaned transcripts led to the highest classification
accuracy is encouraging, but we will have to test
the robustness to recognition errors and the depen-
dence on sentence boundary annotations.
Acknowledgments
This research was supported by the Natural Sciences and En-
gineering Research Council of Canada and National Institutes
of Health R01DC01948 and R01DC008552.
141
References
Harald Baayen, Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows:
Using syntactic annotation to enhance authorship
attribution. Literary and Linguistic Computing,
11(3):121?132.
Roelien Bastiaanse and Cynthia K. Thompson. 2012.
Perspectives on Agrammatism. Psychology Press.
Julian Brooke and Graeme Hirst. 2012. Robust, lex-
icalized native language identification. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 391?408.
Jieun Chae and Ani Nenkova. 2009. Predicting the
fluency of text with shallow structural features: case
studies of machine translation and human-written
text. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 139?147.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132?139.
Franca Debole and Fabrizio Sebastiani. 2004. Super-
vised term weighting for automated text categoriza-
tion. In Text mining and its applications, pages 81?
97. Springer.
Kathleen Fraser, Frank Rudzicz, Naida Graham, and
Elizabeth Rochon. 2013a. Automatic speech recog-
nition in the diagnosis of primary progressive apha-
sia. In Proceedings of the Fourth Workshop on
Speech and Language Processing for Assistive Tech-
nologies, pages 47?54.
Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham,
Carol Leonard, Graeme Hirst, Sandra E. Black, and
Elizabeth Rochon. 2013b. Automated classification
of primary progressive aphasia subtypes from narra-
tive speech transcripts. Cortex.
Harold Goodglass, Julie Ann Christiansen, and
Roberta E. Gallagher. 1994. Syntatic construc-
tions used by agrammatic speakers: Comparison
with conduction aphasics and normals. Neuropsy-
chology, 8(4):598.
Khairun-nisa Hassanali, Yang Liu, Aquiles Iglesias,
Thamar Solorio, and Christine Dollaghan. 2013.
Automatic generation of the index of productive
syntax for child language transcripts. Behavior re-
search methods, pages 1?9.
David I. Holmes and Sameer Singh. 1996. A stylo-
metric analysis of conversational speech of apha-
sic patients. Literary and Linguistic Computing,
11(3):133?140.
Brian MacWhinney, Davida Fromm, Margaret Forbes,
and Audrey Holland. 2011. Aphasiabank: Methods
for studying discourse. Aphasiology, 25(11):1286?
1307.
Ljiljana Progovac. 2006. The Syntax of Nonsen-
tentials: Multidisciplinary Perspectives, volume 93.
John Benjamins.
Emily T. Prud?hommeaux, Brian Roark, Lois M.
Black, and Jan van Santen. 2011. Classification of
atypical language in autism. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, CMCL ?11, pages 88?96.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffery Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Au-
dio, Speech, and Language Processing, 19(7):2081?
2090.
Elizabeth Rochon, Eleanor M. Saffran, Rita Sloan
Berndt, and Myrna F. Schwartz. 2000. Quantita-
tive analysis of aphasic sentence production: Further
development and new data. Brain and Language,
72(3):193?218.
Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.
Schwartz. 1989. The quantitative analysis of
agrammatic production: Procedure and data. Brain
and Language, 37(3):440?479.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic develop-
ment in child language. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 197?204.
Ben Swanson and Eugene Charniak. 2012. Native lan-
guage detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 193?
197.
Cynthia K. Thompson, Lewis P. Shapiro, Ligang Li,
and Lee Schendel. 1995. Analysis of verbs and
verb-argument structure: A method for quantifica-
tion of aphasic language production. Clinical Apha-
siology, 23:121?140.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification.
In Proceedings of the Australasian Language Tech-
nology Association Workshop, pages 67?75.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610.
Victor Yngve. 1960. A model and hypothesis for lan-
guage structure. Proceedings of the American Phys-
ical Society, 104:444?466.
142
