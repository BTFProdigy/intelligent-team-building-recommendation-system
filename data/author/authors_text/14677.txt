Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1128?1137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Lingual Latent Topic Extraction
Duo Zhang
University of Illinois at
Urbana-Champaign
dzhang22@cs.uiuc.edu
Qiaozhu Mei
University of Michigan
qmei@umich.edu
ChengXiang Zhai
University of Illinois at
Urbana-Champaign
czhai@cs.uiuc.edu
Abstract
Probabilistic latent topic models have re-
cently enjoyed much success in extracting
and analyzing latent topics in text in an un-
supervised way. One common deficiency
of existing topic models, though, is that
they would not work well for extracting
cross-lingual latent topics simply because
words in different languages generally do
not co-occur with each other. In this paper,
we propose a way to incorporate a bilin-
gual dictionary into a probabilistic topic
model so that we can apply topic models to
extract shared latent topics in text data of
different languages. Specifically, we pro-
pose a new topic model called Probabilis-
tic Cross-Lingual Latent Semantic Anal-
ysis (PCLSA) which extends the Proba-
bilistic Latent Semantic Analysis (PLSA)
model by regularizing its likelihood func-
tion with soft constraints defined based on
a bilingual dictionary. Both qualitative and
quantitative experimental results show that
the PCLSA model can effectively extract
cross-lingual latent topics from multilin-
gual text data.
1 Introduction
As a robust unsupervised way to perform shallow
latent semantic analysis of topics in text, prob-
abilistic topic models (Hofmann, 1999a; Blei et
al., 2003b) have recently attracted much atten-
tion. The common idea behind these models is the
following. A topic is represented by a multino-
mial word distribution so that words characteriz-
ing a topic generally have higher probabilities than
other words. We can then hypothesize the exis-
tence of multiple topics in text and define a gener-
ative model based on the hypothesized topics. By
fitting the model to text data, we can obtain an es-
timate of all the word distributions corresponding
to the latent topics as well as the topic distributions
in text. Intuitively, the learned word distributions
capture clusters of words that co-occur with each
other probabilistically.
Although many topic models have been pro-
posed and shown to be useful (see Section 2 for
more detailed discussion of related work), most
of them share a common deficiency: they are de-
signed to work only for mono-lingual text data and
would not work well for extracting cross-lingual
latent topics, i.e. topics shared in text data in
two different natural languages. The deficiency
comes from the fact that all these models rely on
co-occurrences of words forming a topical cluster,
but words in different language generally do not
co-occur with each other. Thus with the existing
models, we can only extract topics from text in
each language, but cannot extract common topics
shared in multiple languages.
In this paper, we propose a novel topic model,
called Probabilistic Cross-Lingual Latent Seman-
tic Analysis (PCLSA) model, which can be used to
mine shared latent topics from unaligned text data
in different languages. PCLSA extends the Proba-
bilistic Latent Semantic Analysis (PLSA) model
by regularizing its likelihood function with soft
constraints defined based on a bilingual dictio-
nary. The dictionary-based constraints are key to
bridge the gap of different languages and would
force the captured co-occurrences of words in
each language by PCLSA to be ?synchronized?
so that related words in the two languages would
have similar probabilities. PCLSA can be esti-
mated efficiently using the General Expectation-
Maximization (GEM) algorithm. As a topic ex-
traction algorithm, PCLSA would take a pair of
unaligned document sets in different languages
and a bilingual dictionary as input, and output a
set of aligned word distributions in both languages
that can characterize the shared topics in the two
languages. In addition, it also outputs a topic cov-
1128
erage distribution for each language to indicate the
relative coverage of different shared topics in each
language.
To the best of our knowledge, no previous work
has attempted to solve this topic extraction prob-
lem and generate the same output. The closest
existing work to ours is the MuTo model pro-
posed in (Boyd-Graber and Blei, 2009) and the
JointLDA model published recently in (Jagarala-
mudi and Daume? III, 2010). Both used a bilingual
dictionary to bridge the language gap in a topic
model. However, the goals of their work are dif-
ferent from ours in that their models mainly focus
on mining cross-lingual topics of matching word
pairs and discovering the correspondence at the
vocabulary level. Therefore, the topics extracted
using their model cannot indicate how a common
topic is covered differently in the two languages,
because the words in each word pair share the
same probability in a common topic. Our work fo-
cuses on discovering correspondence at the topic
level. In our model, since we only add a soft con-
straint on word pairs in the dictionary, their prob-
abilities in common topics are generally different,
naturally capturing which shows the different vari-
ations of a common topic in different languages.
We use a cross-lingual news data set and a re-
view data set to evaluate PCLSA. We also propose
a ?cross-collection? likelihood measure to quanti-
tatively evaluate the quality of mined topics. Ex-
perimental results show that the PCLSA model
can effectively extract cross-lingual latent topics
from multilingual text data, and it outperforms a
baseline approach using the standard PLSA on text
data in each language.
2 Related Work
Many topic models have been proposed, and the
two basic models are the Probabilistic Latent Se-
mantic Analysis (PLSA) model (Hofmann, 1999a)
and the Latent Dirichlet Allocation (LDA) model
(Blei et al, 2003b). They and their extensions
have been successfully applied to many prob-
lems, including hierarchical topic extraction (Hof-
mann, 1999b; Blei et al, 2003a; Li and McCal-
lum, 2006), author-topic modeling (Steyvers et al,
2004), contextual topic analysis (Mei and Zhai,
2006), dynamic and correlated topic models (Blei
and Lafferty, 2005; Blei and Lafferty, 2006), and
opinion analysis (Mei et al, 2007; Branavan et al,
2008). Our work is an extension of PLSA by in-
corporating the knowledge of a bilingual dictio-
nary as soft constraints. Such an extension is sim-
ilar to the extension of PLSA for incorporating so-
cial network analysis (Mei et al, 2008a) but our
constraint is different.
Some previous work on multilingual topic mod-
els assume documents in multiple languages are
aligned either at the document level, sentence level
or by time stamps (Mimno et al, 2009; Zhao and
Xing, 2006; Kim and Khudanpur, 2004; Ni et al,
2009; Wang et al, 2007). However, in many ap-
plications, we need to mine topics from unaligned
text corpus. For example, mining topics from
search results in different languages can facilitate
summarization of multilingual search results.
Besides all the multilingual topic modeling
work discussed above, comparable corpora have
also been studied extensively (e.g. (Fung, 1995;
Franz et al, 1998; Masuichi et al, 2000; Sadat
et al, 2003; Gliozzo and Strapparava, 2006)), but
most previous work aims at acquiring word trans-
lation knowledge or cross-lingual text categoriza-
tion from comparable corpora. Our work differs
from this line of previous work in that our goal is
to discover shared latent topics from multi-lingual
text data that are weakly comparable (e.g. the data
does not have to be aligned by time).
3 Problem Formulation
In general, the problem of cross-lingual topic ex-
traction can be defined as to extract a set of com-
mon cross-lingual latent topics covered in text col-
lections in different natural languages. A cross-
lingual latent topic will be represented as a multi-
nomial word distribution over the words in all
the languages, i.e. a multilingual word distri-
bution. For example, given two collections of
news articles in English and Chinese, respectively,
we would like to extract common topics simul-
taneously from the two collections. A discov-
ered common topic, such as the terrorist attack
on September 11, 2001, would be characterized
by a word distribution that would assign relatively
high probabilities to words related to this event in
both English and Chinese (e.g. ?terror?, ?attack?,
?afghanistan?, ?taliban?, and their translations in
Chinese).
As a computational problem, our input is a
multi-lingual text corpus, and output is a set of
cross-lingual latent topics. We now define this
problem more formally.
1129
Definition 1 (Multi-Lingual Corpus) A multi-
lingual corpus C is a set of text collections
{C1, C2, . . . , Cs}, where Ci = {di1, di2, . . . , diMi}
is a collection of documents in language Li with
vocabulary Vi = {wi1, wi2, . . . , wiNi}. Here, Mi is
the total number of documents in Ci, Ni is the to-
tal number of words in Vi, and dij is a document in
collection Ci.
Following the common assumption of bag-of-
words representation, we represent document dij
with a bag of words {wij1 , w
i
j2 , . . . , w
i
jd}, and use
c(wik, dij) to denote the count of word wik in docu-
ment dij .
Definition 2 (Cross-Lingual Topic): A cross-
lingual topic ? is a semantically coherent multi-
nomial distribution over all the words in the vo-
cabularies of languages L1, ..., Ls. That is, p(w|?)
would give the probability of a word w which can
be in any of the s languages under consideration. ?
is semantically coherent if it assigns high probabil-
ities to words that are semantically related either in
the same language or across different languages.
Clearly, we have
?s
i=1
?
w?Vi p(w|?) = 1 for any
cross-lingual topic ?.
Definition 3 (Cross-Lingual Topic Extrac-
tion) Given a multi-lingual corpus C, the task of
cross-lingual topic extraction is to model and ex-
tract k major cross-lingual topics {?1, ?2, . . . , ?k}
from C, where ?i is a cross-lingual topic, and k is
a user specified parameter.
The extracted cross-lingual topics can be di-
rectly used as a summary of the common con-
tent of the multi-lingual data set. Note that once
a cross-lingual topic is extracted, we can eas-
ily obtain its representation in each language Li
by ?splitting? the cross-lingual topic into multi-
ple word distributions in different languages. For-
mally, the word distribution of a cross-lingual
topic ? in language Li is given by pi(wi|?) =
p(wi|?)
?
w?Vi
p(w|?) .
These aligned language-specific word distribu-
tions can directly review the variations of topics
in different languages. They can also be used to
analyze the difference of the coverage of the same
topic in different languages. Moreover, they are
also useful for retrieving relevant articles or pas-
sages in each language and aligning them to the
same common topic, thus essentially also allow-
ing us to integrate and align articles in multiple
languages.
4 Probabilistic Cross-Lingual Latent
Semantic Analysis
In this section, we present our probabilistic cross-
lingual latent semantic analysis (PCLSA) model
and discuss how it can be used to extract cross-
lingual topics from multi-lingual text data.
The main reason why existing topic models
can?t be used for cross-lingual topic extraction is
because they cannot cross the language barrier.
Intuitively, in order to cross the language barrier
and extract a common topic shared in articles in
different languages, we must rely on some kind
of linguistic knowledge. Our PCLSA model as-
sumes the availability of bi-lingual dictionaries for
at least some language pairs, which are generally
available for major language pairs. Specifically,
for text data in languages L1, ..., Ls, if we rep-
resent each language as a node in a graph and
connect those language pairs for which we have a
bilingual dictionary, the minimum requirement is
that the whole graph is connected. Thus, as a min-
imum, we will need s? 1 distinct bilingual dictio-
naries. This is so that we can potentially cross all
the language barriers.
Our key idea is to ?synchronize? the extraction
of monolingual ?component topics? of a cross-
lingual topic from individual languages by forcing
a cross-lingual topic word distribution to assign
similar probabilities to words that are potential
translations according to a Li-Lj bilingual dictio-
nary. We achieve this by adding such preferences
formally to the likelihood function of a probabilis-
tic topic model as ?soft constraints? so that when
we estimate the model, we would try to not only
fit the text data well (which is necessary to extract
coherent component topics from each language),
but also satisfy our specified preferences (which
would ensure the extracted component topics in
different languages are semantically related). Be-
low we present how we implement this idea in
more detail.
A bilingual dictionary for languages Li and Lj
generally would give us a many-to-many map-
ping between the vocabularies of the two lan-
guages. With such a mapping, we can construct
a bipartite graph Gij = (Vij , Eij) between the
two languages where if one word can be poten-
tially translated into another word, the two words
would be connected with an edge. An edge can
be weighted based on the probability of the cor-
responding translation. An example graph for
1130
Chinese-English dictionary is shown in Figure 1.
Figure 1: A Dictionary based Word Graph
With multiple bilingual dictionaries, we can
merge the graphs to generate a multi-partite graph
G = (V,E). Based on this graph, the PCLSA
model extends the standard PLSA by adding a
constraint to the likelihood function to ?smooth?
the word distributions of topics in PLSA on the
multi-partite graph so that we would encourage the
words that are connected in the graph (i.e. pos-
sible translations of each other) to be given simi-
lar probabilities by every cross-lingual topic. Thus
when a cross-lingual topic picks up words that co-
occur in mono-lingual text, it would prefer pick-
ing up word pairs whose translations in other lan-
guages also co-occur with each other, giving us a
coherent multilingual word distribution that char-
acterizes well the content of text in different lan-
guages.
Specifically, let ? = {?j} (j = 1, ..., k) be a set
of k cross-lingual topic models to be discovered
from a multilingual text data set with s languages
such that p(w|?i) is the probability of word w ac-
cording to the topic model ?i.
If we are to use the regular PLSA to model our
data, we would have the following log-likelihood
and we usually use a maximum likelihood estima-
tor to estimate parameters and discover topics.
L(C) =
s
?
i=1
?
d?Ci
?
w
c(w, d) log
k
?
j=1
p(?j |d)p(w|?j)
Our main extension is to add to L(C) a cross-
lingual constraint term R(C) to incorporate the
knowledge of bilingual dictionaries. R(C) is de-
fined as
R(C) = 12
?
?u,v??E
w(u, v)
k
?
j=1
(p(wu|?j)Deg(u) ?
p(wv|?j)
Deg(v) )
2
where w(u, v) is the weight on the edge between
u and v in the multi-partite graph G = (V,E),
which in our experiments is set to 1, and Deg(u)
is the degree of word u, i.e. the sum of the weights
of all the edges ending with u.
Intuitively, R(C) measures the difference be-
tween p(wu|?j) and p(wv|?j) for each pair (u, v)
in a bilingual dictionary; the more they differ, the
larger R(C) would be. So it can be regarded as
a ?loss function? to help us assess how well the
?component word distributions? in multiple lan-
guages are correlated semantically. Clearly, we
would like the extracted topics to have a small
R(C). We choose this specific form of loss func-
tion because it would make it convenient to solve
the optimization problem of maximizing the cor-
responding regularized maximum likelihood (Mei
et al, 2008b). The normalization with Deg(u)
and Deg(v) can be regarded as a way to compen-
sate for the potential ambiguity of u and v in their
translations.
Putting L(C) and R(C) together, we would
like to maximize the following objective function
which is a regularized log-likelihood:
O(C, G) = (1 ? ?)L(C)? ?R(C) (1)
where ? ? (0, 1) is a parameter to balance the
likelihood and the regularizer. When ? = 0, we
recover the standard PLSA.
Specifically, we will search for a set of values
for all our parameters that can maximize the ob-
jective function defined above. Our parameters
include all the cross-lingual topics and the cov-
erage distributions of the topics in all documents,
which we denote by ? = {p(w|?j), p(?j |d)}d,w,j
where j = 1, ..., k, w varies over the entire vo-
cabularies of all the languages , d varies over
all the documents in our collection. This opti-
mization problem can be solved using a General-
ized Expectation-Maximization (GEM) algorithm
as described in (Mei et al, 2008a).
Specifically, in the E-step of the algorithm, the
distribution of hidden variables is computed using
Eq. 2.
z(w, d, j) = p(?j |d)p(w|?j)?
j? p(?j? |d)p(w|?j?)
(2)
Then in the M-step, we need to maximize the
complete data likelihood Q(?;?n):
Q(?;?n) = (1? ?)L?(C)? ?R(C)
1131
where
L?(C) =
?
d
?
w
c(w, d)
?
j
z(w, d, j) log p(?j |d)p(w|?j), (3)
with the constraints that
?
j p(?j |d) = 1 and
?
w p(w|?j) = 1.
There is a closed form solution if we only want
to maximize the L?(C) part:
p(n+1)(?j |d) =
?
w c(w, d)z(w, d, j)
?
w
?
j? c(w, d)z(w, d, j?)
p(n+1)(w|?j) =
?
d c(w, d)z(w, d, j)
?
d
??
w c(w?, d)z(w?, d, j)
(4)
However, there is no closed form solution in the
M-step for the whole objective function. Fortu-
nately, according to GEM we do not need to find
the local maximum of Q(?;?n) in every M-step,
and we only need to find a new value ?n+1 to im-
prove the complete data likelihood, i.e. to make
sure Q(?n+1; ?n) ? Q(?n; ?n). So our method
is to first maximize the L?(C) part using Eq. 4 and
then use Eq. 5 to gradually increase the R(C) part.
p(t+1)(wu|?j) = (1? ?)p(t)(wu|?j) (5)
+ ?
?
?u,v??E
w(u, v)
Deg(v)
p(t)(wv|?j)
Here, parameter ? is the length of each smooth-
ing step. Obviously, after each smoothing step,
the sum of the probabilities of all the words in one
topic is still equal to 1. We smooth the parameters
until we cannot get a better parameter set ?n+1.
Then, we continue to the next E-step. If there is
no ?n+1 s.t. Q(?n+1; ?n) ? Q(?n; ?n), then
we consider ?n to be the local maximum point of
the objective function Eq. 1.
5 Experiment Design
5.1 Data Set
The data set we used in our experiment is collected
from news articles of Xinhua English and Chi-
nese newswires. The whole data set is quite big,
containing around 40,000 articles in Chinese and
35,000 articles in English. For different purpose of
our experiments, we randomly selected different
number of documents from the whole corpus, and
we will describe the concrete statistics in each ex-
periment. To process the Chinese corpus, we use
a simple segmenter1 to split the data into Chinese
phrases. Both Chinese and English stopwords are
removed from our data.
The dictionary file we used for our PCLSA
model is from mandarintools.com2. For each Chi-
nese phrase, if it has several English meanings, we
add an edge between it and each of its English
translation. If one English translation is an En-
glish phrase, we add an edge between the Chinese
phrase and each English word in the phrase.
5.2 Baseline Method
As a baseline method, we can apply the standard
PLSA (Hofmann, 1999a) directly to the multi-
lingual corpus. Since PLSA takes advantage of
the word co-occurrences in the document level to
find semantic topics, directly using it for a multi-
lingual corpus will result in finding topics mainly
reflecting a single language (because words in dif-
ferent languages would not co-occur in the same
document in general). That is, the discovered top-
ics are mostly monolingual. These monolingual
topics can then be aligned based on a bilingual dic-
tionary to suggest a possible cross-lingual topic.
6 Experimental Results
6.1 Qualitative Comparison
To qualitatively compare PCLSAwith the baseline
method, we compare the word distributions of top-
ics extracted by them. The data set we used in this
experiment is selected from the Xinhua News data
during the period from Jun. 8th, 2001 to Jun. 15th,
2001. There are totally 1799 English articles and
1485 Chinese articles in the data set. The num-
ber of topics to be extracted is set to 10 for both
methods.
Table 1 shows the experimental results. To
make it easier to understand, we add an English
translation to each Chinese phrase in our results.
The first ten rows show sample topics of the mod-
eling results of traditional PLSA model. We can
see that it only contains mono-language topics,
i.e. the topics are either in Chinese or in En-
glish. The next ten rows are the results from
our PCLSA model. Compared with the base-
line method, PCLSA can not only find coherent
topics from the cross-lingual corpus, but it can
also show the content about one topic from both
two language corpora. For example, in ?Topic 2?
1http://www.mandarintools.com/segmenter.html
2http://www.mandarintools.com/cedict.html
1132
Table 2: Synthetic Data Set from Xinhua News
English Shrine Olympic Championship
90 101 70
Chinese CPC Anniversary Afghan War Championship
95 206 72
which is about ?Israel? and ?Palestinian?, the Chi-
nese corpus mentions a lot about ?Arafat? who is
the leader of ?Palestinian?, while the English cor-
pus discusses more on topics such as ?cease fire?
and ?women?. Similarly, in ?Topic 9?, the topic
is related to Philippine, the Chinese corpus men-
tions some environmental situation in Philippine,
while the English corpus mentions a lot about
?Abu Sayyaf?.
6.2 Discovering Common Topics
To demonstrate the ability of PCLSA for finding
common topics in cross-lingual corpus, we use
some event names, e.g. ?Shrine? and ?Olympic?,
as queries and randomly select a certain number of
documents from the whole corpus, which are re-
lated to the queries. The number of documents for
each query in the synthetic data set is shown in Ta-
ble 2. In either the English corpus or the Chinese
corpus, we select a smaller number of documents
about topic ?Championship? combined with the
other two topics in the same corpus. In this way,
when we want to extract two topics from either En-
glish or Chinese corpus, the ?Championship? topic
may not be easy to extract, because the other two
topics have more documents in the corpus. How-
ever, when we use PCLSA to extract four topics
from the two corpora together, we expect that the
topic ?Championship? will be found, because now
the sum of English and Chinese documents related
to ?Championship? is larger than other topics. The
experimental result is shown in Table 3. The first
two columns are the two topics extracted from En-
gish corpus, the third and the forth columns are
two topics from Chinese corpus, and the other four
columns are the results from cross-lingual cor-
pus. We can see that in either the Chinese sub-
collection or the English sub-collection, the topic
?Championship? is not extracted as a significant
topic. But, as expected, the topic ?Championship?
is extracted from the cross-lingual corpus, while
the topic ?Olympic? and topic ?Shrine? are merged
together. This demonstrate that PCLSA is capable
of extracting common topics from a cross-lingual
corpus.
6.3 Quantitative Evaluation
We also quantitatively evaluate how well our
PCLSA model can discover common topics
among corpus in different languages. We pro-
pose a ?cross-collection? likelihood measure for
this purpose. The basic idea is: suppose we got
k cross-lingual topics from the whole corpus, then
for each topic, we split the topic into two sepa-
rate set of topics, English topics and Chinese top-
ics, using the splitting formula described before,
i.e. pi(wi|?) = p(w
i|?)
?
w?Vi
p(w|?) . Then, we use the
word distribution of the Chinese topics (translating
the words into English) to fit the English Corpus
and use the word distribution of the English top-
ics (translating the words into Chinese) to fit the
Chinese Corpus. If the topics mined are common
topics in the whole corpus, then such a ?cross-
collection? likelihood should be larger than those
topics which are not commonly shared by the En-
glish and the Chinese corpus. To calculate the
likelihood of fitness, we use the folding-in method
proposed in (Hofmann, 2001). To translate topics
from one language to another, e.g. Chinese to En-
glish, we look up the bilingual dictionary and do
word-to-word translation. If one Chinese word has
several English translations, we simply distribute
its probability mass equally to each English trans-
lation.
For comparison, we use the standard PLSA
model as the baseline. Basically, suppose PLSA
mined k semantic topics in the Chinese corpus and
k semantic topics in the English corpus. Then, we
also use the ?cross-collection? likelihood measure
to see how well those k semantic Chinese topics fit
the English corpus and those k semantic English
topics fit the Chinese corpus.
We totally collect three data sets to compare the
performance. For the first data set, (English 1,
Chinese 1), both the Chinese and English corpus
are chosen from the Xinhua News Data during
the period from 2001.06.08 to 2001.06.15, which
has 1799 English articles and 1485 Chinese ar-
ticles. For the second data set, (English 2, Chi-
nese 2), the Chinese corpus Chinese 2 is the same
as Chinese 1, but the English corpus is chosen
from 2001.06.14 to 2001.06.19 which has 1547
documents. For the third data set, (English 3, Chi-
nese 3), the Chinese corpus is the same as in data
set one, but the English corpus is chosen from
2001.10.02 to 2001.10.07 which contains 1530
documents. In other words, in the first data set,
1133
Table 1: Qualitative Evaluation
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9
j(party) +"(crime) ?C(athlete) ?(palestine) \*(collaboration) s?(education) israel bt dollar china
??j(communist) @(agriculture) 	(champion) ????(palestine) ?0(shanghai) E(ball) palestinian beat percent cooperate
??(revolution) @?(travel) ?)?(championship) 1??(israel) ?(relation) ??(league) eu final million shanghai
j?(party member) Qs(heathendom) ?(base) *?(cease fire) ?)(bilateral) E(soccer) police championship index develop
??(central) ??(public security) ??E(badminton) ?\)(UN) ?4(trade) I?(minute) report play stock beije
?B(ism) w(name) ?(sports) ??(mid east) :(president) ??(team member) secure champion point particulate
?\(cadre) ?(case) ??(final) ??(lebanon) )(country) s(teacher) kill win share matter
??(chairman mao) ?(law enforcement) E(women) j??(macedon) ?P(friendly) ?B?(school) europe olympic close sco
??(chinese communist) =(city) 6?(chess) ?B(conflict) ??(meet) E?(team) egypt game 0 invest
s(leader) ?(penalize) H?(fitness) ??(talk) [?(russia)  (grade A) treaty cup billion project
?)(bilateral) ??(league) israel cooperate ?C(athlete) party eu invest 0 ??(absorb)
\*(collaboration) w(name) 1??(israel) sco particulate j(party) khatami =?(investment) dollar ?
??(talk) E(ball) bt develop 	 communist ireland 7?(billion) percent ?Y?e(abu)
?P(friendly) ??(shenhua) palestinian country athlete revolution ?}(ireland) s?(education) index ?
?(palestine) ??(host) ceasefire president champion ?B(-ism) elect ??(environ. protect.) million (?(particle)country A ?n(arafat) apec ii ?(antiwar) vote ??(money) stock philippine
?\)(UN) ball women shanghai 6?(chess) 3?(comrade) presidential ?B?(school) billion abu
s|(leader) ?y(jinde) jerusalem africa competition ??(revolution) cpc market point ?(base)bilateral ?(season) mideast meet contestant j?(party) iran s(teacher) 7(billion) ?state E?(player) lebanon T?(zemin jiang) v(gymnastics) ideology referendum business share ?(object)
Table 3: Effectiveness of Extracting Common Topics
English 1 English 2 Chinese 1 Chinese 2 Cross 1 Cross 2 Cross 3 Cross 4
japan olympic ??j(CPC) ??F(afghan) koizumi ??(taliban) swim ?|(worker)
shrine ioc ?(championship) ?(taliban) yasukuni /(military) ?(championship) party
visit beije -(world) ??(taliban) ioc city ?y(free style) ??(three)
koizumi game ?.(thought) /(military) japan refugee !y(diving) j.?(marx)
yasukuni july ?X(theory) K?(attack) olympic side ?)?(championship) communist
war bid j.?(marx) ?(US army) beije ?(US army) ???(semi final) marx
august swim ?y(swim) [(laden) shrine q(bomb) competition theory
asia vote ?)?(championship) \?(army) visit 	Y(kabul) ?y(swim) Oj(found party)
criminal championship j(party) q(bomb) ???(olympic) 8?(attack) ?9(record) ??j(CPC)
ii committee Oj(found party) 	Y(kabul) ???.(olympic) 
?(refugee) [??(xuejuan luo) revolution
the English corpus and Chinese corpus are com-
parable with each other, because they cover simi-
lar events during the same period. In the second
data set, the English and Chinese corpora share
some common topics during the overlap period.
The third data is the most tough one since the two
corpora are from different periods. The purpose of
using these three different data sets for evaluation
is to test how well PCLSA can mine common top-
ics from either a data set where the English corpus
and the Chinese corpus are comparable or a data
set where the English corpus and the Chinese cor-
pus rarely share common topics.
The experimental results are shown in Table 4.
Each row shows the ?cross-collection? likelihood
of using the ?cross-collection? topics to fit the data
set named in the first column. For example, in
the first row, the values are the ?cross-collection?
likelihood of using Chinese topics found by differ-
ent methods from the first data set to fit English 1.
The last collum shows howmuch improvement we
got from PCLSA compared with PLSA. From the
results, we can see that in all the data sets, our
PCLSA has higher ?cross-collection? likelihood
value, which means it can find better common top-
ics compared to the baseline method. Notice that
the Chinese corpora are the same in all three data
sets. The results show that both PCLSA and PLSA
get lower ?cross-collection? likelihood for fitting
the Chinese corpora when the data set becomes
?tougher?, i.e. less topic overlapping, but the im-
Table 4: Quantitative Evaluation of Common
Topic Finding (?cross-collection? log-likelihood)
PCLSA PLSA Rel. Imprv.
English 1 -2.86294E+06 -3.03176E+06 5.6%
Chinese 1 -4.69989E+06 -4.85369E+06 3.2%
English 2 -2.48174E+06 -2.60805E+06 4.8%
Chinese 2 -4.73218E+06 -4.88906E+06 3.2%
English 3 -2.44714E+06 -2.60540E+06 6.1%
Chinese 3 -4.79639E+06 -4.94273E+06 3.0%
provement of PCLSA over PLSA does not drop
much. On the other hand, the improvement of
PCLSA over PLSA on the three English corpora
does not show any correlation with the difficulty
of the data set.
6.4 Extracting from Multi-Language Corpus
In the previous experiments, we have shown the
capability and effectiveness of the PCLSA model
in latent topic extraction from two language cor-
pora. In fact, the proposed model is general and
capable of extracting latent topics from multi-
language corpus. For example, if we have dic-
tionaries among multiple languages, we can con-
struct a multi-partite graph based on the corre-
spondence between those vocabularies, and then
smooth the PCLSA model with this graph.
To show the effectiveness of PCLSA in min-
ing multiple language corpus, we first construct a
simulated data set based on 1115 reviews of three
brands of laptops, namely IBM (303), Apple(468)
and DELL(344). To simulate a three language cor-
1134
Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
cd(apple) battery(dell) mouse(dell) print(apple) port(ibm) laptop(ibm) os(apple) port(dell)port(apple) drive(dell) button(dell) resolution(dell) card(ibm) t20(ibm) run(apple) 2(dell)drive(apple) 8200(dell) touchpad(dell) burn(apple) modem(ibm) thinkpad(ibm) 1(apple) usb(dell)airport(apple) inspiron(dell) pad(dell) normal(dell) display(ibm) battery(ibm) ram(apple) 1(dell)firewire(apple) system(dell) keyboard(dell) image(dell) built(ibm) notebook(ibm) mac(apple) 0(dell)dvd(apple) hour(dell) point(dell) digital(apple) swap(ibm) ibm(ibm) battery(apple) slot(dell)usb(apple) sound(dell) stick(dell) organize(apple) easy(ibm) 3(ibm) hour(apple) firewire(dell)rw(apple) dell(dell) rest(dell) cds(apple) connector(ibm) feel(ibm) 12(apple) display(dell)card(apple) service(dell) touch(dell) latch(apple) feature(ibm) hour(ibm) operate(apple) standard(dell)mouse(apple) life(dell) erase(dell) advertise(dell) cd(ibm) high(ibm) word(apple) fast(dell)
osx(apple) applework(apple) port(dell) battery(dell) lightest(ibm) uxga(dell) light(ibm) battery(apple)memory(dell) file(apple) port(apple) battery(ibm) quality(dell) ultrasharp(dell) ultrabay(ibm) point(dell)special(dell) bounce(apple) port(ibm) battery(apple) year(ibm) display(dell) connector(ibm) touchpad(dell)crucial(dell) quit(apple) firewire(apple) geforce4(dell) hassle(ibm) organize(apple) dvd(ibm) button(dell)memory(apple) word(apple) imac(apple) 100mhz(apple) bania(dell) learn(apple) nice(ibm) hour(apple)memory(ibm) file(ibm) firewire(dell) 440(dell) 800mhz(apple) logo(apple) modem(ibm) battery(ibm)netscape(apple) file(dell) firewire(ibm) bus(apple) trackpad(apple) postscript(apple) connector(dell) battery(dell)reseller(apple) microsoft(apple) jack(apple) 8200(dell) cover(ibm) ll(apple) light(apple) fan(dell)10(dell) ms(apple) playback(dell) 8100(dell) workmanship(dell) sxga(dell) light(dell) erase(dell)special(apple) excel(apple) jack(dell) chipset(dell) section(apple) warm(apple) floppy(ibm) point(apple)
2000(ibm) ram(apple) port(dell) itune(apple) uxga(dell) port(apple) pentium(dell) drive(ibm)window(ibm) ram(ibm) port(apple) applework(apple) screen(dell) port(ibm) processor(dell) drive(dell)2000(apple) ram(dell) port(ibm) imovie(apple) screen(ibm) port(dell) p4(dell) drive(apple)2000(dell) screen(apple) 2(dell) import(apple) screen(apple) usb(apple) power(dell) hard(ibm)window(apple) 1(apple) 2(apple) battery(apple) ultrasharp(dell) plug(apple) pentium(apple) osx(apple)window(dell) screen(ibm) 2(ibm) iphoto(apple) 1600x1200(dell) cord(apple) pentium(ibm) hard(dell)portege(ibm) screen(dell) speak(dell) battery(ibm) display(dell) usb(ibm) keyboard(dell) hard(apple)option(ibm) 1(ibm) toshiba(dell) battery(dell) display(apple) usb(dell) processor(ibm) card(ibm)hassle(ibm) 1(dell) speak(ibm) hour(apple) display(ibm) firewire(apple) processor(apple) dvd(ibm)device(ibm) maco(apple) toshiba(ibm) hour(ibm) view(dell) plug(ibm) power(apple) card(dell)
pus, we use an ?IBM? word, an ?Apple? word, and
a ?Dell? word to replace an English word in their
corpus. For example, we use ?IBM10?, ?Apple10?,
?Dell10? to replace the word ?CD? whenever it ap-
pears in an IBM?s, Apple?s, or Dell?s review. Af-
ter the replacement, the reviews about IBM, Ap-
ple, and Dell will not share vocabularies with each
other. On the other hand, for any three created
words which represent the same English word, we
add three edges among them, and therefore we
get a simulated dictionary graph for our PCLSA
model.
The experimental result is shown in Table 5, in
which we try to extract 8 topics from the cross-
lingual corpus. The first ten rows show the re-
sult of our PCLSA model, in which we set a very
small value to the weight parameter ? for the reg-
ularizer part. This can be used as an approxima-
tion of the result from the traditional PLSA model
on this three language corpus. We can see that
the extracted topics are mainly written in mono-
language. As we set the value of parameter ?
larger, the extracted topics become multi-lingual,
which is shown in the next ten rows. From this
result, we can see the difference between the re-
views of different brands about the similar topic.
In addition, if we set the ? even larger, we will
get topics that are mostly made of the same words
from the three different brands, which means the
extracted topics are very smooth on the dictionary
graph now.
7 Conclusion
In this paper, we study the problem of cross-
lingual latent topic extraction where the task is to
extract a set of common latent topics from multi-
lingual text data. We propose a novel probabilistic
topic model (i.e. the Probabilistic Cross-Lingual
Latent Semantic Analysis (PCLSA) model) that
can incorporate translation knowledge in bilingual
dictionaries as a regularizer to constrain the pa-
rameter estimation so that the learned topic models
would be synchronized in multiple languages. We
evaluated the model using several data sets. The
experimental results show that PCLSA is effec-
tive in extracting common latent topics from mul-
tilingual text data, and it outperforms the baseline
method which uses the standard PLSA to fit each
monolingual text data set.
Our work opens up some interesting future re-
search directions to further explore. First, in
this paper, we have only experimented with uni-
form weighting of edge in the bilingual graph.
It should be very interesting to explore how to
assign weights to the edges and study whether
weighted graphs can further improve performance.
Second, it would also be interesting to further
extend PCLSA to accommodate discovering top-
ics in each language that aren?t well-aligned with
other languages.
8 Acknowledgments
We sincerely thank the anonymous reviewers for
their comprehensive and constructive comments.
The work was supported in part by NASA grant
1135
NNX08AC35A, by the National Science Foun-
dation under Grant Numbers IIS-0713581, IIS-
0713571, and CNS-0834709, and by a Sloan Re-
search Fellowship.
References
David Blei and John Lafferty. 2005. Correlated topic
models. In NIPS ?05: Advances in Neural Informa-
tion Processing Systems 18.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 113?
120.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
chinese restaurant process. In Neural Information
Processing Systems (NIPS) 16.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent Dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
J. Boyd-Graber and D. Blei. 2009. Multilingual topic
models for unaligned text. In Uncertainty in Artifi-
cial Intelligence.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of ACL 2008.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 553?560, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
T. Hofmann. 1999a. Probabilistic latent semantic anal-
ysis. In Proceedings of UAI 1999, pages 289?296.
Thomas Hofmann. 1999b. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In IJCAI? 99, pages 682?687.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Mach. Learn.,
42(1-2):177?196.
Jagadeesh Jagaralamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proceedings of the European Conference on In-
formation Retrieval (ECIR), Milton Keynes, United
Kingdom.
Woosung Kim and Sanjeev Khudanpur. 2004. Lex-
ical triggers and latent semantic analysis for cross-
lingual language model adaptation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 3(2):94?112.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In ICML ?06: Proceedings of the 23rd in-
ternational conference on Machine learning, pages
577?584.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs. In Proc. 18th COLINC, pages 1066?
1070.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In Proceedings of
KDD ?06, pages 649?655.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of WWW ?07.
Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008a. Topic modeling with network regular-
ization. In WWW, pages 101?110.
Qiaozhu Mei, Duo Zhang, and ChengXiang Zhai.
2008b. A general optimization framework for
smoothing language models on graph structures. In
SIGIR ?08: Proceedings of the 31st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 611?618,
New York, NY, USA. ACM.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew Mccallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880?889, Singapore,
August. Association for Computational Linguistics.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from wikipedia.
In WWW ?09: Proceedings of the 18th international
conference on World wide web, pages 1155?1156,
New York, NY, USA. ACM.
F. Sadat, M. Yoshikawa, and S. Uemura. 2003. Bilin-
gual terminology acquisition from comparable cor-
pora and phrasal translation to cross-language infor-
mation retrieval. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 141?144.
1136
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,
and Thomas Griffiths. 2004. Probabilistic author-
topic models for information discovery. In Proceed-
ings of KDD?04, pages 306?315.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and
Richard Sproat. 2007. Mining correlated bursty
topic patterns from coordinated text streams. In
KDD ?07: Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 784?793, New York, NY,
USA. ACM.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics.
1137
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1526?1535,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Structural Topic Model for Latent Topical Structure Analysis
Hongning Wang, Duo Zhang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana IL, 61801 USA
{wang296, dzhang22, czhai}@cs.uiuc.edu
Abstract
Topic models have been successfully applied
to many document analysis tasks to discover
topics embedded in text. However, existing
topic models generally cannot capture the la-
tent topical structures in documents. Since
languages are intrinsically cohesive and coher-
ent, modeling and discovering latent topical
transition structures within documents would
be beneficial for many text analysis tasks.
In this work, we propose a new topic model,
Structural Topic Model, which simultaneously
discovers topics and reveals the latent topi-
cal structures in text through explicitly model-
ing topical transitions with a latent first-order
Markov chain. Experiment results show that
the proposed Structural Topic Model can ef-
fectively discover topical structures in text,
and the identified structures significantly im-
prove the performance of tasks such as sen-
tence annotation and sentence ordering.
1 Introduction
A great amount of effort has recently been made in
applying statistical topic models (Hofmann, 1999;
Blei et al, 2003) to explore word co-occurrence pat-
terns, i.e. topics, embedded in documents. Topic
models have become important building blocks of
many interesting applications (see e.g., (Blei and
Jordan, 2003; Blei and Lafferty, 2007; Mei et al,
2007; Lu and Zhai, 2008)).
In general, topic models can discover word clus-
tering patterns in documents and project each doc-
ument to a latent topic space formed by such word
clusters. However, the topical structure in a docu-
ment, i.e., the internal dependency between the top-
ics, is generally not captured due to the exchange-
ability assumption (Blei et al, 2003), i.e., the doc-
ument generation probabilities are invariant to con-
tent permutation. In reality, natural language text
rarely consists of isolated, unrelated sentences, but
rather collocated, structured and coherent groups of
sentences (Hovy, 1993). Ignoring such latent topi-
cal structures inside the documents means wasting
valuable clues about topics and thus would lead to
non-optimal topic modeling.
Taking apartment rental advertisements as an ex-
ample, when people write advertisements for their
apartments, it?s natural to first introduce ?size? and
?address? of the apartment, and then ?rent? and
?contact?. Few people would talk about ?restric-
tion? first. If this kind of topical structures are cap-
tured by a topic model, it would not only improve
the topic mining results, but, more importantly, also
help many other document analysis tasks, such as
sentence annotation and sentence ordering.
Nevertheless, very few existing topic models at-
tempted to model such structural dependency among
topics. The Aspect HMM model introduced in
(Blei and Moreno, 2001) combines pLSA (Hof-
mann, 1999) with HMM (Rabiner, 1989) to perform
document segmentation over text streams. However,
Aspect HMM separately estimates the topics in the
training set and depends on heuristics to infer the
transitional relations between topics. The Hidden
Topic Markov Model (HTMM) proposed by (Gru-
ber et al, 2007) extends the traditional topic models
by assuming words in each sentence share the same
topic assignment, and topics transit between adja-
cent sentences. However, the transitional structures
among topics, i.e., how likely one topic would fol-
low another topic, are not captured in this model.
1526
In this paper, we propose a new topic model,
named Structural Topic Model (strTM) to model and
analyze both latent topics and topical structures in
text documents. To do so, strTM assumes: 1) words
in a document are either drawn from a content topic
or a functional (i.e., background) topic; 2) words in
the same sentence share the same content topic; and
3) content topics in the adjacent sentences follow a
topic transition that satisfies the first order Markov
property. The first assumption distinguishes the se-
mantics of the occurrence of each word in the doc-
ument, the second requirement confines the unreal-
istic ?bag-of-word? assumption into a tighter unit,
and the third assumption exploits the connection be-
tween adjacent sentences.
To evaluate the usefulness of the identified top-
ical structures by strTM, we applied strTM to the
tasks of sentence annotation and sentence ordering,
where correctly modeling the document structure
is crucial. On the corpus of 8,031 apartment ad-
vertisements from craiglist (Grenager et al, 2005)
and 1,991 movie reviews from IMDB (Zhuang et
al., 2006), strTM achieved encouraging improve-
ment in both tasks compared with the baseline meth-
ods that don?t explicitly model the topical structure.
The results confirm the necessity of modeling the
latent topical structures inside documents, and also
demonstrate the advantages of the proposed strTM
over existing topic models.
2 Related Work
Topic models have been successfully applied to
many problems, e.g., sentiment analysis (Mei et
al., 2007), document summarization (Lu and Zhai,
2008) and image annotation (Blei and Jordan, 2003).
However, in most existing work, the dependency
among the topics is loosely governed by the prior
topic distribution, e.g., Dirichlet distribution.
Some work has attempted to capture the interre-
lationship among the latent topics. Correlated Topic
Model (Blei and Lafferty, 2007) replaces Dirichlet
prior with logistic Normal prior for topic distribu-
tion in each document in order to capture the cor-
relation between the topics. HMM-LDA (Griffiths
et al, 2005) distinguishes the short-range syntactic
dependencies from long-range semantic dependen-
cies among the words in each document. But in
HMM-LDA, only the latent variables for the syn-
tactic classes are treated as a locally dependent se-
quence, while latent topics are treated the same as in
other topic models. Chen et al introduced the gen-
eralized Mallows model to constrain the latent topic
assignments (Chen et al, 2009). In their model,
they assume there exists a canonical order among
the topics in the collection of related documents and
the same topics are forced not to appear in discon-
nected portions of the topic sequence in one docu-
ment (sampling without replacement). Our method
relaxes this assumption by only postulating transi-
tional dependency between topics in the adjacent
sentences (sampling with replacement) and thus po-
tentially allows a topic to appear multiple times in
disconnected segments. As discussed in the pre-
vious section, HTMM (Gruber et al, 2007) is the
most similar model to ours. HTMM models the
document structure by assuming words in the same
sentence share the same topic assignment and suc-
cessive sentences are more likely to share the same
topic. However, HTMM only loosely models the
transition between topics as a binary relation: the
same as the previous sentence?s assignment or draw
a new one with a certain probability. This simpli-
fied coarse modeling of dependency could not fully
capture the complex structure across different docu-
ments. In contrast, our strTM model explicitly cap-
tures the regular topic transitions by postulating the
first order Markov property over the topics.
Another line of related work is discourse analysis
in natural language processing: discourse segmen-
tation (Sun et al, 2007; Galley et al, 2003) splits a
document into a linear sequence of multi-paragraph
passages, where lexical cohesion is used to link to-
gether the textual units; discourse parsing (Soricut
and Marcu, 2003; Marcu, 1998) tries to uncover a
more sophisticated hierarchical coherence structure
from text to represent the entire discourse. One work
in this line that shares a similar goal as ours is the
content models (Barzilay and Lee, 2004), where an
HMM is defined over text spans to perform infor-
mation ordering and extractive summarization. A
deficiency of the content models is that the identi-
fication of clusters of text spans is done separately
from transition modeling. Our strTM addresses this
deficiency by defining a generative process to simul-
taneously capture the topics and the transitional re-
1527
lationship among topics: allowing topic modeling
and transition modeling to reinforce each other in a
principled framework.
3 Structural Topic Model
In this section, we formally define the Structural
Topic Model (strTM) and discuss how it captures the
latent topics and topical structures within the docu-
ments simultaneously. From the theory of linguistic
analysis (Kamp, 1981), we know that document ex-
hibits internal structures, where structural segments
encapsulate semantic units that are closely related.
In strTM, we treat a sentence as the basic structure
unit, and assume all the words in a sentence share the
same topical aspect. Besides, two adjacent segments
are assumed to be highly related (capturing cohesion
in text); specifically, in strTM we pose a strong tran-
sitional dependency assumption among the topics:
the choice of topic for each sentence directly de-
pends on the previous sentence?s topic assignment,
i.e., first order Markov property. Moveover, tak-
ing the insights from HMM-LDA that not all the
words are content conveying (some of them may
just be a result of syntactic requirement), we intro-
duce a dummy functional topic zB for every sen-
tence in the document. We use this functional topic
to capture the document-independent word distribu-
tion, i.e., corpus background (Zhai et al, 2004). As
a result, in strTM, every sentence is treated as a mix-
ture of content and functional topics.
Formally, we assume a corpus consists of D doc-
uments with a vocabulary of size V, and there are
k content topics embedded in the corpus. In a given
document d, there arem sentences and each sentence
i hasNi words. We assume the topic transition prob-
ability p(z|z?) is drawn from a Multinomial distribu-
tionMul(?z?), and the word emission probability un-
der each topic p(w|z) is drawn from a Multinomial
distribution Mul(?z).
To get a unified description of the generation
process, we add another dummy topic T-START in
strTM, which is the initial topic with position ?-1?
for every document but does not emit any words.
In addition, since our functional topic is assumed to
occur in all the sentences, we don?t need to model
its transition with other content topics. We use a
Binomial variable pi to control the proportion be-
tween content and functional topics in each sen-
tence. Therefore, there are k+1 topic transitions, one
for T-START and others for k content topics; and k
emission probabilities for the content topics, with an
additional one for the functional topic zB (in total
k+1 emission probability distributions).
Conditioned on the model parameters ? =
(?, ?, pi), the generative process of a document in
strTM can be described as follows:
1. For each sentence si in document d:
(a) Draw topic zi from Multinomial distribu-
tion conditioned on the previous sentence
si?1?s topic assignment zi?1:
zi ? Mul(?zi?1)
(b) Draw each word wij in sentence si from
the mixture of content topic zi and func-
tional topic zB:
wij ? pip(wij |?, zi)+(1?pi)p(wij |?, zB)
The joint probability of sentences and topics in
one document defined by strTM is thus given by:
p(S0, S1, . . . , Sm, z|?, ?, pi) =
m
?
i=1
p(zi|?, zi?1)p(Si|zi)
(1)
where the topic to sentence emission probability is
defined as:
p(Si|zi) =
Ni
?
j=0
[
pip(wij |?, zi) + (1? pi)p(wij |?, zB)
]
(2)
This process is graphically illustrated in Figure 1.
 
zmz0 ??..
wm??..
NmD
K+1
w0
N0
K+1
 
z1
w1
N1
Tstart
Figure 1: Graphical Representation of strTM.
From the definition of strTM, we can see that the
document structure is characterized by a document-
specific topic chain, and forcing the words in one
1528
sentence to share the same content topic ensures se-
mantic cohesion of the mined topics. Although we
do not directly model the topic mixture for each doc-
ument as the traditional topic models do, the word
co-occurrence patterns within the same document
are captured by topic propagation through the transi-
tions. This can be easily understood when we write
down the posterior probability of the topic assign-
ment for a particular sentence:
p(zi|S0, S1, . . . , Sm,?)
=p(S0, S1, . . . , Sm|zi,?)p(zi)
p(S0, S1, . . . , Sm)
? p(S0, S1, . . . , Si, zi)? p(Si+1, Si+2, . . . , Sm|zi)
=
?
zi?1
p(S0, . . . , Si?1, zi?1)p(zi|zi?1)p(Si|zi)
?
?
zi+1
p(Si+1, . . . , Sm|zi+1)p(zi+1|zi) (3)
The first part of Eq(3) describes the recursive in-
fluence on the choice of topic for the ith sentence
from its preceding sentences, while the second part
captures how the succeeding sentences affect the
current topic assignment. Intuitively, when we need
to decide a sentence?s topic, we will look ?back-
ward? and ?forward? over all the sentences in the
document to determine a ?suitable? one. In addition,
because of the first order Markov property, the local
topical dependency gets more emphasis, i.e., they
are interacting directly through the transition proba-
bilities p(zi|zi?1) and p(zi+1|zi). And such interac-
tion on sentences farther away would get damped by
the multiplication of such probabilities. This result
is reasonable, especially in a long document, since
neighboring sentences are more likely to cover sim-
ilar topics than two sentences far apart.
4 Posterior Inference and Parameter
Estimation
The chain structure in strTM enables us to perform
exact inference: posterior distribution can be ef-
ficiently calculated by the forward-backward algo-
rithm, the optimal topic sequence can be inferred
using the Viterbi algorithm, and parameter estima-
tion can be solved by the Expectation Maximization
(EM) algorithm. More technical details can be found
in (Rabiner, 1989). In this section, we only discuss
strTM-specific procedures.
In the E-Step of EM algorithm, we need to col-
lect the expected count of a sequential topic pair
(z, z?) and a topic-word pair (z, w) to update the
model parameters ? and ? in the M-Step. In strTM,
E[c(z, z?)] can be easily calculated by forward-
backward algorithm. But we have to go one step
further to fetch the required sufficient statistics for
E[c(z, w)], because our emission probabilities are
defined over sentences.
Through forward-backward algorithm, we can get
the posterior probability p(si, z|d,?). In strTM,
words in one sentence are independently drawn from
either a specific content topic z or functional topic
zB according to the mixture weight pi. Therefore,
we can accumulate the expected count of (z, w) over
all the sentences by:
E[c(z, w)] =
?
d,s?d
pip(w|z)p(s, z|d,?)c(w, s)
pip(w|z) + (1? pi)p(w|zB)
(4)
where c(w, s) indicates the frequency of word w in
sentence s.
Eq(4) can be easily explained as follows. Since
we already observe topic z and sentence s co-
occur with probability p(s, z|d,?), each word w
in s should share the same probability of be-
ing observed with content topic z. Thus the ex-
pected count of c(z, w) in this sentence would be
p(s, z|d,?)c(w, s). However, since each sentence
is also associated with the functional topic zB , the
word w may also be drawn from zB . By applying
the Bayes? rule, we can properly reallocate the ex-
pected count of c(z, w) by Eq(4). The same strategy
can be applied to obtain E[c(zB, w)].
As discussed in (Johnson, 2007), to avoid the
problem that EM algorithm tends to assign a uni-
form word/state distribution to each hidden state,
which deviates from the heavily skewed word/state
distributions empirically observed, we can apply a
Bayesian estimation approach for strTM. Thus we
introduce prior distributions over the topic transi-
tion Mul(?z?) and emission probabilities Mul(?z),
and use the Variational Bayesian (VB) (Jordan et al,
1999) estimator to obtain a model with more skewed
word/state distributions.
Since both the topic transition and emission prob-
abilities are Multinomial distributions in strTM,
the conjugate Dirichlet distribution is the natural
1529
choice for imposing a prior on them (Diaconis and
Ylvisaker, 1979). Thus, we further assume:
?z ? Dir(?) (5)
?z ? Dir(?) (6)
where we use exchangeable Dirichlet distributions
to control the sparsity of ?z and ?z . As ? and ? ap-
proach zero, the prior strongly favors the models in
which each hidden state emits as few words/states as
possible. In our experiments, we empirically tuned
? and ? on different training corpus to optimize log-
likelihood.
The resulting VB estimation only requires a mi-
nor modification to the M-Step in the original EM
algorithm:
??z =
?(E[c(z?, z)] + ?)
?(E[c(z)] + k?)
(7)
??z =
?(E[c(w, z)] + ?)
?(E[c(z)] + V ?)
(8)
where ?(x) is the exponential of the first derivative
of the log-gamma function.
The optimal setting of pi for the proportion of con-
tent topics in the documents is empirically tuned by
cross-validation over the training corpus to maxi-
mize the log-likelihood.
5 Experimental Results
In this section, we demonstrate the effectiveness
of strTM in identifying latent topical structures
from documents, and quantitatively evaluate how the
mined topic transitions can help the tasks of sen-
tence annotation and sentence ordering.
5.1 Data Set
We used two different data sets for evaluation: apart-
ment advertisements (Ads) from (Grenager et al,
2005) and movie reviews (Review) from (Zhuang et
al., 2006).
The Ads data consists of 8,767 advertisements for
apartment rentals crawled from Craigslist website.
302 of them have been labeled with 11 fields, in-
cluding size, feature, address, etc., on the sentence
level. The review data contains 2,000 movie reviews
discussing 11 different movies from IMDB. These
reviews are manually labeled with 12 movie feature
labels (We didn?t use the additional opinion anno-
tations in this data set.) , e.g., VP (vision effects),
MS (music and sound effects), etc., also on the sen-
tences, but the annotations in the review data set is
much sparser than that in the Ads data set (see in Ta-
ble 1). The sentence-level annotations make it pos-
sible to quantitatively evaluate the discovered topic
structures.
We performed simple preprocessing on these
two data sets: 1) removed a standard list of stop
words, terms occurring in less than 2 documents;
2) discarded the documents with less than 2 sen-
tences; 3) aggregated sentence-level annotations
into document-level labels (binary vector) for each
document. Table 1 gives a brief summary on these
two data sets after the processing.
Ads Review
Document Size 8,031 1,991
Vocabulary Size 21,993 14,507
Avg Stn/Doc 8.0 13.9
Avg Labeled Stn/Doc 7.1* 5.1
Avg Token/Stn 14.1 20.0
*Only in 302 labeled ads
Table 1: Summary of evaluation data set
5.2 Topic Transition Modeling
First, we qualitatively demonstrate the topical struc-
ture identified by strTM from Ads data1. We trained
strTM with 11 content topics in Ads data set, used
word distribution under each class (estimated by
maximum likelihood estimator on document-level
labels) as priors to initialize the emission probabil-
ity Mul(?z) in Eq(6), and treated document-level la-
bels as the prior for transition from T-START in each
document, so that the mined topics can be aligned
with the predefined class labels. Figure 2 shows the
identified topics and the transitions among them. To
get a clearer view, we discarded the transitions be-
low a threshold of 0.1 and removed all the isolated
nodes.
From Figure 2, we can find some interesting top-
ical structures. For example, people usually start
with ?size?, ?features? and ?address?, and end
with ?contact? information when they post an apart-
1Due to the page limit, we only show the result in Ads data
set.
1530
TELEPHONE
appointment
information
contact
email
parking
kitchen
room
laundry
storage
close
shopping
transportation
bart
location
http
photos
click
pictures
view
deposit
month
lease
rent
year
pets
kitchen
cat
negotiate
smoking
water
garbage
included
paid
utilities
NUM
bedroom
bath
room
large
Figure 2: Estimated topics and topical transitions in Ads data set
ment ads. Also, we can discover a strong transition
from ?size? to ?features?. This intuitively makes
sense because people usually write ?it?s a two bed-
rooms apartment? first, and then describe other ?fea-
tures? about the apartment. The mined topics are
also quite meaningful. For example, ?restrictions?
are usually put over pets and smoking, and parking
and laundry are always the major ?features? of an
apartment.
To further quantitatively evaluate the estimated
topic transitions, we used Kullback-Leibler (KL) di-
vergency between the estimated transition matrix
and the ?ground-truth? transition matrix as the met-
ric. Each element of the ?ground-truth? transition
matrix was calculated by Eq(9), where c(z, z?) de-
notes how many sentences annotated by z? immedi-
ately precede one annotated by z. ? is a smoothing
factor, and we fixed it to 0.01 in the experiment.
p?(z|z?) = c(z, z
?) + ?
c(z) + k?
(9)
The KL divergency between two transition matri-
ces is defined in Eq(10). Because we have a k ? k
transition matrix (Tstart is not included), we calcu-
lated the average KL divergency against the ground-
truth over all the topics:
avgKL=
?k
i=1 KL(p(z|z
?
i)||p?(z|z?i))+KL(p?(z|z?i)||p(z|z?i))
2k
(10)
where p?(z|z?) is the ground-truth transition proba-
bility estimated by Eq(9), and p(z|z?) is the transi-
tion probability given by the model.
We used pLSA (Hofmann, 1999), latent permuta-
tion model (lPerm) (Chen et al, 2009) and HTMM
(Gruber et al, 2007) as the baseline methods for the
comparison. Because none of these three methods
can generate a topic transition matrix directly, we
extended them a little bit to achieve this goal. For
pLSA, we used the document-level labels as priors
for the topic distribution in each document, so that
the estimated topics can be aligned with the prede-
fined class labels. After the topics were estimated,
for each sentence we selected the topic that had
the highest posterior probability to generate the sen-
tence as its class label. For lPerm and HTMM, we
used Kuhn-Munkres algorithm (Lova?sz and Plum-
mer, 1986) to find the optimal topic-to-class align-
ment based on the sentence-level annotations. Af-
ter the sentences were annotated with class labels,
we estimated the topic transition matrices for all of
these three methods by Eq(9).
1531
Since only a small portion of sentences are an-
notated in the Review data set, very few neighbor-
ing sentences are annotated at the same time, which
introduces many noisy transitions. As a result, we
only performed the comparison on the Ads data set.
The ?ground-truth? transition matrix was estimated
based on all the 302 annotated ads.
pLSA+prior lPerm HTMM strTM
avgKL 0.743 1.101 0.572 0.372
p-value 0.023 1e-4 0.007 ?
Table 2: Comparison of estimated topic transitions on
Ads data set
In Table 2, the p-value was calculated based on t-
test of the KL divergency between each topic?s tran-
sition probability against strTM. From the results,
we can see that avgKL of strTM is smaller than the
other three baseline methods, which means the esti-
mated transitional relation by strTM is much closer
to the ground-truth transition. This demonstrates
that strTM captures the topical structure well, com-
pared with other baseline methods.
5.3 Sentence Annotation
In this section, we demonstrate how the identified
topical structure can benefit the task of sentence an-
notation. Sentence annotation is one step beyond the
traditional document classification task: in sentence
annotation, we want to predict the class label for
each sentence in the document, and this will be help-
ful for other problems, including extractive summa-
rization and passage retrieval. However, the lack of
detailed annotations on sentences greatly limits the
effectiveness of the supervised classification meth-
ods, which have been proved successful on docu-
ment classifications.
In this experiment, we propose to use strTM to ad-
dress this annotation task. One advantage of strTM
is that it captures the topic transitions on the sen-
tence level within documents, which provides a reg-
ularization over the adjacent predictions.
To examine the effectiveness of such structural
regularization, we compared strTM with four base-
line methods: pLSA, lPerm, HTMM and Naive
Bayes model. The sentence labeling approaches for
strTM, pLSA, lPerm and HTMM have been dis-
cussed in the previous section. As for Naive Bayes
model, we used EM algorithm 2 with both labeled
and unlabeled data for the training purpose (we used
the same unigram features as in topics models). We
set weights for the unlabeled data to be 10?3 in
Naive Bayes with EM.
The comparison was performed on both data sets.
We set the size of topics in each topic model equal
to the number of classes in each data set accord-
ingly. To tackle the situation where some sentences
in the document are not strictly associated with any
classes, we introduced an additional NULL content
topic in all the topic models. During the training
phase, none of the methods used the sentence-level
annotations in the documents, so that we treated the
whole corpus as the training and testing set.
To evaluate the prediction performance, we cal-
culated accuracy, recall and precision based on the
correct predictions over the sentences, and averaged
over all the classes as the criterion.
Model Accuracy Recall Precison
pLSA+prior 0.432 0.649 0.457
lPerm 0.610 0.514 0.471
HTMM 0.606 0.588 0.443
NB+EM 0.528 0.337 0.612
strTM 0.747 0.674 0.620
Table 3: Sentence annotation performance on Ads data
set
Model Accuracy Recall Precison
pLSA+prior 0.342 0.278 0.250
lPerm 0.286 0.205 0.184
HTMM 0.369 0.131 0.149
NB+EM 0.341 0.354 0.431
strTM 0.541 0.398 0.323
Table 4: Sentence annotation performance on Review
data set
Annotation performance on the two data sets is
shown in Table 3 and Table 4. We can see that strTM
outperformed all the other baseline methods on most
of the metrics: strTM has the best accuracy and re-
call on both of the two data sets. The improvement
confirms our hypothesis that besides solely depend-
ing on the local word patterns to perform predic-
2Mallet package: http://mallet.cs.umass.edu/
1532
tions, adjacent sentences provide a structural reg-
ularization in strTM (see Eq(3)). Compared with
lPerm, which postulates a strong constrain over the
topic assignment (sampling without replacement),
strTM performed much better on both of these two
data sets. This validates the benefit of modeling lo-
cal transitional relation compared with the global or-
dering. Besides, strTM achieved over 46% accu-
racy improvement compared with the second best
HTMM in the review data set. This result shows
the advantage of explicitly modeling the topic tran-
sitions between neighbor sentences instead of using
a binary relation to do so as in HTMM.
To further testify how the identified topical struc-
ture can help the sentence annotation task, we first
randomly removed 100 annotated ads from the train-
ing corpus and used them as the testing set. Then,
we used the ground-truth topic transition matrix es-
timated from the training data to order those 100 ads
according to their fitness scores under the ground-
truth topic transition matrix, which is defined in
Eq(11). We tested the prediction accuracy of differ-
ent models over two different partitions, top 50 and
bottom 50, according to this order.
fitness(d) = 1
|d|
|d|
?
i=0
log p?(ti|ti?1) (11)
where ti is the class label for ith sentence in doc-
ument d, |d| is the number of sentences in docu-
ment d, and p?(ti|ti?1) is the transition probability
estimated by Eq(9).
Top 50 p-value Bot 50 p-value
pLSA+prior 0.496 4e-12 0.542 0.004
lPerm 0.669 0.003 0.505 8e-4
HTMM 0.683 0.004 0.579 0.003
NB + EM 0.492 1e-12 0.539 0.002
strTM 0.752 ? 0.644 ?
Table 5: Sentence annotation performance according to
structural fitness
The results are shown in Table 5. From this table,
we can find that when the testing documents follow
the regular patterns as in the training data, i.e., top
50 group, strTM performs significantly better than
the other methods; when the testing documents don?t
share such structure, i.e., bottom 50 group, strTM?s
performance drops. This comparison confirms that
when a testing document shares similar topic struc-
ture as the training data, the topical transitions cap-
tured by strTM can help the sentence annotation task
a lot. In contrast, because pLSA and Naive Bayes
don?t depend on the document?s structure, their per-
formance does not change much over these two par-
titions.
5.4 Sentence Ordering
In this experiment, we illustrate how the learned top-
ical structure can help us better arrange sentences in
a document. Sentence ordering, or text planning, is
essential to many text synthesis applications, includ-
ing multi-document summarization (Goldstein et al,
2000) and concept-to-text generation (Barzilay and
Lapata, 2005).
In strTM, we evaluate all the possible orderings
of the sentences in a given document and selected
the optimal one which gives the highest generation
probability:
??(m) = argmax
?(m)
?
z
p(S?[0], S?[1], . . . , S?[m], z|?)
(12)
where ?(m) is a permutation of 1 to m, and ?[i] is
the ith element in this permutation.
To quantitatively evaluate the ordering result, we
treated the original sentence order (OSO) as the per-
fect order and used Kendall?s ?(?) (Lapata, 2006) as
the evaluation metric to compute the divergency be-
tween the optimum ordering given by the model and
OSO. Kendall?s ?(?) is widely used in information
retrieval domain to measure the correlation between
two ranked lists and it indicates how much an order-
ing differs from OSO, which ranges from 1 (perfect
matching) to -1 (totally mismatching).
Since only the HTMM and lPerm take the order
of sentences in the document into consideration, we
used them as the baselines in this experiment. We
ranked OSO together with candidate permutations
according to the corresponding model?s generation
probability. However, when the size of documents
becomes larger, it?s infeasible to permutate all the
orderings, therefore we randomly permutated 200
possible orderings of sentences as candidates when
there were more than 200 possible candidates. The
1533
2bedroom 1bath in very nice complex! Pool,
carport, laundry facilities!! Call Don (650)207-
5769 to see! Great location!! Also available,
2bed.2bath for $1275 in same complex.
=?
2bedroom 1bath in very nice complex! Pool, car-
port, laundry facilities!! Great location!! Also
available, 2bed.2bath for $1275 in same complex.
Call Don (650)207-5769 to see!
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Please drive by and call Marilyn for
appointment 650-652-5806. Address: 517 Price
Way, Vallejo. No Pets Please!
=?
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Address: 517 Price Way, Vallejo. No
Pets Please! Please drive by and call Marilyn for
appointment 650-652-5806.
Table 6: Sample results for document ordering by strTM
experiment was performed on both data sets with
80% data for training and the other 20% for testing.
We calculated the ?(?) of all these models for
each document in the two data sets and visualized
the distribution of ?(?) in each data set with his-
togram in Figure 3. From the results, we could ob-
serve that strTM?s ?(?) is more skewed towards the
positive range (with mean 0.619 in Ads data set and
0.398 in review data set) than lPerm?s results (with
mean 0.566 in Ads data set and 0.08 in review data
set) and HTMM?s results (with mean 0.332 in Ads
data set and 0.286 in review data set). This indi-
cates that strTM better captures the internal structure
within the documents.
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
100200
300400
500600
700800
900
?(?)
# of Doc
uments
AdslPermHTMMstrTM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
20
40
60
80
100
120
140
160
?(?)
# of Doc
uments
ReviewlPermHTMMstrTM
(a) Ads (b) Review
Figure 3: Document Ordering Performance in ?(?).
We see that all methods performed better on the
Ads data set than the review data set, suggesting
that the topical structures are more coherent in the
Ads data set than the review data. Indeed, in the
Ads data, strTM perfectly recovered 52.9% of the
original sentence order. When examining some mis-
matched results, we found that some of them were
due to an ?outlier? order given by the original docu-
ment (in comparison to the ?regular? patterns in the
set). In Table 6, we show two such examples where
we see the learned structure ?suggested? to move
the contact information to the end, which intuitively
gives us a more regular organization of the ads. It?s
hard to say that in this case, the system?s ordering is
inferior to that of the original; indeed, the system or-
der is arguably more natural than the original order.
6 Conclusions
In this paper, we proposed a new structural topic
model (strTM) to identify the latent topical struc-
ture in documents. Different from the traditional
topic models, in which exchangeability assumption
precludes them to capture the structure of a docu-
ment, strTM captures the topical structure explicitly
by introducing transitions among the topics. Experi-
ment results show that both the identified topics and
topical structure are intuitive and meaningful, and
they are helpful for improving the performance of
tasks such as sentence annotation and sentence or-
dering, where correctly recognizing the document
structure is crucial. Besides, strTM is shown to out-
perform not only the baseline topic models that fail
to model the dependency between the topics, but
also the semi-supervised Naive Bayes model for the
sentence annotation task.
Our work can be extended by incorporating richer
features, such as named entity and co-reference, to
enhance the model?s capability of structure finding.
Besides, advanced NLP techniques for document
analysis, e.g., shallow parsing, may also be used to
further improve structure finding.
7 Acknowledgments
We thank the anonymous reviewers for their use-
ful comments. This material is based upon work
supported by the National Science Foundation un-
der Grant Numbers IIS-0713581 and CNS-0834709,
and NASA grant NNX08AC35A.
1534
References
R. Barzilay and M. Lapata. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings
of the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331?338.
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models, with applications to generation
and summarization. In Proceedings of HLT-NAACL,
pages 113?120.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of the 26th annual international
ACM SIGIR conference, pages 127?134.
D.M. Blei and J.D. Lafferty. 2007. A correlated topic
model of science. The Annals of Applied Statistics,
1(1):17?35.
D.M. Blei and P.J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence, page 348. ACM.
D.M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3(2-3):993 ? 1022.
H. Chen, SRK Branavan, R. Barzilay, and D.R. Karger.
2009. Global models of document structure using la-
tent permutations. In Proceedings of HLT-NAACL,
pages 371?379.
P. Diaconis and D. Ylvisaker. 1979. Conjugate pri-
ors for exponential families. The Annals of statistics,
7(2):269?281.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1,
pages 562?569.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence ex-
traction. In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 40?48.
T. Grenager, D. Klein, and C.D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In Proceedings of the 43rd an-
nual meeting on association for computational linguis-
tics, pages 371?378.
T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenen-
baum. 2005. Integrating topics and syntax. Advances
in neural information processing systems, 17:537?
544.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic markov models. volume 2, pages 163?
170.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 50?57.
E.H. Hovy. 1993. Automated discourse generation using
discourse structure relations. Artificial intelligence,
63(1-2):341?385.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine learning, 37(2):183?
233.
H. Kamp. 1981. A theory of truth and semantic repre-
sentation. Formal methods in the study of language,
1:277?322.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
L. Lova?sz and M.D. Plummer. 1986. Matching theory.
Elsevier Science Ltd.
Y. Lu and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. In Proceeding of
the 17th international conference on World Wide Web,
pages 121?130.
Daniel Marcu. 1998. The rhetorical parsing of natural
language texts. In ACL ?98, pages 96?103.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th interna-
tional conference on World Wide Web, pages 171?180.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the NAACL-
HTC, pages 149?156.
B. Sun, P. Mitra, C.L. Giles, J. Yen, and H. Zha. 2007.
Topic segmentation with shared topic detection and
alignment of multiple documents. In Proceedings of
the 30th ACM SIGIR, pages 199?206.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
minning. In Proceeding of the 10th ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 743?748.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
1535
