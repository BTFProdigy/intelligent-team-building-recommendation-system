Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 82?88,
New York, June 2006. c?2006 Association for Computational Linguistics
Named Entity Transliteration and Discovery from Multilingual Comparable
Corpora
Alexandre Klementiev Dan Roth
Department of Computer Science
University of Illinois
Urbana, IL 61801
 
klementi,danr  @uiuc.edu
Abstract
Named Entity recognition (NER) is an im-
portant part of many natural language pro-
cessing tasks. Most current approaches
employ machine learning techniques and
require supervised data. However, many
languages lack such resources. This paper
presents an algorithm to automatically dis-
cover Named Entities (NEs) in a resource
free language, given a bilingual corpora
in which it is weakly temporally aligned
with a resource rich language. We ob-
serve that NEs have similar time distribu-
tions across such corpora, and that they
are often transliterated, and develop an al-
gorithm that exploits both iteratively. The
algorithm makes use of a new, frequency
based, metric for time distributions and a
resource free discriminative approach to
transliteration. We evaluate the algorithm
on an English-Russian corpus, and show
high level of NEs discovery in Russian.
1 Introduction
Named Entity recognition has been getting much
attention in NLP research in recent years, since it
is seen as a significant component of higher level
NLP tasks such as information distillation and ques-
tion answering, and an enabling technology for bet-
ter information access. Most successful approaches
to NER employ machine learning techniques, which
require supervised training data. However, for many
languages, these resources do not exist. Moreover,
it is often difficult to find experts in these languages
both for the expensive annotation effort and even for
language specific clues. On the other hand, compa-
rable multilingual data (such as multilingual news
streams) are increasingly available (see section 4).
In this work, we make two independent observa-
tions about Named Entities encountered in such cor-
pora, and use them to develop an algorithm that ex-
tracts pairs of NEs across languages. Specifically,
given a bilingual corpora that is weakly temporally
aligned, and a capability to annotate the text in one
of the languages with NEs, our algorithm identifies
the corresponding NEs in the second language text,
and annotates them with the appropriate type, as in
the source text.
The first observation is that NEs in one language
in such corpora tend to co-occur with their coun-
terparts in the other. E.g., Figure 1 shows a his-
togram of the number of occurrences of the word
Hussein and its Russian transliteration in our bilin-
gual news corpus spanning years 2001 through late
2005. One can see several common peaks in the two
histograms, largest one being around the time of the
beginning of the war in Iraq. The word Russia, on
the other hand, has a distinctly different temporal
signature. We can exploit such weak synchronicity
of NEs across languages as a way to associate them.
In order to score a pair of entities across languages,
we compute the similarity of their time distributions.
The second observation is that NEs are often
transliterated or have a common etymological origin
across languages, and thus are phonetically similar.
Figure 2 shows an example list of NEs and their pos-
82
 0
 5
 10
 15
 20
?hussein? (English)
 0
 5
 10
 15
 2
?hussein? (Russian)
 0
 5
 10
 15
 2
01/01/01 10/05/05
N
um
be
r o
f O
cc
ur
en
ce
s
Time
?russia? (English)
Figure 1: Temporal histograms for Hussein (top),
its Russian transliteration (middle), and of the word
Russia (bottom).
sible Russian transliterations.
Approaches that attempt to use these two charac-
teristics separately to identify NEs across languages
would have significant shortcomings. Translitera-
tion based approaches require a good model, typi-
cally handcrafted or trained on a clean set of translit-
eration pairs. On the other hand, time sequence sim-
ilarity based approaches would incorrectly match
words which happen to have similar time signatures
(e.g. Taliban and Afghanistan in recent news).
We introduce an algorithm we call co-ranking
which exploits these observations simultaneously to
match NEs on one side of the bilingual corpus to
their counterparts on the other. We use a Discrete
Fourier Transform (Arfken, 1985) based metric for
computing similarity of time distributions, and we
score NEs similarity with a linear transliteration
model. For a given NE in one language, the translit-
eration model chooses a top ranked list of candidates
in another language. Time sequence scoring is then
used to re-rank the candidates and choose the one
best temporally aligned with the NE. That is, we at-
tempt to choose a candidate which is both a good
transliteration (according to the current model) and
is well aligned with the NE. Finally, pairs of NEs
 	
  		 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 817?824,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Weakly Supervised Named Entity Transliteration and Discovery from
Multilingual Comparable Corpora
Alexandre Klementiev Dan Roth
Dept. of Computer Science
University of Illinois
Urbana, IL 61801
 
klementi,danr  @uiuc.edu
Abstract
Named Entity recognition (NER) is an
important part of many natural language
processing tasks. Current approaches of-
ten employ machine learning techniques
and require supervised data. However,
many languages lack such resources. This
paper presents an (almost) unsupervised
learning algorithm for automatic discov-
ery of Named Entities (NEs) in a resource
free language, given a bilingual corpora in
which it is weakly temporally aligned with
a resource rich language. NEs have similar
time distributions across such corpora, and
often some of the tokens in a multi-word
NE are transliterated. We develop an algo-
rithm that exploits both observations itera-
tively. The algorithm makes use of a new,
frequency based, metric for time distribu-
tions and a resource free discriminative ap-
proach to transliteration. Seeded with a
small number of transliteration pairs, our
algorithm discovers multi-word NEs, and
takes advantage of a dictionary (if one ex-
ists) to account for translated or partially
translated NEs. We evaluate the algorithm
on an English-Russian corpus, and show
high level of NEs discovery in Russian.
1 Introduction
Named Entity recognition has been getting much
attention in NLP research in recent years, since it
is seen as significant component of higher level
NLP tasks such as information distillation and
question answering. Most successful approaches
to NER employ machine learning techniques,
which require supervised training data. However,
for many languages, these resources do not ex-
ist. Moreover, it is often difficult to find experts
in these languages both for the expensive anno-
tation effort and even for language specific clues.
On the other hand, comparable multilingual data
(such as multilingual news streams) are becoming
increasingly available (see section 4).
In this work, we make two independent obser-
vations about Named Entities encountered in such
corpora, and use them to develop an algorithm that
extracts pairs of NEs across languages. Specifi-
cally, given a bilingual corpora that is weakly tem-
porally aligned, and a capability to annotate the
text in one of the languages with NEs, our algo-
rithm identifies the corresponding NEs in the sec-
ond language text, and annotates them with the ap-
propriate type, as in the source text.
The first observation is that NEs in one language
in such corpora tend to co-occur with their coun-
terparts in the other. E.g., Figure 1 shows a his-
togram of the number of occurrences of the word
Hussein and its Russian transliteration in our bilin-
gual news corpus spanning years 2001 through
late 2005. One can see several common peaks
in the two histograms, largest one being around
the time of the beginning of the war in Iraq. The
word Russia, on the other hand, has a distinctly
different temporal signature. We can exploit such
weak synchronicity of NEs across languages to
associate them. In order to score a pair of enti-
ties across languages, we compute the similarity
of their time distributions.
The second observation is that NEs often con-
tain or are entirely made up of words that are pho-
netically transliterated or have a common etymo-
logical origin across languages (e.g. parliament in
English and 	
 , its Russian translation),
and thus are phonetically similar. Figure 2 shows
817
 0
 5
 10
 15
 20
?hussein? (English)
 0
 5
 10
 15
 2
?hussein? (Russian)
 0
 5
 10
 15
 2
01/01/01 10/05/05
N
um
be
r o
f O
cc
ur
en
ce
s
Time
?russia? (English)
Figure 1: Temporal histograms for Hussein (top),
its Russian transliteration (middle), and of the
word Russia (bottom).
an example list of NEs and their possible Russian
transliterations.
Approaches that attempt to use these two
characteristics separately to identify NEs across
languages would have significant shortcomings.
Transliteration based approaches require a good
model, typically handcrafted or trained on a clean
set of transliteration pairs. On the other hand, time
sequence similarity based approaches would in-
correctly match words which happen to have sim-
ilar time signatures (e.g., Taliban and Afghanistan
in recent news).
We introduce an algorithm we call co-ranking
which exploits these observations simultaneously
to match NEs on one side of the bilingual cor-
pus to their counterparts on the other. We use a
Discrete Fourier Transform (Arfken, 1985) based
metric for computing similarity of time distribu-
tions, and show that it has significant advantages
over other metrics traditionally used. We score
NEs similarity with a linear transliteration model.
We first train a transliteration model on single-
word NEs. During training, for a given NE in one
language, the current model chooses a list of top
ranked transliteration candidates in another lan-
guage. Time sequence scoring is then used to re-
rank the list and choose the candidate best tem-
porally aligned with the NE. Pairs of NEs and the
best candidates are then used to iteratively train the
 	
  		 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?22,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Bayesian Approach to Unsupervised Semantic Role Induction
Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{titov|aklement}@mmci.uni-saarland.de
Abstract
We introduce two Bayesian models for un-
supervised semantic role labeling (SRL)
task. The models treat SRL as clustering
of syntactic signatures of arguments with
clusters corresponding to semantic roles.
The first model induces these clusterings
independently for each predicate, exploit-
ing the Chinese Restaurant Process (CRP)
as a prior. In a more refined hierarchical
model, we inject the intuition that the clus-
terings are similar across different predi-
cates, even though they are not necessar-
ily identical. This intuition is encoded as
a distance-dependent CRP with a distance
between two syntactic signatures indicating
how likely they are to correspond to a single
semantic role. These distances are automat-
ically induced within the model and shared
across predicates. Both models achieve
state-of-the-art results when evaluated on
PropBank, with the coupled model consis-
tently outperforming the factored counter-
part in all experimental set-ups.
1 Introduction
Semantic role labeling (SRL) (Gildea and Juraf-
sky, 2002), a shallow semantic parsing task, has
recently attracted a lot of attention in the com-
putational linguistic community (Carreras and
Ma`rquez, 2005; Surdeanu et al 2008; Hajic? et
al., 2009). The task involves prediction of predi-
cate argument structure, i.e. both identification of
arguments as well as assignment of labels accord-
ing to their underlying semantic role. For exam-
ple, in the following sentences:
(a) [A0 Mary] opened [A1 the door].
(b) [A0 Mary] is expected to open [A1 the door].
(c) [A1 The door] opened.
(d) [A1 The door] was opened [A0 by Mary].
Mary always takes an agent role (A0) for the pred-
icate open, and door is always a patient (A1).
SRL representations have many potential appli-
cations in natural language processing and have
recently been shown to be beneficial in question
answering (Shen and Lapata, 2007; Kaisser and
Webber, 2007), textual entailment (Sammons et
al., 2009), machine translation (Wu and Fung,
2009; Liu and Gildea, 2010; Wu et al 2011; Gao
and Vogel, 2011), and dialogue systems (Basili et
al., 2009; van der Plas et al 2011), among others.
Though syntactic representations are often predic-
tive of semantic roles (Levin, 1993), the interface
between syntactic and semantic representations is
far from trivial. The lack of simple determinis-
tic rules for mapping syntax to shallow semantics
motivates the use of statistical methods.
Although current statistical approaches have
been successful in predicting shallow seman-
tic representations, they typically require large
amounts of annotated data to estimate model pa-
rameters. These resources are scarce and ex-
pensive to create, and even the largest of them
have low coverage (Palmer and Sporleder, 2010).
Moreover, these models are domain-specific, and
their performance drops substantially when they
are used in a new domain (Pradhan et al 2008).
Such domain specificity is arguably unavoidable
for a semantic analyzer, as even the definitions
of semantic roles are typically predicate specific,
and different domains can have radically different
distributions of predicates (and their senses). The
necessity for a large amounts of human-annotated
data for every language and domain is one of the
major obstacles to the wide-spread adoption of se-
mantic role representations.
These challenges motivate the need for unsu-
pervised methods which, instead of relying on la-
beled data, can exploit large amounts of unlabeled
texts. In this paper, we propose simple and effi-
12
cient hierarchical Bayesian models for this task.
It is natural to split the SRL task into two
stages: the identification of arguments (the iden-
tification stage) and the assignment of semantic
roles (the labeling stage). In this and in much
of the previous work on unsupervised techniques,
the focus is on the labeling stage. Identification,
though an important problem, can be tackled with
heuristics (Lang and Lapata, 2011a; Grenager and
Manning, 2006) or, potentially, by using a super-
vised classifier trained on a small amount of data.
We follow (Lang and Lapata, 2011a), and regard
the labeling stage as clustering of syntactic sig-
natures of argument realizations for every predi-
cate. In our first model, as in most of the previous
work on unsupervised SRL, we define an indepen-
dent model for each predicate. We use the Chi-
nese Restaurant Process (CRP) (Ferguson, 1973)
as a prior for the clustering of syntactic signatures.
The resulting model achieves state-of-the-art re-
sults, substantially outperforming previous meth-
ods evaluated in the same setting.
In the first model, for each predicate we inde-
pendently induce a linking between syntax and se-
mantics, encoded as a clustering of syntactic sig-
natures. The clustering implicitly defines the set
of permissible alternations, or changes in the syn-
tactic realization of the argument structure of the
verb. Though different verbs admit different alter-
nations, some alternations are shared across mul-
tiple verbs and are very frequent (e.g., passiviza-
tion, example sentences (a) vs. (d), or dativiza-
tion: John gave a book to Mary vs. John gave
Mary a book) (Levin, 1993). Therefore, it is nat-
ural to assume that the clusterings should be sim-
ilar, though not identical, across verbs.
Our second model encodes this intuition by re-
placing the CRP prior for each predicate with
a distance-dependent CRP (dd-CRP) prior (Blei
and Frazier, 2011) shared across predicates. The
distance between two syntactic signatures en-
codes how likely they are to correspond to a sin-
gle semantic role. Unlike most of the previous
work exploiting distance-dependent CRPs (Blei
and Frazier, 2011; Socher et al 2011; Duan et al
2007), we do not encode prior or external knowl-
edge in the distance function but rather induce it
automatically within our Bayesian model. The
coupled dd-CRP model consistently outperforms
the factored CRP counterpart across all the experi-
mental settings (with gold and predicted syntactic
parses, and with gold and automatically identified
arguments).
Both models admit efficient inference: the es-
timation time on the Penn Treebank WSJ corpus
does not exceed 30 minutes on a single proces-
sor and the inference algorithm is highly paral-
lelizable, reducing inference time down to sev-
eral minutes on multiple processors. This sug-
gests that the models scale to much larger corpora,
which is an important property for a successful
unsupervised learning method, as unlabeled data
is abundant.
The rest of the paper is structured as follows.
Section 2 begins with a definition of the seman-
tic role labeling task and discuss some specifics
of the unsupervised setting. In Section 3, we de-
scribe CRPs and dd-CRPs, the key components
of our models. In Sections 4 ? 6, we describe
our factored and coupled models and the infer-
ence method. Section 7 provides both evaluation
and analysis. Finally, additional related work is
presented in Section 8.
2 Task Definition
In this work, instead of assuming the availabil-
ity of role annotated data, we rely only on auto-
matically generated syntactic dependency graphs.
While we cannot expect that syntactic structure
can trivially map to a semantic representation
(Palmer et al 2005)1, we can use syntactic cues
to help us in both stages of unsupervised SRL.
Before defining our task, let us consider the two
stages separately.
In the argument identification stage, we imple-
ment a heuristic proposed in (Lang and Lapata,
2011a) comprised of a list of 8 rules, which use
nonlexicalized properties of syntactic paths be-
tween a predicate and a candidate argument to it-
eratively discard non-arguments from the list of
all words in a sentence. Note that inducing these
rules for a new language would require some lin-
guistic expertise. One alternative may be to an-
notate a small number of arguments and train a
classifier with nonlexicalized features instead.
In the argument labeling stage, semantic roles
are represented by clusters of arguments, and la-
beling a particular argument corresponds to decid-
ing on its role cluster. However, instead of deal-
1Although it provides a strong baseline which is diffi-
cult to beat (Grenager and Manning, 2006; Lang and Lapata,
2010; Lang and Lapata, 2011a).
13
ing with argument occurrences directly, we rep-
resent them as predicate specific syntactic signa-
tures, and refer to them as argument keys. This
representation aids our models in inducing high
purity clusters (of argument keys) while reducing
their granularity. We follow (Lang and Lapata,
2011a) and use the following syntactic features to
form the argument key representation:
? Active or passive verb voice (ACT/PASS).
? Argument position relative to predicate
(LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
In the example sentences in Section 1, the argu-
ment keys for candidate arguments Mary for sen-
tences (a) and (d) would be ACT:LEFT:SBJ and
PASS:RIGHT:LGS->by,2 respectively. While
aiming to increase the purity of argument key
clusters, this particular representation will not al-
ways produce a good match: e.g. the door in
sentence (c) will have the same key as Mary in
sentence (a). Increasing the expressiveness of the
argument key representation by flagging intransi-
tive constructions would distinguish that pair of
arguments. However, we keep this particular rep-
resentation, in part to compare with the previous
work.
In this work, we treat the unsupervised seman-
tic role labeling task as clustering of argument
keys. Thus, argument occurrences in the corpus
whose keys are clustered together are assigned the
same semantic role. Note that some adjunct-like
modifier arguments are already explicitly repre-
sented in syntax and thus do not need to be clus-
tered (modifiers AM-TMP, AM-MNR, AM-LOC, and
AM-DIR are encoded as ?syntactic? relations TMP,
MNR, LOC, and DIR, respectively (Surdeanu et al
2008)); instead we directly use the syntactic labels
as semantic roles.
3 Traditional and Distance-dependent
CRPs
The central components of our non-parametric
Bayesian models are the Chinese Restaurant Pro-
cesses (CRPs) and the closely related Dirichlet
Processes (DPs) (Ferguson, 1973).
CRPs define probability distributions over par-
titions of a set of objects. An intuitive metaphor
2LGS denotes a logical subject in a passive construction
(Surdeanu et al 2008).
for describing CRPs is assignment of tables to
restaurant customers. Assume a restaurant with a
sequence of tables, and customers who walk into
the restaurant one at a time and choose a table to
join. The first customer to enter is assigned the
first table. Suppose that when a client number i
enters the restaurant, i ? 1 customers are sitting
at each of the k ? (1, . . . ,K) tables occupied so
far. The new customer is then either seated at one
of theK tables with probability Nki?1+? , whereNk
is the number customers already sitting at table
k, or assigned to a new table with the probability
?
i?1+? . The concentration parameter ? encodes
the granularity of the drawn partitions: the larger
?, the larger the expected number of occupied ta-
bles. Though it is convenient to describe CRP in a
sequential manner, the probability of a seating ar-
rangement is invariant of the order of customers?
arrival, i.e. the process is exchangeable. In our
factored model, we use CRPs as a prior for clus-
tering argument keys, as we explain in Section 4.
Often CRP is used as a part of the Dirich-
let Process mixture model where each subset in
the partition (each table) selects a parameter (a
meal) from some base distribution over parame-
ters. This parameter is then used to generate all
data points corresponding to customers assigned
to the table. The Dirichlet processes (DP) are
closely connected to CRPs: instead of choosing
meals for customers through the described gener-
ative story, one can equivalently draw a distribu-
tion G over meals from DP and then draw a meal
for every customer from G. We refer the reader
to Teh (2010) for details on CRPs and DPs. In
our method, we use DPs to model distributions of
arguments for every role.
In order to clarify how similarities between
customers can be integrated in the generative pro-
cess, we start by reformulating the traditional
CRP in an equivalent form so that distance-
dependent CRP (dd-CRP) can be seen as its gen-
eralization. Instead of selecting a table for each
customer as described above, one can equiva-
lently assume that a customer i chooses one of
the previous customers ci as a partner with prob-
ability 1i?1+? and sits at the same table, or occu-
pies a new table with the probability ?i?1+? . The
transitive closure of this seating-with relation de-
termines the partition.
A generalization of this view leads to the defini-
tion of the distance-dependent CRP. In dd-CRPs,
14
a customer i chooses a partner ci = j with
the probability proportional to some non-negative
score di,j (di,j = dj,i) which encodes a similarity
between the two customers.3 More formally,
p(ci = j|D,?) ?
{
di,j , i 6= j
?, i = j
(1)
where D is the entire similarity graph. This pro-
cess lacks the exchangeability property of the tra-
ditional CRP but efficient approximate inference
with dd-CRP is possible with Gibbs sampling.
For more details on inference with dd-CRPs, we
refer the reader to Blei and Frazier (2011).
Though in previous work dd-CRP was used ei-
ther to encode prior knowledge (Blei and Fra-
zier, 2011) or other external information (Socher
et al 2011), we treat D as a latent variable
drawn from some prior distribution over weighted
graphs. This view provides a powerful approach
for coupling a family of distinct but similar clus-
terings: the family of clusterings can be drawn by
first choosing a similarity graph D for the entire
family and then re-usingD to generate each of the
clusterings independently of each other as defined
by equation (1). In Section 5, we explain how we
use this formalism to encode relatedness between
argument key clusterings for different predicates.
4 Factored Model
In this section we describe the factored method
which models each predicate independently. In
Section 2 we defined our task as clustering of ar-
gument keys, where each cluster corresponds to a
semantic role. If an argument key k is assigned
to a role r (k ? r), all of its occurrences are la-
beled r.
Our Bayesian model encodes two common as-
sumptions about semantic roles. First, we enforce
the selectional restriction assumption: we assume
that the distribution over potential argument fillers
is sparse for every role, implying that ?peaky? dis-
tributions of arguments for each role r are pre-
ferred to flat distributions. Second, each role nor-
mally appears at most once per predicate occur-
rence. Our inference will search for a clustering
which meets the above requirements to the maxi-
mal extent.
3It may be more standard to use a decay function f :
R ? R and choose a partner with the probability propor-
tional to f(?di,j). However, the two forms are equivalent
and using scores di,j directly is more convenient for our in-
duction purposes.
Our model associates two distributions with
each predicate: one governs the selection of argu-
ment fillers for each semantic role, and the other
models (and penalizes) duplicate occurrence of
roles. Each predicate occurrence is generated in-
dependently given these distributions. Let us de-
scribe the model by first defining how the set of
model parameters and an argument key clustering
are drawn, and then explaining the generation of
individual predicate and argument instances. The
generative story is formally presented in Figure 1.
We start by generating a partition of argument
keys Bp with each subset r ? Bp representing
a single semantic role. The partitions are drawn
from CRP(?) (see the Factored model section of
Figure 1) independently for each predicate. The
crucial part of the model is the set of selectional
preference parameters ?p,r, the distributions of ar-
guments x for each role r of predicate p. We
represent arguments by their syntactic heads,4 or
more specifically, by either their lemmas or word
clusters assigned to the head by an external clus-
tering algorithm, as we will discuss in more detail
in Section 7.5 For the agent role A0 of the pred-
icate open, for example, this distribution would
assign most of the probability mass to arguments
denoting sentient beings, whereas the distribution
for the patient role A1 would concentrate on ar-
guments representing ?openable? things (doors,
boxes, books, etc).
In order to encode the assumption about sparse-
ness of the distributions ?p,r, we draw them from
the DP prior DP (?,H(A)) with a small concen-
tration parameter ?, the base probability distribu-
tionH(A) is just the normalized frequencies of ar-
guments in the corpus. The geometric distribution
?p,r is used to model the number of times a role
r appears with a given predicate occurrence. The
decision whether to generate at least one role r is
drawn from the uniform Bernoulli distribution. If
0 is drawn then the semantic role is not realized
for the given occurrence, otherwise the number
of additional roles r is drawn from the geometric
distribution Geom(?p,r). The Beta priors over ?
4For prepositional phrases, we take as head the head noun
of the object noun phrase as it encodes crucial lexical infor-
mation. However, the preposition is not ignored but rather
encoded in the corresponding argument key, as explained
in Section 2.
5Alternatively, the clustering of arguments could be in-
duced within the model, as done in (Titov and Klementiev,
2011).
15
Clustering of argument keys:
Factored model:
for each predicate p = 1, 2, . . . :
Bp ? CRP (?) [partition of arg keys]
Coupled model:
D ? NonInform [similarity graph]
for each predicate p = 1, 2, . . . :
Bp ? dd-CRP (?,D) [partition of arg keys]
Parameters:
for each predicate p = 1, 2, . . . :
for each role r ? Bp:
?p,r ? DP (?,H(A)) [distrib of arg fillers]
?p,r ? Beta(?0, ?1) [geom distr for dup roles]
Data Generation:
for each predicate p = 1, 2, . . . :
for each occurrence l of p:
for every role r ? Bp:
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(p, r) [draw one arg]
while [n ? ?p,r] = 1: [continue generation]
GenArgument(p, r) [draw more args]
GenArgument(p, r):
kp,r ? Unif(1, . . . , |r|) [draw arg key]
xp,r ? ?p,r [draw arg filler]
Figure 1: Generative stories for the factored and cou-
pled models.
can indicate the preference towards generating at
most one argument for each role. For example,
it would express the preference that a predicate
open typically appears with a single agent and a
single patient arguments.
Now, when parameters and argument key clus-
terings are chosen, we can summarize the re-
mainder of the generative story as follows. We
begin by independently drawing occurrences for
each predicate. For each predicate role we in-
dependently decide on the number of role occur-
rences. Then we generate each of the arguments
(see GenArgument) by generating an argument
key kp,r uniformly from the set of argument keys
assigned to the cluster r, and finally choosing its
filler xp,r, where the filler is either a lemma or a
word cluster corresponding to the syntactic head
of the argument.
5 Coupled Model
As we argued in Section 1, clusterings of argu-
ment keys implicitly encode the pattern of alter-
nations for a predicate. E.g., passivization can be
roughly represented with the clustering of the key
ACT:LEFT:SBJ with PASS:RIGHT:LGS->by
and ACT:RIGHT:OBJ with PASS:LEFT:SBJ.
The set of permissible alternations is predicate-
specific,6 but nevertheless they arguably repre-
sent a small subset of all clusterings of argu-
ment keys. Also, some alternations are more
likely to be applicable to a verb than others: for
example, passivization and dativization alterna-
tions are both fairly frequent, whereas, locative-
preposition-drop alternation (Mary climbed up the
mountain vs. Mary climbed the mountain) is less
common and applicable only to several classes
of predicates representing motion (Levin, 1993).
We represent this observation by quantifying how
likely a pair of keys is to be clustered. These
scores (di,j for every pair of argument keys i and
j) are induced automatically within the model,
and treated as latent variables shared across pred-
icates. Intuitively, if data for several predicates
strongly suggests that two argument keys should
be clustered (e.g., there is a large overlap be-
tween argument fillers for the two keys) then the
posterior will indicate that di,j is expected to be
greater for the pair {i, j} than for some other pair
{i?, j?} for which the evidence is less clear. Con-
sequently, argument keys i and j will be clustered
even for predicates without strong evidence for
such a clustering, whereas i? and j? will not.
One argument against coupling predicates may
stem from the fact that we are using unlabeled
data and may be able to obtain sufficient amount
of learning material even for less frequent pred-
icates. This may be a valid observation, but an-
other rationale for sharing this similarity structure
is the hypothesis that alternations may be easier
to detect for some predicates than for others. For
example, argument key clustering of predicates
with very restrictive selectional restrictions on ar-
gument fillers is presumably easier than clustering
for predicates with less restrictive and overlap-
ping selectional restriction, as compactness of se-
lectional preferences is a central assumption driv-
ing unsupervised learning of semantic roles. E.g.,
predicates change and defrost belong to the same
Levin class (change-of-state verbs) and therefore
admit similar alternations. However, the set of po-
tential patients of defrost is sufficiently restricted,
6Or, at least specific to a class of predicates (Levin,
1993).
16
whereas the selectional restrictions for the patient
of change are far less specific and they overlap
with selectional restrictions for the agent role, fur-
ther complicating the clustering induction task.
This observation suggests that sharing clustering
preferences across verbs is likely to help even if
the unlabeled data is plentiful for every predicate.
More formally, we generate scores di,j , or
equivalently, the full labeled graph D with ver-
tices corresponding to argument keys and edges
weighted with the similarity scores, from a prior.
In our experiments we use a non-informative prior
which factorizes over pairs (i.e. edges of the
graph D), though more powerful alternatives can
be considered. Then we use it, in a dd-CRP(?,
D), to generate clusterings of argument keys for
every predicate. The rest of the generative story is
the same as for the factored model. The part rele-
vant to this model is shown in the Coupled model
section of Figure 1.
Note that this approach does not assume that
the frequencies of syntactic patterns correspond-
ing to alternations are similar, and a large value
for di,j does not necessarily mean that the corre-
sponding syntactic frames i and j are very fre-
quent in a corpus. What it indicates is that a large
number of different predicates undergo the corre-
sponding alternation; the frequency of the alterna-
tion is a different matter. We believe that this is an
important point, as we do not make a restricting
assumption that an alternation has the same dis-
tributional properties for all verbs which undergo
this alternation.
6 Inference
An inference algorithm for an unsupervised
model should be efficient enough to handle vast
amounts of unlabeled data, as it can easily be ob-
tained and is likely to improve results. We use
a simple approximate inference algorithm based
on greedy MAP search. We start by discussing
MAP search for argument key clustering with the
factored model and then discuss its extension ap-
plicable to the coupled model.
6.1 Role Induction
For the factored model, semantic roles for every
predicate are induced independently. Neverthe-
less, search for a MAP clustering can be expen-
sive, as even a move involving a single argument
key implies some computations for all its occur-
rences in the corpus. Instead of more complex
MAP search algorithms (see, e.g., (Daume III,
2007)), we use a greedy procedure where we start
with each argument key assigned to an individual
cluster, and then iteratively try to merge clusters.
Each move involves (1) choosing an argument key
and (2) deciding on a cluster to reassign it to. This
is done by considering all clusters (including cre-
ating a new one) and choosing the most probable
one.
Instead of choosing argument keys randomly at
the first stage, we order them by corpus frequency.
This ordering is beneficial as getting clustering
right for frequent argument keys is more impor-
tant and the corresponding decisions should be
made earlier.7 We used a single iteration in our
experiments, as we have not noticed any benefit
from using multiple iterations.
6.2 Similarity Graph Induction
In the coupled model, clusterings for different
predicates are statistically dependent, as the simi-
larity structureD is latent and shared across pred-
icates. Consequently, a more complex inference
procedure is needed. For simplicity here and in
our experiments, we use the non-informative prior
distribution over D which assigns the same prior
probability to every possible weight di,j for every
pair {i, j}.
Recall that the dd-CRP prior is defined in terms
of customers choosing other customers to sit with.
For the moment, let us assume that this relation
among argument keys is known, that is, every ar-
gument key k for predicate p has chosen an argu-
ment key cp,k to ?sit? with. We can compute the
MAP estimate for all di,j by maximizing the ob-
jective:
argmax
di,j , i 6=j
?
p
?
k?Kp
log
dk,cp,k
?
k??Kp dk,k?
,
where Kp is the set of all argument keys for the
predicate p. We slightly abuse the notation by us-
ing di,i to denote the concentration parameter ?
in the previous expression. Note that we also as-
sume that similarities are symmetric, di,j = dj,i.
If the set of argument keysKp would be the same
for every predicate, then the optimal di,j would
7This idea has been explored before for shallow semantic
representations (Lang and Lapata, 2011a; Titov and Klemen-
tiev, 2011).
17
be proportional to the number of times either i se-
lects j as a partner, or j chooses i as a partner.8
This no longer holds if the sets are different, but
the solution can be found efficiently using a nu-
meric optimization strategy; we use the gradient
descent algorithm.
We do not learn the concentration parameter
?, as it is used in our model to indicate the de-
sired granularity of semantic roles, but instead
only learn di,j (i 6= j). However, just learning
the concentration parameter would not be suffi-
cient as the effective concentration can be reduced
or increased arbitrarily by scaling all the similar-
ities di,j (i 6= j) at once, as follows from expres-
sion (1). Instead, we enforce the normalization
constraint on the similarities di,j . We ensure that
the prior probability of choosing itself as a part-
ner, averaged over predicates, is the same as it
would be with uniform di,j (di,j = 1 for every
key pair {i, j}, i 6= j). This roughly says that
we want to preserve the same granularity of clus-
tering as it was with the uniform similarities. We
accomplish this normalization in a post-hoc fash-
ion by dividing the weights after optimization by
?
p
?
k,k??Kp, k? 6=k dk,k?/
?
p |Kp|(|Kp| ? 1).
If D is fixed, partners for every predicate p and
every k can be found using virtually the same al-
gorithm as in Section 6.1: the only difference is
that, instead of a cluster, each argument key itera-
tively chooses a partner.
Though, in practice, both the choice of partners
and the similarity graphs are latent, we can use an
iterative approach to obtain a joint MAP estimate
of ck (for every k) and the similarity graph D by
alternating the two steps.9
Notice that the resulting algorithm is again
highly parallelizable: the graph induction stage
is fast, and induction of the seat-with relation
(i.e. clustering argument keys) is factorizable over
predicates.
One shortcoming of this approach is typical
for generative models with multiple ?features?:
when such a model predicts a latent variable, it
tends to ignore the prior class distribution and re-
lies solely on features. This behavior is due to
the over-simplifying independence assumptions.
It is well known, for instance, that the poste-
8Note that weights di,j are invariant under rescaling
when the rescaling is also applied to the concentration pa-
rameter ?.
9In practice, two iterations were sufficient.
rior with Naive Bayes tends to be overconfident
due to violated conditional independence assump-
tions (Rennie, 2001). The same behavior is ob-
served here: the shared prior does not have suf-
ficient effect on frequent predicates.10 Though
different techniques have been developed to dis-
count the over-confidence (Kolcz and Chowdhury,
2005), we use the most basic one: we raise the
likelihood term in power 1T , where the parameter
T is chosen empirically.
7 Empirical Evaluation
7.1 Data and Evaluation
We keep the general setup of (Lang and Lapata,
2011a), to evaluate our models and compare them
to the current state of the art. We run all of our
experiments on the standard CoNLL 2008 shared
task (Surdeanu et al 2008) version of Penn Tree-
bank WSJ and PropBank. In addition to gold
dependency analyses and gold PropBank annota-
tions, it has dependency structures generated au-
tomatically by the MaltParser (Nivre et al 2007).
We vary our experimental setup as follows:
? We evaluate our models on gold and auto-
matically generated parses, and use either
gold PropBank annotations or the heuristic
from Section 2 to identify arguments, result-
ing in four experimental regimes.
? In order to reduce the sparsity of predicate
argument fillers we consider replacing lem-
mas of their syntactic heads with word clus-
ters induced by a clustering algorithm as a
preprocessing step. In particular, we use
Brown (Br) clustering (Brown et al 1992)
induced over RCV1 corpus (Turian et al
2010). Although the clustering is hierarchi-
cal, we only use a cluster at the lowest level
of the hierarchy for each word.
We use the purity (PU) and collocation (CO) met-
rics as well as their harmonic mean (F1) to mea-
sure the quality of the resulting clusters. Purity
measures the degree to which each cluster con-
tains arguments sharing the same gold role:
PU =
1
N
?
i
max
j
|Gj ? Ci|
where if Ci is the set of arguments in the i-th in-
duced cluster,Gj is the set of arguments in the jth
10The coupled model without discounting still outper-
forms the factored counterpart in our experiments.
18
gold cluster, and N is the total number of argu-
ments. Collocation evaluates the degree to which
arguments with the same gold roles are assigned
to a single cluster. It is computed as follows:
CO =
1
N
?
j
max
i
|Gj ? Ci|
We compute the aggregate PU, CO, and F1
scores over all predicates in the same way as
(Lang and Lapata, 2011a) by weighting the scores
of each predicate by the number of its argument
occurrences. Note that since our goal is to evalu-
ate the clustering algorithms, we do not include
incorrectly identified arguments (i.e. mistakes
made by the heuristic defined in Section 2) when
computing these metrics.
We evaluate both factored and coupled models
proposed in this work with and without Brown
word clustering of argument fillers (Factored,
Coupled, Factored+Br, Coupled+Br). Our mod-
els are robust to parameter settings, they were
tuned (to an order of magnitude) on the develop-
ment set and were the same for all model variants:
? = 1.e-3, ? = 1.e-3, ?0 = 1.e-3, ?1 = 1.e-10,
T = 5. Although they can be induced within the
model, we set them by hand to indicate granular-
ity preferences. We compare our results with the
following alternative approaches. The syntactic
function baseline (SyntF) simply clusters predi-
cate arguments according to the dependency re-
lation to their head. Following (Lang and Lapata,
2010), we allocate a cluster for each of 20 most
frequent relations in the CoNLL dataset and one
cluster for all other relations. We also compare
our performance with the Latent Logistic classifi-
cation (Lang and Lapata, 2010), Split-Merge clus-
tering (Lang and Lapata, 2011a), and Graph Parti-
tioning (Lang and Lapata, 2011b) approaches (la-
beled LLogistic, SplitMerge, and GraphPart, re-
spectively) which achieve the current best unsu-
pervised SRL results in this setting.
7.2 Results
7.2.1 Gold Arguments
Experimental results are summarized in Ta-
ble 1. We begin by comparing our models to the
three existing clustering approaches on gold syn-
tactic parses, and using gold PropBank annota-
tions to identify predicate arguments. In this set of
experiments we measure the relative performance
of argument clustering, removing the identifica-
gold parses auto parses
PU CO F1 PU CO F1
LLogistic 79.5 76.5 78.0 77.9 74.4 76.2
SplitMerge 88.7 73.0 80.1 86.5 69.8 77.3
GraphPart 88.6 70.7 78.6 87.4 65.9 75.2
Factored 88.1 77.1 82.2 85.1 71.8 77.9
Coupled 89.3 76.6 82.5 86.7 71.2 78.2
Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6
Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8
SyntF 81.6 77.5 79.5 77.1 70.9 73.9
Table 1: Argument clustering performance with gold
argument identification. Bold-face is used to highlight
the best F1 scores.
tion stage, and minimize the noise due to auto-
matic syntactic annotations. All four variants of
the models we propose substantially outperform
other models: the coupled model with Brown
clustering of argument fillers (Coupled+Br) beats
the previous best model SplitMerge by 2.9% F1
score. As mentioned in Section 2, our approach
specifically does not cluster some of the modifier
arguments. In order to verify that this and argu-
ment filler clustering were not the only aspects
of our approach contributing to performance im-
provements, we also evaluated our coupled model
without Brown clustering and treating modifiers
as regular arguments. The model achieves 89.2%
purity, 74.0% collocation, and 80.9% F1 scores,
still substantially outperforming all of the alter-
native approaches. Replacing gold parses with
MaltParser analyses we see a similar trend, where
Coupled+Br outperforms the best alternative ap-
proach SplitMerge by 1.5%.
7.2.2 Automatic Arguments
Results are summarized in Table 2.11 The
precision and recall of our re-implementation of
the argument identification heuristic described in
Section 2 on gold parses were 87.7% and 88.0%,
respectively, and do not quite match 88.1% and
87.9% reported in (Lang and Lapata, 2011a).
Since we could not reproduce their argument
identification stage exactly, we are omitting their
results for the two regimes, instead including the
results for our two best models Factored+Br and
Coupled+Br. We see a similar trend, where the
coupled system consistently outperforms its fac-
tored counterpart, achieving 85.8% and 83.9% F1
11Note, that the scores are computed on correctly iden-
tified arguments only, and tend to be higher in these ex-
periments probably because the complex arguments get dis-
carded by the heuristic.
19
gold parses auto parses
PU CO F1 PU CO F1
Factored+Br 87.8 82.9 85.3 85.8 81.1 83.4
Coupled+Br 89.2 82.6 85.8 87.4 80.7 83.9
SyntF 83.5 81.4 82.4 81.4 79.1 80.2
Table 2: Argument clustering performance with auto-
matic argument identification.
for gold and MaltParser analyses, respectively.
We observe that consistently through the four
regimes, sharing of alternations between predi-
cates captured by the coupled model outperforms
the factored version, and that reducing the argu-
ment filler sparsity with clustering also has a sub-
stantial positive effect. Due to the space con-
straints we are not able to present detailed anal-
ysis of the induced similarity graph D, however,
argument-key pairs with the highest induced sim-
ilarity encode, among other things, passivization,
benefactive alternations, near-interchangeability
of some subordinating conjunctions and preposi-
tions (e.g., if and whether), as well as, restoring
some of the unnecessary splits introduced by the
argument key definition (e.g., semantic roles for
adverbials do not normally depend on whether the
construction is passive or active).
8 Related Work
Most of SRL research has focused on the super-
vised setting (Carreras and Ma`rquez, 2005; Sur-
deanu et al 2008), however, lack of annotated re-
sources for most languages and insufficient cover-
age provided by the existing resources motivates
the need for using unlabeled data or other forms
of weak supervision. This work includes methods
based on graph alignment between labeled and
unlabeled data (Fu?rstenau and Lapata, 2009), us-
ing unlabeled data to improve lexical generaliza-
tion (Deschacht and Moens, 2009), and projection
of annotation across languages (Pado and Lapata,
2009; van der Plas et al 2011). Semi-supervised
and weakly-supervised techniques have also been
explored for other types of semantic representa-
tions but these studies have mostly focused on re-
stricted domains (Kate and Mooney, 2007; Liang
et al 2009; Titov and Kozhevnikov, 2010; Gold-
wasser et al 2011; Liang et al 2011).
Unsupervised learning has been one of the cen-
tral paradigms for the closely-related area of re-
lation extraction, where several techniques have
been proposed to cluster semantically similar ver-
balizations of relations (Lin and Pantel, 2001;
Banko et al 2007). Early unsupervised ap-
proaches to the SRL problem include the work
by Swier and Stevenson (2004), where the Verb-
Net verb lexicon was used to guide unsupervised
learning, and a generative model of Grenager and
Manning (2006) which exploits linguistic priors
on syntactic-semantic interface.
More recently, the role induction problem has
been studied in Lang and Lapata (2010) where
it has been reformulated as a problem of detect-
ing alterations and mapping non-standard link-
ings to the canonical ones. Later, Lang and La-
pata (2011a) proposed an algorithmic approach
to clustering argument signatures which achieves
higher accuracy and outperforms the syntactic
baseline. In Lang and Lapata (2011b), the role
induction problem is formulated as a graph parti-
tioning problem: each vertex in the graph corre-
sponds to a predicate occurrence and edges repre-
sent lexical and syntactic similarities between the
occurrences. Unsupervised induction of seman-
tics has also been studied in Poon and Domin-
gos (2009) and Titov and Klementiev (2010) but
the induced representations are not entirely com-
patible with the PropBank-style annotations and
they have been evaluated only on a question an-
swering task for the biomedical domain. Also, the
related task of unsupervised argument identifica-
tion was considered in Abend et al(2009).
9 Conclusions
In this work we introduced two Bayesian models
for unsupervised role induction. They treat the
task as a family of related clustering problems,
one for each predicate. The first factored model
induces each clustering independently, whereas
the second model couples them by exploiting a
novel technique for sharing clustering preferences
across a family of clusterings. Both methods
achieve state-of-the-art results with the coupled
model outperforming the factored counterpart in
all regimes.
Acknowledgements
The authors acknowledge the support of the MMCI
Cluster of Excellence, and thank Hagen Fu?rstenau,
Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,
Caroline Sporleder and the anonymous reviewers for
their suggestions, and Joel Lang for answering ques-
tions about their methods and data.
20
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In ACL-IJCNLP.
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI.
Roberto Basili, Diego De Cao, Danilo Croce,
Bonaventura Coppola, and Alessandro Moschitti.
2009. Cross-language frame semantics transfer in
bilingual corpora. In CICLING.
David M. Blei and Peter Frazier. 2011. Distance de-
pendent chinese restaurant processes. Journal of
Machine Learning Research, 12:2461?2488.
Peter F. Brown, Vincent Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models for natural language. Compu-
tational Linguistics, 18(4):467?479.
Xavier Carreras and Llu??s Ma`rquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Semantic
Role Labeling. In CoNLL.
Hal Daume III. 2007. Fast search for dirichlet process
mixture models. In AISTATS.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the
Latent Words Language Model. In EMNLP.
Jason Duan, Michele Guindani, and Alan Gelfand.
2007. Generalized spatial dirichlet process models.
Biometrika, 94:809?825.
Thomas S. Ferguson. 1973. A Bayesian analysis
of some nonparametric problems. The Annals of
Statistics, 1(2):209?230.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In EMNLP.
Qin Gao and Stephan Vogel. 2011. Corpus expansion
for statistical machine translation with semantic role
label substitution rules. In ACL:HLT.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labelling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In ACL.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings
of the 13th Conference on Computational Natural
Language Learning (CoNLL-2009), June 4-5.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Work-
shop on Deep Linguistic Processing.
Rohit J. Kate and Raymond J. Mooney. 2007. Learn-
ing language semantics from ambigous supervision.
In AAAI.
Aleksander Kolcz and Abdur Chowdhury. 2005. Dis-
counting over-confidence of naive bayes in high-
recall text classification. In ECML.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised
semantic role induction via split-merge clustering.
In ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In ACL-IJCNLP.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL: HLT.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In KDD.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Coling.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In EMNLP-
CoNLL.
Sebastian Pado and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Alexis Palmer and Caroline Sporleder. 2010. Evalu-
ating FrameNet-style semantic parsing: the role of
coverage gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34:289?310.
Jason Rennie. 2001. Improving multi-class text
classification with Naive bayes. Technical Report
AITR-2001-004, MIT.
M. Sammons, V. Vydiswaran, T. Vieira, N. Johri,
M. Chang, D. Goldwasser, V. Srikumar, G. Kundu,
Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009.
Relation alignment for textual entailment recogni-
tion. In Text Analysis Conference (TAC).
21
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
Richard Socher, Andrew Maas, and Christopher Man-
ning. 2011. Spectral chinese restaurant processes:
Nonparametric clustering based on similarities. In
AISTATS.
Mihai Surdeanu, Adam Meyers Richard Johansson,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In EMNLP.
Yee Whye Teh. 2010. Dirichlet processes. In Ency-
clopedia of Machine Learning. Springer.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In ACL.
Ivan Titov and Mikhail Kozhevnikov. 2010.
Bootstrapping semantic analyzers from non-
contradictory texts. In ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL.
Lonneke van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual
semantic role annotation. In ACL.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: A hybrid two-pass model. In NAACL.
Dekai Wu, Marianna Apidianaki, Marine Carpuat, and
Lucia Specia, editors. 2011. Proc. of Fifth Work-
shop on Syntax, Semantics and Structure in Statisti-
cal Translation. ACL.
22
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130?140,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Toward Statistical Machine Translation without Parallel Corpora
Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
Center for Language and Speech Processing
Johns Hopkins University
Abstract
We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.
1 Introduction
The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a
novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al 2003) estimated using these mono-
lingual features.
Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:
? In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).
? In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.
? In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.
? Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.
130
2 Background
We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ?2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.
2.1 Parameters of phrase-based SMT
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:
? Phrase pairs. Phrase extraction heuristics
(Venugopal et al 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.
? Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, ?(e|f) and ?(f |e),
which are typically calculated via maximum
likelihood estimation.
? Lexical weighting. Since MLE overestimates
? for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-
How
much
should
you
charge
for
your
W
i
e
v
i
e
l
s
o
l
l
t
e
m
a
n
a
u
f
r
g
u
n
d
s
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
v
e
r
d
i
e
n
e
n
Facebook
profile
s
d
m
m
m
d
d
Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).
age word translation probabilities, w(ei|fj),
are calculated via phrase-pair-internal word
alignments.
? Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.
? Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.
The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.
131
2.2 Bilingual lexicon induction for SMT
Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.
A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al(2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.
In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100
l
llll
lll
l
0 100 200 300 400 500 600
0
10
20
30
40
Accu
racy
, %
Corpus Frequency
l Top 1Top 10
Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.
nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al(2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.
In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.
Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must
1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.
132
s1
s
2
s
3
s
N-1
s
N
?
?
?
t
1
t
2
t
3
t
M-1
t
M
?
?
dict.
project
?
?
?
?
?
?
c
o
m
p
a
r
e
para crecer
to expand
activity of
economic
activity
policy
growth
foreign
economico
tasa
planeta
empleo
extranjero
policy
para crecer
(projected)
ES Context
Vector
Projected ES
Context Vector
EN Context
Vectors
Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.
be compared against the vectors for all words in
the target language vocabulary. The size of the n2
comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.
3 Monolingual Parameter Estimation
We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.
3.1 Phrasal similarity features
Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, . . . sN ) and (t1, t2, . . . tM ) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an
terrorist (en)terrorista (es)
Occ
urre
nces
terrorist (en)riqueza (es)
Occ
urre
nces
Time
Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.
N - and target phrase e with an M -dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f .
Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f ?s
contextual vector as follows:
wk = nf,k ? (log(n/nk) + 1)
where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.
Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may
133
be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asia?tico is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al 2009), but it is scal-
able to full bilingual lexicon induction.
3.2 Lexical similarity features
In addition to the three phrase similarity features
used in our model ? c(f, e), t(f, e) and w(f, e) ?
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).
Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.
The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities ? and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).
3.3 Reordering
The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating
134
Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f (i), e(i))}Ni=1.
Output: Orientation features (pm, ps, pd).
Sf ? sentences containing f in Cf ;
Se ? sentences containing e in Ce;
(Bf ,?,?)? CollectOccurs(f,?Ni=1f
(i), Sf );
(Be, Ae, De)? CollectOccurs(e,?Ni=1e
(i), Se);
cm = cs = cd = 0;
foreach unique f ? in Bf do
foreach translation e? of f ? in T do
cm = cm + #Be (e
?);
cs = cs + #Ae (e
?);
cd = cd + #De (e
?);
c? cm + cs + cd;
return ( cmc ,
cs
c ,
cd
c )
CollectOccurs(r, R, S)
B ? (); A? (); D ? ();
foreach sentence s ? S do
foreach occurrence of phrase r in s do
B ? B + (longest preceding r and in R);
A? A + (longest following r and in R);
D ? D + (longest discontinuous w/ r and in
R);
return (B, A, D);
Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.
po(orientation|f, e) from two monolingual cor-
pora instead a bitext.
Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
?Profils?, e = ?profile?), we count its orien-
tation with the previously translated phrase pair
(f ? = ?in Facebook?, e? = ?Facebook?) across
all translated sentence pairs in the bitext.
In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = ?Profils?, and at least one other phrase
that we have a translation for, like f ? = ?in
Facebook?. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
?profile?) and any translation of f ?. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f ?) and English sentences con-
D
a
s
A
n
l
e
g
e
n
e
i
n
e
s
P
r
o
fi
l
s
i
n
F
a
c
e
b
o
o
k
i
s
t
e
i
n
f
a
c
h
s
What
does
your
Facebook
profile
reveal
Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (?profile?, ?Profils?)
from non-parallel sentences (the German sentence
translates as ?Creating a Facebook profile is easy?).
taining their corresponding translations (e, e?), we
are able to increment orientation counts for (f, e)
by looking at whether e and e? are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.
One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2
Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf ), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f ? preceding f , we look up transla-
tions in the phrase table T. Next, we count3 how
2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.
3#L(x) returns the count of object x in list L.
135
Monolingual training corpora
Europarl Gigaword Wikipedia
date range 4/96-10/09 5/94-12/08 n/a
uniq shared dates 829 5,249 n/a
Spanish articles n/a 3,727,954 59,463
English articles n/a 4,862,876 59,463
Spanish lines 1,307,339 22,862,835 2,598,269
English lines 1,307,339 67,341,030 3,630,041
Spanish words 28,248,930 774,813,847 39,738,084
English words 27,335,006 1,827,065,374 61,656,646
Spanish-English phrase table
Phrase pairs 3,093,228
Spanish phrases 89,386
English phrases 926,138
Spanish unigrams 13,216
Avg # translations 98.7
Spanish bigrams 41,426
Avg # translations 31.9
Spanish trigrams 34,744
Avg # translations 13.5
Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
many translations e? of f ? appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientation|f, e).
4 Experimental Setup
We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.
The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al 2010).4 MERT
4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.
was re-run for every experiment.
We estimate the parameters of our model from
two sets of monolingual data, detailed in Table 1:
? First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al(2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.
? Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.
To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.
The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the
5We use the afp, apw and xin sections of the corpora.
136
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Eur parl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
BLE
U
0
5
10
15
20
25 21.87 21.54
12.86
4.00
10.52
15.35 14.02 14.78
16.85 17.50
22.92
-
/
-
B
/
B
B
/
-
-
/
B
-
/
M
t
/
-
o
/
-
c
/
-
M
/
-
M
/
M
B
M
/
B
1 2 3 4 5 6 7 8 9 10 11
            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion
5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono
11, 19   BM/B   bilingual + all mono / bilingual
Estimated Using Europarl
Estimated Using Monolingual Corpora
B
L
E
U
.
BLE
U
0
5
10
15
20
25
10.15
13.13 14.02 14.07
17.00 17.92 18.79
23.36
23.
18.
17.
17
B
M
/
B
M
/
M
M
/
-
w
/
-
c
/
-
o
/
-
t
/
-
-
/
M
0
5
1
0
1
5
2
0
2
5
B
L
E
U
14
14
13.
10.
18 19171615141312
Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.
phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.
Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/?anni/papers/lowresmt/
5 Experimental Results
Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.
5.1 Lesion experiments
Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities ? and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.
5.2 Adding equivalent monolingual features
estimated using Europarl
Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating
137
reordering probabilities from monolingual data (?
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (?/?) to the
model with bilingual reordering features (?/B).
Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/?), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use ?mono-
lingual? corpora which have practically identical
phrasal and temporal distributions.
5.3 Estimating features using truly
monolingual corpora
Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.
Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).
Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.
6 Additional Related Work
Carbonell et al(2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.
Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
Sa?nchez-Cartagena et al(2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.
Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.
Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al(2005) and Daume? and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.
A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al 2010;
Uszkoreit et al 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al 2011).
These are complementary but orthogonal to our
research goals.
7 Conclusion
This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.
138
References
Enrique Alfonseca, Massimiliano Ciaramita, and
Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.
Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).
Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.
Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157?160.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Hal Daume? and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop
on Data-Driven Machine Translation, Toulouse,
France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.
Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.
Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284?293, Edinburgh,
Scotland, UK.
David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.
Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
139
Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of ACL.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.
Vctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-
Martnez, and Juan Antonio Pe?rez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.
Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.
Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
140
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1445?1455,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Bayesian Model for Unsupervised Semantic Parsing
Ivan Titov
Saarland University
Saarbruecken, Germany
titov@mmci.uni-saarland.de
Alexandre Klementiev
Johns Hopkins University
Baltimore, MD, USA
aklement@jhu.edu
Abstract
We propose a non-parametric Bayesian model
for unsupervised semantic parsing. Follow-
ing Poon and Domingos (2009), we consider
a semantic parsing setting where the goal is to
(1) decompose the syntactic dependency tree
of a sentence into fragments, (2) assign each
of these fragments to a cluster of semanti-
cally equivalent syntactic structures, and (3)
predict predicate-argument relations between
the fragments. We use hierarchical Pitman-
Yor processes to model statistical dependen-
cies between meaning representations of pred-
icates and those of their arguments, as well
as the clusters of their syntactic realizations.
We develop a modification of the Metropolis-
Hastings split-merge sampler, resulting in an
efficient inference algorithm for the model.
The method is experimentally evaluated by us-
ing the induced semantic representation for
the question answering task in the biomedical
domain.
1 Introduction
Statistical approaches to semantic parsing have re-
cently received considerable attention. While some
methods focus on predicting a complete formal rep-
resentation of meaning (Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Mooney, 2007), others
consider more shallow forms of representation (Car-
reras and Ma`rquez, 2005; Liang et al, 2009). How-
ever, most of this research has concentrated on su-
pervised methods requiring large amounts of labeled
data. Such annotated resources are scarce, expensive
to create and even the largest of them tend to have
low coverage (Palmer and Sporleder, 2010), moti-
vating the need for unsupervised or semi-supervised
techniques.
Conversely, research in the closely related task
of relation extraction has focused on unsupervised
or minimally supervised methods (see, for example,
(Lin and Pantel, 2001; Yates and Etzioni, 2009)).
These approaches cluster semantically equivalent
verbalizations of relations, often relying on syn-
tactic fragments as features for relation extraction
and clustering (Lin and Pantel, 2001; Banko et al,
2007). The success of these methods suggests that
semantic parsing can also be tackled as clustering
of syntactic realizations of predicate-argument rela-
tions. While a similar direction has been previously
explored in (Swier and Stevenson, 2004; Abend et
al., 2009; Lang and Lapata, 2010), the recent work
of (Poon and Domingos, 2009) takes it one step
further by not only predicting predicate-argument
structure of a sentence but also assigning sentence
fragments to clusters of semantically similar expres-
sions. For example, for a pair of sentences on Fig-
ure 1, in addition to inducing predicate-argument
structure, they aim to assign expressions ?Steelers?
and ?the Pittsburgh team? to the same semantic
class Steelers, and group expressions ?defeated?
and ?secured the victory over?. Such semantic rep-
resentation can be useful for entailment or question
answering tasks, as an entailment model can ab-
stract away from specifics of syntactic and lexical
realization relying instead on the induced semantic
representation. For example, the two sentences in
Figure 1 have identical semantic representation, and
therefore can be hypothesized to be equivalent.
1445
Ravens??defeated??Steelers
Ravenas?
dftSlrtS
cuh?vl io??y?nl
Ravv?n Pbbfv?vo
Ravens?secured?the?victory?over?the?Pittsburgh?team
io??y?nl
Ravenas?
lrtS dftS
bbgfh?n
cuh?vl
Ravv?n Pbbfv?vo
vmfd
Figure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens,
Steelers).
From the statistical modeling point of view, joint
learning of predicate-argument structure and dis-
covery of semantic clusters of expressions can also
be beneficial, because it results in a more compact
model of selectional preference, less prone to the
data-sparsity problem (Zapirain et al, 2010). In this
respect our model is similar to recent LDA-based
models of selectional preference (Ritter et al, 2010;
Se?aghdha, 2010), and can even be regarded as their
recursive and non-parametric extension.
In this paper, we adopt the above definition of un-
supervised semantic parsing and propose a Bayesian
non-parametric approach which uses hierarchical
Pitman-Yor (PY) processes (Pitman, 2002) to model
statistical dependencies between predicate and ar-
gument clusters, as well as distributions over syn-
tactic and lexical realizations of each cluster. Our
non-parametric model automatically discovers gran-
ularity of clustering appropriate for the dataset, un-
like the parametric method of (Poon and Domingos,
2009) which have to perform model selection and
use heuristics to penalize more complex models of
semantics. Additional benefits generally expected
from Bayesian modeling include the ability to en-
code prior linguistic knowledge in the form of hy-
perpriors and the potential for more reliable model-
ing of smaller datasets. More detailed discussion of
relation between the Markov Logic Network (MLN)
approach of (Poon and Domingos, 2009) and our
non-parametric method is presented in Section 3.
Hierarchical Pitman-Yor processes (or their spe-
cial case, hierarchical Dirichlet processes) have pre-
viously been used in NLP, for example, in the con-
text of syntactic parsing (Liang et al, 2007; John-
son et al, 2007). However, in all these cases the
effective size of the state space (i.e., the number
of sub-symbols in the infinite PCFG (Liang et al,
2007), or the number of adapted productions in the
adaptor grammar (Johnson et al, 2007)) was not
very large. In our case, the state space size equals
the total number of distinct semantic clusters, and,
thus, is expected to be exceedingly large even for
moderate datasets: for example, the MLN model in-
duces 18,543 distinct clusters from 18,471 sentences
of the GENIA corpus (Poon and Domingos, 2009).
This suggests that standard inference methods for hi-
erarchical PY processes, such as Gibbs sampling,
Metropolis-Hastings (MH) sampling with uniform
proposals, or the structured mean-field algorithm,
are unlikely to result in efficient inference: for ex-
ample in standard Gibbs sampling all thousands of
alternatives should be considered at each sampling
move. Instead, we use a split-merge MH sampling
algorithm, which is a standard and efficient infer-
ence tool for non-hierarchical PY processes (Jain
and Neal, 2000; Dahl, 2003) but has not previously
been used in hierarchical setting. We extend the
sampler to include composition-decomposition of
syntactic fragments in order to cluster fragments of
variables size, as in the example Figure 1, and also
include the argument role-syntax alignment move
which attempts to improve mapping between seman-
tic roles and syntactic paths for some fixed predicate.
Evaluating unsupervised models is a challenging
task. We evaluate our model both qualitatively, ex-
amining the revealed clustering of syntactic struc-
tures, and quantitatively, on a question answering
task. In both cases, we follow (Poon and Domingos,
2009) in using the corpus of biomedical abstracts.
Our model achieves favorable results significantly
outperforming the baselines, including state-of-the-
art methods for relation extraction, and achieves
scores comparable to those of the MLN model.
The rest of the paper is structured as follows. Sec-
tion 2 begins with a definition of the semantic pars-
ing task. Sections 3 and 4 give background on the
MLN model and the Pitman-Yor processes, respec-
tively. In Sections 5 and 6, we describe our model
and the inference method. Section 7 provides both
qualitative and quantitative evaluation. Finally, ad-
1446
ditional related work is presented in Section 8.
2 Semantic Parsing
In this section, we briefly define the unsupervised
semantic parsing task and underlying aspects and as-
sumptions relevant to our model.
Unlike (Poon and Domingos, 2009), we do not
use the lambda calculus formalism to define our task
but rather treat it as an instance of frame-semantic
parsing, or a specific type of semantic role label-
ing (Gildea and Jurafsky, 2002). The reason for this
is two-fold: first, the frame semantics view is more
standard in computational linguistics, sufficient to
describe induced semantic representation and conve-
nient to relate our method to the previous work. Sec-
ond, lambda calculus is a considerably more power-
ful formalism than the predicate-argument structure
used in frame semantics, normally supporting quan-
tification and logical connectors (for example, nega-
tion and disjunction), neither of which is modeled
by our model or in (Poon and Domingos, 2009).
In frame semantics, the meaning of a predicate
is conveyed by a frame, a structure of related con-
cepts that describes a situation, its participants and
properties (Fillmore et al, 2003). Each frame is
characterized by a set of semantic roles (frame el-
ements) corresponding to the arguments of the pred-
icate. It is evoked by a frame evoking element (a
predicate). The same frame can be evoked by differ-
ent but semantically similar predicates: for exam-
ple, both verbs ?buy? and ?purchase? evoke frame
Commerce buy in FrameNet (Fillmore et al, 2003).
The aim of the semantic role labeling task is to
identify all of the frames evoked in a sentence and
label their semantic role fillers. We extend this task
and treat semantic parsing as recursive prediction of
predicate-argument structure and clustering of argu-
ment fillers. Thus, parsing a sentence into this rep-
resentation involves (1) decomposing the sentence
into lexical items (one or more words), (2) assigning
a cluster label (a semantic frame or a cluster of ar-
gument fillers) to every lexical item, and (3) predict-
ing argument-predicate relations between the lexical
items. This process is illustrated in Figure 1. For
the leftmost example, the sentence is decomposed
into three lexical items: ?Ravens?, ?defeated?
and ?Steelers?, and they are assigned to clusters
Ravens, WinPrize and Steelers, respectively.
Then Ravens and Steelers are selected as a
Winner and an Opponent in the WinPrize frame.
In this work, we define a joint model for the label-
ing and argument identification stages. Similarly to
core semantic roles in FrameNet, semantic roles are
treated as frame-specific in our model, as our model
does not try to discover any correspondences be-
tween roles in different frames.
As you can see from the above description, frames
(which groups predicates with similar meaning such
as the WinPrize frame in our example) and clus-
ters of argument fillers (Ravens and Steelers) are
treated in our definition in a similar way. For con-
venience, we will refer to both types of clusters as
semantic classes.1
This definition of semantic parsing is closely re-
lated to a realistic relation extraction setting, as both
clustering of syntactic forms of relations (or extrac-
tion patterns) and clustering of argument fillers for
these relations is crucial for automatic construction
of knowledge bases (Yates and Etzioni, 2009).
In this paper, we make three assumptions. First,
we assume that each lexical item corresponds to a
subtree of the syntactic dependency graph of the
sentence. This assumption is similar to the ad-
jacency assumption in (Zettlemoyer and Collins,
2005), though ours may be more appropriate for lan-
guages with free or semi-free word order, where syn-
tactic structures are inherently non-projective. Sec-
ond, we assume that the semantic arguments are lo-
cal in the dependency tree; that is, one lexical item
can be a semantic argument of another one only if
they are connected by an arc in the dependency tree.
This is a slight simplification of the semantic role
labeling problem but one often made. Thus, the ar-
gument identification and labeling stages consist of
labeling each syntactic arc with a semantic role la-
bel. In comparison, the MLN model does not explic-
itly assume contiguity of lexical items and does not
make this directionality assumption but their clus-
tering algorithm uses initialization and clusterization
moves such that the resulting model also obeys both
of these constraints. Third, as in (Poon and Domin-
gos, 2009), we do not model polysemy as we assume
1Semantic classes correspond to lambda-form clusters in
(Poon and Domingos, 2009) terminology.
1447
that each syntactic fragment corresponds to a single
semantic class. This is not a model assumption and
is only used at inference as it reduces mixing time of
the Markov chain. It is not likely to be restrictive for
the biomedical domain studied in our experiments.
As in some of the recent work on learning se-
mantic representations (Eisenstein et al, 2009; Poon
and Domingos, 2009), we assume that dependency
structures are provided for every sentence. This as-
sumption allows us to construct models of seman-
tics not Markovian within a sequence of words (see
for an example a model described in (Liang et al,
2009)), but rather Markovian within a dependency
tree. Though we include generation of the syntac-
tic structure in our model, we would not expect that
this syntactic component would result in an accurate
syntactic model, even if trained in a supervised way,
as the chosen independence assumptions are over-
simplistic. In this way, we can use a simple gener-
ative story and build on top of the recent success in
syntactic parsing.
3 Relation to the MLN Approach
The work of (Poon and Domingos, 2009) models
joint probability of the dependency tree and its latent
semantic representation using Markov Logic Net-
works (MLNs) (Richardson and Domingos, 2006),
selecting parameters (weights of first-order clauses)
to maximize the probability of the observed depen-
dency structures. For each sentence, the MLN in-
duces a Markov network, an undirected graphical
model with nodes corresponding to ground atoms
and cliques corresponding to ground clauses.
The MLN is a powerful formalism and allows for
modeling complex interaction between features of
the input (syntactic trees) and latent output (seman-
tic representation), however, unsupervised learn-
ing of semantics with general MLNs can be pro-
hibitively expensive. The reason for this is that
MLNs are undirected models and when learned to
maximize likelihood of syntactically annotated sen-
tences, they would require marginalization over se-
mantic representation but also over the entire space
of syntactic structures and lexical units. Given the
complexity of the semantic parsing task and the need
to tackle large datasets, even approximate methods
are likely to be infeasible. In order to overcome
this problem, (Poon and Domingos, 2009) group pa-
rameters and impose local normalization constraints
within each group. Given these normalization con-
straints, and additional structural constraints satis-
fied by the model, namely that the clauses should
be engineered in such a way that they induce tree-
structured graphs for every sentence, the parameters
can be estimated by a variant of the EM algorithm.
The class of such restricted MLNs is equivalent
to the class of directed graphical models over the
same set of random variables corresponding to frag-
ments of syntactic and semantic structure. Given
that the above constraints do not directly fit into the
MLN methodology, we believe that it is more nat-
ural to regard their model as a directed model with
an underlying generative story specifying how the
semantic structure is generated and how the syntac-
tic parse is drawn for this semantic structure. This
view would facilitate understanding what kind of
features can easily be integrated into the model, sim-
plify application of non-parametric Bayesian tech-
niques and expedite the use of inference techniques
designed specifically for directed models. Our ap-
proach makes one step in this direction by proposing
a non-parametric version of such generative model.
4 Hierarchical Pitman-Yor Processes
The central component of our non-parametric
Bayesian model are Pitman-Yor (PY) processes,
which are a generalization of the Dirichlet processes
(DPs) (Ferguson, 1973). We use PY processes to
model distributions of semantic classes appearing as
an argument of other semantic classes. We also use
them to model distributions of syntactic realizations
for each semantic class and distributions of syntactic
dependency arcs for argument types. In this section
we present relevant background on PY processes.
For a more detailed consideration we refer the reader
to (Teh et al, 2006).
The Pitman-Yor process over a set S, denoted
PY (?, ?,H), is a stochastic process whose samples
G0 constitute probability measures on partitions of
S. In practice, we do not need to draw measures,
as they can be analytically marginalized out. The
conditional distribution of xj+1 given the previous
j draws, with G0 marginalized out, follows (Black-
1448
well and MacQueen, 1973)
xj+1|x1, . . . xj ?
K?
k=1
jk ? ?
j+?
??k +
K? + ?
j+?
H. (1)
where ?1, . . . , ?K are K values assigned to
x1, x2, . . . , xj . The number of times ?k was as-
signed is denoted jk, so that j =
?K
k=1 jk. The
parameter ? < 1 controls how heavy the tail of the
distribution is: when it approaches 1, a new value is
assigned to every draw, when ? = 0 the PY process
reduces to DP. The expected value of K scales as
O(?n?) with the number of draws n, while it scales
only logarithmically for DP processes. PY processes
are expected to be more appropriate for many NLP
problems, as they model power-law type distribu-
tions common for natural language (Teh, 2006).
Hierarchical Dirichlet Processes (HDP) or hierar-
chical PY processes are used if the goal is to draw
several related probability measures for the same
set S. For example, they can be used to generate
transition distributions of a Markov model, HDP-
HMM (Teh et al, 2006; Beal et al, 2002). For
such a HMM, the top-level state proportions are
drawn from the top-level stick breaking construction
? ? GEM(?, ?), and then the individual transi-
tion distributions for every state z = 1, 2, . . . ?z are
drawn from PY (?, ??, ??). The parameters ?? and
?? control how similar the individual transition dis-
tributions ?z are to the top-level state proportions ?,
or, equivalently, how similar the transition distribu-
tions are to each other.
5 A Model for Semantic Parsing
Our model of semantics associates with each seman-
tic class a set of distributions which govern the gen-
eration of corresponding syntactic realizations2 and
the selection of semantic classes for its arguments.
Each sentence is generated starting from the root of
its dependency tree, recursively drawing a seman-
tic class, its syntactic realization, arguments and se-
mantic classes for the arguments. Below we de-
scribe the model by first defining the set of the model
parameters and then explaining the generation of in-
2Syntactic realizations are syntactic tree fragments, and
therefore they correspond both to syntactic and lexical varia-
tions.
dividual sentences. The generative story is formally
presented in Figure 2.
We associate with each semantic class c, c =
1, 2, . . . , a distribution of its syntactic realizations
?c. For example, for the frame WinPrize illus-
trated in Figure 1 this distribution would concen-
trate at syntactic fragments corresponding to lexical
items ?defeated?, ?secured the victory? and ?won?.
The distribution is drawn from DP (w(C), H(C)),
where H(C) is a base measure over syntactic sub-
trees. We use a simple generative process to define
the probability of a subtree, the underlying model is
similar to the base measures used in the Bayesian
tree-substitution grammars (Cohn et al, 2009). We
start by generating a word w uniformly from the
treebank distribution, then we decide on the num-
ber of dependents of w using the geometric distribu-
tion Geom(q(C)). For every dependent we generate
a dependency relation r and a lexical form w? from
P (r|w)P (w?|r), where probabilities P are based on
add-0.1 smoothed treebank counts. The process is
continued recursively. The smaller the parameter
q(C), the lower is the probability assigned to larger
sub-trees.
Parameters ?c,t and ?
+
c,t, t = 1, . . . , T , de-
fine a distribution over vectors (m1,m2, . . . ,mT )
where mt is the number of times an argument of
type t appears for a given semantic frame occur-
rence3. For the frame WinPrize these parameters
would enforce that there exists exactly one Winner
and exactly one Opponent for each occurrence of
WinPrize. The parameter ?c,t defines the probabil-
ity of having at least one argument of type t. If 0 is
drawn from ?c,t then mt = 0, otherwise the number
of additional arguments of type t (mt ? 1) is drawn
from the geometric distribution Geom(?+c,t). This
generative story is flexible enough to accommodate
both argument types which appear at most once per
semantic class occurrence (e.g., agents), and argu-
ment types which frequently appear multiple times
per semantic class occurrence (e.g., arguments cor-
responding to descriptors).
Parameters ?c,t, t = 1, . . . , T , define the dis-
3For simplicity, we assume that each semantic class has T
associated argument types, note that this is not a restrictive as-
sumption as some of the argument types can remain unused,
and T can be selected to be sufficiently large to accommodate
all important arguments.
1449
Parameters:
? ? GEM(?0, ?0) [top-level proportions of classes]
?root ? PY (?root, ?root, ?) [distrib of sem classes at root]
for each sem class c = 1, 2, . . . :
?c ? DP (w(C), H(C)) [distribs of synt realizations]
for each arg type t = 1, 2, . . . T :
?c,t ? Beta(?0, ?1) [first argument generation]
?+c,t ? Beta(?
+
0 , ?
+
1 ) [geom distr for more args]
?c,t ? DP (w(A), H(A)) [distribs of synt paths]
?c,t ? PY (?, ?, ?) [distrib of arg fillers]
Data Generation:
for each sentence:
croot ? ?root [choose sem class for root]
GenSemClass(croot)
GenSemClass(c):
s ? ?c [draw synt realization]
for each arg type t = 1, . . . , T :
if [n ? ?c,t] = 1: [at least one arg appears]
GenArgument(c, t) [draw one arg]
while [n ? ?+c,t] = 1: [continue generation]
GenArgument(c, t) [draw more args]
GenArgument(c, t):
ac,t ? ?c,t [draw synt relation]
c?c,t ? ?c,t [draw sem class for arg]
GenSemClass(c?c,t) [recurse]
Figure 2: The generative story for the Bayesian model for
unsupervised semantic parsing.
tributions over syntactic paths for the argument
type t. In our example, for argument type
Opponent, this distribution would associate most
of the probability mass with relations pp over, dobj
and pp against. These distributions are drawn from
DP (w(A), H(A)). In this paper we only consider
paths consisting of a single relation, therefore the
base probability distributionH(A) is just normalized
frequencies of dependency relations in the treebank.
The crucial part of the model are the selection-
preference parameters ?c,t, the distributions of se-
mantic classes c? for each argument type t of class
c. For arguments Winner and Opponent of the
frame WinPrize these distributions would assign
most of the probability mass to semantic classes de-
noting teams or players. Distributions ?c,t are drawn
from a hierarchical PY process: first, top-level pro-
portions of classes ? are drawn fromGEM(?0, ?0),
and then the individual distributions ?c,t over c? are
chosen from PY (?, ?, ?).
For each sentence, we first generate a class corre-
sponding to the root of the dependency tree from the
root-specific distribution of semantic classes ?root.
Then we recursively generate classes for the entire
sentence. For a class c, we generate the syntactic
realization s and for each of the T types, decide
how many arguments of that type to generate (see
GenSemClass in Figure 2). Then we generate each
of the arguments (see GenArgument) by first gen-
erating a syntactic arc ac,t, choosing a class as its
filler c?c,t and, finally, recursing.
6 Inference
In our model, latent states, modeled with hierarchi-
cal PY processes, correspond to distinct semantic
classes and, therefore, their number is expected to
be very large for any reasonable model of semantics.
As a result, many standard inference techniques,
such as Gibbs sampling, or the structured mean-field
method are unlikely to result in tractable inference.
One of the standard and most efficient samplers for
non-hierarchical PY processes are split-merge MH
samplers (Jain and Neal, 2000; Dahl, 2003). In this
section we explain how split-merge samplers can be
applied to our model.
6.1 Split and Merge Moves
On each move, split-merge samplers decide either
to merge two states into one (in our case, merge two
semantic classes), or split one state into two. These
moves can be computed efficiently for our model of
semantics. Note that for any reasonable model of
semantics only a small subset of the entire set of se-
mantic classes can be used as an argument for some
fixed semantic class due to selectional preferences
exhibited by predicates. For instance, only teams or
players can fill arguments of the frame WinPrize
in our running example. As a result, only a small
number of terms in the joint distribution has to be
evaluated on every move we may consider.
When estimating the model, we start with assign-
ing each distinct word (or, more precisely, a tuple
of a word?s stem and its part-of-speech tag) to an
individual semantic class. Then, we would iterate
by selecting a random pair of class occurrences, and
decide, at random, whether we attempt to perform a
split-merge move or a compose-decompose move.
1450
6.2 Compose and Decompose Moves
The compose-decompose operations modify syntac-
tic fragments assigned to semantic classes, com-
posing two neighboring dependency sub-trees or
decomposing a dependency sub-tree. If the two
randomly-selected syntactic fragments s and s? cor-
respond to different classes, c and c?, we attempt
to compose them into s? and create a new semantic
class c?. All occurrences of s? are assigned to this new
class c?. For example, if two randomly-selected oc-
currences have syntactic realizations ?secure? and
?victory? they can be composed to obtain the syn-
tactic fragment ?secure
dobj
??? victory?. This frag-
ment will be assigned to a new semantic class which
can later be merged with other classes, such as the
ones containing syntactic realizations ?defeat? or
?win?.
Conversely, if both randomly-selected syntactic
fragments are already composed in the correspond-
ing class, we attempt to split them.
6.3 Role-Syntax Alignment Move
Merge, compose and decompose moves require re-
computation of mapping between argument types
(semantic roles) and syntactic fragments. Comput-
ing the best statistical mapping is infeasible and
proposing a random mapping will result in many
attempted moves being rejected. Instead we use
a greedy randomized search method called Gibbs
scan (Dahl, 2003). Though it is a part of the above 3
moves, this alignment move is also used on its own
to induce semantic arguments for classes (frames)
with a single syntactic realization.
The Gibbs scan procedure is also used during the
split move to select one of the newly introduced
classes for each considered syntactic fragment.
6.4 Informed Proposals
Since the number of classes is very large, selecting
examples at random would result in a relatively low
proportion of moves getting accepted, and, conse-
quently, in a slow-mixing Markov chain. Instead of
selecting both class occurrences uniformly, we se-
lect the first occurrence from a uniform distribution
and then use a simple but effective proposal distri-
bution for selecting the second class occurrence.
Let us denote the class corresponding to the first
occurrence as c1 and its syntactic realization as s1
with a head word w1. We begin by selecting uni-
formly randomly whether to attempt a compose-
decompose or a split-merge move.
If we chose a compose-decompose move, we look
for words (children) which can be attached below
the syntactic fragment s1. We use the normalized
counts of these words conditioned on the parent s1 to
select the second word w2. We then select a random
occurrence of w2; if it is a part of syntactic realiza-
tion of c1 then a decompose move is attempted. Oth-
erwise, we try to compose the corresponding clus-
ters together.
If we selected a split-merge move, we use a dis-
tribution based on the cosine similarity of lexical
contexts of the words. The context is represented
as a vector of counts of all pairs of the form (head
word, dependency type) and (dependent, depen-
dency type). So, instead of selecting a word occur-
rence uniformly, each occurrence of every word w2
is weighted by its similarity to w1, where the simi-
larity is based on the cosine distance.
As the moves are dependent only on syntactic rep-
resentations, all the proposal distributions can be
computed once at the initialization stage.4
7 Empirical Evaluation
We induced a semantic representation over a collec-
tion of texts and evaluated it by answering questions
about the knowledge contained in the corpus. We
used the GENIA corpus (Kim et al, 2003), a dataset
of 1999 biomedical abstracts, and a set of questions
produced by (Poon and Domingos, 2009). A exam-
ple question is shown in Figure 3.
All model hyperpriors were set to maximize the
posterior, except for w(A) and w(C), which were set
to 1.e?10 and 1.e?35, respectively. Inference was
run for around 300,000 sampling iterations until the
percentage of accepted split-merge moves became
lower than 0.05%.
Let us examine some of the induced semantic
classes (Table 1) before turning to the question an-
swering task. Almost all of the clustered syntactic
4In order to minimize memory usage, we used frequency
cut-off of 10. For split-merge moves, we select words based
on the cosine distance if the distance is below 0.95 and sample
the remaining words uniformly. This also reduces the required
memory usage.
1451
Class Variations
1 motif, sequence, regulatory element, response ele-
ment, element, dna sequence
2 donor, individual, subject
3 important, essential, critical
4 dose, concentration
5 activation, transcriptional activation, transactiva-
tion
6 b cell, t lymphocyte, thymocyte, b lymphocyte, t
cell, t-cell line, human lymphocyte, t-lymphocyte
7 indicate, reveal, document, suggest, demonstrate
8 augment, abolish, inhibit, convert, cause, abrogate,
modulate, block, decrease, reduce, diminish, sup-
press, up-regulate, impair, reverse, enhance
9 confirm, assess, examine, study, evaluate, test, re-
solve, determine, investigate
10 nf-kappab, nf-kappa b, nfkappab, nf-kb
11 antiserum, antibody, monoclonal antibody, ab, an-
tisera, mab
12 tnfalpha, tnf-alpha, il-6, tnf
Table 1: Examples of the induced semantic classes.
realizations have a clear semantic connection. Clus-
ter 6, for example, clusters lymphocytes with the ex-
ception of thymocyte, a type of cell which gener-
ates T cells. Cluster 8 contains verbs roughly corre-
sponding to Cause change of position on a
scale frame in FrameNet. Verbs in class 9 are used
in the context of providing support for a finding or
an action, and many of them are listed as evoking
elements for the Evidence frame in FrameNet.
Argument types of the induced classes also show
a tendency to correspond to semantic roles. For ex-
ample, an argument type of class 2 is modeled as
a distribution over two argument parts, prep of and
prep from. The corresponding arguments define the
origin of the cells (transgenic mouse, smoker, volun-
teer, donor, . . . ).
We now turn to the QA task and compare our
model (USP-BAYES) with the results of baselines
considered in (Poon and Domingos, 2009). The first
set of baselines looks for answers by attempting to
match a verb and its argument in the question with
the input text. The first version (KW) simply re-
turns the rest of the sentence on the other side of the
verb, while the second (KW-SYN) uses syntactic in-
formation to extract the subject or the object of the
verb.
Other baselines are based on state-of-the-art re-
lation extraction systems. When the extracted rela-
tion and one of the arguments match those in a given
Total Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP-MLN 334 295 88%
USP-BAYES 325 259 80%
Table 2: Performance on the QA task.
question, the second argument is returned as an an-
swer. The systems include TextRunner (TR) (Banko
et al, 2007), RESOLVER (RS) (Yates and Etzioni,
2009) and DIRT (Lin and Pantel, 2001). The EX-
ACT versions of the methods return answers when
they match the question argument exactly, and the
SUB versions produce answers containing the ques-
tion argument as a substring.
Similarly to the MLN system (USP-MLN), we
generate answers as follows. We use our trained
model to parse a question, i.e. recursively decom-
pose it into lexical items and assign them to seman-
tic classes induced at training. Using this semantic
representation, we look for the type of an argument
missing in the question, which, if found, is reported
as an answer. It is clear that overly coarse clusters
of argument fillers or clustering of semantically re-
lated but not equivalent relations can hurt precision
for this evaluation method.
Each system is evaluated by counting the answers
it generates, and computing the accuracy of those
answers.5 Table 2 summarizes the results. First,
both USP models significantly outperform all other
baselines: even though the accuracy of KW-SYN
and TR-EXACT are comparable with our accuracy,
the number of correct answers returned by USP-
Bayes is 4 and 11 times smaller than those of KW-
SYN and TR-EXACT, respectively. While we are
not beating the MLN baseline, the difference is not
significant. The effective number of questions is rel-
atively small (less than 80 different questions are an-
swered by any of the models). More than 50% of
USP-BAYES mistakes were due to wrong interpre-
tation of only 5 different questions. From another
point of view, most of the mistakes are explained
5The true recall is not known, as computing it would require
exhaustive annotation of the entire corpus.
1452
Question: What does cyclosporin A suppress?
Answer: expression of EGR-2
Sentence: As with EGR-3 , expression of EGR-2 was blocked
by cyclosporin A .
Question: What inhibits tnf-alpha?
Answer: IL -10
Sentence: Our previous studies in human monocytes have
demonstrated that interleukin ( IL ) -10 inhibits lipopolysac-
charide ( LPS ) -stimulated production of inflammatory cy-
tokines , IL-1 beta , IL-6 , IL-8 , and tumor necrosis factor (
TNF ) -alpha by blocking gene transcription .
Figure 3: An example of questions, answers by our model
and the corresponding sentences from the dataset.
by overly coarse clustering corresponding to just 3
classes, namely, 30%, 25% and 20% of errors are
due to the clusters 6, 8 and 12 (Figure 1), respec-
tively. Though all these clusters have clear semantic
interpretation (white blood cells, predicates corre-
sponding to changes and cykotines associated with
cancer progression, respectively), they appear to be
too coarse for the QA method we use in our exper-
iments. Though it is likely that tuning and differ-
ent heuristics may result in better scores, we chose
not to perform excessive tuning, as the evaluation
dataset is fairly small.
8 Related Work
There is a growing body of work on statistical learn-
ing for different versions of the semantic parsing
problem (e.g., (Gildea and Jurafsky, 2002; Zettle-
moyer and Collins, 2005; Ge and Mooney, 2005;
Mooney, 2007)), however, most of these methods
rely on human annotation, or some weaker forms of
supervision (Kate and Mooney, 2007; Liang et al,
2009; Titov and Kozhevnikov, 2010; Clarke et al,
2010) and very little research has considered the un-
supervised setting.
In addition to the MLN model (Poon and Domin-
gos, 2009), another unsupervised method has been
proposed in (Goldwasser et al, 2011). In that work,
the task is to predict a logical formula, and the only
supervision used is a lexicon providing a small num-
ber of examples for every logical symbol. A form of
self-training is then used to bootstrap the model.
Unsupervised semantic role labeling with a gen-
erative model has also been considered (Grenager
and Manning, 2006), however, they do not attempt
to discover frames and deal only with isolated pred-
icates. Another generative model for SRL has been
proposed in (Thompson et al, 2003), but the param-
eters were estimated from fully annotated data.
The unsupervised setting has also been consid-
ering for the related problem of learning narrative
schemas (Chambers and Jurafsky, 2009). However,
their approach is quite different from our Bayesian
model as it relies on similarity functions.
Though in this work we focus solely on the un-
supervised setting, there has been some success-
ful work on semi-supervised semantic-role label-
ing, including the Framenet version of the prob-
lem (Fu?rstenau and Lapata, 2009). Their method
exploits graph alignments between labeled and un-
labeled examples, and, therefore, crucially relies on
the availability of labeled examples.
9 Conclusions and Future Work
In this work, we introduced a non-parametric
Bayesian model for the semantic parsing problem
based on the hierarchical Pitman-Yor process. The
model defines a generative story for recursive gener-
ation of lexical items, syntactic and semantic struc-
tures. We extend the split-merge MH sampling algo-
rithm to include composition-decomposition moves,
and exploit the properties of our task to make it effi-
cient in the hierarchical setting we consider.
We plan to explore at least two directions in our
future work. First, we would like to relax some of
unrealistic assumptions made in our model: for ex-
ample, proper modeling of alterations requires joint
generation of syntactic realizations for predicate-
argument relations (Grenager and Manning, 2006;
Lang and Lapata, 2010), similarly, proper model-
ing of nominalization implies support of arguments
not immediately local in the syntactic structure. The
second general direction is the use of the unsuper-
vised methods we propose to expand the coverage of
existing semantic resources, which typically require
substantial human effort to produce.
Acknowledgements
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Chris Callison-Burch, Alexis
Palmer, Caroline Sporleder, Ben Van Durme and the
anonymous reviewers for their helpful comments and
suggestions.
1453
References
O. Abend, R. Reichart, and A. Rappoport. 2009. Unsu-
pervised argument identification for semantic role la-
beling. In Proceedings of ACL-IJCNLP, pages 28?36,
Singapore.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proc. of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 2670?2676.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2002. The infinite hidden markov model. In
Machine Learning, pages 29?245. MIT Press.
David Blackwell and James B. MacQueen. 1973. Fergu-
son distributions via polya urn schemes. The Annals
of Statistics, 1(2):353?355.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of the 9th Conference on Natu-
ral Language Learning, CoNLL-2005, Ann Arbor, MI
USA.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP).
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proc. of the Conference on Com-
putational Natural Language Learning (CoNLL).
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In HLT-NAACL, pages 548?556.
David B. Dahl. 2003. An improved merge-split sampler
for conjugate dirichlet process mixture models. Tech-
nical Report 1086, Department of Statistics, Univer-
sity of Wiscosin - Madison, November.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings of
EMNLP.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209?230.
C. J. Fillmore, C. R. Johnson, and M. R. L. Petruck.
2003. Background to framenet. International Journal
of Lexicography, 16:235?250.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ruifang Ge and Raymond J. Mooney. 2005. A statistical
semantic parser that integrates syntax and semantics.
In Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning (CONLL-05), Ann
Arbor, Michigan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In Proc. of the Meeting of Association for
Computational Linguistics (ACL), Portland, OR, USA.
Trond Grenager and Christoph Manning. 2006. Unsu-
pervised discovery of a statistical verb lexicon. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP).
Sonia Jain and Radford Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158?182.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Rochester, USA.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambigous supervision. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 895?900.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. Genia corpus?a semantically annotated
corpus for bio-textmining. Bioinformatics, 19:i180?
i182.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirich-
let processes. In Joint Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
688?697, Prague, Czech Republic.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proc. of the Annual Meeting of the Association
for Computational Linguistics and International Joint
Conference on Natural Language Processing (ACL-
IJCNLP).
Dekang Lin and Patrick Pantel. 2001. Dirt ? discovery
of inference rules from text. In Proc. of International
Conference on Knowledge Discovery and Data Min-
ing, pages 323?328.
1454
Raymond J. Mooney. 2007. Learning for semantic pars-
ing. In Proceedings of the 8th International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, pages 982?991.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
framenet-style semantic parsing: the role of coverage
gaps in framenet. In Proceedings of the Conference on
Computational Linguistics (COLING-2000), Beijing.
Jim Pitman. 2002. Poisson-dirichlet and gem invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11:501?514.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, (EMNLP-09).
Matt Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48rd
Annual Meeting of the Association for Computational
Linguistics (ACL), Uppsala, Sweden.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labelling. In Proceedings of EMNLP, pages
95?102, Barcelona, Spain.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566?1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985?
992.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role
labeling. In In Senseval-3, pages 397?408.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Upp-
sala, Sweden.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
B. Zapirain, E. Agirre, L. L. Ma`rquez, and M. Surdeanu.
2010. Improving semantic role classification with se-
lectional prefrences. In Proceedings of the Meeting
of the North American chapter of the Association for
Computational Linguistics (NAACL 2010), Los Ange-
les.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammar. In
Proceedings of the Twenty-first Conference on Uncer-
tainty in Artificial Intelligence, Edinburgh, UK, Au-
gust.
1455
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 647?656,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Crosslingual Induction of Semantic Roles
Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{titov|aklement}@mmci.uni-saarland.de
Abstract
We argue that multilingual parallel data pro-
vides a valuable source of indirect supervision
for induction of shallow semantic representa-
tions. Specifically, we consider unsupervised
induction of semantic roles from sentences an-
notated with automatically-predicted syntactic
dependency representations and use a state-
of-the-art generative Bayesian non-parametric
model. At inference time, instead of only
seeking the model which explains the mono-
lingual data available for each language, we
regularize the objective by introducing a soft
constraint penalizing for disagreement in ar-
gument labeling on aligned sentences. We
propose a simple approximate learning algo-
rithm for our set-up which results in efficient
inference. When applied to German-English
parallel data, our method obtains a substantial
improvement over a model trained without us-
ing the agreement signal, when both are tested
on non-parallel sentences.
1 Introduction
Learning in the context of multiple languages simul-
taneously has been shown to be beneficial to a num-
ber of NLP tasks from morphological analysis to
syntactic parsing (Kuhn, 2004; Snyder and Barzilay,
2010; McDonald et al, 2011). The goal of this work
is to show that parallel data is useful in unsupervised
induction of shallow semantic representations.
Semantic role labeling (SRL) (Gildea and Juraf-
sky, 2002) involves predicting predicate argument
structure, i.e. both the identification of arguments
and their assignment to underlying semantic roles.
For example, in the following sentences:
(a) [A0Peter] blamed [A1Mary] [A2for planning a theft].
(b) [A0Peter] blamed [A2planning a theft] [A1on Mary].
(c) [A1Mary] was blamed [A2for planning a theft] [A0by
Peter]
the arguments ?Peter?, ?Mary?, and ?planning a theft?
of the predicate ?blame? take the agent (A0), patient
(A1) and reason (A2) roles, respectively. In this
work, we focus on predicting argument roles.
SRL representations have many potential appli-
cations in NLP and have recently been shown
to benefit question answering (Shen and Lapata,
2007; Kaisser and Webber, 2007), textual entailment
(Sammons et al, 2009), machine translation (Wu
and Fung, 2009; Liu and Gildea, 2010; Wu et al,
2011; Gao and Vogel, 2011), and dialogue systems
(Basili et al, 2009; van der Plas et al, 2011), among
others. Though syntactic representations are often
predictive of semantic roles (Levin, 1993), the inter-
face between syntactic and semantic representations
is far from trivial. Lack of simple deterministic rules
for mapping syntax to shallow semantics motivates
the use of statistical methods.
Most of the current statistical approaches to SRL
are supervised, requiring large quantities of human
annotated data to estimate model parameters. How-
ever, such resources are expensive to create and only
available for a small number of languages and do-
mains. Moreover, when moved to a new domain,
performance of these models tends to degrade sub-
stantially (Pradhan et al, 2008). Sparsity of anno-
tated data motivates the need to look to alternative
647
resources. In this work, we make use of unsuper-
vised data along with parallel texts and learn to in-
duce semantic structures in two languages simulta-
neously. As does most of the recent work on unsu-
pervised SRL, we assume that our data is annotated
with automatically-predicted syntactic dependency
parses and aim to induce a model of linking between
syntax and semantics in an unsupervised way.
We expect that both linguistic relatedness and
variability can serve to improve semantic parses in
individual languages: while the former can pro-
vide additional evidence, the latter can serve to re-
duce uncertainty in ambiguous cases. For example,
in our sentences (a) and (b) representing so-called
blame alternation (Levin, 1993), the same informa-
tion is conveyed in two different ways and a success-
ful model of semantic role labeling needs to learn
the corresponding linkings from the data. Induc-
ing them solely based on monolingual data, though
possible, may be tricky as selectional preferences
of the roles are not particularly restrictive; similar
restrictions for patient and agent roles may further
complicate the process. However, both sentences
(a) and (b) are likely to be translated in German
as ?[A0Peter] beschuldigte [A1Mary] [A2einen Dieb-
stahl zu planen]?. Maximizing agreement between
the roles predicted for both languages would pro-
vide a strong signal for inducing the proper linkings
in our examples.
In this work, we begin with a state-of-the-art
monolingual unsupervised Bayesian model (Titov
and Klementiev, 2012) and focus on improving its
performance in the crosslingual setting. It induces
a linking between syntax and semantics, encoded as
a clustering of syntactic signatures of predicate ar-
guments. The clustering implicitly defines the set of
permissible alternations. For predicates present in
both sides of a bitext, we guide models in both lan-
guages to prefer clusterings which maximize agree-
ment between predicate argument structures pre-
dicted for each aligned predicate pair. We experi-
mentally show the effectiveness of the crosslingual
learning on the English-German language pair.
Our model admits efficient inference: the estima-
tion time on CoNLL 2009 data (Hajic? et al, 2009)
and Europarl v.6 bitext (Koehn, 2005) does not ex-
ceed 5 hours on a single processor and the infer-
ence algorithm is highly parallelizable, reducing in-
ference time down to less than half an hour on mul-
tiple processors. This suggests that the models scale
to much larger corpora, which is an important prop-
erty for a successful unsupervised learning method,
as unlabeled data is abundant.
In summary, our contributions are as follows.
? This work is the first to consider the crosslin-
gual setting for unsupervised SRL.
? We propose a form of agreement penalty and
show its efficacy on English-German language
pair when used in conjunction with a state-of-
the-art non-parametric Bayesian model.
? We demonstrate that efficient approximate in-
ference is feasible in the multilingual setting.
The rest of the paper is organized as follows. Sec-
tion 2 begins with a definition of the crosslingual
semantic role induction task we address in this pa-
per. In Section 3, we describe the base monolingual
model, and in Section 4 we propose an extension for
the crosslingual setting. In Section 5, we describe
our inference procedure. Section 6 provides both
evaluation and analysis. Finally, additional related
work is presented in Section 7.
2 Problem Definition
As we mentioned in the introduction, in this work
we focus on the labeling stage of semantic role la-
beling. Identification, though an important prob-
lem, can be tackled with heuristics (Lang and Lap-
ata, 2011a; Grenager and Manning, 2006; de Marn-
effe et al, 2006) or potentially by using a supervised
classifier trained on a small amount of data.
Instead of assuming the availability of role an-
notated data, we rely only on automatically gener-
ated syntactic dependency graphs in both languages.
While we cannot expect that syntactic structure can
trivially map to a semantic representation1, we can
make use of syntactic cues. In the labeling stage,
semantic roles are represented by clusters of ar-
guments, and labeling a particular argument corre-
sponds to deciding on its role cluster. However, in-
stead of dealing with argument occurrences directly,
1Although it provides a strong baseline which is difficult to
beat (Grenager and Manning, 2006; Lang and Lapata, 2010;
Lang and Lapata, 2011a).
648
we represent them as predicate-specific syntactic
signatures, and refer to them as argument keys. This
representation aids our models in inducing high pu-
rity clusters (of argument keys) while reducing their
granularity. We follow (Lang and Lapata, 2011a)
and use the following syntactic features for English
to form the argument key representation:
? Active or passive verb voice (ACT/PASS).
? Arg. position relative to predicate (LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
In the example sentences in Section 1, the argu-
ment keys for candidate arguments Peter for sen-
tences (a) and (c) would be ACT:LEFT:SBJ and
PASS:RIGHT:LGS->by,2 respectively. While aim-
ing to increase the purity of argument key clusters,
this particular representation will not always pro-
duce a good match: e.g. planning a theft in sen-
tence (b) will have the same key as Mary in sen-
tence (a). Increasing the expressiveness of the ar-
gument key representation by using features of the
syntactic frame would enable us to distinguish that
pair of arguments. However, we keep this particular
representation, in part to compare with the previous
work. In German, we do not include the relative po-
sition features, because they are not very informative
due to variability in word order.
In sum, we treat the unsupervised semantic role
labeling task as clustering of argument keys. Thus,
argument occurrences in the corpus whose keys are
clustered together are assigned the same semantic
role. The objective of this work is to improve ar-
gument key clusterings by inducing them simulta-
neously in two languages.
3 Monolingual Model
In this section we describe one of the Bayesian mod-
els for semantic role induction proposed in (Titov
and Klementiev, 2012). Before describing our
method, we briefly introduce the central compo-
nents of the model: the Chinese Restaurant Pro-
cesses (CRPs) and Dirichlet Processes (DPs) (Fer-
guson, 1973; Pitman, 2002). For more details we
refer the reader to (Teh, 2007).
2LGS denotes a logical subject in a passive construction
(Surdeanu et al, 2008).
3.1 Chinese Restaurant Processes
CRPs define probability distributions over partitions
of a set of objects. An intuitive metaphor for de-
scribing CRPs is assignment of tables to restaurant
customers. Assume a restaurant with a sequence of
tables, and customers who walk into the restaurant
one at a time and choose a table to join. The first
customer to enter is assigned the first table. Sup-
pose that when a client number i enters the restau-
rant, i ? 1 customers are sitting at each of the k ?
(1, . . . ,K) tables occupied so far. The new cus-
tomer is then either seated at one of the K tables
with probability Nki?1+? , where Nk is the number of
customers already sitting at table k, or assigned to a
new table with the probability ?i?1+? , ? > 0.
If we continue and assume that for each table ev-
ery customer at a table orders the same meal, with
the meal for the table chosen from an arbitrary base
distributionH , then all ordered meals will constitute
a sample from the Dirichlet Process DP (?,H).
An important property of the non-parametric pro-
cesses is that a model designer does not need to spec-
ify the number of tables (i.e. clusters) a-priori as it
is induced automatically on the basis of the data and
also depending on the choice of the concentration
parameter ?. This property is crucial for our task,
as the intended number of roles cannot possibly be
specified for every predicate.
3.2 The Generative Story
In Section 2 we defined our task as clustering of ar-
gument keys, where each cluster corresponds to a
semantic role. If an argument key k is assigned to a
role r (k ? r), all of its occurrences are labeled r.
The Bayesian model encodes two common as-
sumptions about semantic roles. First, it enforces the
selectional restriction assumption: namely it stip-
ulates that the distribution over potential argument
fillers is sparse for every role, implying that ?peaky?
distributions of arguments for each role r are pre-
ferred to flat distributions. Second, each role nor-
mally appears at most once per predicate occur-
rence. The inference algorithm will search for a
clustering which meets the above requirements to
the maximal extent.
The model associates two distributions with each
predicate: one governs the selection of argument
649
fillers for each semantic role, and the other mod-
els (and penalizes) duplicate occurrence of roles.
Each predicate occurrence is generated indepen-
dently given these distributions. Let us describe the
model by first defining how the set of model param-
eters and an argument key clustering are drawn, and
then explaining the generation of individual predi-
cate and argument instances. The generative story is
formally presented in Figure 1.
For each predicate p, we start by generating a par-
tition of argument keys Bp with each subset r ?
Bp representing a single semantic role. The parti-
tions are drawn from CRP(?) independently for each
predicate. The crucial part of the model is the set of
selectional preference parameters ?p,r, the distribu-
tions of arguments x for each role r of predicate p.
We represent arguments by lemmas of their syntac-
tic heads.3
The preference for sparseness of the distributions
?p,r is encoded by drawing them from the DP prior
DP (?,H(A)) with a small concentration parameter
?, the base probability distribution H(A) is just the
normalized frequencies of arguments in the corpus.
The geometric distribution ?p,r is used to model the
number of times a role r appears with a given predi-
cate occurrence. The decision whether to generate at
least one role r is drawn from the uniform Bernoulli
distribution. If 0 is drawn then the semantic role is
not realized for the given occurrence, otherwise the
number of additional roles r is drawn from the ge-
ometric distribution Geom(?p,r). The Beta priors
over ? can indicate the preference towards generat-
ing at most one argument for each role.
Now, when parameters and argument key clus-
terings are chosen, we can summarize the remain-
der of the generative story as follows. We begin by
independently drawing occurrences for each predi-
cate. For each predicate role we independently de-
cide on the number of role occurrences. Then each
of the arguments is generated (see GenArgument)
by choosing an argument key kp,r uniformly from
the set of argument keys assigned to the cluster r,
and finally choosing its filler xp,r, where the filler is
the lemma of the syntactic head of the argument.
3For prepositional phrases, the head noun of the object noun
phrase is taken as it encodes crucial lexical information. How-
ever, the preposition is not ignored but rather encoded in the
corresponding argument key, as explained in Section 2.
Clustering of argument keys:
for each predicate p = 1, 2, . . . :
Bp ? CRP (?) [partition of arg keys]
Parameters:
for each predicate p = 1, 2, . . . :
for each role r ? Bp:
?p,r ? DP (?,H(A)) [distrib of arg fillers]
?p,r ? Beta(?0, ?1) [geom distr for dup roles]
Data generation:
for each predicate p = 1, 2, . . . :
for each occurrence s of p:
for every role r ? Bp:
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(p, r) [draw one arg]
while [n ? ?p,r] = 1: [continue generation]
GenArgument(p, r) [draw more args]
GenArgument(p, r):
kp,r ? Unif(1, . . . , |r|) [draw arg key]
xp,r ? ?p,r [draw arg filler]
Figure 1: The generative story for predicate-argument
structure.
4 Multilingual Extension
As we argued in Section 1, our goal is to penalize
for disagreement in semantic structures predicted for
each language on parallel data. In doing so, as in
much of previous work on unsupervised induction of
linguistic structures, we rely on automatically pro-
duced word alignments. In Section 6, we describe
how we use word alignment to decide if two argu-
ments are aligned; for now, we assume that (noisy)
argument alignments are given.
Intuitively, when two arguments are aligned in
parallel data, we expect them to be labeled with the
same semantic role in both languages. This corre-
spondence is simpler than the one expected in mul-
tilingual induction of syntax and morphology where
systematic but unknown relation between structures
in two language is normally assumed (e.g., (Snyder
et al, 2008)). A straightforward implementation of
this idea would require us to maintain one-to-one
mapping between semantic roles across languages.
Instead of assuming this correspondence, we penal-
ize for the lack of isomorphism between the sets of
roles in aligned predicates with the penalty depen-
dent on the degree of violation. This softer approach
650
is more appropriate in our setting, as individual ar-
gument keys do not always deterministically map to
gold standard roles4 and strict penalization would
result in the propagation of the corresponding over-
coarse clusters to the other language. Empirically,
we observed this phenomenon on the held-out set
with the increase of the penalty weight.
Encoding preference for the isomorphism directly
in the generative story is problematic: sparse Dirich-
let priors can be used in a fairly trivial way to encode
sparsity of the mapping in one direction or another
but not in both. Instead, we formalize this preference
with a penalty term similar to the expectation criteria
in KL-divergence form introduced in McCallum et
al. (2007). Specifically, we augment the joint proba-
bility with a penalty term computed on parallel data:
?
p(1), p(2)
(
? ?(1)
?
r(1)?B
p(1)
fr(1) argmax
r(2)?B
p(2)
log P? (r(2)|r(1))
??(2)
?
r(2)?B
p(2)
fr(2) argmax
r(1)?B
p(1)
log P? (r(1)|r(2))
)
,
where P? (r(l)|r(l
?)) is the proportion of times the role
r(l
?) of predicate p(l
?) in language l? is aligned to the
role r(l) of predicate p(l) in language l, and fr(l) is
the total number of times the role is aligned, ?(l) is a
non-negative constant. The rationale for introducing
the individual weighting fr(l) is two-fold. First, the
proportions P? (r(l)|r(l
?)) are more ?reliable? when
computed from larger counts. Second, more fre-
quent roles should have higher penalty as they com-
pete with the joint probability term, the likelihood
part of which scales linearly with role counts.
Space restrictions prevent us from discussing the
close relation between this penalty formulation and
the existing work on injecting prior and side infor-
mation in learning objectives in the form of con-
straints (McCallum et al, 2007; Ganchev et al,
2010; Chang et al, 2007).
In order to support efficient and parallelizable in-
ference, we simplify the above penalty by consider-
ing only disjoint pairs of predicates, instead of sum-
ming over all pairs p(1) and p(2). When choosing
4The average purity for argument keys with automatic argu-
ment identification and using predicted syntactic trees, before
any clustering, is approximately 90.2% on English and 87.8%
on German.
the pairs, we aim to cover the maximal number of
alignment counts so as to preserve as much informa-
tion from parallel corpora as possible. This objective
corresponds to the classic maximum weighted bipar-
tite matching problem with the weight for each edge
p(1) and p(2) equal to the number of times the two
predicates were aligned in parallel data. We use the
standard polynomial algorithm (the Hungarian algo-
rithm, (Kuhn, 1955)) to find an optimal solution.
5 Inference
An inference algorithm for an unsupervised model
should be efficient enough to handle vast amounts
of unlabeled data, as it can easily be obtained and is
likely to improve results. We use a simple approx-
imate inference algorithm based on greedy search.
We start by discussing search for the maximum a-
posteriori clustering of argument keys in the mono-
lingual set-up and then discuss how it can be ex-
tended to accommodate the role alignment penalty.
5.1 Monolingual Setting
In the model, a linking between syntax and seman-
tics is induced independently for each predicate.
Nevertheless, searching for a MAP clustering can
be expensive: even a move involving a single ar-
gument key implies some computations for all its
occurrences in the corpus. Instead of more com-
plex MAP search algorithms (see, e.g., (Daume III,
2007)), we use a greedy procedure where we start
with each argument key assigned to an individual
cluster, and then iteratively try to merge clusters.
Each move involves (1) choosing an argument key
and (2) deciding on a cluster to reassign it to. This is
done by considering all clusters (including creating
a new one) and choosing the most probable one.
Instead of choosing argument keys randomly at
the first stage, we order them by corpus frequency.
This ordering is beneficial as getting clustering right
for frequent argument keys is more important and
the corresponding decisions should be made earlier.5
We used a single iteration in our experiments, as we
have not noticed any benefit from using multiple it-
erations.
5This has been explored before for shallow semantic rep-
resentations (Lang and Lapata, 2011a; Titov and Klementiev,
2011).
651
5.2 Incorporating the Alignment Penalty
Inference in the monolingual setting is done inde-
pendently for each predicate, as the model factor-
izes over the predicates. The role alignment penalty
introduces interdependencies between the objectives
for each bilingual predicate pair chosen by the as-
signment algorithm as discussed in Section 4. For
each pair of predicates, we search for clusterings
to maximize the sum of the log-probability and the
negated penalty term.
At first glance it may seem that the alignment
penalty can be easily integrated into the greedy MAP
search algorithm: instead of considering individual
argument keys, one could use pairs of argument keys
and decide on their assignment to clusters jointly.
However, given that there is no isomorphic mapping
between argument keys across languages, this solu-
tion is unlikely to be satisfactory.6 Instead, we use
an approximate inference procedure similar in spirit
to annotation projection techniques.
For each predicate, we first induce semantic roles
independently for the first language, as described
in Section 5.1, and then use the same algorithm for
the second language but take the penalty term into
account. Then we repeat the process in the reverse
direction. Among these two solutions, we choose
the one which yields the higher objective value. In
this way, we begin with producing a clustering for
the side which is easier to cluster and provides more
clues for the other side.7
6 Empirical Evaluation
We begin by describing the data and evaluation met-
rics we use before discussing results.
6.1 Data
We run our main experiments on the English-
German section of Europarl v6 parallel corpus
6We also considered a variation of this idea where a pair of
argument keys is chosen randomly proportional to their align-
ment frequency and multiple iterations are repeated. Despite
being significantly slower than our method, it did not provide
any improvement in accuracy.
7In preliminary experiments, we studied an even simpler in-
ference method where the projection direction was fixed for all
predicates. Though this approach did outperform the monolin-
gual model, the results were substantially worse than achieved
with our method.
(Koehn, 2005) and the CoNLL 2009 distributions
of the Penn Treebank WSJ corpus (Marcus et al,
1993) for English and the SALSA corpus (Burchardt
et al, 2006) for German. As standard for unsuper-
vised SRL, we use the entire CoNLL training sets
for evaluation, and use held-out sets for model se-
lection and parameter tuning.
Syntactic annotation. Although the CoNLL 2009
dataset aleady has predicted dependency structures,
we could not reproduce them so that we could use
the same parser to annotate Europarl. We chose to
reannotate it, since using different parsing models
for both datasets would be undesirable. We used
MaltParser (Nivre et al, 2007) for English and the
syntactic component of the LTH system (Johansson
and Nugues, 2008) for German.
Predicate and argument identification. We select all
non-auxiliary verbs as predicates. For English, we
identify their arguments using a heuristic proposed
in (Lang and Lapata, 2011a). It is comprised of a
list of 8 rules, which use nonlexicalized properties
of syntactic paths between a predicate and a candi-
date argument to iteratively discard non-arguments
from the list of all words in a sentence. For Ger-
man, we use the LTH argument identification classi-
fier. Accuracy of argument identification on CoNLL
2009 using predicted syntactic analyses was 80.7%
and 86.5% for English and German, respectively.
Argument alignment. We use GIZA++ (Och and
Ney, 2003) to produce word alignments in Europarl:
we ran it in both directions and kept the intersec-
tion of the induced word alignments. For every ar-
gument identified in the previous stage, we chose a
set of words consisting of the argument?s syntactic
head and, for prepositional phrases, the head noun
of the object noun phrase. We mark arguments in
two languages as aligned if there is any word align-
ment between the corresponding sets and if they are
arguments of aligned predicates.
6.2 Evaluation Metrics
We use the standard purity (PU) and collocation
(CO) metrics as well as their harmonic mean (F1) to
measure the quality of the resulting clusters. Purity
measures the degree to which each cluster contains
arguments sharing the same gold role:
652
PU =
1
N
?
i
max
j
|Gj ? Ci|
where Ci is the set of arguments in the i-th induced
cluster, Gj is the set of arguments in the jth gold
cluster, and N is the total number of arguments.
Collocation evaluates the degree to which arguments
with the same gold roles are assigned to a single
cluster:
CO =
1
N
?
j
max
i
|Gj ? Ci|
We compute the aggregate PU, CO, and F1 scores
over all predicates in the same way as (Lang and La-
pata, 2011a) by weighting the scores of each pred-
icate by the number of its argument occurrences.
Since our goal is to evaluate the clustering algo-
rithms, we do not include incorrectly identified ar-
guments when computing these metrics.
6.3 Parameters and Set-up
Our models are robust to parameter settings; the pa-
rameters were tuned (to an order of magnitude) to
optimize the F1 score on the held-out development
set and were as follows. Parameters governing du-
plicate role generation, ?(?)0 and ?
(?)
1 , and penalty
weights ?(?) were set to be the same for both lan-
guages, and are 100, 1.e-3 and 10, respectively. The
concentration parameters were set as follows: for
English, they were set to ?(1) = 1.e-3, ?(1) = 1.e-3,
and, for German, they were ?(2) = 0.1, ?(2) = 1.
Domains of Europarl (parliamentary proceedings)
and German/English CoNLL data (newswire) are
substantially different. Since the influence of do-
main shift is not the focus of work, we try to min-
imize its effect by computing the likelihood part of
the objective on CoNLL data alone. This also makes
our setting more comparable to prior work.8
6.4 Results
Base monolingual model. We begin by evaluat-
ing our base monolingual model MonoBayes alone
against the current best approaches to unsupervised
semantic role induction. Since we do not have ac-
cess to the systems, we compare on the marginally
different English CoNLL 2008 (Surdeanu et al,
8Preliminary experiments on the entire dataset show a slight
degradation in performance.
PU CO F1
LLogistic 79.5 76.5 78.0
GraphPart 88.6 70.7 78.6
SplitMerge 88.7 73.0 80.1
MonoBayes 88.1 77.1 82.2
SyntF 81.6 77.5 79.5
Table 1: Argument clustering performance with gold
argument identification and gold syntactic parses on
CoNLL 2008 shared-task dataset. Bold-face is used to
highlight the best F1 scores.
2008) shared task dataset used in their experiments.
We report the results using gold argument identifi-
cation and gold syntactic parses in order to focus
the evaluation on the argument labeling stage and to
minimize the noise due to automatic syntactic anno-
tations. The methods are Latent Logistic classifica-
tion (Lang and Lapata, 2010), Split-Merge cluster-
ing (Lang and Lapata, 2011a), and Graph Partition-
ing (Lang and Lapata, 2011b) (labeled LLogistic,
SplitMerge, and GraphPart, respectively) achieving
the current best unsupervised SRL results in this set-
ting. Additionally, we compute the syntactic func-
tion baseline (SyntF), which simply clusters predi-
cate arguments according to the dependency relation
to their head. Following (Lang and Lapata, 2010),
we allocate a cluster for each of 20 most frequent
relations in the CoNLL dataset and one cluster for
all other relations. Our model substantially outper-
forms other models (see Table 1).
Multilingual extensions. Next, we improve our
model performance using agreement as an addi-
tional supervision signal during training (see Sec-
tion 4). We compare the performance of indi-
vidual English and German models induced sepa-
rately (MonoBayes) with the jointly induced mod-
els (MultiBayes) as well as the syntactic baseline,
see Table 2.9 While we see little improvement
in F1 for English, the German system improves
by 1.8%. For German, the crosslingual learning
also results in 1.5% improvement over the syntac-
tic baseline, which is considered difficult to outper-
form (Grenager and Manning, 2006; Lang and Lap-
ata, 2010). Note that recent unsupervised SRL meth-
9Note that the scores are computed on correctly identified ar-
guments only, and tend to be higher in these experiments prob-
ably because the complex arguments get discarded by the argu-
ment identifier.
653
English German
PU CO F1 PU CO F1
MonoBayes 87.5 80.1 83.6 86.8 75.7 80.9
MultiBayes 86.8 80.7 83.7 85.0 80.6 82.7
SyntF 81.5 79.4 80.4 83.1 79.3 81.2
Table 2: Results on CoNLL 2009 with automatic argu-
ment identification and automatic syntactic parses.
ods do not always improve on it, see Table 1.
The relatively low expressivity and limited purity
of our argument keys (see discussion in Section 4)
are likely to limit potential improvements when us-
ing them in crosslingual learning. The natural next
step would be to consider crosslingual learning with
a more expressive model of the syntactic frame and
syntax-semantics linking.
7 Related Work
Unsupervised learning in crosslingual setting has
been an active area of research in recent years. How-
ever, most of this research has focused on induc-
tion of syntactic structures (Kuhn, 2004; Snyder
et al, 2009) or morphologic analysis (Snyder and
Barzilay, 2008) and we are not aware of any pre-
vious work on induction of semantic representa-
tions in the crosslingual setting. Learning of se-
mantic representations in the context of monolin-
gual weakly-parallel data was studied in Titov and
Kozhevnikov (2010) but their setting was semi-
supervised and they experimented only on a re-
stricted domain.
Most of the SRL research has focused on the
supervised setting, however, lack of annotated re-
sources for most languages and insufficient cover-
age provided by the existing resources motivates
the need for using unlabeled data or other forms
of weak supervision. This includes methods based
on graph alignment between labeled and unlabeled
data (Fu?rstenau and Lapata, 2009), using unlabeled
data to improve lexical generalization (Deschacht
and Moens, 2009), and projection of annotation
across languages (Pado and Lapata, 2009; van der
Plas et al, 2011). Semi-supervised and weakly-
supervised techniques have also been explored for
other types of semantic representations but these
studies again have mostly focused on restricted do-
mains (Kate and Mooney, 2007; Liang et al, 2009;
Goldwasser et al, 2011; Liang et al, 2011).
Early unsupervised approaches to the SRL task
include (Swier and Stevenson, 2004), where the
VerbNet verb lexicon was used to guide unsuper-
vised learning, and a generative model of Grenager
and Manning (2006) which exploits linguistic priors
on syntactic-semantic interface.
More recently, the role induction problem has
been studied in Lang and Lapata (2010) where it
has been reformulated as a problem of detecting al-
ternations and mapping non-standard linkings to the
canonical ones. Later, Lang and Lapata (2011a) pro-
posed an algorithmic approach to clustering argu-
ment signatures which achieves higher accuracy and
outperforms the syntactic baseline. In Lang and La-
pata (2011b), the role induction problem is formu-
lated as a graph partitioning problem: each vertex in
the graph corresponds to a predicate occurrence and
edges represent lexical and syntactic similarities be-
tween the occurrences. Unsupervised induction of
semantics has also been studied in Poon and Domin-
gos (2009) and Titov and Klementiev (2011) but the
induced representations are not entirely compatible
with the PropBank-style annotations and they have
been evaluated only on a question answering task
for the biomedical domain. Also, a related task of
unsupervised argument identification has been con-
sidered in Abend et al (2009).
8 Conclusions
This work adds unsupervised semantic role labeling
to the list of NLP tasks benefiting from the crosslin-
gual induction setting. We show that an agreement
signal extracted from parallel data provides indi-
rect supervision capable of substantially improving
a state-of-the-art model for semantic role induction.
Although in this work we focused primarily on
improving performance for each individual lan-
guage, cross-lingual semantic representation could
be extracted by a simple post-processing step. In
future work, we would like to model cross-lingual
semantics explicitly.
Acknowledgements
The work was supported by the MMCI Cluster of Excel-
lence and a Google research award. The authors thank
Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,
Caroline Sporleder and the anonymous reviewers for their
suggestions.
654
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In ACL-IJCNLP.
Roberto Basili, Diego De Cao, Danilo Croce, Bonaven-
tura Coppola, and Alessandro Moschitti. 2009. Cross-
language frame semantics transfer in bilingual cor-
pora. In CICLING.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2006. The SALSA corpus: a german
corpus resource for lexical semantics. In LREC.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In ACL.
Hal Daume III. 2007. Fast search for dirichlet process
mixture models. In AISTATS.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the La-
tent Words Language Model. In EMNLP.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. The Annals of Statis-
tics, 1(2):209?230.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
EMNLP.
Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research (JMLR), 11:2001?2049.
Qin Gao and Stephan Vogel. 2011. Corpus expansion for
statistical machine translation with semantic role label
substitution rules. In ACL:HLT.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised semantic
parsing. In ACL.
Trond Grenager and Christoph Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL 2009: Shared Task.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL Workshop
on Deep Linguistic Processing.
Rohit J. Kate and Raymond J. Mooney. 2007. Learning
language semantics from ambigous supervision. In
AAAI.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit.
Harold W. Kuhn. 1955. The hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:83?97.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In ACL.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In ACL-IJCNLP.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In ACL: HLT.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Coling.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum, Gideon Mann, and Gregory Druck.
2007. Generalized expectation criteria. Techni-
cal Report TR 2007-60, University of Massachusetts,
Amherst, MA.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In EMNLP-
CoNLL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
655
Sebastian Pado and Mirella Lapata. 2009. Cross-lingual
annotation projection for semantic roles. Journal of
Artificial Intelligence Research, 36:307?340.
Jim Pitman. 2002. Poisson-Dirichlet and GEM invari-
ant distributions for split-and-merge transformations
of an interval partition. Combinatorics, Probability
and Computing, 11:501?514.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Com-
putational Linguistics, 34:289?310.
M. Sammons, V. Vydiswaran, T. Vieira, N. Johri,
M. Chang, D. Goldwasser, V. Srikumar, G. Kundu,
Y. Tu, K. Small, J. Rule, Q. Do, and D. Roth. 2009.
Relation alignment for textual entailment recognition.
In Text Analysis Conference (TAC).
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In ACL.
Benjamin Snyder and Regina Barzilay. 2010. Climbing
the tower of Babel: Unsupervised multilingual learn-
ing. In ICML.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In ACL.
Mihai Surdeanu, Adam Meyers Richard Johansson, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP.
Yee Whye Teh. 2007. Dirichlet process. Encyclopedia
of Machine Learning.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
model for unsupervised semantic parsing. In ACL.
Ivan Titov and Alexandre Klementiev. 2012. A Bayesian
approach to unsupervised semantic role induction. In
EACL.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In ACL.
Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual seman-
tic role annotation. In ACL.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: A hybrid two-pass model. In NAACL.
Dekai Wu, Marianna Apidianaki, Marine Carpuat, and
Lucia Specia, editors. 2011. Proc. of Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation. ACL.
656
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 108?113,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Mechanical Turk to Annotate Lexicons for
Less Commonly Used Languages
Ann Irvine and Alexandre Klementiev
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218
{anni,aklement}@jhu.edu
Abstract
In this work we present results from using
Amazon?s Mechanical Turk (MTurk) to an-
notate translation lexicons between English
and a large set of less commonly used lan-
guages. We generate candidate translations for
100 English words in each of 42 foreign lan-
guages using Wikipedia and a lexicon induc-
tion framework. We evaluate the MTurk an-
notations by using positive and negative con-
trol candidate translations. Additionally, we
evaluate the annotations by adding pairs to our
seed dictionaries, providing a feedback loop
into the induction system. MTurk workers are
more successful in annotating some languages
than others and are not evenly distributed
around the world or among the world?s lan-
guages. However, in general, we find that
MTurk is a valuable resource for gathering
cheap and simple annotations for most of the
languages that we explored, and these anno-
tations provide useful feedback in building a
larger, more accurate lexicon.
1 Introduction
In this work, we make use of several free and cheap
resources to create high quality lexicons for less
commonly used languages. First, we take advan-
tage of small existing dictionaries and freely avail-
able Wikipedia monolingual data to induce addi-
tional lexical translation pairs. Then, we pay Me-
chanical Turk workers a small amount to check and
correct our system output. We can then use the up-
dated lexicons to inform another iteration of lexicon
induction, gather a second set of MTurk annotations,
and so on.
Here, we provide results of one iteration of MTurk
annotation. We discuss the feasibility of using
MTurk for annotating translation lexicons between
English and 42 less commonly used languages. Our
primary goal is to enlarge and enrich the small,
noisy bilingual dictionaries that we have for each
language. Our secondary goal is to study the quality
of annotations that we can expect to obtain for our
set of low resource languages. We evaluate the anno-
tations both alone and as feedback into our lexicon
induction system.
2 Inducing Translation Candidates
Various linguistic and corpus cues are helpful for re-
lating word translations across a pair of languages.
A plethora of prior work has exploited orthographic,
topic, and contextual similarity, to name a few
(Rapp, 1999; Fung and Yee, 1998; Koehn and
Knight, 2000; Mimno et al, 2009; Schafer and
Yarowsky, 2002; Haghighi et al, 2008; Garera et
al., 2008). In this work, our aim is to induce trans-
lation candidates for further MTurk annotation for a
large number of language pairs with varying degrees
of relatedness and resource availability. Therefore,
we opt for a simple and language agnostic approach
of using contextual information to score translations
and discover a set of candidates for further anno-
tation. Table 1 shows our 42 languages of interest
and the number of Wikipedia articles with interlin-
gual links to their English counterparts. The idea
is that tokens which tend to appear in the context
of a given type in one language should be similar
to contextual tokens of its translation in the other
language. Each word can thus be represented as a
108
Tigrinya 36 Punjabi 401
Kyrgyz 492 Somali 585
Nepali 1293 Tibetan 1358
Uighur 1814 Maltese 1896
Turkmen 3137 Kazakh 3470
Mongolian 4009 Tatar 4180
Kurdish 5059 Uzbek 5875
Kapampangan 6827 Urdu 7674
Irish 9859 Azeri 12568
Tamil 13470 Albanian 13714
Afrikaans 14315 Hindi 14824
Bangla 16026 Tagalog 17757
Latvian 22737 Bosnian 23144
Welsh 25292 Latin 31195
Basque 38594 Thai 40182
Farsi 58651 Bulgarian 68446
Serbian 71018 Indonesian 73962
Slovak 76421 Korean 84385
Turkish 86277 Ukrainan 91022
Romanian 97351 Russian 295944
Spanish 371130 Polish 438053
Table 1: Our 42 languages of interest and the number of
Wikipedia pages for each that have interlanguage links
with English.
vector of contextual word indices. Following Rapp
(1999), we use a small seed dictionary to project1
the contextual vector of a source word into the tar-
get language, and score its overlap with contextual
vectors of candidate translations, see Figure 1. Top
scoring target language words obtained in this man-
ner are used as candidate translations for MTurk an-
notation. While longer lists will increase the chance
of including correct translations and their morpho-
logical variants, they require more effort on the part
of annotators. To strike a reasonable balance, we ex-
tracted relatively short candidate lists, but allowed
MTurk users to type their own translations as well.
3 Mechanical Turk Task
Following previous work on posting NLP tasks on
MTurk (Snow et al, 2008; Callison-Burch, 2009),
we use the service to gather annotations for proposed
bilingual lexicon entries. For 32 of our 42 languages
of interest, we were able to induce lexical translation
1A simple string match is used for projection. While we
expect that more sophisticated approaches (e.g. exploiting mor-
phological analyses) are likely to help, we cannot assume that
such linguistic resources are available for our languages.
e
(1)
e
(2)
e
(3)
e
(K-1)
e
(K)
e
(i)
?
?
?
f 
(1)
f 
(2)
f 
(3)
f 
(N-1)
f 
(N)
?
?
?
dict.
project
?
?
?
?
?
?
?
?
f 
(1)
f 
(2)
f 
(N)
c
o
m
p
a
r
e
Figure 1: Lexicon induction using contextual informa-
tion. First, contextual vectors are projected using small
dictionaries and then they are compared with the target
language candidates.
candidates and post them on MTurk for annotation.
We do not have dictionaries for the remaining ten,
so, for those languages, we simply posted a set of
100 English words and asked workers for manual
translations. We had three distinct workers translate
each word.
For the 32 languages for which we proposed
translation candidates, we divided our set of 100
English words into sets of ten English words to be
completed within a single HIT. MTurk defines HIT
(Human Intelligence Task) as a self-contained unit
of work that requesters can post and pay workers a
small fee for completing. We requested that three
MTurk workers complete each of the ten HITs for
each language. For each English word within a HIT,
we posted ten candidate translations in the foreign
language and asked users to check the boxes beside
any and all of the words that were translations of the
English word. We paid workers $0.10 for complet-
ing each HIT. If our seed dictionary included an en-
try for a given English word, we included that in the
candidate list as a positive control. Additionally, we
included a random word in the foreign language as
a negative control. The remaining eight or nine can-
didate translations were proposed by our induction
system. We randomized the order in which the can-
didates appeared to workers and presented the words
as images rather than text to discourage copying and
pasting into online translation systems.
In addition to gathering annotations on candidate
109
!"#$%&'"$()#&*(%()+&
,-.%"$%&
/%0$+(%"&
1%2)#-"$%& 3-")&
4%(5$%&6%"78%#)+9&
/9$8$::$")+&
;(9)<&
Figure 2: Distribution of MTurk workers around the
world
translations, we gathered the following information
in each HIT:
? Manual translations of each English word, es-
pecially for the cases where none of our pro-
posed candidate translations were accurate
? Geographical locations via IP addresses
? How the HIT was completed: knowledge of the
languages, paper dictionary, online dictionary
? Whether the workers were native speakers of
each language (English and foreign), and for
how many years they have spoken each
4 Results
Figure 2 shows the percent of HITs that were com-
pleted in different countries. More than 60% of HITs
were completed by workers in India, more than half
of which were completed in the single city of Chen-
nai. Another 18% were completed in the United
States, and roughly 2% were completed in Romania,
Pakistan, Macedonia, Latvia, Bangladesh, and the
Philippines. Of all annotations, 54% reported that
the worker used knowledge of the two languages,
while 28% and 18% reported using paper and online
dictionaries, respectively, to complete the HITs.
Ninety-three MTurk workers completed at least
one of our HITs, and 53 completed at least two.
The average number of HITs completed per worker
was 12. One worker completed HITs for 17 differ-
ent languages, and nine workers completed HITs in
more than three languages. Of the ten prolific work-
ers, one was located in the United States, one in the
United Kingdom, and eight in India. Because we
posted each HIT three times, the minimum number
of workers per language was three. Exactly three
workers completed all ten HITs posted in the fol-
lowing languages: Kurdish, Maltese, Tatar, Kapam-
pangan, Uzbek, and Latvian. We found that the av-
erage number of workers per language was 5.2. Ten
distinct workers (identified with MTurk worker IDs)
completed Tamil HITs, and nine worked on the Farsi
HITs.
4.1 Completion Time
Figure 3 shows the time that it took for our HITs
for 37 languages to be completed on MTurk. The
HITs for the following languages were posted for a
week and were never completed: Tigrinya, Uighur,
Tibetan, Kyrgyz, and Kazakh. All five of the un-
completed HIT sets required typing annotations, a
more time consuming task than checking transla-
tion candidates. Not surprisingly, languages with
many speakers (Hindi, Spanish, and Russian) and
languages spoken in and near India (Hindi, Tamil,
Urdu) were completed very quickly. The languages
for which we posted a manual translation only HIT
are marked with a * in Figure 3. The HIT type does
not seem to have affected the completion time.
4.2 Annotation Quality
Lexicon Check Agreement. Figure 4 shows the
percent of positive control candidate translations
that were checked by the majority of workers (at
least two of three). The highest amounts of agree-
ment with the controls were for Spanish and Polish,
which indicates that those workers completed the
HITs more accurately than the workers who com-
pleted, for example, the Tatar and Thai HITs. How-
ever, as already mentioned, the seed dictionaries are
very noisy, so this finding may be confounded by
discrepancies in the quality of our dictionaries. The
noisy dictionaries also explain why agreement with
the positive controls is, in general, relatively low.
We also looked at the degree to which workers
agreed upon negative controls. The average per-
cent agreement between the (majority of) workers
and the negative controls over all 32 languages is
only 0.21%. The highest amount of agreement with
negative controls is for Kapampangan and Turkmen
(1.28% and 1.26%, respectively). These are two of
110
0	 ?
20	 ?
40	 ?
60	 ?
80	 ?
100	 ?
120	 ?
140	 ?
160	 ?
Hin
di*	 ?
Spa
nish
	 ?
Rus
sian
	 ?
Tam
il	 ?
Urd
u*	 ?
Tur
kish
	 ?
Rom
ania
n	 ?
Ukr
aina
n*	 ?
Latv
ian	 ? Poli
sh	 ?
Alb
ania
n	 ?
Afri
kaa
ns	 ?
Bul
gar
ian	 ? Fars
i	 ?
Ind
one
sian
	 ?
We
lsh	 ?
Slov
ak	 ?
Tag
alog
	 ?
Ma
ltes
e	 ?
Ser
bian
	 ?
Uzb
ek	 ? Tha
i	 ?
Bos
nian
	 ?
Kor
ean
	 ?
Nep
ali*
	 ?
Irish
	 ?
Mo
ngo
lian
*	 ?
Aze
ri	 ?
Pun
jabi
	 ?
Ban
gla	 ? Tata
r	 ?
Kap
am
pan
gan
	 ?
Kur
dish
	 ?
La?	 ?
Tur
kme
n	 ?
Som
ali	 ?
Bas
que
	 ?
Hou
rs	 ?
Time	 ?to	 ?complete	 ?HITs	 ?
Time	 ?to	 ?first	 ?hit	 ?
Figure 3: Number of hours HITs posted on MTurk before completion; division of the time between posting and the
completion of one HIT and the time between the completion of the first and last HIT shown. HITs that required lexical
translation only (not checking candidate translations) are marked with an *.
the languages for which there was little agreement
with the positive controls, substantiating our claim
that those HITs were completed less accurately than
for other languages.
Manual Translation Agreement. For each En-
glish word, we encouraged workers to manually pro-
vide one or more translations into the foreign lan-
guage. Figure 5 shows the percent of English words
for which the MTurk workers provided and agreed
upon at least one manual translation. We defined
agreement as exact string match between at least
two of three workers, which is a conservative mea-
sure, especially for morphologically rich languages.
As shown, there was a large amount of agreement
among the manual translations for Ukrainian, Farsi,
Thai, and Korean. The MTurk workers did not pro-
vide any manual translations at all for the following
languages: Somali, Kurdish, Turkmen, Uzbek, Ka-
pampangan, and Tatar.
It?s easy to speculate that, despite discouraging
the use of online dictionaries and translation systems
by presenting text as images, users reached this high
level of agreement for manual translations by using
the same online translation systems. However, we
searched for 20 of the 57 English words for which
the workers agreed upon a manually entered Russian
translation in Google translate, and we found that the
Russian translation was the top Google translation
for only 11 of the 20 English words. Six of the Rus-
sian words did not appear at all in the list of trans-
lations for the given English word. Thus, we con-
clude that, at least for some of our languages of in-
terest, MTurk workers did provide accurate, human-
generated lexical translations.
4.3 Using MTurk Annotations in Induction
To further test the usefulness of MTurk generated
bilingual lexicons, we supplemented our dictionar-
ies for each of the 37 languages for which we gath-
ered MTurk annotations with translation pairs that
workers agreed were good (both chosen from the
candidate set and manually translated). We com-
pared seed dictionaries of size 200 with those sup-
plemented with, on average, 69 translation pairs. We
found an average relative increase in accuracy of
our output candidate set (evaluated against complete
available dictionaries) of 53%. This improvement is
further evidence that we are able to gather high qual-
ity translations from MTurk, which can assist the
lexicon induction process. Additionally, this shows
that we could iteratively produce lexical translation
candidates and have MTurk workers annotate them,
supplementing the induction dictionaries over many
iterations. This framework would allow us to gener-
111
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
Spa
nis
h	 ?
Pol
ish
	 ?
Bos
nia
n	 ?
Rom
ani
an	 ? Iris
h	 ?
Bul
gar
ian
	 ?
Tag
alo
g	 ?
Rus
sian
	 ?
Afr
ika
ans
	 ?
We
lsh
	 ?
Tam
il	 ?
Ma
ltes
e	 ?
Tur
kish
	 ?
Lat
via
n	 ?
Pun
jab
i	 ?
Ind
one
sian
	 ?
Aze
ri	 ?
Slo
vak
	 ?
La?	 ?
Alb
ani
an	 ?
Uzb
ek	 ?
Bas
que
	 ?
Ser
bia
n	 ?
Tur
km
en	 ?
Kor
ean
	 ?
Ban
gla
	 ?
Som
ali	 ?
Kur
dis
h	 ?
Far
si	 ?
Kap
am
pan
gan
	 ?
Tat
ar	 ? Tha
i	 ?
Per
cen
t	 ?ag
ree
me
nt	 ?o
n	 ?	 ?	 ?
pos
i?v
e	 ?c
ont
rols
	 ?
Figure 4: Percent of positive control candidate translations for which two or three workers checked as accurate.
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
90	 ?
100	 ?
Ukr
ain
an*
	 ?
Far
si	 ? Tha
i	 ?
Kor
ean
	 ?
Iris
h	 ?
Ma
ltes
e	 ?
Alb
ani
an	 ?
Slo
vak
	 ?
Lat
via
n	 ?
Tur
kish
	 ?
Ser
bia
n	 ?
We
lsh
	 ?
Afr
ika
ans
	 ?
Ind
one
sian
	 ?
Hin
di*
	 ?
Pol
ish
	 ?
Rus
sian
	 ?
Urd
u*	 ?
Tag
alo
g	 ?
Rom
ani
an	 ?
Mo
ngo
lian
*	 ?
Spa
nis
h	 ?
Bul
gar
ian
	 ?
Bos
nia
n	 ?
Aze
ri	 ?
Ne
pal
i*	 ?
La?	 ? Tam
il	 ?
Pun
jab
i	 ?
Ban
gla
	 ?
Bas
que
	 ?
Som
ali	 ?
Kur
dis
h	 ?
Tur
km
en	 ?
Uzb
ek	 ?
Kap
am
pan
gan
	 ?
Tat
ar	 ?Per
cen
t	 ?of
	 ?En
glis
h	 ?w
ord
s	 ?w
ith
	 ?
agr
eed
	 ?up
on	 ?
	 ?ma
nua
l	 ?tr
ans
la?
ons
	 ?
Figure 5: Percent of 100 English words for which at least two of three MTurk workers provided at least one matching
manual translation; HITs that required lexical translation only (not checking candidate translations) are marked with
an *.
ate very large and high quality dictionaries starting
with a very small set of seed translation pairs.
5 Conclusion
The goal of this work was to use Amazon?s Mechan-
ical Turk to collect and evaluate the quality of trans-
lation lexicons for a large set of low resource lan-
guages. In order to make the annotation task easier
and maximize the amount of annotation given our
budget and time constraints, we used contextual sim-
ilarity along with small bilingual dictionaries to ex-
tract a set of translation candidates for MTurk anno-
tation. For ten of our languages without dictionaries,
we asked workers to type translations directly. We
were able to get complete annotations of both types
quickly for 37 of our languages. The other five lan-
guages required annotations of the latter type, which
may explain why they remained unfinished.
We used annotator agreement with positive and
negative controls to assess the quality of generated
lexicons and provide an indication of the relative
difficulty of obtaining high quality annotations for
each language. Not surprisingly, annotation agree-
ment tends to be low for those languages which are
especially low resource, as measured by the num-
ber of Wikipedia pages. Because there are relatively
few native speakers of these languages in the on-
line community, those HITs were likely completed
by non-native speakers. Finally, we demonstrated
that augmenting small seed dictionaries with the ob-
tained lexicons substantially impacts contextual lex-
icon induction with an average relative gain of 53%
in accuracy across languages.
In sum, we found that the iterative approach of au-
112
tomatically generating noisy annotation and asking
MTurk users to correct it to be an effective means of
obtaining supervision. Our manual annotation tasks
are simple and annotation can be obtained quickly
for a large number of low resource languages.
References
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazons mechan-
ical turk. In Proceedings of EMNLP.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of ACL, pages 414?420.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2008. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In Proceedings
of CoNLL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
AAAI.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of EMNLP.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of ACL, pages 519?526.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity mea-
sures and bridge languages. In Proceedings of CoNLL,
pages 146?152.
Rion Snow, Brendan OConnor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of EMNLP.
113
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1?7,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Induction of Frame-Semantic Representations
Ashutosh Modi Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{amodi|titov|aklement}@mmci.uni-saarland.de
Abstract
The frame-semantic parsing task is challeng-
ing for supervised techniques, even for those
few languages where relatively large amounts
of labeled data are available. In this prelim-
inary work, we consider unsupervised induc-
tion of frame-semantic representations. An
existing state-of-the-art Bayesian model for
PropBank-style unsupervised semantic role
induction (Titov and Klementiev, 2012) is ex-
tended to jointly induce semantic frames and
their roles. We evaluate the model perfor-
mance both quantitatively and qualitatively by
comparing the induced representation against
FrameNet annotations.
1 Introduction
Shallow representations of meaning, and semantic
role labels in particular, have a long history in lin-
guistics (Fillmore, 1968). In this paper we focus on
frame-semantic representations: a semantic frame is
a conceptual structure describing a situation (or an
entity) and its participants (or its properties). Par-
ticipants and properties are associated with seman-
tic roles (also called frame elements). For example,
following the FrameNet annotation guidelines (Rup-
penhofer et al, 2006), in the following sentences:
(a) [COOK Mary] cooks [FOOD the broccoli]
[CONTAINER in a small pan].
(b) Sautee [FOOD the onions] [MANNER gently ]
[TEMP SETTING on low heat].
the same semantic frame Apply Heat is evoked
by verbs cook and sautee, and roles COOK and
FOOD in the sentence (a) are filled by Mary and
the broccoli, respectively. Note that roles are spe-
cific to the frame, not to the individual lexical units
(verbs cook and sautee, in the example).1
Most approaches to predicting these representa-
tions, called semantic role labeling (SRL), have re-
lied on large annotated datasets (Gildea and Juraf-
sky, 2002; Carreras and Ma`rquez, 2005; Surdeanu
et al, 2008; Hajic? et al, 2009). By far, most of
this work has focused on PropBank-style represen-
tations (Palmer et al, 2005) where roles are defined
for each individual verb, or even individual senses of
a verb. The only exceptions are modifiers and roles
A0 and A1 which correspond to proto-agent (a doer,
or initiator of the action) and proto-patient (an af-
fected entity), respectively. However, the SRL task
is known to be especially hard for the FrameNet-
style representations for a number of reasons, in-
cluding, the lack of cross-frame correspondence for
most roles, fine-grain definitions of roles and frames
in FrameNet, and relatively small amounts of statis-
tically representative data (Erk and Pado, 2006; Das
et al, 2010; Palmer and Sporleder, 2010; Das and
Smith, 2011). Another reason for reduced interest in
predicting FrameNet representations is the lack of
annotated resources for most languages, with anno-
tated corpora available or being developed only for
English (Ruppenhofer et al, 2006), German (Bur-
chardt et al, 2006), Spanish (Subirats, 2009) and
Japanese (Ohara et al, 2004).
Due to scarcity of labeled data, purely unsuper-
vised set-ups recently started to receive considerable
attention (Swier and Stevenson, 2004; Grenager and
Manning, 2006; Lang and Lapata, 2010; Lang and
1More accurately, FrameNet distinguishes core and non-
core roles with non-core roles mostly corresponding to mod-
ifiers, e.g., MANNER in sentence (b). Non-core roles are
expected to generalize across frames.
1
cooks
Mary
the broccoli in a small  pan  
CONTAINER
COOK FOOD
Apply_Heat
Figure 1: An example of a semantic dependency graph.
Lapata, 2011a; Lang and Lapata, 2011b; Titov and
Klementiev, 2012). However, all these approaches
have focused on PropBank-style representations.
This may seem somewhat unnatural as FrameNet
representations, though arguably more powerful, are
harder to learn in the supervised setting, harder to
annotate, and annotated data is available for a con-
siderably fewer languages. This is the gap which we
address in this preliminary study.
More specifically, we extend an existing state-
of-the-art Bayesian model for unsupervised seman-
tic role labeling and apply it to support FrameNet-
style semantics. In other words, our method jointly
induces both frames and frame-specific semantic
roles. We experiment only with verbal predicates
and evaluate the performance of the model with re-
spect to some natural baselines. Though the scores
for frame induction are not high, we argue that this is
primarily due to very high granularity of FrameNet
frames which is hard to reproduce for unsupervised
systems, as the implicit supervision signal is not ca-
pable of providing these distinctions.
2 Task Definition
In this work, we use dependency representations
of frame semantics. Dependency representations
for SRL (Johansson and Nugues, 2008) were made
popular by CoNLL-2008 and CoNLL-2009 shared
tasks (Surdeanu et al, 2008; Hajic? et al, 2009), but
for English were limited to PropBank. Recently,
English FrameNet was also released in the depen-
dency format (Bauer et al, 2012). Instead of pre-
dicting argument spans, in dependency representa-
tion the goal is, roughly, to predict the syntactic head
of the argument. The semantic dependency repre-
sentation for sentence (a) is shown in Figure 1, la-
bels on edges denote roles and labels on words de-
note frames. Note that in practice the structures can
be more complex, as, for example, arguments can
evoke their own frames or the same arguments can
be shared by multiple predicates, as in right node
raising constructions.
The SRL task, or more specifically frame-
semantic parsing task consists, at least conceptually,
of four stages: (1) identification of frame-evoking
elements(FEE), (2) identification of arguments, (3)
frame labeling and (4) role labeling. In this work,
we focus only on the frame labeling and role label-
ing stages, relying on gold standard (i.e. the oracle)
for FEEs and role identification. In other words, our
goal is to label (or cluster) edges and nodes in the
dependency graph, Figure 1. Since we focus in this
study on verbal predicates only, the first stage would
be trivial and the second stage could be handled with
heuristics as in much of previous work on unsuper-
vised SRL (Lang and Lapata, 2011a; Titov and Kle-
mentiev, 2012).
Additionally to considering only verbal predi-
cates, we also assume that every verb belongs to
a single frame. This assumption, though restric-
tive, may be reasonable in practice as (a) the dis-
tributions across frames (i.e. senses) are gener-
ally highly skewed, (b) current state-of-the-art tech-
niques for word-sense induction hardly beat most-
frequent-sense baselines in accuracy metrics (Man-
andhar et al, 2010). This assumption, or its minor
relaxations, is relatively standard in work on unsu-
pervised semantic parsing tasks (Poon and Domin-
gos, 2009; Poon and Domingos, 2010; Titov and
Klementiev, 2011). From the modeling prospective,
there are no major obstacles to relaxing this assump-
tion, but it would lead to a major explosion of the
search space and, as a result, slow inference.
3 Model and Inference
We follow previous work on unsupervised seman-
tic role labeling (Lang and Lapata, 2011a; Titov
and Klementiev, 2012) and associate arguments with
their frame specific syntactic signatures which we
refer to as argument keys:
? Active or passive verb voice (ACT/PASS).
? Argument position relative to predicate
(LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
Semantic roles are then represented as clusters of
argument keys instead of individual argument occur-
rences. This representation aids our models in in-
ducing high purity clusters (of argument keys) while
2
reducing their granularity. Thus, if an argument key
k is assigned to a role r (k ? r), all of its occurrences
are labeled r.
3.1 A model for frame-semantic parsing
Our approach is similar to the models of Titov and
Klementiev (2012; 2011). Please, see Section 5 for
a discussion of the differences.
Our model encodes three assumptions about
frames and semantic roles. First, we assume that
the distribution of lexical units (verbal predicates)
is sparse for each semantic frame. Second, we en-
force the selectional restriction assumption: we as-
sume that the distribution over potential argument
fillers is sparse for every role, implying that ?peaky?
distributions of arguments for each role r are pre-
ferred to flat distributions. Third, each role normally
appears at most once per predicate occurrence. Our
inference will search for a frame and role clustering
which meets the above requirements to the maximal
extent.
Our model associates three distributions with each
frame. The first one (?) models the selection of lex-
ical units, the second (?) governs the selection of ar-
gument fillers for each semantic role, and the third
(?) models (and penalizes) duplicate occurrence of
roles. Each frame occurrence is generated indepen-
dently given these distributions. Let us describe the
model by first defining how the set of model param-
eters and an argument key clustering are drawn, and
then explaining the generation of individual frame
instances. The generative story is formally presented
in Figure 2.
For each frame, we begin by drawing a dis-
tribution of its lexical units from a DP prior
DP (?, H(P )) with a small concentration parame-
ter ?, and a base distribution H(P ), pre-computed as
normalized counts of all verbs in our dataset. Next,
we generate a partition of argument keys Bf from
CRP(?) with each subset r ? Bf representing a sin-
gle frame specific semantic role. The crucial part
of the model is the set of selectional preference pa-
rameters ?f,r, the distributions of arguments x for
each role r of frame f . We represent arguments by
lemmas of their syntactic heads.2 In order to encode
2For prepositional phrases, we take as head the head noun of
the object noun phrase as it encodes crucial lexical information.
However, the preposition is not ignored but rather encoded in
the assumption about sparseness of the distributions
?f,r, we draw them from the DP prior DP (?, H(A))
with a small concentration parameter ?, the base
probability distribution H(A) is just the normalized
frequencies of arguments in the corpus. Finally,
the geometric distribution ?f,r is used to model the
number of times a role r appears with a given frame
occurrence. The decision whether to generate at
least one role r is drawn from the uniform Bernoulli
distribution. If 0 is drawn then the semantic role is
not realized for the given occurrence, otherwise the
number of additional roles r is drawn from the ge-
ometric distribution Geom(?f,r). The Beta priors
over ? indicate the preference towards generating at
most one argument for each role.
Now, when parameters and argument key cluster-
ings are chosen, we can summarize the remainder of
the generative story as follows. We begin by inde-
pendently drawing occurrences for each frame. For
each frame occurrence, we first draw its lexical unit.
Then for each role we independently decide on the
number of role occurrences. Then we generate each
of the arguments (seeGenArgument in Figure 2) by
generating an argument key kf,r uniformly from the
set of argument keys assigned to the cluster r, and fi-
nally choosing its filler xf,r, where the filler is either
a lemma or the syntactic head of the argument.
3.2 Inference
We use a simple approximate inference algo-
rithm based on greedy search for the maximum a-
posteriori clustering of lexical units and argument
keys. We begin by assigning each verbal predi-
cate to its own frame, and then iteratively choose
a pair of frames and merge them. Note that each
merge involves inducing a new set of roles, i.e. a
re-clustering of argument keys, for the new merged
frame. We use the search procedure proposed in
(Titov and Klementiev, 2012), in order to cluster ar-
gument keys for each frame.
Our search procedure chooses a pair of frames to
merge based on the largest incremental change to the
objective due to the merge. Computing the change
involves re-clustering of argument keys, so consider-
ing all pairs of initial frames containing single verbal
predicates is computationally expensive. Instead, we
the corresponding argument key.
3
Parameters:
for each frame f = 1, 2, . . . :
?f ? DP (?, H(P )) [distrib of lexical units]
Bf ? CRP (?) [partition of arg keys]
for each role r ? Bf :
?f,r ? DP (?, H(A)) [distrib of arg fillers]
?f,r ? Beta(?0, ?1) [geom distr for dup roles]
Data Generation:
for each frame f = 1, 2, . . . :
for each occurrence of frame f :
p ? ?f [draw a lexical unit]
for every role r ? Bf :
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(f, r) [draw one arg]
while [n ? ?f,r] = 1: [continue generation]
GenArgument(f, r) [draw more args]
GenArgument(f, r):
kf,r ? Unif(1, . . . , |r|) [draw arg key]
xf,r ? ?f,r [draw arg filler]
Figure 2: Generative story for the frame-semantic parsing
model.
prune the space of possible pairs of verbs using a
simple but effective pre-processing step. Each verb
is associated with a vector of normalized aggregate
corpus counts of syntactic dependents of the verb
(ignoring the type of dependency relation). Cosine
similarity of these vectors are then used to prune the
pairs of verbs so that only verbs which are distribu-
tionally similar enough are considered for a merge.
Finally, the search terminates when no additional
merges result in a positive change to the objective.
4 Experimental Evaluation
4.1 Data
We used the dependency representation of the
FrameNet corpus (Bauer et al, 2012). The corpus is
automatically annotated with syntactic dependency
trees produced by the Stanford parser. The data con-
sists of 158,048 sentences with 3,474 unique verbal
predicates and 722 gold frames.
4.2 Evaluation Metrics
We cannot use supervised metrics to evaluate our
models, since we do not have an alignment between
gold labels and clusters induced in the unsupervised
setup. Instead, we use the standard purity (PU) and
collocation (CO) metrics as well as their harmonic
mean (F1) to measure the quality of the resulting
clusters. Purity measures the degree to which each
cluster contains arguments (verbs) sharing the same
gold role (gold frame) and collocation evaluates the
degree to which arguments (verbs) with the same
gold roles (gold frame) are assigned to a single clus-
ter, see (Lang and Lapata, 2010). As in previous
work, for role induction, the scores are first com-
puted for individual predicates and then averaged
with the weights proportional to the total number oc-
currences of roles for each predicate.
4.3 Model Parameters
The model parameters were tuned coarsely by visual
inspection: ? = 1.e-5, ? = 1.e-4, ? = 1, ?0 = 100,
?1 = 1.e-10. Only a single model was evaluated
quantitatively to avoid overfitting to the evaluation
set.
4.4 Qualitative Evaluation
Our model induced 128 multi-verb frames from the
dataset. Out of 78,039 predicate occurrences in the
data, these correspond to 18,963 verb occurrences
(or, approximately, 25%). Some examples of the
induced multi-verb frames are shown in Table 1.
As we can observe from the table, our model clus-
ters semantically related verbs into a single frame,
even though they may not correspond to the same
gold frame in FrameNet. Consider, for example, the
frame (ratify::sign::accede): the verbs are semanti-
cally related and hence they should go into a single
frame, as they all denote a similar action.
Another result worth noting is that the model of-
ten clusters antonyms together as they are often used
in similar context. For example, consider the frame
(cool::heat::warm), the verbs cool, heat and warm,
all denote a change in temperature. This agrees well
with annotation in FrameNet. Similarly, we clus-
ter sell and purchase together. This contrasts with
FrameNet annotation as FrameNet treats them not
as antonyms but as different views on same situation
and according to their guidelines, different frames
are assigned to different views.
Often frames in FrameNet correspond to more
fine-grained meanings of the verbs, as we can see
in the example for (plait::braid::dye). The three de-
scribe a similar activity involving hair but FrameNet
4
Induced frames FrameNet frames corresponding to the verbs
(rush::dash::tiptoe) rush : [Self motion](150) [Fluidic motion](19)
dash : [Self motion](100)
tiptoe : [Self motion](114)
(ratify::sign::accede) ratify : [Ratification](41)
sign : [Sign agreement](81) [Hiring](18) [Text Creation](1)
accede : [Sign Agreement](31)
(crane::lean::bustle) crane : [Body movement](26)
lean: [Change posture](70) [Placing](22) [Posture](12)
bustle : [Self motion](55)
(cool::heat::warm) cool : [Cause temperature change](27)
heat: [Cause temperature change](52)
warm: [Cause temperature change](41) [Inchoative change of temperature](16)
(want::fib::dare) want : [Desiring](105) [Possession](44)
fib : [Prevarication](9)
dare : [Daring](21)
(encourage::intimidate::confuse) encourage : [Stimulus focus](49)
intimidate : [Stimulus focus](26)
confuse: [Stimulus focus](45)
(happen::transpire::teach) happen : [Event](38) [Coincidence](21) [Eventive affecting](1)
transpire : [Event](15)
teach : [Education teaching](7)
(do::understand::hope) do : [Intentionally affect](6) [Intentionally act](56)
understand : [Grasp](74) [Awareness](57) [Categorization](15)
hope : [Desiring](77)
(frighten::vary::reassure) frighten : [Emotion directed](44)
vary : [Diversity](24)
reassure : [Stimulus focus](35)
(plait::braid::dye) plait : [Hair configuration](11) [Grooming](12)
braid : [Hair configuration](7) [Clothing parts](6) [Rope manipulation](4)
dye : [Processing materials](18)
(sell::purchase) sell : [Commerce sell](107)
purchase : [Commerce buy](93)
(glisten::sparkle::gleam) glisten : [Location of light](52) [Light movement](1)
sparkle : [Location of light](23) [Light movement](3)
gleam : [Location of light](77) [Light movement](4)
(forestall::shush) forestall : [Thwarting](12)
shush : [Silencing](6)
Table 1: Examples of the induced multi-verb frames. The left column shows the induced verb clusters and the right
column lists the gold frames corresponding to each verb and the number in the parentheses are their occurrence counts.
gives them a finer distinction. Arguably, implicit su-
pervision signal present in the unlabeled data is not
sufficient to provide such fine-grained distinctions.
The model does not distinguish verb senses, i.e. it
always assigns a single frame to each verb, so there
is an upper bound on our clustering performance.
4.5 Quantitative Evaluation
Nowwe turn to quantitative evaluation of both frame
and role induction.
Frame Labeling. In this section, we evaluate how
well the induced frames correspond to the gold stan-
dard annotation. Because of the lack of relevant
previous work, we use only a trivial baseline which
places each verb in a separate cluster (NoCluster-
ing). The results are summarized in Table 3.
As we can see from the results, our model
achieves a small, but probably significant, improve-
ment in the F1-score. Though the scores are
fairly low, note that, as discussed in Section 4.4,
the model is severely penalized even for induc-
ing semantically plausible frames such as the frame
(plait::braid::dye).
Role Labeling. In this section, we evaluate how
well the induced roles correspond to the gold stan-
dard annotation. We use two baselines: one is
the syntactic baseline SyntF, which simply clus-
ters arguments according to the dependency rela-
5
PU CO F1
Our approach 78.9 71.0 74.8
NoFrameInduction 79.2 70.7 74.7
SyntF 69.9 73.3 71.6
Table 2: Role labeling performance.
tion to their head, as described in (Lang and La-
pata, 2010), and the other one is a version of our
model which does not attempt to cluster verbs and
only induces roles (NoFrameInduction). Note that
the NoFrameInduction baseline is equivalent to the
factoredmodel of Titov and Klementiev (2012). The
results are summarized in Table 2.
First, observe that both our full model and its sim-
plified version NoFrameInduction significantly out-
perform the syntactic baseline. It is important to
note that the syntactic baseline is not trivial to beat
in the unsupervised setting (Lang and Lapata, 2010).
Though there is a minor improvement from inducing
frames, it is small and may not be significant.3
Another observation is that the absolute scores
of all the systems, including the baselines, are sig-
nificantly below the results reported in Titov and
Klementiev (Titov and Klementiev, 2012) on the
CoNLL-08 version of PropBank in a comparable
setting (auto parses, gold argument identification):
73.9 % and 77.9 % F1 for SyntF and NoFrameIn-
duction, respectively. We believe that the main rea-
son for this discrepancy is the difference in the syn-
tactic representations. The CoNLL-08 dependencies
include function tags (e.g., TMP, LOC), and, there-
fore, modifiers do not need to be predicted, whereas
the Stanford syntactic dependencies do not provide
this information and the model needs to induce it.
It is clear from these results, and also from the
previous observation that only 25% of verb occur-
rences belong to multi-verb clusters, that the model
does not induce sufficiently rich clustering of verbs.
Arguably, this is largely due to the relatively small
size of FrameNet, as it may not provide enough evi-
dence for clustering. Given that our method is quite
efficient, a single experiment was taking around 8
hours on a single CPU, and the procedure is highly
parallelizable, the next step would be to use a much
larger and statistically representative corpus to in-
duce the representations.
3There is no well-established methodology for testing statis-
tical significance when comparing two clustering methods.
PU CO F1
Our approach 77.9 31.4 44.7
NoClustering 80.8 29.0 42.7
Table 3: Frame labeling performance.
Additional visual inspection suggest that the data
is quite noisy primarily due to mistakes in parsing.
The large proportion of mistakes can probably be ex-
plained by the domain shift: the parser is trained on
the WSJ newswire data and tested on more general
BNC texts.
5 Related Work
The space constraints do not permit us to pro-
vide a comprehensive overview of related work.
Aside from the original model of Titov and Klemen-
tiev (2012), the most related previous method is the
Bayesian method of Titov and Klementiev (2011).
In that work, along with predicate-argument struc-
ture, they also induce clusterings of dependency
tree fragments (not necessarily verbs). However,
their approach uses a different model for argument
generation, a different inference procedure, and it
has only been applied and evaluated on biomedi-
cal data. The same shallow semantic parsing task
has also been considered in the work of Poon and
Domingos (2009; 2010), but using a MLN model
and, again, only on the biomedical domain. An-
other closely related vein of research is on semi-
supervised frame-semantic parsing (Fu?rstenau and
Lapata, 2009; Das and Smith, 2011).
6 Conclusions
This work is the first to consider the task of unsuper-
vised frame-semantic parsing. Though the quantita-
tive results are mixed, we showed that meaningful
semantic frames are induced. In the future work, we
intend to consider much larger corpora and to focus
on a more general set-up by relaxing the assumption
that frames are evoked only by verbal predicates.
Acknowledgements
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Caroline Sporleder, Alexis
Palmer and the anonymous reviewers for their sugges-
tions.
6
References
Daniel Bauer, Hagen Fu?rstenau, and Owen Rambow.
2012. The dependency-parsed framenet corpus. In
International conference on Language Resources and
Evaluation (LREC), Istanbul, Turkey.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2006. The SALSA corpus: a german
corpus resource for lexical semantics. In LREC.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In CoNLL.
D. Das and N.A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1435?1444. Associa-
tion for Computational Linguistics.
D. Das, N. Schneider, D. Chen, and N.A. Smith. 2010.
Probabilistic frame-semantic parsing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 948?956. Associa-
tion for Computational Linguistics.
K. Erk and S. Pado. 2006. Shalmaneser?a toolchain for
shallow semantic parsing. In Proceedings of LREC,
volume 6. Citeseer.
Charles J. Fillmore. 1968. The case for case. In Bach
E. and Harms R.T., editors, Universals in Linguistic
Theory, pages 1?88. Holt, Rinehart, andWinston, New
York.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
EMNLP.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Trond Grenager and Christoph Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning (CoNLL-2009), June 4-5.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction and disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation.
K.H. Ohara, S. Fujii, T. Ohori, R. Suzuki, H. Saito, and
S. Ishizaki. 2004. The japanese framenet project:
An introduction. In Proceedings of LREC-04 Satellite
Workshop Building Lexical Resources from Semanti-
cally Annotated Corpora(LREC 2004), pages 9?11.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
FrameNet-style semantic parsing: the role of coverage
gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
H. Poon and P. Domingos. 2010. Unsupervised ontol-
ogy induction from text. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 296?305. Association for Computa-
tional Linguistics.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. available at http:
//framenet.icsi.berkeley.edu/index.
php?option=com_wrapper&Itemid=126.
C. Subirats. 2009. Spanish framenet: A frame-semantic
analysis of the spanish lexicon. Berlin/New York:
Mouton de Gruyter, pages 135?162.
Mihai Surdeanu, Adam Meyers Richard Johansson, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
model for unsupervised semantic parsing. In ACL.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to semantic role induction. In Proc. EACL,
Avignon, France.
7
