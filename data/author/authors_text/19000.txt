Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1274?1283,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Pattern Dictionary of English Prepositions 
 
 
Ken Litkowski 
CL Research 
9208 Gue Road 
Damascus, MD 20872 USA 
ken@clres.com 
 
  
 
Abstract 
We present a new lexical resource for the 
study of preposition behavior, the Pattern 
Dictionary of English Prepositions (PDEP). 
This dictionary, which follows principles laid 
out in Hanks? theory of norms and exploita-
tions, is linked to 81,509 sentences for 304 
prepositions, which have been made available 
under The Preposition Project (TPP). Nota-
bly, 47,285 sentences, initially untagged, 
provide a representative sample of preposi-
tion use, unlike the tagged sentences used in 
previous studies. Each sentence has been 
parsed with a dependency parser and our sys-
tem has near-instantaneous access to features 
developed with this parser to explore and an-
notate properties of individual senses. The 
features make extensive use of WordNet. We 
have extended feature exploration to include 
lookup of FrameNet lexical units and 
VerbNet classes for use in characterizing 
preposition behavior. We have designed our 
system to allow public access to any of the 
data available in the system. 
 
1 Introduction 
Recent studies (Zapirain et al (2013); Srikumar 
and Roth (2011)) have shown the value of prepo-
sitional phrases in joint modeling with verbs for 
semantic role labeling. Although recent studies 
have shown improved preposition disambigua-
tion, they have received little systematic treat-
ment from a lexicographic perspective. Recently, 
a new corpus has been made available that prom-
ises to be much more representative of preposi-
tion behavior. Our initial examination of this 
corpus has suggested clear indications of senses 
previously overlooked and reduced prominence 
for senses thought to constitute a large role in 
preposition use. 
In section 2, we describe the interface to the 
Pattern Dictionary of English Prepositions 
(PDEP), identifying how we are building upon 
data developed in The Preposition Project (TPP) 
and investigating its sense inventory with corpo-
ra also made available under TPP. Section 3 de-
scribes the procedures for tagging a representa-
tive corpus drawn from the British National Cor-
pus, including some findings that have emerged 
in assessing previous studies of preposition dis-
ambiguation. Section 4 describes how we are 
able to investigate the relationship of WordNet, 
FrameNet, and VerbNet to this effort and how 
this examination of preposition behavior can be 
used in working with these resources. Section 5 
describes how we can use PDEP for the analysis 
of semantic role and semantic relation invento-
ries. Section 6 describes how we envision further 
developments of PDEP and how the data are 
available for further analysis. In section 7, we 
present our conclusions for PDEP. 
2 The Pattern Dictionary of English 
Prepositions 
Litkowski and Hargraves (2005) and Litkowski 
and Hargraves (2006) describe The Preposition 
Project (TPP) as an attempt to describe preposi-
tion behavior using a sense inventory made 
available for public use from the Oxford Dic-
tionary of English (Stevenson and Soanes, 2003) 
by tagging sentences drawn from FrameNet. In 
TPP, each sense was characterized with its com-
plement and attachment (or governor) properties, 
its class and semantic relation, substitutable 
prepositions, its syntactic positions, and any 
FrameNet frame and frame element usages 
(where available). The FrameNet sentences were 
sense-tagged using the sense inventory and were 
1274
later used as the basis for a preposition disam-
biguation task in SemEval 2007 (Litkowski and 
Hargraves, 2007). 
Initial results in SemEval achieved a best ac-
curacy of 69.3 percent (Ye and Baldwin, 2007). 
The data from SemEval has subsequently been 
used in several further investigations of preposi-
tion disambiguation. Most notably, Tratz (2011) 
achieved a result of 88.4 percent accuracy and 
Srikumar and Roth (2013) achieved a similar 
result. However, Litkowski (2013b) showed that 
these results did not extend to other corpora, 
concluding that the FrameNet-based corpus may 
not have been representative, with a reduction of 
accuracy to 39.4 percent using a corpus devel-
oped by Oxford. 
Litkowski (2013a) announced the creation of 
the TPP corpora in order to develop a more rep-
resentative account of preposition behavior. The 
TPP corpora includes three subcorpora: (1) the 
full SemEval 2007 corpus (drawn from 
FrameNet data, henceforth FN), (2) sentences 
taken from the Oxford English Corpus to exem-
plify preposition senses in the Oxford Dictionary 
of English (henceforth, OEC), and (3) a sample 
of sentences drawn from the written portion of 
the British National Corpus (BNC), using the 
Word Sketch Engine as implemented in the sys-
tem for the Corpus Pattern Analysis of verbs 
(henceforth, CPA or TPP). 
We have used the TPP data and the TPP cor-
pora to implement an editorial interface, the Pat-
tern Dictionary of English Prepositions (PDEP).1 
This dictionary is intended to identify the proto-
typical syntagmatic patterns with which preposi-
tions in use are associated, identifying linguistic 
units used sequentially to make well-formed 
structures and characterizing the relationship be-
tween these units. In the case of prepositions, the 
units are the complement (object) of the preposi-
tion and the governor (point of attachment) of the 
prepositional phrase. The editorial interface is 
used to make changes in the underlying data-
bases, as described in the following subsections. 
Editorial access to make changes is limited, but 
the system can be explored publicly and the un-
derlying data can be accessed publicly, either in 
its entirety or through publicly available scripts 
used in accessing the data during editorial opera-
tions. 
Standard dictionaries include definitions of 
prepositions, but only loosely characterize the 
syntagmatic patterns associated with each sense. 
                                                 
1 http://www.clres.com/db/TPPEditor.html 
PDEP takes this a step further, looking for proto-
typical sentence contexts to characterize the pat-
terns. PDEP is modeled on the principles of Cor-
pus Pattern Analysis (CPA), developed to char-
acterize syntagmatic patterns for verbs. 2  These 
principles are described more fully in Hanks 
(2013). Currently, CPA is being used in the pro-
ject Disambiguation of Verbs by Collocation to 
develop a Pattern Dictionary of English Verbs 
(PDEV). 3  PDEP is closely related to PDEV, 
since most syntagmatic patterns for prepositions 
are related to the main verb in a clause. PDEP is 
viewed as subordinate to PDEV, sufficiently so 
that PDEP employs significant portions of code 
being used in PDEV, with appropriate modifica-
tions as necessary to capture the syntagmatic pat-
terns for prepositions.4 
2.1 The Preposition Inventory 
After a start page for entry into PDEP, a table of 
all prepositions in the sense inventory is dis-
played. Figure 1 contains a truncated snapshot of 
this table. The table has a row for each of 304 
prepositions as identified in TPP. The second 
column indicates the number of patterns (senses) 
for each preposition. The next two columns show 
the number of TPP (CPA) instances that have 
been tagged and the total number of TPP in-
stances that have been obtained as the sample 
from the total number of instances in the BNC. 
 
 
Figure 1. Preposition Inventory 
 
Additional columns not shown in Figure 1 
show (1) the status of the analysis for the prepo-
sition, (2) the number of instances from 
FrameNet (i.e., FN Insts, as developed for 
SemEval 2007), and (3) the number of instances 
from the Oxford English Corpus (i.e., OEC 
Insts). The number of prepositions with 
                                                 
2 See http://nlp.fi.muni.cz/projects/cpa/. 
3 See http://clg.wlv.ac.uk/projects/DVC 
4 PDEP is implemented as a combination of HTML 
and Javascript. Within the Javascript code, calls are 
made to PHP scripts to retrieve data from MySQL 
database tables and from additional files (described 
below). 
1275
FrameNet instances is 57 (larger than the 34 
prepositions used in SemEval). There are no 
OEC instances for 57 prepositions. There are no 
TPP instances for 41 prepositions. Notwithstand-
ing the lack of instances, there are TPP charac-
terizations for all 304 prepositions. 
The BNC frequency shown in Figure 1 pro-
vides a basis for extrapolating results from PDEP 
to the totality of prepositions. In total, the num-
ber of instances in the BNC is 5,391,042, which 
can be used as the denominator when examining 
the relative frequency of any preposition (e.g., 
between has a frequency of 0.0109, 
58,865/5,391,042).5 
In general, the target sample size was 250 
CPA instances. If the number available was less 
than 250, all instances were used. The TPP CPA 
corpus contains 250 instances for 170 preposi-
tions. Where the number of senses for a preposi-
tion was large (about 15 or more), larger samples 
of 750 (of, to, on, and with) or 500 (in, for, by, 
from, at, into, over, like, and through) were 
drawn. 
2.2 Preposition Patterns 
When a row in Figure 1 is clicked, the preposi-
tion is selected and a new page is opened to show 
the patterns for that preposition. Figure 2 shows 
the four patterns for below. Each pattern is pre-
sented as an instance of the template [[Gover-
nor]] prep [[Complement]], followed by its 
primary implicature, where the current definition 
is substituted for the preposition. 
 
 
Figure 2. Preposition Pattern List 
The display in Figure 2 provides an overview 
for each preposition, with the top line showing 
the number of tagged instances available from 
                                                 
5 The total number of instances for of and in in this 
estimate is 1,000,000. As a result, the relative fre-
quency calculation should not be construed as com-
pletely accurate. 
each corpus. For the TPP instances, this identi-
fies the number of instances that have been 
tagged and the number that remain to be tagged. 
In the body of the table, the first column shows 
the TPP sense number. The next three columns 
show the number of instances that have been 
tagged with this sense. Note that the top line of 
the pattern list includes a menu option for adding 
a pattern, for the case when we find that a new 
sense is required by the corpus evidence. 
Clicking on any row in the pattern list opens 
the details for that pattern, with a pattern box 
entitled with the preposition and the pattern 
number, as shown in Figure 3. The pattern box 
contains data developed in TPP and several new 
fields intended to capture our enhancements. 
TPP data include the fields for the Comple-
ment, the Governor, the TPP Class, the TPP 
Relation, the Substitutable Prepositions, the 
Syntactic Position, the Quirk Reference, the 
Sense Relation, and the Comment. We have 
added the checkboxes for complement type 
(common nouns, proper nouns, WH-phrases, and 
-ing phrases), as well as a field to identify a par-
ticular lexical item (lexset) if the sense is an idi-
omatic usage. We have added the Selector fields 
for the complement and the governor. For the 
complement, we have a field Category to hold 
its ontological category (using the shallow ontol-
ogy being developed for verbs in the DVC pro-
ject mentioned above).6 We also provided a field 
for the Semantic Class of the governor; this field 
has not yet been implemented. 
We have added two Cluster/Relation fields. 
The Cluster field is based on data available from 
Tratz (2011), where senses in the SemEval 2007 
data have been put into 34 clusters. The Relation 
field is based on data available from Srikumar 
and Roth (2013), where senses in the SemEval 
2007 data have been put into 32 classes. A key 
element of Srikumar and Roth was the use of 
these classes to model semantic relations across 
prepositions (e.g., grouping all the Temporal 
senses of the SemEval prepositions). In the pat-
tern box, each of these two fields has a drop-
down list of the clusters and relations, enabling 
us to categorize the senses of other prepositions 
with these classes. Below, we describe how we 
are able to use the TPP classes and relations 
along with the Tratz clusters and Srikumar rela-
tions in an analysis of these classes across the 
                                                 
6 This ontology is an evolution of the Brandeis Se-
mantic Ontology (Pustejovsky et al, 2006). 
1276
full set of prepositions, instead of just those used 
in SemEval. 
Any number of pattern boxes may be opened 
at one time. The data in any of the fields may be 
altered (with the menu bar changing color to red) 
and then saved to the underlying databases. An 
individual pattern box may then be closed. 
The drop-down box labeled Corpus Instances 
in the menu bar is used to open the set of corpus 
instances for the given sense. As shown in Figure 
2, this sense has 6 FN instances, 20 OEC in-
stances, and 15 TPP instances. The drop-down 
box has an option for each of these sets, along 
with an option for all TPP instances that have not 
yet been tagged. When one of these options is 
selected, the corresponding set of instances is 
opened in a new tab, discussed in the next sec-
tion. 
2.3 Preposition Corpus Instances 
As indicated, selecting an instance set from the 
pattern box opens this set in a separate tab, as 
shown in Figure 4. This tab, labeled Annotation: 
below (3(1b)), identifies the preposition and the 
sense, if any, associated with the instance set (the 
sense will be identified as unk if the set has not 
yet been tagged. The instance set is displayed, 
identifying the corpus, the instance identifier, the 
TPP sense (if identified, or ?unk? if not), the lo-
cation in the sentence of the target preposition, 
and the sentence, with the preposition in bold. 
This tab is where the annotation takes place. 
Any set of sentences may be selected; each se-
lected sentence is highlighted in yellow (as 
shown in Figure 6). The sense value may be 
changed using the drop-down box labeled Tag 
Instances in the menu bar. This drop-down box 
contains all the current senses for the preposition, 
along with possible tags x (to indicate that the 
instance is invalid for the preposition) and unk 
(to indicate that a tagging decision has not yet 
been made). The sense tags in Figure 4 were 
originally untagged in the CPA (TPP) corpus and 
were tagged in this manner. 
In general, sense-tagging follows standard lex-
icographic principles, where an attempt is made 
to group instances that appear to represent dis-
tinct senses. PDEP provides an enhanced envi-
ronment for this process. Firstly, we can make 
use of the current TPP sense inventory to tag 
sentences. Since the pattern sets (definitions) are 
based on the Oxford Dictionary of English, the 
likelihood that the coverage and accuracy of the 
sense distinctions is quite high. However, since 
prepositions have not generally received the 
close attention of words in other parts of speech, 
Figure 3. Preposition Pattern Details 
Figure 4. Preposition Corpus Instance Annotation 
1277
PDEP is intended to ensure the coverage and ac-
curacy. During the tagging of the SemEval in-
stances, the lexicographer found it necessary to 
increase the number of senses by about 10 per-
cent. Since the lack of coverage of FrameNet is 
well-recognized, the representative sample de-
veloped for the TPP corpus should provide the 
basis for ensuring the coverage and accuracy. 
In addition to adhering to standard lexico-
graphic principles, the availability of the tagged 
FN and OEC instances can be used as the basis 
for tagging decisions. Where available, these 
tagged instances can be opened in separate tabs 
and used as examples for tagging the unknown 
TPP instances. 
3 Tagging the TPP Corpus 
3.1 Examining Corpus Instances 
The main contribution of the present work is the 
ability to interactively examine characteristics of 
the context surrounding the target preposition in 
the corpus instances. In the menu bar shown in 
Figure 4, there is an Examine item. Next to it are 
two drop-down boxes, one labeled WFRs (word-
finding rules) and one labeled FERs (feature ex-
traction rules). These rules are taken from the 
system described in Tratz and Hovy (2011) and 
Tratz (2011). 7  The TPP corpora described in 
Litkowski (2013a) includes full dependency 
parses and feature files for all sentences. Each 
sentence may have as many as 1500 features de-
scribing the context of the target preposition. We 
have made the feature files for these sentences 
(1309 MB) available for exploration in PDEP. 
In our system, we make available seven word-
finding rules and nine feature extraction rules. 
The word-finding rules fall into two groups: 
words pertaining to the governor and words per-
taining to the complement. The five governor 
word-finding rules are (1) verb or head to the left 
(l), (2) head to the left (hl), (3) verb to the left 
(vl), (4) word to the left (wl), and (5) governor 
(h). The two complement word-finding rules are 
(1) syntactic preposition complement (c) and (2) 
heuristic preposition complement (hr). The fea-
ture extraction rules are (1) word class (wc), (2) 
part of speech (pos), (3) lemma (l), (4) word (w), 
(5) WordNet lexical name (ln), (6) WordNet 
synonyms (s), (7) WordNet hypernyms (h), (8) 
whether the word is capitalized (c), and (9) affix-
es (af). Thus, we are able to examine any of 63 
                                                 
7 An updated version of this system is available at 
http://sourceforge.net/projects/miacp/. 
WFR FER combinations for whatever corpus set 
happens to be open. 
In addition to these features, we are able to de-
termine the extent to which prepositions associ-
ated with FrameNet lexical units and VerbNet 
classes occur in a given corpus set. In Figure 4, 
there is a checkbox labeled FN next to the FERs 
drop-down list to examine FrameNet lexical 
units. There is a similar checkbox labeled VN to 
examine members of VerbNet classes. These 
boxes appear only when either of these resources 
has identified the given preposition as part of its 
frame (75 for FrameNet and 31 for VerbNet). 
When a particular WFR-FER combination is 
selected and the Examine menu item is clicked, 
a new tab is opened showing the values for those 
features for the given corpus set, as shown in 
Figure 5. The tab shows the WFR and FER that 
were used, the number of features for which the 
value was found in the feature data, the values, 
and the count for each feature. The description 
column is used when displaying results for the 
part of speech, the affix type, FrameNet frame 
elements, and VerbNet classes, since the value 
column for these hits are not self-explanatory. 
The example in Figure 5 is showing the lemma, 
which requires no further explanation. 
 
 
Figure 5. Feature Examination Results 
 
For most features (e.g., lemma or part of 
speech), the number of possible values is rela-
tively small, limited by the number of instances 
in the corpus set. For features such as the 
WordNet lexical name, synonyms and 
hypernyms, the number of values may be much 
larger. For FrameNet and VerbNet, the feature 
examination is limited to the combination of the 
WFR for the governor (h) and the FER lemma 
(l), both of which will generally identify verbs in 
the value column. 
The general objective of examining features is 
to identify those that are diagnostic of specific 
senses. When applied to the full untagged TPP 
corpus set, this process is akin to developing 
1278
word sketches for prepositions (Kilgarriff et al, 
2004). However, since we have tagged corpus 
sets for most preposition senses, we can begin 
our efforts looking at these sets. The hypothesis 
is that the tagged corpora will show patterns 
which can then be used for tagging instances in 
the TPP corpus.8 
The first step in examining features generally 
is to look at the word classes and parts of speech 
for the complement and the governor.9 These are 
useful for filling in their checkboxes in Figure 3. 
Another useful feature is word to the left (wl), 
which can be used to verify the syntactic position 
checkboxes, particularly the adverbial positions 
(adjunct, subjunct, disjunct, and conjunct). These 
first steps provide a general overview of a 
sense?s behavior. 
The next step of feature examination delves 
more into the semantic characteristics of the 
complement and the governor. Tratz (2011) re-
ported that the use of heuristics provided a more 
accurate identification of the preposition com-
plement; this is the WFR hr in our system. After 
getting some idea of the word class and the part 
of speech, we next examine the WordNet lexical 
name of the complement to determine its broad 
semantic grouping. As mentioned, this feature 
may return a number of values larger than the 
size of the corpus set, since WordNet senses for a 
given lexeme may be polysemous. Notwithstand-
ing, this feature examination generally shows the 
dominant categories and can be used to charac-
                                                 
8 Currently, 21.5 percent of the TPP instances (10347 
of 47,285) have been tagged. 
9 Accurate identification of the complement and gov-
ernor is likely improved with the reliance on the Tratz 
dependency parser. Moreover, this is likely to im-
prove the word sketches in PDEP. Ambati et al 
(2012) report that dependency parses provide im-
proved word sketches over purpose-built finite-state 
grammars. Their findings provide additional support 
for the methods presented here. 
terize and act as a selector for the complement in 
the pattern details. Similar procedures are used 
for characterizing the governor selection criteria. 
In the example in Figure 3, for below, sense 
3(1b), our preliminary analysis shows hr:pos:cd 
(i.e., a cardinal number) and hr:l:average, 
standard (i.e., the lemmas average and stand-
ard) are particularly useful for identifying this 
sense.  
3.2 Selecting Corpus Instances 
In addition to enabling feature examination, 
PDEP also facilitates selection of corpus instanc-
es. We can use the specifications for any WFR - 
FER combination, along with one of the values 
(as shown in Figure 5), to select the corpus in-
stances having that feature. Figure 6 shows, in 
part, the result of the WFR hr and FER l with the 
value average, against the instances in the open 
corpus set. 
As shown in the menu bar in Figure 6, we can 
select all instances and unselect all selections. 
Based on any selections, we can then tag such 
instances with one of the options that appear in 
the Tag Instances drop-down box. In the specif-
ic example, we could change all the selected in-
stances to some other sense, if we have decided 
that the current assignment is not the best. 
The selection mechanism is not used absolute-
ly. For example, in examining the untagged in-
stances for over, we used the specification 
hr:ln:noun.time (looking for instances with the 
heuristic complement having the WordNet lexi-
cal name noun.time). Out of 500 instances, we 
found 122 with this property. We then scrolled 
through the selected items, deselecting instances 
that did not provide a time period, and then 
tagged 99 instances with the sense 14(5), with 
the meaning expressing duration. Once we have 
made such a tagging, we can look at just those 
instances the next time we examine this sense. In 
this case, we might decide, pace the TPP lexicog-
rapher?s comment, that the instances should be 
Figure 6. Selected Corpus Instances 
1279
broken down into those which express a time 
period and those which describe ?accompanying 
circumstances? (e.g., over coffee). 
3.3 Accuracy of Features 
PDEP uses the output from Tratz? system (2011), 
which is of high quality, but which is not always 
correct. In addition, the TPP corpus also has 
some shortcomings, which are revealed in exam-
ining the instances. The TPP corpus has not been 
cleaned in the same manner as the FN and the 
OEC corpora. As a result, we see many cases 
which are more difficult to parse and hence, from 
which to generate feature sets. We believe this 
provides a truer real-world picture of the com-
plexities of preposition behavior. As a result, in 
the Tag Instances drop-down box, we have in-
cluded an option to tag a sentence as x, to indi-
cate that it is not a valid instance. 
A small percentage of the TPP instances are 
ill-formed, i.e., incomplete sentences; these are 
marked as x. For some prepositions, e.g., down, a 
substantial number of instances are not preposi-
tions, but rather adverbs or particles. For some 
phrasal prepositions, such as on the strength of, 
the phrase is literal, rather than the preposition 
idiom; in this case, 20 of 124 instances were 
marked as x. The occurrence of these invalid in-
stances provides an opportunity for improving 
taggers, parsers, and semantic role labelers. 
4 Assessment of Lexical Resources 
Since the PDEP system enables exploration of 
features from WordNet, FrameNet, and VerbNet, 
we are able to make some assessment of these 
resources. 
WordNet played a statistically significant role 
in the systems developed by Tratz (2011) and 
Srikumar and Roth (2013). This includes the 
WordNet lexicographer?s file name (e.g., 
noun.time), synsets, and hypernyms. We make 
extensive use of the file name, but less so from 
the synsets and hypernyms. However, in general, 
we find that the file names are too coarse-grained 
and the synsets and hypernyms too fine-grained 
for generalizations on the selectors for the com-
plements and the governors. The issue of granu-
larity also affects the use of the DVC ontology. 
We discuss this issue further in section 6, on in-
vestigations of suitable categorization schemes 
for PDEP. 
In using FrameNet, our results illustrate the 
unbalanced corpus used in SemEval 2007 (as 
suggested in Litkowski (2013b)). For the sense 
of of, ?used to indicate the contents of a contain-
er?, we first examined the FrameNet corpus set 
for that sense, which contains 278 instances (out 
of 4482, or 6.2 percent). Using PDEP, we found 
that FrameNet feature values for the governor 
accounted for 264 of these instances (95 per-
cent), all of which were related to the frame ele-
ments Contents or Stuff. However, in the TPP 
corpus, only 3 out of 750 instances were identi-
fied for this sense (0.4 percent). Thus, while 
FrameNet culled a large number of instances 
which had these frame element realizations, the-
se instances do not appear to be representative of 
their occurrence in a random sample of of uses. 
We have seen similar patterns for the other 
SemEval prepositions. 
A similar situation exists for Cause senses of 
major prepositions: for (385 in FrameNet, 5/500 
in TPP), from (71 in FrameNet, 16/500 in TPP), 
of (68 in FrameNet, 0/750 in TPP), and with (127 
in FrameNet, 8/750 in TPP). Each of these cases 
further emphasizes how the SemEval 2007 in-
stances are not representative and thus degrade 
the ability to apply existing preposition disam-
biguation results beyond these instances. )We 
discuss Cause senses further in the wider context 
of all PDEP prepositions in the next section on 
class analyses.) 
As indicated earlier, VerbNet identifies fewer 
prepositions in its frames than FrameNet. We 
believe this is the case since VerbNet preposi-
tions are generally arguments, rather than ad-
juncts. Many of the FrameNet prepositions are 
evoking peripheral and extra-thematic frame el-
ements, so the number of prepositions is corre-
spondingly higher. Also, VerbNet contains fewer 
members in its verb classes. As a result, the 
number of hits when using VerbNet is somewhat 
smaller, although some use of VerbNet classes is 
possible with the governor selectors. 
PDEP provides a vehicle for expanding the 
items in all these resources. While prepositions 
are not central to these resources, their support-
ing role provides additional information that 
might be useful in developing and using these 
other resources. 
5 Class Analyses 
In SemEval 2007, Yuret (2007) investigated the 
possibility of using the substitutable prepositions 
as the basis for disambiguation (as part of more 
general lexical sample substitution). Although 
his methodology yielded significant gains over 
the baseline, his best results were only 54.7 per-
1280
cent accuracy, concluding that preposition use is 
highly idiosyncratic. Srikumar and Roth (2013) 
broadened this perspective by considering a 
class-based approach by collapsing semantically-
related senses across prepositions, thereby deriv-
ing a semantic relation inventory. While their 
emphasis was on modeling semantic relations, 
they achieved an accuracy of 83.53 percent for 
preposition disambiguation. 
As mentioned above, PDEP has a field for the 
Srikumar semantic relation, initially populated 
for the SemEval prepositions, and being extend-
ed to cover all other prepositions. For example, 
Srikumar and Roth identified 21 temporal senses 
across 14 SemEval prepositions, while we have 
thus far identified 62 senses across 50 preposi-
tions. Similar increases in the sizes of other clas-
ses occur as well. For causal senses, Srikumar 
and Roth identified 11 senses over 7 preposi-
tions, while PDEP has 27 senses under 25 prepo-
sitions. 
PDEP enables an in-depth analysis of TPP 
classes, Tratz clusters, and Srikumar semantic 
realations. First, we query the database underly-
ing Figure 3 to identify all senses with a particu-
lar class. We then examine each sense on each 
list in detail. 
We follow the procedures laid out above for 
examining the features to add information about 
selectors, complement types, and categories. We 
use this information to tag the TPP instances, 
conservatively assuring the tagging, e.g., leaving 
untagged questionable instances. Finally, we 
carefully place each sense into a preposition 
class or subclass, grouping senses together and 
making annotations that attempt to capture any 
nuance of meaning that distinguishes the sense 
from other members of the class. 
To build a description of the class and its sub-
classes, we make use of the Quirk reference in 
Figure 3 (i.e., the relevant discussions in Quirk et 
al. (1985)). We build the description of a class as 
a separate web page and make this available as a 
menu item in Figure 3 (not shown for the Scalar 
class when that screenshot was made). The de-
scription provides an overview of the class, mak-
ing use of the TPP data and the Quirk discussion, 
and indicating the number of senses and the 
number of prepositions. Next, the description 
provides a list of the categories within the class, 
characterizing the complements of the category 
and then listing each sense in the category, with 
any nuance of meaning as necessary. Finally, we 
attempt to summarize the selection criteria that 
have been used across all the senses in the class. 
The process of building a class description re-
veals inconsistencies in each of the class fields. 
When we place a preposition sense into the class, 
we may find it necessary to make changes in the 
underlying data. 
At the top level, these class analyses in effect 
constitute a coarse-grained sense inventory. As 
the subclasses are developed, a finer-grained 
analysis of a particular area is available. We be-
lieve these analyses may provide a comprehen-
sive characterization of particular semantic roles 
that can be used for various NLP applications. 
6 Availability of PDEP Data and Poten-
tial for Further Enhancements 
As indicated above, each of the tables shown in 
the figures is generated in Javascript through a 
system call to a PHP script. Each of these scripts 
is described in detail at the PDEP web site. Each 
script returns data in Javascript Object Notation 
(JSON), enabling users to obtain whatever data is 
of interest to them and perhaps using this data 
dynamically. 
While PDEP provides access to a large 
amount of data, the architecture is very flexible 
and easy to extend. For this, we are grateful for 
the Tratz parser and the DVC code. 
In building PDEP, we found it necessary to 
reprocess the SemEval 2007 data of the full 
28,052 sentences that were available through 
TPP, rather than just those that were used in the 
SemEval task itself. Tagging, parsing, and creat-
ing feature files for these sentences took less than 
10 minutes, with an equal time to upload the fea-
ture files. We would be able to add or substitute 
new corpora to the PDEP databases with rela-
tively little effort. 
Similarly, we can add new elements or modify 
existing elements that describe preposition pat-
terns. This would require easily-made modifica-
tions to the underlying MySQL database tables. 
The PHP scripts that access these tables are also 
easily developed or modified. Most of these 
scripts use less than 100 lines of code. 
In developing PDEP, we have added various 
resources incrementally. This applies to such 
resources as the DVC ontology, FrameNet, and 
VerbNet. Each of these resources required rela-
tively little effort to integrate into PDEP. We will 
continue to investigate the utility of other re-
sources that will assist in characterizing preposi-
tion behavior. We have begun to look at the noun 
clusters used in Srikumar and Roth (2013) for 
better characterizing complements. We are also 
1281
examining an Oxford noun hierarchy as another 
alternative for complement analysis. We are ex-
amining the WordNet detour to FrameNet, as 
described in Burchardt et al (2005), particularly 
for use in further characterizing the governors. 
We recognize that an important element of 
PDEP will be in its utility for preposition disam-
biguation. While we have not yet begun the nec-
essary experimentation and evaluation, we be-
lieve the representativeness and sample sizes of 
the TPP corpus (mostly with 250 or more sen-
tences per preposition) should provide a basis for 
constructing the needed studies. We expect that 
this will follow techniques used by Cinkova et al 
(2012), in examining the Pattern Dictionary of 
English Verbs developed as the precursor to 
DVC. 
We expect that interaction with the NLP 
community will help PDEP evolve into a useful 
resource, not only for characterizing preposition 
behavior, but also for assisting in the develop-
ment of other lexical resources. 
7 Conclusion and Future Plans 
We have described the Pattern Dictionary of 
English Prepositions (PDEP) as a new lexical 
resource for examining and recording preposition 
behavior. PDEP does not introduce any ideas that 
have not already been explored in the investiga-
tion of other parts of speech. However, by bring-
ing together work from these disparate sources, 
we have shown that it is possible to analyze 
preposition behavior in a manner equivalent to 
the major parts of speech. Since dictionary pub-
lishers have not previously devoted much effort 
in analyzing preposition behavior, we believe 
PDEP may serve an important role, particularly 
for various NLP applications in which semantic 
role labeling is important. 
On the other hand, PDEP as described in this 
paper is only in its initial stages. In following the 
principles laid out for verbs in PDEV, a main 
goal is to provide a sufficient characterization of 
how frequently different preposition patterns 
(senses) occur, with some idea of a statistical 
characterization of the probability of the con-
junction of a preposition, its complement, and its 
governor. Better development of a desired syn-
tagmatic characterization of preposition behav-
ior, consistent with the principles of TNE, is still 
needed. Since preposition behavior is strongly 
linked to verb behavior, further effort is needed 
to link PDEP to PDEV. 
The resource will benefit from futher experi-
mentation and evaluation stages. We expect that 
desired improvements will come from usage in 
various NLP tasks, particularly word-sense dis-
ambiguation and semantic role labeling. In par-
ticular, we anticipate that interaction with the 
NLP community will identify further enhance-
ments, developments, and hints from usage. 
Acknowledgments 
Stephen Tratz (and Dirk Hovy) provided consid-
erable assistance in using the Tratz parser. Vivek 
Srikumar graciously provided his data on prepo-
sition classes. Vitek Baisa similarly helped with 
the adaptation of the PDEV Javascript modules. 
Orin Hargraves, Patrick Hanks, and Eduard 
Hovy continued to provide valuable insights. 
Reviewer comments helped sharpen the draft 
version of the paper. 
References 
Bharat Ram Ambati, Siva  Reddy, and Adam 
Kilgarriff. 2012. Word Sketches for Turkish. In 
Proceedings of the Eighth International Confer-
ence on Language Resources and Evaluation 
(LREC). Istanbul, 2945-2950. 
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 
2005. A WordNet Detour to FrameNet. Proceed-
ings of GLDV workshop GermaNet II. Bonn. 
Silvie Cinkova, Martin Holub, Adam Rambousek, and 
Lenka Smejkalova. 2012. A database of semantic 
clusters of verb usages. Lexical Resources and 
Evaluation Conference. Istanbul, 3176-83. 
Patrick Hanks. 2004. Corpus Pattern Analysis. In 
EURALEX Proceedings. Vol. I, pp. 87-98. Lorient, 
France: Universit? de Bretagne-Sud. 
Patrick Hanks. 2013. Lexical Analysis: Norms and 
Exploitations. MIT Press. 
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and Da-
vid Tugwell. 2004. The Sketch Engine. Proceed-
ings of EURALEX. Lorient, France, pp. 105-16. 
Ken Litkowski. 2013a. The Preposition Project Cor-
pora. Technical Report 13-01. Damascus, MD: CL 
Research. 
Ken Litkowski. 2013b. Preposition Disambiguation: 
Still a Problem. Technical Report 13-02. Damas-
cus, MD: CL Research. 
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The 
Linguistic Dimensions of Prepositions and Their 
Use in Computational Linguistic Formalisms and 
Applications?, pages 171?179. 
 Ken Litkowski and Orin Hargraves. 2006. Coverage 
and Inheritance in The Preposition Project. In: 
Proceedings of the Third ACL-SIGSEM Workshop 
on Prepositions. Trento, Italy.ACL. 89-94. 
1282
 Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of 
Prepositions. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations 
(SemEval-2007), Prague, Czech Republic. 
James Pustejovsky, Catherine Havasi, Jessica 
Littman, Anna Rumshisky, and Marc Verhagen. 
2006. Towards a Generative Lexical Resource: The 
Brandeis Semantic Ontology. 5th Edition of the In-
ternational Conference on Lexical Resources and 
Evaluation., 1702-5. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. New York: Long-
man Inc. 
Vivek Srikumar and Dan Roth. 2011. A Joint Model 
for Extended Semantic Role Labeling. In Proceed-
ings of the 2011 Conference on Empirical Methods 
in Natural Language Processing. ACL, 129-139. 
Vivek Srikumar and Dan Roth. 2013. Modeling Se-
mantic Relations Expressed by Prepositions. 
Transactions of the Association for Computational 
Linguistics, 1. 
Angus Stevenson and Catherine Soanes (Eds.). 2003. 
The Oxford Dictionary of English. Oxford: Claren-
don Press. 
Stephen Tratz. 2011. Semantically-Enriched Parsing 
for Natural Language Understanding. PhD Thesis, 
University of Southern California. 
Stephen Tratz and Eduard Hovy. 2011. A Fast, Accu-
rate, Non-Projective, Semantically-Enriched Par-
ser. In Proceedings of the 2011 Conference on 
Empirical Methods in Natural Language Pro-
cessing. Edinburgh, Scotland, UK. 
Deniz Yuret. 2007. KU: Word Sense Disambiguation 
by Substitution. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations 
(SemEval-2007), Prague, Czech Republic. 
Zapirain, B., E. Agirre, L. Marquez, and M. Surdeanu. 
2013. Selectional Preferences for Semantic Role 
Classification. Computational Linguistics, 39:3. 
1283
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 300?303,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CLR: Linking Events and Their Participants in Discourse Using a 
Comprehensive FrameNet Dictionary 
 
 
Ken Litkowski 
CL Research 
Damascus, MD USA. 
                    ken@clres.com 
 
  
 
Abstract 
The CL Research system for SemEval-2 Task 
10 for linking events and their participants in 
discourse is an exploration of the use of a spe-
cially created FrameNet dictionary that cap-
tures all FrameNet information about frames, 
lexical units, and frame-to-frame relations.  
This system is embedded in a specially de-
signed interface, the Linguistic Task Analyzer. 
The implementation of this system was quite 
minimal at the time of submission, allowing 
only an initial completion of the role recogni-
tion and labeling task, with recall of 0.112, 
precision of 0.670, and F-score of 0.192. We 
describe the design of the system and the con-
tinuing efforts to determine how much of this 
task can be performed with the available lexi-
cal resources. Changes since the official sub-
mission have improved the F-score to 0.266. 
1 Introduction 
The semantic role labeling (SRL) task has re-
ceived considerable attention in recent years, 
with previous tasks in Senseval-2 (Litkowski, 
2004), Semeval-1 (Baker et al, 2007), and 
CoNLL (Carreras & Marquez, 2004; Carreras & 
Marquez, 2005). The current task, Linking 
Events and their Participants in Discourse, con-
tinues the evolution of SRL tasks with the intent 
of identifying Null Instantiations, i.e., frame 
elements that are absent from the local context, 
but potentially recoverable from the wider dis-
course context. 
CL Research participated in one subtask, role 
recognition and labeling, unable to implement 
techniques for the null instantiation subtask. This 
paper describes our efforts thus far (clearly a 
work in progress), specifically the implementa-
tion of a development interface (section 2), the 
use of a specially constructed FrameNet dictio-
nary (section 3), techniques for performing the 
role recognition and labeling task (section 4), our 
results (section 5), and future developments (sec-
tion 6). 
 
2 The Linguistic Task Analyzer 
CL Research participated in the linking task by 
extending its Linguistic Task Analyzer (LTA), 
an interface also used for such tasks as word-
sense disambiguation and recognizing textual 
entailment. LTA includes a wide array of mod-
ules, including a full-scale parser, post-parsing 
semantic analysis routines, the use of XML func-
tionality for creating and analyzing input and 
output, and access to several integrated dictiona-
ries (used for semantic analysis). Modification of 
LTA for the linking task involves using existing 
functionality and implementing new functionali-
ty specific to the task. We describe LTA in some 
detail to illustrate steps that might be relevant to 
a symbolic approach to the linking task. 
Each task in LTA consists of a set of items to 
be analyzed, in this case, an identifier for each 
sentence in the document being analyzed. LTA 
loads the appropriate XML files (usually the an-
notation file and the gold file) and provides vari-
ous data for each sentence, including the number 
of terminals, non-terminals, frames, frame ele-
ments that have been recognized, true positives, 
false positives, false negatives, and a characteri-
zation of problems that have been encountered. 
Summary statistics are given, showing such 
things as the total number of frames and the scor-
ing for the current annotation (when a gold file is 
available). 
Whenever a sentence is selected in the LTA, 
the text is shown (accomplished by querying the 
XML for the selected sentence and retrieving all 
its terminals). LTA provides a capability for se-
300
lecting all sentences matching particular criteria, 
e.g., all sentences containing a Color frame or all 
sentences having targets that have problematic 
entries in the FrameNet dictionary. 
LTA contains a basic command to run and 
evaluate the system against the selected sen-
tences. This can be used during development to 
test the effect of changes to the underlying code 
for performing any of the tasks. During the test 
phase, all sentences are selected, the Run and 
Evaluate command is executed, the XML test 
file is modified with the insertion of frame ele-
ments constituting the system?s answers, and the 
XML file is saved for the official submission. 
For the official submission, this took less than a 
minute for each of the two chapters. 
A single sentence can be selected in the LTA 
for detailed examination. This Sentence Detail 
shows (1) the sentence itself (as in the main 
form), (2) a tree of the frames in the sentence, 
along with each of the frame elements that have 
been identified, minimally showing the target, 
and the text that has been identified for the frame 
element, and (3) from the training data, the frame 
element differences from the gold file, along 
with their terminal or non-terminal id references. 
The Sentence Detail also has buttons to (1) 
score the annotation against the gold file for the 
sentence, (2) identify the missing core frame 
elements, (3) examine the FrameNet entries for 
the targets, and (4) perform the task. The func-
tionality underlying the scoring and the task per-
formance are called from the main form when all 
or selected sentences are to be processed (e.g., in 
the Run and Evaluate command). 
Implementation of the scoring functionality 
for the Sentence Detail form attempts to follow 
the implementation in the official scorer. We 
have not yet captured every nuance of the scorer; 
however, we seem to have 99.9 percent agree-
ment. 
The Sentence Detail functionality is at the 
heart of the investigation and implementation of 
techniques for performing the tasks. At this time, 
we must view the implementation as only in its 
initial stages, minimally capable of performing 
the role recognition and labeling task. Further 
details about the implementation, including its 
shortcomings, will be described below. 
3 The FrameNet Dictionary 
Central to the performance of the linking task is 
the use of a dictionary constructed from the Fra-
meNet data. This dictionary is in a format used 
by the CL Research DIMAP dictionary mainten-
ance program. 1  The FrameNet dictionary at-
tempts to capture all the information in Frame-
Net, in a form that can be easily accessed and 
used for tasks such as the linking task. This dic-
tionary is also used in general word-sense dis-
ambiguation tasks, when all words in a text are 
simultaneously disambiguated with several dic-
tionaries. The FrameNet dictionary has almost 
11,000 entries 2  of four main types: frames, 
frame-to-frame relations, normal entries, and 
frame elements 3 . This dictionary was initially 
described in Litkowski (2007), but is described 
in more detail in the following subsections in 
order to show how the information in these en-
tries is used in the linking task. 
3.1 Frame Entries 
A FrameNet frame is entered in the dictionary by 
preceding its name with a ?#? sign to distinguish 
it from other types of entries. A frame entry, 
such as #Abandonment, consists of one sense 
with no part of speech. This sense contains a list 
of its frame elements and the coreness of each 
frame element. The sense also lists all the lexical 
units associated with the frame, along with the 
identifying number for each so that a link can be 
made if necessary to the appropriate lexical unit 
and lexical entry XML files. The sense identifies 
any frame-to-frame relations in which the frame 
participates, such as ?IS_INHERITED_BY? with 
a link to the inheriting frame. Thus, whenever a 
specific frame is signaled in the linking task, its 
properties can be accessed and we can investi-
gate which of the frame elements might be 
present in the context. 
3.2 Frame-to-Frame Relations 
While the entries for the individual frames iden-
tify the frame-to-frame relations in which a 
frame participates, separate entries are created to 
                                               
1 These dictionaries are stored in a Btree file format for 
rapid access. A free demonstration version of DIMAP is 
available at CL Research (http://www.clres.com). This ver-
sion can be used to manipulate any of several dictionaries 
that are also available. These include WordNet and the basic 
FrameNet. CL Research also makes available a publicly 
available FrameNet Explorer and a DIMAP Frame Element 
Hierarchy dictionary. 
2 By contrast, the DIMAP dictionary for WordNet contains 
147,000 entries. 
3 When a new version of FrameNet is made available, a new 
version of the DIMAP dictionary is created. This was the 
case with the preliminary FrameNet version 1.4a made 
available by the task organizers. This creation takes about 
two hours. 
301
hold the mappings between the frame elements 
of the two frames. These entries are prefixed 
with an ?@? sign, followed by the name of a 
frame, the frame relation, and the name of the 
second frame, as in the name 
?@Abounding_with INHERITS Loca-
tive_relation?. The single sense for such an entry 
shows the mapping, e.g., of the Location frame 
element of Abounding_with to the Figure frame 
element of Locative_relation. The information 
in these entries has not yet been used in the link-
ing task. 
3.3 Frame Elements 
Frame element entries are preceded with a ?%?, 
as in %Toxic_substance. We have a taxonomy 
of the 1131 uniquely-named frame elements in 
all the FrameNet frames. 4  Each frame element 
entry identifies its superordinate frame element 
(or none for the 12 roots) and the frame elements 
in which it is used. The information in these en-
tries has not yet been used in the linking task. 
3.4 Main Entries 
The bulk of the entries in the FrameNet dictio-
nary are for the lexical units. An entry was 
created for each unique form, with senses for 
each lexical unit of the base form. Thus, beat has 
four senses, two verb, one noun, and one adjec-
tive. Minimally, each sense contains its part of 
speech, its frame, and its id number. A sense may 
also contain a definition and its source, if present 
n the FrameNet lexical unit files. 
If available, the information available in the 
lexical entry (LE) files is encapsulated in the 
sense, from the FERealization elements. This 
captures the phrase type, the grammatical func-
tion, the frame element, and the frequency in the 
FrameNet annotation files. An example of what 
information is available for one verb sense of 
beat is shown in Table 1. 
 
Table 1. Lexical Entry Syntactic Patterns for ?beat? 
Feature Name Feature Value 
NP(Ext) Loser (12) 
   Loser (28) 
  Winner (5) 
  Winner (5) 
  Winner (2) 
  Winner (31) 
NP(Obj) 
PP[by](Dep) 
CNI() 
PP[against](Dep) 
NP(Ext) 
                                               
4 This taxonomy can be viewed at 
http://www.clres.com/db/feindex.html, which provides links 
describing how it was constructed and which can be down-
loaded in DIMAP or MySQL format. 
At the present time, this type of information is 
the primary information used in the linking task. 
4 Role Recognition and Labeling 
To perform the role recognition and labeling 
task, the system first retrieves all the frames for 
the sentence and then iterates over each. The 
frame name and the target are retrieved. From 
the target XML, the id reference is used to re-
trieve the part of speech and lemma from the tar-
gets terminal node. With this information, an 
attempt is made to add child nodes to the frame 
node in the XML, thus supplying the system?s 
performance of the task. After any nodes have 
thus been added, it is only necessary to save the 
modified XML as the output file. 
The first step in adding child nodes is to obtain 
the lexical entries from the FrameNet dictionary 
for the frame and the lemma. Since the lemma 
may have multiple senses, we obtain the specific 
sense that corresponds to the frame. We iterate 
through the features for the sense, focusing on 
those providing syntactic patterns, such as those 
in Table 1. We deconstruct the feature value into 
its frame element name and its frequency. We 
then call a function with the feature name and the 
target?s id reference to see if we can find a 
matching constituent; if successful, we create a 
child node of the frame with the frame element 
name and the id reference (for the child <fe-
node> of frame element <fe> node). 
The matching constituent function operates on 
the syntactic pattern, calling specific functions to 
search the XML terminals and non-terminals for 
constituent that fit the syntactic criterion. At 
present, this only operates on four patterns: 
DEN(), Poss(Gen), NP(Ext), and N(Head). 5 As 
an example, for Poss(Gen), we select the non-
terminals with the target as the ?head? and search 
these for a terminal node marked as PRP$. A 
special constituent matching function was also 
written to look for the Supported frame element 
in the Support frame. 
5 System Results 
CL Research?s results for the role recognition 
and labeling task are shown in Table 2. These 
results are generally consistent across the two 
chapters in the test and with results obtained with 
the training data during development. Combining 
                                               
5 The DEN pattern identifies incorporated frame elements. 
Since the official submission, two patterns (NP(OBJ) and 
PP(Dep)  have been added.  
302
the two chapters, the recall was 0.112, the preci-
sion was 0.670, and the F-score was 0.192. 6 
 
Table 2. Scores for Chapters 13 and 14 
Measure Ch. 13 Ch. 14 
True Positives 191 246 
False Positives 82 133 
False Negatives 1587 1874 
Correct Labels 189 237 
Precision 0.700 0.649 
Recall 0.107 0.116 
F-Score 0.186 0.197 
Label Accuracy 0.106 0.112 
 
As can be seen, for entries with patterns (albeit 
a low recall), a substantial number of frame ele-
ments could be recognized with high precision 
from a very small number of constituent match-
ing functions. A detailed analysis of the results, 
identifying the contribution of each pattern rec-
ognition and the problem of false positives, has 
not yet been completed. One such observation is 
that when the same syntactic pattern is present 
for more than one frame element, such as 
NP(Ext) for both Loser and Winner in the case 
of beat as shown in Table 1, the same constituent 
will be identified for both. 
A significant shortcoming in the system oc-
curs when there are no syntactic patterns availa-
ble for a particular sense (27 percent of the tar-
gets). For example, the lemma hour frequently 
appears in the training set as the target of either 
the Measure_duration or Calendric_unit 
frames, but it has no syntactic patterns (i.e., the 
FrameNet data contain no annotations for this 
lexical unit), while decade, also used in the same 
frames, does have syntactic patterns. This is a 
frequent occurrence with the FrameNet dictio-
nary. 
6 Future Developments 
As should be clear from the preceding descrip-
tion, there are many opportunities for improve-
ment. First, several improvements can be made 
in the LTA to improve the ability to facilitate 
development. The LTA has only barely begun 
exploitation of the many integrated modules that 
are available. Additional functionality needs to 
be developed so that it will be possible to deter-
mine the effect of any changes in constituent 
matching, i.e., what is the effect on recall and 
                                               
6The additional patterns described in the previous footnote 
have improved recall to 0.166 and F-score to 0.266, while 
maintaining a high precision (0.676).  
precision. The sentence detail form can be im-
proved to provide better insights into the relation 
between syntactic patterns and their matching 
constituents. 
Secondly, major improvements appear likely 
from greater exploitation of the FrameNet dictio-
nary. At present, no use is made of the frequency 
information or the weighting of choices for 
matching constituents. When a given lemma has 
no syntactic patterns, it is likely that some use of 
the patterns for other lexical units in the frame 
can be made. It is also possible that some general 
patterns can be discerned using the frame ele-
ment taxonomy. 
It is important to see how far the FrameNet da-
ta can be further exploited and where other lexi-
cal data, such as available in WordNet or in more 
traditional lexical databases, can be used. The 
data developed for this linking task provide 
many opportunities for further exploration. 
References  
Collin Baker, Michael Ellsworth, and Katrin Erk. 
2007. Semeval-2007 Task 19: Frame Semantic 
Structure Extraction. Proceedings of the Fourth In-
ternational Workshop on Semantic Evaluations 
(SemEval-2007). Prague, Czech Republic, Associa-
tion for Computational Linguistics, pp. 99-104.  
Xavier Carreras and Luis Marquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task Semantic 
Role Labeling. Proceedings of the Eighth Confe-
rence on Computational Natural Language Learn-
ing (CoNLL-2004) International Workshop on Se-
mantic Evaluations (SemEval-2007). Boston, MA 
Association for Computational Linguistics, pp. 89-
97.  
Xavier Carreras and Luis Marquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task Semantic 
Role Labeling. Proceedings of the Eighth Confe-
rence on Computational Natural Language Learn-
ing (CoNLL-2004) International Workshop on Se-
mantic Evaluations (SemEval-2007). Ann Arbor, 
MI Association for Computational Linguistics, pp. 
152-164.  
Kenneth C. Litkowski. 2004. Senseval-3 Task: Auto-
matic Labeling of Semantic Roles. Proceedings of 
Senseval-3: The Third International Workshop on 
the Evaluation of Systems for the Semantic Analy-
sis of Text. Barcelona, Spain, Association for 
Computational Linguistics, pp. 9-12. 
Kenneth C. Litkowski. 2007. CLR: Integration of 
FrameNet in a Text Representation System. Pro-
ceedings of the Fourth International Workshop on 
Semantic Evaluations (SemEval-2007). Prague, 
Czech Republic, Association for Computational 
Linguistics, pp. 113-6. 
303
