Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286?295,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning 5000 Relational Extractors
Raphael Hoffmann, Congle Zhang, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA-98195, USA
{raphaelh,clzhang,weld}@cs.washington.edu
Abstract
Many researchers are trying to use information
extraction (IE) to create large-scale knowl-
edge bases from natural language text on the
Web. However, the primary approach (su-
pervised learning of relation-specific extrac-
tors) requires manually-labeled training data
for each relation and doesn?t scale to the thou-
sands of relations encoded in Web text.
This paper presents LUCHS, a self-supervised,
relation-specific IE system which learns 5025
relations ? more than an order of magnitude
greater than any previous approach ? with an
average F1 score of 61%. Crucial to LUCHS?s
performance is an automated system for dy-
namic lexicon learning, which allows it to
learn accurately from heuristically-generated
training data, which is often noisy and sparse.
1 Introduction
Information extraction (IE), the process of gen-
erating relational data from natural-language text,
has gained popularity for its potential applications
in Web search, question answering and other tasks.
Two main approaches have been attempted:
? Supervised learning of relation-specific ex-
tractors (e.g., (Freitag, 1998)), and
? ?Open? IE ? self-supervised learning of
unlexicalized, relation-independent extractors
(e.g., Textrunner (Banko et al, 2007)).
Unfortunately, both methods have problems.
Supervised approaches require manually-labeled
training data for each relation and hence can?t
scale to handle the thousands of relations encoded
in Web text. Open extraction is more scalable,
but has lower precision and recall. Furthermore,
open extraction doesn?t canonicalize relations, so
any application using the output must deal with
homonymy and synonymy.
A third approach, sometimes refered to as weak
supervision, is to heuristically match values from
a database to text, thus generating a set of train-
ing data for self-supervised learning of relation-
specific extractors (Craven and Kumlien, 1999).
With the Kylin system (Wu and Weld, 2007) ap-
plied this idea to Wikipedia by matching values
of an article?s infobox1 attributes to corresponding
sentences in the article, and suggested that their
approach could extract thousands of relations (Wu
et al, 2008). Unfortunately, however, they never
tested the idea on more than a dozen relations. In-
deed, no one has demonstrated a practical way to
extract more than about one hundred relations.
We note that Wikipedia?s infobox ?ontology? is
a particularly interesting target for extraction. As a
by-product of thousands of contributors, it is broad
in coverage and growing quickly. Unfortunately,
the schemata are surprisingly noisy and most are
sparsely populated; challenging conditions for ex-
traction.
This paper presents LUCHS, an autonomous,
self-supervised system, which learns 5025 rela-
tional extractors ? an order of magnitude greater
than any previous effort. Like Kylin, LUCHS cre-
ates training data by matching Wikipedia attribute
values with corresponding sentences, but by itself,
this method was insufficient for accurate extrac-
tion of most relations. Thus, LUCHS introduces
a new technique, dynamic lexicon features, which
dramatically improves performance when learning
from sparse data and that way enables scalability.
1.1 Dynamic Lexicon Features
Figure 1 summarizes the architecture of LUCHS.
At the highest level, LUCHS?s offline training pro-
cess resembles that of Kylin. Wikipedia pages
1A sizable fraction of Wikipedia articles have associated
infoboxes ? relational summaries of the key aspects of the
subject of the article. For example, the infobox for Alan Tur-
ing?s Wikipedia page lists the values of 10 attributes, includ-
ing his birthdate, nationality and doctoral advisor.
286
Matcher Harvester
CRF 
Learner
Filtered Lists
WWW
Lexicon 
Learner
Classifier
Learner
Training Data
Extractor
Training Data
Lexicons
TuplesPages
Article
Classifier ExtractorExtractorClassified Pages
Extraction
Learning
Figure 1: Architecture of LUCHS. In order to
handle sparsity in its heuristically-generated train-
ing data, LUCHS generates custom lexicon features
when learning each relational extractor.
containing infoboxes are used to train a classi-
fier that can predict the appropriate schema for
pages missing infoboxes. Additionally, the val-
ues of infobox attributes are compared with article
sentences to heuristically generate training data.
LUCHS?s major innovation is a feature-generation
process, which starts by harvesting HTML lists
from a 5B document Web crawl, discarding 98%
to create a set of 49M semantically-relevant lists.
When learning an extractor for relation R, LUCHS
extracts seed phrases from R?s training data and
uses a semi-supervised learning algorithm to cre-
ate several relation-specific lexicons at different
points on a precision-recall spectrum. These lex-
icons form Boolean features which, along with
lexical and dependency parser-based features, are
used to produce a CRF extractor for each relation
? one which performs much better than lexicon-
free extraction on sparse training data.
At runtime, LUCHS feeds pages to the article
classfier, which predicts which infobox schema
is most appropriate for extraction. Then a small
set of relation-specific extractors are applied to
each sentence, outputting tuples. Our experiments
demonstrate a high F1 score, 61%, across the 5025
relational extractors learned.
1.2 Summary
This paper makes several contributions:
? We present LUCHS, a self-supervised IE sys-
tem capable of learning more than an order
of magnitude more relation-specific extractors
than previous systems.
? We describe the construction and use of dy-
namic lexicon features, a novel technique, that
enables hyper-lexicalized extractors which
cope effectively with sparse training data.
? We evaluate the overall end-to-end perfor-
mance of LUCHS, showing an F1 score of 61%
when extracting relations from randomly se-
lected Wikipedia pages.
? We present a comprehensive set of additional
experiments, evaluating LUCHS?s individual
components, measuring the effect of dynamic
lexicon features, testing sensitivity to varying
amounts of training data, and categorizing the
types of relations LUCHS can extract.
2 Heuristic Generation of Training Data
Wikipedia is an ideal starting point for our long-
term goal of creating a massive knowledge base of
extracted facts for two reasons. First, it is com-
prehensive, containing a diverse body of content
with significant depth. Perhaps more importantly,
Wikipedia?s structure facilitates self-supervised
extraction. Infoboxes are short, manually-created
tabular summaries of many articles? key facts ?
effectively defining a relational schema for that
class of entity. Since the same facts are often ex-
pressed in both article and ontology, matching val-
ues of the ontology to the article can deliver valu-
able, though noisy, training data.
For example, the Wikipedia article on ?Jerry Se-
infeld? contains the sentence ?Seinfeld was born
in Brooklyn, New York.? and the article?s infobox
contains the attribute ?birth place = Brooklyn?.
By matching the attribute?s value ?Brooklyn? to
the sentence, we can heuristically generate train-
ing data for a birth place extractor. This data is
noisy; some attributes will not find matches, while
others will find many co-incidental matches.
3 Learning Extractors
We first assume that each Wikipedia infobox at-
tribute corresponds to a unique relation (but see
Section 5.6) for which we would like to learn a
specific extractor. A major challenge with such
an approach is scalability. Running a relation-
specific extractor for each of Wikipedia?s 34,000
unique infobox attributes on each of Wikipedia?s
50 million sentences would require 1.7 trillion ex-
tractor executions.
We therefore choose a hierarchical approach
that combines both article classifiers and rela-
tion extractors. For each infobox schema, LUCHS
trains a classifier that predicts if an article is likely
to contain that schema. Only when an article
287
is likely to contain a schema, does LUCHS run
that schema?s relation extractors. To extract in-
fobox attributes from all of Wikipedia, LUCHS
now needs orders of magnitude fewer executions.
While this approach does not propagate infor-
mation from extractors back to article classifiers,
experiments confirm that our article classifiers
nonetheless deliver accurate results (Section 5.2),
reducing the potential benefit of joint inference. In
addition, our approach reduces the need for extrac-
tors to keep track of the larger context, thus sim-
plifying the extraction problem.
We briefly summarize article classification: We
use a linear, multi-class classifier with six kinds of
features: words in the article title, words in the
first sentence, words in the first sentence which
are direct objects to the verb ?to be?, article sec-
tion headers, Wikipedia categories, and their an-
cestor categories. We use the voted perceptron al-
gorithm (Freund and Schapire, 1999) for training.
More challenging are the attribute extractors,
which we wish to be simple, fast, and able to well
capture local dependencies. We use a linear-chain
conditional random field (CRF) ? an undirected
graphical model connecting a sequence of input
and output random variables, x = (x0, . . . , xT )
and y = (y0, . . . , yT ) (Lafferty et al, 2001). In-
put variables are assigned words w. The states
of output variables represent discrete labels l, e.g.
Argi-of-Relj and Other. In our case, variables
are connected in a chain, following the first-order
Markov assumption. We train to maximize condi-
tional likelihood of output variables given an input
probability distribution. The CRF models p(y|x)
are represented with a log-linear distribution
p(y|x) =
1
Z(x)
exp
T?
t=1
K?
k=1
?kfk(yt?1, yt, x, t)
where feature functions, f , encode sufficient
statistics of (x, y), T is the length of the sequence,
K is the number of feature functions, and ?k are
parameters representing feature weights, which
we learn during training. Z(x) is a partition func-
tion used to normalize the probabilities to 1. Fea-
ture functions allow complex, overlapping global
features with lookahead.
Common techniques for learning the weights ?k
include numeric optimization algorithms such as
stochastic gradient descent or L-BFGS. In our ex-
periments, we again use the simpler and more effi-
cient voted-perceptron algorithm (Collins, 2002).
The linear-chain layout enables efficient interence
using the dynamic programming-based Viterbi al-
gorithm (Lafferty et al, 2001).
We evaluate nine kinds of Boolean features:
Words For each input word w we introduce fea-
ture fww (yt?1, yt, x, t) := 1[xt=w].
State Transitions For each transition be-
tween output labels li, lj we add feature
f tranli,lj (yt?1, yt, x, t) := 1[yt?1=li?yt=lj ].
Word Contextualization For parameters p and
s we add features fprevw (yt?1, yt, x, t) :=
1[w?{xt?p,...,xt?1}] and f
sub
w (yt?1, yt, x, t) :=
1[w?{xt+1,...,xt+s}] which capture a window of
words appearing before and after each position t.
Capitalization We add feature
fcap(yt?1, yt, x, t) := 1[xtis capitalized].
Digits We add feature fdig(yt?1, yt, x, t) :=
1[xtis digits].
Dependencies We set fdep(yt?1, yt, x, t) to the
lemmatized sequence of words from xt to the root
of the dependency tree, computed using the Stan-
ford parser (Marneffe et al, 2006).
First Sentence We set f fs(yt?1, yt, x, t) :=
1[xtin first sentence of article].
Gaussians For numeric attributes, we fit a Gaus-
sian (?, ?) and add feature fgaui (yt?1, yt, x, t) :=
1[|xt??|<i?] for parameters i.
Lexicons For non-numeric attributes, and for a
lexicon l, i.e. a set of related words, we add fea-
ture f lexl (yt?1, yt, x, t) := 1[xt?l]. Lexicons are
explained in the following section.
4 Extraction with Lexicons
It is often possible to group words that are likely
to be assigned similar labels, even if many of these
words do not appear in our training set. The ob-
tained lexicons then provide an elegant way to im-
prove the generalization ability of an extractor, es-
pecially when only little training data is available.
However, there is a danger of overfitting, which
we discuss in Section 4.2.4.
The next section explains how we mine the Web
to obtain a large corpus of quality lists. Then Sec-
tion 4.2 presents our semi-supervised algorithm
for learning semantic lexicons from these lists.
288
4.1 Harvesting Lists from the Web
Domain-independence requires access to an ex-
tremely large number of lists, but our tight in-
tegration of lexicon acquisition and CRF learn-
ing requires that relevant lists be accessed instan-
taneously. Approaches using search engines or
wrappers at query time (Etzioni et al, 2004; Wang
and Cohen, 2008) are too slow; we must extract
and index lists prior to learning.
We begin with a 5 billion page Web crawl.
LUCHS can be combined with any list harvesting
technique, but we choose a simple approach, ex-
tracting lists defined by HTML <ul> or <ol>
tags. The set of lists obtained in this way is ex-
tremely noisy ? many lists comprise navigation
bars, tag sets, spam links, or a series of long text
paragraphs. This is consistent with the observation
that less than 2% of Web tables are relational (Ca-
farella et al, 2008).
We therefore apply a series of filtering steps.
We remove lists of only one or two items, lists
containing long phrases, and duplicate lists from
the same host. After filtering we obtain 49 million
lists, containing 56 million unique phrases.
4.2 Semi-Supervised Learning of Lexicons
While training a CRF extractor for a given rela-
tion, LUCHS uses its corpus of lists to automati-
cally generate a set of semantic lexicons ? spe-
cific to that relation. The technique proceeds in
three steps, which have been engineered to run ex-
tremely quickly:
1. Seed phrases are extracted from the labeled
training set.
2. A learning algorithm expands the seed
phrases into a set of lexicons.
3. The semantic lexicons are added as features
to the CRF learning algorithm.
4.2.1 Extracting Seed Phrases
For each training sentence LUCHS first identifies
subsequences of labeled words, and for each such
labeled subsequence, LUCHS creates one or more
seed phrases p. Typically, a set of seeds con-
sists precisely of the labeled subsequences. How-
ever, if the labeled subsequences are long and have
substructure, e.g., ?San Remo, Italy?, our system
splits at the separator token, and creates additional
seed sets from prefixes and postfixes.
4.2.2 From Seeds to Lexicons
To expand a set of seeds into a lexicon, LUCHS
must identify relevant lists in the corpus. Rele-
vancy can be computed by defining a similarity be-
tween lists using the vector-space model. Specifi-
cally, let L denote the corpus of lists, and P be the
set of unique phrases from L. Each list l0 ? L can
be represented as a vector of weighted phrases p ?
P appearing on the list, l0 = (l0p1 l
0
p2 . . . l
0
p|P|). Fol-
lowing the notion of inverse document frequency,
a phrase?s weight is inversely proportional to the
number of lists containing the phrase. Popular
phrases which appear on many lists thus receive
a small weight, whereas rare phrases are weighted
higher:
l0pi =
1
|{l ? L|p ? l}|
Unlike the vector space model for documents, we
ignore term frequency, since the vast majority of
lists in our corpus don?t contain duplicates. This
vector representation supports the simple cosine
definition of list similarity, which for lists l0, l1 ?
L is defined as
simcos :=
l0 ? l1
?l0??l1?
.
Intuitively, two lists are similar if they have many
overlapping phrases, the phrases are not too com-
mon, and the lists don?t contain many other
phrases. By representing the seed set as another
vector, we can find similar lists, hopefully contain-
ing related phrases. We then create a semantic lex-
icon by collecting phrases from a range of related
lists.
For example, one lexicon may be created as the
union of all phrases on lists that have non-zero
similarity to the seed list. Unfortunately, due to
the noisy nature of the Web lists such a lexicon
may be very large and may contain many irrele-
vant phrases. We expect that lists with higher sim-
ilarity are more likely to contain phrases which are
related to our seeds; hence, by varying the sim-
ilarity threshold one may produce lexicons rep-
resenting different compromises between lexicon
precision and recall. Not knowing which lexicon
will be most useful to the extractors, LUCHS gen-
erates several and lets the extractors learn appro-
priate weights.
However, since list similarities vary depending
on the seeds, fixed thresholds are not an option. If
#similarlists denotes the number of lists that have
non-zero similarity to the seed list and #lexicons
289
the total number of lexicons we want to generate,
LUCHS sets lexicon i ? {0, . . . ,#lexicons ? 1}
to be the union of prases on the
#similarlistsi/#lexicons
most similar lists.2
4.2.3 Efficiently Creating Lexicons
We create lexicons from lists that are similar to
our seed vector, so we only consider lists that have
at least one phrase in common. Importantly, our
index structures allow LUCHS to select the rele-
vant lists efficiently. For each seed, LUCHS re-
trieves the set of containing lists as a sorted se-
quence of list identifiers. These sequences are
then merged yielding a sequence of list identifiers
with associated seed-hit counts. Precomputed list
lengths and inverse document frequencies are also
retrieved from indices, allowing efficient compu-
tation of similarity. The worst case complexity is
O(log(S)SK) where S is the number of seeds and
K the maximum number of lists to consider per
seed.
4.2.4 Preventing Lexicon Overfitting
Finally, we integrate the acquired semantic lexi-
cons as features into the CRF. Although Section 3
discussed how to use lexicons as CRF features,
there are some subtleties. Recall that the lexi-
cons were created from seeds extracted from the
training set. If we now train the CRF on the same
examples that generated the lexicon features, then
the CRF will likely overfit, and weight the lexicon
features too highly!
Before training, we therefore split the training
set into k partitions. For each example in a par-
tition we assign features based on lexicons gener-
ated from only the k?1 remaining partitions. This
avoids overfitting and ensures that we will not per-
form much worse than without lexicon features.
When we apply the CRF to our test set, we use the
lexicons based on all k partitions. We refer to this
technique as cross-training.
5 Experiments
We start by evaluating end-to-end performance of
LUCHS when applied to Wikipedia text, then an-
alyze the characteristics of its components. Our
experiments use the 10/2008 English Wikipedia
dump.
2For practical reasons, we exclude the case i = #lexicons
in our experiments.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
recall
precision
Figure 2: Precision / recall curve for end-to-end
system performance on 100 random articles.
5.1 Overall Extraction Performance
To evaluate the end-to-end performance of
LUCHS, we test the pipeline which first classifies
incoming pages, activating a small set of extrac-
tors on the text. To ensure adequate training and
test data, we limit ourselves to infobox classes
with at least ten instances; there exist 1,583 such
classes, together comprising 981,387 articles. We
only consider the first ten sentences for each ar-
ticle, and we only consider 5025 attributes.3 We
create a test set by sampling 100 articles ran-
domly; these articles are not used to train article
classifiers or extractors. Each test article is then
automatically classified, and a random attribute
of the predicted schema is selected for extraction.
Gold labels for the selected attribute and article are
created manually by a human judge and compared
to the token-level predictions from the extractors
which are trainined on the remaining articles with
heuristic matches.
Overall, LUCHS reaches a precision of .55 at a
recall of .68, giving an F1-score of .61 (Figure 2).
Analyzing the errors in more detail, we find that in
11 of 100 cases an article was incorrectly classi-
fied. We note that in at least two of these cases the
predicted class could also be considered correct.
For example, instead of Infobox Minor Planet the
extractor predicted Infobox Planet.
On five of the selected attributes the extrac-
tor failed because the attributes could be consid-
ered unlearnable: The flexibility of Wikipedia?s
infobox system allows contributors to introduce
attributes for formatting, for example defining el-
3Attributes were selected to have at least 10 heuristic
matches, to have 10% of values covered by matches, and 10%
of articles with attribute in infobox covered by matches.
290
ement order. In the future we wish to train LUCHS
to ignore this type of attribute.
We also compared the heuristic matches con-
tained in the selected 100 articles to the gold stan-
dard: The matches reach a precision of .90 at a
recall of .33, giving an F1-score of .48. So while
most heuristic matches hit mentions of attribute
values, many other mentions go unmatched. Man-
ual analysis shows that these values are often miss-
ing from an infobox, are formatted differently, or
are inconsistent to what is stated in the article.
So why did the low recall of the heuristic
matches not adversely affect recall of our extrac-
tors? For most articles, an attribute can be as-
signed a single unique value. When training an
attribute extractor, only articles that contained a
heuristic match for that attribute were considered,
thus avoiding many cases of unmatched mentions.
Subsequent experiments evaluate the perfor-
mance of LUCHS components in more detail.
5.2 Article Classification
The first step in LUCHS?s run-time pipeline is de-
termining which infobox schemata are most likely
to be found in a given article. To test this, we ran-
domly split our 981,387 articles into 4/5 for train-
ing and 1/5 for testing, and train a single multi-
class classifier. For this experiment, we use the
original infobox class of an article as its gold la-
bel. We compute the accuracy of the prediction at
.92. Since some classes can be considered inter-
changeable, this number represents a lower bound
on performance.
5.3 Factors Affecting Extraction Accuracy
We now evaluate attribute extraction assuming
perfect article classification. To keep training time
manageable, we sample 100 articles for training
and 100 articles for testing4 for each of 100 ran-
dom attributes. We again only consider the first
ten sentences of each article, and we only con-
sider articles that have heuristic matches with the
attribute. We measure F1-score at a token-level,
taking the heuristic matches as ground-truth.
We first test the performance of extractors
trained using our basic features (Section 3)5, not
including lexicons and Gaussians. We begin us-
ing word features and obtain a token-level F1-
score of .311 for text and .311 for numeric at-
tributes. Adding any of our additional features
4These numbers are smaller for attributes with less train-
ing data available, but the same split is maintained.
5For contextualization features we choose p, s = 5.
Features F1-Score
Text attributes
Baseline .491
Baseline + Lexicons w/o CT .367
Baseline + Lexicons .545
Numeric attributes
Baseline .586
Baseline + Gaussians w/o CT .623
Baseline + Gaussians .627
Table 1: Impact of Lexicon and Gaussian features.
Cross-Training (CT) is essential to improve per-
formance.
improves these scores, but the relative improve-
ments vary: For both text and numeric attributes,
contextualization and dependency features deliver
the largest improvement. We then iteratively add
the feature with largest improvement until no fur-
ther improvement is observed. We finally obtain
an F1-score of .491 for text and .586 for numeric
attributes. For text attributes the extractor uses
word, contextualization, first sentence, capitaliza-
tion, and digit features; for numeric attributes the
extractor uses word, contextualization, digit, first
sentence, and dependency features. We use these
extractors as a baseline to evaluate our lexicon and
Gaussian features.
Varying the size of the training sets affects re-
sults: Taking more articles raises the F1-score, but
taking more sentences per article reduces it. This
is because Wikipedia articles often summarize a
topic in the first few paragraphs and later discuss
related topics, necessitating reference resolution
which we plan to add in future work.
5.4 Lexicon and Gaussian Features
We next study how our distribution features6 im-
pact the quality of the baseline extractors (Table
1). Without cross-training we observe a reduction
in performance, due to overfitting. Cross-training
avoids this, and substantially improves results over
the baseline. While cross-training is particularly
critical for lexicon features, it is less needed for
Gaussians where only two parameters, mean and
deviation, are fitted to the training set.
The relative improvements depend on the num-
ber of available training examples (Table 2). Lex-
icon and Gaussian features especially benefit ex-
tractors for sparse attributes. Here we can also see
that the improvements are mainly due to increases
in recall.
6We set the number of lexicon and Gaussian features to 4.
291
# Train F1-B F1-LUCHS ?F1 ?Pr ?Re
Text attributes
10 .379 .439 +16% +10% +20%
25 .447 .504 +13% +7% +20%
100 .491 .545 +11% +5% +17%
Numeric attributes
10 .484 .531 +10% +4% +13%
25 .552 .596 +8% +4% +10%
100 .586 .627 +7% +5% +8%
Table 2: Lexicon and Gaussian features greatly ex-
pand F1 score (F1-LUCHS) over the baseline (F1-
B), in particular for attributes with few training ex-
amples. Gains are mainly due to increased recall.
5.5 Scaling to All of Wikipedia
Finally, we take our best extractors and run them
on all 5025 attributes, again assuming perfect ar-
ticle classification and using heuristic matches as
gold-standard. Figure 3 shows the distribution of
obtained F1 scores. 810 text attributes and 328 nu-
meric attributes reach a score of 0.80 or higher.
The performance depends on the number of
available training examples, and that number is
governed by a long-tailed distribution. For ex-
ample, 61% of the attributes in our set have 50
or fewer examples, 36% have 20 or fewer. Inter-
estingly, the number of training examples had a
smaller effect on performance than expected. Fig-
ure 4 shows the correlation between these vari-
ables. Lexicon and Gaussian features enables ac-
ceptable performance even for sparse attributes.
Averaging across all attributes we obtain F1
scores of 0.56 and 0.60 for textual and numeric
values respectively. We note that these scores
assume that all attributes are equally important,
weighting rare attributes just like common ones.
If we weight scores by the number of attribute in-
stances, we obtain F1 scores of 0.64 (textual) and
0.78 (numeric). In each case, precision is slightly
higher than recall.
5.6 Towards an Attribute Ontology
The true promise of relation-specific extractors
comes when an ontology ties the system together.
By learning a probabilistic model of selectional
preferences, one can use joint inference to improve
extraction accuracy. One can also answer scien-
tific questions, such as ?How many of the learned
Wikipedia attributes are distinct?? It is clear that
many duplicates exist due to collaborative sloppi-
ness, but semantic similarity is a matter of opinion
and an exact answer is impossible.
0% 20% 40% 60% 80% 100%
0.0
0.2
0.4
0.6
0.8
1.0
Text attr. (3962)
Numeric attr. (1063)
# Attributes
F1 Score
Figure 3: F1 scores among attributes, ranked by
score. 810 text attributes (20%) and 328 numeric
attributes (31%) had an F1-score of .80 or higher.
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
Text attr.
Numeric attr.
# Training Examples
Average F1 Sco
re
Figure 4: Average F1 score by number of training
examples. While more training data helps, even
sparse attributes reach acceptable performance.
Nevertheless, we clustered the textual attributes
in several ways. First, we cleaned the attribute
names heuristically and performed spell check.
The ?distance? between two attributes was calcu-
lated with a combination of edit distance and IR
metrics with Wordnet synonyms; then hierarchical
agglomerative clustering was performed. We man-
ually assigned names to the clusters and cleaned
them, splitting and joining as needed. The result is
too crude to be called an ontology, but we continue
its elaboration. There are a total of 3962 attributes
grouped in about 1282 clusters (not yet counting
attributes with numerical values); the largest clus-
ter, location, has 115 similar attributes. Figure 5
shows the confusion matrix between attributes in
the biggest clusters; the shade of the i, jth pixel
indicates the F1 score achieved by training on in-
stances of attribute i and testing on attribute j.
292
loca
tion
birt
hpla
cep
title cou
ntry
full
 nam
e
city nat
iona
lity
nati
ona
lity
birt
h na
me
date
 of 
birt
h
date
 of 
dea
th
date stat
es
Figure 5: Confusion matrix for extractor accuracy
training on one attribute then testing on another.
Note the extraction similarity between title and
full-name, as well as between dates of birth and
death. Space constraints allow us to show only
1000 of LUCHS?s 5025 extracted attributes, those
in the largest clusters.
6 Related Work
Large-scale extraction A popular approach to IE
is supervised learning of relation-specific extrac-
tors (Freitag, 1998). Open IE, self-supervised
learning of unlexicalized, relation-independent ex-
tractors (Banko et al, 2007), is a more scalable
approach, but suffers from lower precision and
recall, and doesn?t canonicalize the relations. A
third approach, weak supervision, performs self-
supervised learning of relation-specific extractors
from noisy training data, heuristically generated
by matching database values to text. (Craven and
Kumlien, 1999; Hirschman et al, 2002) apply this
technique to the biological domain, and (Mintz
et al, 2009) apply it to 102 relations from Free-
base. LUCHS differs from these approaches in that
its ?database? ? the set of infobox values ? itself
is noisy, contains many more relations, and has
few instances per relation. Whereas the existing
approaches focus on syntactic extraction patterns,
LUCHS focuses on lexical information enhanced
by dynamic lexicon learning.
Extraction from Wikipedia Wikipedia has
become an interesting target for extraction.
(Suchanek et al, 2008) build a knowledgebase
from Wikipedia?s semi-structured data. (Wang et
al., 2007) propose a semisupervised positive-only
learning technique. Although that extracts from
text, its reliance on hyperlinks and other semi-
structured data limits extraction. (Wu and Weld,
2007; Wu et al, 2008)?s systems generate train-
ing data similar to LUCHS, but were only on a few
infobox classes. In contrast, LUCHS shows that
the idea scales to more than 5000 relations, but
that additional techniques, such as dynamic lexi-
con learning, are necessary to deal with sparsity.
Extraction with lexicons While lexicons have
been commonly used for IE (Cohen and Sarawagi,
2004; Agichtein and Ganti, 2004; Bellare and Mc-
Callum, 2007), many approaches assume that lex-
icons are clean and are supplied by a user before
training. Other approaches (Talukdar et al, 2006;
Miller et al, 2004; Riloff, 1993) learn lexicons
automatically from distributional patterns in text.
(Wang et al, 2009) learns lexicons from Web lists
for query tagging. LUCHS differs from these ap-
proaches in that it is not limited to a small set of
well-defined relations. Rather than creating large
lexicons of common entities, LUCHS attempts to
efficiently instantiate a series of lexicons from a
small set of seeds to bias extractors of sparse at-
tributes. Crucual to LUCHS?s different setting is
also the need to avoid overfitting.
Set expansion A large amount of work has
looked at automatically generating sets of related
items. Starting with a set of seed terms, (Etzioni
et al, 2004) extract lists by learning wrappers for
Web pages containing those terms. (Wang and Co-
hen, 2007; Wang and Cohen, 2008) extend the
idea, computing term relatedness through a ran-
dom walk algorithm that takes into account seeds,
documents, wrappers and mentions. Other ap-
proaches include Bayesian methods (Ghahramani
and Heller, 2005) and graph label propagation al-
gorithms (Talukdar et al, 2008; Bengio et al,
2006). The goal of set expansion techniques is
to generate high precision sets of related items;
hence, these techniques are evaluated based on
lexicon precision and recall. For LUCHS, which is
evaluated based on the quality of an extractor us-
ing the lexicons, lexicon precision is not important
? as long as it does not confuse the extractor.
7 Future Work
We envision a Web-scale machine reading system
which simultaneously learns ontologies and ex-
tractors, and we believe that LUCHS?s approach
of leveraging noisy semi-structured information
(such as lists or formatting templates) is a key to-
wards this goal. For future work, we plan to en-
hance LUCHS in two major ways.
First, we note that a big weakness is that the
system currently only works for Wikipedia pages.
293
For example, LUCHS assumes that each page cor-
responds to exactly one schema and that the sub-
ject of relations on a page are the same. Also,
LUCHS makes predictions on a token basis, thus
sometimes failing to recognize larger segments.
To remove these limitations we plan to add a
deeper linguistic analysis, making better use of
parse and dependency information and including
coreference resolution. We also plan to employ
relation-independent Open extraction techniques,
e.g. as suggested in (Wu and Weld, 2008) (retrain-
ing).
Second, we note that LUCHS?s performance
may benefit substantially from an attribute ontol-
ogy. As we showed in Section 5.6, LUCHS?s cur-
rent extractors can also greatly facilitate learning
a full attribute ontology. We therefore plan to in-
terleave extractor learning and ontology inference,
hence jointly learning ontology and extractors.
8 Conclusion
Many researchers are trying to use IE to cre-
ate large-scale knowledge bases from natural lan-
guage text on the Web, but existing relation-
specific techniques do not scale to the thousands
of relations encoded in Web text ? while relation-
independent techniques suffer from lower preci-
sion and recall, and do not canonicalize the rela-
tions. This paper shows that ? with new techniques
? self-supervised learning of relation-specific ex-
tractors from Wikipedia infoboxes does scale.
In particular, we present LUCHS, a self-
supervised IE system capable of learning more
than an order of magnitude more relation-specific
extractors than previous systems. LUCHS uses
dynamic lexicon features that enable hyper-
lexicalized extractors which cope effectively with
sparse training data. We show an overall perfor-
mance of 61% F1 score, and present experiments
evaluating LUCHS?s individual components.
Datasets generated in this work are available to
the community7.
Acknowledgments
We thank Jesse Davis, Oren Etzioni, Andrey Kolobov,
Mausam, Fei Wu, and the anonymous reviewers for helpful
comments and suggestions.
This material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by the Air
Force Research Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of
7http://www.cs.washington.edu/ai/iwp
the author(s) and do not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Eugene Agichtein and Venkatesh Ganti. 2004. Mining refer-
ence tables for automatic text segmentation. In Proceed-
ings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2004),
pages 20?29.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007.
Dbpedia: A nucleus for a web of open data. In Proceed-
ings of the 6th International Semantic Web Conference
and 2nd Asian Semantic Web Conference (ISWC/ASWC-
2007), pages 722?735.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of the
20th International Joint Conference on Artificial Intelli-
gence (IJCAI-2007), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learning ex-
tractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration
on the Web.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion. In
Olivier Chapelle, Bernhard Scho?lkopf, and Alexander
Zien, editors, Semi-Supervised Learning, pages 193?216.
MIT Press.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eu-
gene Wu, and Yang Zhang. 2008. Webtables: exploring
the power of tables on the web. Proceedings of the In-
ternational Conference on Very Large Databases (VLDB-
2008), 1(1):538?549.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. 2009a. Coupling semi-supervised
learning of categories and relations. In NAACL HLT 2009
Workskop on Semi-supervised Learning for Natural Lan-
guage Processing.
Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009b.
Learning a named entity tagger from gazetteers with the
partial perceptron. In AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
William W. Cohen and Sunita Sarawagi. 2004. Exploiting
dictionaries in named entity extraction: combining semi-
markov extraction processes and data integration methods.
In Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD-2004), pages 89?98.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information from
text sources. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology
(ISMB-1999), pages 77?86.
294
Benjamin Van Durme and Marius Pasca. 2008. Finding cars,
goddesses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extraction.
In Proceedings of the Twenty-Third AAAI Conference on
Artificial Intelligence (AAAI-2008), pages 1243?1248.
Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004. Methods for domain-
independent information extraction from the web: An ex-
perimental comparison. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
2004), pages 391?398.
Dayne Freitag. 1998. Toward general-purpose learning for
information extraction. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
404?408. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Neural Information Processing Systems
(NIPS-2005).
Lynette Hirschman, Alexander A. Morgan, and Alexander S.
Yeh. 2002. Rutabaga by any other name: extracting
biological names. Journal of Biomedical Informatics,
35(4):247?259.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning (ICML-2001), pages 282?289.
Marie-Catherine De Marneffe, Bill Maccartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
fifth international conference on Language Resources and
Evaluation (LREC-2006).
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL-2004).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky.
2009. Distant supervision for relation extraction without
labeled data. In The Annual Meeting of the Association
for Computational Linguistics (ACL-2009).
Marius Pasca. 2009. Outclassing wikipedia in open-domain
information extraction: Weakly-supervised acquisition of
attributes over conceptual hierarchies. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-2009),
pages 639?647.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proceedings of the
11th National Conference on Artificial Intelligence (AAAI-
1993), pages 811?816.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago: A large ontology from wikipedia and word-
net. Elsevier Journal of Web Semantics, 6(3):203?217.
Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum.
2009. Sofie: A self-organizing framework for informa-
tion extraction. In Proceedings of the 18th International
Conference on World Wide Web (WWW-2009).
Partha Pratim Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern induction
method for named entity extraction. In The Tenth Confer-
ence on Natural Language Learning (CoNLL-X-2006).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of labeled
class instances using graph random walks. In EMNLP,
pages 582?590.
Richard C. Wang and William W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. In Proceedings of the 7th IEEE International Con-
ference on Data Mining (ICDM-2007), pages 342?350.
Richard C. Wang and William W. Cohen. 2008. Iterative set
expansion of named entities using the web. In Proceed-
ings of the 8th IEEE International Conference on Data
Mining (ICDM-2008).
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of the 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC-2007), pages 580?594.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero.
2009. Semi-supervised acquisition of semantic classes ?
from the web and for the web. In International Confer-
ence on Information and Knowledge Management (CIKM-
2009), pages 37?46.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Management
(CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using wikipedia. In The Annual Meeting of the
Association for Computational Linguistics (ACL-2010).
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. In-
formation extraction from wikipedia: moving down the
long tail. In Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2008), pages 731?739.
295
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541?550,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge-Based Weak Supervision for Information Extraction
of Overlapping Relations
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.edu
Abstract
Information extraction (IE) holds the promise
of generating a large-scale knowledge
base from the Web?s natural language text.
Knowledge-based weak supervision, using
structured data to heuristically label a training
corpus, works towards this goal by enabling
the automated learning of a potentially
unbounded number of relation extractors.
Recently, researchers have developed multi-
instance learning algorithms to combat the
noisy training data that can come from
heuristic labeling, but their models assume
relations are disjoint ? for example they
cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for
multi-instance learning with overlapping re-
lations that combines a sentence-level extrac-
tion model with a simple, corpus-level compo-
nent for aggregating the individual facts. We
apply our model to learn extractors for NY
Times text using weak supervision from Free-
base. Experiments show that the approach
runs quickly and yields surprising gains in
accuracy, at both the aggregate and sentence
level.
1 Introduction
Information-extraction (IE), the process of generat-
ing relational data from natural-language text, con-
tinues to gain attention. Many researchers dream of
creating a large repository of high-quality extracted
tuples, arguing that such a knowledge base could
benefit many important tasks such as question an-
swering and summarization. Most approaches to IE
use supervised learning of relation-specific exam-
ples, which can achieve high precision and recall.
Unfortunately, however, fully supervised methods
are limited by the availability of training data and are
unlikely to scale to the thousands of relations found
on the Web.
A more promising approach, often called ?weak?
or ?distant? supervision, creates its own training
data by heuristically matching the contents of a
database to corresponding text (Craven and Kum-
lien, 1999). For example, suppose that r(e1, e2) =
Founded(Jobs,Apple) is a ground tuple in the
database and s =?Steve Jobs founded Apple, Inc.?
is a sentence containing synonyms for both e1 =
Jobs and e2 = Apple, then s may be a natural
language expression of the fact that r(e1, e2) holds
and could be a useful training example.
While weak supervision works well when the tex-
tual corpus is tightly aligned to the database con-
tents (e.g., matching Wikipedia infoboxes to as-
sociated articles (Hoffmann et al, 2010)), Riedel
et al (2010) observe that the heuristic leads to
noisy data and poor extraction performance when
the method is applied more broadly (e.g., matching
Freebase records to NY Times articles). To fix
this problem they cast weak supervision as a form of
multi-instance learning, assuming only that at least
one of the sentences containing e1 and e2 are ex-
pressing r(e1, e2), and their method yields a sub-
stantial improvement in extraction performance.
However, Riedel et al?s model (like that of
previous systems (Mintz et al, 2009)) assumes
that relations do not overlap ? there cannot
exist two facts r(e1, e2) and q(e1, e2) that are
both true for any pair of entities, e1 and e2.
Unfortunately, this assumption is often violated;
541
for example both Founded(Jobs, Apple) and
CEO-of(Jobs, Apple) are clearly true. In-
deed, 18.3% of the weak supervision facts in Free-
base that match sentences in the NY Times 2007 cor-
pus have overlapping relations.
This paper presents MULTIR, a novel model of
weak supervision that makes the following contri-
butions:
? MULTIR introduces a probabilistic, graphical
model of multi-instance learning which handles
overlapping relations.
? MULTIR also produces accurate sentence-level
predictions, decoding individual sentences as
well as making corpus-level extractions.
? MULTIR is computationally tractable. Inference
reduces to weighted set cover, for which it uses
a greedy approximation with worst case running
time O(|R| ? |S|) where R is the set of possi-
ble relations and S is largest set of sentences for
any entity pair. In practice, MULTIR runs very
quickly.
? We present experiments showing that MULTIR
outperforms a reimplementation of Riedel
et al (2010)?s approach on both aggregate (cor-
pus as a whole) and sentential extractions.
Additional experiments characterize aspects of
MULTIR?s performance.
2 Weak Supervision from a Database
Given a corpus of text, we seek to extract facts about
entities, such as the company Apple or the city
Boston. A ground fact (or relation instance), is
an expression r(e) where r is a relation name, for
example Founded or CEO-of, and e = e1, . . . , en
is a list of entities.
An entity mention is a contiguous sequence of tex-
tual tokens denoting an entity. In this paper we as-
sume that there is an oracle which can identify all
entity mentions in a corpus, but the oracle doesn?t
normalize or disambiguate these mentions. We use
ei ? E to denote both an entity and its name (i.e.,
the tokens in its mention).
A relation mention is a sequence of text (in-
cluding one or more entity mentions) which states
that some ground fact r(e) is true. For example,
?Steve Ballmer, CEO of Microsoft, spoke recently
at CES.? contains three entity mentions as well as a
relation mention for CEO-of(Steve Ballmer,
Microsoft). In this paper we restrict our atten-
tion to binary relations. Furthermore, we assume
that both entity mentions appear as noun phrases in
a single sentence.
The task of aggregate extraction takes two inputs,
?, a set of sentences comprising the corpus, and an
extraction model; as output it should produce a set
of ground facts, I , such that each fact r(e) ? I is
expressed somewhere in the corpus.
Sentential extraction takes the same input and
likewise produces I , but in addition it also produces
a function, ? : I ? P(?), which identifies, for
each r(e) ? I , the set of sentences in ? that contain
a mention describing r(e). In general, the corpus-
level extraction problem is easier, since it need only
make aggregate predictions, perhaps using corpus-
wide statistics. In contrast, sentence-level extrac-
tion must justify each extraction with every sentence
which expresses the fact.
The knowledge-based weakly supervised learning
problem takes as input (1) ?, a training corpus, (2)
E, a set of entities mentioned in that corpus, (3) R,
a set of relation names, and (4), ?, a set of ground
facts of relations in R. As output the learner pro-
duces an extraction model.
3 Modeling Overlapping Relations
We define an undirected graphical model that al-
lows joint reasoning about aggregate (corpus-level)
and sentence-level extraction decisions. Figure 1(a)
shows the model in plate form.
3.1 Random Variables
There exists a connected component for each pair of
entities e = (e1, e2) ? E ? E that models all of
the extraction decisions for this pair. There is one
Boolean output variable Y r for each relation name
r ? R, which represents whether the ground fact
r(e) is true. Including this set of binary random
variables enables our model to extract overlapping
relations.
Let S(e1,e2) ? ? be the set of sentences which
contain mentions of both of the entities. For each
sentence xi ? S(e1,e2) there exists a latent variable
Zi which ranges over the relation names r ? R and,
542
E ? E 
? 
R 
S 
?? 
(a)
Steve Jobs was founder  
of Apple. 
Steve Jobs, Steve Wozniak and 
Ronald Wayne founded Apple. 
Steve Jobs is CEO of  
Apple. 
founder ?? founder none 
0 
?????? 
1 0 0 
?? ?? 
... 
... 
... 
????????? ????????? ????????? 
(b)
Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities
Steve Jobs, Apple.
importantly, also the distinct value none. Zi should
be assigned a value r ? R only when xi expresses
the ground fact r(e), thereby modeling sentence-
level extraction.
Figure 1(b) shows an example instantiation of the
model with four relation names and three sentences.
3.2 A Joint, Conditional Extraction Model
We use a conditional probability model that defines
a joint distribution over all of the extraction random
variables defined above. The model is undirected
and includes repeated factors for making sentence
level predictions as well as globals factors for ag-
gregating these choices.
For each entity pair e = (e1, e2), define x to
be a vector concatenating the individual sentences
xi ? S(e1,e2), Y to be vector of binary Y
r random
variables, one for each r ? R, and Z to be the vec-
tor of Zi variables, one for each sentence xi. Our
conditional extraction model is defined as follows:
p(Y = y,Z = z|x; ?)
def
=
1
Zx
?
r
?join(yr, z)
?
i
?extract(zi, xi)
where the parameter vector ? is used, below, to de-
fine the factor ?extract.
The factors ?join are deterministic OR operators
?join(yr, z)
def
=
{
1 if yr = true ? ?i : zi = r
0 otherwise
which are included to ensure that the ground fact
r(e) is predicted at the aggregate level for the as-
signment Y r = yr only if at least one of the sen-
tence level assignments Zi = zi signals a mention
of r(e).
The extraction factors ?extract are given by
?extract(zi, xi)
def
= exp
?
?
?
j
?j?j(zi, xi)
?
?
where the features ?j are sensitive to the relation
name assigned to extraction variable zi, if any, and
cues from the sentence xi. We will make use of the
Mintz et al (2009) sentence-level features in the ex-
peiments, as described in Section 7.
3.3 Discussion
This model was designed to provide a joint approach
where extraction decisions are almost entirely driven
by sentence-level reasoning. However, defining the
Y r random variables and tying them to the sentence-
level variables, Zi, provides a direct method for
modeling weak supervision. We can simply train the
model so that the Y variables match the facts in the
database, treating the Zi as hidden variables that can
take any value, as long as they produce the correct
aggregate predictions.
This approach is related to the multi-instance
learning approach of Riedel et al (2010), in that
both models include sentence-level and aggregate
random variables. However, their sentence level
variables are binary and they only have a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. Addition-
ally, their aggregate decisions make use of Mintz-
style aggregate features (Mintz et al, 2009), that col-
lect evidence from multiple sentences, while we use
543
Inputs:
(1) ?, a set of sentences,
(2)E, a set of entities mentioned in the sentences,
(3) R, a set of relation names, and
(4) ?, a database of atomic facts of the form
r(e1, e2) for r ? R and ei ? E.
Definitions:
We define the training set {(xi,yi)|i = 1 . . . n},
where i is an index corresponding to a particu-
lar entity pair (ej , ek) in ?, xi contains all of
the sentences in ? with mentions of this pair, and
yi = relVector(ej , ek).
Computation:
initialize parameter vector ?? 0
for t = 1...T do
for i = 1...n do
(y?, z?)? arg maxy,z p(y, z|xi; ?)
if y? 6= yi then
z? ? arg maxz p(z|xi,yi; ?)
?? ? + ?(xi, z?)? ?(xi, z?)
end if
end for
end for
Return ?
Figure 2: The MULTIR Learning Algorithm
only the deterministic OR nodes. Perhaps surpris-
ing, we are still able to improve performance at both
the sentential and aggregate extraction tasks.
4 Learning
We now present a multi-instance learning algo-
rithm for our weak-supervision model that treats the
sentence-level extraction random variables Zi as la-
tent, and uses facts from a database (e.g., Freebase)
as supervision for the aggregate-level variables Y r.
As input we have (1) ?, a set of sentences, (2)
E, a set of entities mentioned in the sentences, (3)
R, a set of relation names, and (4) ?, a database
of atomic facts of the form r(e1, e2) for r ? R and
ei ? E. Since we are using weak learning, the Y r
variables in Y are not directly observed, but can be
approximated from the database ?. We use a proce-
dure, relVector(e1, e2) to return a bit vector whose
jth bit is one if rj(e1, e2) ? ?. The vector does not
have a bit for the special none relation; if there is no
relation between the two entities, all bits are zero.
Finally, we can now define the training set to be
pairs {(xi,yi)|i = 1 . . . n}, where i is an index
corresponding to a particular entity pair (ej , ek), xi
contains all of the sentences with mentions of this
pair, and yi = relVector(ej , ek).
Given this form of supervision, we would like to
find the setting for ? with the highest likelihood:
O(?) =
?
i
p(yi|xi; ?) =
?
i
?
z
p(yi, z|xi; ?)
However, this objective would be difficult to op-
timize exactly, and algorithms for doing so would
be unlikely to scale to data sets of the size we con-
sider. Instead, we make two approximations, de-
scribed below, leading to a Perceptron-style addi-
tive (Collins, 2002) parameter update scheme which
has been modified to reason about hidden variables,
similar in style to the approaches of (Liang et al,
2006; Zettlemoyer and Collins, 2007), but adapted
for our specific model. This approximate algorithm
is computationally efficient and, as we will see,
works well in practice.
Our first modification is to do online learning
instead of optimizing the full objective. Define the
feature sums ?(x, z) =
?
j ?(xj , zj) which range
over the sentences, as indexed by j. Now, we can
define an update based on the gradient of the local
log likelihood for example i:
? logOi(?)
??j
= Ep(z|xi,yi;?)[?j(xi, z)]
?Ep(y,z|xi;?)[?j(xi, z)]
where the deterministic OR ?join factors ensure that
the first expectation assigns positive probability only
to assignments that produce the labeled facts yi but
that the second considers all valid sets of extractions.
Of course, these expectations themselves, espe-
cially the second one, would be difficult to com-
pute exactly. Our second modification is to do
a Viterbi approximation, by replacing the expecta-
tions with maximizations. Specifically, we compute
the most likely sentence extractions for the label
facts arg maxz p(z|xi,yi; ?) and the most likely ex-
traction for the input, without regard to the labels,
arg maxy,z p(y, z|xi; ?). We then compute the fea-
tures for these assignments and do a simple additive
update. The final algorithm is detailed in Figure 2.
544
5 Inference
To support learning, as described above, we need
to compute assignments arg maxz p(z|x,y; ?) and
arg maxy,z p(y, z|x; ?). In this section, we describe
algorithms for both cases that use the deterministic
OR nodes to simplify the required computations.
Predicting the most likely joint extraction
arg maxy,z p(y, z|x; ?) can be done efficiently
given the structure of our model. In particular, we
note that the factors ?join represent deterministic de-
pendencies between Z and Y, which when satisfied
do not affect the probability of the solution. It is thus
sufficient to independently compute an assignment
for each sentence-level extraction variable Zi, ignor-
ing the deterministic dependencies. The optimal set-
ting for the aggregate variables Y is then simply the
assignment that is consistent with these extractions.
The time complexity is O(|R| ? |S|).
Predicting sentence level extractions given weak
supervision facts, arg maxz p(z|x,y; ?), is more
challenging. We start by computing extraction
scores ?extract(xi, zi) for each possible extraction as-
signment Zi = zi at each sentence xi ? S, and
storing the values in a dynamic programming table.
Next, we must find the most likely assignment z that
respects our output variables y. It turns out that
this problem is a variant of the weighted, edge-cover
problem, for which there exist polynomial time op-
timal solutions.
Let G = (E ,V = VS ? Vy) be a complete
weighted bipartite graph with one node vSi ? V
S for
each sentence xi ? S and one node v
y
r ? Vy for each
relation r ? R where yr = 1. The edge weights are
given by c((vSi , v
y
r ))
def
= ?extract(xi, zi). Our goal is
to select a subset of the edges which maximizes the
sum of their weights, subject to each node vSi ? V
S
being incident to exactly one edge, and each node
vyr ? Vy being incident to at least one edge.
Exact Solution An exact solution can be obtained
by first computing the maximum weighted bipartite
matching, and adding edges to nodes which are not
incident to an edge. This can be computed in time
O(|V|(|E| + |V| log |V|)), which we can rewrite as
O((|R|+ |S|)(|R||S|+ (|R|+ |S|) log(|R|+ |S|))).
Approximate Solution An approximate solution
can be obtained by iterating over the nodes in Vy,
????????  ???????????  
?? ?? ?? 
????????????? ???????????????? 
????? 
Figure 3: Inference of arg maxz p(Z = z|x,y) requires
solving a weighted, edge-cover problem.
and each time adding the highest weight incident
edge whose addition doesn?t violate a constraint.
The running time is O(|R||S|). This greedy search
guarantees each fact is extracted at least once and
allows any additional extractions that increase the
overall probability of the assignment. Given the
computational advantage, we use it in all of the ex-
perimental evaluations.
6 Experimental Setup
We follow the approach of Riedel et al (2010) for
generating weak supervision data, computing fea-
tures, and evaluating aggregate extraction. We also
introduce new metrics for measuring sentential ex-
traction performance, both relation-independent and
relation-specific.
6.1 Data Generation
We used the same data sets as Riedel et al (2010)
for weak supervision. The data was first tagged with
the Stanford NER system (Finkel et al, 2005) and
then entity mentions were found by collecting each
continuous phrase where words were tagged iden-
tically (i.e., as a person, location, or organization).
Finally, these phrases were matched to the names of
Freebase entities.
Given the set of matches, define ? to be set of NY
Times sentences with two matched phrases, E to be
the set of Freebase entities which were mentioned in
one or more sentences, ? to be the set of Freebase
facts whose arguments, e1 and e2 were mentioned in
a sentence in ?, and R to be set of relations names
used in the facts of ?. These sets define the weak
supervision data.
6.2 Features and Initialization
We use the set of sentence-level features described
by Riedel et al (2010), which were originally de-
545
veloped by Mintz et al (2009). These include in-
dicators for various lexical, part of speech, named
entity, and dependency tree path properties of entity
mentions in specific sentences, as computed with the
Malt dependency parser (Nivre and Nilsson, 2004)
and OpenNLP POS tagger1. However, unlike the
previous work, we did not make use of any features
that explicitly aggregate these properties across mul-
tiple mention instances.
The MULTIR algorithm has a single parameter T ,
the number of training iterations, that must be spec-
ified manually. We used T = 50 iterations, which
performed best in development experiments.
6.3 Evaluation Metrics
Evaluation is challenging, since only a small per-
centage (approximately 3%) of sentences match
facts in Freebase, and the number of matches is
highly unbalanced across relations, as we will see
in more detail later. We use the following metrics.
Aggregate Extraction Let ?e be the set of ex-
tracted relations for any of the systems; we com-
pute aggregate precision and recall by comparing
?e with ?. This metric is easily computed but un-
derestimates extraction accuracy because Freebase
is incomplete and some true relations in ?e will be
marked wrong.
Sentential Extraction Let Se be the sentences
where some system extracted a relation and SF be
the sentences that match the arguments of a fact in
?. We manually compute sentential extraction ac-
curacy by sampling a set of 1000 sentences from
Se ? SF and manually labeling the correct extrac-
tion decision, either a relation r ? R or none. We
then report precision and recall for each system on
this set of sampled sentences. These results provide
a good approximation to the true precision but can
overestimate the actual recall, since we did not man-
ually check the much larger set of sentences where
no approach predicted extractions.
6.4 Precision / Recall Curves
To compute precision / recall curves for the tasks,
we ranked the MULTIR extractions as follows. For
sentence-level evaluations, we ordered according to
1http://opennlp.sourceforge.net/
Recall
Precision
0.00 0.05 0.10 0.15 0.20 0.25 0.30
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
Riedel et al, 2010
MULTIR
Figure 4: Aggregate extraction precision / recall curves
for Riedel et al (2010), a reimplementation of that ap-
proach (SOLOR), and our algorithm (MULTIR).
the extraction factor score ?extract(zi, xi). For aggre-
gate comparisons, we set the score for an extraction
Y r = true to be the max of the extraction factor
scores for the sentences where r was extracted.
7 Experiments
To evaluate our algorithm, we first compare it to an
existing approach for using multi-instance learning
with weak supervision (Riedel et al, 2010), using
the same data and features. We report both aggregate
extraction and sentential extraction results. We then
investigate relation-specific performance of our sys-
tem. Finally, we report running time comparisons.
7.1 Aggregate Extraction
Figure 4 shows approximate precision / recall curves
for three systems computed with aggregate metrics
(Section 6.3) that test how closely the extractions
match the facts in Freebase. The systems include the
original results reported by Riedel et al (2010) as
well as our new model (MULTIR). We also compare
with SOLOR, a reimplementation of their algorithm,
which we built in Factorie (McCallum et al, 2009),
and will use later to evaluate sentential extraction.
MULTIR achieves competitive or higher preci-
sion over all ranges of recall, with the exception
of the very low recall range of approximately 0-
1%. It also significantly extends the highest recall
achieved, from 20% to 25%, with little loss in preci-
sion. To investigate the low precision in the 0-1% re-
call range, we manually checked the ten highest con-
546
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
MULTIR
Figure 5: Sentential extraction precision / recall curves
for MULTIR and SOLOR.
fidence extractions produced by MULTIR that were
marked wrong. We found that all ten were true facts
that were simply missing from Freebase. A manual
evaluation, as we perform next for sentential extrac-
tion, would remove this dip.
7.2 Sentential Extraction
Although their model includes variables to model
sentential extraction, Riedel et al (2010) did not re-
port sentence level performance. To generate the
precision / recall curve we used the joint model as-
signment score for each of the sentences that con-
tributed to the aggregate extraction decision.
Figure 4 shows approximate precision / recall
curves for MULTIR and SOLOR computed against
manually generated sentence labels, as defined in
Section 6.3. MULTIR achieves significantly higher
recall with a consistently high level of precision. At
the highest recall point, MULTIR reaches 72.4% pre-
cision and 51.9% recall, for an F1 score of 60.5%.
7.3 Relation-Specific Performance
Since the data contains an unbalanced number of in-
stances of each relation, we also report precision and
recall for each of the ten most frequent relations. Let
SMr be the sentences where MULTIR extracted an
instance of relation r ? R, and let SFr be the sen-
tences that match the arguments of a fact about re-
lation r in ?. For each r, we sample 100 sentences
from both SMr and S
F
r and manually check accu-
racy. To estimate precision P?r we compute the ratio
of true relation mentions in SMr , and to estimate re-
call R?r we take the ratio of true relation mentions in
SFr which are returned by our system.
Table 1 presents this approximate precision and
recall for MULTIR on each of the relations, along
with statistics we computed to measure the qual-
ity of the weak supervision. Precision is high for
the majority of relations but recall is consistently
lower. We also see that the Freebase matches are
highly skewed in quantity and can be low quality for
some relations, with very few of them actually cor-
responding to true extractions. The approach gener-
ally performs best on the relations with a sufficiently
large number of true matches, in many cases even
achieving precision that outperforms the accuracy of
the heuristic matches, at reasonable recall levels.
7.4 Overlapping Relations
Table 1 also highlights some of the effects of learn-
ing with overlapping relations. For example, in the
data, almost all of the matches for the administra-
tive divisions relation overlap with the contains re-
lation, because they both model relationships for a
pair of locations. Since, in general, sentences are
much more likely to describe a contains relation, this
overlap leads to a situation were almost none of the
administrate division matches are true ones, and we
cannot accurately learn an extractor. However, we
can still learn to accurately extract the contains rela-
tion, despite the distracting matches. Similarly, the
place of birth and place of death relations tend to
overlap, since it is often the case that people are born
and die in the same city. In both cases, the precision
outperforms the labeling accuracy and the recall is
relatively high.
To measure the impact of modeling overlapping
relations, we also evaluated a simple, restricted
baseline. Instead of labeling each entity pair with
the set of all true Freebase facts, we created a dataset
where each true relation was used to create a dif-
ferent training example. Training MULTIR on this
data simulates effects of conflicting supervision that
can come from not modeling overlaps. On average
across relations, precision increases 12 points but re-
call drops 26 points, for an overall reduction in F1
score from 60.5% to 40.3%.
7.5 Running Time
One final advantage of our model is the mod-
est running time. Our implementation of the
547
Relation
Freebase Matches MULTIR
#sents % true P? R?
/business/person/company 302 89.0 100.0 25.8
/people/person/place lived 450 60.0 80.0 6.7
/location/location/contains 2793 51.0 100.0 56.0
/business/company/founders 95 48.4 71.4 10.9
/people/person/nationality 723 41.0 85.7 15.0
/location/neighborhood/neighborhood of 68 39.7 100.0 11.1
/people/person/children 30 80.0 100.0 8.3
/people/deceased person/place of death 68 22.1 100.0 20.0
/people/person/place of birth 162 12.0 100.0 33.0
/location/country/administrative divisions 424 0.2 N/A 0.0
Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy
(% true) of matches between sentences and facts in Freebase.
Riedel et al (2010) approach required approxi-
mately 6 hours to train on NY Times 05-06 and 4
hours to test on the NY Times 07, each without pre-
processing. Although they do sampling for infer-
ence, the global aggregation variables require rea-
soning about an exponentially large (in the number
of sentences) sample space.
In contrast, our approach required approximately
one minute to train and less than one second to test,
on the same data. This advantage comes from the
decomposition that is possible with the determinis-
tic OR aggregation variables. For test, we simply
consider each sentence in isolation and during train-
ing our approximation to the weighted assignment
problem is linear in the number of sentences.
7.6 Discussion
The sentential extraction results demonstrates the
advantages of learning a model that is primarily
driven by sentence-level features. Although previ-
ous approaches have used more sophisticated fea-
tures for aggregating the evidence from individual
sentences, we demonstrate that aggregating strong
sentence-level evidence with a simple deterministic
OR that models overlapping relations is more effec-
tive, and also enables training of a sentence extractor
that runs with no aggregate information.
While the Riedel et al approach does include a
model of which sentences express relations, it makes
significant use of aggregate features that are primar-
ily designed to do entity-level relation predictions
and has a less detailed model of extractions at the
individual sentence level. Perhaps surprisingly, our
model is able to do better at both the sentential and
aggregate levels.
8 Related Work
Supervised-learning approaches to IE were intro-
duced in (Soderland et al, 1995) and are too nu-
merous to summarize here. While they offer high
precision and recall, these methods are unlikely to
scale to the thousands of relations found in text on
the Web. Open IE systems, which perform self-
supervised learning of relation-independent extrac-
tors (e.g., Preemptive IE (Shinyama and Sekine,
2006), TEXTRUNNER (Banko et al, 2007; Banko
and Etzioni, 2008) and WOE (Wu and Weld, 2010))
can scale to millions of documents, but don?t output
canonicalized relations.
8.1 Weak Supervision
Weak supervision (also known as distant- or self su-
pervision) refers to a broad class of methods, but
we focus on the increasingly-popular idea of using
a store of structured data to heuristicaly label a tex-
tual corpus. Craven and Kumlien (1999) introduced
the idea by matching the Yeast Protein Database
(YPD) to the abstracts of papers in PubMed and
training a naive-Bayes extractor. Bellare and Mc-
Callum (2007) used a database of BibTex records
to train a CRF extractor on 12 bibliographic rela-
tions. The KYLIN system aplied weak supervision
to learn relations from Wikipedia, treating infoboxes
as the associated database (Wu and Weld, 2007);
Wu et al (2008) extended the system to use smooth-
ing over an automatically generated infobox taxon-
548
omy. Mintz et al (2009) used Freebase facts to train
100 relational extractors on Wikipedia. Hoffmann
et al (2010) describe a system similar to KYLIN,
but which dynamically generates lexicons in order
to handle sparse data, learning over 5000 Infobox
relations with an average F1 score of 61%. Yao
et al (2010) perform weak supervision, while using
selectional preference constraints to a jointly reason
about entity types.
The NELL system (Carlson et al, 2010) can also
be viewed as performing weak supervision. Its ini-
tial knowledge consists of a selectional preference
constraint and 20 ground fact seeds. NELL then
matches entity pairs from the seeds to a Web cor-
pus, but instead of learning a probabilistic model,
it bootstraps a set of extraction patterns using semi-
supervised methods for multitask learning.
8.2 Multi-Instance Learning
Multi-instance learning was introduced in order to
combat the problem of ambiguously-labeled train-
ing data when predicting the activity of differ-
ent drugs (Dietterich et al, 1997). Bunescu and
Mooney (2007) connect weak supervision with
multi-instance learning and extend their relational
extraction kernel to this context.
Riedel et al (2010), combine weak supervision
and multi-instance learning in a more sophisticated
manner, training a graphical model, which assumes
only that at least one of the matches between the
arguments of a Freebase fact and sentences in the
corpus is a true relational mention. Our model may
be seen as an extension of theirs, since both models
include sentence-level and aggregate random vari-
ables. However, Riedel et al have only a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. We have
discussed the comparison in more detail throughout
the paper, including in the model formulation sec-
tion and experiments.
9 Conclusion
We argue that weak supervision is promising method
for scaling information extraction to the level where
it can handle the myriad, different relations on the
Web. By using the contents of a database to heuris-
tically label a training corpus, we may be able to
automatically learn a nearly unbounded number of
relational extractors. Since the processs of match-
ing database tuples to sentences is inherently heuris-
tic, researchers have proposed multi-instance learn-
ing algorithms as a means for coping with the result-
ing noisy data. Unfortunately, previous approaches
assume that all relations are disjoint ? for exam-
ple they cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple), because
two relations are not allowed to have the same argu-
ments.
This paper presents a novel approach for multi-
instance learning with overlapping relations that
combines a sentence-level extraction model with a
simple, corpus-level component for aggregating the
individual facts. We apply our model to learn extrac-
tors for NY Times text using weak supervision from
Freebase. Experiments show improvements for both
sentential and aggregate (corpus level) extraction,
and demonstrate that the approach is computation-
ally efficient.
Our early progress suggests many interesting di-
rections. By joining two or more Freebase tables,
we can generate many more matches and learn more
relations. We also wish to refine our model in order
to improve precision. For example, we would like
to add type reasoning about entities and selectional
preference constraints for relations. Finally, we are
also interested in applying the overall learning ap-
proaches to other tasks that could be modeled with
weak supervision, such as coreference and named
entity classification.
The source code of our system, its out-
put, and all data annotations are available at
http://cs.uw.edu/homes/raphaelh/mr.
Acknowledgments
We thank Sebastian Riedel and Limin Yao for shar-
ing their data and providing valuable advice. This
material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
549
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-08), pages
28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth International Workshop on Infor-
mation Integration on the Web.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI-10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77?86.
Thomas G. Dietterich, Richard H. Lathrop, and Toma?s
Lozano-Pe?rez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial Intel-
ligence, 89:31?71, January.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
05), pages 363?370.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
286?295.
Percy Liang, A. Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In International Conference on
Computational Linguistics and Association for Com-
putational Linguistics (COLING/ACL).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Neural Information
Processing Systems Conference (NIPS).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computation Linguistics (HLT-
NAACL-06).
Stephen Soderland, David Fisher, Jonathan Aseltine, and
Wendy G. Lehnert. 1995. Crystal: Inducing a concep-
tual dictionary. In Proceedings of the Fourteenth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-1995), pages 1314?1321.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In The Annual Meeting of
the Association for Computational Linguistics (ACL-
2010), pages 118?127.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-2007).
550
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665?670,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Filling Knowledge Base Gaps for Distant Supervision
of Relation Extraction
Wei Xu+ Raphael Hoffmann? Le Zhao#,* Ralph Grishman+
+New York University, New York, NY, USA
{xuwei, grishman}@cs.nyu.edu
?University of Washington, Seattle, WA, USA
raphaelh@cs.washington.edu
#Google Inc., Mountain View, CA, USA
lezhao@google.com
Abstract
Distant supervision has attracted recent in-
terest for training information extraction
systems because it does not require any
human annotation but rather employs ex-
isting knowledge bases to heuristically la-
bel a training corpus. However, previous
work has failed to address the problem
of false negative training examples misla-
beled due to the incompleteness of knowl-
edge bases. To tackle this problem, we
propose a simple yet novel framework that
combines a passage retrieval model using
coarse features into a state-of-the-art rela-
tion extractor using multi-instance learn-
ing with fine features. We adapt the in-
formation retrieval technique of pseudo-
relevance feedback to expand knowledge
bases, assuming entity pairs in top-ranked
passages are more likely to express a rela-
tion. Our proposed technique significantly
improves the quality of distantly super-
vised relation extraction, boosting recall
from 47.7% to 61.2% with a consistently
high level of precision of around 93% in
the experiments.
1 Introduction
A recent approach for training information ex-
traction systems is distant supervision, which ex-
ploits existing knowledge bases instead of anno-
tated texts as the source of supervision (Craven
and Kumlien, 1999; Mintz et al, 2009; Nguyen
and Moschitti, 2011). To combat the noisy train-
ing data produced by heuristic labeling in distant
supervision, researchers (Bunescu and Mooney,
2007; Riedel et al, 2010; Hoffmann et al, 2011;
Surdeanu et al, 2012) exploited multi-instance
*This work was done while Le Zhao was at Carnegie
Mellon University.
learning models. Only a few studies have directly
examined the influence of the quality of the train-
ing data and attempted to enhance it (Sun et al,
2011; Wang et al, 2011; Takamatsu et al, 2012).
However, their methods are handicapped by the
built-in assumption that a sentence does not ex-
press a relation unless it mentions two entities
which participate in the relation in the knowledge
base, leading to false negatives.
aligned 
mentions 
true 
 mentions 5.5% 2.7% 1.7% false  negatives false  
positives 
Figure 1: Noisy training data in distant supervi-
sion
In reality, knowledge bases are often incom-
plete, giving rise to numerous false negatives in
the training data. We sampled 1834 sentences that
contain two entities in the New York Times 2006
corpus and manually evaluated whether they ex-
press any of a set of 50 common Freebase1 rela-
tions. As shown in Figure 1, of the 133 (7.3%)
sentences that truly express one of these relations,
only 32 (1.7%) are covered by Freebase, leaving
101 (5.5%) false negatives. Even for one of the
most complete relations in Freebase, Employee-of
(with more than 100,000 entity pairs), 6 out of 27
sentences with the pattern ?PERSON executive of
ORGANIZATION? contain a fact that is not in-
cluded in Freebase and are thus mislabeled as neg-
ative. These mislabelings dilute the discriminative
capability of useful features and confuse the mod-
els. In this paper, we will show how reducing this
source of noise can significantly improve the per-
formance of distant supervision. In fact, our sys-
tem corrects the relation labels of the above 6 sen-
tences before training the relation extractor.
1http://www.freebase.com
665
 
D ocuments  Knowledge 
Base  
Relation 
Extractor  
Passage  
Retriever  
? 
? 
? 
Pseudo - relevant 
Relation Instances  
? 
? 
? 
Figure 2: Overall system architecture: The system
(1) matches relation instances to sentences and (2)
learns a passage retrieval model to (3) provide rel-
evance feedback on sentences; Relevant sentences
(4) yield new relation instances which are added
to the knowledge base; Finally, instances are again
(5) matched to sentences to (6) create training data
for relation extraction.
Encouraged by the recent success of simple
methods for coreference resolution (Raghunathan
et al, 2010) and inspired by pseudo-relevance
feedback (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Matveeva et al, 2006; Cao et al,
2008) in the field of information retrieval, which
expands or reformulates query terms based on
the highest ranked documents of an initial query,
we propose to increase the quality and quantity
of training data generated by distant supervision
for information extraction task using pseudo feed-
back. As shown in Figure 2, we expand an orig-
inal knowledge base with possibly missing rela-
tion instances with information from the highest
ranked sentences returned by a passage retrieval
model (Xu et al, 2011) trained on the same data.
We use coarse features for our passage retrieval
model to aggressively expand the knowledge base
for maximum recall; at the same time, we exploit
a multi-instance learning model with fine features
for relation extraction to handle the newly intro-
duced false positives and maintain high precision.
Similar to iterative bootstrapping tech-
niques (Yangarber, 2001), this mechanism uses
the outputs of the first trained model to expand
training data for the second model, but unlike
bootstrapping it does not require iteration and
avoids the problem of semantic drift. We further
note that iterative bootstrapping over a single
distant supervision system is difficult, because
state-of-the-art systems (Surdeanu et al, 2012;
Hoffmann et al, 2011; Riedel et al, 2010; Mintz
et al, 2009), detect only few false negatives in the
training data due to their high-precision low-recall
features, which were originally proposed by Mintz
et al (2009). We present a reliable and novel way
to address these issues and achieve significant
improvement over the MULTIR system (Hoff-
mann et al, 2011), increasing recall from 47.7%
to 61.2% at comparable precision. The key to this
success is the combination of two different views
as in co-training (Blum and Mitchell, 1998):
an information extraction technique with fine
features for high precision and an information
retrieval technique with coarse features for high
recall. Our work is developed in parallel with
Min et al (2013), who take a very different
approach by adding additional latent variables to
a multi-instance multi-label model (Surdeanu et
al., 2012) to solve this same problem.
2 System Details
In this section, we first introduce some formal no-
tations then describe in detail each component of
the proposed system in Figure 2.
2.1 Definitions
A relation instance is an expression r(e1, e2)
where r is a binary relation, and e1 and e2 are
two entities having such a relation, for example
CEO-of(Tim Cook, Apple). The knowledge-based
distant supervised learning problem takes as input
(1) ?, a training corpus, (2) E, a set of entities
mentioned in that corpus, (3) R, a set of relation
names, and (4) ?, a set of ground facts of relations
in R. To generate our training data, we further as-
sume (5) T , a set of entity types, as well as type
signature r(E1, E2) for relations.
We define the positive data set POS(r) to be
the set of sentences in which any related pair
of entities of relation r (according to the knowl-
edge base) is mentioned. The negative data set
RAW (r) is the rest of the training data, which
contain two entities of the required types in the
knowledge base, e.g. one person and one or-
ganization for the CEO-of relation in Freebase.
Another negative data set with more conservative
sense NEG(r) is defined as the set of sentences
which contain the primary entity e1 (e.g. person
in any CEO-of relation in the knowledge base) and
any secondary entity e2 of required type (e.g. or-
ganization for the CEO-of relation) but the relation
does not hold for this pair of entities in the knowl-
edge base.
666
2.2 Distantly Supervised Passage Retrieval
We extend the learning-to-rank techniques (Liu,
2011) to distant supervision setting (Xu et al,
2011) to create a robust passage retrieval system.
While relation extraction systems exploit rich and
complex features that are necessary to extract the
exact relation (Mintz et al, 2009; Riedel et al,
2010; Hoffmann et al, 2011), passage retrieval
components use coarse features in order to provide
different and complementary feedback to informa-
tion extraction models.
We exploit two types of lexical features: Bag-
Of-Words and Word-Position. The two types of
simple binary features are shown in the following
example:
Sentence: Apple founder Steve Jobs died.
Target (Primary) entity: Steve Jobs
Bag-Of-Word features: ?apple? ?founder? ?died? ?.?
Word-Position features: ?apple:-2? ?founder:-1?
?died:+1? ?.:+2?
For each relation r, we assume each sentence
has a binary relevance label to form distantly su-
pervised training data: sentences in POS(r) are
relevant and sentences in NEG(r) are irrelevant.
As a pointwise learning-to-rank approach (Nallap-
ati, 2004), the probabilities of relevance estimated
by SVMs (Platt and others, 1999) are used for
ranking all the sentences in the original training
corpus for each relation respectively. We use Lib-
SVM 2 (Chang and Lin, 2011) in our implementa-
tion.
2.3 Psuedo-relevance Relation Feedback
In the field of information retrieval, pseudo-
relevance feedback assumes that the top-ranked
documents from an initial retrieval are likely rel-
evant, and extracts relevant terms to expand the
original query (Xu and Croft, 1996; Lavrenko and
Croft, 2001; Cao et al, 2008). Analogously, our
assumption is that entity pairs that appear in more
relevant and more sentences are more likely to
express the relation, and can be used to expand
knowledge base and reduce false negative noise in
the training data for information extraction. We
identify the most likely relevant entity pairs as fol-
lows:
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
initialize ?? ?? ?
for each relation type r ? R do
learn a passage (sentence) retrieval model L(r)
using coarse features and POS(r)?NEG(r)
as training data
score the sentences in the RAW (r) by L(r)
score the entity pairs according to the scores
of sentences they are involved in
select the top ranked pairs of entities, then add
the relation r to their label in ??
end for
We select the entity pairs whose average score
of the sentences they are involved in is greater
than p, where p is a parameter tuned on develop-
ment data.3 The relation extraction model is then
trained using (?, E,R,??) with a more complete
database than the original knowledge base ?.
2.4 Distantly Supervised Relation Extraction
We use a state-of-the-art open-source system,
MULTIR (Hoffmann et al, 2011), as the rela-
tion extraction component. MULTIR is based
on multi-instance learning, which assumes that
at least one sentence of those matching a given
entity-pair contains the relation of interest (Riedel
et al, 2010) in the given knowledge base to tol-
erate false positive noise in the training data and
superior than previous models (Riedel et al, 2010;
Mintz et al, 2009) by allowing overlapping rela-
tions. MULTIR uses features which are based on
Mintz et al (2009) and consist of conjunctions of
named entity tags, syntactic dependency paths be-
tween arguments, and lexical information.
3 Experiments
For evaluating extraction accuracy, we follow the
experimental setup of Hoffmann et al (2011), and
use their implementation of MULTIR4 with 50
training iterations as our baseline. Our complete
system, which we call IRMIE, combines our pas-
sage retrieval component with MULTIR. We use
the same datasets as in Hoffmann et al (2011) and
Riedel et al (2010), which include 3-years of New
York Times articles aligned with Freebase. The
sentential extraction evaluation is performed on
a small amount of manually annotated sentences,
sampled from the union of matched sentences and
3We found p = 0.5 to work well in practice.
4http://homes.cs.washington.edu/
?raphaelh/mr/
667
Test Data Set Original Test Set Corrected Test Set
P? R? F? ?F? P? R? F? ?F?
MULTIR 80.0 44.6 62.3 92.7 47.7 70.2
IRMIE 84.6 56.1 70.3 +8.0 92.6 61.2 76.9 +6.7
MULTIRLEX 91.8 43.0 67.4 79.6 57.0 68.3
IRMIELEX 89.2 52.5 70.9 +3.5 78.0 69.2 73.6 +5.3
Table 1: Overall sentential extraction performance evaluated on the original test set of Hoffmann et
al. (2011) and our corrected test set: Our proposed relevance feedback technique yields a substantial
increase in recall.
system predictions. We define Se as the sentences
where some system extracted a relation and SF
as the sentences that match the arguments of a
fact in ?. The sentential precision and recall is
computed on a randomly sampled set of sentences
from Se?SF , in which each sentence is manually
labeled whether it expresses any relation in R.
Figure 3 shows the precision/recall curves for
MULTIR with and without pseudo-relevance feed-
back computed on the test dataset of 1000 sen-
tence used by Hoffmann et al (2011). With the
pseudo-relevance feedback from passage retrieval,
IRMIE achieves significantly higher recall at a
consistently high level of precision. At the highest
recall point, IRMIE reaches 78.5% precision and
59.2% recall, for an F1 score of 68.9%.
Because the two types of lexical features used in
our passage retrieval models are not used in MUL-
TIR, we created another baseline MULTIRLEX
by adding these features into MULTIR in order
to rule out the improvement from additional infor-
mation. Note that the sentences are sampled from
the union of Freebase matches and sentences from
which some systems in Hoffmann et al (2011) ex-
tracted a relation. It underestimates the improve-
ments of the newly developed systems in this pa-
per. We therefore also created a new test set of
1000 sentences by sampling from the union of
Freebase matches and sentences where MULTIR-
LEX or IRMIELEX extracted a relation. Table 1
shows the overall precision and recall computed
against these two test datasets, with and without
adding lexical features into multi-instance learn-
ing models. The performance improvement by us-
ing pseudo-feedback is significant (p < 0.05) in
McNemar?s test for both datasets.
4 Conclusion and Perspectives
This paper proposes a novel approach to address
an overlooked problem in distant supervision: the
knowledge base is often incomplete causing nu-
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.5
0.6
0.7
0.8
0.9
1.0
IRMIE
MULTIR
Figure 3: Sentential extraction: precision/recall
curves using exact same training and test data,
features and system settings as in Hoffmann et
al. (2011).
merous false negatives in the training data. It
greatly improves a state-of-the-art multi-instance
learning model by correcting the most likely false
negatives in the training data based on the ranking
of a passage retrieval model.
In the future, we would like to more tightly inte-
grate a coarser featured estimator of sentential rel-
evance and a finer featured relation extractor, such
that a single joint-model can be learned.
Acknowledgments
Supported in part by NSF grant IIS-1018317,
the Air Force Research Laboratory (AFRL)
under prime contract number FA8750-09-C-
0181 and the Intelligence Advanced Research
Projects Activity (IARPA) via Department of In-
terior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright
annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of AFRL, IARPA,
DoI/NBC, or the U.S. Government.
668
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and
Stephen Robertson. 2008. Selecting good expan-
sion terms for pseudo-relevance feedback. In Pro-
ceedigns of the 31st Annual International ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 243?250.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In Proceedings
of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 120?127.
Tie-Yan Liu. 2011. Learning to Rank for Information
Retrieval. Springer-Verlag Berlin Heidelberg.
Irina Matveeva, Chris Burges, Timo Burkard, Andy
Laucius, and Leon Wong. 2006. High accuracy re-
trieval with multiple nested ranker. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 437?444.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2013).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 277?282.
John Platt et al 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regular-
ized likelihood methods. Advances in Large Margin
Classifiers, 10(3):61?74.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference 2011 Workshop.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Chang Wang, James Fan, Aditya Kalyanpur, and David
Gondek. 2011. Relation extraction with relation
topics. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1426?1436.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Hans-
Peter Frei, Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the 19th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 4?11. ACM.
669
Wei Xu, Ralph Grishman, and Le Zhao. 2011. Passage
retrieval for information extraction using distant su-
pervision. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 1046?1054.
Roman Yangarber. 2001. Scenario customization for
information extraction. Ph.D. thesis, Department of
Computer Science, Graduate School of Arts and Sci-
ence, New York University.
670
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 114?121,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Supporting rule-based representations with corpus-derived lexical
information.
Annie Zaenen
Cleo Condoravdi
Daniel G. Bobrow
PARC
3333, Coyote Hill Road
Palo Alto, CA, 94304, USA
{zaenen,condorav,bobrow}@parc.com
Raphael Hoffmann
University of Washington
Box 352350
Seattle, WA, 98195, USA
raphaelh@cs.washington.edu
Abstract
The pervasive ambiguity of language al-
lows sentences that differ in just one lexi-
cal item to have rather different inference
patterns. This would be no problem if the
different lexical items fell into clearly de-
finable and easy to represent classes. But
this is not the case. To draw the correct
inferences we need to look how the refer-
ents of the lexical items in the sentence (or
broader context) interact in the described
situation. Given that the knowledge our
systems have of the represented situation
will typically be incomplete, the classifica-
tions we come up with can only be prob-
abilistic. We illustrate this problem with
an investigation of various inference pat-
terns associated with predications of the
form ?Verb from X to Y?, especially ?go
from X to Y?. We characterize the vari-
ous readings and make an initial proposal
about how to create the lexical classes that
will allow us to draw the correct inferences
in the different cases.
1 Introduction
Machine Reading requires a level of Natural
Language Processing that allows direct infer-
ences to be drawn from the processed texts.
Most heavy duty inferencing will be done by a
reasoning engine working on the output of the
linguistic analysis (with possible loops between
the two) but for this to be possible, the linguistic
analysis should deliver representations where a
certain level of disambiguation and content spec-
ification has been done. For instance, a human
will draw different conclusions from the follow-
ing two sentences about the position of the ref-
erent of the subject: ?Eric went from Paris to
Lyon? and ?The road went from Paris to Lyon?.
The first sentence implies that a person named
Eric was in Paris at some time and in Lyon at
a later time, whereas the second sentence im-
plies that a part of the road was in Paris and a
part of it was in Lyon at the same time. For the
reasoner to draw such conclusions, the linguis-
tic analysis should assign appropriate roles to
the subject argument and the from-to adjunct
or argument phrases of the verbal predicate go
so as to convey that the first sentence involves
movement, while the second involves spatial ex-
tent.
In this paper we look at a range of such in-
ferences associated with from-to phrases. We
limit ourselves to rather simple cases of the
use of from-to phrases: those that describe no
change or gradual changes in the physical world.
We show that beyond inferences about time-
dependent locations and spatial extent of partic-
ular entities, from-to phrases give rise to infer-
ences about change of an entity in some dimen-
sion (e.g. temperature or width) either through
time or through space. We first discuss the in-
ferences we would like to be able to draw, and
describe features of a representation that cap-
tures enough distinctions to enable these infer-
ences to be drawn. This allows us to isolate the
factors leading to such inferences. Finally, we
give a preliminary sketch of a corpus analysis
that would help make the required distinctions
114
and characterize appropriate lexical classes.
2 Some simple inferences
Consider the following sentences:
1. Eric went from Paris to Lyon.
2. The road went from Paris to Lyon.
3. The meeting went from 3 p.m. to 5 p.m.
4. The temperature in the room went from 20
degrees to 30 degrees from 10 to 11 a.m.
5. The temperature went from 20 to 30 de-
grees from the front to the back of the room
6. The temperature went from 20 degrees to
30 degrees.
7. The room went from 20 to 30 degrees.
As indicated above, we would like the system to
be able to conclude from (1) that Eric was in
Paris before being in Lyon, and from (2) that
one part of the road is in Paris whereas another
part is in Lyon at the same time. From (3) the
system should infer that the mentioned event,
the meeting, started at 3 p.m. (or no later than
3 p.m.) and ended at 5 p.m. (or no earlier than
5 p.m.). From (4) the system should infer that
the value of the function temperature as it ap-
plies to the room increases over the given tem-
poral span. It is worth noting at this point that
the two sets of from-to phrases in (4) play differ-
ent roles. The temporal from-to phrases specify
the relevant domain of the temporal argument of
the function, while the measure from-to phrases
specify the range of the function on the given
domain. (5) has a similar implication to that
of (4), that the temperature changes, but this
time over a spatial dimension: the temperature
is implied to vary in different parts of the room,
being 20 degrees in the front of the room and 30
degrees in the back. Again the two sets of from-
to phrases in (5) play different roles. The spa-
tial from-to phrases specify the relevant domain
of the spatial argument of the function and the
measure from-to phrases specify the range of the
function on the given domain. (6) and (7) have
similar implications to those of (4) and, in the
right context, to those of (5) but they present
challenges of their own. In (6) the temporal (or
spatial) dimension is implicit and needs to be in-
ferred. (7) requires the inference that a change
of the values of the function temperature is in-
volved.1
These examples show that sentences that have
substantially the same syntax and even use the
same main verb can exhibit very different rela-
tions between their parts. The first question we
want to address is how to explicate these dif-
ferences and the second question is how to get
from the words used in these sentences to the
information needed about their type of referent
to ensure the right interpretation in each case.
The verb ?to go? is, of course, not the only
one that exhibits this behavior. The difference
in interpretation between examples (1) and (2)
can also be found with manner-of-motion verbs
such as ?run? and ?zigzag?. Some verbs do lexi-
cally encode a particular functional dimension,
such as temperature or width. These are known
as degree achievements (Dowty, 1979; Abusch,
1986).2 Examples of degree achievements in-
clude ?widen?, ?lengthen?, ?shorten?, ?cool?, ?age?.
They exhibit similar patterns of modification
with from-to phrases as we saw above:
8. The road widens from Palo Alto to Menlo
Park.
9. The road widens from 12 to 24 feet.
Here ?widen? is interpreted statively, like ?go? in
(2), and the two sentences imply spatial change
in width, over subparts of the road. The two
from-to phrases, however, have a different func-
tion giving rise to different implications. (8) im-
plies that the road is wider in Menlo Park than it
is in Palo Alto. (9) specifies the relation between
the measures of width at two different subparts
of the road. The from-to phrases in (8) specify
1It is not clear that the change has to be in one direc-
tional in all cases:
This summer, the temperature went from 20 de-
grees to 30 degrees.
In this example, it seems that the temperature varied
from 20 to 30 degrees, not necessarily that 20 degrees
was a starting point or 30 degrees an end point. See
section 4.1 for some further discussion.
2In English most degree achievements are derived
from gradable adjectives. When this is the case, the
meaning of degree achievements and underlying adjec-
tives is systematically related, as argued in (Hay et al,
1999).
115
the domain of the spatial argument of the func-
tion width as it applies to the referent of ?the
road?. Those in (9) specify the range of the val-
ues of the function width as it applies to different
parts of the referent of ?the road?.
In what follows we will distinguish between
extent readings and change readings. Extent
readings specify, in full or in part, the tempo-
ral or spatial extent of a temporal or spatial en-
tity, as seen in (3) and (2). Change readings
specify the values of a function as applied to a
given entity through a temporal or spatial span.
The function is either determined directly by the
verb, as in (8) and (9), or by the verb in com-
bination with one of its arguments, as in (4) ?
(6), or it has to be inferred, as in (1) and (7).
3 Representing the different readings
For the sake of concreteness, in this section we
show how the distinctions discussed above are
represented and implemented in AKR, an ab-
stract knowledge representation language into
which sentences are mapped after they are
parsed in the NL system developed at PARC
(Bobrow et al, 2007). The idea behind AKR is
to canonicalize many variations of an input text
with the same underlying meaning into a more
uniform representation. This ought to make the
task of interfacing with reasoners easier.
The AKR of a sentence consists of a list of
assertions. Terms are generated for each of the
content words of a sentence, such as verbs and
nouns, and are associated with assertions about
the types of events and objects their correspond-
ing words refer to. Predicates and their argu-
ments or modifiers are related via role relations.
The inventory of roles we use extends the set
of semantic or thematic roles often assumed in
linguistic analyses and found in resources such
VerbNet or FrameNet. It includes among other
things temporal or spatial relations of inclusion,
precedence, etc.
We assume that sentences with from-to
phrases imply the existence of a path and that
the further information about the path specified
is about the ?location? of its initial and final
points. In representing such sentences a term is
created to represent a path and the path term
is linked by a role initial to the term for the
complement of from, and by a role final to the
term for the complement of to. On our analysis
then the from-to phrases are used to specify re-
strictions on the path term and do not translate
into thematic roles relating the verbal predicate
and the complement NP, such source or goal.
The path term is related to the verbal term via
different roles, depending on the type of inter-
pretation. Below is an example that shows the
role relations in AKR for sentence (1).
role(theme, go:13, Eric:7)
role(mpath, go:13, path:23)
role(initial,path:23,loc(-at-,Paris:4))
role(final,path:23,loc(-at-,Lyon:6))
role(dimension,path:23,loc)
3.1 Extent interpretations
In extent readings the subject argument denotes
an entity extended in space, as seen in (2), or a
non-punctual event, as seen in (3). The verb
itself does little work other than to signal that
the from-to phrases give information about the
spatial or temporal extent of its subject argu-
ment. The way they do that is by saying that
the given path is a spatial or temporal part of
the entity that is the referent of the subject ar-
gument. Let us start with the representation of
(3), as the representation of its meaning in our
terms is quite intuitive. Temporal paths, such
as from-to-span:11, correspond to time periods.
role(initial,time-span:11,timepoint(-at-,3pm))
role(final,time-span:11,timepoint(-at-,5pm))
role(temporalWithin,time-span:11,meeting:1)
It should now be clear that the representation
for the spatial extent reading would differ min-
imally from that of the temporal extent read-
ing: the relation between the path and the road
terms would be that of spatial inclusion and the
dimension of the path is locational.
role(initial,path:23,loc(-at-,Paris:4))
role(final,path:23,loc(-at-,Lyon:6))
role(spatialWithin,path:23,road:10)
116
3.2 Change interpretations
As discussed in section 2, change interpretations
establish a dependency between two paths which
should be represented explicitly. The paths
themselves may be specified overtly by from-to
phrases or they may be implicit. Functionally
relating two paths of this type was first dis-
cussed, to our knowledge, in (Jackendoff, 1996)
and further developed in (Gawron, 2005) and
(Gawron, 2009).
Let us consider first example (4), where the
two paths are given explicitly. (4) implies a
change in the temperature of the room over time
so the function temperature should be construed
as time-dependent. The temporal path speci-
fies the time period over which the given change
in temperature takes place; the scalar path par-
tially specifies the range of the function over the
given temporal domain. What we can conclude
for certain from (4) is that the temperature in
the room was 20 degrees at 10 a.m. and 30 de-
grees at 11 a.m. The sentence gives no specific
information about the temperature of the room
in between 10 and 11 a.m. though in this case,
given that change in temperature is continuous,
we can conclude that every degree between 20
and 30 was the temperature of the room at some
point within the relevant time period.
In order to represent the dependency between
the two paths we use a higher order predicate
path-map that specifies a function, that varies
over a range (in this case the scalar path from
20 degrees to 30 degrees) with a domain (in
this case the temporal path from 10 a.m. to 11
a.m.). More generally: the higher-order predi-
cate, path-map(F,D,R), relates a function F
and two posets D and R. The path-map relation
expresses that the image of D under F is equal
to R.3 For (4) we end up with the following rep-
resentation.
role(scale,go:5,path:4)
role(dimension, path:4,temperature)
role(initial,path:4,temperature(-at-,20 deg))
role(final,path:4,temperature(-at-,30 deg))
3Depending on what F, D and R are, this mapping
may also be order preserving, i.e. for all elements x, y in
D, if x precedes y then F(x) precedes F(y).
role(initial,time-span:11,timepoint(-at-,10am))
role(final,time-span:11,timepoint(-at-,11am))
path-map(function(temperature,room:2),
time-span:11,path:4)
The fact that path:4 is a scalar path is marked
by relating it to the verbal term via the role
scale.
The other examples discussed in section 2 re-
ceive representations based on this model. (5)
implies a change in the temperature of the room
over its spatial extent oriented from the front to
the back, so the function temperature should be
construed as location-dependent. Below we give
the assertions for the representation of (5) that
differ from those of (4). Note the additional
assertion relating the spatial path term to the
room term.
role(initial,path:11,loc(-at-,front:10))
role(final,path:11,loc(-at-,back:12))
role(spatialWithin,,path:11,room:2)
path-map(function(temperature,room:2),
path:11,path:4)
The representation of sentences with degree
achievements, such as The road widens from 12
to 24 feet from Palo Alto to Menlo Park, would
the same in all relevant respects except that the
dimension of the scalar path would be deter-
mined by the verb, in this case being width.
To derive full representations for (6) and (7)
we need to be able to infer the second and the
first argument of function, respectively. More-
over, we need to fix the dimension of the implicit
path. Generally, when only one path is specified
overtly, as in (6), (7) and (8) and (9) the exis-
tence of the other type of path is understood.
When only the range path is given, the under-
stood domain path can be either temporal or
locational.
We come now to the prototypical use of a
from-to phrase with verbs like ?go? to describe
movement whose origin is specified by the from
phrase and whose destination is specified by the
to phrase. We gave a preliminary representation
for (1) at the beginning of section 3. Missing
from that representation is the explicit link be-
tween the location of the theme argument during
117
the time of the movement. This link, of course,
can now be given in terms of the following path-
map assertion:
path-map(function(location,Eric:7),
time(go:13),path:23)
4 Which elements in the sentence
guide interpretation?
In our system roles and dimensions are intro-
duced by rules that take the output of the syn-
tactic parse of the sentence as input. The exact
form of these rules need not to concern us here.
But an important question for nlp is where the
information comes from that allows us to deter-
mine which role and dimension a path has. As
the examples show, the verb is not necessarily
the place to look: most of the examples use the
verb ?to go?.
In fact, the information can come from various
places in the sentence (or the broader textual
context: ellipsis and anaphoric relations play
their usual roles here). Moreover in some cases
information about, say, the dimension can come
from the arguments of from and to whereas in
other cases this information can come from the
verb. ?Widen? for instance imposes the width-
dimension but if we use the verb ?to go? to de-
scribe a widening event, the information about
the dimension has to come from the arguments
of from and to and the subject.
Similar problems arise with respect to the de-
termination of the roles. Example 1 and 2 seem
to have straightforward interpretations where
the path role in the first case is clearly a move-
ment path whereas in the second case we have to
do with a stative interpretation. At first blush,
it seems that this information could be straight-
forwardly lexically encoded: people move and
roads don?t. But further reflection shows that
this will not do. Take the following example:
10. The train went from one end of the station
to the other.
In this case we can have two interpretations: ei-
ther the length of the train is such that it covers
that of the whole station or the train moved from
one end of the station to the other. What is im-
portant is not an intrinsic characteristic of the
lexical item but whether it is appropriate for the
extent (length) of its referent to be measured by
the from-to phrase.
Some more or less stable relations between
syntax and semantics can help us determine
which analysis to give. For instance, the starting
and end points of movement paths and stative
locational paths are referential (in contradistinc-
tion to those of scalar paths). As such, they tend
to be expressed by proper names or by a noun
phrase with a determiner.4
Manner of motion verbs are surprisingly un-
informative: many of them can have a moving
object or a stationary object or a function such
as the temperature as their subject. The combi-
nations summarized in the following are all pos-
sible:
11. Liz/the road/the temperature
went/crawled/moved/meandered
from X to Y.
With verbs of inherent directed motion, the verb
contributes a polarity for the direction but very
little else, as example 12 illustrates:
12. Liz/the road/the temperature
descended/climbed/ascended/fell/tumbled
from X to Y.
Again whatever information there is about the
type of path or the dimension it has to come
from the subject or from the from-to arguments.
From-to arguments can give the necessary infor-
mation about the dimension (locations, money,
time, degrees) but when they are scalar or tem-
poral, the measurement units will often be omit-
ted and the theme will indicate the dimension.
Degree achievements tend to be more special-
ized. They indicate the dimension (width, tem-
perature). Lexicons can contain many of the
function names but will not help with the cases
of metonymy (where an argument is given in-
stead of the name of the function itself).
4There are, however, exceptions:
He ran from where Bill was to where the field ends.
His tattoo goes from head to toe.
The path meanders from mountain to mountain.
118
4.1 Characterizing components of the
representations
In the previous subsection we have discussed dif-
ferent types of from-to phrases, and the roles
that link the elements of the representations of
these types. The question we address now is how
we can provide our system with the necessary in-
formation to make these distinctions. This is a
preliminary investigation as yet without imple-
mentation.
Ideally, we would have ontologies to give us
the right characteristics of the entities underly-
ing our lexical items and we would have ade-
quate mappings from the lexical items to these
ontologies. These ontologies and these mappings
are currently not available. Natural language
processing applications, however, have taught us
that even if humans can do surprising things and
language can express surprising thoughts, most
of the time, the reality that human language ex-
presses is rather predictable, so that the map-
ping to ontologies can up to a certain point be
mimicked by probabilistic feature assignments
to lexical items. For ?Eric? we can assume that
with a high probability it will be the theme of
a movement path and whereas for ?the road? a
high probability assigns it as the theme of a sta-
tive path. In other cases, however, we need con-
crete co-occurrence statistics to assign the right
representations. Next, we sketch a preliminary
investigation of some Wikipedia data that can
be brought to bear on this issue. We indicate
how the data might help and point out some of
the new problems it brings up.
A first question that arises is of how much
practical relevance the different types that we
have discussed are. We looked at the first 100
?went from X to Y? sentences pulled out of
Wikipedia parsed with the Stanford dependency
parser, that had the required syntactic pattern
and found that 61 fell into the categories de-
scribed in the previous sections (gradual change
or no change in the physical domain) whereas
about 39 are clearly transformational from-to?s
(for instance ?The SU-152 went from design con-
cept to field trials in a record twenty-five days?).
Of these 61, 4 had temporal from-to modifiers,
19 had various scales or numeric from-to mod-
ifiers and 38 were locational. Of the locational
ones, 11 had a stationary reading and 17 had a
movement reading. So all the cases under dis-
cussion are well represented in naturally occur-
ring text.
A second question is how we can obtain
the relevant features from the data. We
see four potential methods: (1) the charac-
terization of words within existing ontologies
like WordNet (Miller, 1995), (2) the combina-
tion of stated facts through reasoning, (3) co-
occurrence statistics of words in text, and (4)
solicitation of novel features from human anno-
tators. We illustrate these methods based on
Wikipedia examples.
A first idea might be that there is at least a
straightforward ontological characterization for
difference between the movement and the sta-
tive reading: for the movement reading we re-
quire living beings and for the stative reading
we require long stationary entities. These im-
pressions are, of course, not completely wrong
but in the first case, we have to include in the
living beings not only groups such as brigades
but also ships (as in ?She went from the Red Sea
to the Mediterranean to relieve USS Coral Sea
...?), flights (as in ?This flight went from Spits-
bergen (Svalbard) to Alaska nonstop, so there
is little doubt that they went over the North
Pole.?) and messages (as in ?The message went
from the Palace in Stockholm to the King at
Drottningholm.?). And in the second categories
we have not only roads and various transporta-
tion lines but also borders (as in ?The bound-
ary of Manila province went from northeast to
southwest, ...?) and trade routes and things such
as (rifle) suppressors as in ?The suppressor, 2
inches in diameter, went all the way from the
back of the barrel to well beyond the muzzle
...?). A quick inspection of WordNet shows that
there is no interesting ancestor node that covers
all the movement cases but it also suggests that
a great number of the cases can be covered with
?conveyance, transport? together with ?motion,
movement, move? as well as ?organism, being?.
But ?organism, being? also covers ?plants? and
?sitter? and ?stander? and other subclasses that
119
don?t seem to be plausible candidates for the
movement analysis. There is no interesting hy-
pernym for both ?road? and ?border? before we
get to the useless level of ?object, physical ob-
ject? and no already existing ontology will help
with the suppressor case. Thus we might get
some data by using the first method but most
likely not everything we want.
As far as the arguments of the from-to phrases
themselves, locations can be indicated by place
names, institution names, nouns referring to lo-
cations, but also nouns referring to spatial lo-
cated entities that we do not think of as loca-
tions, such as parts of pieces of equipment. The
very limited inspection of data we have done up
to now does not lead us to expect that the na-
ture of the from-to arguments occurring with
movement readings is very different from that
found with stationary readings. In the current
state of affairs, many of the arguments of the
from-to phrases can be found either in gazetteers
or through the analysis of a reasonably well-
circumscribed spatial vocabulary.5
Some cases, however, fall outside of these re-
sources. The most interesting problem is pre-
sented by the reference to spatial entities that
are not clearly flagged as locations in ontologies,
such as those found in the suppressor-sentence
(?The suppressor, 2 inches in diameter, went all
the way from the back of the barrel to well be-
yond the muzzle ...?) above. We admit that
his type of sentence seems to be rather rare
in the Wikipedia corpus but it is problematic
because detailed ontological representations of
even common objects are not readily available.
Wikipedia, however, has some information that
might help one to formulate reasonable hypothe-
ses about parts. For instance, the article that
contains the suppressor-sentence, also contains
a structured specification of the carbine under
description mentioning the barrel and the muz-
zle. Here we need to use the second method,
reasoning. The question then becomes whether
we can find reasoning patterns that are general
enough to give interesting results.
5Whereas it is possible to enumerate an extensive part
of the relevant vocabulary, there is no extensive descrip-
tion of meaning contribution of these elements.
The third method, already demonstrated in
the context of semantic parsing (Poon and
Domingos, 2009), seems also to be promising.
For instance, even staying within the class of
movement verbs, different verbs have different
signatures that might help us with the classifi-
cation of their subjects and their from-to argu-
ments. While ?go? has indeed the wide range of
meanings that we expected, ?run? is rather dif-
ferent: apart from three examples where ?run?
refers to the movement of living beings and three
referring to vehicles moving, the other exam-
ples of the combination of ?run? with from-to fall
in two classes: indications of the spatial extent
of roads, railways and the like (27) and tempo-
ral extensions of shows, games or strips running
(16). The nature of the corpus has certainly an
influence here (Wikipedia does not contain nar-
rative texts) but this type of information might
be valuable to disambiguate parses: if we can
distinguish the cases where ?run? occurs with
spatial extent readings and the cases where it
occurs with temporal extent meanings, we can
harvest a set of possible subjects that are also
possible subjects for the spatial extent meaning
of ?go?. The distinction between the two read-
ings of ?run? is not very difficult to make as most
of the temporal extent readings of ?run? have a
temporal from-to phrase.6
A different way in which the characteristics
of specific verbs or verb argument combinations
might at least probabilistically disambiguate
possible readings is illustrated with a difference
between ?go? and ?range? with scalars. In sec-
tion 3.2, we observed that scalar ?go? does not
always imply that there is a steady increase or
decrease over time or space. However in all the
numerical or scalar examples except for one in
our first sample, the interpretation implies such
6But those readings themselves bring up a new clas-
sificatory problem: most of the time the subject is an
event, a show, or a game. However, in most cases the
meaning is not that one performance of the show ran for
several months or year but that several successive perfor-
mances ran. Moreover, the construction cannot only be
used with event-referring expressions but also with enti-
ties such as ?strips?. Here we get into problems of regular
polysemy. The treatment we have given above needs to
be complicated to take these into account.
120
a steady increase or decrease. We also exam-
ined the sentences with ?price ranged? and ?price
went? in the whole of Wikipedia. Unfortunately
there are very few examples but for these, the
difference in interpretation for ?range? and ?go?
seems to hold up: all 4 examples with ?go? had
the interpretation of steady increase or decrease.
So ?the price ranged ...? and ?the price went ...?
statistically might get a different interpretation
even if in some cases ?go? can be synonymous
with ?range?.
Finally, there is a possibility that due to
sparseness some required features can neither be
derived from existing ontologies nor from natu-
ral language text itself. For example, in ?The
2006 Trek the Trail event was organised on the
Railway Reserve Heritage Trail and went from
Mundaring to Darlington? we assume an extent
interpretation, and may thus be inclined to clas-
sify all events that way. However, in ?The case
Arklow vs MacLean went all the way from the
New Zealand High Court to the Privy Council
in London.? we assume a change interpretation
(movement), although WordNet sees ?event? as
a hypernym of ?case?. Interestingly, it is not the
arguments that determine the right interpreta-
tion here, but rather our distinction between dif-
ferent kinds of events: those for which spatial ex-
tent is important (street festivals) and those for
which not (lawsuits). More generally, in cases
where we are unable to make such fine distinc-
tions based on features derived from available
corpora, we can use our fourth method, solicit-
ing additional features from human annotators,
to group concepts in novel ways.
5 Conclusion
In this paper we first described the distinctions
that need to be made to allow a correct in-
terpretation of a subclass of from-to sentences.
We then looked at the resources that are avail-
able to help us guide to the correct interpreta-
tion. We distinguished four different ways to
obtain the information needed: features in an
existing ontology, features statistically derived
for the relations used with a concept, features
computed through reasoning and features ob-
tained through human annotation. We saw that
a small, very preliminary examination of the
data suggests that the three first methods will
allow us to make the right distinctions in an im-
portant number of cases but that there will be
cases in which the fourth method, human anno-
tation, will be necessary.
Acknowledgments
This material is based in part upon work sup-
ported by the Air Force Research Laboratory
(AFRL) under prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material
are those of the author(s) and do not necessar-
ily reflect the view of the Air Force Research
Laboratory (AFRL).
References
Dorit Abusch. 1986. Verbs of Change, Causation,
and Time. Report CSLI, Stanford University.
Daniel Bobrow, Robert Cheslow, Cleo Condoravdi,
Lauri Karttunen, Tracy, Rowan Nairn, Valeria
de Paiva, Lotti Price, and Annie Zaenen. 2007.
PARC?s Bridge question answering system. In
Proceedings of the GEAF (Grammar Engineer-
ing Across Frameworks) 2007 Workshop. Stan-
ford, CA.
David Dowty. 1979. Word Meaning and Montague
Grammar: The Semantics of Verbs and Times in
Generative Semantics and in Montague?s PTQ.
Springer.
Jean Mark Gawron. 2005. Generalized Paths. SALT
17.
Jean Mark Gawron. 2009. The Lexical Semantics of
Extent Verbs.
Jennifer Hay, Christopher Kennedy, and Beth Levin.
1999. Scale structure underlies telicity in ?degree
achievements?. pages 127?144.
Ray Jackendoff. 1996. The Proper Treatment of
Measuring Out, Telicity, and Perhaps Even Quan-
tification in English. Natural Language and Lin-
guistic Theory 14, pages 305?354.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the
ACM, 38(11):39?41.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
121
