DATE: A Dialogue Act Tagging Scheme for Evaluation of
Spoken Dialogue Systems
Marilyn Walker and Rebecca Passonneau
AT&T Shannon Labs
180 Park Ave.
Florham Park, N.J. 07932 fwalker,beckyg@research.att.com
ABSTRACT
This paper describes a dialogue act tagging scheme developed for
the purpose of providing finer-grained quantitative dialogue met-
rics for comparing and evaluating DARPA COMMUNICATOR spo-
ken dialogue systems. We show that these dialogue act metrics can
be used to quantify the amount of effort spent in a dialogue main-
taining the channel of communication or, establishing the frame
for communication, as opposed to actually carrying out the travel
planning task that the system is designed to support. We show that
the use of these metrics results in a 7% improvement in the fit in
models of user satisfaction. We suggest that dialogue act metrics
can ultimately support more focused qualitative analysis of the role
of various dialogue strategy parameters, e.g. initiative, across di-
alogue systems, thus clarifying what development paths might be
feasible for enhancing user satisfaction in future versions of these
systems.
1. INTRODUCTION
Recent research on dialogue is based on the assumption that di-
alogue acts provide a useful way of characterizing dialogue behav-
iors in human-human dialogue, and potentially in human-computer
dialogue as well [16, 27, 11, 7, 1]. Several research efforts have
explored the use of dialogue act tagging schemes for tasks such
as improving recognition performance [27], identifying important
parts of a dialogue [12], and as a constraint on nominal expres-
sion generation [17]. This paper reports on the development and
use of a dialogue act tagging scheme for a rather different task:
the evaluation and comparison of spoken dialogue systems in the
travel domain. We call this scheme DATE: Dialogue Act Tagging
for Evaluation.
Our research on the use of dialogue act tagging for evaluation
focuses on the corpus of DARPA COMMUNICATOR dialogues col-
lected in the June 2000 data collection [28]. This corpus consists of
662 dialogues from 72 users calling the nine different COMMUNI-
CATOR travel planning systems. Each system implemented a log-
file standard for logging system behaviors and calculating a set of
core metrics. Each system utterance and each recognizer result was
logged, and user utterances were transcribed and incorporated into
.
the logfiles. The logfile standard supported the calculation of met-
rics that were hypothesized to potentially affect the user?s percep-
tion of the system; these included task duration, per turn measures,
response latency measures and ASR performance measures. Each
dialogue was also hand labelled for task completion.
The hypothesis underlying our approach is that a system?s di-
alogue behaviors have a strong effect on the user?s perception of
the system. Yet the core metrics that were collected via the logfile
standard represent very little about dialogue behaviors. For exam-
ple, the logging counts system turns and tallies their average length,
but doesn?t distinguish turns that reprompt the user, or give in-
structions, from those that present flight information. Furthermore,
each COMMUNICATOR system had a unique dialogue strategy and a
unique way of achieving particular communicative goals. Thus, in
order to explore our hypothesis about the differential effect of these
strategies, we needed a way to characterize system dialogue behav-
iors that would capture such differences yet be applied uniformly to
all nine systems. While some sites logged system dialogue behav-
iors using site-specific dialogue act naming schemes, there existed
no scheme that could be applied across sites.
Our goal was thus to develop a dialogue act tagging scheme that
would capture important distinctions in this set of dialogues; these
distinctions must be useful for testing particular hypotheses about
differences among dialogue systems. We also believed that it was
important for our tagging scheme to allow for multiple views of
each dialogue act. This would allow us, for example, to investigate
what part of the task an utterance contributes to separately from
what speech act function it serves. A central claim of the paper is
that these goals require a tagging scheme that makes distinctions
within three orthogonal dimensions of utterance classification: (1)
a SPEECH-ACT dimension; (2) a TASK-SUBTASK dimension; and
(3) a CONVERSATIONAL-DOMAIN dimension. Figure 1 shows a
COMMUNICATOR dialogue with each system utterance classified
on these three dimensions. The labels on each utterance are fully
described in the remainder of the paper.
Sections 2, 3, and 4, describe the three dimensions of DATE. In
these sections, we describe two aspects of our annotation scheme
that are not captured in existing tagging schemes, which we be-
lieve are important for characterizing how much effort in a dialogue
is devoted to the task versus different kinds of dialogue mainte-
nance. Section 5 describes how the dialogue act labels are assigned
to system utterances and section 6 discusses results showing that
the DATE dialogue act metrics improve models of user satisfaction
by an absolute 7% (an increase from 38% to 45%). The dialogueue
act metrics that are important predictors of user satisfaction are var-
ious kinds of meta-dialogue, apologies and acts that may be land-
marks for achieving particular dialogueue subtasks. In section 7 we
summarize the paper, discuss our claim that a dialogue annotation
scheme is a partial model of a natural class of dialogues, and dis-
cuss the ways in which the DATE scheme may be generalizable to
other dialogue corpora.
2. CONVERSATIONAL DOMAINS
The CONVERSATIONAL-DOMAIN dimension characterizes each
utterance as primarily belonging to one of three arenas of conver-
sational action. The first arena is the domain task, which in this
case is air travel booking, and which we refer to below as ABOUT-
TASK. The second domain of conversational action is the manage-
ment of the communication channel, which we refer to as ABOUT-
COMMUNICATION. This distinction has been widely adopted [19,
2, 9]. In addition, we identify a third domain of talk that we refer
to as ABOUT-SITUATION-FRAME. This domain is particularly rel-
evant for distinguishing human-computer from human-human dia-
logues, and for distinguishing dialogue strategies across the 9 COM-
MUNICATOR systems. Each domain is described in this section.
2.1 About-Task
The ABOUT-TASK domain reflects the fact that many utterances
in a task-oriented dialogue originate because the goal of the dia-
logue is to complete a particular task to the satisfaction of both
participants. Typically an about-task utterance directly asks for or
presents task-related information, or offers a solution to a task goal.
As Figure 1 shows, most utterances are in the ABOUT-TASK di-
mension, reflecting the fact that the primary goal of the dialogue
is to collaborate on the task of making travel arrangements. The
task column of Figure 1 specifies the subtask that each task-related
utterance contributes to. DATE includes a large inventory of sub-
tasks in the task/subtask dimension in order to make fine-grained
distinctions regarding the dialogue effort devoted to the task or its
subcomponents. Section 4 will describe the task model in more
detail.
2.2 About-Communication
The ABOUT-COMMUNICATION domain reflects the system goal
of managing the verbal channel and providing evidence of what
has been understood [29, 8, 25]. Although utterances of this type
occur in human-human dialogue, they are more frequent in human-
computer dialogue, where they are motivated by the need to avoid
potentially costly errors arising from imperfect speech recognition.
In the COMMUNICATOR corpus, many systems use a conserva-
tive strategy of providing feedback indicating the system?s under-
standing of the information provided by the user after each user
turn. A typical example is the repetition of the origin and des-
tination cities in Figures 1 and 6. This type of repetition is the
IMPLICIT-CONFIRMATION speech-act (see Section 3 below). Some
systems used a variable confirmation strategy where some informa-
tion items may be confirmed as they are understood, but the system
requests explicit confirmation of all task parameters before search-
ing the database for matching flights. An example is in Figure 2.
Here the system asks for explicit confirmation in SYS3 before go-
ing to the database. This is the first opportunity that the user has
for making a correction, which he does in USER3. The system
then again asks for explicit confirmation of its new understanding,
which the user provides in USER4. After the user informs the sys-
tem that it is a one-way flight in USER6, the system accesses the
database. These explicit confirmations have the goal of avoiding
a costly database lookup, where the retrieval is conditioned on the
wrong parameters.
All implicit and explicit confirmation speech-acts are catego-
rized as ABOUT-COMMUNICATION because they are motivated by
the potential errors that the system might make in understanding
Speaker Utterance
SYS1 ....What are your travel plans? ?
USER1 I WISH TO FLY FROM LEXICON KENTUCKY TO
SPRINGFIELD MISSOURI THE MORNING OF OC-
TOBER FIFTH
SYS2 do you have a preferred airline?
USER2 NO
SYS3 I have you going from Kentucky to Gainesville Missouri
on October fifth . Is that correct?
USER3 IT?S LEXINGTON KENTUCKY TO SPRINGFIELD
MISSOURI
SYS4 I have you going from Lexington to Springfield Missouri
on October fifth, Is that correct?
USER4 YES
SYS5 Will you return to Lexington from Springfield?
USER5 NO
SYS6 Is Springfield your final destination?
USER6 YES
SYS7 Hold on while I check availability..... Three options were
returned. Option one has a fare of four hundred and thirty
three dollars.....
Figure 2: Dialogue Illustrating Variable Confirmation Strategy
the caller, or in diagnosing the causes of misunderstandings. In
general, any utterance that reflects the system?s understanding of
something the user said is classified as ABOUT-COMMUNICATION.
A second set of ABOUT-COMMUNICATION utterances are APOLO-
GIES that the system makes for misunderstandings (see Section 3
below), i.e. utterances such as I?m sorry. I?m having trouble under-
standing you., or My mistake again. I didn?t catch that. or I can see
you are having some problems.
The last category of ABOUT-COMMUNICATION utterances are
the OPENINGS/CLOSINGS by which the system greets or says good-
bye to the caller. (Again, see Section 3 below.)
2.3 About Situation-Frame
The SITUATION-FRAME domain pertains to the goal of manag-
ing the culturally relevant framing expectations. The term is in-
spired by Goffman?s work on the organization and maintenance of
social interaction [13, 14]. An obvious example of a framing as-
sumption is that the language of the interaction will be English [13,
14]. Another is that there is an asymmetry between the knowledge
and/or agency of the system (or human travel agent) and that of the
user (or caller): the user cannot issue an airline ticket.
In developing the DATE tagging scheme, we compared human-
human travel planning dialogues collected by CMU with the human-
machine dialogues of the June 2000 data collection and noticed
a striking difference in the ABOUT-FRAME dimension. Namely,
very few ABOUT-FRAME utterances occur in the human-human di-
alogues, whereas they occur frequently enough in human-computer
dialogues that to ignore them is to risk obscuring significant differ-
ences in habitability of different systems. In other words, certain
differences in dialogue strategies across sites could not be fully rep-
resented without such a distinction. Figure 3 provides examples
motivating this dimension.
Dialogue acts that are ABOUT-FRAME are cross-classified as one
of three types of speech-acts, PRESENT-INFO, INSTRUCTION or
APOLOGY. They are not classified as having a value on the TASK-
SUBTASK dimension. Most of the ABOUT-FRAME dialogue acts
fall into the speech-act category of INSTRUCTIONS, utterances di-
rected at shaping the user?s behavior and expectations about how to
interact with a machine. Sites differ regarding how much instruc-
tion is provided up-front versus within the dialogue; most sites have
different utterance strategies for dialogue-initial versus dialogue-
Speech-Act Example
PRESENT-
INFO
You are logged in as a guest user of A T and T Commu-
nicator.
PRESENT-
INFO
I?ll enroll you temporarily as a guest user.
PRESENT-
INFO
I know about the top 150 cities worldwide.
PRESENT-
INFO
This call is being recorded for development purposes,
and may be shared with other researchers.
PRESENT-
INFO
I cannot handle rental cars or hotels yet. Please restrict
your requests to air travel.
PRESENT-
INFO
I heard you ask about fares. I can only price an
itinerary. I cannot provide information on published
fares for individual flights.
INSTRUCTION First, always wait to hear the beep before you say any-
thing
INSTRUCTION You can always start over again completely just by say-
ing: start over.
INSTRUCTION Before we begin, let?s go over a few simple instructions.
INSTRUCTION Please remember to speak after the tone. If you get con-
fused at any point you can say start over to cancel your
current itinerary.
APOLOGY Sorry, an error has occurred. We?ll have to start over.
APOLOGY I am sorry I got confused. Thanks for your patience. Let
us try again.
APOLOGY Something is wrong with the flight retrieval.
APOLOGY I have trouble with my script.
Figure 3: Example About-Frame Utterances
medial instructions. One site gives minimal up-front framing in-
formation; further, the same utterances that can occur up-front also
occur dialogue-medially. A second site gives no up-front fram-
ing information, but it does provide framing information dialogue-
medially. Yet a third site gives framing information dialogue-initially,
but not dialogue-medially. The remaining sites provide different
kinds of general instructions dialogue-initially, e.g. (Welcome. ...You
may say repeat, help me out, start over, or, that?s wrong, you can
also correct and interrupt the system at any time.) versus dialogue-
medially: (Try changing your departure dates or times or a nearby
city with a larger airport.) This category also includes statements
to the user about the system?s capabilities. These occur in response
to a specific question or task that the system cannot handle: I can-
not handle rental cars or hotels yet. Please restrict your requests
to air travel. See Figure 3.
Another type of ABOUT-FRAME utterance is the system?s at-
tempt to disambiguate the user?s utterance; in response to the user
specifying Springfield as a flight destination, the system indicates
that this city name is ambiguous (I know of three Springfields, in
Missouri, Illinois and Ohio. Which one do you want?). The sys-
tem?s utterance communicates to the user that Springfield is am-
biguous, and goes further than a human would to clarify that there
are only three known options. It is important for evaluation pur-
poses to distinguish the question and the user?s response from a
simple question-answer sequence establishing a destination. A di-
rect question, such as What city are you flying to?, functions as a
REQUEST-INFO speech act and solicits information about the task.
The context here contrasts with a direct question in that the system
has already asked for and understood a response from the caller
about the destination city. Here, the function of the system turn is
to remediate the caller?s assumptions about the frame by indicating
the system?s confusion about the destination. Note that the question
within this pattern could easily be reformulated as a more typical
instruction statement, such as Please specify which Springfield you
mean, or Please say Missouri, Illinois or Ohio..
3. THE SPEECH-ACT DIMENSION
The SPEECH-ACT dimension characterizes the utterance?s com-
municative goal, and is motivated by the need to distinguish the
communicative goal of an utterance from its form. As an exam-
ple, consider the functional category of a REQUEST for information,
found in many tagging schemes that annotate speech-acts [24, 18,
6]. Keeping the functional category of a REQUEST separate from
the sentence modality distinction between question and statement
makes it possible to capture the functional similarity between ques-
tion and statement forms of requests, e.g., Can you tell me what
time you would like to arrive? versus Please tell me what time you
would like to arrive.
In DATE, the speech-act dimension has ten categories. We use
familiar speech-act labels, such as OFFER, REQUEST-INFO, PRESENT-
INFO, ACKNOWLEDGMENT, and introduce new ones designed to
help us capture generalizations about communicative behavior in
this domain, on this task, given the range of system and human
behavior we see in the data. One new one, for example, is STATUS-
REPORT, whose speech-act function and operational definition are
discussed below. Examples of each speech-act type are in Figure 4.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dollars.
OFFER Would you like me to hold this option?
ACKNOWLEDGMENT I will book this leg.
STATUS-REPORT Accessing the database; this might take a few sec-
onds.
EXPLICIT-
CONFIRM
You will depart on September 1st. Is that correct?
IMPLICIT-CONFIRM Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENINGS/CLOSINGS Hello. Welcome to the C M U Communicator.
Figure 4: Example Speech Acts
In this domain, the REQUEST-INFO speech-acts are designed to
solicit information about the trip the caller wants to book, such as
the destination city (And what city are you flying to?), the desired
dates and times of travel (What date would you like to travel on), or
information about ground arrangements, such as hotel or car rental
(Will you need a hotel in Chicago?).
The PRESENT-INFO speech-acts also often pertain directly to the
domain task of making travel arrangements: the system presents
the user with a choice of itinerary (There are several flights from
Dallas Fort Worth to Salisbury Maryland which depart between
eight in the morning and noon on October fifth. You can fly on
American departing at eight in the morning or ten thirty two in the
morning, or on US Air departing at ten thirty five in the morning.),
as well as a ticket price (Ticket price is 495 dollars), or hotel or car
options.
OFFERS involve requests by the caller for a system action, such
as to pick a flight (I need you to tell me whether you would like to
take this particular flight) or to confirm a booking (If this itinerary
meets your needs, please press one; otherwise, press zero.) They
typically occur after the prerequisite travel information has been
obtained, and choices have been retrieved from the database.
The ACKNOWLEDGMENT speech act characterizes system utter-
ances that follow a caller?s acceptance of an OFFER, e.g. I will book
this leg or I am making the reservation.
The STATUS-REPORT speech-act is used to inform the user about
the status of the part of the domain task pertaining to the database
retrieval, and can include apologies, mollification, requests to be
patient, and so on. Their function is to let the user know what is
happening with the database lookup, whether there are problems
with it, and what types of problems. While the form of these acts
are typically statements, their communication function is different
than typical presentations of information; they typically function to
keep the user apprised of progress on aspects of the task that the
user has no direct information about, e.g. Accessing the database;
this might take a few seconds. There is also a politeness function
to utterances like Sorry this is taking so long, please hold., and
they often provide the user with error diagnostics: The date you
specified is too far in advance.; or Please be aware that the return
date must be later than the departure date.; or No records satisfy
your request.; or There don?t seem to be any flights from Boston.
The speech-act inventory also includes two types of speech acts
whose function is to confirm information that has already been pro-
vided by the caller. In order to identify and confirm the parameters
of the trip, systems may ask the caller direct questions, as in SYS3
and SYS4 in Figure 2. These EXPLICIT-CONFIRM speech acts are
sometimes triggered by the system?s belief that a misunderstand-
ing may have occurred. A typical example is Are you traveling
to Dallas?. An alternative form of the same EXPLICIT-CONFIRM
speech-act type asserts the information the system has understood
and asks for confirmation in an immediately following question: I
have you arriving in Dallas. Is that correct? In both cases, the
caller is intended to provide a response.
A less intrusive form of confirmation, which we tag as IMPLICIT-
CONFIRM, typically presents the user with the system?s understand-
ing of one travel parameter immediately before asking about the
next parameter. Depending on the site, implicit information can ei-
ther precede the new request for information, as in Flying to Tokyo.
What day are you leaving?, or can occur within the same utter-
ance, as in What day do you want to leave London? More rarely,
an implicit confirmation is followed by PRESENT-INFO: a flight
on Monday September 25. Delta has a flight departing Atlanta at
nine thirty. One question about the use of implicit confirmation
strategy is whether the caller realizes they can correct the system
when necessary [10]. Although IMPLICIT-CONFIRMS typically oc-
cur as part of a successful sequence of extracting trip information
from the caller, they can also occur in situations where the system
is having trouble understanding the caller. In this case, the system
may attempt to instruct the user on what it is doing to remediate
the problem in between an IMPLICIT-CONFIRM and a REQUEST-
INFO: So far, I have you going from Tokyo. I am trying to assemble
enough information to pick a flight. Right now I need you to tell me
your destination.
We have observed that INSTRUCTIONS are a speech-act type
that distinguishes these human-computer travel planning dialogues
from corresponding human-human travel planning dialogues. In-
structions sometimes take the form of a statement or an imperative,
and are characterized by their functional goal of clarifying the sys-
tem?s own actions, correcting the user?s expectations, or changing
the user?s future manner of interacting with the system. Dialogue
systems are less able to diagnose a communication problem than
human travel agents, and callers are less familiar with the capa-
bilities of such systems. As noted above, some systems resort to
explicit instructions about what the system is doing or is able to do,
or about what the user should try in order to assist the system: Try
asking for flights between two major cities; or You can cancel the
San Antonio, Texas, to Tampa, Florida flight request or change it.
To change it, you can simply give new information such as a new
departure time. Note that INSTRUCTIONS, unlike the preceding di-
alogue act types, do not directly involve a domain task.
Like the INSTRUCTION speech-acts, APOLOGIES do not address
a domain task. They typically occur when the system encoun-
ters problems, for example, in understanding the caller (I?m sorry,
I?m having trouble understanding you), in accessing the database
(Something is wrong with the flight retrieval), or with the connec-
tion (Sorry, we seem to have a bad connection. Can you please call
me back later?).
The OPENING/CLOSING speech act category characterizes utter-
ances that open and close the dialogue, such as greetings or good-
byes [26]. Most of the dialogue systems open the interactions with
some sort of greeting?Hello, welcome to our Communicator flight
travel system, and end with a sign-off or salutation?Thank you
very much for calling. This session is now over. We distinguish
these utterances from other dialogue acts, but we do not tag open-
ings separate from closings because they have a similar function,
and can be distinguished by their position in the discourse. We also
include in this category utterances in which the systems survey the
caller as to whether s/he got the information s/he needed or was
happy with the system.
4. THE TASK-SUBTASK DIMENSION
The TASK-SUBTASK dimension refers to a task model of the do-
main task that the system is designed to support and captures dis-
tinctions among dialogue acts that reflect the task structure.1 Our
domain is air travel reservations, thus the main communicative task
is to specify information pertaining to an air travel reservation, such
as the destination city. Once a flight has been booked, ancillary
tasks such as arranging for lodging or a rental car become relevant.
The fundamental motivation for the TASK-SUBTASK dimension in
the DATE scheme is to derive metrics related to subtasks in order to
quantify how much effort a system expends on particular subtasks.2
This dimension distinguishes among 13 subtasks, some of which
can also be grouped at a level below the top level task. The subtasks
and examples are in Figure 5. The TOP-LEVEL-TRIP task describes
the task which contains as its subtasks the ORIGIN, DESTINATION,
DATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVAL and ITINERARY
tasks. The GROUND task includes both the HOTEL and CAR sub-
tasks.
Typically each COMMUNICATOR dialogue system acts as though
it utilizes a task model, in that it has a particular sequence in which
it will ask for task information if the user doesn?t take the initia-
tive to volunteer this information. For example, most systems ask
first for the origin and destination cities, then for the date and time.
Some systems ask about airline preference and others leave it to the
caller to volunteer this information. A typical sequence of tasks for
the flight planning portion of the dialogue is illustrated in Figure 6.
As Figure 6 illustrates, any subtask can involve multiple speech
acts. For example, the DATE subtask can consist of acts requesting,
or implicitly or explicitly confirming the date. A similar exam-
ple is provided by the subtasks of CAR (rental) and HOTEL, which
include dialogue acts requesting, confirming or acknowledging ar-
rangements to rent a car or book a hotel room on the same trip.
1This dimension is used as an elaboration of each speech-act type
in other tagging schemes [24].
2It is tempting to also consider this dimension as a means of in-
ferring discourse structure on the basis of utterance level labels,
since it is widely believed that models of task structure drive the
behavior of dialogue systems [23, 3, 22], and the relationship be-
tween discourse structure and task structure has been a core topic
of research since Grosz?s thesis [15]. However, we leave the infer-
ence of discourse structure as a topic for future work because the
multifunctionality of many utterances suggests that the correspon-
dence between task structure and dialogue structure may not be as
straightforward as has been proposed in Grosz?s work [30].
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
TRIP-TYPE Will you return to Boston from San Jose?
RETRIEVAL Accessing the database; this might take a few seconds.
ITINERARY The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground arrangements?.
HOTEL Would you like a hotel near downtown or near the air-
port?.
CAR Do you need a car in San Jose?
Figure 5: Example Utterances for each Subtask
Speaker Utterance Task
SYS Welcome .... What trip would you like to take? TOP-
LEVEL-
TRIP
USER STARTING FROM ATLANTA GEORGIA TO
LONDON ENGLAND
SYS flying from atlanta to london. ORIGIN,
DESTINA-
TION
SYS What day are you leaving Atlanta? DATE
USER MONDAY SEPTEMBER TWENTY FIVE
SYS on monday, september twenty fifth. DATE
SYS About what time do you want to leave? TIME
USER [UH] DAYTIME
SYS Leaving in the daytime. TIME
SYS Please stand by while I look up the flight sched-
ules on the web.
RETRIEVAL
SYS OK, got them. I have 13 flights. The first flight
is on American at six fifty nine eh M, arriving at
ten forty five PM, with a connection in Chicago.
. Is that OK?
ITINERARY
USER [UH] YES OKAY
SYS Do you want a return flight from London to At-
lanta?
TRIP-TYPE
USER [UH] YES
SYS Returning from london to atlanta. TRIP-TYPE
SYS What day are you leaving London? DATE
USER [UH] LEAVING [UM] MONDAY OCTOBER
THE SECOND
SYS on monday, october second. DATE
SYS About what time do you want to leave? TIME
USER ANY TIME?S OKAY
SYS Please stand by while I look up the flight sched-
ules on the web... OK, got them.
RETRIEVAL,
ITINERARY
Figure 6: Dialogue Illustrating a Typical Task Sequence
There are also differences in how each site?s dialogue strategy
reflects it conceptualization of the travel planning task. For exam-
ple, some systems ask the user explicitly for their airline prefer-
ences whereas others do not (the systems illustrated in Figures 1
and 6 do not, wherase the one in Figure 2 does). Another differ-
ence is whether the system asks the user explicitly whether s/he
wants a round-trip ticket. Some systems ask this information early
on, and search for both the outbound and the return flights at the
same time. Other systems do not separately model round-trip and
multi-leg trips. Instead they ask the user for information leg by leg,
and after requesting the user to select an itinerary for one leg of
the flight, they ask whether the user has an additional destination.
A final difference was that, in the June 2000 data collection, some
systems such as the one illustrated in Figure 1 included the ground
arrangements subtasks, and others did not.
5. IMPLEMENTATION
Our focus in this work is in labelling the system side of the di-
alogue; our goal was to develop a fully automatic 100% correct
dialogue parser for the limited range of utterances produced by the
9 COMMUNICATOR systems. While we believe that it would be
useful to be able to assign dialogue acts to both sides of the con-
versation, we expect that to require hand-labelling [1]. We also
believe that in many cases the system behaviors are highly corre-
lated with the user behaviors of interest; for example when a user
has to repeat himself because of a misunderstanding, the system
has probably prompted the user multiple times for the same item of
information and has probably apologized for doing so. Thus this
aspect of the dialogue would also be likely to be captured by the
APOLOGY dialogue act and by counts of effort expended on the
particular subtask.
We implemented a pattern matcher that labels the system side
of each dialogue. An utterance or utterance sequence is identifed
automatically from a database of patterns that correspond to the di-
alogue act classification we arrived at in cooperation with the site
developers. Where it simplifies the structure of the dialogue parser,
we assign two adjacent utterances that are directed at the same goal
the same DATE label, thus ignoring the utterance level segmenta-
tion, but we count the number of characters used in each act. Since
some utterances are generated via recursive or iterative routines,
some patterns involve wildcards.
The current implementation labels the utterances with tags that
are independent of any particular markup-language or representa-
tion format. We have written a transducer that takes the labelled
dialogues and produces HTML output for the purpose of visual-
izing the distribution of dialogue acts and meta-categories in the
dialogues. An additional summarizer program is used to produce a
summary of the percentages and counts of each dialogue act as well
as counts of meta-level groupings of the acts related to the different
dimensions of the tagging scheme. We intend to use our current
representation to generate ATLAS compliant representations [4].
6. RESULTS
Our primary goal was to achieve a better understanding of the
qualitative aspects of each system?s dialogue behavior. We can
quantify the extent to which the dialogue act metrics have the po-
tential to improve our understanding by applying the PARADISE
framework to develop a model of user satisfaction and then exam-
ining the extent to which the dialogue act metrics improve these
models [31]. In other work, we show that given the standard met-
rics collected for the COMMUNICATOR dialogue systems, the best
model accounts for 38% of the variance in user satisfaction [28].
When we retrain these models with the dialogue act metrics ex-
tracted by our dialogue parser, we find that many metrics are signif-
icant predictors of user satisfaction, and that the model fit increases
from 38% to 45%. When we examine which dialogue metrics are
significant, we find that they include several types of meta-dialogue
such as explicit and implicit confirmations of what the user said,
and acknowledgments that the system is going to go ahead and do
the action that the user has requested. Significant negative predic-
tors include apologies. On interpretation of many of the significant
predictors is that they are landmarks in the dialogue for achieve-
ment of particular subtasks. However the predictors based on the
core metrics included a ternary task completion metric that captures
succinctly whether any task was achieved or not, and whether the
exact task that the user was attempting to accomplish was achieved.
A plausible explanation for the increase in the model fits is that user
satisfaction is sensitive to exactly how far through the task the user
got, even when the user did not in fact complete the task. The
role of the other significant dialogue metrics are plausibly inter-
preted as acts important for error minimization. As with the task-
related dialogue metrics, there were already metrics related to ASR
performance in the core set of metrics. However, several of the
important metrics count explicit confirmations, one of the desired
date of travel, and the other of all information before searching the
database, as in utterances SYS3 and SYS4 in Figure 2.
7. DISCUSSION
This paper has presented DATE, a dialogue act tagging scheme
developed explicitly for the purpose of comparing and evaluating
spoken dialogue systems. We have argued that such a scheme needs
to make three important distinctions in system dialogue behaviors
and we are investigating the degree to which any given type of dia-
logue act belongs in a single category or in multiple categories.
We also propose the view that a tagging scheme be viewed as a
partial model of a natural class of dialogues. It is a model to the de-
gree that it represents claims about what features of the dialogue are
important and are sufficiently well understood to be operationally
defined. It is partial in that the distributions of the features and
their relationship to one another, i.e., their possible manifestations
in dialogues within the class, are an empirical question.
The view that a dialogue tagging scheme is a partial model of a
class of dialogues implies that a pre-existing tagging scheme can be
re-used on a different research project, or by different researchers,
only to the degree that it models the same natural class with respect
to similar research questions, is sufficient for expressing observa-
tions about what actually occurs within the current dialogues of
interest, and is sufficiently well-defined that high reliability within
and across research sites can be achieved. Thus, our need to modify
existing schemes was motivated precisely to the degree that exist-
ing schemes fall short of these requirements. Other researchers who
began with the goal of re-utilizing existing tagging schemes have
also found it necessary to modify these schemes for their research
purposes [11, 18, 7].
The most substantial difference between our dialogue act tag-
ging scheme and others that have been proposed is in our expan-
sion of the two-way distinction between dialogue tout simple vs.
meta-dialogue, into a three-way distinction among the immediate
dialogue goals, meta-dialogue utterances, and meta-situation utter-
ances. Depending on further investigation, we might decide these
three dimensions have equal status within the overall tagging scheme
(or within the overall dialogue-modeling enterprise), or that there
are two types of meta-dialogue: utterances devoted to maintaining
the channel, versus utterances devoted to establishing/maintaining
the frame. Further, in accord with our view that a tagging scheme
is a partial model, and that it is therefore necessarily evolving as
our understanding of dialogue evolves, we also believe that our for-
mulation of any one dimension, such as the speech-act dimension,
will necessarily differ from other schemes that model a speech-act
dimension.
Furthermore, because human-computer dialogue is at an early
stage of development, any such tagging scheme must be a moving
target, i.e., the more progress is made, the more likely it is we may
need to modify along the way the exact features used in an annota-
tion scheme to characterize what is going on. In particular, as sys-
tem capabilities become more advanced in the travel domain, it will
probably be necessary to elaborate the task model to capture differ-
ent aspects of the system?s problem solving activities. For example,
our task model does not currently distinguish between different as-
pects of information about an itinerary, e.g. between presentation
of price information and presentation of schedule information.
We also expect that some domain-independent modifications are
likely to be necessary as dialogue systems become more success-
ful, for example to address the dimension of ?face?, i.e. the posi-
tive politeness that a system shows to the user [5]. As an example,
consider the difference between the interpretation of the utterance,
There are no flights from Boston to Boston, when produced by a
system vs. when produced by a human travel agent. If a human
said this, it would be be interpretable by the recipient as an in-
sult to their intelligence. However when produced by a system, it
functions to identify the source of the misunderstanding. Another
distinction that we don?t currently make which might be useful is
between the initial presentation of an item of information and its
re-presentation in a summary. Summaries arguably have a differ-
ent communicative function [29, 7]. Another aspect of function
our representation doesn?t capture is rhetorical relations between
speech acts [20, 21].
While we developed DATE to answer particular research ques-
tions in the COMMUNICATOR dialogues, there are likely to be as-
pects of DATE that can be applied elsewhere. The task dimension
tagset reflects our model of the domain task. The utility of a task
model may be general across domains and for this particular do-
main, the categories we employ are presumably typical of travel
tasks and so, may be relatively portable.
The speech act dimension includes categories typically found in
other classifications of speech acts, such as REQUEST-INFO, OF-
FER, and PRESENT-INFO. We distinguish information presented to
the user about the task, PRESENT-INFO, from information provided
to change the user?s behavior, INSTRUCTION, and from information
presented in explanation or apology for an apparent interruption in
the dialogue, STATUS-REPORT. The latter has some of the flavor
of APOLOGIES, which have an inter-personal function, along with
OPENINGS/CLOSINGS. We group GREETINGS and SIGN-OFFS into
the single category of OPENINGS/CLOSINGS on the assumption that
politeness forms make less contribution to perceived system suc-
cess than the system?s ability to carry out the task, to correct mis-
understandings, and to coach the user.
Our third dimension, conversational-domain, adds a new cate-
gory, ABOUT-SITUATION-FRAME, to the more familiar distinction
between utterances directed at a task goal vs. utterances directed
at a maintaining the communication. This distinction supports the
separate classification of utterances directed at managing the user?s
assumptions about how to interact with the system on the air travel
task. As we mention above, the ABOUT-SITUATION-FRAME utter-
ances that we find in the human-computer dialogues typically did
not occur in human-human air travel dialogues. In addition, as we
note above, one obvious difference in the dialogue strategies im-
plemented at different sites had to do with whether these utterances
occurred upfront, within the dialogue, or both.
In order to demonstrate the utility of dialogue act tags as metrics
for spoken dialogue systems, we show that the use of these metrics
in the application of PARADISE [31] improves our model of user
satisfaction by an absolute 7%, from 38% to 45%. This is a large
increase, and the fit of these models are very good for models of
human behavior. We believe that we have only begun to discover
the ways in which the output of the dialogue parser can be used. In
future work we will examine whether other representations derived
from the metrics we have applied, such as sequences or structural
relations between various types of acts might improve our perfor-
mance model further. We are also collaborating with other mem-
bers of the COMMUNICATOR community who are investigating the
use of dialogue act and initiative tagging schemes for the purpose
of comparing human-human to human-computer dialogues [1].
8. ACKNOWLEDGMENTS
This work was supported under DARPA GRANT MDA 972 99 3
0003 to AT&T Labs Research. Thanks to Payal Prabhu and Sung-
bok Lee for their assistance with the implementation of the dia-
logue parser. We also appreciate the contribution of J. Aberdeen,
E. Bratt, S. Narayanan, K. Papineni, B. Pellom, J. Polifroni, A.
Potamianos, A. Rudnicky, S. Seneff, and D. Stallard who helped
us understand how the DATE classification scheme applied to their
COMMUNICATOR systems? dialogues.
9. REFERENCES
[1] J. Aberdeen and C. Doran. Human-computer and
human-human dialogues. DARPA Communicator Principle
Investigators Meeting (Philadelphia, PA USA).
http://www.dsic-web.net/ito/meetings/communicator
sep2000/, September, 2000.
[2] J. Allen and M. Core. Draft of DAMSL: Dialog act markup
in several layers. Coding scheme developed by the
MultiParty group, 1st Discourse Tagging Workshop,
University of Pennsylvania, March 1996, 1997.
[3] J. F. Allen. Recognizing intentions from natural language
utterances. In M. Brady and R. Berwick, editors,
Computational Models of Discourse. MIT Press, 1983.
[4] S. Bird and M. Liberman. A formal framework for linguistic
annotation. Speech Communication, 33(1,2):23?60, 2001.
[5] P. Brown and S. Levinson. Politeness: Some universals in
language usage. Cambridge University Press, 1987.
[6] J. C. Carletta, A. Isard, S. Isard, J. C. Kowtko,
G. Dowerty-Sneddon, and A. H. Anderson. The reliability of
a dialogue structure coding scheme. Computational
Linguistics, 23-1:13?33, 1997.
[7] R. Cattoni, M. Danieli, A. Panizza, V. Sandrini, and C. Soria.
Building a corpus of annotated dialogues: the ADAM
experience. In Proc. of the Conference
Corpus-Linguistics-2001, Lancaster, U.K., 2001.
[8] H. H. Clark and E. F. Schaefer. Contributing to discourse.
Cognitive Science, 13:259?294, 1989.
[9] S. L. Condon and C. G. Cech. Functional comparison of
face-to-face and computer-mediated decision-making
interactions. In S. Herring, editor, Computer-Mediated
Converstaion. John Benjamins, 1995.
[10] M. Danieli and E. Gerbino. Metrics for evaluating dialogue
strategies in a spoken language system. In Proceedings of the
1995 AAAI Spring Symposium on Empirical Methods in
Discourse Interpretation and Generation, pages 34?39,
1995.
[11] B. Di Eugenio, P. W. Jordan, J. D. Moore, and R. H.
Thomason. An empirical investigation of collaborative
dialogues. In ACL-COLING98, Proceedings of the
Thirty-sixth Conference of the Association for
Computational Linguistics, 1998.
[12] M. Finke, M. Lapata, A. Lavie, L. Levin, L. M. Tomokiyo,
T. Polzin, K. Ries, A. Waibel, and K. Zechner. Clarity:
Inferring discourse structure from speech. In American
Association for Artificial Intelligence (AAAI) Symposium on
Applying Machine Learning to Discourse Processing
Proceedings, Stanford, California, March 1998.
[13] E. Goffman. Frame Analysis: An Essay on the Organization
of Experience. Harper and Row, New York, 1974.
[14] E. Goffman. Forms of Talk. University of Pennsylvania
Press, Philadelphia, Pennsylvania, USA, 1981.
[15] B. J. Grosz. The representation and use of focus in dialogue
understanding. Technical Report 151, SRI International, 333
Ravenswood Ave, Menlo Park, Ca. 94025, 1977.
[16] A. Isard and J. C. Carletta. Replicability of transaction and
action coding in the map task corpus. In M. Walker and
J. Moore, editors, AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Generation, pages
60?67, 1995.
[17] P. W. Jordan. Intentional Influences on Object Redescriptions
in Dialogue: Evidence from an Empirical Study. PhD thesis,
Intelligent Systems Program, University of Pittsburgh, 2000.
[18] D. Jurafsky, E. Shriberg, and D. Biasca. Swbd-damsl
labeling project coder?s manual. Technical report, University
of Colorado, 1997. available as
http://stripe.colorado.edu/ jurafsky/manual.august1.html.
[19] D. Litman. Plan recognition and discourse analysis: An
integrated approach for understanding dialogues. Technical
Report 170, University of Rochester, 1985.
[20] D. Marcu. Perlocutions: The achilles? heel of speech act
theory. Journal of Pragmatics, 1999.
[21] M. G. Moser, J. Moore, and E. Glendening. Instructions for
coding explanations: Identifying segments, relations and
minimal units. Technical Report 96-17, University of
Pittsburgh, Department of Computer Science, 1996.
[22] R. Perrault and J. Allen. A plan-based analysis of indirect
speech acts. American Journal of Computational Linguistics,
6:167?182, 1980.
[23] R. Power. A Computer Model of Conversation. PhD thesis,
University of Edinburgh, 1974.
[24] N. Reithinger and E. Maier. Utilizing statistical speech act
processing in verbmobil. In ACL 95, 1995.
[25] D. R.Traum and E. A. Hinkelman. Conversation acts in
task-oriented spoken dialogue. Computational Intelligence,
8(3):575?599, 1992.
[26] E. A. Schegloff and H. Sacks. Opening up closings.
Semiotica, 8:289?327, 1977.
[27] E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and C. V.
Ess-Dykema. Can prosody aid the automatic classification of
dialog acts in conversational speech. Language and Speech:
Special Issue on Prosody and Conversation, 2000.
[28] M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garofolo,
L. Hirschman, A. Le, S. Lee, S. Narayanan, K. Papineni,
B. Pellom, J. Polifroni, A. Potamianos, P. Prabhu,
A. Rudnicky, G. Sanders, S. Seneff, D. Stallard, and
S. Whittaker. Darpa communicator dialog travel planning
systems: The june 2000 data collection. In Submitted to
EUROSPEECH 2001, 2001.
[29] M. A. Walker. Redundancy in collaborative dialogue. In
Fourteenth International Conference on Computational
Linguistics, pages 345?351, 1992.
[30] M. A. Walker. Limited attention and discourse structure.
Computational Linguistics, 22-2:255?264, 1996.
[31] M. A. Walker, C. A. Kamm, and D. J. Litman. Towards
developing general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems, 2000.
Speaker Utterance Speech Act Task C-Domain
SYS Welcome. opening/closing nil task
You are logged in as a guest user. instruct nil frame
You may say repeat, help me out, start over, or, that?s wrong, instruct nil frame
you can also correct and interrupt the system at any time. instruct nil frame
What airport woodja like to fly out of? request-info origin task
USER MIAMI FLORIDA
SYS Leaving from Miami, implicit-conf origin communication
And, what city are you flying to? request-info dest task
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, implicit-conf origin, dest communication
What date would you like to fly? request-info date task
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, implicit-conf date communication
And what time didja wanna leave? request-info time task
USER UH LATE MORNING
SYS From Minneapolis. implicit-conf origin communication
Are you returning to Miami? request-info trip-type task
USER UM
SYS Are you going back to Miami? request-info trip-type task
Please say yes or no. instruct nil frame
USER NO
SYS Okay, it will just take a moment while I look. status-report retrieval task
SYS I found 3 flights from Miami to Minneapolis on October 23rd . For option one I have a
flight with United departing at 10 A M and arriving at 3 P M with one stopover and
present-info itinerary task
Ticket price is 390 dollars. present-info price task
Please say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS For option two I have a flight with Delta departing at 12 and arriving at 4 P M non-stop. present-info itinerary task
Ticket price is 450 dollars. present-info itinerary task
Would you like me to hold this option? offer itinerary task
USER NEXT OPTION
SYS For the last option I have a flight with Northwest departing at 2 P M and arriving at 7 P M
with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info itinerary task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS Here is your third option again. a flight with Northwest departing at 2 P M and arriving at
7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info itinerary task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. acknowledge booking task
Okay, Is Minneapolis your final destination? request-info trip-type task
USER YES IT IS
SYS Would you like to make any ground arrangements? offer ground task
USER NO
SYS Now, would you like to hear a summary of your itinerary? offer itinerary task
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest Flight 123 departs Miami at 2 P
M, arrives Pittsburgh at 4 30 P M connecting to Northwest Flight 146 that departs Pittsburgh
at 5 P M and arrives Minneapolis at 7 P M.
present-info itinerary task
Figure 1: Dialogue Illustrating the Speech Act, Task-Subtask and Conversational Domain Dimensions of DATE
Quantitative and Qualitative Evaluation of Darpa Communicator
Spoken Dialogue Systems
Marilyn A. Walker
AT&T Labs ? Research
180 Park Ave, E103
Florham Park, NJ. 07932
walker@research.att.com
Rebecca Passonneau
AT&T Labs ?Research
180 Park Ave, D191
Florham Park, NJ. 07932
becky@research.att.com
Julie E. Boland
Institute of Cognitive Science
University of Louisiana at Lafayette
Lafayette, LA 70504
boland@louisiana.edu
Abstract
This paper describes the application of
the PARADISE evaluation framework
to the corpus of 662 human-computer
dialogues collected in the June 2000
Darpa Communicator data collection.
We describe results based on the stan-
dard logfile metrics as well as results
based on additional qualitative metrics
derived using the DATE dialogue act
tagging scheme. We show that per-
formance models derived via using the
standard metrics can account for 37%
of the variance in user satisfaction, and
that the addition of DATE metrics im-
proved the models by an absolute 5%.
1 Introduction
The objective of the DARPA COMMUNICATOR
program is to support research on multi-modal
speech-enabled dialogue systems with advanced
conversational capabilities. In order to make this
a reality, it is important to understand the con-
tribution of various techniques to users? willing-
ness and ability to use a spoken dialogue system.
In June of 2000, we conducted an exploratory
data collection experiment with nine participating
communicator systems. All systems supported
travel planning and utilized some form of mixed-
initiative interaction. However the systems var-
ied in several critical dimensions: (1) They tar-
geted different back-end databases for travel in-
formation; (2) System modules such as ASR,
NLU, TTS and dialogue management were typ-
ically different across systems.
The Evaluation Committee chaired by Walker
(Walker, 2000), with representatives from the
nine COMMUNICATOR sites and from NIST, de-
veloped the experimental design. A logfile stan-
dard was developed by MITRE along with a set
of tools for processing the logfiles (Aberdeen,
2000); the standard and tools were used by all
sites to collect a set of core metrics for making
cross system comparisons. The core metrics were
developed during a workshop of the Evaluation
Committee and included all metrics that anyone
in the committee suggested, that could be imple-
mented consistently across systems. NIST?s con-
tribution was to recruit the human subjects and to
implement the experimental design specified by
the Evaluation Committee.
The experiment was designed to make it possi-
ble to apply the PARADISE evaluation framework
(Walker et al, 2000), which integrates and unifies
previous approaches to evaluation (Price et al,
1992; Hirschman, 2000). The framework posits
that user satisfaction is the overall objective to be
maximized and that task success and various in-
teraction costs can be used as predictors of user
satisfaction. Our results from applying PARADISE
include that user satisfaction differed consider-
ably across the nine systems. Subsequent model-
ing of user satisfaction gave us some insight into
why each system was more or less satisfactory;
four variables accounted for 37% of the variance
in user-satisfaction: task completion, task dura-
tion, recognition accuracy, and mean system turn
duration.
However, when doing our analysis we were
struck by the extent to which different aspects of
the systems? dialogue behavior weren?t captured
by the core metrics. For example, the core met-
rics logged the number and duration of system
turns, but didn?t distinguish between turns used
to request or present information, to give instruc-
tions, or to indicate errors. Recent research on
dialogue has been based on the assumption that
dialogue acts provide a useful way of character-
izing dialogue behaviors (Reithinger and Maier,
1995; Isard and Carletta, 1995; Shriberg et al,
2000; Di Eugenio et al, 1998). Several research
efforts have explored the use of dialogue act tag-
ging schemes for tasks such as improving recog-
nition performance (Reithinger and Maier, 1995;
Shriberg et al, 2000), identifying important parts
of a dialogue (Finke et al, 1998), and as a con-
straint on nominal expression generation (Jordan,
2000). Thus we decided to explore the applica-
tion of a dialogue act tagging scheme to the task
of evaluating and comparing dialogue systems.
Section 2 describes the corpus. Section 3 de-
scribes the dialogue act tagging scheme we de-
veloped and applied to the evaluation of COM-
MUNICATOR dialogues. Section 4 first describes
our results utilizing the standard logged metrics,
and then describes results using the DATE met-
rics. Section 5 discusses future plans.
2 The Communicator 2000 Corpus
The corpus consists of 662 dialogues from nine
different travel planning systems with the num-
ber of dialogues per system ranging between 60
and 79. The experimental design is described
in (Walker et al, 2001). Each dialogue consists
of a recording, a logfile consistent with the stan-
dard, transcriptions and recordings of all user ut-
terances, and the output of a web-based user sur-
vey. Metrics collected per call included:
  Dialogue Efficiency: Task Duration, System turns,
User turns, Total Turns
  Dialogue Quality: Word Accuracy, Response latency,
Response latency variance
  Task Success: Exact Scenario Completion
  User Satisfaction: Sum of TTS performance, Task
ease, User expertise, Expected behavior, Future use.
The objective metrics focus on measures that
can be automatically logged or computed and a
web survey was used to calculate User Satisfac-
tion (Walker et al, 2001). A ternary definition
of task completion, Exact Scenario Completion
(ESC) was annotated by hand for each call by an-
notators at AT&T. The ESC metric distinguishes
between exact scenario completion (ESC), any
scenario completion (ANY) and no scenario com-
pletion (NOCOMP). This metric arose because
some callers completed an itinerary other than
the one assigned. This could have been due to
users? inattentiveness, e.g. users didn?t correct the
system when it had misunderstood them. In this
case, the system could be viewed as having done
the best that it could with the information that it
was given. This would argue that task completion
would be the sum of ESC and ANY. However,
examination of the dialogue transcripts suggested
that the ANY category sometimes arose as a ratio-
nal reaction by the caller to repeated recognition
error. Thus we decided to distinguish the cases
where the user completed the assigned task, ver-
sus completing some other task, versus the cases
where they hung up the phone without completing
any itinerary.
3 Dialogue Act Tagging for Evaluation
The hypothesis underlying the application of di-
alogue act tagging to system evaluation is that
a system?s dialogue behaviors have a strong ef-
fect on the usability of a spoken dialogue sys-
tem. However, each COMMUNICATOR system has
a unique dialogue strategy and a unique way of
achieving particular communicative goals. Thus,
in order to explore this hypothesis, we needed a
way of characterizing system dialogue behaviors
that could be applied uniformly across the nine
different communicator travel planning systems.
We developed a dialogue act tagging scheme for
this purpose which we call DATE (Dialogue Act
Tagging for Evaluation).
In developing DATE, we believed that it was
important to allow for multiple views of each
dialogue act. This would allow us, for ex-
ample, to investigate what part of the task an
utterance contributes to separately from what
speech act function it serves. Thus, a cen-
tral aspect of DATE is that it makes distinc-
tions within three orthogonal dimensions of ut-
terance classification: (1) a SPEECH-ACT dimen-
sion; (2) a TASK-SUBTASK dimension; and (3) a
CONVERSATIONAL-DOMAIN dimension. We be-
lieve that these distinctions are important for us-
ing such a scheme for evaluation. Figure 1 shows
a COMMUNICATOR dialogue with each system ut-
terance classified on these three dimensions. The
tagset for each dimension are briefly described in
the remainder of this section. See (Walker and
Passonneau, 2001) for more detail.
3.1 Speech Acts
In DATE, the SPEECH-ACT dimension has ten cat-
egories. We use familiar speech-act labels, such
as OFFER, REQUEST-INFO, PRESENT-INFO, AC-
KNOWLEDGE, and introduce new ones designed
to help us capture generalizations about commu-
nicative behavior in this domain, on this task,
given the range of system and human behavior
we see in the data. One new one, for example,
is STATUS-REPORT. Examples of each speech-act
type are in Figure 2.
Speech-Act Example
REQUEST-INFO And, what city are you flying to?
PRESENT-INFO The airfare for this trip is 390 dol-
lars.
OFFER Would you like me to hold this op-
tion?
ACKNOWLEDGE I will book this leg.
STATUS-REPORT Accessing the database; this
might take a few seconds.
EXPLICIT-
CONFIRM
You will depart on September 1st.
Is that correct?
IMPLICIT-
CONFIRM
Leaving from Dallas.
INSTRUCTION Try saying a short sentence.
APOLOGY Sorry, I didn?t understand that.
OPENING/CLOSING Hello. Welcome to the C M U
Communicator.
Figure 2: Example Speech Acts
3.2 Conversational Domains
The CONVERSATIONAL-DOMAIN dimension in-
volves the domain of discourse that an utterance
is about. Each speech act can occur in any of three
domains of discourse described below.
The ABOUT-TASK domain is necessary for
evaluating a dialogue system?s ability to collab-
orate with a speaker on achieving the task goal of
making reservations for a specific trip. It supports
metrics such as the amount of time/effort the sys-
tem takes to complete a particular phase of mak-
ing an airline reservation, and any ancillary ho-
tel/car reservations.
The ABOUT-COMMUNICATION domain re-
flects the system goal of managing the verbal
channel and providing evidence of what has been
understood (Walker, 1992; Clark and Schaefer,
1989). Utterances of this type are frequent in
human-computer dialogue, where they are moti-
vated by the need to avoid potentially costly er-
rors arising from imperfect speech recognition.
All implicit and explicit confirmations are about
communication; See Figure 1 for examples.
The SITUATION-FRAME domain pertains to the
goal of managing the culturally relevant framing
expectations (Goffman, 1974). The utterances in
this domain are particularly relevant in human-
computer dialogues because the users? expecta-
tions need to be defined during the course of the
conversation. About frame utterances by the sys-
tem attempt to help the user understand how to in-
teract with the system, what it knows about, and
what it can do. Some examples are in Figure 1.
3.3 Task Model
The TASK-SUBTASK dimension refers to a task
model of the domain task that the system sup-
ports and captures distinctions among dialogue
acts that reflect the task structure.1 The motiva-
tion for this dimension is to derive metrics that
quantify the effort expended on particular sub-
tasks.
This dimension distinguishes among 14 sub-
tasks, some of which can also be grouped at
a level below the top level task.2, as described
in Figure 3. The TOP-LEVEL-TRIP task de-
scribes the task which contains as its subtasks the
ORIGIN, DESTINATION, DATE, TIME, AIRLINE,
TRIP-TYPE, RETRIEVAL and ITINERARY tasks.
The GROUND task includes both the HOTEL and
CAR subtasks.
Note that any subtask can involve multiple
speech acts. For example, the DATE subtask can
consist of acts requesting, or implicitly or explic-
itly confirming the date. A similar example is pro-
vided by the subtasks of CAR (rental) and HOTEL,
which include dialogue acts requesting, confirm-
ing or acknowledging arrangements to rent a car
or book a hotel room on the same trip.
1This dimension elaborates of each speech-act type in
other tagging schemes (Reithinger and Maier, 1995).
2In (Walker and Passonneau, 2001) we didn?t distinguish
the price subtask from the itinerary presentation subtask.
Task Example
TOP-LEVEL-
TRIP
What are your travel plans?
ORIGIN And, what city are you leaving from?
DESTINATION And, where are you flying to?
DATE What day would you like to leave?
TIME Departing at what time?.
AIRLINE Did you have an airline preference?
TRIP-TYPE Will you return to Boston from San Jose?
RETRIEVAL Accessing the database; this might take
a few seconds.
ITINERARY I found 3 flights from Miami to Min-
neapolis.
PRICE The airfare for this trip is 390 dollars.
GROUND Did you need to make any ground ar-
rangements?.
HOTEL Would you like a hotel near downtown
or near the airport?.
CAR Do you need a car in San Jose?
Figure 3: Example Utterances for each Subtask
3.4 Implementation and Metrics Derivation
We implemented a dialogue act parser that clas-
sifies each of the system utterances in each dia-
logue in the COMMUNICATOR corpus. Because
the systems used template-based generation and
had only a limited number of ways of saying the
same content, it was possible to achieve 100% ac-
curacy with a parser that tags utterances automat-
ically from a database of patterns and the corre-
sponding relevant tags from each dimension.
A summarizer program then examined each di-
alogue?s labels and summed the total effort ex-
pended on each type of dialogue act over the
dialogue or the percentage of a dialogue given
over to a particular type of dialogue behavior.
These sums and percentages of effort were calcu-
lated along the different dimensions of the tagging
scheme as we explain in more detail below.
We believed that the top level distinction be-
tween different domains of action might be rel-
evant so we calculated percentages of the to-
tal dialogue expended in each conversational do-
main, resulting in metrics of TaskP, FrameP and
CommP (the percentage of the dialogue devoted
to the task, the frame or the communication do-
mains respectively).
We were also interested in identifying differ-
ences in effort expended on different subtasks.
The effort expended on each subtask is repre-
sented by the sum of the length of the utterances
contributing to that subtask. These are the met-
rics: TripC, OrigC, DestC, DateC, TimeC, Air-
lineC, RetrievalC, FlightinfoC, PriceC, GroundC,
BookingC. See Figure 3.
We were particularly interested developing
metrics related to differences in the system?s di-
alogue strategies. One difference that the DATE
scheme can partially capture is differences in con-
firmation strategy by summing the explicit and
implicit confirms. This introduces two metrics
ECon and ICon, which represent the total effort
spent on these two types of confirmation.
Another strategy difference is in the types of
about frame information that the systems pro-
vide. The metric CINSTRUCT counts instances
of instructions, CREQAMB counts descriptions
provided of what the system knows about in the
context of an ambiguity, and CNOINFO counts
the system?s descriptions of what it doesn?t know
about. SITINFO counts dialogue initial descrip-
tions of the system?s capabilities and instructions
for how to interact with the system
A final type of dialogue behavior that the
scheme captures are apologies for misunderstand-
ing (CREJECT), acknowledgements of user re-
quests to start over (SOVER) and acknowledg-
ments of user corrections of the system?s under-
standing (ACOR).
We believe that it should be possible to use
DATE to capture differences in initiative strate-
gies, but currently only capture differences at the
task level using the task metrics above. The TripC
metric counts open ended questions about the
user?s travel plans, whereas other subtasks typi-
cally include very direct requests for information
needed to complete a subtask.
We also counted triples identifying dialogue
acts used in specific situations, e.g. the utterance
Great! I am adding this flight to your itinerary
is the speech act of acknowledge, in the about-
task domain, contributing to the booking subtask.
This combination is the ACKBOOKING metric.
We also keep track of metrics for dialogue acts
of acknowledging a rental car booking or a hotel
booking, and requesting, presenting or confirm-
ing particular items of task information. Below
we describe dialogue act triples that are signifi-
cant predictors of user satisfaction.
Metric Coefficient P value
ESC 0.45 0.000
TaskDur -0.15 0.000
Sys Turn Dur 0.12 0.000
Wrd Acc 0.17 0.000
Table 1: Predictive power and significance of
Core Metrics
4 Results
We initially examined differences in cumulative
user satisfaction across the nine systems. An
ANOVA for user satisfaction by Site ID using the
modified Bonferroni statistic for multiple com-
parisons showed that there were statistically sig-
nificant differences across sites, and that there
were four groups of performers with sites 3,2,1,4
in the top group (listed by average user satisfac-
tion), sites 4,5,9,6 in a second group, and sites 8
and 7 defining a third and a fourth group. See
(Walker et al, 2001) for more detail on cross-
system comparisons.
However, our primary goal was to achieve a
better understanding of the role of qualitative as-
pects of each system?s dialogue behavior. We
quantify the extent to which the dialogue act
metrics improve our understanding by applying
the PARADISE framework to develop a model of
user satisfaction and then examining the extent
to which the dialogue act metrics improve the
model (Walker et al, 2000). Section 4.1 describes
the PARADISE models developed using the core
metrics and section 4.2 describes the models de-
rived from adding in the DATE metrics.
4.1 Results using Logfile Standard Metrics
We applied PARADISE to develop models of user
satisfaction using the core metrics; the best model
fit accounts for 37% of the variance in user sat-
isfaction. The learned model is that User Sat-
isfaction is the sum of Exact Scenario Comple-
tion, Task Duration, System Turn Duration and
Word Accuracy. Table 1 gives the details of the
model, where the coefficient indicates both the
magnitude and whether the metric is a positive or
negative predictor of user satisfaction, and the P
value indicates the significance of the metric in
the model.
The finding that metrics of task completion and
Metric Coefficient P value
ESC (Completion) 0.40 0.00
Task Dur -0.31 0.00
Sys Turn Dur 0.14 0.00
Word Accuracy 0.15 0.00
TripC 0.09 0.01
BookingC 0.08 0.03
PriceC 0.11 0.00
AckRent 0.07 0.05
EconTime 0.05 0.13
ReqDate 0.10 0.01
ReqTripType 0.09 0.00
Econ 0.11 0.01
Table 2: Predictive power and significance of Di-
alogue Act Metrics
recognition performance are significant predic-
tors duplicates results from other experiments ap-
plying PARADISE (Walker et al, 2000). The fact
that task duration is also a significant predictor
may indicate larger differences in task duration in
this corpus than in previous studies.
Note that the PARADISE model indicates that
system turn duration is positively correlated with
user satisfaction. We believed it plausible that this
was due to the fact that flight presentation utter-
ances are longer than other system turns. Thus
this metric simply captures whether or not the sys-
tem got enough information to present some po-
tential flight itineraries to the user. We investigate
this hypothesis further below.
4.2 Utilizing Dialogue Parser Metrics
Next, we add in the dialogue act metrics extracted
by our dialogue parser, and retrain our models of
user satisfaction. We find that many of the dia-
logue act metrics are significant predictors of user
satisfaction, and that the model fit for user sat-
isfaction increases from 37% to 42%. The dia-
logue act metrics which are significant predictors
of user satisfaction are detailed in Table 2.
When we examine this model, we note that sev-
eral of the significant dialogue act metrics are cal-
culated along the task-subtask dimension, namely
TripC, BookingC and PriceC. One interpretation
of these metrics are that they are acting as land-
marks in the dialogue for having achieved a par-
ticular set of subtasks. The TripC metric can
be interpreted this way because it includes open
ended questions about the user?s travel plans both
at the beginning of the dialogue and also after
one itinerary has been planned. Other signif-
icant metrics can also be interpreted this way;
for example the ReqDate metric counts utterances
such as Could you tell me what date you wanna
travel? which are typically only produced after
the origin and the destination have been under-
stood. The ReqTripType metric counts utterances
such as From Boston, are you returning to Dal-
las? which are only asked after all the first infor-
mation for the first leg of the trip have been ac-
quired, and in some cases, after this information
has been confirmed. The AckRental metric has a
similar potential interpretation; the car rental task
isn?t attempted until after the flight itinerary has
been accepted by the caller. However, the predic-
tors for the models already include a ternary exact
scenario completion metric (ESC) which speci-
fies whether any task was achieved or not, and
whether the exact task that the user was attempt-
ing to accomplish was achieved. The fact that the
addition of these dialogue metrics improves the fit
of the user satisfaction model suggests that per-
haps a finer grained distinction on how many of
the subtasks of a dialogue were completed is re-
lated to user satisfaction. This makes sense; a user
who the system hung up on immediately should
be less satisfied than one who never could get the
system to understand his destination, and both of
these should be less satisfied than a user who was
able to communicate a complete travel plan but
still did not complete the task.
Other support for the task completion related
nature of some of the significant metrics is that
the coefficient for ESC is smaller in the model
in Table 2 than in the model in Table 1. Note
also that the coefficient for Task Duration is much
larger. If some of the dialogue act metrics that are
significant predictors are mainly so because they
indicate the successful accomplishment of partic-
ular subtasks, then both of these changes would
make sense. Task Duration can be a greater nega-
tive predictor of user satisfaction, only when it is
counteracted by the positive coefficients for sub-
task completion.
The TripC and the PriceC metrics also have
other interpretations. The positive contribution of
the TripC metric to user satisfaction could arise
from a user?s positive response to systems with
open-ended initial greetings which give the user
the initiative. The positive contribution of the
PriceC metric might indicate the users? positive
response to getting price information, since not
all systems provided price information.
As mentioned above, our goal was to de-
velop metrics that captured differences in dia-
logue strategies. The positive coefficient of the
Econ metric appears to indicate that an explicit
confirmation strategy overall leads to greater user
satisfaction than an implicit confirmation strategy.
This result is interesting, although it is unclear
how general it is. The systems that used an ex-
plicit confirmation strategy did not use it to con-
firm each item of information; rather the strategy
seemed to be to acquire enough information to go
to the database and then confirm all of the param-
eters before accessing the database. The other use
of explicit confirms was when a system believed
that it had repeatedly misunderstood the user.
We also explored the hypothesis that the rea-
son that system turn duration was a predictor of
user satisfaction is that longer turns were used
to present flight information. We removed sys-
tem turn duration from the model, to determine
whether FlightInfoC would become a significant
predictor. However the model fit decreased and
FlightInfoC was not a significant predictor. Thus
it is unclear to us why longer system turn dura-
tions are a significant positive predictor of user
satisfaction.
5 Discussion and Future Work
We showed above that the addition of dialogue act
metrics improves the fit of models of user satis-
faction from 37% to 42%. Many of the significant
dialogue act metrics can be viewed as landmarks
in the dialogue for having achieved particular sub-
tasks. These results suggest that a careful defi-
nition of transaction success, based on automatic
analysis of events in a dialogue, such as acknowl-
edging a booking, might serve as a substitute for
the hand-labelling of task completion.
In current work we are exploring the use of tree
models and boosting for modeling user satisfac-
tion. Tree models using dialogue act metrics can
achieve model fits as high as 48% reduction in
error. However, we need to test both these mod-
els and the linear PARADISE models on unseen
data. Furthermore, we intend to explore methods
for deriving additional metrics from dialogue act
tags. In particular, it is possible that sequential or
structural metrics based on particular sequences
or configurations of dialogue acts might capture
differences in dialogue strategies.
We began a second data collection of dialogues
with COMMUNICATOR travel systems in April
2001. In this data collection, the subject pool will
use the systems to plan real trips that they intend
to take. As part of this data collection, we hope
to develop additional metrics related to the qual-
ity of the dialogue, how much initiative the user
can take, and the quality of the solution that the
system presents to the user.
6 Acknowledgements
This work was supported under DARPA GRANT
MDA 972 99 3 0003 to AT&T Labs Research.
Thanks to the evaluation committee members:
J. Aberdeen, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Narayanan, K. Papineni, B. Pellom,
A. Potamianos, A. Rudnicky, G. Sanders, S. Sen-
eff, and D. Stallard who contributed to 2000
COMMUNICATOR data collection.
References
John Aberdeen. 2000. Darpa communicator logfile
standard. http://fofoca.mitre.org/logstandard.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?
294.
Barbara Di Eugenio, Pamela W. Jordan, Johanna D.
Moore, and Richmond H. Thomason. 1998. An
empirical investigation of collaborative dialogues.
In ACL-COLING98, Proc. of the 36th Conference
of the Association for Computational Linguistics.
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
field Tomokiyo, T. Polzin, K. Ries, A. Waibel, and
K. Zechner. 1998. Clarity: Inferring discourse
structure from speech. In AAAI Symposium on
Applying Machine Learning to Discourse Process-
ing.
Erving Goffman. 1974. Frame Analysis: An Essay on
the Organization of Experience. Harper and Row,
New York.
Lynette Hirschman. 2000. Evaluating spoken lan-
guage interaction: Experiences from the darpa spo-
ken language program 1990?1995. In S. Luperfoy,
editor, Spoken Language Discourse. MIT Press,
Cambridge, Mass.
Amy Isard and Jean C. Carletta. 1995. Replicabil-
ity of transaction and action coding in the map task
corpus. In AAAI Spring Symposium: Empirical
Methods in Discourse Interpretation and Genera-
tion, pages 60?67.
Pamela W. Jordan. 2000. Intentional Influences on
Object Redescriptions in Dialogue: Evidence from
an Empirical Study. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Patti Price, Lynette Hirschman, Elizabeth Shriberg,
and Elizabeth Wade. 1992. Subject-based evalu-
ation measures for interactive spoken language sys-
tems. In Proc. of the DARPA Speech and NL Work-
shop, pages 34?39.
Norbert Reithinger and Elisabeth Maier. 1995. Utiliz-
ing statistical speech act processing in verbmobil.
In ACL 95.
E. Shriberg, P. Taylor, R. Bates, A. Stolcke, K. Ries,
D. Jurafsky, N. Coccaro, R. Martin, M. Meteer, and
C. Van Ess-Dykema. 2000. Can prosody aid the
automatic classification of dialog acts in conversa-
tional speech. Language and Speech: Special Issue
on Prosody and Conversation.
M. Walker and R. Passonneau. 2001. Date: A dia-
logue act tagging scheme for evaluation. In Human
Language Technology Conference.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general models
of usability with PARADISE. Natural Language
Engineering: Special Issue on Best Practice in Spo-
ken Dialogue Systems.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayanan,
K. Papineni, B. Pellom, J. Polifroni, A. Potami-
anos, P. Prabhu, A. Rudnicky, G. Sanders, S. Sen-
eff, D. Stallard, and S. Whittaker. 2001. Darpa
communicator dialog travel planning systems: The
june 2000 data collection. In Submitted to EU-
ROSPEECH 2001.
Marilyn A. Walker. 1992. Redundancy in collabora-
tive dialogue. In Fourteenth International Confer-
ence on Computational Linguistics, pages 345?351.
Marilyn Walker. 2000. Communi-
cator evaluation committee webpage.
http://www.research.att.com/ walker/eval/eval.html.
Speaker Utterance Speech Act Task C-Domain
SYS Welcome. opening/closing nil task
You are logged in as a guest user. instruct nil frame
You may say repeat, help me out, start over, or, that?s wrong, instruct nil frame
you can also correct and interrupt the system at any time. instruct nil frame
What airport woodja like to fly out of? request-info origin task
USER MIAMI FLORIDA
SYS Leaving from Miami, implicit-conf origin communication
And, what city are you flying to? request-info dest task
USER MINNEAPOLIS MINNESOTA
SYS Flying from Miami to Minneapolis, implicit-conf origin, dest communication
What date would you like to fly? request-info date task
USER UH MONDAY OCTOBER TWENTY THIRD
SYS Leaving on the 23rd of October, implicit-conf date communication
And what time didja wanna leave? request-info time task
USER UH LATE MORNING
SYS From Minneapolis. implicit-conf origin communication
Are you returning to Miami? request-info trip-type task
USER UM
SYS Are you going back to Miami? request-info trip-type task
Please say yes or no. instruct nil frame
USER NO
SYS Okay, it will just take a moment while I look. status-report retrieval task
SYS I found 3 flights from Miami to Minneapolis on October 23rd .
For option one I have a flight with United departing at 10 A M
and arriving at 3 P M with one stopover and
present-info itinerary task
Ticket price is 390 dollars. present-info price task
Please say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS For option two I have a flight with Delta departing at 12 and
arriving at 4 P M non-stop.
present-info itinerary task
Ticket price is 450 dollars. present-info price task
Would you like me to hold this option? offer itinerary task
USER NEXT OPTION
SYS For the last option I have a flight with Northwest departing at 2
P M and arriving at 7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info price task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER NEXT OPTION
SYS Here is your third option again. a flight with Northwest departing
at 2 P M and arriving at 7 P M with 1 stopover and
present-info itinerary task
Ticket price is 470 dollars. present-info price task
say next option, or, flight details, or, I?ll take it. offer itinerary task
USER I?LL TAKE IT
SYS Great! I am adding this flight to your itinerary. acknowledge booking task
Okay, Is Minneapolis your final destination? request-info trip-type task
USER YES IT IS
SYS Would you like to make any ground arrangements? offer ground task
USER NO
SYS Now, would you like to hear a summary of your itinerary? offer itinerary task
USER YES PLEASE
SYS Okay, Here?s your itinerary. On October 23rd Northwest Flight
123 departs Miami at 2 P M, arrives Pittsburgh at 4 30 P M con-
necting to Northwest Flight 146 that departs Pittsburgh at 5 P M
and arrives Minneapolis at 7 P M.
present-info itinerary task
Figure 1: Dialogue Illustrating the Speech Act, Task-Subtask and Conversational Domain Dimensions
of DATE
Evaluating Content Selection in Summarization: The Pyramid Method
Ani Nenkova and Rebecca Passonneau
Columbia University
Computer Science Department
New York, NY 10027
fani,beckyg@cs.columbia.edu
Abstract
We present an empirically grounded method
for evaluating content selection in summariza-
tion. It incorporates the idea that no single best
model summary for a collection of documents
exists. Our method quantifies the relative im-
portance of facts to be conveyed. We argue that
it is reliable, predictive and diagnostic, thus im-
proves considerably over the shortcomings of
the human evaluation method currently used in
the Document Understanding Conference.
1 Introduction
Evaluating content selection in summarization has proven
to be a difficult problem. Our approach acknowledges
the fact that no single best model summary exists, and
takes this as a foundation rather than an obstacle. In ma-
chine translation, the rankings from the automatic BLEU
method (Papineni et al, 2002) have been shown to corre-
late well with human evaluation, and it has been widely
used since and has even been adapted for summarization
(Lin and Hovy, 2003). To show that an automatic method
is a reasonable approximation of human judgments, one
needs to demonstrate that these can be reliably elicited.
However, in contrast to translation, where the evaluation
criterion can be defined fairly precisely it is difficult to
elicit stable human judgments for summarization (Rath
et al, 1961) (Lin and Hovy, 2002).
Our approach tailors the evaluation to observed dis-
tributions of content over a pool of human summaries,
rather than to human judgments of summaries. Our
method involves semantic matching of content units to
which differential weights are assigned based on their fre-
quency in a corpus of summaries. This can lead to more
stable, more informative scores, and hence to a meaning-
ful content evaluation. We create a weighted inventory of
Summary Content Units?a pyramid?that is reliable, pre-
dictive and diagnostic, and which constitutes a resource
for investigating alternate realizations of the same mean-
ing. No other evaluation method predicts sets of equally
informative summaries, identifies semantic differences
between more and less highly ranked summaries, or con-
stitutes a tool that can be applied directly to further anal-
ysis of content selection.
In Section 2, we describe the DUC method. In Sec-
tion 3 we present an overview of our method, contrast
our scores with other methods, and describe the distribu-
tion of scores as pyramids grow in size. We compare our
approach with previous work in Section 4. In Section 5,
we present our conclusions and point to our next step, the
feasibility of automating our method. A more detailed
account of the work described here, but not including the
study of distributional properties of pyramid scores, can
be found in (Passonneau and Nenkova, 2003).
2 Current Approach: the Document
Understanding Conference
2.1 DUC
Within DUC, different types of summarization have been
studied: the generation of abstracts and extracts of differ-
ent lengths, single- and multi-document summaries, and
summaries focused by topic or opinion. Evaluation in-
volves comparison of a peer summary (baseline, or pro-
duced by human or system) by comparing its content to
a gold standard, or model. In 2003 they provided four
human summaries for each of the 30 multi-document test
sets, any one of which could serve as the model, with no
criteria for choosing among possible models.
The four human summaries for each of the 2003 docu-
ment sets made our study possible. As described in Sec-
tion 3, we used three of these sets, and collected six addi-
tional summaries per set, in order to study the distribution
of content units across increasingly many summaries.
2.2 DUC evaluation procedure
The procedure used for evaluating summaries in DUC is
the following:
1. A human subject reads the entire input set and cre-
ates a 100 word summary for it, called a model.
2. The model summary is split into content units,
roughly equal to clauses or elementary discourse
units (EDUs). This step is performed automatically
using a tool for EDU annotation developed at ISI.1
3. The summary to be evaluated (a peer) is automat-
ically split into sentences. (Thus the content units
are of different granularity?EDUs for the model,
and sentences for the peer).
1http://www.isi.edu/licensed-sw/spade/.
4. Then a human judge evaluates the peer against the
model using the following instructions: For each
model content unit:
(a) Find all peer units that express at least some
facts from the model unit and mark them.
(b) After all such peer units are marked, think about
the whole set of marked peer units and answer
the question:
(c) ?The marked peer units, taken together, express
about k% of the meaning expressed by the cur-
rent model unit?, where k can be equal to 0, 20,
40, 60, 80 and 100.
The final score is based on the content unit coverage.
In the official DUC results tables, the score for the entire
summary is the average of the scores of all the content
model units, thus a number between 0 and 1. Some par-
ticipants use slightly modified versions of the coverage
metric, where the proportion of marked peer units to the
number of model units is factored in.
The selection of units with the same content is facili-
tated by the use of the Summary Evaluation Environment
(SEE)2 developed at ISI, which displays the model and
peer summary side by side and allows the user to make
selections by using a mouse.
2.3 Problems with the DUC evaluation
There are numerous problems with the DUC human eval-
uation method. The use of a single model summary is
one of the surprises ? all research in summarization eval-
uation has indicated that no single good model exists.
Also, since not much agreement is expected between two
summaries, many model units will have no counterpart
in the peer and thus the expected scores will necessarily
be rather low. Additionally, the task of determining the
percentage overlap between two text units turns out to be
difficult to annotate reliably ? (Lin and Hovy, 2002) re-
port that humans agreed with their own prior judgment in
only 82% of the cases.
These methodological anomalies lead to unreliable
scores. Human-written summaries can score as low as
0.1 while machine summaries can score as high as 0.5.
For each of the 30 test sets, three of the four human-
written summaries and the machine summaries were
scored against the fourth human model summary: each
human was scored on ten summaries. Figure 1 shows
a scatterplot of human scores for all 30 sets, and illus-
trates an apparently random relation of summarizers to
each other, and to document sets. This suggests that the
DUC scores cannot be used to distinguish a good human
summarizer from a bad one. In addition, the DUC method
is not powerful enough to distinguish between systems.
2http://www.isi.edu/?cyl/SEE.
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
data$docset
da
ta
$d
uc
sc
ore
0 5 10 15 20 25 30
0.
2
0.
4
0.
6
0.
8
Figure 1: Scatterplot for DUC 2003 Human Summaries
3 The Pyramid Approach
Our analysis of summary content is based on Summa-
rization Content Units, or SCUs and we will now pro-
ceed to define the concept. SCUs emerge from annota-
tion of a corpus of summaries and are not bigger than a
clause. Rather than attempting to provide a semantic or
functional characterisation of what an SCU is, our anno-
tation procedure defines how to compare summaries to
locate the same or different SCUs.
The following example of the emergence of two SCUs
is taken from a DUC 2003 test set. The sentences are
indexed by a letter and number combination, the letter
showing which summary the sentence came from and the
number indicating the position of the sentence within its
respective summary.
A1 In 1998 two Libyans indicted in 1991 for the Locker-
bie bombing were still in Libya.
B1 Two Libyans were indicted in 1991 for blowing up a
Pan Am jumbo jet over Lockerbie, Scotland in 1988.
C1 Two Libyans, accused by the United States and
Britain of bombing a New York bound Pan Am jet over
Lockerbie, Scotland in 1988, killing 270 people, for 10
years were harbored by Libya who claimed the suspects
could not get a fair trail in America or Britain.
D2 Two Libyan suspects were indicted in 1991.
The annotation starts with identifying similar sen-
tences, like the four above, and then proceeds with
finer grained inspection that can lead to identifying
more tightly related subparts. We obtain two SCUs
from the underlined portions of the sentences above.
Each SCU has a weight corresponding to the number of
summaries it appears in; SCU1 has weight=4 and SCU2
has weight=33:
3The grammatical constituents contributing to an SCU are
bracketed and coindexed with the SCU ID.
SCU1 (w=4): two Libyans were officially accused of the
Lockerbie bombing
A1 [two Libyans]1 [indicted]1
B1 [Two Libyans were indicted]1
C1 [Two Libyans,]1 [accused]1
D2 [Two Libyan suspects were indicted]1
SCU2 (w=3): the indictment of the two Lockerbie
suspects was in 1991
A1 [in 1991]2
B1 [in 1991]2
D2 [in 1991.]2
The remaining parts of the four sentences above end up
as contributors to nine different SCUs of different weight
and granularity. Though we look at multidocument sum-
maries rather than single document ones, SCU annotation
otherwise resembles the annotation of factoids in (Hal-
teren and Teufel, 2003); as they do with factoids, we find
increasing numbers of SCUs as the pool of summaries
grows. For our 100 word summaries, we find about 34-
40 distinct SCUs across four summaries; with ten sum-
maries this number grows to about 60. A more complete
comparison of the two approaches follows in section 4.
An SCU consists of a set of contributors that, in their
sentential contexts, express the same semantic content.
An SCU has a unique index, a weight, and a natural
language label. The label, which is subject to revision
throughout the annotation process, has three functions.
First, it frees the annotation process from dependence on
a semantic representation language. Second, it requires
the annotator to be conscious of a specific meaning shared
by all contributors. Third, because the contributors to an
SCU are taken out of context, the label serves as a re-
minder of the full in-context meaning, as in the case of
SCU2 above where the temporal PPs are about a specific
event, the time of the indictment.
Our impression from consideration of three SCU in-
ventories is that the pattern illustrated here between
SCU1 and SCU2 is typical; when two SCUs are seman-
tically related, the one with the lower weight is semanti-
cally dependent on the other. We have catalogued a vari-
ety of such relationships, and note here that we believe it
could prove useful to address semantic interdependencies
among SCUS in future work that would involve adding a
new annotation layer.4 However, in our approach, SCUs
are treated as independent annotation values, which has
the advantage of affording a rigorous analysis of inter-
annotator reliability (see following section). We do not
attempt to represent the subsumption or implicational re-
4We are currently investigating the possibility of incorporat-
ing narrative relations into SCU pyramids in collaboration with
cognitive psychologists.
W=4
W=1
W=2
W=3
W=4
W=1
W=2
W=3
Figure 2: Two of six optimal summaries with 4 SCUs
lations that Halteren and Teufel assign to factoids (Hal-
teren and Teufel, 2003).
After the annotation procedure is completed, the final
SCUs can be partitioned in a pyramid. The partition is
based on the weight of the SCU; each tier contains all
and only the SCUs with the same weight. When we use
annotations from four summaries, the pyramid will con-
tain four tiers. SCUs of weight 4 are placed in the top tier
and SCUs of weight 1 on the bottom, reflecting the fact
that fewer SCUs are expressed in all summaries, more
in three, and so on. For the mid-range tiers, neighbor-
ing tiers sometimes have the same number of SCUs. In
descending tiers, SCUs become less important informa-
tionally since they emerged from fewer summaries.
We use the term ?pyramid of order n? to refer to a pyra-
mid with n tiers. Given a pyramid of order n, we can
predict the optimal summary content?it should contain
all the SCUs from the top tier, if length permits, SCUs
from the next tier and so on. In short, an SCU from
tier (n ? 1) should not be expressed if all the SCUs in
tier n have not been expressed. This characterization of
optimal content ignores many complicating factors (e.g.,
ordering, SCU interdependency). However, it is predic-
tive: among summaries produced by humans, many seem
equally good without having identical content. Figure
2, with two SCUs in the uppermost tier and four in the
next, illustrates two of six optimal summaries of size 4
(in SCUs) that this pyramid predicts.
The score we assign is a ratio of the sum of the weights
of its SCUs to the sum of the weights of an optimal sum-
mary with the same number of SCUs. It ranges from 0
to 1, with higher scores indicating that relatively more of
the content is as highly weighted as possible.
The exact formula we use is computed as follows. Sup-
pose the pyramid has n tiers, T
i
, with tier T
n
on top and
T
1
on the bottom. The weight of SCUs in tier T
i
will be
i.5 Let jT
i
j denote the number of SCUs in tier T
i
. Let D
i
be the number of SCUs in the summary that appear in T
i
.
SCUs in a summary that do not appear in the pyramid are
assigned weight zero. The total SCU weight D is:
D =
?
n
i=1
i  D
i
5This weight is not fixed and the method does not depend
on the specific weights assigned. The weight assignment used
is simply the most natural and intuitive one.
The optimal content score for a summary with X SCUs
is:
Max =
n
?
i=j+1
i  jT
i
j + j  (X ?
n
?
i=j+1
jT
i
j)
where j = max
i
(
n
?
t=i
jT
t
j  X) (1)
In the equation above, j is equal to the index of the
lowest tier an optimally informative summary will draw
from. This tier is the first one top down such that the
sum of its cardinality and the cardinalities of tiers above
it is greater than or equal to X (summary size in SCUs).
For example, if X is less than the cardinality of the most
highly weighted tier, then j = n and Max is simply Xn
(the product of X and the highest weighting factor).
Then the pyramid score P is the ratio of D to Max.
Because P compares the actual distribution of SCUs to
an empirically determined weighting, it provides a direct
correlate of the way human summarizers select informa-
tion from source texts.
3.1 Reliability and Robustness
We aimed for an annotation method requiring relatively
little training, and with sufficient interannotator reliabil-
ity to produce a stable pyramid score. Here we present re-
sults indicating good interannotator reliability, and pyra-
mid scores that are robust across annotations.
SCU annotation involves two types of choices: extract-
ing a contributor from a sentence, and assigning it to an
SCU. In a set of four summaries about the Philippine Air-
lines (PAL), two coders (C1 and C2; the co-authors) dif-
fered on the extent of the following contributor: f
C1
after
f
C2
the ground crew union turned down a settlementg
C1
whichg
C2
. Our approach is to separate syntactic from se-
mantic agreement, as in (Klavans et al, 2003). Because
constituent structure is not relevant here, we normalize all
contributors before computing reliability.
We treat every word in a summary as a coding unit, and
the SCU it was assigned to as the coding value. We re-
quire every surface word to be in exactly one contributor,
and every contributor to be in exactly one SCU, thus an
SCU annotation constitutes a set of equivalence classes.
Computing reliability then becomes identical to compar-
ing the equivalence classes constituting a set of corefer-
ence annotations. In (Passonneau, 2004), we report our
method for computing reliability for coreference annota-
tions, and the use of a distance metric that allows us to
weight disagreements. Applying the same data represen-
tation and reliability formula (Krippendorff?s Alpha) as
in (Passonneau, 2004), and a distance metric that takes
into account relative SCU size, to the two codings C1
and C2 yields ? = 81. Values above .67 indicate good
reliability (Krippendorff, 1980).
A H C J
C1 .97 .87 .83 .82
C2 .94 .87 .84 .74
Consensus .95 .89 .85 .76
Table 1: Pyramid scores across annotations.
1 (9) 2 (36) 3 (84) 4 (128) 5 (128) 6 (84) 7 (36) 8 (9) 9 (1)0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of summaries in the pyramid (number of pyramids)
Su
m
m
ar
y 
sc
or
e
arv d30042.b
min d30042.b
max d30042.b
arv d30042.q
min d30042.q
max d30042.q
Figure 3: Min, max and average scores for two sum-
maries ? one better than the other.
More important than interannotator reliability is the ro-
bustness of the pyramid metric, given different SCU an-
notations. Table 1 gives three sets of pyramid scores for
the same set of four PAL summaries. The rows of scores
correspond to the original annotations (C1, C2) and a
consensus. There is no significant difference in the scores
assigned across the three annotations (between subjects
ANOVA=0.11, p=0.90).
3.2 Pyramid Scores of Human Summaries
Here we use three DUC 2003 summary sets for which
four human summaries were written. In order to provide
as broad a comparison as possible for the least annotation
effort, we selected the set that received the highest DUC
scores (D30042: Lockerbie), and the two that received
the lowest (D31041: PAL; D31050: China). For each set,
we collected six new summaries from advanced under-
graduate and graduate students with evidence of superior
verbal skills; we gave them the same instructions used by
NIST. This turned out to be a large enough corpus to in-
vestigate how many summaries a pyramid needs for score
stability. Here we first compare pyramid scores of the
original summaries with DUC scores. Then we present
results demonstrating the need for at least five summaries
per pyramid, given this corpus of 100-word summaries.
Table 2 compares DUC and pyramid scores for all
three sets. The first two rows of pyramid scores are for
a pyramid of order 3 using a single pyramid with the re-
maining three original DUC summaries (n=3) versus an
Lockerbie (D30042)
Method A B C D
DUC n.a. .82 .54 .74
Pyramid (n=3) .69 .83 .75 .82
Pyramid (Avg. n=3) .68 .82 .74 .76
Pyramid (n=9) .74 .89 .80 .83
PAL (D31041)
Method A H I J
DUC .30 n.a. .30 .10
Pyramid (n=3) .76 .67 .59 .43
Pyramid (Avg. n=3) .46 .50 .52 .57
Pyramid (n=9) .52 .56 .60 .63
China (D31050)
Method C D E F
DUC n.a. .28 .27 .13
Pyramid (n=3) .57 .63 .72 .56
Pyramid (Avg. n=3) .64 .61 .72 .58
Pyramid (n=9) .69 .67 .78 .63
Table 2: Comparison of DUC and Pyramid scores; capital
letters represent distinct human summarizers.
average over all order-3 pyramids (Avg. n=3); the third
row of pyramid scores are for the single pyramid of or-
der 9 (n=9; note that the 10th summary is the one being
scored). Compared to the DUC scores, pyramid scores
show all humans performing reasonably well. While the
Lockerbie set summaries are better overall, the difference
with the PAL and China sets scores is less great than with
the DUC method, which accords with our impressions
about the relative quality of the summaries. Note that
pyramid scores are higher for larger pyramid inventories,
which reflects the greater likelihood that more SCUs in
the summary appear in the pyramid. For a given order
pyramid, the scores for the average and for a specific
pyramid can differ significantly, as, for example, PAL A
and PAL J do (compare rows n=3 and n=9).
The pyramid rows labelled ?n=3? are the most compa-
rable to the DUC scores in terms of the available data.
For the DUC scores there was always a single model, and
no attempt to evaluate the model.
Pyramid scores are quantitatively diagnostic in that
they express what proportion of the content in a summary
is relatively highly weighted, or alternatively, what pro-
portion of the highly weighted SCUs appear in a sum-
mary. The pyramid can also serve as a qualitative diag-
nostic tool. To illustrate both points, consider the PAL A
summary; its score in the n=3 row of .76 indicates that
relatively much of its content is highly weighted. That
is, with respect to the original pyramid with only three
tiers, it contained a relatively high proportion of the top
tier SCUs: 3/4 of the w=3 facts (75%). When we av-
erage over all order-3 pyramids (Avg. n=3) or use the
largest pyramid (n=9), the PAL A score goes down to .46
or .52, respectively. Given the nine-tier pyramid, PAL A
contains only 1/3 of the SCUs of w6, a much smaller
proportion of the most highly weighted ones. There are
four missing highly weighted SCUs and they express the
following facts: to deal with its financial crisis, Pal nego-
tiated with Cathay Pacific for help; the negotiations col-
lapsed; the collapse resulted in part from PAL?s refusal to
cut jobs; and finally, President Estrada brokered an agree-
ment to end the shutdown strike. These facts were in the
original order-3 pyramid with relatively lower weights.
The score variability of PAL A, along with the change
in status of SCUs from having low weights to having high
ones, demonstrates that to use the pyramid method reli-
ably, we need to ask how many summaries are needed
to produce rankings across summaries that we can have
confidence in. We now turn to this analysis.
3.3 Behavior of Scores as Pyramid Grows
Here we address two questions raised by the data from
Table 2, i.e., that scores change as pyramid size increases:
1. How does variability of scores change as pyramid
order increases?
2. At what order pyramid do scores become reliable?
To have confidence in relative ranking of summaries by
pyramid scores, we need to answer the above questions.
It has often been noted that different people write dif-
ferent summaries; we observe that with only a few sum-
maries in a pyramid, there is insufficient data for the
scores associated with a pyramid generated from one
combination of a few summaries to be relatively the same
as those using a different combination of a few sum-
maries. Empirically, we observed that as pyramids grow
larger, and the range between higher weight and lower
weight SCUS grows larger, scores stabilize. This makes
sense in light of the fact that a score is dominated by the
higher weight SCUS that appear in a summary. However,
we wanted to study more precisely at what point scores
become independent of the choice of models that pop-
ulate the pyramid. We conducted three experiments to
locate the point at which scores stabilize across our three
datasets. Each experiment supports the same conclusion,
thus reinforcing the validity of the result.
Our first step in investigating score variability was to
examine all pairs of summaries where the difference in
scores for an order 9 pyramid was greater than 0.1; there
were 68 such pairs out of 135 total. All such pairs ex-
hibit the same pattern illustrated in Figure 3 for two sum-
maries we call ?b? and ?q?. The x-axis on the plot shows
how many summaries were used in the pyramid and the
y-axis shows the min, max and average score scores for
the summaries for a given order of pyramid, 6 Of the two,
6Note that we connected data points with lines to make the
graph more readable.
?b? has the higher score for the order 9 pyramid, and is
perceivably more informative. Averaging over all order-
1 pyramids, the score of ?b? is higher than ?q? but some
individual order-1 pyramids might yield a higher score
for ?q?. The score variability at order-1 is huge: it can
be as high as 0.5. With higher order pyramids, scores
stabilize. Specifically, in our data, if summaries diverge
at some point as in Figure 3, where the minimum score
for the better summary is higher than the maximum score
for the worse summary, the size of the divergence never
decreases as pyramid order increases. For pyramids of
order > 4, the chance that ?b? and ?q? reverse ranking
approaches zero.
For all pairs of divergent summaries, the relationship
of scores follows the same pattern we see in Figure 3 and
the point of divergence where the scores for one summary
become consistently higher than those of the othere, was
found to be stable ? in all pair instances, if summary A
gets higher scores than summary B for all pyramids of
order n, than A gets higher scores for pyramids of order
 n. We analyzed the score distributions for all 67 pairs
of ?divergent? summaries in order to determine what or-
der of pyramid is required to reliably discriminate them.
The expected value for the point of divergence of scores,
in terms of number of summaries in the pyramid, is 5.5.
We take the scores assigned at order 9 pyramids as be-
ing a reliable metric on the assumption that the pattern
we have observed in our data is a general one, namely
that variance always decreases with increasing orders of
pyramid, and that once divergence of scores occurs, the
better summary never gets a lower score than the worse
for any model of higher order.
We postulate that summaries whose scores differ by
less than 0.06 have roughly the same informativeness.
The assumption is supported by two facts. First, this cor-
responds to the difference in PAL scores (D31041) we
find when we use a different one of our three PAL an-
notations (see Table 1). Second, the pairs of summaries
whose scores never clearly diverged had scores differing
by less than 0.06 at pyramid order 9.
Now, for each pair of summaries (sum1, sum2), we
can say whether they are roughly the same when evalu-
ated against a pyramid of order n and we will denote this
as jsum1j ==
n
jsum2j, (scores differ by less than 0.06
for some pyramid of order n) or different (scores differ
by more than 0.06 for all pyramids of order n) and we
will use the notation jsum1j <
n
jsum2j if the score for
sum2 is higher.
When pyramids of lower order are used, the following
errors can happen, with the associated probabilities:
E
1
: jsum1j ==
9
jsum2j but jsum1j <
n
jsum2j or
jsum1j >
n
jsum2j at some lower order n pyramid.
The conditional probability of this type of error is
p
1
= P (jsum1j >
n
jsum2jjjsum1j ==
9
jsum2j).
E
2
: jsum1j <
9
jsum2j but at a lower order
jsum1j ==
n
jsum2j. This error corresponds to
?losing ability to discern?, which means one can tol-
erate it, as long as the goal is not be able to make fine
grained distinctions between the summaries. Here,
p
2
= P (jsum1j ==
n
jsum2jjjsum1j <
9
jsum2j).
E
3
: jsum1j <
9
jsum2j but at lower level
jsum1j >
n
jsum2j Here, p
3
= P (jsum1j >
n
jsum2jjjsum1j <
9
jsum2j) + P (jsum1j <
n
jsum2jjsum1j >
n
jsum2j). This is the most
severe kind of mistake and ideally it should never
happen?the two summaries appear with scores
opposite to what they really are.7
The probabilities p
1
, p
2
and p
3
can be computed di-
rectly by counting how many times the particular error
occurs for all possible pyramids of order n. By taking
each pyramid that does not contain either of sum1 or
sum2 and comparing the scores they are assigned, the
probabilities in Table 3 are obtained. We computed prob-
abilities for pairs of summaries for the same set, then
summed the counts for error occurrence across sets. The
order of the pyramid is shown in column n. ?Data points?
shows how many pyramids of a given order were exam-
ined when computing the probabilities. The total proba-
bility of error p = p1 P (jsum1j ==
9
jsum2j) + (p2 +
p3)  (1 ? P (jsum1j ==
9
jsum2j)) is also in Table 3.
Table 3 shows that for order-4 pyramids, the errors of
type E
3
are ruled out. At order-5 pyramids, the total prob-
ability of error drops to 0.1 and is mainly due to error E
2
,
which is the mildest one.
Choosing a desirable order of pyramid involves balanc-
ing the two desiderata of having less data to annotate and
score stability. Our data suggest that for this corpus, 4 or
5 summaries provide an optimal balance of annotation ef-
fort with reliability. This is reconfirmed by our following
analysis of ranking stability.
n p1 p2 p3 p data points
1 0.41 0.23 0.08 0.35 1080
2 0.27 0.23 0.03 0.26 3780
3 0.16 0.19 0.01 0.18 7560
4 0.09 0.17 0.00 0.14 9550
5 0.05 0.14 0.00 0.10 7560
6 0.02 0.10 0.00 0.06 3780
7 0.01 0.06 0.00 0.04 1080
8 0.00 0.01 0.00 0.01 135
Table 3: Probabilities of errors E1, E2, E3 and total prob-
ability of error
7Note that such an error can happen only for models of order
lower than their point of divergence.
In order to study the issue of how the pyramid scores
behave when several summarizers are compared, not just
two, for each set we randomly selected 5 peer summaries
and constructed pyramids consisting of all possible sub-
sets of the remaining five. We computed the Spearman
rank-correlation coefficient for the ranking of the 5 peer
summaries compared to the ranking of the same sum-
maries given by the order-9 pyramid. Spearman coef-
ficent r
s
(Dixon and Massey, 1969) ranges from -1 to
1, and the sign of the coefficent shows whether the two
rankings are correlated negatively or positively and its
absolute value shows the strength of the correlation. The
statistic r
s
can be used to test the hypothesis that the two
ways to assign scores leading to the respective rankings
are independent. The null hypothesis can be rejected with
one-sided test with level of significance ? = 0.05, given
our sample size N = 5, if r
s
 0.85.
Since there are multiple pyramids of order n  5, we
computed the average ranking coefficient, as shown in
Table 4. Again we can see that in order to have a ranking
of the summaries that is reasonably close to the rankings
produces by a pyramid of order n = 9, 4 or more sum-
maries should be used.
n average r
s
# pyramids
1 0.41 15
2 0.65 30
3 0.77 30
4 0.87 15
5 1.00 3
Table 4: Spearman correlation coefficient average for
pyramids of order n  5
3.4 Rank-correlation with unigram overlap scores
Lin and Hovy (2003) have shown that a unigram co-
occurrence statistic, computed with stop words ignored,
between a summary and a set of models can be used to
assign scores for a test suite that highy correlates with the
scores assigned by human evaluators at DUC. We have
illustrated in Figure 1 above that human scores on human
summaries have large variance, and we assume the same
holds for machine summaries, so we believe the approach
is built on weak assumptions. Also, their approach is not
designed to rank individual summaries.
These qualifications aside, we wanted to test whether it
is possible to use their approach for assigning scores not
for an entire test suite but on a per set basis. We computed
the Spearman rank-coefficent r
s
for rankings assigned by
computing unigram overlap and those by pyramid of or-
der 9. For computing the scores, Lin?s original system
was used, with stop words ignored. Again 5 summaries
were chosen at random to be evaluated against models
composed of the remaining five summaries. Composite
models were obtained by concatenating different combi-
nations of the initial five summaries. Thus scores can be
computed using one, two and so on up to five reference
summaries. Table 5 shows the average values of r
s
that
were obtained.
# models average r
s
# model combinations
1 0.12 15
2 0.27 30
3 0.29 30
4 0.35 15
5 0.33 3
Table 5: Spearman correlation coefficient average for un-
igram overlap score assignment
As noted above, in order to consider the two scoring
methods as being substitutable, r
s
should be bigger than
0.85, given our sample size. Given the figures shown in
Table 5, we don?t have reason to believe that unigram
scores are correlated with pyramid scores.
4 Comparison with previous work
The work closest to ours is (Halteren and Teufel, 2003),
and we profited from the lessons they derived from an
annotation of 50 summaries of a single 600-word docu-
ment into content units that they refer to as factoids. They
found a total of 256 factoids and note that the increase in
factoids with the number of summaries seems to follow a
Zipfian distribution.
We identify four important differences between fac-
toids and SCUs. First, an SCU is a set of contributors
that are largely similar in meaning, thus SCUs differ from
each other in both meaning and weight (number of con-
tributors). In contrast, factoids are semi-formal expres-
sions in a FOPL-style semantics, which are composition-
ally interpreted. We intentionally avoid creating a rep-
resentation language for SCU labels; the function of an
SCU label is to focus the annotator?s attention on the
shared meaning of the contributors. In contrast to Hal-
tern and Teufel, we do not believe it is possible to arrive
at the correct representation for a set of summaries; they
refer to the observation that the factoids arrived at depend
on the summaries one starts with as a disadvantage in that
adding a new summary can require adjustments to the set
of factoids. Given the different knowledge and goals of
different summarizers, we believe there can be no cor-
rect representation of the semantic content of a text or
collection; a pyramid, however, represents an emergent
consensus as to the most frequently recognized content.
In addition to our distinct philosophical views regarding
the utility of a factoid language, we have methodological
concerns: the learning curve required to train annotators
would be high, and interannotator reliability might be dif-
ficult to quantify or to achieve.
Second, (Halteren and Teufel, 2003) do not make di-
rect use of factoid frequency (our weights): to construct
a model 100-word summary, they select factoids that oc-
cur in at least 30% of summaries, but within the resulting
model summary, they do not differentiate between more
and less highly weighted factoids. Third, they annotate
semantic relations among factoids, such as generalization
and implication. Finally, they report reliability of the an-
notation using recall and precision, rather than a reliabil-
ity metric that factors in chance agreement. In (Passon-
neau, 2004), we note that high recall/precision does not
preclude low interannotator reliability on a coreference
annotation task.
Radev et al (2003) also exploits relative importance of
information. Evaluation data consists of human relevance
judgments on a scale from 0 to 10 on for all sentences in
the original documents. Again, information is lost rela-
tive to the pyramid method because a unique reference
summary is produced instead of using all the data. The
reference summary consists of the sentences with highest
relevance judgements that satisfy the compression con-
straints. For multidocument summarization compression
rates are high, so even sentences with the highest rele-
vance judgments are potentially not used.
Lin and Hovy (2002) and Lin and Hovy (2003) were
the first to systematically point out problems with the
large scale DUC evaluation and to look to solutions by
seeking more robust automatic alternatives. In their stud-
ies they found that multiple model summaries lead to
more stable evaluation results. We believe a flaw in their
work is that they calibrate the method to the erratic DUC
scores. When applied to per set ranking of summaries, no
correlation was seen with pyramid scores.
5 Conclusions
There are many open questions about how to parameter-
ize a summary for specific goals, making evaluation in
itself a significant research question (Jing et al, 1998).
Instead of attempting to develop a method to elicit reli-
able judgments from humans, we chose to calibrate our
method to human summarization behavior.
The strengths of pyramid scores are that they are re-
liable, predictive, and diagnostic. The pyramid method
not only assigns a score to a summary, but also allows the
investigator to find what important information is miss-
ing, and thus can be directly used to target improvements
of the summarizer. Another diagnostic strength is that it
captures the relative difficulty of source texts. This allows
for a fair comparison of scores across different input sets,
which is not the case with the DUC method.
We hope to address two drawbacks to our method in
future work. First, pyramid scores ignore interdependen-
cies among content units, including ordering. However,
our SCU annotated summaries and correlated pyramids
provide a valuable data resource that will allow us to in-
vestigate such questions. Second, creating an initial pyra-
mid is laborious so large-scale application of the method
would require an automated or semi-automated approach.
We have started exploring the feasibility of automation
and we are collecting additional data sets.
References
Wilfrid Dixon and Frank Massey. 1969. Introduction to
statistical analysis. McGraw-Hill Book Company.
Hans Halteren and Simone Teufel. 2003. Examining
the consensus between human summaries: initial ex-
periments with factoid analysis. In HLT-NAACL DUC
Workshop.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Michael Elhadad. 1998. Summarization evaluation
methods: Experiments and analysis. In AAAI Sympo-
sium on Intelligent Summarization.
Judith Klavans, Sam Popper, and Rebecca J. Passonneau.
2003. Tackling the internet glossary glut: Extraction
and evaluation of genus phrases. In SIGIR Workshop:
Semantic Web, Toronto.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications, Bev-
erly Hills, CA.
Chin-Yew Lin and Eduard Hovy. 2002. Manual and au-
tomatic evaluation of summaries. In Proceedings of
the Workshop on Automatic Summarization, post con-
ference workshop of ACL 2002.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurance statis-
tics. In Proceedings of HLT-NAACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL.
Rebecca J. Passonneau and Ani Nenkova. 2003. Evaluat-
ing content selection in human- or machine-generated
summaries: The pyramid method. Technical Report
CUCS-025-03, Columbia University.
Rebecca J. Passonneau. 2004. Computing reliability
for coreference annotation. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Dragomir Radev, Simone Teufel, Horacio Saggion, and
W. Lam. 2003. Evaluation challenges in large-scale
multi-document summarization. In ACL.
G. J. Rath, A. Resnick, and R. Savage. 1961. The for-
mation of abstracts by the selection of sentences: Part
1: sentence selection by man and machines. American
Documentation, 2(12):139?208.
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Making Sense of Word Sense Variation
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
Center for Computational Learning Systems
Columbia University
New York, NY, USA
(becky@cs|ansaf@ccls).columbia.edu
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Abstract
We present a pilot study of word-sense an-
notation using multiple annotators, relatively
polysemous words, and a heterogenous cor-
pus. Annotators selected senses for words in
context, using an annotation interface that pre-
sented WordNet senses. Interannotator agree-
ment (IA) results show that annotators agree
well or not, depending primarily on the indi-
vidual words and their general usage proper-
ties. Our focus is on identifying systematic
differences across words and annotators that
can account for IA variation. We identify three
lexical use factors: semantic specificity of the
context, sense concreteness, and similarity of
senses. We discuss systematic differences in
sense selection across annotators, and present
the use of association rules to mine the data
for systematic differences across annotators.
1 Introduction
Our goal is to grapple seriously with the natural
sense variation arising from individual differences in
word usage. It has been widely observed that usage
features such as vocabulary and syntax vary across
corpora of different genres and registers (Biber,
1995), and that serve different functions (Kittredge
et al, 1991). Still, we are far from able to pre-
dict specific morphosyntactic and lexical variations
across corpora (Kilgarriff, 2001), much less quan-
tify them in a way that makes it possible to apply
the same analysis tools (taggers, parsers) without re-
training. In comparison to morphosyntactic proper-
ties of language, word and phrasal meaning is fluid,
and to some degree, generative (Pustejovsky, 1991;
Nunberg, 1979). Based on our initial observations
from a word sense annotation task for relatively pol-
ysemous words, carried out by multiple annotators
on a heterogeneous corpus, we hypothesize that dif-
ferent words lead to greater or lesser interannota-
tor agreement (IA) for reasons that in the long run
should be explicitly modelled in order for Natural
Language Processing (NLP) applications to handle
usage differences more robustly. This pilot study is
a step in that direction.
We present related work in the next section, then
describe the annotation task in the following one. In
Section 4, we present examples of variation in agree-
ment on a matched subset of words. In Section 5
we discuss why we believe the observed variation
depends on the words and present three lexical use
factors we hypothesize to lead to greater or lesser
IA. In Section 6, we use association rules to mine
our data for systematic differences among annota-
tors, thus to explain the variations in IA. We con-
clude with a summary of our findings goals.
2 Related Work
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al, 2005)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al, 2005). Differences in IA
and system performance across part-of-speech have
been examined, as in (Ng et al, 1999; Palmer et al,
2
Word POS No. senses No. occurrences
fair Adj 10 463
long Adj 9 2706
quiet Adj 6 244
land Noun 11 1288
time Noun 10 21790
work Noun 7 5780
know Verb 11 10334
say Verb 11 20372
show Verb 12 11877
tell Verb 8 4799
Table 1: Ten Words
2005). Pedersen (Pedersen, 2002a) examines varia-
tion across individual words in evaluating WSD sys-
tems, but does not attempt to explain it.
Factors that have been proposed as affecting
human or system sense disambiguation include
whether annotators are allowed to assign multilabels
(Veronis, 1998; Ide et al, 2002; Passonneau et al,
2006), the number or granularity of senses (Ng et al,
1999), merging of related senses (Snow et al, 2007),
sense similarity (Chugur et al, 2002), sense perplex-
ity (Diab, 2004), entropy (Diab, 2004; Palmer et
al., 2005), and in psycholinguistic experiments, re-
actions times required to distinguish senses (Klein
and Murphy, 2002; Ide and Wilks, 2006).
With respect to using multiple annotators, Snow
et al included disambiguation of the word
president?a relatively non-polysemous word with
three senses?in a set of tasks given to Amazon Me-
chanical Turkers, aimed at determining how to com-
bine data from multiple non-experts for machine
learning tasks. The word sense task comprised 177
sentences taken from the SemEval Word Sense Dis-
ambiguation Lexical Sample task. Majority voting
among three annotators achieve 99% accuracy.
3 The Annotation Task
The Manually Annotated Sub-Corpus (MASC)
project is creating a small, representative corpus
of American English written and spoken texts
drawn from the Open American National Cor-
pus (OANC).1 The MASC corpus includes hand-
validated or manually produced annotations for a va-
riety of linguistic phenomena. One of the goals of
1http://www.anc.org
Figure 1: MASC word sense annotation tool
the project is to support efforts to harmonize Word-
Net (Miller et al, 1993) and FrameNet (Ruppen-
hofer et al, 2006), in order to bring the sense distinc-
tions each makes into better alignment. As a start-
ing sample, we chose ten fairly frequent, moderately
polysemous words for sense tagging, targeting in
particular words that do not yet exist in FrameNet, as
well as words with different numbers of senses in the
two resources. The ten words with part of speech,
number of senses, and occurrences in the OANC
are shown in Table 1. One thousand occurrences
of each word , including all occurrences appear-
ing in the MASC subset and others semi-randomly2
chosen from the remainder of the 15 million word
OANC, were annotated by at least one annotator of
six undergraduate annotators at Vassar College and
Columbia University.
Fifty occurrences of each word in context were
sense-tagged by all six annotators for the in-depth
study of inter-annotator agreement (IA) reported
here. We have just finished collecting annotations
of fifty new occurrences. All annotations are pro-
2The occurrences were drawn equally from each of the
genre-specific portions of the OANC.
3
duced using the custom-built interface to WordNet
shown in Figure 1: the sentence context is at the top
with the word in boldface (fair), a comment region
below that allows the annotator to keep notes, and
a scrollable area below that shows three of the ten
WordNet senses for ?fair.?
4 Observation: Varying Agreement,
depending on Lexical Items
We expected to find varying levels of interannotator
agreement (IA) among all six annotators, depend-
ing on obvious grouping factors such as the part of
speech, or the number of senses per word. We do
find widely varying levels of agreement, but as de-
scribed here, most of the variation does not depend
on these a priori factors. Inherent usage properties
of the words themselves, and systematic patterns of
variation across annotators, seem to be the primary
factors, with a secondary effect of part of speech.
In previous work (Passonneau, 2004), we have
discussed why we use Krippendorff?s ? (Krippen-
dorff, 1980), and for purposes of comparison we
also report Cohen?s ?; note the similarity in values3.
As with the various agreement coefficients that fac-
tor out the agreement that would occur by chance,
values range from 1 for perfect agreement and -1
for perfect opposition, to 0 for chance agreement.
While there are no hard and fast criteria for what
constitutes good IA, Landis and Koch (Landis and
Koch, 1977) consider values between 0.40 and 0.60
to represent moderately good agreement, and values
above 0.60 as quite good; Krippendorff (Krippen-
dorff, 1980) considers values above 0.67 moderately
good, and values above 0.80 as quite good. (cf. (Art-
stein and Poesio, 2008) for discussion of agreement
measurement for computational linguistic tasks.)
Table 2 shows IA for a pair of adjectives, nouns
and verbs from our sample for which the IA scores
are at the extremes (high and low) in each pair: the
average delta is 0.24. Note that the agreement de-
creases as part-of-speech varies from adjectives to
nouns to verbs, but for all three parts-of-speech,
there is a wide spread of values. It is striking, given
that the same annotators did all words, that one in
each pair has relatively better agreement.
3? handles multiple annotators; Arstein and Poesio (Artstein
and Poesio, 2008) propose an extension of ? (?3) we use here.
POS Word ? ? No. senses Used
adj long 0.6664 0.6665 9 8
fair 0.3546 0.3593 10 5
noun work 0.5359 0.5358 7 7
land 0.2627 0.2671 11 8
verb tell 0.4152 0.4165 8 8
show 0.2636 0.2696 12 11
Table 2: Varying interannotator agreement across words
The average of the agreement values shown in
Table 2 (?=0.4164; ?=0.4191) is somewhat higher
than the average 0.317 found for 191 words anno-
tated for WordNet senses in (Ng et al, 1999), but
lower than their recomputed ? of 0.85 for verbs, af-
ter they reanalyzed the data to merge senses for 42
of the verbs. It is widely recognized that achieving
high ? scores (or percent agreement between anno-
tators, cf. (Palmer et al, 2005)) is difficult for word
sense annotation.
Given that the same annotators have higher IA on
some words, and lower on others, we hypothesize
that it is the word usages themselves that lead to the
high deltas in IA for each part-of-speech pair. We
discuss the impact of three factors on the observed
variations in agreement:
1. Greater specificity in the contexts of use leads to
higher agreement
2. More concrete senses give rise to higher agreement
3. A sense inventory with closely related senses
(e.g., relatively lower average inter-sense similarity
scores) gives rise to lower agreement
5 Explanatory Factors
First we list factors that can not explain the variation
in Table 2. Then we turn to examples illustrating
factors that can, based on a manual search for exam-
ples of two types: examples where most annotators
agreed on a single sense, and examples where two
or three senses were agreed upon by multiple anno-
tators. Later we how how we use association rules
to detect these two types of cases automatically. For
these examples, the WordNet sense number is shown
(e.g., WN S1) with an abbreviated gloss, followed
by the number of annotators who chose it.
4
5.1 Ruled Out Factors
It appears that neither annotator expertise, a word?s
part of speech, the number of senses in WordNet,
the number of senses annotators find in the corpus,
nor the nature of the distribution across senses, can
account for the variation in IA in Table 2. All six
annotators used the same annotation tool, the same
guidelines, and had already become experienced in
the word sense annotation task.
The six annotators all exhibit roughly the same
performance. We measure an individual annotator?s
performance by computing the average pairwise IA
(IA2). For every annotator Ai, we first compute the
pairwise agreement of Ai with every other annota-
tor, then average. This gives us a measure for com-
paring individual annotators with each other: an-
notators that have a higher IA2 have more agree-
ment, on average, with other annotators. Note that
we get the same ranking of individuals when for
each annotator, we calculate how much the agree-
ment among the five remaining annotators improves
over the agreement among all six annotators. If
agreement improves relatively more when annota-
tor Ai is dropped, then Ai agrees less well with the
other five annotators. While both approaches give
the same ranking among annotators, IA2 also pro-
vides a number that has an interpretable value.
On a word-by-word basis, some annotators do
better than others. For example, for long, the best
annotator (A) has IA2=0.79, and the worst (F) has
0.44. However, across ten words annotated by all
six, the average of their IA2 is 0.39 with a standard
deviation of 0.037. F at 0.32 is an outlier; apart from
F, annotators have similar IA across words.
Table 2 lists the distribution of available senses
in WordNet for the four words (column 4), and the
number of senses used (column 5). The words work
and tell have relatively fewer senses (seven and eig-
ith) compared with nine through twelve for the other
words. However, neither the number (or proportion)
of senses used by annotators, nor the distribution
across senses, has a significant correlation with IA,
as given by Pearson?s correlation test.
5.2 Lexical Use Factors
Underspecified contexts lead to ambiguous word
meanings, a factor that has been recognized as be-
ing associated with polysemous contexts (Palmer et
al., 2005). We find that the converse is also true:
relatively specific contexts reduce ambiguity.
The word long seems to engender the greatest IA
primarily because the contexts are concrete and spe-
cific, with a secondary effect that adjectives have
higher IA overall than the other parts of speech. Sen-
tences such as (1.), where a specific unit of temporal
or spatial measurement is mentioned (months), re-
strict the sense to extent in space or time.
1. For 18 long months Michael could not find a job.
WN S1. temporal extent [N=6 of 6]
In the few cases where annotators disagree on
long, the context is less specific or less concrete. In
example (2.), long is predicated of the word chap-
ter, which has non-concrete senses that exemplify
a certain type of productive polysemy (Pustejovsky,
1991). It can be taken to refer to a physical object
(a specific set of pages in an actual book), or a con-
ceptual object (the abstract literary work). The ad-
jective inherits this polysemy. The three annotators
who agree on sense two (spatial extent) might have
the physical object sense in mind; the two who select
sense one (temporal extent) possibly took the point
of view of the reader who requires a long time to
read the chapter.
2. After I had submitted the manuscript my editor at
Simon Schuster had suggested a number of cuts to
streamline what was already a long and involved
chapter on Brians ideas.
WN S2.spatial extent [N=3 of 6],
WN S1.temporal extent [N=2 of 6],
WN S9.more than normal or necessary [N=1 of 6]
Several of the senses of work are concrete, and
quite distinct: sense seven, ?an artist?s or writer?s
output?; sense three, ?the occupation you are paid
for?; sense five, ?unit of force in physics?; sense
six, ?the place where one works.? These are the
senses most often selected by a majority of annota-
tors. Senses one and two, which are closely related,
are the two senses most often selected by different
annotators for the same instance. They also repre-
sent examples of productive polysemy, here between
an activity sense (sense one) and a product-of-the-
activity sense (sense two). Example (3) shows a sen-
5
tence where the verb perform restricts the meaning
to the activity sense, which all annotators selected.
3. The work performed by Rustom and colleagues
suggests that cell protrusions are a general mech-
anism for cell-to-cell communication and that in-
formation exchange is occurring through the direct
membrane continuity of connected cells indepen-
dently of exo- and endocytosis.
WN S1.activity of making something [N=6 of 6]
In sentence (4.), four annotators selected sense
one (activity) and two selected sense two (result):
4. A close friend is a plastic surgeon who did some
minor OK semi-major facial work on me in the past.
WN S1.activity directed toward making something
[N=4 of 6],
WN S2.product of the effort of a person or thing
[N=2 of 6]
For the word fair, if five or six annotators agree,
often they have selected sense one??free of fa-
voritism or bias??as in example (5). However, this
sense is often selected along with sense two??not ex-
cessive or extreme?as in example (6). Both senses
are relatively abstract.
5. By insisting that everything Microsoft has done is
fair competition they risk the possibility that the
public if it accepts the judges finding to the con-
trary will conclude that Microsoft doesn?t know the
difference.
WN S1.free of favoritism/bias [N=6 of 6]
6. I I think that?s true I can remember times my parents
would say well what do you think would be a fair
punishment.
WN S1.free of favoritism/bias [N=3 of 6],
WN S2.not excessive or extreme [N=3 of 6]
Example (7) illustrates a case where all annota-
tors agreed on a sense for land. The named entity
India restricts the meaning to sense five, ?territory
occupied by a nation.? Apart from a few such cases
of high consensus, land seems to have low agree-
ment due to senses being so closely related they can
be merged. Senses one and seven both have to do
with property (cf. example (8))., senses three and
five with geopolitical senses, and senses two and
four with the earth?s surface or soil. If these three
pairs of senses are merged into three senses, the IA
goes up from 0.2627 to 0.3677.
7. India is exhilarating exhausting and infuriating a
land where you?ll find the practicalities of daily life
overlay the mysteries that popular myth attaches to
India.
WN S5.territory occupied by a nation [N=6 of 6]
8. uh the Seattle area we lived outside outside of the
city in the country and uh we have five acres of land
up against a hillside where i grew up and so we did
have a garden about a one a half acre garden
WN S4.solid part of the earth?s surface [N=1 of 6],
WN S1.location of real estate [N=2 of 6],
WN S7.extensive landed property [N=3 of 6]
Examples for tell and show exhibit the same trend
in which agreement is greater when the sense is
more specific or concrete, which we illustrate briefly
with show. Example (9) describes a specific work of
art, an El Greco painting, and agreement is universal
among the six annotators on sense 5. In contrast, ex-
ample (10) shows a fifty-fifty split among annotators
for a sentence with a very specific context, an ex-
periment regarding delivery of a DNA solution, but
where the sense is abstract rather than concrete: the
argument of show is an abstract proposition, namely
a conclusion is drawn regarding what the experiment
demonstrates, rather than a concrete result such as a
specific measurement, or statistical outcome. Sense
two in fact contains the word ?experiment? that oc-
curs in (9), which presumably biases the choice of
sense two. Impressionistically, senses two and three
appear to be quite similar.
9. El Greco shows St. Augustine and St. Stephen,
in splendid ecclesiastical garb, lifting the count?s
body.
WN S5.show in, or as in, a picture, N=6 of 6
10. These experiments show that low-volume jet
injection specifically targeted delivery of a DNA
solution to the skin and that the injection paths did
not reach into the underlying tissue.
WN S2.establish the validity of something, as by
an example, explanation or experiment, N=3 of 6
WN S3.provide evidence for, N=3 of 6
6
5.3 Quantifying Sense Similarity
Application of an inter-sense similarity measure
(ISM) proposed in (Ide, 2006) to the sense invento-
ries for each of the six words supports the observa-
tion that words with very similar senses have lower
IA scores. ISM is computed for each pair in a given
word?s sense inventory, using a variant of the lesk
measure (Banerjee and Pedersen, 2002). Agglom-
erative clustering may then be applied to the result-
ing similarity matrix to reveal the overall pattern of
inter-sense relations.
ISMs for senses pairs of long, fair, work, land,
tell, and show range from 0 to 1.44.4 We compute
a confusion threshhold CT based on the ISMs for all
250 sense pairs as
CT = ?A + 2?A
where A is the sum of the ISMs for the six words? 250
sense pairs.
Table 3 shows the ISM statistics for the six words. The
values show that the ISMs for work and long are signifi-
cantly lower than for land and fair. The ISMs for the two
verbs in the study, show and tell, are distributed across
nearly the same range (0 - 1.38 and 0 - 1.22, respec-
tively), despite substantially lower IA scores for show.
However, the ISMs for three of show?s sense pairs are
well above CT , vs. one for tell, suggesting that in addi-
tion to the range of ISMs for a given word?s senses, the
number of sense pairs with high similarity contributes to
low IA. Overall, the correlation between the percentage
of ISMs above CT for the words in this study and their
IA scores is .8, which supports this claim.
POS Word Max Mean Std. Dev > CT
adj long .71 .28 .18 0
fair 1.25 .28 .34 5
noun work .63 .22 .16 0
land 1.44 .17 .29 3
verb tell 1.22 .15 .25 1
show 1.38 .18 .27 3
Table 3: ISM statistics
6 Association Rules
Association rules express relations among instances
based on their attributes. Here the attributes of interest are
4Note that because the scores are based on overlaps among
WordNet relations, glosses, examples, etc., there is no pre-
defined ceiling value for the ISMs. For the words in this study,
we compute a ceiling value by taking the maximum of the ISMs
for each of the 57 senses with itself, 4.85 in this case.
the annotators who choose one sense versus those who
choose another. Mining association rules to find strong
relations has been studied in many domains (see for in-
stance (Agrawal et al, 1993; Zaki et al, 1997; Salleb-
Aouissi et al, 2007)). Here we illustrate how association
rules can be used to mine relations such as systematic dif-
ferences in word sense choices across annotators.
An association rule is an expression C1 ? C2, where
C1 and C2 express conditions on features describing the
instances in a dataset. The strength of the rules is usually
evaluated by means of measures such as Support (Supp)
and Confidence (Conf). Where C, C1 and C2 express con-
ditions on attributes:
? Supp(C) is the fraction of instances satisfying C
? Supp(C1 ? C2) = Supp(C1 ? C2)
? Conf(C1 ? C2) = Supp(C1 ? C2)/Supp(C1)
Given two thresholds MinSupp (for minimum support)
and MinConf (for minimum confidence), a rule is strong
when its support is greater than MinSupp and its confi-
dence greater than MinConf. Discovering strong rules is
usually a two-step process of retrieving instances above
MinSupp, then from these retrieving instances above
MinConf.
The types of association rules to mine can include
any attributes in either the left hand side or the right
hand side of rules. In our data, the attributes consist
of the word sense assigned by annotators, the annota-
tors, and the instances (words). In order to find rules
that relate annotators to each other, the dataset must be
pre-processed to produce flat (two-dimensional) tables.
Here we focus on annotators to get a flat table in which
each line corresponds to an annotator/sense combination:
Annotator Sense. We denote the six annotators as A1
through A6, and word senses by WordNet sense number.
Here are 15 unique pairs of annotators, so one way
to look at where agreements occur is to determine how
many of these pairs choose the same sense with non-
negligible support and confidence. Tell has much bet-
ter IA than show, but less than long and work. We
would expect association rules among many pairs of
annotators for some but not all of its senses. We
find 11 pairs of rules of the form Ai Tell:Sense1 ?
Aj Tell:Sense1, Aj Tell:Sense1 ? Ai Tell:Sense1,
indicating a bi-directional relationship between pairs of
annotators choosing the same sense, with support rang-
ing from 14% to 44% and confidence ranging from 37%
to 96%. This indicates good support and confidence for
many possible pairs
Our interest here is primarily in mining for systematic
disagreements thus we now turn to pairs of rules where
in one rule, an attribute Annotator Sensei occurs in the
left hand side, and a distinct attributeAnnotator Sensej
occurs in the right. Again, we are especially interested in
7
i j Supp(%) Confi(%) Confj(%)
Ai fair.S1 ? Aj fair.S2
A3 A6 20 100 32.3
A5 A6 20 100 31.2
A1 A2 16 80 40
Ai show.S2 ? Aj show.S3
A1 A3 32 84.2 69.6
A5 A3 24 63.2 80.0
A4 A3 22 91.7 57.9
A4 A6 14 58.3 46.7
A4 A2 12 60.0 50.0
A5 A2 12 60.0 40.0
Ai show.S5 ? Aj show.S10
A1 A6 12 85.7 40.0
A5 A2 10 83.3 50.0
A4 A2 10 83.3 30.5
A4 A6 10 71.4 38.5
A3 A2 8 66.7 40.0
A3 A6 8 57.1 40.0
A5 A6 8 57.1 40.0
Table 4: Association Rules for Systematic Disagreements
bi-directional cases where there is a corresponding rule
with the left and right hand clauses reversed. Table 4
shows some general classes of disagreement rules using a
compact representation with a bidirectional arrow, along
with a table of variables for the different pairs of annota-
tors associated with different levels of support and confi-
dence.
For fair, Table 4 summarizes three pairs of rules with
good support (16-20% of all instances) in which one an-
notator chooses sense 1 of fair and another chooses sense
2: A3 and A5 choose sense 1 where A6 chooses sense 2,
and A1 chooses sense 1 where A2 chooses sense 2. The
confidence varies for each rule, thus in 100% of cases
where A6 selects sense 2 of fair, A3 selects sense 1, but
in only 32.3% of cases is the converse true. Example (6)
where half the annotators picked sense 1 of fair and half
picked sense 2 falls into the set of instances covered by
these rules. The rules indicate this is not isolated, but
rather part of a systematic pattern of usage.
The word land had the lowest interannotator agree-
ment among the six annotators, with eight of eleven
senses were used overall (cf. Table 2). Here we did not
find pairs of rules in which distinct Annotator Sense
attributes that occur in the left and right sides of one rule
occur in the right and left sides of another rule. For show,
Table 4 illustrates two systematic divisions among
groups of annotators. With rather good support rang-
ing from 12% to 32%, senses 2 and 3 exhibit a system-
atic difference: annotators A1, A4 and A5 select sense
2 where annotators A3, A3 and A6 select sense 3. Sim-
ilarly, senses 5 and 10 exhibit a systematic difference:
with a more modest support of 8% to 12%, annotators
A1, A3, A4 and A5 select sense 5 where annotators A2
and A6 select sense 10.
7 Conclusion
We have performed a sense assignment experiment
among multiple annotators for word occurrences drawn
from a broad range of genres, rather than the domain-
specific data utilized in many studies. The selected words
were all moderately polysemous. Based on the results,
we identify several factors that distinguish words with
high vs. low interannotator agreement scores. We also
show the use of association rules to mine the data for
systematic annotator differences. Where relevant, the re-
sults can be used to merge senses, as done in much pre-
vious work, or to identify internal structure within a set
of senses, such as a word-based sense-hierarchy. In our
future work, we want to develop the use of association
rules in several ways. First, we hope to fully automated
the process of finding systematic patterns of difference
across annotators. Second, we hope to extend their use
to mining associations among the representations of in-
stances in order to further investigate the lexical use fac-
tors discussed here.
Acknowledgments
This work was supported in part by National Science
Foundation grant CRI-0708952.
References
Rakesh Agrawal, Tomasz Imielinski, and Arun N.
Swami. 1993. Mining association rules between sets
of items in large databases. In Peter Buneman and
Sushil Jajodia, editors, Proceedings of the 1993 ACM
SIGMOD International Conference on Management of
Data, Washington, D.C., May 26-28, 1993, pages 207?
216. ACM Press.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 136?45,
Mexico City, Mexico.
Douglas Biber. 1995. Dimensions of register variation :
a cross-linguistic comparison. Cambridge University
Press, Cambridge.
8
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32?39,
Philadelphia.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 303?311.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors, Word
Sense Disambiguation: Algorithms and Applications,
pages 47?74, Dordrecht, The Netherlands. Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of ACL?02 Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions, pages 54?60,
Philadelphia.
Nancy Ide. 2006. Making senses: Bootstrapping sense-
tagged lists of semantically-related words. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text, pages 13?27, Dordrecht, The Nether-
lands. Springer.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference on
Language Resources and Evaluation (LREC), pages
581?588, Granada.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6:1?37.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication knowl-
edge. Computational Intelligence, 7(4):305?314.
Devra Klein and Gregory Murphy. 2002. Paper has been
my ruin: Conceptual relations of polysemous words.
Journal of Memory and Language, 47:548?70.
Klaus Krippendorff. 1980. Content analysis: An intro-
duction to its methodology. Sage Publications, Bev-
erly Hills, CA.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science Labo-
ratory (CSL) Report 43, Princeton University, Prince-
ton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop On
Standardizing Lexical Resources.
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143?184.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fin-egrained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering, 13.2:137?163.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a multilin-
gual semantic annotation task. In Proceedings of the
International Conference on Language Resources and
Evaluation (LREC), pages 1951?1956, Genoa, Italy.
Rebecca J. Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Portugal.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02 Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 40?46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pages 81?87.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguitics, 17(4):409?441.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Ansaf Salleb-Aouissi, Christel Vrain, and Cyril Nortet.
2007. Quantminer: A genetic algorithm for mining
quantitative association rules. In IJCAI, pages 1035?
1040.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1005?1014, Prague.
Jean Veronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mit-
sunori Ogihara, and Wei Li. 1997. New algorithms
for fast discovery of association rules. In KDD, pages
283?286.
9
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357?366,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Contrasting the Interaction Structure of an Email and a Telephone
Corpus: A Machine Learning Approach to Annotation of Dialogue
Function Units
Jun Hu
Department of Computer Science
Columbia University
New York, NY, USA
jh2740@columbia.edu
Rebecca J. Passonneau
CCLS
Columbia University
New York, NY, USA
becky@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We present a dialogue annotation scheme
for both spoken and written interaction,
and use it in a telephone transaction cor-
pus and an email corpus. We train classi-
fiers, comparing regular SVM and struc-
tured SVM against a heuristic baseline.
We provide a novel application of struc-
tured SVM to predicting relations between
instance pairs.
1 Introduction
We present an annotation scheme for verbal inter-
action which can be applied to corpora that vary
across many dimensions: modality of signal (oral,
textual), medium (e.g., email, voice alone, voice
over electronic channel), register (such as infor-
mal conversation versus formal legal interroga-
tion), number of participants, immediacy (online
versus offline), and so on.1 We test it by anno-
tating transcribed phone conversations and email
threads. We then use three algorithms, two of
which use machine learning (including a novel ap-
proach to using Structured SVM), to predict labels
and links (a generalization of adjacency pairs) on
unseen data. We conclude that we can indeed use
a common annotation scheme, and that the email
modality is easier to tag for dialogue acts, but that
it is harder in email to find the links.
2 Related Work
Annotation for dialogue acts (DAs), inspired by
Searle and Austin?s work on speech acts, arose
largely as a means to understand, evaluate and
1This research was supported in part by the National Sci-
ence Foundation under grants IIS-0745369 and IIS-0713548,
and by the Human Language Technology Center of Excel-
lence. Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the sponsors.
We would like to thank three anonymous reviewers for their
thoughtful comments.
model human-human and human-machine com-
munication. The need for the enterprise derives
from the fact that the relationship between lexico-
grammatical form (including mood, e.g., interrog-
ative) and communicative actions cannot be enu-
merated; there are complex dependencies on the
linguistic and situational contexts of use. Many
DA schemes exist: they can be hierarchical or
flat (Popescu-Belis, 2008), can comprise a large
(Devillers et al, 2002; Hardy et al, 2003) or small
repertoire (Komatani et al, 2005), or can be ori-
ented towards human-human dialogue (Allen and
Core, 1997; Devillers et al, 2002; Thompson et
al., 1993; Traum and Heeman, 1996; Stolcke et
al., 2000) or multi-party interactions (Galley et al,
2004), or human-computer interaction (Walker
and Passonneau, 2001; Hardy et al, 2003), in-
cluding multimodal ones (Thompson et al, 1993;
Kruijff-Korbayova? et al, 2006).
A major focus of the cited work is on how to
recognize or generate speech acts for interactive
systems, or how to classify speech acts for dis-
tributional analyses. The focus can be on a spe-
cific type of speech act (e.g., grounding and re-
pairs (Traum and Heeman, 1996; Frampton and
Lemon, 2008)), or on more general comparisons,
such as the contrast between human-human and
human-computer dialogues (Doran et al, 2001).
While there is a large degree of overlap across
schemes, the set of DA types will differ due to dif-
ferences in the nature of the communicative goals;
thus information-seeking versus task-oriented di-
alogues differ in the set of speech acts and their
relative frequencies.
Our motivation in providing a new DA annota-
tion scheme is that our focus differs from much
of this prior work. We aim for a relatively ab-
stract annotation scheme in order to make compar-
isons across interactions of widely differing prop-
erties. Our initial focus is less on speech act types
and more on the patterns of local alternation be-
357
tween an initiating speech act and a responding
one?the analog of adjacency pairs (Sacks et al,
1974). The most closely related effort is (Gal-
ley et al, 2004), which aims to automatically
identify adjacency pairs in the ICSI Meeting cor-
pus, a large corpus of 75 meetings, using a small
tagset. Their maximum entropy ranking approach
achieved 90% accuracy on the 4-way classifica-
tion into agreement, disagreement, backchannel
and other. Using the switchboard corpus, (Stolcke
et al, 2000) achieved good dialogue act labeling
accuracy (71% on manual transcriptions) for a set
of 42 dialogue act types, and constructed proba-
bilistic models of dialogue act sequencing in order
to test the hypothesis that dialogue act sequence
information could boost speech recognition per-
formance.
There has been far less work on developing
manual and automatic dialogue act annotation
schemes for email. We summarize some salient
recent work. Carvalho and Cohen (2006) use word
n-grams (with extensive preprocssing) to classify
entire emails into a complex ontology of speech
acts. However, in their experiments, they con-
centrate on detecting only a subset of speech acts,
which is comparable in size to ours. Speech acts
are assigned for entire emails, but several speech
acts can be assigned to one email. Apparently,
they develop separate binary classifiers for each
speech act. Corston-Oliver et al (2004) are in-
terested in identifying tasks in email. They label
each sentence in email with tags from a set which
describes the type of content of the sentence (de-
scribing a task, scheduling a meeting), but are less
interested in the interactive aspect of email com-
munication (creating an obligation to respond).
There has been some work which relates to find-
ing links, but limited to finding question-answer
pairs. Shrestha and McKeown (2004) first de-
tect questions using lexical and part-of-speech fea-
tures, and then find the paragraph that answers the
question. They use features related to the structure
of the email thread, as well as lexical features. As
do we, they find that classifying is easier than link-
ing.
Ding et al (2008) argue that in order to do
well at finding answers to questions, one must
also find the context of the question, since it of-
ten contains the information needed to identify the
answer. They use a corpus of online discussion
forums, and use slip-CRFs and two-dimensional
CRFs, models related to those we use. We will
investigate their proposal to consider the question
context in future work.
While they do not use dialogue act tagging
to compare modalities, as we do, Murray and
Carenini (2008) compare spoken conversation
with email by comparing a common summariza-
tion architecture across both modalities. They get
similar performance, but the features differ.
Table 1: DFU speech act labels
Request-Information (R-I)
Request-Action (R-A)
Inform (Inf)
Commit (Comm)
Conventional (Conv)
Perform (Perf)
Backchannel (Bch) (+/- Grounding)
Other
3 Annotation Scheme
Figure 1: Example DFU illustrating the relation of
extent (segmentation) to speech act type
M1.2 I have completed the invoices for April,
May and June
M1.3 and we owe Pasadena each month for a to-
tal of $3,615,910.62.
M1.4 I am waiting to hear back from Patti on May
and June to make sure they are okay with her.
[Inform(1.2-1.4): status of Pasadena invoicing-
completed & pending approval ? versus amount
due]
Sflink(1.2-1.4)
M2.1 That?s fine.
[Inform(2.1): acknowledgement of status of
Pasadena invoicing]
Blink(1.2-1.4)
The annotation scheme presented here consists
of Dialogue Function Units (DFUs), which are
intended to represent abstract units of interac-
tion. The last two authors developed the annota-
tion on three contrasting corpora: email threads,
telephone conversations, and court transcripts. It
builds on our previous work in intention-based
segmentation (Passonneau and Litman, 1997),
and on mixing a formal schema with natural lan-
guage descriptions (Nenkova et al, 2007). In this
358
paper, we investigate the modalities of telephone
two-person conversation in a library setting, and
multi-party email in a workplace setting. Our ini-
tial focus is on the structure of turn-taking. By
using a relatively abstract annotation scheme, we
can compare and contrast this behavior across dif-
ferent types of interaction.
Our unit of annotation is the DFU. DFUs have
an extent, a dialogue act (DA) label along with
a description, and possibly one or more forward
and/or backward links. We explain each compo-
nent of the annotation in turn. We use the exam-
ple in Figure 1; the example is drawn from actual
messages, but has been modified to yield a more
succinct example.
The extent of a DFU roughly corresponds to that
portion of a turn (conversational turn; email mes-
sage; etc.) that corresponds to a coherent com-
municative intention. Because we do not address
automatic identification of the segmentation into
DFU units in this paper, we do not discuss how
annotators are instructed to identify extent.
As illustrated in Figure 1, the communicative
function of a DFU is captured by a speech act
type, and a natural language description. This is
somewhat analogous to the natural language de-
scriptions associated with Summary Content Units
(SCUs) in pyramid annotation (Nenkova et al,
2007), or with the intention-based segmentation
of (Passonneau and Litman, 1997). The pur-
pose in all cases is to require annotators to artic-
ulate briefly but specifically the unifying intention
(Passonneau and Litman, 1997), semantic content
(Nenkova et al, 2007), or speech act. We use the
eight dialogue act types listed in the upper left of
Table 1. To accommodate discontinuous speech
acts, due to the interruptions that are common to
conversation, each speech act can have an oper-
ator affix such as ?-Continue?. We have previ-
ously shown (Passonneau and Litman, 1997) that
intention-based segmentation can be done reliably
by multiple annotators. For twenty narratives each
segmented by the same seven annotators, using
Cochran?sQ (Cochran, 1950), we found the prob-
abilities associated with the null hypothesis that
the observed distributions could have arisen by
chance to be at or below p=0.1 ?10?6. Partition-
ingQ by number of annotators gave significant re-
sults for all values of A ranging over the number
of annotators apart from A = 2. We would expect
similar patterns of agreement on DFU segmen-
tation, but have not collected segmentation data
from multiple annotators on the two corpora pre-
sented here.
DFU Links, or simply Links, correspond to ad-
jacency pairs, but need not be adjacent. A forward
link (Flink) is the analog of a ?first pair-part? of
an adjacency pair (Sacks et al, 1974), and is sim-
ilarly restricted to specific speech act types. All
Request-Information and Request-Action DFUs
are assigned Flinks. The responses to such re-
quests are assigned a backward link (Blink). In
principle, a response can be any of the speech act
types, thus it can be an answer to a question (In-
form), a rejection of a Request-Action or a com-
mitment to take the requested action (Commit),
a request for clarification (Request-Information),
and so on. In most but not all cases, requests are
responded to, thus most Flinks and Blinks come in
pairs. We refer to Flinks with no matching Blink
as dangling links. If an utterance can be inter-
preted as a response to a preceding DFU, it will
get a Blink even where the preceding DFU has no
Flink. The preceding DFU taken to be the ?first
pair-part? of the Link will be assigned a secondary
forward link (Sflink). All links except dangling
links are annotated with the address of the DFU
from which they originate. Figure 1 illustrates an
email message (M2) containing a single sentence
(?That?s fine?) that is a response to a DFU in a
prior email (M1), where the prior email had no
Flink because it only contains Inform DAs; thus
M1 gets an Sflink.
4 Corpora
The Loqui corpus consists of 82 transcribed dia-
logues from a larger set of 175 dialogues that were
recorded at New York City?s Andrew Heiskell
Braille and Talking Book Library during the sum-
mer of 2005. All of the transcribed dialogues per-
tain to one or more book requests. Forty-eight
dialogues were annotated; the annotators worked
from a combination of the transcription and the au-
dio. Three annotators were trained together, anno-
tated up to a dozen dialogues independently, then
discussed, adjudicated and merged ten of them.
During this phase, the annotation guidelines were
refined and revised. One of the three annotators
subsequently annotated 38 additional dialogues.
We also annotated 122 email threads of the En-
ron email corpus, consisting of email messages
in the inboxes and outboxes of Enron corporation
359
Table 2: Distributional Characteristics of Dialogue
Acts in Enron and Loqui
Loqui Enron
Words 21097 17924
DFUs 3845 1400
Speech Act Labels
Inform 1928 50% 853 61%
Request-Inf. 761 20% 149 11%
Request-Action 39 1% 37 3%
Commit 338 9% 3 0%
Conventional 254 7% 356 25%
Backchannel 507 13% 0 0
Other 18 0% 2 0%
Total 3845 100% 1400 100%
Links
Paired Links 1204 63% 193 28%
Flink/Blink 702 58% 83 43%
Sflink/Blink 502 42% 110 57%
Dangling Links 90 2% 97 7%
Mutliple Blinks 4 0% 4 0%
Links by Speech Act Labels
Inform 1003 83% 142 74%
Request-Inf. 170 14% 44 23%
Request-Action 1 0% 5 3%
Commit 13 1% 2 1%
Conventional 2 0% 0 0
Backchannel 15 1% 0 0
1204 100% 193 100%
employees. Most of the emails are concerned with
exchanging information, scheduling meetings, and
solving problems, but there are also purely social
emails. We used a version of the corpus with some
missing messages restored from other emails in
which they were quoted (Yeh and Harnly, 2006).
The annotator of the majority of the Loqui corpus
also annotated the Enron corpus. She received ad-
ditional training and guidance based on our experi-
ence with a pilot annotator who helped us develop
the initial guidelines.
Table 2 illustrates differences between the two
corpora. The DFUs in the Loqui data are much
shorter, with 5.5 words on average compared with
12.8 words in Enron. The distribution of DFU la-
bels shows a similarly high proportion of Inform
acts, comprising 50% of all Loqui DFUs and 61%
of all Enron DFUs. Otherwise, the distributions
are quite distinct. The Loqui interactions are all
two party telephone dialogues where the callers
(library patrons) tend to have limited goals (re-
questing books). The Enron threads consist of
two or more parties, and exhibit a much broader
range of communicative goals. In the Loqui data,
backchannels are relatively frequent (13%) but do
not occur in the email corpus for obvious reasons.
There are some Commits (9%), typically reflect-
ing cases where the librarian indicates she will
send requested items to the caller by mail, or place
them on reserve. There are no Commits in the
Enron data. Neither corpus has many Request-
Actions; the Loqui corpus has many more requests
for information, which includes requests made by
the librarian, e.g., for the patrons? identifying in-
formation, or by the caller.
The most striking differences between the two
corpora pertain to the distribution of DFU Links.
In Loqui, 63% of the DFUs are the first pair-part
or the second pair-part of a Link compared with
28% in Enron. In Loqui, the majority of Links
are initiated by overt requests (58% of Links are
Flink/Blink pairs), whereas in Enron, the major-
ity of Links involve SFlinks (57%). There are
relatively few dangling Links in either dataset,
with more than three times as many in Enron (7%
versus 2% in Loqui). Most of the DFU types
in the second pair-part of Links are Informs and
Request-Information, with a different proportion
in each dataset. In Loqui, 83% of DFUs that are
second pair-part of a Link are Informs compared
with 74% in Enron; correspondingly, only 14% of
DFUs in Links are Request-Information in Loqui
versus 23% in Enron.
5 Dialogue Act Tagging and Link
Prediction
There are two machine learning tasks in our prob-
lem. The first is Dialogue Act (DA) Tagging, in
which we assign DAs to every Dialogue Func-
tional Unit (DFU). The second is Link predic-
tion, in which we predict if two DFUs form a link
pair. In this paper, we assume that the DFUs are
given. We propose three systems to tackle the
problem. The first system is a non-strawman Base-
line Heuristics system, which uses the structural
characteristics of dialogue. The second is Regu-
lar SVM. The third is Structured SVM. Structured
SVM is a discriminative method that can predict
complex structured output. Recently, discrimi-
native Probabilistic Graphical Models have been
widely applied in structural problems (Getoor and
360
Taskar, 2007) such as link prediction. However,
Structured SVM (Taskar et al, 2003; Tsochan-
taridis et al, 2005) is also a compelling method
which has the potential to handle the interdepen-
dence between labeling and sequencing, due to its
ability to handle dependencies among features and
prediction results within the structure. sequence
labeling (Tsochantaridis et al, 2005). We have
adapted Structured SVM to our problem, provided
a novel method for link prediction, and shown that
it is superior in some aspects to Regular SVM.
5.1 Features
We have two sets of features. DFU features are as-
sociated with a particular DFU, and link features
describe the relationship between two DFUs. DFU
features are used in both tasks. Link features are
only used in link prediction. The feature vector of
a link contains two sets of DFU features and the
link features that are defined over the two DFUs.
Table 3 gives the features we used, which are al-
most identical for both corpora, so we could com-
pare the performance.
Because a lot of Flinks are questions, we
chose some features that are tailored to Question-
Answer detection, such as presence of a question
mark. Dialogue fillers and acceptance words af-
fect the accuracy of Part-Of-Speech tagging. On
the other hand, they are helpful indicators of dis-
fluency or confirmation. So we hand-picked a list
of filler and acceptance words, removed them from
the sentence, and added features counting their oc-
currences.
5.2 Baseline Heuristics
Dialogue Act Tagging We use the most frequent
DA as the heuristic for prediction. In both Enron
and Loqui, this DA is Inform.
Link Prediction In link prediction, the heuris-
tics for Enron and Loqui corpora are different due
to structural differences. In Loqui, whenever we
see a DFU with a Forward Link (DA is Request-
Information or Request-Action), we predict that
the target of the link is the first following DFU that
is available and acceptable. ?Available? means
that the second DFU has not been assigned a Back-
ward Link yet. ?Acceptable? means that the sec-
ond DFU has a DA that is very frequent in a Back-
ward Link and it is of a different speaker to the
first DFU. We enforce similar constraints in Enron
corpus for link prediction, except that the second
Table 3: DFU features (E: Enron, L: Loqui)
Structural for DA prediction
E,L First three POS
E,L Relative Position in the Dialogue
E Existence of Question Mark
E,L Does the first POS start with ?w? or ?v?
E,L Length of the DFU
E Head, body, tail of the Message
E,L Dialogue Act (Only used in link prediction)
Lexical for DA prediction
E,L Bag of Words
E,L Number of Content Words
L Number of Filler Words, as ?uh?, ?hmm?
E,L Number of Acceptance Words, as ?yes?
Structural for Link prediction
E,L The distance between two DFUs
Lexical for Link prediction
E,L Overlapping number of content words
DFU not only has to be from a different author,
but also has to be in a message which is a direct
descendant in the reply chain of the message that
contains the first DFU. The baseline link predic-
tion algorithm uses the DAs as predicted by the
Regular SVM. If we used the baseline DA predic-
tion, the result would be too low to make a valid
comparison against other systems in terms of link
prediction because all DAs would be identical.
5.3 Regular SVM
We have used the Yamcha support vector machine
package (chasen.org/?taku/software/yamcha/).
The advantage of Yamcha is that it extends the
traditional SVM by enabling using dynamically
generated features such as preceding labels.
Dialogue Act Tagging We use the feature vector
of the current DFU as well as the predicted DA of
the preceding DFU as features to predict the DA
of the current DFU.
Link Prediction First, in order to limit search
space, we specify a certain window size to produce
a space S of DFU pairs under consideration. For
a particular DFU, we look at all succeeding DFUs
and check if these two DFUs satisfy the follow-
ing constraint: in Loqui, they must be of different
speakers; in Email, one must be another?s ancestor
and they must be of different authors. We consider
all valid pairs starting from the current DFU until
361
the number of considered valid pairs reaches the
window size. Then we proceed to the next DFU
and collect more DFU pairs into our consideration
space.
Second, we train a link binary classifier with all
DFU pairs in this consideration space along with a
binary classification correct/not correct as training
data. This classifier takes the feature vectors of the
two DFUs as well as the link features such as the
distance between these two DFUs as features.
Third, we apply a greedy algorithm to gener-
ate links in the test data with the binary classifier.
The algorithm firstly uses the classifier to generate
scores for all DFU pairs in the consideration space
of the test data, then it scans the dialogue sequen-
tially, checks all preceding DFUs that are allowed
to link to the current DFU (i.e., the DFU pair is in
the consideration space), and assigns correspond-
ing links to the most likely DFU pair. We impose a
restriction that there can be at most one Flink, one
Sflink and one Blink for any given DFU.
5.4 Structured SVM
A Structured SVM is able to predict com-
plex output instead of simply a binary result
as in a regular SVM. There are several vari-
ants. We have followed the margin-rescaling ap-
proach (Tsochantaridis et al, 2005), and im-
plemented our systems using SVMpython, which
is a python interface to the SVMstruct package
(svmlight.joachims.org/svm struct.html). Gener-
ally, Structured SVM learns a discriminant func-
tion F : X?Y ? R, which estimates a score of
how likely the output y is given the input x. Cru-
cially, y can be a complex structure. Section A in
the appendix; here, we summarize the main intu-
itions.
Dialogue Act Tagging The input x is a sequence
of DFUs, and y is the corresponding sequence of
DAs to predict. Compared to Regular SVM, in-
stead of predicting yt one at a time, Structured
SVM optimizes the sequence as a whole and pre-
dicts all labels simultaneously. Due to the similar-
ity to HMM, the maximization problem is solved
by the Viterbi algorithm (Tsochantaridis et al,
2005).
Link Prediction The input now contains the DFU
sequence, a link consideration space, as well as
a label sequence, which we get from the previ-
ous stage. The output structure chooses among
the possible links in the link consideration space,
such that there is at most one Flink/SFlink or Blink
for any given DFU, and that there are no crossing
links. (Note that all the constraints are only en-
forced in training and prediction; in testing, we
compare results against the complete manual an-
notations which do not follow these constraints.)
Then the maximization problem can be solved by a
straightforward dynamic programming algorithm.
Table 4: Result of DA prediction
Baseline Regular Struct
Loqui 50.14% 68.30% 70.26%
Enron 60.93% 88.34% 88.71%
Note: Structured SVM parameters for Loqui are C =
300, ? = 1; Structured SVM parameters for Enron
are C = 1000, ? = 1.
6 Experiments
We have three hypotheses for our experiments:
Hypothesis 1 Link prediction is harder than Dia-
logue Act prediction.
Hypothesis 2 Enron is harder than Loqui.
Hypothesis 3 Structured SVM is better than Reg-
ular SVM, and Baseline is the worst.
We have applied the algorithm described in Sec-
tion 5 to both the Enron and Loqui corpora. The
data set is annotated with DFUs; we focus on the
DA labels and Links. As discussed before, every
system is a pipeline that would preprocess the data
into separate DFUs, predict the Dialogue Acts,
and then feed the Dialogue Acts into the link pre-
diction algorithm. The size of the data set is shown
in Table 2. We do five-fold cross-validation.
Table 4 shows the accuracy of three systems on
Enron and Loqui. Structured SVM has a clear lead
to Regular SVM in Loqui; but the advantage is less
clear in Enron. Tables 6 and 7 give detailed results
of DA prediction.We do not show DAs that do not
exist in the corpora, or that were not predicted by
the algorithms. Both Regular SVM and Structured
SVM performed consistently for the two corpora.
Table 5 gives Link prediction results. Note that
when we compute the combined result for both
types of links, we are only concerned with the
Link position. The seperate results for Flink/Blink
and Sflink/Blink require us to identify the types of
links first, so here we not only compare the posi-
tion of predicted links against the gold, but also
require predicted DAs to indicate the link type
(e.g., the DA of the first DFU must be Request-
362
Table 5: Link Prediction for Enron and Loqui
Baseline Regular Struct
Enron R P F R P F R P R
Paired Links 16.66% 40% 23.52% 18.75% 55.38% 28.01% 31.25% 39.47% 34.88%
Flink/Blink 32.53% 33.75% 33.13% 26.50% 61.11% 36.97% 34.93% 47.54% 40.27%
Sflink/Blink 0.0% 0.0% 0.0% 11.92% 44.82% 18.83% 22.93% 27.47% 25.00%
Loqui
Paired Links 30% 56.15% 39.11% 43.59% 60.60% 50.71% 44.15% 56.02% 49.38%
Flink/Blink 43.30% 46.47% 44.83% 40.58% 57.73% 47.66% 43.55% 60.04% 50.48%
Sflink/Blink 0.0% 0.0% 0.0% 21.76% 29.36% 25.00% 22.88% 26.24% 24.45%
Note: Structured SVM parameters for Enron are C = 2000, ? = 2., for Loqui C = 1000, ? = 4.
Information or Request-Action to qualify as a
Flink/Blink).
Table 6: Recall/Precision/F-measure of DA pre-
diction for Loqui (in %)
Regular Struct
P R F P R F
R-A 50.0 51.7 50.9 43.3 43.3 43.3
R-I 51.3 61.1 55.8 52.3 71.2 60.3
Inf 73.9 73.0 73.5 76.9 74.1 75.5
Bch 65.3 51.7 57.7 65.1 53.6 58.8
Com 5.6 33.3 9.5 5.6 33.3 9.5
Conv 81.2 84.0 82.6 83.7 83.3 83.5
Table 7: Recall/Precision/F-measure of DA pre-
diction for Enron (in %)
Regular Struct
R P F R P F
R-A 27.8 55.6 37.0 25.0 75.0 37.5
R-I 77.9 82.3 80.0 77.2 83.3 80.1
Inf 92.5 90.6 91.5 92.1 91.2 91.7
Conv 90.5 87.3 88.9 93.4 85.6 89.3
7 Discussion
Hypothesis 1 The result of DA prediction is dras-
tically better than link prediction. There are usu-
ally indicators of DA types such as ?thank you? for
Conventional, so learning algorithms could easily
capture them. But in link prediction, we frequently
need to handle deep semantic inference and some-
times useful information exists in the surrounding
context rather than the DFU itself. Both of these
scenarios imply that in order to predict links or re-
lationships better, we need more sophisticated fea-
tures.
Hypothesis 2 This hypothesis turns out to be half-
correct. The DA prediction accuracy for Enron
is better than that of Loqui. The higher percent-
age of Inform and less diversity of DAs in Enron
(See Appendix for statistics) may be part of the
reason. Another possible explanation is that as a
set of spoken dialogue data, Loqui is inherently
more difficult to process than written form, since
some common tasks such Part-Of-Speech tagging
have lower accuracy for spoken data. On the other
hand, the result of link prediction did confirm our
hypothesis. The first reason is that there are far
fewer links in Enron than in Loqui, so we have less
training data. The tree structure of the reply chain
in the email threads also makes prediction more
difficult. And the link distance is longer, because
in email, people can respond to a very early mes-
sage, while in a phone conversation, people tend
to respond to immediate requests.
Hypothesis 3 Both SVM models perform better
than the baseline. Generally, Structured SVM per-
forms better than Regular SVM, especially in link
prediction for Enron. This confirms the advan-
tage of using Structured SVM for output involv-
ing inter-dependencies. The only exception is the
Sflink prediction in Loqui, which in turn affects
the overall accuracy of link prediction.
References
James Allen and Mark Core. 1997. Damsl:
Dialogue act markup in several layers.
http://www.cs.rochester.edu/research/cisd/resources/damsl.
Vitor Carvalho and William Cohen. 2006. Improving ?email
speech acts? analysis via n-gram selection. In Proceed-
ings of the Analyzing Conversations in Text and Speech.
William G. Cochran. 1950. The comparison of percentages
in matched samples. Biometrika, 37:256?266.
363
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Stan Szpakowicz Marie-Francine Moens, edi-
tor, Text Summarization Branches Out: Proceedings of the
ACL-04 Workshop.
Laurence Devillers, Sophie Rosset, Bonneau-Helene May-
nard, and Lamel Lori. 2002. Annotations for dynamic
diagnosis of the dialog state. In LREC.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract contexts
and answers of questions from online forums. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio.
Christine Doran, John Aberdeen, Laurie Damianos, and
Lynette Hirschman. 2001. Comparing several aspects of
human-computer and human-human dialogues. In Pro-
ceedings of the 2nd SIGDIAL Workshop on Discourse and
Dialogue.
Matthew Frampton and Oliver Lemon. 2008. Using dialogue
acts to learn better repair strategies for spoken dialogue
systems. In ICASSP.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 669?676.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
Statistical Relational Learning. The MIT Press.
Hilda Hardy, Kirk Baker, Bonneau-Helene Maynard, Lau-
rence Devillers, Sophie Rosset, and Tomek Strza-
lkowski. 2003. Semantic and dialogic annotation
for automated multilingual customer service. In Eu-
rospeech/Interspeech.
Kazunori Komatani, Nayouki Kanda, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2005. Contextual constraints based on
dialogue models in database search task for spoken dia-
logue systems. In Eurospeech.
Ivana Kruijff-Korbayova?, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Ver-
ena Rieser, and Jan Schehl. 2006. The Sammie corpus of
multimodal dialogues with an mp3 player. In LREC.
Gabriel Murray and Giuseppe Carenini. 2008. Summarizing
spoken and written conversations. In EMNLP.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKe-
own. 2007. The pyramid method: incorporating human
content selection variation in summarization evaluation.
ACM Transactions on Speech and Language Processing,
4(2).
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1).
Andrei Popescu-Belis. 2008. Dimensionality of dialogue act
tagsets: An empirical analysis of large corpora. LREC,
42(1).
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systemics for the organization of turn-
taking for conversation. Language, 50(4).
Lokesh Shrestha and Kathleen McKeown. 2004. Detection
of question-answer pairs in email conversations. In COL-
ING.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,
Rachel Martin, Carol Van Ess-Dykena, and Meteer Marie.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. International Jour-
nal of Computational Linguistics, 26(3).
Ben Taskar, Crlos Guestrin, and Daphne Koller. 2003. Max-
margin markov networks. In NIPS.
Henry S. Thompson, Anne H. Anderson, Ellen Gurman Bard,
Gwyneth Doherty-Sneddon, Alison Newlands, and Cathy
Sotillo. 1993. The HCRC map task corpus: Natural
dialogue for speech recognition. In Proceedings of the
DARPA Human Language Technology Workshop.
David Traum and Peter Heeman. 1996. Utterance units and
grounding in spoken dialogue. In Interspeech/ICSLP.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. JMLR,
6.
Marilyn A. Walker and Rebecca Passonneau. 2001. Date:
A dialogue act tagging scheme for evaluation of spoken
dialogue systems. In HLT.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread re-
assembly using similarity matching. In Conference on
Email and Anti-Spam.
364
A Appendix: Structured SVM
This section provides mathematical background
for Secton 5.4. The hypothesis function is given
by:
f(x,w) = argmaxy?YF (x,y : w)
And in addition, we assume F to be linear to a
joint feature map ?(x,y).
F (x,y : w) = ?w,?(x,y)?
We also define a loss function ?(y,y) which de-
fines the deviation of the predicted output y to the
correct output.
As a result, given a sequence of training
examples,(x1,y1) ? ? ? (xn,yn) ? X ? Y, the
function we need to optimize becomes:
minw,? 12 ?w?
2 + Cn
?n
i=1 ?i
s.t. ?i?y ? Y\y(i) : ?w, ??i(y)? >
?(y(i),y)? ?i where,
?w, ??i(y)? =
?
w,?(x(i),y(i))??(x(i),y)
?
w is optimized towards maximizing the margin
between the true structured output y and any
other suboptimal configurations for all training in-
stances.
A cutting plane optimization algorithm is im-
plemented in SVMstruct. However, for any prob-
lem, we need to implement the feature map
?(x,y), the loss function ?(y,y), and a maxi-
mization problem which enables the cutting plane
optimization, i.e.
y = argmaxy?Y?(y(i),y)? ?w, ??i(y)?
Only certain feature maps that would make
solving this maximization effectively, usually by
dynamic programming, could be handled this way.
For Dialogue Act Tagging, let x =(
x1,x2 . . .xT
)
be the sequence of DFUs,
and y =
(
y1,y2 . . .yT
)
the corresponding se-
quence of dialogue acts. ?(xt) represents the DFU
features and ?(xt) ? RD. yt ? L = {l1, . . . , lK}
where L contains the set of available DAs. The
feature map is (Tsochantaridis et al, 2005):
?(x,y) =
( ?T
t=1 ?(x
t)??(yt)
?(yt?1)??(yt)
)
where ?(yt) = [?(l1,y), . . . , ?(lk,y)] and ? is
an indicator function that returns 1 if two parame-
ters are equal. ?-operator is defined as:
RD ?RK ? RD?K , (a? b)i+(j?1)D ? ai ? bj
In analogy to an HMM, the lower part in
?(x,y) encodes the histogram of adjacent DA
transitions in y ; the upper part encodes the DA
emissions from a specific label to one dimension
in the DFU feature vector. Hence, the total num-
ber of dimensions in ?(x,y) is K2 + DK. As
a result, F (x,y : w) = ?w,?(x,y)? gives a
global score based on all transitions and emissions
in the sequence, which captures the dependecies
among nearby labels and mimics the behaviour of
an HMM. Figure 2 gives an example of how to
compute the feature map.
The loss function is the sum of all zero-one
losses across the sequence, i.e.
?(y,y) = ?
?T
t=1 ?(y
t,yt)
? denotes a cost assigned to every DA loss.
For Link Prediction, the input contains the
DFU sequence x, a link consideration space
s = {(i, j) :,DFU i and j is being considered},
as well as label sequence y which we get from
the previous stage. ?(xi,xj) is the link feature
defined over two DFUs. Let the dimension of
link feature be B. The output structure u ={
u1,u2 . . .uT
}
specifies the link plan. ut denotes
that there is a link from DFU t ? ut to t with the
exception that ut being zero denotes there is no
link pointing to t. The setup of u constraints that
there can be at most one Flink/SFlink or Blink for
any given DFU. In addition u is also subject to the
constraint that all specified links must be in the
link consideration space.
The discriminant function becomes F : X ?
Y?S?U? R. Similar to structured DA predic-
tion, the discriminant function should give a global
evaluation as to how likely is the link plan spec-
ified by U with respect to all the input vectors.
Our solution is to decompose the score, and cor-
respondingly, the feature representation into two
components, link emission and no-link emission;
the details can be found in Figure 3 in the appendix
and an example is in Figure 2.
Similarly, we could define the loss function as
the sum of all zero-one losses across the sequence
, i.e.
?(u,u) = ?
?T
t=1 ?(u
t,ut)
? denotes a cost assigned to every Link loss.
365
Figure 2: A full example of feature map for Structured SVM
x1 = ?are you you sure?
x2 = ?sure?
y1 = ?Req-Info?
y2 = ?Inform?
u1 = 0
u2 = 1
?(x1) = (1, 2, 1)
?(x2) = (0, 0, 1)
?(x1,x2) = (1, 1)
?da =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0
0
0
1
0
0
1
1
2
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Inform to Inform
Inform to Req-Info
Req-Info to Inform
Req-Info to Inform
Inform with ?are?
Inform with ?you?
Inform with ?sure?
Req-Info with ?are?
Req-Info with ?you?
Req-Info with ?sure?
?link =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1
2
1
0
1
0
0
1
1
0
1
1
1
2
1
0
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1st link pair-part with?are?
1st link pair-part with?you?
1st link pair-part with?sure?
1st link pair-part with Inform
1st link pair-part with Req-Info
2nd link pair-part with?are?
2nd link pair-part with?you?
2nd link pair-part with?sure?
2nd link pair-part with Inform
2nd link pair-part with Req-Info
distance of link
overlap of link
No-Link with?are?
No-Link with?you?
No-Link with?sure?
No-Link with Inform
No-Link with Req-Info
Note: In this example, ?(xt) extracts the bag-of-words features from xt. ?are?,?you?,?sure? are the 1st, 2nd
and 3rd DFU feature respectively. ?(xi,xj) extracts the distance and number of the overlap content, which are
the link features, from the 1st and 2nd pair-part in a DFU link pair. There is a link from DFU 1 to DFU 2 as
specified by j ? uj = i, but there is no link pointing to DFU 1.
Figure 3: The feature map of link prediction for
the structured SVM
?L =?
?
?
?
?
?
?
?
?T?1
i=1
?T
j=i+1 ?(x
i)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(y
i)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(x
j)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(y
j)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(x
i,xj)?(i, j ? uj)
?
?
?
?
?
?
?
?
?NL =
( ?T
i=1 ?(x
i)?(0,ui)
?T
i=1 ?(y
i)?(0,ui)
)
?(x,y, s,u) =
(
?L
?NL
)
Note: ?L and ?NL correspond to the link and no-
link emissions in the feature map ?(x,y, s,u) re-
spectively as shown in the equations. The total di-
mension of the feature map is 3D + 3K +B.
366
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 565?576, Dublin, Ireland, August 23-29 2014.
Biber Redux: Reconsidering Dimensions of Variation in American English
Rebecca J. Passonneau
Center for Computational Learning Systems
Columbia University
New York, New York USA
becky@ccls.columbia.edu
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
Songqiao Su
Department of Computer Science
Columbia University
New York, New York USA
ss4555@columbia.edu
Jesse Stuart
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
jestuart@cs.vassar.edu
Abstract
Genre classification has been found to improve performance in many applications of statistical
NLP, including language modeling for spoken language, domain adaptation of statistical parsers,
and machine translation. It has also been found to benefit retrieval of spoken or written docu-
ments. At its base, however, classification assumes separability. This paper revisits an assump-
tion that genre variation is continuous along multiple dimensions, and an early use of principal
component analysis to find these dimensions. Results on a very heterogeneous corpus of post-
1990s American English reveal four major dimensions, three of which echo those found in prior
work and the fourth depending on features not used in the earlier study. The resulting model
can provide a basis for more detailed analysis of sub-genres and the relation between genre and
situations of language use, as well as a means to predict distributional properties of new genres.
1 Introduction
Although a precise definition of the term ?genre? has traditionally proven to be elusive, it cannot be dis-
puted that a genre represents a set of shared regularities among written or spoken documents that enables
readers, writers, listeners and speakers to signal discourse function, and that conditions their expectations
of linguistic form. Genre distinctions are therefore an important aspect of language use and understand-
ing. They clearly have a role to play in statistical language processing, which relies on regularities of
form as well as content. Indeed, with the advent of the Web, statistical methods for genre differenti-
ation have been applied to information retrieval to limit search criteria and organize results (Karlgren
and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of
genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More
recently, the development of genre-dependent models for a variety of natural language processing (NLP)
tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition
(Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine trans-
lation (Wang et al., 2012) has been found to significantly improve performance. The ability to match
documents by genre has also become important for collecting data to train language models for spoken
language understanding, given the difficulty of creating large repositories of transcribed spoken language
corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
565
While the utility of document characterization by genre for empirical language analysis is widely ac-
knowledged, there is relatively little agreement on methodology. In part, this stems from the difficulty of
providing a comprehensive list of genres or even an operational definition of what constitutes a distinct
genre, much less a definitive set of features to characterize genre differences. The earliest large-scale
statistical study of genre is that of Biber (Biber, 1988), who applied principal component analysis (PCA)
to a one-million word corpus consisting of heterogeneous varieties of spoken and written discourse in
order to identify multiple dimensions of variation in language. Biber argued that linguistic variation was
continuous along six dimensions: involved vs. informational, narrative vs. non-narrative, explicit vs.
situation-dependent reference, overt expression of persuasion, abstract vs. non-abstract information, and
on-line information elaboration; he identified features associated with each dimension, and characterized
kinds of discourse by joint assessment of similarities and differences across these dimensions. Inter-
estingly, since Biber?s study, there has been comparatively little investigation of how genres vary using
multivariate distributional methods (see, for example, the discussion in (Kilgarriff, 2001)).
Biber?s work, which was completed in the mid-1980?s, relied on a large number of features extracted
using somewhat ad hoc methods and reported no reliability measures. Given the renewed interest in genre
classification and the increasing interest in automatic techniques to adapt NLP tools across different kinds
of corpora, we feel it is worth subjecting Biber?s thesis to a new test, utilizing state-of-the-art methods
for extracting features from a high quality, very heterogeneous corpus. In addition to replicating Biber?s
basic approach with more reliable features, we include newer genres (e.g., email, blogs, tweets) in an
attempt to verify that these methods can generalize over different kinds of data. We use a smaller feature
set that overlaps with Biber?s for the most part, but which also includes features unavailable in the
earlier work. In our set, each feature was identified using freely available NLP tools and was manually
validated. In our use of different features, our experiment constitutes a strong test of Biber?s claim
that the dimensions of variation he identified arise from underlying constraints on usage. We find three
components similar to his, and a new one he did not find, based on our use of Named Entity features. We
find that genres that are separable on one component are often co-extensive on another. To quantify the
distinctiveness of each of the genres relative to the others, we use a metric that has previously been used
to measure separability of classes.
2 Related work and motivation
Our work builds on Biber?s 1988 study, but differs in the corpus and features used. Biber?s corpus and
MASC (Ide et al., 2010), the corpus used in our study, differ in source language (British English versus
American English), time coverage (skewed towards a single year versus three decades), and the situations
of use. Biber?s corpus was drawn from the Lancaster-Oslo-Bergen (LOB) Corpus of British English,
consisting of works published in 1961, the London-Lund corpus of spoken English, consisting of 87
texts of British English from private conversation, public interviews and panel discussions, telephone
conversations, radio broadcasts, spontaneous speeches and prepared speeches produced in the 1970s. To
these Biber added a collection of his own professional and personal letters. MASC represents a larger time
slice (1990s to present) and is more heterogeneous, including a wider range of traditional genres as well
as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage
of MASC?s rich set of validated annotations to include features that would not have been (easily) available
at the time of Biber?s study, and reconsider the use of some features used in his work.
Some work on genre classification contrasts with Biber?s approach, which assumes that documents
fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task
(Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini,
2004), or combined with topic classification (Rauber and M?uller-K?ogler, 2001; Lee and Myaeng, 2002).
All of these studies assume that documents fall discretely into distinct classes or clusters. These studies
vary in their approach to determining the genre of text, either by using corpora with pre-defined classes
(Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre
classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets
in genre studies have remained rather stable over the past three decades, mostly utilizing word-based
566
features similar to many of Biber?s such as individual lexical items and/or their orthographic charac-
teristics (e.g., contractions), part-of-speech (POS), punctuation (Kessler et al., 1997; Stamatatos et al.,
2000b), derivative statistics (e.g., average word/sentence length, ratios among lexical or POS classes),
and POS-ngrams (Santini, 2004; Feldman et al., 2009).
Karlgren and Cutting (1994) apply discriminant analysis to pre-defined classes from the Brown corpus
using easily identifiable information such as POS counts, type/token ratios, and sentence length. They
achieve relatively low accuracy of 52%. Kessler et al. (1997) also use the Brown corpus and classify
documents into three facets: brow, narrative, and genre. They extract 55 features, avoiding features
at the syntactic level that are computationally expensive to identify, and characterize them as lexical,
character-level, and derivative (log ratios and their sums). They achieve nearly 80% accuracy on their
six genre classes (reportage, editorial, scitech, legal, non-fiction, fiction). Feldman et al. (2009) create
a corpus of eight genres of speech and web text and test an approach to factor documents by genre,
formality and number of speakers. They achieve accuracy of 55% using quadratic discriminant analysis
on a representation consisting of features based on POS tags, words, and punctuation, reduced using
PCA. Santini (2004) applies high-dimensional POS trigram vectors to ten BBC genres (four spoken, six
written) with Na??ve Bayes classification. A document representation using a length-835 vector achieves
82.6% accuracy for 10-fold cross-validation on all 10 genres, and a Kappa agreement of 0.80.
Rauber and M?uller-K?ogler (2001) apply self-organizing maps (Kohonen, 1995) for both topic and
genre clustering, using features typical of readability measures (e.g., sentence and word lengths, punc-
tuation frequency). Lee and Myaeng (2002) address classification of web text and also do simultaneous
genre and subject (topic) classification, using a Naive Bayes learner. Tests on seven genres for both
English and Korean achieve 0.80 micro-averaged f-measure or 0.87 cosine similarity.
More recent work finds good performance from the use of ngram features for words, characters and
part-of-speech (Gries et al., 2009; Kanaris and Stamatatos, 2009; Sharoff et al., 2010). Gries et al. (2009)
relies only on word ngrams of various lengths to produce clusters with high maximum average silhouette
width, where higher widths represent more homogeneous clusters that are more distinct from one another.
They find that trigrams do best. Kanaris and Stamatatos (2009) uses frequently occurring character
ngrams without regard to their discriminatory power, and Sharoff et al. (2010) find that character ngrams
outperform word and pos ngrams. On benchmark corpora with from 4 to 8 genres, the latter two works
achieve accuracies of up to 96-97% on some corpora. They assume that genres can be taken as a given,
although Sharoff et al. (2010) note that chance-corrected human agreement on the gold standard is only
moderate.
Another strand of investigation addresses genre variation as a requirement for achieving better perfor-
mance in new domains, as in language modeling for speech applications (Bulyko and Ostendorf, 2003;
Sarikaya et al., 2005) or statistical parsers applied to text (Ravi et al., 2008; McClosky et al., 2010; Roux
et al., 2012), where downstream applications can include assignment of semantic argument structure.
Bulyko and Ostendorf (2003) select web text for class-based n-gram language modeling. They locate
relevant documents using queries representative of conversational speech, rather than characterizing the
documents as a whole in terms of statistical features, but demonstrate a significant reduction in Word
Error Rate (WER) for their enhanced language models. Sarikaya et al. (2005) achieve even higher im-
provements using a similar query methodology, then use BLEU scores, a machine translation similarity
method (Papineni et al., 2002), to find sentences that are closest to a domain sample. Ravi et al. (2008)
propose a method to predict parser accuracy based on properties of the new domain of interest and prop-
erties of the domain on which the parser was trained. Lexical features for words other than the 500
most frequent were found to generalize less well than features such as POS and sentence length. Subse-
quent work models corpus differences using regression models to predict parser accuracy McClosky et
al. (2010), or incorporates explicit genre classifiers Roux et al. (2012).
In our initial exploration of genre variation in MASC, we exploited a set of features that subsume most
of those discussed in the works reviewed above. We applied a variety of methods, including k-means
clustering, discriminative classifiers such as Na??ve Bayes, and PCA. Through comparison of results, we
discovered that classification had variable performance, and that PCA provided an explanation: docu-
567
Genre Code No. words Pct corpus
Court transcript CT 30052 6%
Debate transcript DT 32325 6%
Email EM 27642 6%
Essay ES 25590 5%
Fiction FT 31518 6%
Gov?t documents GV 24578 5%
Journal JO 25635 5%
Letters LT 23325 5%
Newspaper NP 23545 5%
Non-fiction NF 25182 5%
Spoken SP 25783 5%
Technical TC 27895 6%
Travel guides TG 26708 5%
Twitter TW 24180 5%
Blog BG 28199 6%
Ficlets FC 26299 5%
Movie script MS 28240 6%
Spam SM 23490 5%
Jokes JK 26582 5%
TOTAL 506768
(a) Genre distribution in MASC
Annotation type No. words
Logical 506659
Token 506659
Sentence 506659
POS/lemma (GATE) 506659
POS (Penn) 506659
Noun chunks 506659
Verb chunks 506659
Named Entities 506659
FrameNet 39160
Penn Treebank 506659
Coreference 506659
Discourse structure* 506659
Opinion 51243
TimeBank *55599
PropBank 88530
Committed Belief 4614
Event 4614
Dependency treebank 5434
(b) Summary of MASC annotations
Figure 1: Composition of the Manually Annotated Sub-Corpus
ments from distinct classes often fell within an identifiable region on one or more dimensions discovered
by PCA, but these regions overlapped one another along other dimensions. We concluded that whether
or not a set of documents can be categorized into relatively distinct classes by their linguistic forms rather
than content depends on how the documents are selected, how the classes are defined, and what features
are used. Our goal here is to refine a method to learn key dimensions of variation relevant for the same
types of applications referenced in work on genre identification, as discussed in Section 7.
3 Corpus and data preparation
MASC is a 500,000 word corpus of post 1990s American English comprised of texts from nineteen genres
of spoken and written language data in roughly equal amounts, shown in Figure 1a). Roughly 15% of
the corpus consists of spoken transcripts, both formal (court and debate) and informal (face-to-face,
telephone conversation, etc.); the remaining 85% covers a wide range of written genres, including social
media (tweets, blogs). The annotation types and coverage in MASC are given in Figure 1b); all MASC
annotations are hand-validated or manually produced. The corpus is fully open and freely available.
1
To prepare the data, we developed a framework in Groovy
2
(a dialect of Java) to extract linguistic
features, using version 1.2.0 of the GrAF API
3
to access the MASC data and annotations. Most texts
in MASC comprise complete discourse units, e.g. full conversations, letters, chapters from a book, etc.,
with the exception of tweets, jokes, and (to some extent) ficlets.
4
As shown in Figure 1a), although
each MASC genre contains roughly 25,000 tokens, the number of texts in any given genre varies widely,
from as few as two to over 100. To standardize the number of data points per genre, the texts in each
genre were concatenated and then divided into samples of even length, rounded to the nearest sentence
boundary. Portions of the texts containing email headers, bibliographic references, and computer code,
which contain an excess of certain punctuation and other special characters, were eliminated prior to
creating the samples.
Initially, we created sample sets consisting of 1,000 tokens per sample,
5
motivated by Biber?s observa-
tion that even rare linguistic features are relatively stable across samples of this size (Biber, 1993). Our
1
MASC is downloadable from http://www.anc.org/data/masc and available from the Linguistic Data Consortium (LDC).
2
http://groovy.codehaus.org
3
http://sourceforge.net/projects/iso-graf/
4
Ficlets are story fragments to which ?prequels? or ?sequels? are added by online participants.
5
We use tokens as the unit of analysis rather than blank-separated words (strings), which, given the MASC tokenization
strategy, means that hyphenated words such as ?able-bodied? and possessive markers (?s) are treated as individual tokens.
568
1 1st/2nd person pro.
2 3rd person pro.
3 Pronoun it
4 Copula verbs
5 All NEs
6 NEs w/o date
7 Verbs, base
8 Verbs, past
9 Gerunds/Pres. ptp.
10 Past ptp.
11 1st/2nd pres. sg. V
12 3rd pres. sg. V
13 Common nouns
14 All verbs
15 Proper nouns
16 Adjectives
17 Adverbs
18 Superlatives
19 All pers. pro.
20 Prepositions
21 Foreign words
22 Exist. there
23 Interjec.
24 NEs, person
25 NEs, date
26 NEs, location
27 NEs, org.
28 Suasive verbs
29 Stative verbs
30 Noun chunk length
31 Verb chunk length
32 Tokens/sentence
33 Characters/token
34 Periods
35 Questions
36 Exclamations
37 Commas
(a) Thirty-seven features
(b) Boxplots of the 37 features: the box shows the range of the 25th to 75th percentiles with the
median value identified by the vertical red bar. The black whiskers show the extreme values not
considered outliers, and the red are the outliers. The most extreme outliers of feature 21 were
dropped to save space.
Figure 2: Feature names and boxplots
experiments showed, however, that for the features used here, results were comparable using 500-token
chunks, which enabled us to work with a set of data points of the same size as Biber?s. Our process
generated 965 500-token chunks, with roughly 50 chunks per genre.
4 Features and feature analysis
Biber used sixty-seven features consisting primarily of lexical items and groups, parts of speech, and
quasi-syntactic features such as coordination, negation, relative pronoun deletion, that-clauses, and so
on. Many of the features in our set overlap with Biber?s, but we also exploit annotations in MASC to
provide additional features. All the MASC annotations have been manually validated, including those
produced by automated tools such as POS-taggers, NE recognizers, and shallow parsers.
PCA is appropriate for data with normally distributed values and can be used to reduce the number of
features to include only those that are the least correlated. It highlights features with the greatest varia-
tion. Figure 2b) shows boxplots of thirty-seven features we began with. These are mainly frequencies
normalized by the total token count in the document samples we created. They also include the average
characters per word, and average tokens per sentence, noun chunk, and verb chunk. Figure 2a) lists the
features by number. Features 21, 23, 28 and 36, which are foreign words, interjections, suasive verbs
and exclamations, have median values (red line within the box) near the 25th percentile, so are highly
skewed. We therefore dropped these and carried out the PCA with the remaining thirty-three.
6
Hierarchical clustering of the dataset by MASC genre yields the dendogram in Figure 3. We used
the city block metric (also known as taxicab distance), which is similar to Euclidean distance but less
sensitive to outliers. The legend identifies six major clusters for the 19 genres, with two singletons (Travel
guides and Technical documents), a cluster with three spoken genres (Court and Debate transcripts,
and transcripts of face-to-face and telephone conversations), two four-genre clusters, and one six-genre
6
To insure comparability of feature influence, all our features were re-scaled in [-1,1] with mean 0.
569
(a) Six groups from hierarchical clustering
(b) Hierarchical clustering
Figure 3: Hierarchical clustering of 19 MASC genres
cluster. These larger clusters consist of ?story-telling? genres (ficlets, fiction, jokes and movie scripts),
offline-interactive genres (letters, spam, email and tweets), and discursive text (blog, essay, journal, non-
fiction, government documents, and news). Thus the distribution of our features across the data predict
groupings that correspond well with our intuitions about the genres defined in MASC, providing some
justification for both our feature selection and the genre assignments in the corpus. The groupings also
reflect several of Biber?s dimensions of variation, as discussed in Section 7.
Here, we describe PCA in general terms to present four principal components identified in our analysis.
We focus on features associated with the components, and on the six MASC document clusters.
PCA starts with a covariance matrix of all features: a square matrix where each cell value is the
covariance of feature x
i
with feature x
j
for i, j ? M . Covariance of x
i
, x
j
is analogous to variance: for
all datapoints n ? [1 : N ], you subtract x
i
n
from x
i
, x
j
n
from x
j
, sum the products of these differences,
and normalize by n-1.
7
A common explanatory visualization will show a scatterplot of hypothetical
data values in a sausage shape at a diagonal to the x-axis. A line along the maximum width of the
sausage represents the dimension of greatest variation. A second axis can be placed orthogonal to this
first component; it will account for less of the variance in the data, and in a different direction. PCA
consists of computation of these axes (eigenvectors) from a covariance matrix.
5 PCA results
Figure 4a) shows a plot of our first principal component by the second component and the features
that contribute most to each, based on the features? loadings (weights) on the new components. The
components are rotated to become the new x,y axes and centered at zero. Projection of the individual
features onto the rotated axes shows which features contribute most directly to each dimension. Figure
5a) shows a similar plot for the third and fourth components. Twenty-seven features have loadings of at
least 0.2 on any component. Many have similar loadings (e.g., commas and prepositions on the fourth
component), indicating the data could be represented with fewer, uncorrelated features.
Past tense verbs, copula verbs, personal pronouns, and adverbs load heavily on one pole of the first
principal component, while characters per word, noun chunk length and nouns load higher on the op-
posite pole. This component corresponds rather well to Biber?s first component, which had similar
loadings for personal pronouns, adverbs, nouns and word length, and which he interpreted as involved
versus informational?i.e., interactive, unplanned, primarily spoken data vs. polished written documents
conveying (sometimes dense) information about a given topic.
7
See any text on covariance for an explanation of why n-1 is a better normalization term than n.
570
(a) First and second principal components
(b) Document regions for components one and two
Figure 4: First and Second Principal Components
(a) Third and fourth principal components
(b) Document regions for components three and four
Figure 5: Third and Fourth Principal Components
Our second principal component is defined almost entirely by the contrast between NEs and common
nouns. It corresponds to none of Biber?s components; he had no NE features. Our third component has
loadings from 3rd person present tense verbs (and other verb forms) at one end, and past tense verbs,
third person pronouns, and person NEs at the other. It corresponds to Biber?s second component, which
had similar loadings for past tense verbs and third person pronouns, and somewhat less for present tense
verbs. He interpreted this dimension as representing the variation from non-narrative to narrative.
Our fourth component corresponds to Biber?s fifth, which he characterized as abstract versus non-
abstract. At one extreme we have commas, prepositions, sentence length (in tokens) and past participles,
with base verbs loading to some degree on the other extreme. The features loaded on Biber?s fifth
component were conjuncts, which might correlate with longer sentence length, past participles, and
agentless passives. In the corresponding scatterplots (Figures 4b and 5b), each datapoint (document
chunk) has been color-coded according to the six clusters found in the preceding section. There are
clearly distinct regions along the first component for spoken interactions (black), story telling (red),
offline interaction (pink) and discursive (blue), but with a great deal of overlap. Travel guides (green) and
technical (gold) are at the blue extreme, but at different locations along the second dimension. Moving
from left to right in Figure 4b), each next color has greater dispersion along the second component,
apart from green and gold, which have clearly separate locations from each other, at the top and bottom,
571
Story telling Discursive Offline Interaction Spoken Interaction Travel Guide Technical
Story Telling 0.00 0.23 0.13 0.06 0.63 0.91
Discursive 0.23 0.00 0.21 0.24 0.15 0.35
Offline Interaction 0.13 0.21 0.00 0.07 0.57 1.07
Spoken Interaction 0.06 0.24 0.07 0.00 0.68 0.88
Travel Guide 0.63 0.15 0.57 0.68 0.00 0.78
Technical 0.91 0.35 1.07 0.88 0.78 0.00
Table 1: Mean Bhattacharyya Distance of all Genre Pairs using PCA Scores
respectively. In Figure 5b), the overall dispersion is more even across both dimensions, with separate
centers for each of the four major colors (black, pink, red and blue), but again without sharp separation.
6 Genre Distance Measurement
A metric that summarizes how separable a pair of genres are in the defined PCA space would be more
convenient than the visualizations in Figures 4b and 5b. Bhattacharyya distance, which measures the
similarity of two discrete or continuous probability distributions, has been used in image segmentation
and signal selection, to minimize the probability of misclustering for segmentations (Coleman and An-
drews, 1979), or the probability of misclassifying different signals (Kailath, 1967). Here we illustrate its
use in summarizing the separability of a pair of genres across the four principal components.
In statistics, the Bhattacharyya distance measures the similarity of two discrete or continuous proba-
bility distributions. It is closely related to the Bhattacharyya coefficient, which measures the amount of
overlap between two statistical samples or populations.
The Bhattacharyya coefficient for two continuous probability distributions p(x) and q(x) is:
Bhattacharyya coefficient = ? =
?
C
?
q(x)p(x)dx
Where C is the domain of probability density p(x) and q(x). The Bhattacharyya coefficient takes on
values in [0,1]. Bhattacharyya distance maps the Bhattacharyya coefficient to [0,?]:
Bhattacharyya distance = B = ? ln ?
We take the mean Bhattacharyya Distance of a pair of genres across all four components as a summary
measure of seprability. As an illustration, consider the two clusters of offline interaction (pink) and
discursive text (blue) from Figures 4b) and 5b). Their Bhattacharyya Distances on the first through
fourth components, using the PCA scores, are: 0.05, 0.01, 0.14, 0.63. They have the largest distance on
the fourth component, the axis of abstract vs non-abstract, which is consistent with the visualizations.
The summary statistic is then the mean of the four individual distances: 0.21.
Table 1 gives the mean Bhattacharyya Distance of each pair of genres for the four components. The
pair of genres that is the closest on all four components is story telling and spoken interaction (0.06;
underlined). The pair that is the most distant on all four components is technical and offline interaction
(1.07; in bold). Bhattacharyya Distance can also be computed for each pair of genres using the original
normalized feature values. In three cases the Bhattacharyya Distance in the PCA space is the same as in
the original feature space, but in all other cases the Bhattacharyya Distance is much greater.
7 Discussion
Strong patterns of similarity in dimensions of variation across many genres of English emerge from our
comparison with Biber?s study, despite differences in the features used, the contrast between American
and British English, and the use of new media types. The results support the view that relatively stable
dimensions of variation arise from properties of the situations of use across varieties of English. This
applies as well to genres that did not exist in Biber?s time (email, twitter, spam), which group with the
interactive genre included in Biber?s corpus (letters) and are similar to other offline discourse despite
representing an interactive form?albeit an ?offline interactive? form?of discourse.
572
A significant departure from Biber?s results concerns the component defined primarily by Named Enti-
ties (NEs), which emerges as the second strongest dimension of variation in our study. This demonstrates
that additional features?in particular, features beyond those based on orthographic and morpho-syntactic
properties that have figured in most genre studies to date?can dramatically impact Biber?s original model
and extend the range of properties that can characterize particular text types. It also suggests that higher-
level linguistic properties and other more complex features can contribute substantially to genre charac-
terization and discrimination, a topic we plan to pursue in the future.
In what follows, we discuss similarities and differences in the two PCA analyses, the conclusions
this leads to regarding the feasibility of genre classification, and ways in which the analysis can support
retrieval, language modeling, and domain adaptation.
Our first principal component is very similar to Biber?s first factor, which he interpreted as differen-
tiating situations of use with more of an informational focus from those with an interactive or affective
function. In addition, he noted a contrast between online and offline production?i.e., spoken vs. written
production modes. The heavily loaded features the two analyses have in common are consistent with
the interpretation: 1st/2nd person pronouns, many verb features, and adverbs are at one pole, with word
length and nouns at the other. He claimed that this distinction is obviously a very powerful factor . . . not
an artifact of the factor extraction technique, meaning that it arises from differences between the de-
mands of face-to-face, online interaction and those of offline, expository discourse. Having found a very
similar dimension using different (correlated) features, we agree with this claim. Figure 4b) shows that
the spoken interaction documents in MASC fall on the ?involved? side of this dimension, while expository
texts fall on the ?informational? side.
Interestingly, the genres that did not exist in Biber?s time (email, twitter, spam) group with the inter-
active genre included in Biber?s corpus (letters), and they are similar to other offline discourse despite
representing an interactive form?albeit an ?offline interactive? form?of discourse. This provides a strong
argument for the validity of the first component and its link to underlying situational factors of language
use. In Figure 4b), the hypothetical centroid of the pink (offline interactive) region seems somewhat less
to the right on the x-axis than a corresponding centroid for the blue (expository) set, but the pink and
blue are relatively co-extensive, and in particular, are clearly separated from both the black (face-to-face
online interaction) and red (storytelling) genres. This makes intuitive sense, as storytelling genres often
depict face-to-face interaction (?so the elephant says to the camel?), and therefore mimic its immediacy.
Our second principal component is defined primarily by Named Entities (NEs), which has no correlate
in Biber?s study; his features included proper nouns but not NEs. Person NEs load with past tense
verbs and third person pronouns on our third component, which resembles Biber?s narrative dimension.
Most of the MASC genres seem to be dispersed all along our second dimension, suggesting that NE
frequency varies across texts in these genres; the exception is travel guides, which consistently include
larger numbers of NEs. The explanation here is less on production constraints than on function, as travel
guides survey geographical points of interest, historical monuments and persons, hotels and restaurants,
and so on.
As noted in Section 5, our third component is very similar to Biber?s second (narrative versus non-
narrative), and our fourth is somewhat similar to Biber?s fifth (abstract versus non-abstract). Note that
the fourth dimension shows a greater separation of expository (blue) and offline-interactive (pink) gen-
res, which substantially overlap on the first dimension. This provides a good example of how the 4-
dimensional visualization provided by the scatterplots reveals potentially very different relations among
genres across the components, which in turn explains why fixed definitions of genre are difficult, if not
impossible, and why genre classification can be hard to achieve. We observe that the genre classes can be
more or less separable on one dimension but not another. As another example, travel guides and technical
documents are at distinct locations on the second component, but span the same locations on the first.
This lack of separability on one or more dimensions is true for nearly all pairs of our six genre classes,
as well as for any pair of dimensions. This suggests that an application that requires genre classification
could use PCA to find dimensions of variation that lead to the best separation, and summarize the sepa-
rability using the mean Bhattacharyya distance. As the number of genres one needs to classify increases,
573
it could be that the number of orthogonal dimensions required to lead to the best separation might also
increase. In Table 1, for example, with the exception of the row for Discursive Text, all rows have at least
one cell with a value close to or above 0.80, indicating that each of the six genres can be clearly sepa-
rated from at least one other genre. We would predict that Discursive Text would be the most difficult to
classify using genre features alone.
The strong similarities among the major components in Biber?s study and ours support the view that
genre variation is continuous along multiple dimensions due to contextual properties such as cognitive
constraints, interactivity, and function. As such, we view the dimensions as arising from observable
properties of discourse situations. Given a new genre, it should be possible to predict where it would be
located in the PCA space defined here. We would predict that chats, for example, would pattern more
closely with face-to-face interaction than with offline interactive genres. The same methodology could be
applied to a sub-genre, such as the discursive texts, to discover more specific dimensions to differentiate
among them.
Because language use changes over time, and new genres arise, we do not view the 4-dimensions
as a definitive representation of genre space. We do, however, envision a concrete application of this
particular representation, namely to measure corpus similarity in a multivariate fashion. Because our
PCA analysis makes it possible to locate new documents in the defined space, it would be possible
to identify which MASC documents a new set of documents is most similar to. PCA scores could be
computed on the four dimensions for corresponding features in the new documents. This approach could
be used in any application where it is desirable to find similar documents, such as retrieval, language
modeling, or domain adaptation. For example, in recent work on domain adaptation of parsers, McClosky
et al. (2010) present a confusion matrix with six corpora to demonstrate how performance of a Charniak
parser (Charniak, 2000) varies depending on which corpus it is trained on. They assume that a new
target domain will be a mixture of their six source domains and build a simple regression (three features)
to predict which of the six parsers will perform best on a new corpus. They subsequently state that
an alternative approach could use a high-dimensional vector space to compare corpora. Inspired by
this suggestion, we are currently developing a web service that will allow researchers to locate their
corpora in the 4-dimensional space identified in this study, and to compute the values of their PCA
scores. This would make it possible to use Bhattacharrya distance as described in Section 6 to measure
the similarity of corpora in genre space, which could be quite relevant for adapting parsers or other
NLP tools. This contrasts with the similarity measures used in Ravi and Knight (Ravi et al., 2008) and
McClosky (McClosky et al., 2010), which are based on lexical features.
8 Conclusion
Using a relatively small set of under three dozen features to represent the linguistic forms in discourse,
PCA reveals four principal components of variation in a very heterogeneous corpus of post 1990s Amer-
ican English that are comparable to those identified in Biber?s work, as well as additional dimensions
based on features not included in that earlier study. Six genres derived from the MASC corpus using
hierarchical clustering are separable on some but not all components. These differences in separabil-
ity potentially explain the variations in performance across different works that do genre classification.
The resulting 4-dimensional genre space provides a basis for more detailed analysis of sub-genres, for
a better understanding of the relation between genre and situations of language use, and for predicting
the distributional properties of new genres. In future work, we plan to build on this basis to develop an
increasingly detailed and, at the same time, generalizable characterization of genre.
Our results depict a big picture for how discourse in English varies with respect to style or form,
and how different genres are conditioned by aspects of the situations of language use. We believe that
exploration of genre in these terms can provide a more viable approach to measuring distinctions among
texts than the approach used in most recent work, and can provide a more informed basis to incorporate
genre distinctions in information retrieval, language modeling, and domain adaptation for statistical NLP.
574
Acknowledgements
This work was supported in part by NSF CRI-1059312.
References
Douglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press, Cambridge, UK.
Douglas Biber. 1993. The multi-dimensional approach to linguistic analyses of genre variation: An overview of
methodology and findings. Computers and the Humanities, 26:331?345.
Ivan Bulyko and Mari Ostendorf. 2003. Getting more mileage from web text sources for conversational speech
language modeling using class-dependent mixtures. In Proc. HLT-NAACL 2003, pages 7?9.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American Chapter
of the Association for Computational Linguistics Conference, NAACL 2000, pages 132?139, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guy Barrett Coleman and Harry C Andrews. 1979. Image segmentation by clustering. Proceedings of the IEEE,
67(5):773?785.
Sergey Feldman, Marius Marin, Julie Medero, and Mari Ostendorf. 2009. Classifying factored genres with part-
of-speech histograms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,
pages 173?176, Boulder, Colorado, June. Association for Computational Linguistics.
Stefan Th. Gries, John Newman, Cyrus Shaoul, and Philip Dilts. 2009. N-grams and the clustering of genres.
Paper presented at the workshop on Corpus, Colligation, Register Variation at the 31st Annual Meeting of the
Deutsche Gesellschaft fr Sprachwissenschaft.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Rebecca Passonneau. 2010. The Manually Annotated Sub-
Corpus: A Community Resource for and by the People. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July. Association for Computational Linguistics.
Rukmini Iyer and Mari Ostendorf. 1999. Relevance weighting for combining multi-domain data for n-gram
language modeling. Computer Speech & Language, 13(3):267?282.
Thomas Kailath. 1967. The divergence and Bhattacharyya distance measures in signal selection. Communication
Technology, IEEE Transactions on, 15(1):52?60.
Ioannis Kanaris and Efstathios Stamatatos. 2009. Learning to recognize webpage genres. Information Processing
and Management, 45(5):499?512, September.
Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant
analysis. In Proceedings of the 15th Conference on Computational Linguistics - Volume 2, COLING ?94, pages
1071?1075, Stroudsburg, PA, USA. Association for Computational Linguistics.
Brett Kessler, Geoffrey Nunberg, and Hinrich Sch?utze. 1997. Automatic detection of text genre. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the
European Chapter of the Association for Computational Linguistics, ACL ?98, pages 32?38, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):1?37.
Teuvo Kohonen. 1995. Self-organizing Maps. Springer-Verlag, Berlin.
Yong-Bae Lee and Sung Hyon Myaeng. 2002. Text genre classification with genre-revealing and subject-revealing
features. In SIGIR ?02: Proceedings of the 25th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 145?150, New York, NY, USA. ACM Press.
David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proceedings
of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large
Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics
- Volume 13, EMNLP ?00, pages 207?215, Stroudsburg, PA, USA. Association for Computational Linguistics.
575
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 28?36, Stroudsburg, PA, USA. Association for Computational
Linguistics.
A. Mehler, S. Sharoff, and M. Santini. 2010. Genres on the Web: Computational Models and Empirical Studies.
Text, Speech and Language Technology. Springer.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.
Andreas Rauber and Alexander M?uller-K?ogler. 2001. Integrating automatic genre analysis into digital libraries.
In First ACM-IEEE Joint Conference on Digital Libraries, pages 1?10.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887?896, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad, Zadeh Kaljahi, and Anton Bryl. 2012. DUC-
Paris13 systems for the SANCL 2012 shared task.
Marina Santini. 2004. A shallow approach to syntactic feature extraction for genre classification. Technical Report
ITRI-04-02, Information Technology Research Institute, University of Brighton. Also published in Proceedings
of the 7th Annual Colloquium for the UK Special Interest Group for Computational Linguistics, Birmingham,
UK.
Ruhi Sarikaya, Agust??n Gravano, and Yuqing Gao. 2005. Rapid language model development using external re-
sources for new spoken dialog domains. In International Congress of Acoustics, Speech, and Signal Processing
(ICASSP), pages 573?576, Philadelphia, PA, USA. IEEE, Signal Processing Society.
Serge Sharoff, Zhili Wu, and Katja Markert. 2010. The web library of Babel: evaluating genre collections. In
Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC?10), Valletta, Malta, may. European Language Resources Associa-
tion (ELRA).
Efstathios Stamatatos, Nikos Fakotakis, and George Kokkinakis. 2000a. Text genre detection using common word
frequencies. In Proceedings of the 18th Conference on Computational Linguistics - Volume 2, COLING ?00,
pages 808?814, Stroudsburg, PA, USA. Association for Computational Linguistics.
Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis. 2000b. Automatic text categorization in terms of
genre and author. Computational Linguistics, 26(4):471?495, December.
Wei Wang, Klaus Macherey, Wolfgang Macherey, Franz Och, and Peng Xu. 2012. Improved domain adaptation
for statistical machine translation. In AMTA-2012.
Nigel G. Ward and Steven D. Werner. 2013. Using dialog-activity similarity for spoken information retrieval. In
Fr?ed?eric Bimbot, Christophe Cerisara, C?ecile Fougeron, Guillaume Gravier, Lori Lamel, Franc?ois Pellegrino,
and Pascal Perrier, editors, 14th Annual Conference of the International Speech Communication Association,
Interspeech, pages 1569?1573. ISCA.
576
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 840?848,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning about Voice Search for Spoken Dialogue Systems 
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio2,  
Joshua B. Gordon4, Pravin Bhutada4 
1Center for Computational Learning Systems, Columbia University 
2Department of Computer Science, Hunter College of The City University of New York 
3Department of Computer Science, The Graduate Center of The City University of New York 
4Department of Computer Science, Columbia University 
becky@cs.columbia.edu, susan.epstein@hunter.cuny.edu, tligorio@gc.cuny.edu, 
joshua@cs.columbia.edu, pravin.bhutada@gmail.com 
Abstract 
In a Wizard-of-Oz experiment with multiple 
wizard subjects, each wizard viewed automated 
speech recognition (ASR) results for utterances 
whose interpretation is critical to task success: 
requests for books by title from a library data-
base. To avoid non-understandings, the wizard 
directly queried the application database with 
the ASR hypothesis (voice search). To learn 
how to avoid misunderstandings, we investi-
gated how wizards dealt with uncertainty in 
voice search results. Wizards were quite suc-
cessful at selecting the correct title from query 
results that included a match. The most suc-
cessful wizard could also tell when the query 
results did not contain the requested title. Our 
learned models of the best wizard?s behavior 
combine features available to wizards with 
some that are not, such as recognition confi-
dence and acoustic model scores.  
1 Introduction 
Wizard-of-Oz (WOz) studies have long been used 
for spoken dialogue system design. In a relatively 
new variant, a subject (the wizard) is presented 
with real or simulated automated speech recogni-
tion (ASR) to observe how people deal with incor-
rect speech recognition output (Rieser, Kruijff-
Korbayov?, & Lemon, 2005; Skantze, 2003; 
Stuttle, Williams, & Young, 2004; Williams & 
Young, 2003, 2004; Zollo, 1999). In these experi-
ments, when a wizard could not interpret the ASR 
output (non-understanding), she rarely asked users 
to repeat themselves. Instead, the wizard found 
other ways to continue the task.  
This paper describes an experiment that pre-
sented wizards with ASR results for utterances 
whose interpretation is critical to task success: re-
quests for books from a library database, identified 
by title. To avoid non-understandings, wizards 
used voice search (Wang et al, 2008): they direct-
ly queried the application database with ASR out-
put. To investigate how to avoid errors in 
understanding (misunderstandings), we examined 
how wizards dealt with uncertainty in voice search 
results. When the voice search results included the 
requested title, all seven of our wizards were likely 
to identify it. One wizard, however, recognized far 
better than the others when the voice search results 
did not contain the requested title. The experiment 
employed a novel design that made it possible to 
include system features in models of wizard beha-
vior. The principal result is that our learned models 
of the best wizard?s behavior combine features that 
are available to wizards with some that are not, 
such as recognition confidence and acoustic model 
scores. 
The next section of the paper motivates our ex-
periment. Subsequent sections describe related 
work, the dialogue system and embedded wizard 
infrastructure, experimental design, learning me-
thods, and results. We then discuss how to general-
ize from the results of our study for spoken 
dialogue system design. We conclude with a sum-
mary of results and their implications. 
2 Motivation 
Rather than investigate full dialogues, we ad-
dressed a single type of turn exchange or adjacency 
pair (Sacks et al, 1974): a request for a book by its 
840
title. This allowed us to collect data exclusively 
about an utterance type critical for task success in 
our application domain. We hypothesized that low-
level features from speech recognition, such as 
acoustic model fit, could independently affect 
voice search confidence. We therefore applied a 
novel approach, embedded WOz, in which a wizard 
and the system together interpret noisy ASR. 
To address how to avoid misunderstandings, we 
investigated how wizards dealt with uncertainty in 
voice search returns. To illustrate what we mean 
by uncertainty, if we query our book title database 
with the ASR hypothesis: 
ROLL DWELL 
our voice search procedure returns, in this order: 
CROMWELL 
ROBERT LOWELL 
ROAD TO WEALTH 
The correct title appears last because of the score it 
is assigned by the string similarity metric we use.  
Three factors motivated our use of voice search 
to interpret book title requests: noisy ASR, un-
usually long query targets, and high overlap of the 
vocabulary across different query types (e.g., au-
thor and title) as well as with non-query words in 
caller utterances (e.g., ?Could you look up . . .?).  
First, accurate speech recognition for a real-
world telephone application can be difficult to 
achieve, given unpredictable background noise and 
transmission quality. For example, the 68% word 
error rate (WER) for the fielded version of Let?s 
Go Public! (Raux et al, 2005) far exceeded its 
17% WER under controlled conditions. Our appli-
cation handles library requests by telephone, and 
would benefit from robustness to noisy ASR. 
Second, the book title field in our database dif-
fers from the typical case for spoken dialogue sys-
tems that access a relational database. Such 
systems include travel booking (Levin et al, 2000), 
bus route information (Raux et al, 2006), restau-
rant guides (Johnston et al, 2002; Komatani et al, 
2005), weather (Zue et al, 2000) and directory 
services (Georgila et al, 2003). In general for these 
systems, a few words are sufficient to retrieve the 
desired attribute value, such as a neighborhood, a 
street, or a surname. Mean utterance length in a 
sample of 40,000 Let?s Go Public! utterances, for 
example, is 2.4 words. The average book title 
length in our database is 5.4 words. 
Finally, our dialogue system, CheckItOut, al-
lows users to choose whether to request books by 
title, author, or catalogue number. The database 
represents 5028 active patrons (with real borrow-
ing histories and preferences but fictitious personal 
information), 71,166 book titles and 28,031 au-
thors. Though much smaller than a database for a 
directory service application (Georgila et al, 
2003), this is much larger than that of many current 
research systems. For example, Let?s Go Public! 
accesses a database with 70 bus routes and 1300 
place names. Titles and author names contribute 
50,394 words to the vocabulary, of which 57.4% 
occur only in titles, 32.1% only in author names, 
and 10.5% in both. Many book titles (e.g., You See 
I Haven?t Forgotten, You Never Know) have a high 
potential for confusability with non-title phrases in 
users? book requests. Given the longer database 
field and the confusability of the book title lan-
guage, integrating voice search is likely to have a 
relatively larger impact in CheckItOut.  
We seek to minimize non-understandings and 
misunderstandings for several reasons. First, user 
corrections in both situations have been shown to 
be more poorly recognized than non-correction ut-
terances (Litman et al, 2006). Non-understandings 
typically result in re-prompting the user for the 
same information. This often leads to hyper-
articulation and concomitant degradation in recog-
nition performance. Second, users seem to prefer 
systems that minimize non-understandings and mi-
sunderstandings, even at the expense of dialogue 
efficiency. Users of the TOOT train information 
spoken dialogue system preferred system-initiative 
to mixed- or user-initiative, and preferred explicit 
confirmation to implicit or no confirmation 
(Litman & Pan, 1999). This was true despite the 
fact that a mixed-initiative, implicit confirmation 
strategy led to fewer turns for the same task. Most 
of the more recent work on spoken dialogue sys-
tems focuses on mixed-initiative systems in labora-
tory settings. Still, recent work suggests that while 
mixed- or user-initiative is rated highly in usability 
studies, under real usage it ?fails to provide [a] ro-
bust enough interface? (Turunen et al, 2006). In-
corporating accurate voice search into spoken 
dialogue systems could lead to fewer non-
understandings and fewer misunderstandings. 
3 Related Work 
Our approach to noisy ASR contrasts with many 
other information-seeking and transaction-based 
dialogue systems. Those systems typically perform 
841
natural language understanding on ASR output be-
fore database query with techniques that try to im-
prove or expand ASR output. None that we know 
of use voice search. For one directory service ap-
plication, users spell the first three letters of sur-
names, and then ASR results are expanded using 
frequently confused phones (Georgila et al, 2003). 
A two-pass recognition architecture added to Let?s 
Go Public! improved concept recognition in post-
confirmation user utterances (Stoyanchev & Stent, 
2009). In (Komatani et al, 2005), a shallow se-
mantic interpretation phase was followed by deci-
sion trees to classify utterances as relevant either to 
query type or to specific query slots, to narrow the 
set of possible interpretations. CheckItOut is most 
similar in spirit to the latter approach, but relies on 
the database earlier, and only for semantic interpre-
tation, not to also guide the dialogue strategy. 
Our approach to noisy ASR is inspired by pre-
vious WOz studies with real (Skantze, 2003; Zollo, 
1999) or simulated ASR (Kruijff-Korbayov? et al, 
2005; Rieser et al, 2005; Williams & Young, 
2004). Simulation makes it possible to collect di-
alogues without building a speech recognizer, and 
to control for WER. In the studies that involved 
task-oriented dialogues, wizards typically focused 
more on the task and less on resolving ASR errors 
(Williams & Young, 2004; Skantze, 2003; Zollo, 
1999). In studies more like the information-seeking 
dialogues addressed here, an entirely different pat-
tern is observed (Kruijff-Korbayov? et al, 2005; 
Rieser et al, 2005). 
Zollo collected seven dialogues with different 
human-wizard pairs to develop an evacuation plan. 
The overall WER was 30%. Of the 227 cases of 
incorrect ASR, wizard utterances indicated a fail-
ure to understand for only 35% of them. Wizards 
ignored words not salient in the domain and hy-
pothesized words based on phonetic similarity. In 
(Skantze, 2003), both users and wizards knew 
there was no dialogue system; 44 direction-finding 
dialogues were collected with 16 subjects. Despite 
a WER of 43%, the wizard operators signaled mis- 
understanding only 5% of the time, in part because 
they often ignored ASR errors and continued the 
dialogue. For the 20% of non-understandings, op-
erators continued a route description, asked a task-
related question, or requested a clarification.  
Williams and Young collected 144 dialogues 
simulating tourist requests for directions and other 
negotiations. WER was constrained to be high, 
medium, or low. Under medium WER, a task-
related question in response to a non-understanding 
or misunderstanding led to full understanding more 
often than explicit repairs. Under high WER, how-
ever, the reverse was true. Misunderstandings sig-
nificantly increased when wizards followed non-
understandings or misunderstandings with a task-
related question instead of a repair. 
In (Rieser et al, 2005), wizards simulated a 
multimodal MP3 player application with access to 
a database of 150K music albums. Responses 
could be presented verbally or graphically. In the 
noisy transcription condition, wizards made clarifi-
cation requests about twice as often as that found 
in similar human-human dialogue.  
In a system like CheckItOut, user utterances that 
request database information must be understood. 
We seek an approach that would reduce the rate of 
misunderstandings observed for high WER in 
(Williams & Young, 2004) and the rate of clarifi-
cation requests observed in (Rieser et al, 2005). 
4 CheckItOut and Embedded Wizards 
CheckItOut is modeled on library transactions at 
the Andrew Heiskell Braille and Talking Book Li-
brary, a branch of the New York Public Library 
and part of the National Library of Congress. Bor-
rowing requests are handled by telephone. Books, 
mainly in a proprietary audio format, travel by 
mail. In a dialogue with CheckItOut, a user identi-
fies herself, requests books, and is told which are 
available for immediate shipment or will go on re-
serve. The user can request a book by catalogue 
number, title, or author. 
CheckItOut builds on the Olympus/RavenClaw 
framework (Bohus & Rudnicky, 2009) that has 
been the basis for about a dozen dialogue systems 
in different domains, including Let?s Go Public! 
(Raux et al, 2005). Speech recognition relies on 
PocketSphinx. Phoenix, a robust context-free 
grammar (CFG) semantic parser, handles natural 
language understanding (Ward & Issar, 1994). The 
Apollo interaction manager (Raux & Eskenazi, 
2007) detects utterance boundaries using informa-
tion from speech recognition, semantic parsing, 
and Helios, an utterance-level confidence annotator 
(Bohus & Rudnicky, 2002). The dialogue manager 
is implemented in RavenClaw. 
842
To design CheckItOut?s dialogue manager, we 
recorded 175 calls (4.5 hours) from patrons to li-
brarians. We identified 82 book request calls, tran-
scribed them, aligned the utterances with the 
speech signal, and annotated the transcripts for di-
alogue acts. Because active patrons receive 
monthly newsletters listing new titles in the desired 
formats, patrons request specific items with ad-
vance knowledge of the author, title, or catalogue 
number. Most book title requests accurately repro-
duce the exact title, the title less an initial deter-
miner (?the,? ?a?), or a subtitle.  
We exploited the Galaxy message passing archi-
tecture of Olympus/RavenClaw to insert a wizard 
server into CheckItOut. The hub passes messages 
between the system and a wizard?s graphical user 
interface (GUI), allowing us to collect runtime in-
formation that can be included in models of wi-
zards? actions.  
For speech recognition, CheckItOut relies on 
PocketSphinx 0.5, a Hidden Markov Model-based 
recognizer. Speech recognition for this experiment, 
relied on the freely available Wall Street Journal 
?read speech? acoustic models. We did not adapt 
the models to our population or to spontaneous 
speech, thus insuring that wizards would receive 
relatively noisy recognition output.  
We built trigram language models from the 
book titles using the CMU Statistical Language 
Modeling Toolkit. Pilot tests with one male and 
one female native speaker indicated that a lan-
guage model based on 7500 titles would yield 
WER in the desired range. (Average WER for the 
book title requests in our experiment was 71%.) To 
model one aspect of the real world useful for an ac-
tual system, titles with below average circulation 
were eliminated. An offline pilot study had demon-
strated that one-word titles were easy for wizards, 
so we eliminated those as well. A random sample 
of 7,500 was chosen from the remaining 19,708 
titles to build the trigram language model. 
We used Ratcliff/Obersherhelp (R/O) to meas-
ure the similarity of an ASR string to book titles in 
the database (Ratcliff & Metzener, 1988). R/O cal-
culates the ratio r of the number of matching cha-
racters to the total length of both strings, but 
requires O(r2) time on average and O(r3) time in 
the worst case. We therefore computed an upper 
bound on the similarity of a title/ASR pair prior to 
full R/O to speed processing.  
5 Experimental Design 
In this experiment, a user and a wizard sat in sepa-
rate rooms where they could not overhear one 
another. Each had a headset with microphone and a 
GUI. Audio input on the wizard?s headset was dis-
abled. When the user requested a title, the ASR 
hypothesis for the title appeared on the wizard?s 
GUI. The wizard then selected the ASR hypothesis 
to execute a voice search against the database.  
Given the ASR and the query return, the wi-
zard?s task was to guess which candidate in the 
query return, if any, matched the ASR hypothesis. 
Voice search accessed the full backend of 71,166 
titles. The custom query designed for the experi-
ment produced four types of return, in real time, 
based on R/O scores: 
? Singleton: a single best candidate (R/O ? 0.85) 
? AmbiguousList: two to five moderately good 
candidates (0.85 > R/O ? 0.55) 
? NoisyList: six to ten poor but non-random can-
didates (0.55 > R/O ? 0.40) 
? Empty: No candidate titles (max R/O < 0.40) 
In pilot tests, 5%-10% of returns were empty ver-
sus none in the experiment. The distribution of 
other returns was: 46.7% Singleton, 50.5% Ambi-
guousList, and 2.8% NoisyList. 
Seven undergraduate computer science majors 
at Hunter College participated. Two were non-
native speakers of English (one Spanish, one Ro-
manian). Each of the possible 21 pairs of students 
met for five trials. During each trial, one student 
served as wizard and the other as user for a session 
of 20 title cycles. They immediately reversed roles 
for a second session, as discussed further below. 
The experiment yielded 4172 title cycles rather 
than the full 4200, because users were permitted to 
end sessions early. All titles were selected from the 
7500 used to construct the language model.  
Each user received a printed list of 20 titles and 
a brief synopsis of each book. The acoustic quality 
of titles read individually from a list is unlikely to 
approximate that of a patron asking for a specific 
title. Therefore, immediately before each session, 
the user was asked to read a synopsis of each book, 
and to reorder the titles to reflect some logical 
grouping, such as genre or topic. Users requested 
titles in this new order that they had created.  
Participants were encouraged to maximize a ses-
sion score, with a reward for the experiment win-
ner. Scoring was designed to foster cooperative 
843
strategies. The wizard scored +1 for a correctly 
identified title, +0.5 for a thoughtful question, and 
-1 for an incorrect title. The user scored +0.5 for a 
successfully recognized title. User and wizard 
traded roles for the second session, to discourage 
participants from sabotaging the others? scores.  
The wizard?s GUI presented a real-time live 
feed of ASR hypotheses, weighted by grayscale to 
reflect acoustic confidence. Words in each candi-
date title that matched a word in the ASR appeared 
darker: dark black for Singleton or AmbiguousList, 
and medium black for NoisyList. All other words 
were in grayscale in proportion to the degree of 
character overlap. The wizard queried the database 
with a recognition hypothesis for one utterance at a 
time, but could concatenate successive utterances, 
possibly with some limited editing.  
After a query, the wizard?s GUI displayed can-
didate matches in descending order of R/O score. 
The wizard had four options: make a firm choice of 
a candidate, make a tentative choice, ask a ques-
tion, or give up to end the title cycle. Questions 
were recorded. The wizard?s GUI showed the suc-
cess or failure of each title cycle before the next 
one began. The user?s GUI posted the 20 titles to 
be read during the session. On the GUI, the user 
rated the wizard?s title choices as correct or incor-
rect. Titles were highlighted green if the user 
judged a wizard?s offered title correct, red if incor-
rect, yellow if in progress, and not highlighted if 
still pending. The user also rated the wizard?s 
questions. Average elapsed time for each 20-title 
session was 15.5 minutes. 
A questionnaire similar to the type used in 
PARADISE evaluations (Walker et al, 1998) was 
administered to wizards and users for each pair of 
sessions. On a 5-point Likert scale, the average re-
sponse to the question ?I found the system easy to 
use this time? was 4 (sd=0; 4=Agree), indicating 
that participants were comfortable with the task. 
All other questions received an average score of 
Neutral (3) or Disagree (2). For example, partici-
pants were neutral (3) regarding confidence in 
guessing the correct title, and disagreed (2) that 
they became more confident as time went on. 
6 Learning Method and Goals 
To model wizard actions, we assembled 60 fea-
tures that would be available at run time. Part of 
our task was to detect their relative independence, 
meaningfulness, and predictive ability. Features 
described the wizard?s GUI, the current title ses-
sion, similarity between ASR and candidates, ASR 
relevance to the database, and recognition and con-
fidence measures. Because the number of voice 
search returns varied from one title to the next, fea-
tures pertaining to candidates were averaged.  
We used three machine-learning techniques to 
predict wizards? actions: decision trees, linear re-
gression, and logistic regression. All models were 
produced with the Weka data mining package, us-
ing 10-fold cross-validation (Witten & Frank, 
2005). A decision tree is a predictive model that 
maps feature values to a target value. One applies a 
decision tree by tracing a path from the root (the 
top node) to a leaf, which provides the target value. 
Here the leaves are the wizard actions: firm choice, 
tentative choice, question, or give up. The algo-
rithm used is a version of C4.5 (Quinlan, 1993), 
where gain ratio is the splitting criterion. 
To confirm the learnability and quality of the 
decision tree models, we also trained logistic re-
gression and linear regression models on the same 
data, normalized in [0, 1]. The logistic regression 
model predicts the probability of wizards? actions 
by fitting the data to a logistic curve. It generalizes 
the linear model to the prediction of categorical da-
ta; here, categories correspond to wizards? actions. 
The linear regression models represent wizards? 
actions numerically, in decreasing value: firm 
choice, tentative choice, question, give up.  
Although analysis of individual wizards has not 
been systematic in other work, we consider the 
variation in human performance significant. Be-
cause we seek excellent, not average, teachers for 
CheckItOut, our focus is on understanding good 
wizardry. Therefore, we learned two kinds of mod-
els with each of the three methods: the overall 
model using data from all of our wizards, and indi-
vidual wizard models.  
Preliminary cross-correlation confirmed that 
many of the 60 features were heavily interdepen-
dent. Through an initial manual curation phase, we 
isolated groups of features with R2 > 0.5. When 
these groups referenced semantically similar fea-
tures, we selected a single representative from the 
group and retained only that one. For example, the 
features that described similarity between hypo-
theses and candidates were highly correlated, so 
we chose the most comprehensive one: the number 
of exact word matches. We also grouped together 
844
and represented by a single feature: three features 
that described the gaps between exact word 
matches, three that described the data presented to 
the wizard, nine that described various system con-
fidence scores, and three that described the user?s 
speaking rate. This left 28 features.  
Next we ran CfsSubsetEval, a supervised 
attribute selection algorithm for each model 
(Witten & Frank, 2005). This greedy, hill-climbing 
algorithm with backtracking evaluates a subset of 
attributes by the predictive ability of each feature 
and the degree of redundancy among them. This 
process further reduced the 28 features to 8-12 fea-
tures per model. Finally, to reduce overfitting for 
decision trees, we used pruning and subtree rising. 
For linear regression we used the M5 method, re-
peatedly removing the attribute with the smallest 
standardized coefficient until there was no further 
improvement in the error estimate given by the 
Akaike information criterion. 
7 Results 
Table 1 shows the number of title cycles per wi-
zard, the raw session score according to the formu-
la given to the wizards, and accuracy. Accuracy is 
the proportion of title cycles where the wizard 
found the correct title, or correctly guessed that the 
correct title was not present (asked a question or 
gave up). Note that score and accuracy are highly 
correlated (R=0.91, p=0.0041), indicating that the 
instructions to participants elicited behavior con-
sistent with what we wanted to measure. 
Wizards clearly differed in performance, large-
ly due to their response when the candidate list did 
not include the correct title. Analysis of variance 
with wizard as predictor and accuracy as the de-
pendent variable is highly significant (p=0.0006); 
significance is somewhat greater (p=0.0001) where 
session score is the dependent variable. Table 2 
shows the distribution of correct actions: to offer a 
candidate at a given position in the query return 
(Returns 1 through 9), or to ask a question or give 
up. As reflected in Table 2, a baseline accuracy of 
about 65% could be achieved by offering the first 
return. The fifth column of Table 1 shows how of-
ten wizards did that (Offered Return 1), and clearly 
illustrates that those who did so most often (W3 
and W6) had accuracy results closest to the base-
line. The wizard who did so least often (W4) had 
the highest accuracy, primarily because she more 
often correctly offered no title, as shown in the last 
column of Table 1. We conclude that a spoken di-
alogue system would do well to emulate W4. 
Overall, our results in modeling wizards? actions 
were uniform across the three learning methods, 
gauged by accuracy and F measure. For the com-
bined wizard data, logistic regression had an accu-
racy of 75.2%, and F measures of 0.83 for firm 
choices and 0.72 for tentative choices; the decision 
tree accuracy was 82.2%, and the F measures for 
firm versus tentative choices were respectively 
0.82 and 0.71. The decision tree had a root mean 
squared error of 0.306, linear regression 0.483. Ta-
ble 3 shows the accuracy and F measures on firm 
choices for the decision trees by individual wizard, 
along with the numbers of attributes and nodes per 
Table 1. Raw session score, accuracy, proportion of offered titles that were listed first in the query return, and 
frequency of correct non-offers for seven participants. 
 
Participant Cycles Session Score Accuracy Offered Return 1 Correct Non-Offers 
W4 600 0.7585 0.8550 0.70 0.64 
W5 600 0.7584 0.8133 0.76 0.43 
W7 599 0.6971 0.7346 0.76 0.14 
W1 593 0.6936 0.7319 0.79 0.16 
W2 599 0.6703 0.7212 0.74 0.10 
W3 581 0.6648 0.6954 0.81 0.20 
W6 600 0.6103 0.6950 0.86 0.03 
Table 2. Distribution of correct actions 
 
Correct Action N % 
Return 1 2722 65.2445 
Return 2 126 3.0201 
Return 3 56 1.3423 
Return 4 46 1.1026 
Return 5 26 0.6232 
Return 7 7 0.1678 
Return 8 1 0.0002 
Return 9 2 0.0005 
Question or Giveup 1186 28.4276 
Total 4172 1.0000 
845
tree. Although relatively few attributes appeared in 
any one tree, most attributes appeared in multiple 
nodes. W1 was the exception, with a very small 
pruned tree of 7 nodes. 
Accuracy of the decision trees does not correlate 
with wizard rank. In general, the decision trees 
could consistently predict a confident choice (0.80 
? F ? 0.87), but were less consistent on a tentative 
choice (0.60 ? F ? 0.89), and could predict a ques-
tion only for W4, the wizard with the highest accu-
racy and greatest success at detecting when the 
correct title was not in the candidates.  
What wizards saw on the GUI, their recent suc-
cess, and recognizer confidence scores were key 
attributes in the decision trees. The five features 
that appeared most often in the root and top-level 
nodes of all tree models reported in Table 3 were: 
? DisplayType of the return (Singleton, Ambi-
guous List, NoisyList) 
? RecentSuccess, how often the wizard chose the 
correct title within the last three title cycles 
? ContiguousWordMatch, the maximum number 
of contiguous exact word matches between a 
candidate and the ASR hypothesis (averaged 
across candidates) 
? NumberOfCandidates, how many titles were re-
turned by the voice search 
? Confidence, the Helios confidence score 
DisplayType, NumberOfCandidates and Conti-
guousWordMatch pertain to what the wizard could 
see on her GUI. (Recall that DisplayType is distin-
guished by font darkness, as well as by number of 
candidates.) The impact of RecentSuccess might 
result not just from the wizard?s confidence in her 
current strategy, but also from consistency in the 
user?s speech characteristics. The Helios confi-
dence annotation uses a learned model based on 
features from the recognizer, the parser, and the di-
alogue state. Here confidence primarily reflects 
recognition confidence; due to the simplicity of our 
grammar, parse results only indicate whether there 
is a parse. In addition to these five features, every 
tree relied on at least one measure of similarity be-
tween the hypothesis and the candidates.  
W4 achieved superior accuracy: she knew when 
to offer a title and when not to. In the learned tree 
for W4, if the DisplayType was NoisyList, W4 
asked a question; if DisplayType was Ambiguous-
List, the features used to predict W4?s action in-
cluded the five listed above, along with the acous-
tic model score, word length of the ASR, number 
of times the wizard had asked the user to repeat, 
and the maximum size of the gap between words in 
the candidates that matched the ASR hypothesis. 
To focus on W4?s questioning behavior, we 
trained an additional decision tree to learn how W4 
chose between two actions: offering a title versus 
asking a question. This 37-node, 8-attribute tree 
was based on 600 data points, with F=0.91 for 
making an offer and F=0.68 for asking a question. 
The tree is distinctive in that it splits at the root on 
the number of frames in the ASR. If the ASR is 
short (as measured both by the number of recogni-
tion frames and the words), W4 asks a question 
when DisplayType = AmbiguousList or NoisyList, 
either RecentSuccess ? 1 or ContiguousWord-
Match = 0, and the acoustic model score is low. 
Note that shorter titles are more confusable. If the 
ASR is long, W4 asks a question when Conti-
guousWordMatch ? 1, RecentSuccess ? 2, and ei-
ther CandidateDisplay = NoisyList, or Confidence 
is low, and there is a choice of titles. 
8 Discussion 
Our experiment addressed whether voice search 
can compensate for incorrect ASR hypotheses and 
permit identification of a user?s desired book, giv-
en a request by title. The results show that with 
high WER, a baseline dialogue strategy that always 
offers the highest-ranked database return can nev-
ertheless achieve moderate accuracy. This is true 
even with the relatively simplistic measure of simi-
larity between the ASR hypothesis and candidate 
titles used here. As a result, we have integrated 
voice search into CheckItOut, along with a linguis-
tically motivated grammar for book titles. Our cur-
rent Phoenix grammar relies on CFG rules 
automatically generated from dependency parses 
of the book titles, using the MICA parser 
Table 3. Learning results for wizards 
 
Tree Rank Nodes Attributes Accuracy F firm 
W4 1  55 12 75.67 0.85 
W5 2  21 10 76.17 0.85 
W1 3  7 8 80.44 0.87 
W7 4  45 11 73.62 0.83 
W3 5  33 10 77.42 0.84 
W2 6  35 10 78.49 0.85 
W6 7  23 10 85.19 0.80 
 
846
(Bangalore et al, 2009). As described in (Gordon 
& Passonneau, 2010), a book title parse can con-
tain multiple title slots that consume discontinuous 
sequences of words from the ASR hypothesis, thus 
accommodating noisy ASR. For the voice search 
phase, we now concatenate the words consumed by 
a sequence of title slots. We are also experimenting 
with a statistical machine learning approach that 
will replace or complement the semantic parsing. 
Computers clearly do some tasks faster and 
more accurately than people, including database 
search. To benefit from such strengths, a dialogue 
system should also accommodate human prefe-
rences in dialogue strategy. Previous work has 
shown that user satisfaction depends in part on task 
success, but also on minimizing behaviors that can 
increase task success but require the user to correct 
the system (Litman et al, 2006). 
The decision tree that models W4 has lower ac-
curacy than other models? (see Table 3), in part be-
cause her decisions had finer granularity. A spoken 
dialogue system could potentially do as well as or 
better than the best human at detecting when the 
title is not present, given the proper training data. 
To support this, a dataset could be created that was 
biased toward a larger proportion of cases where 
not offering a candidate is the correct action.  
9 Conclusion and Current Work 
This paper presents a novel methodology that em-
beds wizards in a spoken dialogue system, and col-
lects data for a single turn exchange. Our results 
illustrate the merits of ranking wizards, and learn-
ing from the best. Our wizards were uniformly 
good at choosing the correct title when it was 
present, but most were overly eager to identify a 
title when it was not among the candidates. In this 
respect, the best wizard (W4) achieved the highest 
accuracy because she demonstrated a much greater 
ability to know when not to offer a title. We have 
shown that it is feasible to replicate this ability in a 
model learned from features that include the pres-
entation of the search results (length of the candi-
date list, amount of word overlap of candidates 
with the ASR hypothesis), recent success at select-
ing the correct candidate, and measures pertaining 
to recognition results (confidence, acoustic model 
score, speaker rate). If replicated in a spoken di-
alogue system, such a model could support integra-
tion of voice search in a way that avoids 
misunderstandings. We conclude that learning 
from embedded wizards can exploit a wider range 
of relevant features, that dialogue managers can 
profit from access to more fine-grained representa-
tions of user utterances, and that machine learners 
should be selective about which people to model. 
That wizard actions can be modeled using sys-
tem features bodes well for future work. Our next 
experiment will collect full dialogues with embed-
ded wizards whose actions will again be restricted 
through an interface. This time, NLU will integrate 
voice search with the linguistically motivated CFG 
rules for book titles described earlier, and a larger 
language model and grammar for database entities. 
We will select wizards who perform well during 
pilot tests. Again, the goal will be to model the 
most successful wizards, based upon data from 
recognition results, NLU, and voice search results. 
Acknowledgements 
This research was supported by the National 
Science Foundation under IIS-0745369, IIS-
084966, and IIS-0744904. We thank the anonym-
ous reviewers, the Heiskell Library, our CMU col-
laborators, our statistical wizard Liana Epstein, and 
our enthusiastic undergraduate research assistants. 
References 
Bangalore, Srinivas; Bouillier, Pierre; Nasr, Alexis; 
Rambow, Owen; Sagot, Benoit (2009). MICA: a 
probabilistic dependency parser based on tree 
insertion grammars. Application Note. Human 
Language Technology and North American Chapter 
of the Association for Computational Linguistics, 
pp. 185-188.  
Bohus, D.; Rudnicky, A.I. (2009). The RavenClaw 
dialog management framework: Architecture and 
systems. Computer Speech and Language, 23(3), 
332-361. 
Bohus, Daniel; Rudnicky, Alex (2002). Integrating 
multiple knowledge sources for utterance-level 
confidence annotation in the CMU Communicator 
spoken dialog system (Technical Report No. CS-
190): Carnegie Mellon University. 
Georgila, Kallirroi; Sgarbas, Kyrakos; Tsopanoglou, 
Anastasios; Fakotakis, Nikos; Kokkinakis, George 
(2003). A speech-based human-computer interaction 
system for automating directory assistance services. 
International Journal of Speech Technology, Special 
Issue on Speech and Human-Computer Interaction, 
6(2), 145-59. 
847
Gordon, Joshua, B.; Passonneau, Rebecca J. (2010). An 
evaluation framework for natural language 
understanding in spoken dialogue systems. Seventh 
International Conference on Language Resources 
and Evaluation (LREC). 
Johnston, Michael; Bangalore, Srinivas; Vasireddy, 
Gunaranjan; Stent, Amanda; Ehlen, Patrick; Walker, 
Marilyn A., et al (2002). MATCH--An architecture 
for multimodal dialogue systems. Proceedings of the 
40th Annual Meeting of the Association for 
Computational Linguistics, pp. 376-83.  
Komatani, Kazunori; Kanda, Naoyuki; Ogata, Tetsuya; 
Okuno, Hiroshi G. (2005). Contextual constraints 
based on dialogue models in database search task 
for spoken dialogue systems. The Ninth European 
Conference on Speech Communication and 
Technology (Eurospeech), pp. 877-880.  
Kruijff-Korbayov?, Ivana; Blaylock, Nate; 
Gerstenberger, Ciprian; Rieser, Verena; Becker, 
Tilman; Kaisser, Michael, et al (2005). An 
experiment setup for collecting data for adaptive 
output planning in a multimodal dialogue system. 
10th European Workshop on Natural Language 
Generation (ENLG), pp. 191-196.  
Levin, Esther; Narayanan, Shrikanth; Pieraccini, 
Roberto; Biatov, Konstantin; Bocchieri, E.; De 
Fabbrizio, Giuseppe, et al (2000). The AT&T-
DARPA Communicator Mixed-Initiative Spoken 
Dialog System. Sixth International Conference on 
Spoken Dialogue Processing (ICLSP), pp. 122-125.  
Litman, Diane; Hirschberg, Julia; Swerts, Marc (2006). 
Characterizing and predicting corrections in spoken 
dialogue systems. Computational Linguistics, 32(3), 
417-438. 
Litman, Diane; Pan, Shimei (1999). Empirically 
evaluating an adaptable spoken dialogue system. 7th 
International Conference on User Modeling (UM), 
pp. 55-46.  
Quinlan, J. Ross (1993). C4.5: Programs for Machine 
Learning. San Mateo, CA: Morgan Kaufmann. 
Ratcliff, John W.; Metzener, David (1988). Pattern 
Matching: The Gestalt Approach. Dr. Dobb's 
Journal, 46 
Raux, Antoine; Bohus, Dan; Langner, Brian; Black, 
Alan W.; Eskenazi, Maxine (2006). Doing research 
on a deployed spoken dialogue system: one year of 
Let's Go! experience. Ninth International 
Conference on Spoken Language Processing 
(Interspeech/ICSLP).  
Raux, Antoine; Eskenazi, Maxine (2007). A Multi-layer 
architecture for semi-synchronous event-driven 
dialogue management.IEEE Workshop on 
Automatic Speech Recognition and Understanding 
(ASRU 2007), Kyoto, Japan. 
Raux, Antoine; Langner, Brian; Black, Alan W.; 
Eskenazi, Maxine (2005). Let's Go Public! Taking a 
spoken dialog system to the real world.Interspeech 
2005 (Eurospeech), Lisbon, Portugal. 
Rieser, Verena; Kruijff-Korbayov?, Ivana; Lemon, 
Oliver (2005). A corpus collection and annotation 
framework for learning multimodal clarification 
strategies. Sixth SIGdial Workshop on Discourse 
and Dialogue, pp. 97-106.  
Sacks, Harvey; Schegloff, Emanuel A.; Jefferson, Gail 
(1974). A simplest systematics for the organization 
of turn-taking for conversation. Language, 50(4), 
696-735. 
Skantze, Gabriel (2003). Exploring human error 
handling strategies: Implications for Spoken 
Dialogue Systems. Proceedings of ISCA Tutorial 
and Research Workshp on Error Handling in Spoken 
Dialogue Systems, pp. 71-76.  
Stoyanchev, Svetlana; Stent, Amanda (2009). 
Predicting concept types in user corrections in 
dialog. Proceedings of the EACL Workshop SRSL 
2009, the Second Workshop on Semantic 
Representation of Spoken Language, pp. 42-49.  
Turunen, Markku; Hakulinen, Jaakko; Kainulainen, 
Anssi (2006). Evaluation of a spoken dialogue 
system with usability tests and long-term pilot 
studies. Ninth International Conference on Spoken 
Language Processing (Interspeech 2006 - ICSLP). 
Walker, M A.; Litman, D, J.; Kamm, C. A.; Abella, A. 
(1998). Evaluating Spoken Dialogue Agents with 
PARADISE: Two Case Studies. Computer Speech 
and Language, 12, 317-348. 
Wang, Ye-Yi; Yu, Dong; Ju, Yun-Cheng; Acero, Alex 
(2008). An introduction to voice search. IEEE 
Signal Process. Magazine, 25(3). 
Ward, Wayne; Issar, Sunil (1994). Recent improvements 
in the CMU spoken language understanding 
system.ARPA Human Language Technology 
Workshop, Plainsboro, NJ. 
Williams, Jason D.; Young, Steve (2004). 
Characterising Task-oriented Dialog using a 
Simulated ASR Channel. Eight International 
Conference on Spoken Language Processing 
(ICSLP/Interspeech), pp. 185-188.  
Witten, Ian H.; Frank, Eibe (2005). Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd ed.). San Francisco: Morgan Kaufmann. 
Zollo, Teresa (1999). A study of human dialogue 
strategies in the presence of speech recognition 
errors. Proceedings of AAAI Fall Symposium on 
Psychological Models of Communication in 
Collaborative Systems, pp. 132-139.  
Zue, Victor; Seneff, Stephanie; Glass, James; Polifroni, 
Joseph; Pao, Christine; Hazen, Timothy J., et al 
(2000). A Telephone-based conversational interface 
for weather information. IEEE Transactions on 
Speech and Audio Processing, 8, 85-96. 
 
848
Proceedings of NAACL-HLT 2013, pages 1082?1091,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Open Dialogue Management for Relational Databases 
 Ben Hixon Rebecca J. Passonneau Computer Science & Engineering Center for Computational Learning Systems University of Washington Columbia University Seattle, WA 98195, USA New York, New York, USA bhixon@cs.washington.edu becky@ccls.columbia.edu       Abstract We present open dialogue management and its application to relational databases. An open dialogue manager generates dialogue states, ac-tions, and default strategies from the semantics of its application domain. We define three open dialogue management tasks. First, vocabulary selection finds the intelligible attributes in each database table. Second, focus discovery selects candidate dialogue foci, tables that have the most potential to address basic user goals. Third, a focus agent is instantiated for each dia-logue focus with a default dialogue strategy governed by efficiency. We demonstrate the portability of open dialogue management on three very different databases. Evaluation of our system with simulated users shows that us-ers with realistically limited domain knowledge have dialogues nearly as efficient as those of users with complete domain knowledge.  1 Introduction This paper presents open dialogue management. An open dialogue manager (ODM) generates dia-logue states, actions, and strategies from knowledge it computes about the semantics of its domain. A dialogue strategy is the procedure by which a system chooses its next action given the current state of the dialogue. The system's dialogue policy completely specifies which strategy to use in any dialogue state. Strategies can be handcrafted or learned. Reinforcement learning, the leading method for dialogue strategy learning, can yield powerful results but relies on small sets of states and actions predefined by the researcher. This reliance on domain expertise limits machine 
learned dialogue managers to the domains for which they were specifically designed, and contributes to the prevalence of handcrafted strategies over machine learning approaches for dialogue management in commercial applications (Paek & Pieraccini, 2008). We argue that open dia-logue management, which exploits the semantics and contents of its database to generate actions, states and default strategies, is a step towards a dialogue manager that operates across domains.  As a first step to open dialogue management we present ODDMER (OPEN-DOMAIN DIALOGUE MANAGER), the first dialogue system to generate its own dialogue strategy from relational databases. ODDMER?s vocabulary selection module uses supervised learning to determine each table?s intel-ligible attributes, those most likely to be in the us-er?s vocabulary. Its focus discovery module finds candidate dialogue foci, tables that have the most potential to address basic user goals. Foci are iden-tified with schema summarization through a ran-dom walk over the database schema that ranks ta-bles by size, linguistic information, and connectivi-ty. For each candidate focus, ODDMER instanti-ates a focus agent that prompts users for values of intelligible attributes ordered by efficiency.  
 Figure 1. ODDMER uses focus discovery and vocabu-lary selection to choose its states, actions, and strategy. 
1082
This paper addresses a particular type of infor-mation-seeking dialogue in which the user?s goal is to select a tuple from a table. Tuples are identified by constraints, attribute-value pairs elicited from a user during the dialogue. A typical user, however, cannot supply all values with equal readiness. For example, attributes such as primary or foreign keys are irrelevant or unintelligible to users. This results in a vocabulary problem, a mismatch between sys-tem and user vocabulary (Furnas et al, 1987). Fur-thermore, tables differ in their relevance to users. Tables that contain little semantic information have less potential to address user goals. Dialogue sys-tems for relational databases often rely on manual pre-processing to select the attributes a typical user can most readily supply and identify the tables with the most relevance to basic user goals. An open dialogue system obviates this manual step by exploiting the database semantics.  For example, Heiskell is a library database that includes a table for books (BOOK) and a table for book subject headings (HEADING). A typical pa-tron wants a book, not a heading. Due to BOOK's larger size, its greater number of intelligible attrib-utes, and its higher connectivity to other tables, ODDMER recognizes that a BOOK tuple satisfies a more basic user goal. BOOK has 32 attributes, most of which are numeric fields familiar to a librarian but arcane to the user. ODDMER selects the ta-ble?s intelligible attributes as its vocabulary. It rec-ognizes that a book?s author and title are intelligi-ble, but the book?s ISBN is not. Consequently, ODDMER will not ask the user for the ISBN. ODDMER assumes a user of the Heiskell data-base will be likely to know one or more intelligible attributes of books. ODDMER ranks intelligible attribute-value pairs by their semantic specificity, the degree to which they uniquely identify a tuple. To demonstrate the benefit of pre-computing this semantic information, we test ODDMER on three databases with simulated users of two knowledge levels. Complete-knowledge users know all attrib-ute values. They have no vocabulary problem, will always be able to supply a requested constraint, and require no vocabulary selection to achieve maximum dialogue efficiency. Incomplete-knowledge users have a more realistic vocabulary. They know values for different attributes with dif-ferent probabilities. Without vocabulary selection, these users have long, inefficient dialogues. Given ODDMER?s vocabulary selection and efficient 
dialogue strategy, these users achieve their goals nearly as efficiently as complete-knowledge users.  2 Related Work ODDMER is the first dialogue system to examine a database and choose which tables and attributes to use in dialogue. We envision open dialogue management as a suite of domain-independent pro-cedures through which a dialogue manager can exploit its knowledge base. Hastie, Liu, and Lemon (2009) also generate policies from databases. They do not consider multiple tables, and they depend on handcrafted Business Process Models that ex-plicitly specify the dialogue flow for the domain. This limits their method to domains with available models. Polifroni, Chung, and Seneff (2003) also argue for the importance of generic, domain-independent dialogue managers. Their portable information presentation strategies cluster attribute values to summarize database contents for users. Neither of these works considers how to choose attributes or find which domain entities are likely objects of dialogue goals. Chotimongkol and Rudnicky (2008) use unsupervised learning to au-tomatically acquire task-specific information from a corpus of in-domain human-human dialogue transcripts. They require a large corpus whereas we need only the underlying database.  The vocabulary problem has received relatively little attention in dialogue research, and no method to automatically identify intelligible constraints has been previously demonstrated. Demberg and Moore (2006) choose constraints with a user model that records user importance, such as ?price? for a student in a restaurant domain. They require a manually crafted user model and must match mod-el to user. Polifroni and Walker (2006) use attrib-ute entropy to order system initiative prompts, but assume that both the table and the relevant attrib-utes are known a priori. Varges, Weng, and Pon-Barry (2006) develop a WOZ system in which a wizard recommends to real users what constraint to provide that will best narrow down results. Each of these works assumes all constraints are intelligible.  Two recent works concentrate more closely on the vocabulary problem. Janarthanam and Lemon (2010) build a system that determines a user?s level of referring expression expertise, but manually de-termine the set of possible expressions. Selfridge and Heeman (2010) simulate users with different 
1083
levels of domain knowledge. A novice has a 10% chance to know any constraint, and an expert a 90% chance. They do not consider users who know different constraints with different probabilities as we do, and do not consider databases that contain attributes likely to be unintelligible to most users.  Reinforcement learning, the leading approach for learning a dialogue strategy, demonstrates powerful results. For example, Rieser and Lemon (2009) find the optimal size of a list of tuples that match a user?s constraints and when to display it in different user environments. A dialogue strategy is treated as a policy, a function that maps states to actions. Policy optimization is a Markov decision process. Paek and Pieraccini (2008) argue that re-inforcement learning is limited by its reliance upon small sets of manually defined states and actions, with no standard method to determine these. ODDMER identifies dialogue states and actions automatically. Its default strategy could be opti-mized with reinforcement learning. Portability is an important research area in natu-ral-language interfaces to databases (NLIDBs). An NLIDB parses a user utterance into a logical form, which is transformed into a database query. Users typically know the database structure and contents. TEAM (Grosz, 1983), the first portable NLIDB, questions a domain expert to acquire linguistic knowledge for new databases. More recently, the ORAKEL interface (Cimiano et al, 2008) partially derives a domain-specific lexicon from a generic ontology. Here we do not focus on parsing of user questions, but on the acquisition of dialogue states, actions, and strategies from a database. 3 The ODDMER Dialogue System ODDMER?s vocabulary selection module finds each table?s intelligible attributes. Its focus discov-ery module identifies candidate foci. A focus agent module instantiates dialogue agents for each focus. Their default strategy elicits attribute values from users in order of semantic specificity.  3.1 Vocabulary Selection A vocabulary is the set of words and expressions used to discuss a domain. Domain entities can be identified by their descriptions, or sets of attribute-value pairs. In order for a system and a user to profitably engage in a natural language dialogue about database items, descriptions should consist 
of attributes and values understood by both system and user. We define the vocabulary selection task as the automatic selection of attribute-value pairs that the system expects its users will use to de-scribe domain entities. Successful vocabulary selection solves the vo-cabulary problem. The vocabulary problem is a bottleneck to portability because the attributes a user is likely to know must be predetermined for existing systems. ODDMER learns a classifier to determine a table?s intelligible attributes. An at-tribute is intelligible if its values are in a user?s vocabulary. A user interested in a particular item but unfamiliar with the structure of a database is more likely to recognize an intelligible attribute, and to know all or part of the relevant value.  To determine intelligible attributes, ODDMER currently relies on a binary classifier that takes as input the values of each attribute found in a partic-ular instantiation of a relational database. To train the classifier, we labeled a set of 84 attributes be-longing to tables taken from the Microsoft Adven-tureWorks Cycle Company database, a benchmark database packaged with Microsoft SQL Server. An attribute was labeled as intelligible if its values were likely to be known to a user. Four annotators worked independently to label the attributes. Pair-wise agreement was 69%, and Krippendorff?s al-pha (Krippendorff, 1980) was 0.42.  The low agreement can be attributed in part to the many ways to interpret the question annotators were to answer. The instructions indicated that the goal was to identify attributes corresponding to com-mon-sense knowledge, but for a given table, anno-tators were shown all the attributes and asked whether they would know a value. For an employ-ee table, annotators disagreed on attributes such as birthdate, hire date, and organization level. If they had instead been asked whether anyone without access to the table might know a value, there may have been more agreement.  Ratio of unique to total characters in all values Mean ratio of unique to total characters per value Ratio of numeric to total characters in all values Ratio of unique to total values Ratio of unique to total words in all values Total number of characters in all values Table 1. Representative features for attribute classifica-tion used in the best-performing intelligibility classifier.  
1084
The training data for the classifier consisted of 67 attributes that at least three annotators agreed on (22 intelligible, 45 not intelligible); pairwise agreement was 0.81 and Krippendorff?s alpha was 0.61. They represented 8 tables and contained a total of 393,520 values, 123,901 of which were unique. For each attribute we extracted 17 features to represent the linguistic expressiveness of the attributes? values. An attribute whose values are more like natural language is more intelligible. Table 1 lists the features of the best classifier. We tested several binary classifiers in Weka (Hall et al, 2009). ADTree (Freund & Mason, 1999) with ten boosting iterations performed best, with 91% recall and 91% precision under 10-fold cross-validation. However, the ADTree models were overfitted to the AdventureWorks domain. The RIPPER rule-learning algorithm (Cohen, 1995) achieved 77% precision and 78% recall. Be-cause its learned model of three simple rules gen-eralizes better to our domains, ODDMER uses the RIPPER intelligibility classifier. Given a table, the vocabulary selection module returns which of its attributes should be in the sys-tem?s vocabulary. For the Heiskell Library domain, the 32 attributes of the BOOK table include many internal codes understood by librarians but not by users. Only the seven attributes shown in Table 2 are classified as intelligible. Dialogues with only intelligible attributes should be more efficient for users with incomplete domain knowledge, because they will be more likely to know their values.  ODDMER?s vocabulary selection module also computes the semantic specificity score of each attribute (Hixon, Passonneau, & Epstein, 2012). Semantic specificity rates an attribute on a scale from 0 to 1 according to how unambiguously its values map to rows in the database. More specific attributes are expected to be more efficient prompts. Table 2 lists the specificity values of the intelligible attributes for BOOK.   Intelligible Attributes Specificity ANNOTATION TITLE SORTTITLE AUTHOR NARRATOR PUBLISHER SERIES 
0.958	 ?0.878	 ?0.878	 ?0.300	 ?0.018	 ?0.016	 ?0.003	 ?Table 2. Intelligible attributes for BOOK sorted by speci-ficity. (SORTTITLE is a duplicate of TITLE.) 
3.2 Candidate Dialogue Focus Discovery Information-seeking dialogues address diverse dia-logue goals. For example, users may want to iden-tify a tuple in a table (?I want a certain book by Stephen King.?), retrieve the value of an attribute for a given tuple (?Is my plane on time?? ?Who wrote Moby Dick??), aggregate over a set of tuples (?How many Italian restaurants are in this neigh-borhood??), or compare values of different tuples (?Which restaurant is more expensive??). Each of these dialogue goals represents a distinct infor-mation need. However, not all possible information needs in a domain are equally likely. For example, a user is unlikely to ask for the value of a primary key attribute, or to select a tuple from a table that contains only primary and foreign keys. A dialogue system should place less priority on addressing these peripheral dialogue goals.  Given a particular domain, we assume that some goals are more basic than others. For example, the basic function of a library is to provide books to borrowers. Some libraries will also provide other material, or perform reference functions, but these are less basic. This notion of a basic goal is related to the basic categories proposed by Rosch (1978), who claimed that not all categories are equally use-ful for cognition. Basic categories are more differ-entiated from other categories, and have attributes that are common to all or most members of the category, thus provide us with more information (the principle of cognitive economy). Basic catego-ries also mirror the structure of the perceptual and functional attributes of the natural world, thus serve us better in our daily activities. Typically a domain expert will identify the basic dialogue goals in a domain, but we suggest that the basic dialogue goals are discoverable in the underlying database. While a difficult problem, we are moti-vated by work in the database literature to identify and rank the most likely queries for an arbitrary database (Jayapandian & Jagadish, 2008).  We approach the problem of recovering dia-logue goals from a database by restricting our at-tention to the tuple selection task, a commonly studied type of information-seeking dialogue in which the user?s goal is to select a tuple from a table. A relational database typically consists of multiple tables, and each table can satisfy different user goals. Given a database composed of multiple tables, an open dialogue system calculates which 
1085
tables are larger, have more natural language con-tent, and greater connectivity to other tables. We refer to these tables as candidate dialogue foci. This notion of candidate focus for a dialogue is similar to focus of attention (Grosz & Sidner, 1986) in that information-seeking dialogues can be segmented to reflect the table both participants fo-cus their attention on at a given time. We denote the task of identifying candidate foci in a relational database as focus discovery. ODDMER?s focus discovery module returns an ordered list of candidate foci, the focal summary. The highest ranked focus is the most relevant to basic user goals, those goals that pertain to the most information-rich and intelligible table. For our tuple-selection task, the highest-ranked focus is the table from which the system predicts a user will most likely want to select a tuple. A system that begins a dialogue by first mentioning the most relevant tables communicates the structure of the database better than does a system that lists all ta-bles in a random order. Users with more special-ized goals may be interested in more peripheral tables. For these users, more effort will be required to establish a dialogue focus: several tables may be proposed by the system and rejected before the user agrees to consider a given table.  In tests with real users, we would expect them to find it ac-ceptable for a specialized goal to take more effort than a basic goal.  We use schema summarization to find candidate focus tables. According to Yu and Jagadish (2006), a schema summary should convey a concise under-standing of the underlying database schema. They identify table importance and coverage as criteria for good schema summaries. Their summaries for XML databases rank entities with higher cardinali-ty (number of rows) and connectivity (number of joins) as more important. Yang, Procopiuc, and Srivastava (2009) extend schema summarization to relational databases. We closely follow the Yang algorithm but make modifications for dialogue to account for attribute intelligibility. A database schema is an undirected graph  G = <R,E> where each node r  R corresponds to a table in a database and each edge e  E denotes a join between two tables. A schema summary is a set of the most important nodes in the schema. Yang and colleagues compute the importance of a table as a function of its size, total entropy of its attributes, and connectivity to other tables. To in-
corporate connectivity, they employ a random walk over the schema graph. The most important tables maintain the highest information content in the steady state of a random walk over the schema. A significant feature of their algorithm is that a table?s high-entropy attributes largely determine its importance. It is possible to artificially inflate a table?s importance under the Yang algorithm by introducing a new column of distinct integer val-ues; numeric and linguistic values contribute equally to importance. For dialogue applications, this is undesirable. A table with more intelligible attributes is a more likely candidate focus because it can more readily be discussed. We therefore modify the Yang algorithm to compute table ver-bality. Verbality is similar to importance except that where Yang and colleagues use all attributes, we use intelligible attributes identified by vocabu-lary selection. A table?s verbality score is a function of its car-dinality, the entropy of its intelligible attributes, and its connectivity to other tables. We apply vo-cabulary selection to find natural language attrib-utes. To calculate the verbality of a table T, let A be the attributes of T and let A?? A be those attrib-utes of T whose values are intelligible, found by the classifier described previously. For BOOK, A?? consists of the attributes shown in Table 2. Define V, the verbal information content of a table, as  where |T| is the cardinality of the table and H(a) is the entropy of the attribute a in A??. The entropy of a is given by ? ? = ? ?? log ?????  where K is the set of distinct values of a and pk is the fraction of rows in T for which a=k. If table T has no joins, V(T) is the final verbality score of T. Table 3 shows V(T) for each T in Heiskell. To incorporate connectivity into verbality, we create a matrix of transition probabilities between every pair of nodes in the schema and determine which table maintains the highest information. Let J A be the attributes of T that join to other tables. The information transfer (IT) over the join j J is ?? ? = ? ?? ? + ??? ????  where qa is the number of joins in which attribute a participates. Let P(T,R) be the transition probabil-ity from table T to table R. For T?R, P(T,R) is the sum of IT(j) for all joins j between T and R. These 
? ?
?V (T ) = log(|T |)+ H (a)a?A '?? ?
1086
probabilities represent the flow of information be-tween tables over their joins. The diagonal entries of the transition matrix are given by P(T,T) = , the information that stays within table T. We then define the verbality of table Ti to be the ith element in the stable distribution of a random walk over the NxN matrix whose elements are P(Ti,Tj) for i,j N. We follow Yang and colleagues and use power iteration to find the stable distribution.  T V(T) Verbality(T) Book 88.2 45.4 Heading 31.7 22.4 BibHeadingLink 19.2 23.6 CirculationHistory 17.9 24.9 Holding Stats 15.2 24.0 Patron Properties 12.7 21.9 Reserve 12.2 25.5 Patron 9.0 18.6 Table 3. Verbalities of Heiskell. V(T) is the verbal in-formation of T. Verbality(T) incorporates connectivity. Table 3 illustrates the verbalities of Heiskell be-fore and after information transfer. BOOK clearly dominates. Before information transfer there is more verbal information V in BOOK than in the next four tables combined. After information trans-fer reaches a steady state, its connectivity with oth-er tables increases their verbality, but BOOK re-mains the leading candidate by a large margin. HEADING?s verbality decreases sharply after in-formation transfer because of its low connectivity.  The focus discovery module returns a focal summary, a list of the top k tables with the highest verbalities. The focal summary is similar to the intensional summary described by Polifroni and Walker (2008), which communicates the contents of a single table to a user. A key difference is that the focal summary pertains to the entire database. ODDMER is currently limited to the table and attribute labels assigned by the database designer. For example, the Heiskell ?BOOK? table was la-beled ?BIBREC? by the database designers, for Bib-liographic Record. We renamed this table prior to its use as a backend for the dialogue system. But ODDMER has no way to determine if labels are meaningful. In many cases there is a disincentive towards meaningful table names to avoid conflicts with SQL keywords. Future work will explore how to infer more meaningful table and attribute labels 
from a database instantiation, for example by con-sulting external knowledge bases to predict the entity a table represents. 3.3 Focus agent generation Focus discovery fits naturally into an agent-based or agenda-based approach to dialogue management (Bohus & Rudnicky, 2009; Nguyen & Wobcke, 2005). At the onset of a dialogue, ODDMER?s root agent announces the focal summary and prompts the user for her goal. Upon receipt of a user reply, it launches the appropriate focus agent, a finite state machine based on Information State Update (Traum & Larsson, 2003), whose parameters are a table, its list of intelligible attributes, and their computed specificity scores. The agent elicits con-straints from the user until the current goal is satis-fied or the user abandons the focus. Control over the dialogue then returns to the root, which queries the user for a new focus or ends the dialogue. Fig-ure 2 shows a sample dialogue.  1. S: Hello, I can help you find a Book. Would you like to find a Book?  2. U: I?d like a Book. 3. S: Do you know the annotation? 4: U: No. 5: S: Do you know the title? 6. U: It?s Gone with the Wind Figure 2. ODDMER?s root agent gives the focal sum-mary (k=1) in line 1. The BOOK focus agent launches in line 3 and prompts for the value of the most specific intelligible attribute.  The default strategy of the focus agent is to elicit the most specific, intelligible constraints first. While intelligible attributes are more likely to be known by users, they are not equally valuable as item descriptions. As shown in Table 2, the speci-ficities of NARRATOR, PUBLISHER, and SERIES are so low that a strategy involving them will likely lead to inefficient dialogues and unsatisfied users. The focus agent therefore orders its prompts for constraints by their specificity, and requests the most specific attributes first. Because specificity is a function of the database instantiation and not user knowledge, we expect that this strategy will lead to shorter, more efficient dialogues for all users. The dialogue starts with the root agent in con-trol of the dialogue. The root agent announces the focal summary, the k tables with highest verbality. 
1 ( , )
T R
P T R
?
?? ?
1087
Here we let k=1. The root agent parses the user?s reply to determine the focus of interest and launch-es the appropriate agent. The agent interacts with the user to find a tuple from the table. Its default strategy elicits constraints from the user in order of semantic specificity. Because semantic specificity can apply to combinations of attributes, future work will investigate tradeoffs between efficiency and user effort in prompts for multiple attributes. The focus agent queries the database upon re-ceipt of each new constraint. If the return size is small enough (here, a single tuple) it announces the result. Otherwise it continues to elicit constraints until all intelligible attributes have elicited values, at which point it announces all matching results.   ODDMER deals with three goal setbacks: (1) dis-ambiguation, in which a query is under-constrained and matches multiple tuples; (2) over-constrained queries that have no matching tuple; and (3) attrib-utes whose values are unknown to the user. The third setback is particularly prevalent in real-world databases. Our system addresses these setbacks with specific, intelligible vocabulary.  4 Evaluation ODDMER finds foci and vocabulary for any rela-tional database. We evaluate it on three very dif-ferent domains. These databases were chosen for their variety, and are not equally suitable for dia-logue. Our primary domain of consideration is Heiskell, the database of the Heiskell Library. Heiskell has eight tables. The largest table is CIR-CULATION HISTORY, which contains 16 attributes with 244,072 rows. However, focus discovery identifies BOOK as the top candidate focus, which matches our intuition. Though BOOK is smaller at only 71,166 rows, it has 32 attributes of which seven are classified as intelligible. The classifier finds none of CIRCULATION HISTORY?s attributes intelligible. Manual inspection revealed CIRCULA-TION HISTORY to consist primarily of alphanumeric codes relevant to the library rather than to users. The second domain we consider is Grocery, a small supermarket database used as a teaching tool in an undergraduate class at Hunter College. Gro-cery has 20 tables. Their cardinalities range from 7 to 197 rows. The top focus in Grocery represents the products sold in the store. It was the largest table with the greatest connectivity, and makes 
intuitive sense; it is the table a supermarket cus-tomer would most likely want to talk about.  To challenge our system we consider Eve, a freely available database for the virtual game Eve Online, a massively multiplayer online role-playing game with over 400,000 subscribers. Eve has 78 tables and the game?s active community regularly accesses it to determine in-game goals and objects of interest. Eve is a challenge for ODDMER because it primarily contains numeric data for objects in the game world. These numeric attributes are of great interest to players but con-found our assumption that dialogue goals correlate with high verbality. Moreover, connectivity plays no role in table verbality because Eve contains no joins. Focus discovery on Eve identifies INVTYPES, a table that represents in-game inventory items, as the best focus, even though vocabulary selection identifies only three of its 15 attributes as intelligi-ble. The 12 unintelligible attributes were all nu-meric. In general, focus discovery and vocabulary selection proved less effective on Eve than on Heiskell. In Eve the verbality scores of the top ta-bles were close together without one outstanding focus candidate. 4.1 Simulating the vocabulary problem A typical evaluation of a spoken dialogue system provides users with all the information needed to carry out a dialogue. Such a completely knowl-edgeable user can unrealistically describe objects in the domain with the same vocabulary that the system uses. This means it does not experience the vocabulary problem. To test vocabulary selection, we simulate users with incomplete domain knowledge. In contrast with Selfridge and Heeman (2010), our limited-knowledge users are more like-ly to know some attributes than others. User simulation is often used to stand in for real user dialogues, but it is a concern whether the dia-logues are sufficiently realistic (Schatzmann, Georgila, & Young, 2005). Here, we use simula-tion to exercise each dialogue system with a large number of cases in a highly controlled fashion. For simulated users, we can specify exactly what each user knows about the domain, thus simulation makes it possible to hold everything else the same while contrasting users with complete versus in-complete domain knowledge.  We view this as a preliminary step towards evaluation with real us-ers, which we hope to do in future work. 
1088
 Heiskell  Grocery Eve  C/N/R 15.9 ? 0.4	 ? 11.1 ? 0.2 16.3 ? 0.4 C/N/S 9.0 ? 0.0 9.0 ? 0.0 9.0 ? 0.0 C/V/R 11.5 ? 0.2 11.1 ? 0.3 10.6 ? 0.1 C/V/S 9.5 ? 0.1 9.0 ? 0.0 9.0 ? 0.0 L/N/R 25.3 ? 0.8 13.1 ? 0.2 17.7 ? 0.5 L/N/S 16.5 ? 0.6 9.3 ? 0.1	 ? 9.4 ? 0.1	 ?L/V/R 12.4 ? 0.2	 ? 12.2 ? 0.1	 ? 10.7 ? 0.1	 ?L/V/S 11.3 ? 0.2	 ? 9.7 ? 0.1	 ? 9.0 ? 0.0	 ?Table 4. Mean dialogue length across domains. C/*/*: complete-knowledge user; L/*/*: incomplete-knowledge user; */N/*: no vocabulary selection; */V/*: with vocabulary selection; */*/R: random order; */*/S: order by specificity. Intervals are 95% confidence.  Our simulated user knows each attribute?s value with a different probability. Ideally we might esti-mate these probabilities from a language model of a corpus in the domain. Unfortunately our domains contain many obscure names and non-verbal val-ues for which we need non-zero probabilities. In-stead, we estimate a probability for attribute value knowledge by calculating the frequency of total value occurrences in a subset of the New York Times portion of the English Gigaword corpus (Parker et al, 2011), a 4 million word corpus of news articles. These frequency values could have been used during vocabulary selection but we chose to reserve them for evaluation. The probability that the limited-knowledge user knows a particular value is the normalized fre-quency that the attribute?s values appear in Giga-word. We tokenized attribute values in our data-bases to remove punctuation and case. We ignored word order so that, for example, the author values ?King, Stephen? and ?Stephen King? are equiva-lent. For each value, we counted the articles in which all the value?s tokens co-occurred. For each attribute, we took the sum of these counts over all its values, and took its log to represent the proba-bility that the user knows that attribute. We then normalized by the log of the highest-frequency attribute to enforce our assumption that the user usually knows at least one piece of information about her goal. This method is robust to attributes with missing values. Probabilities for the BOOK attributes were 100% for TITLE, 78% for AUTHOR, 75% for PLACE PUBLISHED, and 73% for PUB-LISHER. ISBN has a 0% probability because none of its values occur in the corpus. ANNOTATIONS, whose values are brief plot descriptions of each 
book, has a low 37%. Although its values con-tained many common words, the words in a single annotation rarely co-occurred in one article. 4.2 Testing the impact of domain knowledge We evaluate dialogue efficiency with two simulat-ed users as measured by number of turns. The first user, C, has complete domain knowledge and al-ways knows every constraint. The second, L, has limited, incomplete domain knowledge. When con-fronted with the focal summary, the simulated user always chooses the top suggested focus. The dia-logue ends when either a single tuple matching the constraints is found, or all constraints have been requested, in which case all matching tuples are announced. We measure average dialogue length of 1000 simulated dialogues for each user with vocabulary selection (V) and without (N), and with prompts ordered randomly (R) or by specificity (S). Table 4 shows the results. ANOVAs of all pairs of comparisons were highly significant. The longest dialogues for both users occur without vocabulary selection and with prompts in random order (*/N/R). The more attributes there are, the longer it takes a random order to achieve a constraint combination that forms a key, so C has long dialogues even though it knows every con-straint. L experiences much longer dialogues be-cause it is prompted for inefficient constraints, and is unlikely to know most of them. This difference is particularly noticeable in Heiskell. On average, L?s dialogues are ten turns longer. Ordering prompts by specificity without vocabu-lary selection (*/N/S) yields a sharp increase in efficiency for both users. C?s dialogues achieve the minimum number of turns because it is immediate-ly asked for the most specific constraint, which it always knows. In Heiskell, specificity decreases L?s average length from 25.3 to 16.5, a large in-crease in efficiency but still much worse than C. For Eve, L performs better in the absence of vo-cabulary selection. Specificity alone brings its av-erage efficiency near optimum. This is because for Eve?s INVTYPES table, the most specific intelligible attribute is the item?s NAME, which our domain knowledge model predicts L will always know. Vocabulary selection is more effective than specificity for L on Heiskell. L is much more likely to know the selected attributes and its efficiency increases even when prompted for intelligible at-tributes in a random order. Vocabulary selection is 
1089
less effective than specificity for C because C knows every attribute, but in general the intelligi-ble attributes are also more specific, so selection increases C?s efficiency even with random prompts. Vocabulary selection combined with specificity (*/V/S) leads to a small decrease in ef-ficiency for C on Heiskell over specificity alone. This is because the most specific intelligible attrib-ute is slightly ambiguous, and C must occasionally supply extra constraints to disambiguate. However, with both specificity and vocabulary selection, L achieves a mean dialogue length of 11.3, requiring only two turns more than C to order a book.  For Eve, vocabulary selection and order-by-specificity are each effective individually, and yield similar dialogues for both L and C. This is because INVTYPES has only three intelligible at-tributes, so the dialogue ends after at most three prompts. Our domain knowledge model predicts close to 100% knowledge for two of these.  A comparison of the order-by-specificity strate-gy used here with the order-by-entropy strategy described by Polifroni and Walker (2006) yielded no significant difference in dialogue length. The two strategies produce similar attribute orders. 5 Conclusion and Open Questions We have demonstrated an open dialogue manage-ment system, ODDMER, which formulates a dia-logue strategy by computing metaknowledge about its database: table verbality, attribute intelligibility, and attribute specificity. Candidate dialogue foci are the tables with high verbality. For each candi-date focus, ODDMER chooses an intelligible do-main vocabulary and generates a default strategy that orders prompts by specificity. A simulated user facing the vocabulary problem achieves more efficient dialogues with vocabulary selection. Our method works well on the Heiskell Library data-base, which has a particularly prominent candidate focus showing a clear separation between intelligi-ble and unintelligible attributes. Focus discovery and vocabulary selection are less effective for nu-meric databases without clear dialogue goals. For example, Eve?s top focus scored the highest verbal-ity, even though the table contained only three in-telligible attributes.  Questions that arise from this work include how to extend focus discovery and vocabulary selection to numerical databases, how to extract strategies 
for goals other than tuple-selection from a data-base, and how to automatically infer intelligible table and attribute labels. We are also interested in discovery of less rigid dialogue goals, for example, a library patron who would be satisfied by an al-ternative book, and goals involving information aggregation where user utterances map to sophisti-cated queries. We would like to investigate how optimal policies learned through reinforcement learning vary across domains. Future work will also scale to mixed-initiative open dialogue man-agement, explore more sophisticated models of user domain knowledge, and evaluate portability on more databases. ODDMER uses the semantics of its domain rep-resentation to discover what to talk about and how to talk about it. We envision a rich toolkit that ena-bles a system to explore its database for knowledge to exploit in collaborative dialogues with its users. Acknowledgements The first author was supported in part by NSF grant IIS-0803481, ONR grant N00014-11-1-0294, and DARPA contract FA8750-09-C-0179. The second author was supported by the NSF grant IIS-0745369. This research was carried out at Colum-bia University. We thank Susan Epstein, Oren Etzioni, Wlodek Zadrozny, and the anonymous reviewers for helpful comments. We thank Luis Gravano for valuable literature suggestions, and Susan Epstein for use of the Grocery database. References  Bohus, D., & Rudnicky, A. (2009). The RavenClaw Dialog Management Framework: Architecture and systems. Computer Speech and Language 23(3), 332-361.  Chotimongkol, A., & Rudnicky, A. I. (2008). Acquiring Domain-Specific Dialog Information from Task-Oriented Human-Human Interaction through an Unsupervised Learning. Paper presented at the Conference on Empirical Methods in Natural Language Processing (EMNLP '08). Cimiano, P., Haase, P., Heizmann, J., Mantel, M., & Studer, R. (2008). Towards Portable Natural Language Interfaces to Knowledge Bases - The case of the ORAKEL system. Data & Knowledge Engineering, 65(2), 325-354.  Cohen, W. W. (1995). Fast Effective Rule Induction. Paper presented at the Twelfth International Conference on Machine Learning. 
1090
Demberg, V., & Moore, J. D. (2006). Information Presentation in Spoken Dialogue Systems. Paper presented at the 11th Conference of the European Chapter of the Association of Computational Linguistics (EACL 2006). Freund, Y., & Mason, L. (1999). The Alternating Decision Tree Learning Algorithm. Paper presented at the Sixteenth International Conference on Machine Learning (ICML). Furnas, G. W., Landauer, T. K., Gomez, L. M., & Dumais, S. T. (1987). The Vocabulary Problem in Human-System Communication. Communications of the ACM, 30(11), 964-971.  Grosz, B. J. (1983). TEAM: a Transportable Natural-Language Interface System. Paper presented at the First conference on Applied natural language processing. Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3), 175-204.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009). The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1).  Hastie, H., Liu, X., & Lemon, O. (2009). Automatic Generation of Information State Update Dialogue Systems that Dynamically Create Voice XML, as Demonstrated on the iPhone. Paper presented at the 10th Annual SIGdial Meeting on Discourse and Dialogue (SIGdial 2009). Hixon, B., Passonneau, R. J., & Epstein, S. L. (2012). Semantic Specificity in Spoken Dialogue Requests. Paper presented at the 13th Annual SIGdial Meeting on Discourse and Dialogue (SIGdial 2012). Janarthanam, S., & Lemon, O. (2010). Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems. Paper presented at the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden. Jayapandian, M., & Jagadish, H. V. (2008). Automated creation of a forms-based database query interface. Paper presented at the 34th international conference on Very large data bases (VLDB '08). Krippendorff, K. (1980). Content Analysis: An Introduction to its Methodology. Beverly Hills, CA: Sage Publications. Nguyen, A., & Wobcke, W. (2005). An Agent-based Approach to Dialogue Management in Personal Assistants. Paper presented at the 10th international conference on Intelligent user interfaces (IUI '05), New York. Paek, T., & Pieraccini, R. (2008). Automating Spoken Dialogue Management Design Using Machine Learning: An Industry Perspective. Speech Communication, 50, 716-729.  
Parker, R., Graff, D., Kong, J., Chen, K., & Maeda, K. (2011). English Gigaword Fifth Edition. Philadelphia: Linguistic Data Consortium. Polifroni, J., Chung, G., & Seneff, S. (2003). Towards the Automatic Generation of Mixed-Initiative Dialogue Systems from Web Content. Paper presented at the Eurospeech 2003. Polifroni, J., & Walker, M. (2006). Learning Database Content for Spoken Dialogue System Design. Paper presented at the 5th International Conference on Language Resources and Evaluation (LREC). Polifroni, J., & Walker, M. (2008). Intensional Summaries as Cooperative Responses in Dialogue: Automation and Evaluation. Paper presented at the ACL-HLT. Rieser, V., & Lemon, O. (2009). Does this list contain what you were searching for? Learning Adaptive Dialogue Strategies for Interactive Question Answering. Natural Language Engineering, 15(1), 55-72.  Rosch, E. (1978). Principles of Categorization. In E. Rosch & B. Lloyd (Eds.), Cognition and Categorization (pp. 27-48). Hillsdale, NJ: Lawrence Erlbaum. Schatzmann, J., Georgila, K., & Young, S. (2005). Quantitative Evaluation of User Simulation Techniques for Spoken Dialogue Systems. Paper presented at the 6th SIGdial Workshop on Discourse and Dialogue. Selfridge, E., & Heeman, P. (2010). Importance-Driven Turn-Bidding for Spoken Dialogue Systems. Paper presented at the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Traum, D., & Larsson, S. (2003). The Information State Approach to Dialogue Management. In Jan van Kuppevelt and Ronnie Smith (Eds.), Current and new directions in discourse and dialogue. Kluwer: 325-353.  Varges, S., Weng, F., & Pon-Barry, H. (2006). Interactive Question Answering and Constraint Relaxation in Spoken Dialogue Systems. Paper presented at the 7th Annual SIGdial Meeting on Discourse and Dialogue (SIGdial 2006). Yang, X., Procopiuc, C. M., & Srivastava, D. (2009). Summarizing Relational Databases. Paper presented at the 35th international conference on Very large data bases (VLDB '09). Yu, C., & Jagadish, H. V. (2006). Schema Summarization. Paper presented at the 32nd international conference on Very large data bases (VLDB '06 ).   
1091
Proceedings of the ACL 2010 Conference Short Papers, pages 68?73,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Manually Annotated Sub-Corpus:
A Community Resource For and By the People
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Collin Baker
International Computer Science Institute
Berkeley, California USA
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, New Jersey USA
fellbaum@princeton.edu
Rebecca Passonneau
Columbia University
New York, New York USA
becky@cs.columbia.edu
Abstract
The Manually Annotated Sub-Corpus
(MASC) project provides data and annota-
tions to serve as the base for a community-
wide annotation effort of a subset of the
American National Corpus. The MASC
infrastructure enables the incorporation of
contributed annotations into a single, us-
able format that can then be analyzed as
it is or ported to any of a variety of other
formats. MASC includes data from a
much wider variety of genres than exist-
ing multiply-annotated corpora of English,
and the project is committed to a fully
open model of distribution, without re-
striction, for all data and annotations pro-
duced or contributed. As such, MASC
is the first large-scale, open, community-
based effort to create much needed lan-
guage resources for NLP. This paper de-
scribes the MASC project, its corpus and
annotations, and serves as a call for con-
tributions of data and annotations from the
language processing community.
1 Introduction
The need for corpora annotated for multiple phe-
nomena across a variety of linguistic layers is
keenly recognized in the computational linguistics
community. Several multiply-annotated corpora
exist, especially for Western European languages
and for spoken data, but, interestingly, broad-
based English language corpora with robust anno-
tation for diverse linguistic phenomena are rela-
tively rare. The most widely-used corpus of En-
glish, the British National Corpus, contains only
part-of-speech annotation; and although it con-
tains a wider range of annotation types, the fif-
teen million word Open American National Cor-
pus annotations are largely unvalidated. The most
well-known multiply-annotated and validated cor-
pus of English is the one million word Wall Street
Journal corpus known as the Penn Treebank (Mar-
cus et al, 1993), which over the years has been
fully or partially annotated for several phenomena
over and above the original part-of-speech tagging
and phrase structure annotation. The usability of
these annotations is limited, however, by the fact
that many of them were produced by independent
projects using their own tools and formats, mak-
ing it difficult to combine them in order to study
their inter-relations. More recently, the OntoNotes
project (Pradhan et al, 2007) released a one mil-
lion word English corpus of newswire, broadcast
news, and broadcast conversation that is annotated
for Penn Treebank syntax, PropBank predicate ar-
gument structures, coreference, and named enti-
ties. OntoNotes comes closest to providing a cor-
pus with multiple layers of annotation that can be
analyzed as a unit via its representation of the an-
notations in a ?normal form?. However, like the
Wall Street Journal corpus, OntoNotes is limited
in the range of genres it includes. It is also limited
to only those annotations that may be produced by
members of the OntoNotes project. In addition,
use of the data and annotations with software other
than the OntoNotes database API is not necessar-
ily straightforward.
The sparseness of reliable multiply-annotated
corpora can be attributed to several factors. The
greatest obstacle is the high cost of manual pro-
duction and validation of linguistic annotations.
Furthermore, the production and annotation of
corpora, even when they involve significant scien-
tific research, often do not, per se, lead to publish-
able research results. It is therefore understand-
68
able that many research groups are unwilling to
get involved in such a massive undertaking for rel-
atively little reward.
The Manually Annotated Sub-Corpus
(MASC) (Ide et al, 2008) project has been
established to address many of these obstacles
to the creation of large-scale, robust, multiply-
annotated corpora. The project is providing
appropriate data and annotations to serve as the
base for a community-wide annotation effort,
together with an infrastructure that enables the
representation of internally-produced and con-
tributed annotations in a single, usable format
that can then be analyzed as it is or ported to any
of a variety of other formats, thus enabling its
immediate use with many common annotation
platforms as well as off-the-shelf concordance
and analysis software. The MASC project?s aim is
to offset some of the high costs of producing high
quality linguistic annotations via a distribution of
effort, and to solve some of the usability problems
for annotations produced at different sites by
harmonizing their representation formats.
The MASC project provides a resource that is
significantly different from OntoNotes and simi-
lar corpora. It provides data from a much wider
variety of genres than existing multiply-annotated
corpora of English, and all of the data in the cor-
pus are drawn from current American English so
as to be most useful for NLP applications. Per-
haps most importantly, the MASC project is com-
mitted to a fully open model of distribution, with-
out restriction, for all data and annotations. It is
also committed to incorporating diverse annota-
tions contributed by the community, regardless of
format, into the corpus. As such, MASC is the
first large-scale, open, community-based effort to
create a much-needed language resource for NLP.
This paper describes the MASC project, its corpus
and annotations, and serves as a call for contribu-
tions of data and annotations from the language
processing community.
2 MASC: The Corpus
MASC is a balanced subset of 500K words of
written texts and transcribed speech drawn pri-
marily from the Open American National Corpus
(OANC)1. The OANC is a 15 million word (and
growing) corpus of American English produced
since 1990, all of which is in the public domain
1http://www.anc.org
Genre No. texts Total words
Email 2 468
Essay 4 17516
Fiction 4 20413
Gov?t documents 1 6064
Journal 10 25635
Letters 31 10518
Newspaper/newswire 41 17951
Non-fiction 4 17118
Spoken 11 25783
Debate transcript 2 32325
Court transcript 1 20817
Technical 3 15417
Travel guides 4 12463
Total 118 222488
Table 1: MASC Composition (first 220K)
or otherwise free of usage and redistribution re-
strictions.
Where licensing permits, data for inclusion in
MASC is drawn from sources that have already
been heavily annotated by others. So far, the
first 80K increment of MASC data includes a
40K subset consisting of OANC data that has
been previously annotated for PropBank predi-
cate argument structures, Pittsburgh Opinion an-
notation (opinions, evaluations, sentiments, etc.),
TimeML time and events2, and several other lin-
guistic phenomena. It also includes a handful of
small texts from the so-called Language Under-
standing (LU) Corpus3 that has been annotated by
multiple groups for a wide variety of phenomena,
including events and committed belief. All of the
first 80K increment is annotated for Penn Tree-
bank syntax. The second 120K increment includes
5.5K words of Wall Street Journal texts that have
been annotated by several projects, including Penn
Treebank, PropBank, Penn Discourse Treebank,
TimeML, and the Pittsburgh Opinion project. The
composition of the 220K portion of the corpus an-
notated so far is shown in Table 1. The remain-
ing 280K of the corpus fills out the genres that are
under-represented in the first portion and includes
a few additional genres such as blogs and tweets.
3 MASC Annotations
Annotations for a variety of linguistic phenomena,
either manually produced or corrected from output
of automatic annotation systems, are being added
2The TimeML annotations of the data are not yet com-
pleted.
3MASC contains about 2K words of the 10K LU corpus,
eliminating non-English and translated LU texts as well as
texts that are not free of usage and redistribution restrictions.
69
Annotation type Method No. texts No. words
Token Validated 118 222472
Sentence Validated 118 222472
POS/lemma Validated 118 222472
Noun chunks Validated 118 222472
Verb chunks Validated 118 222472
Named entities Validated 118 222472
FrameNet frames Manual 21 17829
HSPG Validated 40* 30106
Discourse Manual 40* 30106
Penn Treebank Validated 97 87383
PropBank Validated 92 50165
Opinion Manual 97 47583
TimeBank Validated 34 5434
Committed belief Manual 13 4614
Event Manual 13 4614
Coreference Manual 2 1877
Table 2: Current MASC Annotations (* projected)
to MASC data in increments of roughly 100K
words. To date, validated or manually produced
annotations for 222K words have been made avail-
able.
The MASC project is itself producing annota-
tions for portions of the corpus forWordNet senses
and FrameNet frames and frame elements. To de-
rive maximal benefit from the semantic informa-
tion provided by these resources, the entire cor-
pus is also annotated and manually validated for
shallow parses (noun and verb chunks) and named
entities (person, location, organization, date and
time). Several additional types of annotation have
either been contracted by the MASC project or
contributed from other sources. The 220K words
ofMASC I and II include seventeen different types
of linguistic annotation4, shown in Table 2.
All MASC annotations, whether contributed or
produced in-house, are transduced to the Graph
Annotation Framework (GrAF) (Ide and Suder-
man, 2007) defined by ISO TC37 SC4?s Linguistic
Annotation Framework (LAF) (Ide and Romary,
2004). GrAF is an XML serialization of the LAF
abstract model of annotations, which consists of
a directed graph decorated with feature structures
providing the annotation content. GrAF?s primary
role is to serve as a ?pivot? format for transducing
among annotations represented in different for-
mats. However, because the underlying data struc-
ture is a graph, the GrAF representation itself can
serve as the basis for analysis via application of
4This includes WordNet sense annotations, which are not
listed in Table 2 because they are not applied to full texts; see
Section 3.1 for a description of the WordNet sense annota-
tions in MASC.
graph-analytic algorithms such as common sub-
tree detection.
The layering of annotations over MASC texts
dictates the use of a stand-off annotation repre-
sentation format, in which each annotation is con-
tained in a separate document linked to the pri-
mary data. Each text in the corpus is provided in
UTF-8 character encoding in a separate file, which
includes no annotation or markup of any kind.
Each file is associated with a set of GrAF standoff
files, one for each annotation type, containing the
annotations for that text. In addition to the anno-
tation types listed in Table 2, a document contain-
ing annotation for logical structure (titles, head-
ings, sections, etc. down to the level of paragraph)
is included. Each text is also associated with
(1) a header document that provides appropriate
metadata together with machine-processable in-
formation about associated annotations and inter-
relations among the annotation layers; and (2) a
segmentation of the primary data into minimal re-
gions, which enables the definition of different to-
kenizations over the text. Contributed annotations
are also included in their original format, where
available.
3.1 WordNet Sense Annotations
A focus of the MASC project is to provide corpus
evidence to support an effort to harmonize sense
distinctions in WordNet and FrameNet (Baker and
Fellbaum, 2009), (Fellbaum and Baker, to appear).
The WordNet and FrameNet teams have selected
for this purpose 100 common polysemous words
whose senses they will study in detail, and the
MASC team is annotating occurrences of these
words in the MASC. As a first step, fifty oc-
currences of each word are annotated using the
WordNet 3.0 inventory and analyzed for prob-
lems in sense assignment, after which the Word-
Net team may make modifications to the inven-
tory if needed. The revised inventory (which will
be released as part of WordNet 3.1) is then used to
annotate 1000 occurrences. Because of its small
size, MASC typically contains less than 1000 oc-
currences of a given word; the remaining occur-
rences are therefore drawn from the 15 million
words of the OANC. Furthermore, the FrameNet
team is also annotating one hundred of the 1000
sentences for each word with FrameNet frames
and frame elements, providing direct comparisons
of WordNet and FrameNet sense assignments in
70
attested sentences.5
For convenience, the annotated sentences are
provided as a stand-alone corpus, with the Word-
Net and FrameNet annotations represented in
standoff files. Each sentence in this corpus is
linked to its occurrence in the original text, so that
the context and other annotations associated with
the sentence may be retrieved.
3.2 Validation
Automatically-produced annotations for sentence,
token, part of speech, shallow parses (noun and
verb chunks), and named entities (person, lo-
cation, organization, date and time) are hand-
validated by a team of students. Each annotation
set is first corrected by one student, after which it
is checked (and corrected where necessary) by a
second student, and finally checked by both auto-
matic extraction of the annotated data and a third
pass over the annotations by a graduate student
or senior researcher. We have performed inter-
annotator agreement studies for shallow parses in
order to establish the number of passes required to
achieve near-100% accuracy.
Annotations produced by other projects and
the FrameNet and Penn Treebank annotations
produced specifically for MASC are semi-
automatically and/or manually produced by those
projects and subjected to their internal quality con-
trols. No additional validation is performed by the
ANC project.
The WordNet sense annotations are being used
as a base for an extensive inter-annotator agree-
ment study, which is described in detail in (Pas-
sonneau et al, 2009), (Passonneau et al, 2010).
All inter-annotator agreement data and statistics
are published along with the sense tags. The re-
lease also includes documentation on the words
annotated in each round, the sense labels for each
word, the sentences for each word, and the anno-
tator or annotators for each sense assignment to
each word in context. For the multiply annotated
data in rounds 2-4, we include raw tables for each
word in the form expected by Ron Artstein?s cal-
culate alpha.pl perl script6, so that the agreement
numbers can be regenerated.
5Note that several MASC texts have been fully annotated
for FrameNet frames and frame elements, in addition to the
WordNet-tagged sentences.
6http://ron.artstein.org/resources/calculate-alpha.perl
4 MASC Availability and Distribution
Like the OANC, MASC is distributed without
license or other restrictions from the American
National Corpus website7. It is also available
from the Linguistic Data Consortium (LDC)8 for
a nominal processing fee.
In addition to enabling download of the entire
MASC, we provide a web application that allows
users to select some or all parts of the corpus and
choose among the available annotations via a web
interface (Ide et al, 2010). Once generated, the
corpus and annotation bundle is made available to
the user for download. Thus, the MASC user need
never deal directly with or see the underlying rep-
resentation of the stand-off annotations, but gains
all the advantages that representation offers. The
following output formats are currently available:
1. in-line XML (XCES9), suitable for use with
the BNCs XAIRA search and access inter-
face and other XML-aware software;
2. token / part of speech, a common input for-
mat for general-purpose concordance soft-
ware such as MonoConc10, as well as the
Natural Language Toolkit (NLTK) (Bird et
al., 2009);
3. CONLL IOB format, used in the Confer-
ence on Natural Language Learning shared
tasks.11
5 Tools
The ANC project provides an API for GrAF an-
notations that can be used to access and manip-
ulate GrAF annotations directly from Java pro-
grams and render GrAF annotations in a format
suitable for input to the open source GraphViz12
graph visualization application.13 Beyond this, the
ANC project does not provide specific tools for
use of the corpus, but rather provides the data in
formats suitable for use with a variety of available
applications, as described in section 4, together
with means to import GrAF annotations into ma-
jor annotation software platforms. In particular,
the ANC project provides plugins for the General
7http://www.anc.org
8http://www.ldc.upenn.edu
9XML Corpus Encoding Standard, http://www.xces.org
10http://www.athel.com/mono.html
11http://ifarm.nl/signll/conll
12http://www.graphviz.org/
13http://www.anc.org/graf-api
71
Architecture for Text Engineering (GATE) (Cun-
ningham et al, 2002) to input and/or output an-
notations in GrAF format; a ?CAS Consumer?
to enable using GrAF annotations in the Un-
structured Information Management Architecture
(UIMA) (Ferrucci and Lally, 2004); and a corpus
reader for importing MASC data and annotations
into NLTK14.
Because the GrAF format is isomorphic to in-
put to many graph-analytic tools, existing graph-
analytic software can also be exploited to search
and manipulate MASC annotations. Trivial merg-
ing of GrAF-based annotations involves simply
combining the graphs for each annotation, after
which graph minimization algorithms15 can be ap-
plied to collapse nodes with edges to common
subgraphs to identify commonly annotated com-
ponents. Graph-traversal and graph-coloring al-
gorithms can also be applied in order to iden-
tify and generate statistics that could reveal in-
teractions among linguistic phenomena that may
have previously been difficult to observe. Other
graph-analytic algorithms ? including common
sub-graph analysis, shortest paths, minimum span-
ning trees, connectedness, identification of artic-
ulation vertices, topological sort, graph partition-
ing, etc. ? may also prove to be useful for mining
information from a graph of annotations at multi-
ple linguistic levels.
6 Community Contributions
The ANC project solicits contributions of anno-
tations of any kind, applied to any part or all of
the MASC data. Annotations may be contributed
in any format, either inline or standoff. All con-
tributed annotations are ported to GrAF standoff
format so that they may be used with other MASC
annotations and rendered in the various formats
the ANC tools generate. To accomplish this, the
ANC project has developed a suite of internal tools
and methods for automatically transducing other
annotation formats to GrAF and for rapid adapta-
tion of previously unseen formats.
Contributions may be emailed to
anc@cs.vassar.edu or uploaded via the
ANC website16. The validity of annotations
and supplemental documentation (if appropriate)
are the responsibility of the contributor. MASC
14Available in September, 2010.
15Efficient algorithms for graph merging exist; see,
e.g., (Habib et al, 2000).
16http://www.anc.org/contributions.html
users may contribute evaluations and error reports
for the various annotations on the ANC/MASC
wiki17.
Contributions of unvalidated annotations for
MASC and OANC data are also welcomed and are
distributed separately. Contributions of unencum-
bered texts in any genre, including stories, papers,
student essays, poetry, blogs, and email, are also
solicited via the ANC web site and the ANC Face-
Book page18, and may be uploaded at the contri-
bution page cited above.
7 Conclusion
MASC is already the most richly annotated corpus
of English available for widespread use. Because
the MASC is an open resource that the commu-
nity can continually enhance with additional an-
notations and modifications, the project serves as a
model for community-wide resource development
in the future. Past experience with corpora such
as the Wall Street Journal shows that the commu-
nity is eager to annotate available language data,
and we anticipate even greater interest in MASC,
which includes language data covering a range of
genres that no existing resource provides. There-
fore, we expect that as MASC evolves, more and
more annotations will be contributed, thus creat-
ing a massive, inter-linked linguistic infrastructure
for the study and processing of current American
English in its many genres and varieties. In addi-
tion, by virtue of its WordNet and FrameNet anno-
tations, MASC will be linked to parallel WordNets
and FrameNets in languages other than English,
thus creating a global resource for multi-lingual
technologies, including machine translation.
Acknowledgments
The MASC project is supported by National
Science Foundation grant CRI-0708952. The
WordNet-FrameNet algnment work is supported
by NSF grant IIS 0705155.
References
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguistic
17http://www.anc.org/masc-wiki
18http://www.facebook.com/pages/American-National-
Corpus/42474226671
72
Annotation Workshop, pages 125?129, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, 1st edition.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of ACL?02.
Christiane Fellbaum and Collin Baker. to appear.
Aligning verbs in WordNet and FrameNet. Linguis-
tics.
David Ferrucci and Adam Lally. 2004. UIMA: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3-4):327?348.
Michel Habib, Christophe Paul, and Laurent Viennot.
2000. Partition refinement techniques: an interest-
ing algorithmic tool kit. International Journal of
Foundations of Computer Science, 175.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural Language Engineering, 10(3-4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the Linguistic Annotation Workshop, pages
1?8, Prague, Czech Republic, June. Association for
Computational Linguistics.
Nancy Ide, Collin Baker, Christiane Fellbaum, Charles
Fillmore, and Rebecca Passonneau. 2008. MASC:
The Manually Annotated Sub-Corpus of American
English. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
Nancy Ide, Keith Suderman, and Brian Simms. 2010.
ANC2Go: A web application for customized cor-
pus creation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Valletta, Malta, May. European Lan-
guage Resources Association.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and
Nancy Ide. 2009. Making sense of word sense
variation. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 2?9, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Passonneau, Ansaf Salleb-Aouissi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense an-
notation of polysemous words by multiple annota-
tors. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
73
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 873?883,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Frames to Predict Stock Price Movement
Boyi Xie, Rebecca J. Passonneau, Leon Wu
Center for Computational Learning Systems
Columbia University
New York, NY USA
(bx2109|becky|leon.wu)@columbia.edu
Germa?n G. Creamer
Howe School of Technology Management
Stevens Institute of Technology
Hoboken, NJ USA
gcreamer@stevens.edu
Abstract
Semantic frames are a rich linguistic re-
source. There has been much work
on semantic frame parsers, but less that
applies them to general NLP problems.
We address a task to predict change in
stock price from financial news. Seman-
tic frames help to generalize from spe-
cific sentences to scenarios, and to de-
tect the (positive or negative) roles of spe-
cific companies. We introduce a novel tree
representation, and use it to train predic-
tive models with tree kernels using sup-
port vector machines. Our experiments
test multiple text representations on two
binary classification tasks, change of price
and polarity. Experiments show that fea-
tures derived from semantic frame pars-
ing have significantly better performance
across years on the polarity task.
1 Introduction
A growing literature evaluates the financial effects
of media on the market (Tetlock, 2007; Engel-
berg and Parsons, 2011). Recent work has applied
NLP techniques to various financial media (con-
ventional news, tweets) to detect sentiment in con-
ventional news (Devitt and Ahmad, 2007; Haider
and Mehrotra, 2011) or message boards (Chua
et al, 2009), or discriminate expert from non-
expert investors in financial tweets (Bar-Haim et
al., 2011). With the exception of Bar-Haim et al
(2011), these NLP studies have relied on small
corpora of hand-labeled data for training or evalu-
ation, and the connection to market events is done
indirectly through sentiment detection. We hy-
pothesize that conventional news can be used to
predict changes in the stock price of specific com-
panies, and that the semantic features that best
represent relevant aspects of the news vary across
On Wednesday, April 11th, 2012, Google Inc announced
its first
 quarterly earnings report, a week before the April20 options contracts expiration in contrast to its history
of reporting a day before monthly options expirations.
The stock price of Google surged 3.85% from April
10th?s $626.86 to 12th?s $651.01. On Friday, April 13th,
news reported Oracle Corp would sue
 Google Inc ,
claiming Google?s Android operating system tramples its intellectual property rights . Jury selection was set for
the next Monday. Google?s stock price tumbled 4.06% on
Friday, and continued to drop in the following week.
Figure 1: Summary of financial news items per-
taining to Google in April, 2012.
market sectors. To test this hypothesis, we use
price information to label data from six years of
financial news. Our experiments test several doc-
ument representations for two binary classification
tasks, change of price and polarity. Our main con-
tribution is a novel tree representation based on
semantic frame parses that performs significantly
better than enriched bag-of-words vectors.
Figure 1 shows a constructed example based
on extracts from financial news about Google in
April, 2012. It illustrates how a series of events
reported in the news precedes and potentially
predicts a large change in Google?s stock price.
Google?s early announcement of quarterly earn-
ings possibly presages trouble, and its stock price
falls soon after reports of a legal action against
Google by Oracle. To produce a coherent story,
the original sentences were edited for Figure 1,
but they are in the style of actual sentences from
our dataset. Accurate detection of events and re-
lations that might have an impact on stock price
should benefit from document representation that
captures sentiment in lexical items (e.g., aggres-
sive) combined with the conceptual relations cap-
tured by FrameNet (Ruppenhofer and Rehbein,
2012). A frame is a lexical semantic representa-
873
tion of the conceptual roles played by parts of a
clause, and relates different lexical items (e.g., re-
port, announce) to the same situation type. In the
figure, some of the words that evoke frames have
been underlined, and role fillers are outlined by
boxes or ovals. Sentiment words are in italics.
To the best of our knowledge, this paper is
the first to apply semantic frames in this do-
main. On the polarity task, the semantic frame fea-
tures encoded as trees perform significantly better
across years and sectors than bag-of-words vectors
(BOW), and outperform BOW vectors enhanced
with semantic frame features, and a supervised
topic modeling approach. The results on the price
change task show the same trend, but are not sta-
tistically significant, possibly due to the volatility
of the market in 2007 and the following several
years. Yet even modest predictive performance
on both tasks could have an impact, as discussed
below, if incorporated into financial models such
as Rydberg and Shephard (2003). We first dis-
cuss the motivation and related work. Section 4
presents vector-based and tree-based features from
semantic frame parses, and section 5 describes our
dataset. The experimental design and results ap-
pear in the following section, followed by discus-
sion and conclusions.
2 Motivation
Financial news is a rich vein for NLP applica-
tions to mine. Many news organizations that fea-
ture financial news, such as Reuters, the Wall
Street Journal and Bloomberg, devote significant
resources to the analysis of corporate news.
Much of the data that would support studies of
a link between the news media and the market are
publicly available. As pointed out by Tetlock et
al. (2008), linguistic communication is a poten-
tially important source of information about firms?
fundamental values. Because very few stock mar-
ket investors directly observe firms? production ac-
tivities, they get most of their information sec-
ondhand. Their three main sources are analysts?
forecasts, quantifiable publicly disclosed account-
ing variables, and descriptions of firms? current
and future profit-generating activities. If analyst
and accounting variables are incomplete or biased
measures of firms? fundamental values, linguis-
tic variables may have incremental explanatory
power for firms? future earnings and returns.
Consider the following sentences:
Oracle sued Google in August 2010, saying
Google?s Android mobile operating system in-
fringes its copyrights and patents for the Java pro-
gramming language. (a)
Oracle has accused Google of violating its in-
tellectual property rights to the Java programming
language. (b)
Oracle has blamed Google and alleged that the
latter has committed copyright infringement re-
lated to Java programming language held by Ora-
cle. (c)
Oracle?s Ellison says couldn?t sway Google on
Java. (d)
Sentences a, b and c are semantically similar,
but lexically rather distinct: the shared words are
the company names and Java (programming lan-
guage). Bag-of-Words (BOW) document repre-
sentation is difficult to surpass for many document
classification tasks, but cannot capture the de-
gree of semantic similarity among these sentences.
Methods that have proven successful for para-
phrase detection (Deerwester et al, 1990; Dolan
et al, 2004), as in the main clauses of b and
c, include latent variable models that simultane-
ously capture the semantics of words and sen-
tences, such as latent semantic analysis (LSA) or
latent Dirichlet alocation (LDA). However, our
task goes beyond paraphrase detection. The first
three sentences all indicate an adversarial relation
of Oracle to Google involving a negative judge-
ment. It would be useful to capture the similarities
among all three of these sentences, and to distin-
guish the role of each company (who is suing and
who is being sued). Further, these three sentences
potentially have a greater impact on market per-
ception of Google in contrast to a sentence like d,
that refers to the same conflict more indirectly, and
whose main clause verb is say. We hypothesize
that semantic frames can address these issues.
Most of the NLP literature on semantic frames
addresses how to build robust semantic frame
parsers, with intrinsic evaluation against gold stan-
dard parses. There have been few applications of
semantic frame parsing for extrinsic tasks. To test
for measurable benefits of semantic frame parsing,
this paper poses the following questions:
1. Are semantic frames useful for document
representation of financial news?
2. What aspects of frames are most useful?
3. What is the relative performance of document
representation that relies on frames?
874
4. What improvements could be made to best
exploit semantic frames?
Our work is not aimed at investment profit.
Rather, we investigate whether computational lin-
guistic methodologies can improve our under-
standing of a company?s fundamental market
value, and whether linguistic information derived
from news produces a consistent enough result to
benefit more comprehensive financial models.
3 Related Work
NLP has recently been applied to financial text
for market analysis, primarily using bag-of-
words (BOW) document representation. Luss
and d?Aspremont (2008) use text classification to
model price movements of financial assets on a
per-day basis. They try to predict the direction
of return, and abnormal returns, defined as an ab-
solute return greater than a predefined threshold.
Kogan et al (2009) address a text regression prob-
lem to predict the financial risk of investment in
companies. They analyze 10-K reports to predict
stock return volatility. They also predict whether
a company will be delisted following its 10-K re-
port. Ruiz et al (2012) correlate text with finan-
cial time series volume and price data. They find
that graph centrality measures like page rank and
degree are more strongly correlated to both price
and traded volume for an aggregation of similar
companies, while individual stocks are less corre-
lated. Lavrenko et al (2000) present an approach
to identify news stories that influence the behavior
of financial markets, and predict trends in stock
prices based on the content of news stories that
precede the trends. Luss and d?Aspremont (2008)
and Lavrenko et al (2000) both point out the de-
sire for document feature engineering as future re-
search directions. We explore a rich feature space
that relies on frame semantic parsing.
Sentiment analysis figures strongly in NLP
work on news. General Inquirer (GI), a content
analysis program, is used to quantify pessimism of
news in Tetlock (2007) and Tetlock et al (2008).
Other resources for sentiment detection include
the Dictionary of Affect in Language (DAL) to
score the prior polarity of words, as in Agarwal
et al (2011) on social media data. Our study in-
corporates DAL scores along with other features.
FrameNet is a rich lexical resource (Fillmore et
al., 2003), based on the theory of frame seman-
tics (Fillmore, 1976). There is active research
Category Features Value type
Frame F, FT, FE N
attributes wF, wFT, wFE R?0
BOW UniG, BiG, TriG N
wUniG, wBiG, wTriG R?0
pDAL all-Pls, all-Act, all-Img R??=0,std=1
VB-Pls, VB-Act, VB-Img R??=0,std=1
JJ-Pls, JJ-Act, JJ-Img R??=0,std=1
RB-Pls, RB-Act, RB-Img R??=0,std=1
Table 1: FWD features (Frame, bag-of-Words,
part-of-speech DAL score) and their value types.
to build more accurate parsers (Das and Smith,
2011; Das and Smith, 2012). Semantic role label-
ing using FrameNet has been used to identify an
opinion with its holder and topic (Kim and Hovy,
2006). For deep representation of sentiment anal-
ysis, Ruppenhofer and Rehbein (2012) propose
SentiFrameNet.
Our work addresses classification tasks that
have potential relevance to an influential financial
model (Rydberg and Shephard, 2003). This model
decomposes stock price analysis of financial data
into a three-part ADS model - activity (a binary
process modeling the price move or not), direction
(another binary process modeling the direction of
the moves) and size (a number quantifying the size
of the moves). Our two binary classification tasks
for news, price change and polarity, are analogous
to their activity and direction. In contrast to the
ADS model, our approach does not calculate the
conditional probability of each factor. At present,
our goal is limited to the determination of whether
NLP features can uncover information from news
that could help predict stock price movement or
support analysts? investigations.
4 Methods
We propose two approaches for the use of seman-
tic frames. The first is a rich vector space based
on semantic frames, word forms and DAL affect
scores. The second is a tree representation that
encodes semantic frame features, and depends on
tree kernel measures for support vector machine
classification. The semantic parses of both meth-
ods are derived from SEMAFOR1 (Das and Smith,
2012; Das and Smith, 2011), which solves the se-
mantic parsing problem by rule-based target iden-
tification, log-linear model based frame identifica-
tion and frame element filling.
1http://www.ark.cs.cmu.edu/SEMAFOR.
875
Frame (F) Judgment comm. Commerce buy
accuse buy
Target (FT) sue purchase
charge bid
Frame COMMUNICATOR BUYER
Element EVALUEE SELLER
(FE) REASON GOODS
Table 2: Sample frames.
4.1 Semantic Frame based FWD Features
Table 1 lists 24 types of features, including seman-
tic Frame attributes, bag-of-Words, and scores for
words in the Dictionary of Affect in Language by
part of speech (pDAL). We refer to these features
as FWD features throughout the paper. FWD fea-
tures are used alone and in combinations.
FrameNet defines hundreds of frames, each of
which represents a scenario associated with se-
mantic roles, or frame elements, that serve as
participants in the scenario the frame signifies.
Table 2 shows two frames. The frame Judg-
ment communication (JC or Judgment comm. in
the rest of the paper) represents a scenario in
which a COMMUNICATOR communicates a judg-
ment of an EVALUEE for some REASON. It is
evoked by (target) words such as accuse or sue.
Here we use F for the frame name, FT for the
target words, and FE for frame elements. We use
both frequency and weighted scores. For exam-
ple, we define idf -adjusted weighted frame fea-
tures, such as wF for attribute F in document d as
wFF,d = f(F, d) ? log |D||d?D:F?d| , where f(F, d)is the frequency of frame F in d, D is the whole
document set and |?| is the cardinality operator.
Bag-of-Words features include term frequency
and tfidf of unigrams, bigrams, and trigrams.
DAL (Dictionary of Affect in Language) is a
psycholinguistic resource to measure the emo-
tional meaning of words and texts (Whissel,
1989). It includes 8,742 words that were anno-
tated for three dimensions: Pleasantness (Pls), Ac-
tivation (Act), and Imagery (Img). Agarwal et
al. (2009) introduced part-of-speech specific DAL
features for sentiment analysis. We follow their
approach by averaging the scores for all words,
verb only, adjective only, and adverb only words.
Feature values are normalized to mean of zero and
standard deviation of one.
4.2 SemTree Feature Space and Kernels
We propose SemTree as another feature space to
encode semantic information in trees. SemTree
can distinguish the roles of each company of in-
terest, or designated object (e.g. who is suing and
who is being sued).
4.2.1 Construction of Tree Representation
The semantic frame parse of a sentence is a forest
of trees, each of which corresponds to a semantic
frame. SemTree encodes the original frame struc-
ture and its leaf words and phrases, and highlights
a designated object at a particular node as follows.
For each lexical item (target) that evokes a frame, a
backbone is found by extracting the path from the
root to the role filler mentioning a designated ob-
ject; the backbone is then reversed to promote the
designated object. If multiple frames have been
assigned to the same designated object, their back-
bones are merged. Lastly, the frame elements and
frame targets are inserted at the frame root.
The top of Figure 2 shows the semantic parse
for sentence a from section 2; we use it to illus-
trate tree construction for designated object Ora-
cle. The parse has two frames (Figure 2-(1)&(2)),
one corresponding to the main clause (verb sue),
and the other for the tenseless adjunct (verb say).
The reversed paths extracted from each frame root
to the designated object Oracle become the back-
bones (Figures 2-(3)&(4)). After merging the two
backbones we get the resulting SemTree, as shown
in Figure 2-(5). By the same steps, this sentence
would also yield a SemTree with Google at the
root, in the role of EVALUEE.
4.2.2 Kernels and Tree Substructures
The tree kernel (Moschitti, 2006; Collins and
Duffy, 2002) is a function of tree similarity, based
on common substructures (tree fragments). There
are two types of substructures. A subtree (ST) is
defined as any node of a tree along with all its de-
scendants. A subset tree (SST) is defined as any
node along with its immediate children and, op-
tionally, part or all of the children?s descendants.
Each tree is represented by a d dimensional vec-
tor where the i?th component counts the number
of occurrences of the i?th tree fragment.
Define the function hi(T ) as the number of
occurrences of the i?th tree fragment in tree
T , so that T is now represented as h(T ) =
(h1(T ), h2(T ), ..., hd(T )). We define the set of
nodes in tree T1 and T2 as NT1 and NT2 respec-
tively. We define the indicator function Ii(n) to be
1 if subtree i is seen rooted at node n, and 0 oth-
erwise. It follows that hi(T1) = ?n1?NT1 Ii(n1)
876
Designated object: Oracle (ORCL)
Sentence: Oracle sued Google in August 2010, saying Google?s Android mobile operating system infringes its copyrights and patents for the Java pro-
gramming language.
SRL: [OracleJC.FE.Communicator,Stmt.FE.Speaker] [suedJC.Target] [GoogleJC.FE.Evaluee] in August 2010, [sayingStmt.Target]
[Googles? Android mobile operating system infringes its copyrights and patents for the Java programming languageStmt.FE.Message].
(1) Judgment comm.
FE.Evaluee
GOOG
FE.Communicator
ORCL
Judgment comm.Target
sue
(2) Statement
FE.Message
GOOG?s Android ... language
FE.Speaker
ORCL
Statement.Target
say
(3) ORCL
FE.Communicator
Judgment comm.
(4) ORCL
FE.Speaker
Statement
(5) ORCL
Speaker
Statement
FE.MessageFE.SpeakerStatement.Target
say
Communicator
Judgment comm.
FE.EvalueeFE.CommunicatorJudgment comm.Target
sue
Figure 2: Constructing a tree representation for the designated object Oracle in sentence shown.
and hi(T2) = ?n2?NT2 Ii(n2). Their similaritycan be efficiently computed by the inner product,
K(T1, T2) = h(T1) ? h(T2)
=
?
i hi(T1)hi(T2)
=
?
i(
?
n1?NT1
Ii(n1))(
?
n2?NT2
Ii(n2))
=
?
n1?NT1
?
n2?NT2
?
i Ii(ni)Ii(n2)
=
?
n1?NT1
?
n2?NT2
?(n1, n2)
where ?(n1, n2) is the number of common frag-
ments rooted in the nodes n1 and n2. If the pro-
ductions of these two nodes (themselves and their
immediate children) differ, ?(n1, n2) = 0; other-
wise iterate their children recursively to evaluate
?(n1, n2) =
?|children|
j (?+?(c
j
n1 , cjn2)) , where
? = 0 for ST kernel and ? = 1 for SST kernel.
The kernel computational complexity is
O(|NT1 | ? |NT2 |), where all pairwise compar-
isons are carried out between T1 and T2. However,
there are fast algorithms for kernel computation
that run in linear time on average, either by
dynamic programming (Collins and Duffy, 2002),
or pre-sorting production rules before training
(Moschitti, 2006). We use the latter.
5 Dataset
We use publicly available financial news from
Reuters from January 2007 through August 2012.
This time frame includes a severe economic down-
turn in 2007-2010 followed by a modest recovery
in 2011-2012.
An information extraction pipeline is used to
pre-process the data. News full text is extracted
from HTML. The timestamp of the news is ex-
tracted for a later alignment with stock price infor-
mation, which will be discussed in section 6. The
company mentioned is identified by a rule-based
matching of a finite list of companies.
There are a total of 10 sectors in the Global In-
dustry Classification Standard (GICS), an industry
taxonomy used by the S&P 500.2 To explore our
approach for this domain, we select three sectors
for our experiment: Telecommunication Services
(TS, the sector with the smallest number of com-
panies), Information Technology (IT), and Con-
sumer Staples (CS), due to our familiarity with the
companies in these sectors and an expectation of
different characteristics they may exhibit. In the
expectation there would be semantic differences
associated with these sectors, experiments are per-
formed independently for each sector. There are
also differences in the number of companies in the
sector, and the amount of news.
We bin news articles by sector. We remove ar-
ticles that only list stock prices or only show ta-
bles of accounting reports. The first preprocess-
ing step is to extract sentences that mention the
2Standard & Poor?s 500 is an equity market index that
includes 500 U.S. leading companies in leading industries.
877
CS (N=40) IT (N=69) TS (N=8)
avg # news 5,702?749 13446?1,272 2,177?188
avg # sentences 16,090?2,316 48,929?5,927 6,970?1,383
avg # com./sent. 1.07?0.01 1.06?0.20 1.14?0.03
avg # total 17,131?2,339 51,306?8,637 7,947?1,576
Table 3: Data statistics of mean and standard devi-
ation by year from January 2007 to August 2012,
for three sectors, with the number of companies.
relevant companies. Each data instance is a sen-
tence and one of the target companies it mentions.
Table 3 summarizes the data statistics. For exam-
ple, the consumer staples sector has 40 companies.
It has an average of 5,702 news articles (16,090
sentences) per year. Each sentence that mentions
a consumer staple company mentions 1.07 com-
panies on average. On average, this sector has
17,131 instances per year.
6 Experiments
Our current experiments are carried out for each
year, training on one year and testing on the next.
The choice to use a coarse time interval with no
overlap was an expedience to permit more numer-
ous exploratory experiments, given the computa-
tional resources these experiments require. We test
the influence of news to predict (1) a change in
stock price (change task), and (2) the polarity of
change (increase vs. decrease; polarity task). Ex-
periments evaluate the FWD and SemTree feature
spaces compared to two baselines: bag-of-words
(BOW) and supervised latent Dirichlet alocation
(sLDA) (Blei and McAuliffe, 2007). BOW in-
cludes features of unigram, bigram and trigram.
sLDA is a statistical model to classify documents
based on LDA topic models, using labeled data. It
has been applied to and shown good performance
in topical text classification, collaborative filter-
ing, and web page popularity prediction problems.
6.1 Labels, Evaluation Metrics, and Settings
We align publicly available daily stock price data
from Yahoo Finance with the Reuters news us-
ing a method to avoid back-casting. In particular,
we use the daily adjusted closing price - the price
quoted at the end of a trading day (4PM US East-
ern Time), then adjusted by dividends, stock split,
and other corporate actions. We create two types
of labels for news documents using the price data,
to label the existence of a change and the direc-
tion of change. Both tasks are treated as binary
classification problems. Based on the finding of
a one-day delay of the price response to the in-
formation embedded in the news by Tetlock et al
(2008), we use ?t = 1 in our experiment. To
constrain the number of parameters, we also use a
threshold value (r) of a 2% change, based on the
distribution of price changes across our data. In
future work, this could be tuned to sector or time.
change=
{
+1 if |pt(0)+?t?pt(?1)|pt(?1) > r
?1 otherwise
polarity=
{
+1 if pt(0)+?t > pt(?1) and change = +1
?1 if pt(0)+?t < pt(?1) and change = +1
pt(?1) is the adjusted closing price at the end of
the last trading day, and pt(0)+?t is the price of
the end of the trading day after the ?t day delay.
Only the instances with changes are included in
the polarity task.
There is high variance across years in the pro-
portion of positive labels, and often highly skewed
classes in one direction or the other. The average
ratios of +/- classes for change and polarity over
the six years? data are 0.73 (std=0.35) and 1.12
(std=0.25), respectively. Because the time frame
for our experiments includes an economic crisis
followed by a recovery period, we note that the
ratio between increase and decrease of price flips
between 2007, where it is 1.40, and 2008, where it
is 0.71. Accuracy is very sensitive to skew: when a
class has low frequency, accuracy can be high us-
ing a baseline that makes prediction on the major-
ity class. Given the high data skew, and the large
changes from year to year in positive versus nega-
tive skew, we use a more robust evaluation metric.
Our evaluation relies on the Matthews corre-
lation coefficient (MCC, also known as the ?-
coefficient) (Matthews, 1975) to avoid the bias of
accuracy due to data skew, and to produce a ro-
bust summary score independent of whether the
positive class is skewed to the majority or minor-
ity. In contrast to f-measure, which is a class-
specific weighted average of precision and recall,
and whose weighted version depends on a choice
of whether the class-specific weights should come
from the training or testing data, MCC is a sin-
gle summary value that incorporates all 4 cells of
a 2 ? 2 confusion matrix (TP, FP, TN and FN for
True or False Positive or Negative). We have also
observed that MCC has a lower relative standard
deviation than f-measure.
For a 2 ? 2 contingency table, MCC corre-
sponds to the square root of the average ?2 statis-
tic ??2/n, with values in [-1,1]. It has been sug-
878
Change
test years BOW sLDA FWD SemTreeFWD
Consumer Staples
2008-2010 0.1015 0.0774 0.1079 0.1426
2011-2012 0.1663 0.1203 0.1664 0.1736
5 years 0.1274 0.0945 0.1313 0.1550
Information Technology
2008-2010 0.0580 0.0585 0.0701 0.0846
2011-2012 0.0894 0.0681 0.1076 0.1273
5 years 0.0705 0.0623 0.0851 0.1017
Telecommunication Services
2008-2010 0.1501 0.1615 0.1497 0.2409
2011-2012 0.2256 0.2084 0.2191 0.4009
5 years 0.1803 0.1803 0.1774 0.3049
Polarity
Consumer Staples
2008-2010 0.0359 0.0383 0.0956 0.1054
2011-2012 0.0938 0.0270 0.1131 0.1285
5 years 0.0590 0.0338 0.1026 0.1147
p-value >>0.1000 0.0918 0.0489
Information Technology
2008-2010 0.0551 0.0332 0.0697 0.0763
2011-2012 0.0591 0.0516 0.0764 0.0857
5 years 0.0567 0.0405 0.0723 0.0801
p-value 0.0626 0.0948 0.0103
Telecommunication Services
2008-2010 0.0402 0.0464 0.0821 0.0745
2011-2012 0.0366 0.0781 0.0611 0.0809
5 years 0.0388 0.0591 0.0737 0.0770
p-value >>0.1000 0.0950 0.0222
Table 4: Average MCC for the change and polarity
tasks by feature representation, for 2008-2010; for
2011-2012; for all 5 years and associated p-values
of ANOVAs for comparison to BOW.
gested as one of the best methods to summarize
into a single value the confusion matrix of a binary
classification task (Jurman and Furlanello, 2010;
Baldi et al, 2000). Given the confusion matrix(TP FN
FP TN
) :
MCC = TP ?TN?FP ?FN?
(TP+FP )(TP+FN)(TN+FP )(TN+FN)
.
All sentences with at least one company men-
tion are used for the experiment. We remove
stop words and use Stanford CoreNLP for part-
of-speech tagging and named entity recognition.
Models are constructed using linear kernel sup-
port vector machines for both classification tasks.
SVM-light with tree kernels3 (Joachims, 2006;
Moschitti, 2006) is used for both the FWD and
SemTree feature spaces.
6.2 Results
Table 4 shows the mean MCC values for each task,
for each sector. Separate means are shown for
the test years of financial crisis (2008-2010) and
economic recovery (2011-2012) to highlight the
differences in performance that might result from
market volatility.
3SVM-light: http://svmlight.joachims.org and Tree
Kernels in SVM-light: http://disi.unitn.it/moschitti/Tree-
Kernel.htm.
pos. 1 dow, investors, index, retail, data
pos. 2 costs, food, price, prices, named entity 4
neu. 1 q3, q1, nov, q2, apr
neu. 2 cents, million, share, year, quarter
neg. 1 cut, sales, prices, hurt, disappointing
neg. 2 percent, call, company, fell, named entity 7
Table 5: Sample sLDA topics for consumer staples
for test year 2010 (train on 2009), polarity task.
SemTree combined with FWD (SemTreeFWD)
generally gives the best performance in both
change and polarity tasks. SemTree results here
are based on the subset tree (SST) kernel, be-
cause of its greater precision in computing com-
mon frame structures and consistently better per-
formance over the subtree (ST) kernel. SemTree
also provides interpretable features for manual
analysis as discussed in the next section.
Analysis of Variance (ANOVA) tests were per-
formed on the full 5 years for each sector, to com-
pare each feature representation as a predictor of
MCC score with the baseline BOW. The ANOVAs
yield the p-values shown in Table 4. There were no
significant differences from BOW on the change
task. For polarity detection, SemTreeFWD was
significantly better than BOW for each sector (see
boldface p-values). No other method was sig-
nificantly better than BOW, although FWD ap-
proaches significance on all sectors, and sLDA ap-
proaches significance on IT.
sLDA has promising MCC scores for the
telecommunication sector, which has only 8 com-
panies, thus many fewer data instances. Table 5
displays a sample of sLDA topics with good per-
formance on polarity for the consumer staples sec-
tor for training year 2009. The positive topics are
related to stock index details and retail data. The
negative topics contain many words with negative
sentiment (e.g., hurt, disappointing).
7 Discussion
7.1 Semantic Parse Quality
In general, SEMAFOR parses capture most of
the important frames for our purposes. There is,
however, significant room for improvement. On
a small, randomly selected sample of sentences
from all three sectors, two of the authors working
independently evaluated the semantic parses, with
approximately 80% agreement. Some of the in-
accuracies in frame parses result from errors prior
to the SEMAFOR parse, such as tokenization or
879
+ (Target(jump))
+ (RECIPIENT(Receiving))
+ (VICTIM(Defend))
+ (PERCEIVER AGENTIVE(Perception active(Target)
(PERCEIVER AGENTIVE)(PHENOMENON)))
+ (DONOR(Giving(Target)(THEME)(DONOR)))
+ (Target(beats))
...
- (PHENOMENON(Perception active(Target)(PERCEIVER
AGENTIVE)(PHENOMENON)))
- (TRIGGER(Response))
- (Target(cuts))
- (VICTIM(Cause harm(Target(hurt))(VICTIM)))
Figure 3: Best performing SemTree fragments for
increase (+) and decrease (-) of price for consumer
staples sector across training years.
dependency parsing errors. The average sentence
length for the sample was 33.3 words, with an av-
erage of 14 frames per sentence, 3 of them with a
GICS company as a role filler. Because SemTree
encodes only the frames containing a designated
object (company), these are the frames we eval-
uated. On average, about half the frames with
a designated object were correct, and two thirds
of those frames we judged to be important. Be-
sides errors due to incorrect tokenization or depen-
dency parsing, we observed that about 8% to 10%
of frames were incorrectly assigned to due word
sense ambiguity.
7.2 Feature Analysis
The experimental results show the SemTree space
to be the one representation tested here that is sig-
nificantly better than BOW, but only for the po-
larity task. Post hoc analysis indicates this may
be due to the aptness of semantic frame parsing
for polarity. Limitations in our treatment of time
point to directions for improvement regarding the
change task.
Some strengths of our approach are the separate
treatment of different sectors, and the benefits of
SemTree features. To analyze which were the best
performing features within sectors, we extracted
the best performing frame fragments for the po-
larity task using a tree kernel feature engineering
method presented in Pighin and Moschitti (2009).
The algorithm selects the most relevant features in
accordance with the weights estimated by SVM,
and uses these features to build an explicit repre-
sentation of the kernel space. Figure 3 shows the
best performing SemTree fragments of the polar-
ity task for the consumer staples sector.
Recall that we hypothesized differences in
semantic frame features across sectors. This
shows up as large differences in the strength
of features across sectors. More strikingly, the
same feature can differ in polarity across sec-
tors. For example, in consumer staples, (EVAL-
UEE(Judgment communication)) has positive po-
larity, compared with negative polarity in informa-
tion technology sector. The examples we see indi-
cate that the positive cases pertain to aggressive re-
tail practices that lead to lawsuits with only small
fines, but whose larger impact benefits the bottom
line. A typical case is the sentence, The plaintiffs
accused Wal-Mart of discriminating against dis-
abled customers by mounting ?point-of-sale? ter-
minals in many stores at elevated heights that can-
not be reached. Lawsuits in the IT sector, on the
other hand, are often about technology patent dis-
putes, and are more negative, as illustrated by our
example sentence in Figure 2.
SemTree features capture the differences be-
tween semantic roles for the same frame, and be-
tween the same semantic role in different frames.
For example, the PERCEIVER AGENTIVE role of
the Perception active frame contributes to predic-
tion of an increase in price, as in R.J. Reynolds
is watching this situation closely and will respond
as appropriate. Conversely, a company that fills
the PHENOMENON role of the same frame con-
tributes to prediction of a price decrease, as in In-
vestors will get a clearer look at how the market
values the Philip Morris tobacco businesses when
Altria Group Inc. ?when-issued? shares begin
trading on Tuesday. When a company fills the
VICTIM role in the Cause harm frame, this can
predict a decrease in price, as in Hershey has
been hurt by soaring prices for cocoa, energy and
other commodities, whereas filling the VICTIM
role in the Defend frame is associated with an in-
crease in price, as in At Berkshire?s annual share-
holder meeting earlier this month, Warren Buffett
defended Wal-Mart , saying the scandal did not
change his opinion of the company.
One weakness of our approach that we dis-
cussed above is that there is a strong effect of
time that we do not address. The same SemTree
feature can be predictive for one time period and
not for another. (GOODS(Commerce sell)) is re-
lated to a decrease in price for 2008 and 2009 but
to an increase in price for 2010-2012. There is
clearly an influence of the overall economic con-
text that we do not take into account. For example,
880
the practices of acquiring or selling a business are
different in downturning versus recovering mar-
kets. An important observation of the MCC val-
ues, especially in the case of SemTreeFWD is that
MCC increases during the years 2011-2012. We
attribute this change to the difficulty of predicting
stock price trends when there is the high volatil-
ity typical of a financial crisis. The effect of news
on volatility, however, can be explored indepen-
dently. For example, Creamer et al (2012) detect
a strong association.
Another weakness of our approach is that we
take sentences out of context, which can lead
to prediction errors. For example, the sentence
Longs? real estate assets alone are worth some
$2.9 billion, or $71.50 per share, Ackman wrote,
meaning that CVS would essentially be paying
for real estate, but gaining Longs? pharmacy ben-
efit management business and retail operations for
free is treated as predicting a positive polarity for
CVS. This would be accurate if CVS was actually
going to acquire Longs? business. Later in the
same news item, however, there is a sentence indi-
cating that the sale will not go through, which pre-
dicts negative polarity for CVS: Pershing Square
Capital Management said on Thursday it won?t
support a tender offer from CVS Caremark Corp
for rival Longs Drug Stores Corp because the of-
fer price ?materially understates the fair value of
the company,? according to a filing.
8 Conclusion
We have presented a model for predicting stock
price movement from news. We proposed FWD
(Frames, BOW, and part-of-speech specific DAL)
features and SemTree data representations. Our
semantic frame-based model benefits from tree
kernel learning using support vector machines.
The experimental results for our feature represen-
tation perform significantly better than BOW on
the polarity task, and show promise on the change
task. It also facilitates human interpretable analy-
sis to understand the relation between a company?s
market value and its business activities. The sig-
nals generated by this algorithm could improve the
prediction of a financial time series model, such as
ADS (Rydberg and Shephard, 2003).
Our future work will consider the contextual in-
formation for sentence selection, and an aggrega-
tion of weighted news content based on the decay
effect over time for individual companies. We plan
to use a moving window for training and testing.
We will also explore different labeling methods,
such as a threshold for price change tuned by sec-
tors and background economics.
9 Acknowledgements
The authors thank the anonymous reviewers for
their insightful comments.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24?
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, LSM ?11, pages
30?38. Association for Computational Linguistics.
Pierre Baldi, S?ren Brunak, Yves Chauvin, Claus A. F.
Andersen, and Henrik Nielsen. 2000. Assessing the
accuracy of prediction algorithms for classification:
an overview. Bioinformatics, 16:412 ? 424.
Roy Bar-Haim, Elad Dinur, Ronen Feldman, Moshe
Fresko, and Guy Goldstein. 2011. Identifying
and following expert investors in stock microblogs.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1310?1319, Edinburgh, Scotland, UK., July. Asso-
ciation for Computational Linguistics.
David M. Blei and Jon D. McAuliffe. 2007. Super-
vised topic models. In Advances in Neural Informa-
tion Processing Systems, Proceedings of the Twenty-
First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia,
Canada, December 3-6.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for
internet stock message boards. In Proceedings of
the Australasian Language Technology Association
Workshop 2009, pages 89?93, Sydney, Australia,
December.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, ACL ?02, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
881
Germa?n G. Creamer, Yong Ren, and Jeffrey V. Nicker-
son. 2012. A Longitudinal Analysis of Asset Re-
turn, Volatility and Corporate News Network. In
Business Intelligence Congress 3 Proceedings.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown
predicates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1435?1444, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In HLT-NAACL, pages 677?687. The Association
for Computational Linguistics.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984?991, Prague, Czech Republic,
June. Association for Computational Linguistics.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
Proceedings of the 20th International Conference on
Computational Linguistics.
Joseph Engelberg and Christopher A. Parsons. 2011.
The causal impact of media in financial markets.
Journal of Finance, 66(1):67?97.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
Framenet. International Journal of Lexicography,
16(3):235?250, September.
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences, 280(1):20?32.
Syed Aqueel Haider and Rishabh Mehrotra. 2011.
Corporate news classification and valence predic-
tion: A supervised approach. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis (WASSA 2.011),
pages 175?181, Portland, Oregon, June. Association
for Computational Linguistics.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ?06, pages 217?226, New
York, NY, USA. ACM.
Giuseppe Jurman and Cesare Furlanello. 2010. A uni-
fying view for performance measures in multi-class
prediction. ArXiv e-prints.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 272?280, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In In pro-
ceedings of the 6th ACM SIGKDD Int?l Conference
on Knowledge Discovery and Data Mining Work-
shop on Text Mining, pages 37?44.
Ronny Luss and Alexandre d?Aspremont. 2008. Pre-
dicting abnormal returns from news using text clas-
sification. CoRR, abs/0809.2792.
Brian W. Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of t4 phage
lysozyme. Biochimica et Biophysica Acta (BBA) -
Protein Structure, 405(2):442 ? 451.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2009. Re-
verse engineering of tree kernel feature spaces. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2009, 6-7 August 2009, Singapore, pages 111?120.
Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo,
Aristides Gionis, and Alejandro Jaimes. 2012. Cor-
relating financial time series with micro-blogging
activity. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
WSDM ?12, pages 513?522, New York, NY, USA.
ACM.
Josef Ruppenhofer and Ines Rehbein. 2012. Se-
mantic frames as an anchor representation for sen-
timent analysis. In Proceedings of the 3rd Work-
shop in Computational Approaches to Subjectivity
and Sentiment Analysis, WASSA ?12, pages 104?
109, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tina H. Rydberg and Neil Shephard. 2003. Dynam-
ics of Trade-by-Trade Price Movements: Decompo-
sition and Models. Journal of Financial Economet-
rics, 1(1):2?25.
882
Paul C. Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than Words: Quantifying
Language to Measure Firms? Fundamentals. The
Journal of Finance.
Paul C. Tetlock. 2007. Giving Content to Investor Sen-
timent: The Role of Media in the Stock Market. The
Journal of Finance.
Cynthia M. Whissel. 1989. The dictionary of affect in
language. Emotion: Theory, Research, and Experi-
ence, 39(4):113?131.
883
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 143?147,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automated Pyramid Scoring of Summaries using Distributional Semantics
Rebecca J. Passonneau? and Emily Chen? and Weiwei Guo? and Dolores Perin?
?Center for Computational Learning Systems, Columbia University
?Department of Computer Science, Columbia University
?Teachers College, Columbia University
(becky@ccls.|ec2805@|weiwei@cs.)columbia.edu, perin@tc.edu
Abstract
The pyramid method for content evaluation of auto-
mated summarizers produces scores that are shown
to correlate well with manual scores used in edu-
cational assessment of students? summaries. This
motivates the development of a more accurate auto-
mated method to compute pyramid scores. Of three
methods tested here, the one that performs best re-
lies on latent semantics.
1 Introduction
The pyramid method is an annotation and scor-
ing procedure to assess semantic content of sum-
maries in which the content units emerge from
the annotation. Each content unit is weighted
by its frequency in human reference summaries.
It has been shown to produce reliable rank-
ings of automated summarization systems, based
on performance across multiple summarization
tasks (Nenkova and Passonneau, 2004; Passon-
neau, 2010). It has also been applied to assessment
of oral narrative skills of children (Passonneau et
al., 2007). Here we show its potential for assess-
ment of the reading comprehension of community
college students. We then present a method to au-
tomate pyramid scores based on latent semantics.
The pyramid method depends on two phases of
manual annotation, one to identify weighted con-
tent units in model summaries written by profi-
cient humans, and one to score target summaries
against the models. The first annotation phase
yields Summary Content Units (SCUs), sets of
text fragments that express the same basic content.
Each SCU is weighted by the number of model
summaries it occurs in.
Figure 1 illustrates a Summary Content Unit
taken from pyramid annotation of five model sum-
maries of an elementary physics text. The ele-
ments of an SCU are its index; a label, created by
the annotator; contributors (Ctr.), or text fragments
from the model summaries; and the weight (Wt.),
corresponding to the number of contributors from
distinct model summaries. Four of the five model
Index 105
Label Matter is what makes up all objects or substances
Ctr. 1 Matter is what makes up all objects or substances
Ctr. 2 matter as the stuff that all objects and substances
in the universe are made of
Ctr. 3 Matter is identified as being present everywhere
and in all substances
Ctr. 4 Matter is all the objects and substances around us
Wt. 4
Figure 1: A Summary Content Unit (SCU)
summaries contribute to SCU 105 shown here.
The four contributors have lexical items in com-
mon (matter, objects, substances), and many dif-
ferences (makes up, being present). SCU weights,
which range from 1 to the number of model sum-
maries M , induce a partition on the set of SCUs
in all summaries into subsets Tw, w ? 1, . . . ,M .
The resulting partition is referred to as a pyramid
because, starting with the subset for SCUs with
weight 1, each next subset has fewer SCUs.
To score new target summaries, they are first
annotated to identify which SCUs they express.
Application of the pyramid method to assessment
of student reading comprehension is impractical
without an automated method to annotate target
summaries. Previous work on automated pyramid
scores of automated summarizers performs well
at ranking systems on many document sets, but
is not precise enough to score human summaries
of a single text. We test three automated pyramid
scoring procedures, and find that one based on dis-
tributional semantics correlates best with manual
pyramid scores, and has higher precision and re-
call for content units in students? summaries than
methods that depend on string matching.
2 Related Work
The most prominent NLP technique applied to
reading comprehension is LSA (Landauer and Du-
mais, 1997), an early approach to latent semantic
analysis claimed to correlate with reading compre-
hension (Foltz et al, 2000). More recently, LSA
143
has been incorporated with a suite of NLP metrics
to assess students? strategies for reading compre-
hension using think-aloud protocols (Boonthum-
Denecke et al, 2011). The resulting tool, and sim-
ilar assesment tools such as Coh-Metrix, assess
aspects of readability of texts, such as coherence,
but do not assess students? comprehension through
their writing (Graesser et al, 2004; Graesser et al,
2011). E-rater is an automated essay scorer for
standardized tests such as GMAT that also relies
on a suite of NLP techniques (Burstein et al, 1998;
Burstein, 2003). The pyramid method (Nenkova
and Passonneau, 2004), was inspired in part by
work in reading comprehension that scores con-
tent using human annotation (Beck et al, 1991).
An alternate line of research attempts to repli-
cate human reading comprehension. An auto-
mated tool to read and answer questions relies on
abductive reasoning over logical forms extracted
from text (Wellner et al, 2006). One of the perfor-
mance issues is resolving meanings of words: re-
moval of WordNet features degraded performance.
The most widely used automated content evalu-
ation is ROUGE (Lin, 2004; Lin and Hovy, 2003).
It relies on model summaries, and depends on
ngram overlap measures of different types. Be-
cause of its dependence on strings, it performs bet-
ter with larger sets of model summaries. In con-
trast to ROUGE, pyramid scoring is robust with as
few as four or five model summaries (Nenkova and
Passonneau, 2004). A fully automated approach
to evaluation for ranking systems that requires no
model summaries incorporates latent semantic dis-
tributional similarities across words (Louis and
Nenkova, 2009). The authors note, however, it
does not perform well on individual summaries.
3 Criteria for Automated Scoring
Pyramid scores of students? summaries correlate
well with a manual main ideas score developed
for an intervention study with community college
freshmen who attended remedial classes (Perin et
al., In press). Twenty student summaries by stu-
dents who attended the same college and took the
same remedial course were selected from a larger
set of 322 that summarized an elementary physics
text. All were native speakers of English, and
scored within 5 points of the mean reading score
for the larger sample. For the intervention study,
student summaries had been assigned a score to
represent how many main ideas from the source
text were covered (Perin et al, In press). Inter-
rater reliability of the main ideas score, as given
by the Pearson correlation coefficient, was 0.92.
One of the co-authors created a model pyra-
mid from summaries written by proficient Masters
of Education students, annotated 20 target sum-
maries against this pyramid, and scored the re-
sult. The raw score of a target summary is the
sum of its SCU weights. Pyramid scores have
been normalized by the number of SCUs in the
summary (analogous to precision), or the average
number of SCUs in model summaries (analogous
to recall). We normalized raw scores as the aver-
age of the two previous normalizations (analogous
to F-measure). The resulting scores have a high
Pearson?s correlation of 0.85 with the main idea
score (Perin et al, In press) that was manually as-
signed to the students? summaries.
To be pedagogically useful, an automated
method to assign pyramid scores to students? sum-
maries should meet the following criteria: 1) reli-
ably rank students? summaries of a source text, 2)
assign correct pyramid scores, and 3) identify the
correct SCUs. A method could do well on crite-
rion 1 but not 2, through scores that have uniform
differences from corresponding manual pyramid
scores. Also, since each weight partition will have
more than one SCU, it is possible to produce the
correct numeric score by matching incorrect SCUs
that have the correct weights. Our method meets
the first two criteria, and has superior performance
on the third to other methods.
4 Approach: Dynamic Programming
Previous work observed that assignment of SCUs
to a target summary can be cast as a dynamic
programming problem (Harnly et al, 2005). The
method presented there relied on unigram overlap
to score the closeness of the match of each eli-
gible substring in a summary against each SCU
in the pyramid. It returned the set of matches
that yielded the highest score for the summary.
It produced good rankings across summarization
tasks, but assigned scores much lower than those
assigned by humans. Here we extend the DP ap-
proach in two ways. We test two new semantic
text similarities, a string comparison method and a
distributional semantic method, and we present a
general mechanism to set a threshold value for an
arbitrary computation of text similarity.
Unigram overlap ignores word order, and can-
not consider the latent semantic content of a
string, only the observed unigram tokens. To
144
take order into account, we use Ratcliff/Obershelp
(R/O), which measures overlap of common sub-
sequences (Ratcliff and Metzener, 1988). To take
the underlying semantics into account, we use co-
sine similarity of 100-dimensional latent vectors
of the candidate substrings and of the textual com-
ponents of the SCU (label and contributors). Be-
cause the algorithm optimizes for the total sum of
all SCUs, when there is no threshold similarity to
count as a match, it favors matching shorter sub-
strings to SCUs with higher weights. Therefore,
we add a threshold to the algorithm, below which
matches are not considered. Because each sim-
ilarity metric has different properties and distri-
butions, a single absolute value threshhold is not
comparable across metrics. We present a method
to set comparable thresholds across metrics.
4.1 Latent Vector Representations
To represent the semantics of SCUs and candidate
substrings of target summaries, we applied the la-
tent vector model of Guo and Diab (2012).1 Guo
and Diab find that it is very hard to learn a 100-
dimension latent vector based only on the lim-
ited observed words in a short text. Hence they
include unobserved words that provide thousands
more features for a short text. This produces more
accurate results for short texts, which makes the
method suitable for our problem. Weighted ma-
trix factorization (WMF) assigns a small weight
for missing words so that latent semantics depends
largely on observed words.
A 100-dimension latent vector representation
was learned for every span of contiguous words
within sentence bounds in a target summary, for
the 20 summaries. The training data was selected
to be domain independent, so that our model could
be used for summaries across domains. Thus we
prepared a corpus that is balanced across topics
and genres. It is drawn from from WordNet sense
definitions, Wiktionary sense definitions, and the
Brown corpus. It yields a co-occurrence matrix
M of unique words by sentences of size 46,619
? 393,666. Mij holds the TF-IDF value of word
wi in sentence sj . Similarly, the contributors
to and the label for an SCU were given a 100-
dimensional latent vector representation. These
representations were then used to compare candi-
dates from a summary to SCUs in the pyramid.
1http://www.cs.columbia.edu/?weiwei/
code.html#wtmf.
4.2 Three Comparison Methods
An SCU consists of at least two text strings: the
SCU label and one contributor. As in Harnly et
al. (2005), we use three similarity comparisons
scusim(X,SCU), where X is the target summary
string. When the comparison parameter is set to
min (max, or mean), the similarity of X to
each SCU contributor and the label is computed
in turn, and the minimum (max, or mean) is re-
turned.
4.3 Similarity Thresholds
We define a threshold parameter for a target SCU
to match a pyramid SCU based on the distributions
of scores each similarity method gives to the target
SCUs identified by the human annotator. Annota-
tion of the target summaries yielded 204 SCUs.
The similarity score being a continuous random
variable, the empirical sample of 204 scores is
very sparse. Hence, we use a Gaussian kernel den-
sity estimator to provide a non-parametric estima-
tion of the probability densities of scores assigned
by each of the similarity methods to the manually
identified SCUs. We then select five threshold val-
ues corresponding to those for which the inverse
cumulative density function (icdf) is equal to 0.05,
0.10, 0.15, 0.20 and 0.25. Each threshold rep-
resents the probability that a manually identified
SCU will be missed.
5 Experiment
The three similarity computations, three methods
to compare against SCUs, and five icdf thresh-
olds yield 45 variants, as shown in Figure 2. Each
variant was evaluated by comparing the unnormal-
ized automated variant, e.g., Lvc, max, 0.64 (its
0.15 icdf) to the human gold scores, using each of
the evaluation metrics described in the next sub-
section. To compute confidence intervals for the
evaluation metrics for each variant, we use boot-
strapping with 1000 samples (Efron and Tibshi-
rani, 1986).
To assess the 45 variants, we compared their
scores to the manual scores. We also compared
the sets of SCUs retrieved. By our criterion 1), an
automated score that correlates well with manual
scores for summaries of a given text could be used
(3 Similarities) ? (3 Comparisons) ? (5 Thresholds) = 45
(Uni, R/O, Lvc) ? (min, mean, max) ? (0.05, . . . , 0.25)
Figure 2: Notation used for the 45 variants
145
Variant (with icdf) P (95% conf.), rank S (95% conf.), rank K (95% conf.), rank ? Diff. T test
LVc, max, 0.64 (0.15) 0.93 (0.94, 0.92), 1 0.94 (0.93, 0.97), 1 0.88 (0.85, 0.91), 1 49.9 15.65 0.0011
R/O, mean, 0.23 (0.15) 0.92 (0.91, 0.93), 3 0.93 (0.91,0.95), 2 0.83 (0.80, 0.86), 3 49.8 15.60 0.0012
R/O, mean, 0.26 (0.20) 0.92 (0.90, 0.93), 4 0.92 (0.90, 0.94) 4 0.80 (0.78, 0.83), 5 47.7 13.45 0.0046
LVc, max, 0.59 (0.10) 0.91 (0.89, 0.92), 8 0.93 (0.91, 0.95) 3 0.83 (0.80, 0.87), 2 52.7 18.50 0.0002
LVc, min, 0.40 (0.20) 0.92 (0.90,0.93), 2 0.87 (0.84, 0.91) 11 0.74 (0.69, 0.79), 11 37.5 3.30 0.4572
Table 1: Five variants from the top twelve of all correlations, with confidence interval and rank (P=Pearson?s, S=Spearman,
K=Kendall?s tau), mean summed SCU weight, difference of mean from mean gold score, T test p-value.
to indicate how well students rank against other
students. We report several types of correlation
tests. Pearsons tests the strength of a linear cor-
relation between the two sets of scores; it will be
high if the same order is produced, with the same
distance between pairs of scores. The Spearman
rank correlation is said to be preferable for ordi-
nal comparisons, meaning where the unit interval
is less relevant. Kendall?s tau, an alternative rank
correlation, is less sensitive to outliers and more
intuitive. It is the proportion of concordant pairs
(pairs in the same order) less the proportion of dis-
cordant pairs. Since correlations can be high when
differences are uniform, we use Student?s T to test
whether differences score means statistically sig-
nificant. Criterion 2) is met if the correlations are
high and the means are not significantly different.
6 Results
The correlation tests indicate that several variants
achieve sufficiently high correlations to rank stu-
dents? summaries (criterion 2). On all correla-
tion tests, the highest ranking automated method
is LVc, max, 0.64; this similarity threshold corre-
sponds to the 0.15 icdf. As shown in Table 1, the
Pearson correlation is 0.93. Note, however, that it
is not significantly higher than many of its com-
petitors. LVc, min, 0.40 did not rank as highly for
Speaman and Kendall?s tau correlations, but the
Student?s T result in column 3 of Table 1 shows
that this is the only variant in the table that yields
absolute scores that are not significantly different
from the human annotated scores. Thus this vari-
ant best balances criteria 1 and 2.
The differences in the unnormalized score com-
puted by the automated systems from the score as-
signed by human annotation are consistently posi-
tive. Inspection of the SCUs retrieved by each au-
tomated variant reveals that the automated systems
lean toward the tendency to identify false posi-
tives. This may result from the DP implementation
decision to maximize the score. To get a measure
of the degree of overlap between the SCUs that
were selected automatically versus manually (cri-
terion 4), we computed recall and precision for the
various methods. Table 2 shows the mean recall
and precision (with standard deviations) across all
five thresholds for each combination of similarity
method and method of comparison to the SCU.
The low standard deviations show that the recall
and precision are relatively similar across thresh-
olds for each variant. The LVc methods outper-
form R/O and unigram overlap methods, particu-
larly for the precision of SCUs retrieved, indicat-
ing the use of distributional semantics is a supe-
rior approach for pyramid summary scoring than
methods based on string matching.
The unigram overlap and R/O methods show the
least variation across comparison methods (min,
mean, max). LVc methods outperform them, on
precision (Table 2). Meeting all three criteria is
difficult, and the LVc method is clearly superior.
7 Conclusion
We extended a dynamic programming frame-
work (Harnly et al, 2005) to automate pyramid
scores more accurately. Improvements resulted
from principled thresholds for similarity, and from
a vector representation (LVc) to capture the latent
semantics of short spans of text (Guo and Diab,
2012). The LVc methods perform best at all three
criteria for a pedagogically useful automatic met-
ric. Future work will address how to improve pre-
cision and recall of the gold SCUs.
Acknowledgements
We thank the reviewers for very valuable insights.
Variant ? Recall (std) ? Precision (std) F score
Uni, min 0.69 (0.08) 0.35 (0.02) 0.52
Uni, max 0.70 (0.03) 0.35 (0.04) 0.53
Uni, mean 0.69 (0.02) 0.39 (0.04) 0.54
R/O, min 0.69 (0.08) 0.34 (0.01) 0.51
R/O, max 0.72 (0.03) 0.33 (0.04) 0.52
R/O, mean 0.71 (0.06) 0.38 (0.02) 0.54
LVc, min 0.61 (0.03) 0.38 (0.04) 0.49
LVc, max 0.74 (0.06) 0.48 (0.01) 0.61
LVc, mean 0.75 (0.06) 0.50 (0.02) 0.62
Table 2: Recall and precision for SCU selection
146
References
Isabel L. Beck, Margaret G. McKeown, Gale M. Sina-
tra, and Jane A. Loxterman. 1991. Revising social
studies text from a text-processing perspective: Ev-
idence of improved comprehensibility. Reading Re-
search Quarterly, pages 251?276.
Chutima Boonthum-Denecke, Philip M. McCarthy,
Travis A. Lamkin, G. Tanner Jackson, Joseph P.
Maglianoc, and Danielle S. McNamara. 2011. Au-
tomatic natural language processing and the de-
tection of reading skills and reading comprehen-
sion. In Proceedings of the Twenty-Fourth Interna-
tional Florida Artificial Intelligence Research Soci-
ety Conference, pages 234?239.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi
Lu, Martin Chodorow, Lisa Braden-Harder, and
Mary Dee Harris. 1998. Automated scoring us-
ing a hybrid feature identification technique. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 206?210, Montreal, Quebec, Canada, August.
Association for Computational Linguistics.
Jill Burstein. 2003. The e-rater R?scoring engine: Au-
tomated essay scoring with natural language pro-
cessing. In M. D. Shermis and J. Burstein, editors,
Automated Essay Scoring: A Cross-disciplinary
Perspective. Lawrence Erlbaum Associates, Inc.,
Hillsdale, NJ.
Bradley Efron and Robert Tibshirani. 1986. Boot-
strap methods for standard errors, confidence inter-
vals, and other measures of statistical accuracy. Sta-
tistical Science, 1:54?77.
Peter W. Foltz, Sara Gilliam, and Scott Kendall. 2000.
Supporting content-based feedback in on-line writ-
ing evaluation with LSA. Interactive Learning En-
vironments, 8:111?127.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louwerse, and Zhiqiang Cai. 2004. Coh-Metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments, and Computers,
36:193202.
Arthur C. Graesser, Danielle S. McNamara, and
Jonna M. Kulikowich. 2011. Coh-Metrix: Provid-
ing multilevel analyses of text characteristics. Edu-
cational Researcher, 40:223?234.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 864?872.
Aaron Harnly, Ani Nenkova, Rebecca J. Passonneau,
and Owen Rambow. 2005. Automation of summary
evaluation by the Pyramid Method. In Recent Ad-
vances in Natural Language Processing (RANLP),
pages 226?232.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 71?78.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Proceedings
of the Human Language Technology Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL), pages
463?470.
Annie Louis and Ani Nenkova. 2009. Evaluating con-
tent selection in summarization without human mod-
els. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 306?314, Singapore, August. Association for
Computational Linguistics.
Ani Nenkova and Rebecca J. Passonneau. 2004.
Evaluating content selection in summarization: The
Pyramid Method. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 145?152.
Rebecca J. Passonneau, Adam Goodkind, and Elena
Levy. 2007. Annotation of children?s oral narra-
tions: Modeling emergent narrative skills for com-
putational applications. In Proceedings of the Twen-
tieth Annual Meeting of the Florida Artificial Intel-
ligence Research Society (FLAIRS-20), pages 253?
258. AAAI Press.
Rebecca Passonneau. 2010. Formal and functional as-
sessment of the Pyramid Method for summary con-
tent evaluation. Natural Language Engineering, 16.
D. Perin, R. H. Bork, S. T. Peverly, and L. H. Mason.
In press. A contextualized curricular supplement for
developmental reading and writing. Journal of Col-
lege Reading and Learning.
J. W. Ratcliff and D. Metzener. 1988. Pattern match-
ing: the Gestalt approach.
Ben Wellner, Lisa Ferro, Warren R. Greiff, and Lynette
Hirschman. 2006. Reading comprehension tests for
computer-based understanding evaluation. Natural
Language Engineering, 12(4):305?334.
147
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 20?28,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotation Scheme for Social Network Extraction from Text
Apoorv Agarwal
Computer Science Department
Columbia University
New York, U.S.A.
apoorv@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, U.S.A.
rambow@ccls.columbia.edu
Rebecca J. Passonneau
CCLS
Columbia University
New York, U.S.A.
becky@cs.columbia.edu
Abstract
We are interested in extracting social net-
works from text. We present a novel an-
notation scheme for a new type of event,
called social event, in which two people
participate such that at least one of them
is cognizant of the other. We compare
our scheme in detail to the ACE scheme.
We perform a detailed analysis of inter-
annotator agreement, which shows that
our annotations are reliable.
1 Introduction
Our task is to extract a social network from written
text. The extracted social network can be used for
various applications such as summarization, ques-
tion answering, or the detection of main charac-
ters in a story. We take a ?social network? to be
a network consisting of individual human beings
and groups of human beings who are connected
to each other through various relationships by the
virtue of participating in events. A text can de-
scribe a social network in two ways: explicitly, by
stating the type of relationship between two indi-
viduals (Example ??); or implicitly, by describ-
ing an event which creates or perpetuates a so-
cial relationship (Example 2). We are interested in
the implicit description of social relations through
events. We will call these types of events so-
cial events. Crucially, many social relations are
described in text largely implicitly, or even en-
tirely implicitly. This paper presents an annotation
project for precisely such social events.
To introduce the terminology and conventions
we use throughout the paper, consider the follow-
ing Example 2. In this example, there are two
entities: Iraqi officials and Timothy McVeigh.
These entities are present in text as nominal
and named entity mentions respectively (within
[. . .]). Furthermore, these entities are related by
an event, whose type we call INR.NONVERBAL-
NEAR (a non-verbal interaction that occurs in
physical proximity), and whose textual mention is
the extent (or span of text) provided money and
training.1
(1) [[Sharif]?s {wife} Tahari Shad Tabussum],
27, (. . .) made no application for bail at the
court, according to local reports. PER-SOC
(2) The suit claims [Iraqi officials] {provided
money and training} to [convicted bomber
Timothy McVeigh] (. . .) INR.Nonverbal-
Near
One question that immediately comes to mind
is how would these annotations be useful? Let
us consider the problem of finding the hierarchy
of people in the Enron Email corpus (Klimt and
Yang, 2004; Diesner et al, 2005). Much work to
solve this problem has focused on using social net-
work analysis algorithms for calculating the graph
theoretical quantities (like degree centrality, clus-
tering coefficient (Wasserman and Faust, 1994))
of people in the email sender-receiver network
(Rowe et al, 2007). Attempts have been made to
incorporate the content of emails usually by us-
ing topic modeling techniques (McCallum et al,
2007; Pathak et al, 2008). These techniques con-
sider a distribution of words in emails to classify
the interaction between people into topics and then
cluster together people that talk about the same
topic. Researchers also map relationships among
individuals based on their patterns of word use
in emails (Keila and Skillicorn, 2005). But these
techniques do not attempt to create an accurate so-
cial network in terms of interaction or cognitive
states of people. In comparison, our data allows
1Throughout this paper we will follow this representation
scheme for examples ? entity mentions will be enclosed in
square brackets [. . .] and relation mentions will be enclosed
in set brackets {. . .}
20
Sender? Receiver Email content
Kate? Sam [Jacob], the City attorney had a couple of questions which [I] will {attempt to
relay} without having a copy of the documents.
Sam? Kate, Mary Can you obtain the name of Glendale?s bond counsel (lawyer?s name, phone
number, email, etc.)?
Kate? Sam Glendale?s City Attorney is Jacob. Please let [me] {know} if [you] need any-
thing else.
Mary? Sam I do not see a copy of an opinion in the file nor have we received one since [I]
{sent} the execution copies of the ISDA to [Jacob].
Kate? Jacob Jacob, could you provide the name, phone number, etc. of your bond council
for our attorney, Sam?
Kate? Sam [I] will {work on this for} [you] - and will be in touch.
Figure 1: An email thread from the Enron Email Corpus. (For space concerns some part of the conversation is removed. The
missing conversation does not affect our discussion.)
Kate
Sam
MaryJacob
Figure 2: Network formed by considering email exchanges
as links. Identical color or shape implies structural equiva-
lence. Only Sam and Mary are structurally equivalent
for such a technique to be created. This is because
our annotations capture interactions described in
the content of the email such as face-to-face meet-
ings, physical co-presence and cognizance.
To explore if this is useful, we analyzed an En-
ron thread which is presented in Figure 1. Fig-
ure 2 shows the network formed when only the
email exchange is considered. It is easy to see
that Sam and Mary are structurally equivalent and
thus have the same role and position in the so-
cial network. When we analyze the content of the
thread, a link gets added between Mary and Ja-
cob since Mary in her email to Sam talks about
sending something to Jacob. This link changes
the roles and positions of people in the network. In
the new network, Figure 3, Kate and Mary appear
structurally equivalent to each other, as do Sam
and Jacob. Furthermore, Mary now emerges as
a more important player than the email exchange
on its own suggests. This rather simple example is
an indication of the degree to which a link may af-
fect the social network analysis results. In emails
where usually a limited number of people are in-
volved, getting an accurate network seems to be
crucial to the hierarchal analysis.
There has been much work in the past on an-
Kate
Sam
MaryJacob
Figure 3: Network formed by augmenting the email ex-
change network above with links that occur in the content of
the emails. Now, Kate and Mary are structurally equivalent,
as are Sam and Jacob.
notating entities, relations and events in free text,
most notably the ACE effort (Doddington et al,
2004). We intend to leverage this work as much
as possible. The task of social network extrac-
tion can be broadly divided into 3 tasks: 1) en-
tity extraction; 2) social relation extraction; 3) so-
cial event extraction. We are only interested in the
third task, social event extraction. For the first two
tasks, we can simply use the annotation guidelines
developed by the ACE effort. Our social events,
however, do not clearly map to the ACE events:
we introduce a comprehensive set of social events
which are very different from the event annotation
that already exists for ACE. This paper is about the
annotation of social events.
The structure of the paper is as follows. In Sec-
tion 2 we present a list of social relations that we
annotate. We also talk about some design deci-
sions and explain why we took them. We com-
pare this annotation to existing annotation, notably
the ACE annotation, in Section 3. In section 4
we present the procedure of annotation. Section 5
gives details of our inter-annotator agreement cal-
culation procedure and shows the inter-annotator
agreement on our task. We conclude in section 6
21
and mention future direction of research.
2 Social Event Annotation
In this section we define the social events that the
annotators were asked to annotate. Here, we are
interested in the meaning of the annotation; de-
tails of the annotation procedure can be found in
Section 4. Note that in this annotation effort, we
do not consider issues related to the truth of the
claims made in the text we are analyzing ? we
are interested in finding social events whether they
are claimed as being true, presented as specula-
tion, or presented as wishful thinking. We assume
that other modules will be able to determine the
factive status of the described social events, and
that social events do not differ from other types of
events in this respect.
A social event is an event in which two or more
entities relate, communicate or are associated such
that for at least one participant, the interaction is
deliberate and conscious. Put differently, at least
one participant must be aware of relating to the
other participant. In this definition, what consti-
tutes a social relation is an aspect of cognitive
state: an agent is aware of being in a particular re-
lation to another agent. While two people passing
each other on a street without seeing each other
may be a nice plot device in a novel, it is not a
social event in our sense, since it does not entail a
social relation.
Following are the four types of social events that
were annotated:2
Interaction event (INR): When both entities
participating in an event have each other in their
cognitive state (i.e., are aware of the social re-
lation) we say they have an INR relation. The
requirement is actually deeper: it extends to the
transitive closure under mutual awareness, what in
the case of belief is called ?mutual belief?. An
INR event could either be of sub-type VERBAL or
NONVERBAL. Note that a verbal interaction event
does not mean that all participants must actively
communicate verbally, it is enough if one partic-
ipant communicates verbally and the others are
aware of this communication.3 Furthermore, the
interaction can be in physical proximity or from a
distance. Therefore, we have further subtypes of
2Details of the annotation guidelines can be found in the
unpublished annotation manual, which we will refer to in the
final version of the paper.
3For this reason we explicitly annotate legal events as
VERBAL because legal interactions usually involve words
INR relation: NEAR and FAR. In all, INR has
four subtypes: VERBAL-NEAR, VERBAL-FAR,
NONVERBAL-NEAR, NONVERBAL-FAR. Con-
sider the following Example (3). In this sen-
tence, our annotators recorded an INR.VERBAL-
FAR between entities Toujan Faisal and the com-
mittee.
(3) [Toujan Faisal], 54, {said} [she] was
{informed} of the refusal by an [Inte-
rior Ministry committee] overseeing election
preparations. INR.Verbal-Far
As is intuitive, if one person informs the other
about something, both have to be cognizant of
each other and of the informing event. Also, the
event of informing involves words, therefore, it is a
verbal interaction. From the context it is not clear
if Toujan was informed personally, in which case
it would be a NEAR relation, or not. We decided
to default to FAR in case the physical proximity is
unclear from the context. We decided this because,
on observation, we found that if the author of the
news article was reporting an event that occurred
in close proximity, the author would explicitly say
so or give an indication. INR is the only relation
which is bi-directional.
Cognition event (COG): When only one person
(out of the two people that are participating in an
event) has the other in his or her cognitive state,
we say there exists a cognition relationship be-
tween entities. Consider the aforementioned Ex-
ample (3). In this sentence, the event said marks
a COG relation between Toujan Faisal and the
committee. This is because, when one person
talks about the other person, the other person must
be present in the first person?s cognitive state.
COG is a directed event from the entity which
has the other entity in its cognitive state to the
other entity. In the example under consideration,
it would be from Toujan Faisal to the committee.
There are no subtypes of this relation.
Physical Proximity event (PPR): We record a
PPR event when both the following conditions
hold: 1) exactly one entity has the other entity in
their cognitive state (this is the same requirement
as that for COG) and 2) both the entities are
physically proximate. Consider the following
Example (4). Here, one can reasonably assume
that Asif Muhammad Hanif was aware of being
in physical proximity to the three people killed,
while the inverse was not necessarily true.
22
(4) [Three people] were killed when (. . .), [Asif
Muhammad Hanif], (. . .), {detonated explo-
sives strapped to [his] body} PPR
PPR is a directed event like COG. There are no
subtypes of this relation. Note that if there exists
a PPR event then of course there would also be
a COG event. In such cases, the PPR event sub-
sumes COG, and we do not separately record a
COG event.
Perception event (PCR): The Perception Rela-
tionship is the distant equivalent of the Physi-
cal Proximity event. The point is not physical
distance; rather, the important ingredient is the
awareness required for PPR, except that physical
proximity is not required, and in fact physical dis-
tance is required. This kind of relationship usually
exists if one entity is watching the other entity on
TV broadcast, listening to him or her on the radio
or using a listening device, or reading about the
other entity in a newspaper or magazine etc. Con-
sider the following Example (5). In this example,
we record a PCR relation between the pair and
the Nepalese babies. This is because, the babies
are of course not aware of the pair. Moreover, the
pair heard about the babies so there is no physical
proximity. It is not COG because there was an ex-
plicit external information source which brought
the babies to the attention of the pair.
(5) [The pair] flew to Singapore last year af-
ter {hearing} of the successful surgery on
[Nepalese babies] [Ganga] and [Jamuna
Shrestha], (. . .). PCR
PCR is a directed event like COG. There are no
subtypes of this relation. Note that if there exists
a PCR event then we do not separately record a
COG event.
Figure 4 represents the series of decisions that
an annotator is required to take before reaching a
terminal node (or an event annotation label). The
interior nodes of the tree represent questions that
annotators answer to progress downwards in the
tree. Each question has a binary answer. For ex-
ample, the first question the annotators answer to
get to the type and subtype of an event is: ?Is
the relation directed (1-way) or bi-directional (2-
way)?? Depending on the answer, they move to
the left or the right in the tree respectively. If its a
2-way relation, then it has to one of the sub-types
of INR because only INR requires that both enti-
ties be aware of each other.
	 ?
Event	 ?Present	 ?Event	 ?Absent	 ?
Verbal	 ?
2-??Way	 ?
Nonverbal	 ?
1-??Way	 ?
Mind	 ?Far	 ?	 ?	 ?	 ?	 ?	 ?Near	 ?
Near	 ? Far	 ?
Near	 ? 	 ?	 ?	 ?Far	 ?
Figure 4: Tree representation of decision points for select-
ing an event type/subtype out of the list of social events. Each
decision point is numbered for easy reference. We refer to
these number later when we present our results. The num-
bers in braces ([. . .]) are the number of examples that reach a
decision point.
3 Comparison Between Social Events
and ACE Annotations
In this section, we compare our annotations
with existing annotation efforts. To the best of
our knowledge, no annotation effort has been
geared towards extracting social events, or to-
wards extracting expressions that convey social
relations in text. The Automated Content Ex-
traction (ACE) annotations are the most similar
to ours because ACE also annotates Person Enti-
ties (PER.Individual, PER.Group), Relations be-
tween people (PER-SOC), and various types of
Events. Our annotation scheme is different, how-
ever, because the focus of our event annotation is
on events that occur only between people. Fur-
thermore, we annotate text that expresses the cog-
nitive states of the people involved, or allows the
annotator to infer it. Therefore, at the top level
of classification we differentiate between events
in which only one entity is cognizant of the other
versus events when both entities are cognizant of
each other. This distinction is, we believe, novel
in event or relation annotation. In the remainder
of this section, we will present statistics and de-
tailed examples to highlight differences between
our event annotations and the ACE event annota-
tions.
The statistics we present are based on 62 docu-
ments from the ACE-2005 corpus that one of our
annotator also annotated.4 Since our event types
and subtypes are not directly comparable to the
4Due to space constraints we do not give statistics for the
other annotator.
23
ACE event types, we say there is a ?match? when
both the following conditions hold:
1. The span of text that represents an event in
the ACE event annotations overlap with ours.
2. The entities participating in the ACE event
are same as the entities participating in our
event.5
Our annotator recorded a total of 212 events
in 62 documents. We found a total of 63 can-
didate ACE events that had at least two Per-
son entities involved. Out of these 63 candi-
date events, 54 match both the aforementioned
conditions and hence our annotations. A clas-
sification of all of the events (those found by
our annotators and the ACE events involving at
least two persons) into our social event categories
and into the ACE categories is given in Fig-
ure 5. The figure shows that the majority of so-
cial events that match the ACE events are of type
INR.VERBAL-NEAR. On analysis, we found that
most of these correspond to the ACE type/subtype
CONTACT.MEET. It should be noted, how-
ever, our type/subtype INR.VERBAL-NEAR has a
broader definition than ACE type/subtype CON-
TACT.MEET, as will become apparent later in this
section. In the following, we discuss the 9 ACE
events that are not social events, and then we dis-
cuss the 158 social events that are not ACE events.
Out of the nine candidate ACE events which did
not match our social event annotation, we found
five are our annotation errors, i.e. when we an-
alyzed manually and looked for ACE events that
did not correspond to our annotations, we found
that our annotator missed these events. The re-
maining four, in contrast, are useful for our dis-
cussion because they highlight the differences in
ACE and our annotation perspectives. This will
become clearer with the following example:
(6) In central Baghdad, [a Reuters cameraman]
and [a cameraman for Spain?s Telecinco]
died when an American tank fired on the
Palestine Hotel
ACE has annotated the above example as an
event of type CONFLICT-ATTACK in which there
are two entities that are of type person: the
Reuters cameraman and the cameraman for
5Recall that our event annotations are between exactly
two entities of type PER.Individual or PER.Group.
Spain?s Telecinco, both of which are arguments
of type ?Victim?. Being an event that has two per-
son entities involved makes the above sentence a
valid candidate (or potential) ACE event that we
match with our annotations. However, it fails to
match our annotations, since we do not annotate
an event in this sentence. The reason is that this
example does not reveal the cognitive states of the
two entities ? we do not know whether one was
aware of the other.
We now discuss social events that are not ACE
events. From Figure 5 we see that most of the
events that did not overlap with ACE event anno-
tations were Cognition (COG) social events. In
the following, our annotator records a COG rela-
tion between Digvijay Singh and Abdul Kalam
(also Atal Behari Vajpayee and Varuna). The
reason is that by virtue of talking about the two
entities, Digvijay Singh?s cognitive state contains
those entities. However, the sentence does not re-
veal the cognitive states of the other two entities
and therefore it is not an INR event. In contrast,
ACE does not have any event annotation for this
sentence.
(7) The Times of India newspaper quoted [Digvi-
jay Singh] as {saying} that [Prime Minister
Atal Behari Vajpayee] and [President Abdul
Kalam] had offended [the Hindu rain God
Varuna] by remaining bachelors. COG
It is easy to see why COG relations are not usu-
ally annotated as ACE events. But it is counter-
intuitive for INR social events not to be annotated
as ACE events. We explain this using Example (3)
in Section 2. Our annotator recorded an INR re-
lation between Toujan Faisal and the commit-
tee (event span: informed). ACE did not record
any event between the two entities.6 This exam-
ple highlights the difference between our defini-
tion of Interaction events and ACE?s definition of
Contact events. For this reason, in Figure 5, 51 of
our INR relations do not overlap with ACE event
categories.
4 Annotation Procedure
We used Callisto (a configurable workbench) (Day
et al, 2004) to annotate the ACE-2005 corpus for
6The ACE event annotated in the sentence is of type
?Personell-Elect? (span election) which is not recorded as an
event between two or more entities and is not relevant here.
24
62 Documents Conflict (5) Contact (32) Justice-* (13) Life (7) Transaction (2) Not FoundAttack Meet Phone-Write Die Divorce Injure Transfer-Money
 INR 
 Verbal  Near (66) 0 26 0 9 0 0 0 0 31 Far (17) 0 0 3 3 0 1 0 0 10
 NonVerbal  Near (14) 3 0 0 0 2 0 0 1 8 Far (3) 0 0 0 0 0 0 0 1 2
 COG (109) 2 0 0 0 1 0 0 0 106
 PPR (2)  0  0  0  0  1  0  1  0  0 
 PCR (1)  0  0  0  0  0  0  0  0  1 
 Errors  0  3  0  1  1  0  0  0 
Figure 5: This table maps the type and subtype of ACE events to our types and subtypes of social events. The columns have
ACE event types and sub-types. The rows represent our social event types and sub-types. The last column is the number of our
events that are not annotated as ACE events. The last row has the number of social events that our annotator missed but are
ACE events.
the social events we defined earlier. The ACE-
2005 corpus has already been annotated for enti-
ties as part of the ACE effort. The entity anno-
tation is therefore not part of this annotation ef-
fort. We hired two annotators. Annotators opened
ACE-2005 files one by one in Callisto. They could
see the whole document at one time (top screen
of Figure 6) with entities highlighted in blue (bot-
tom screen of Figure 6). These entities were only
of type PER.Individual and PER.Group and be-
longed to class SPC. All other ACE entity annota-
tions were removed. The annotators were required
to read the whole document (not just the part that
has entities) and record a social event span (high-
lighted in dark blue in Figure 6), social event type,
subtype and the two participating entities in the
event.
The span of a event mention is the minimum
span of text that best represents the presence of the
type of event being recorded. It can also be viewed
as the span of text that evokes the type of event be-
ing recorded. The span may be a word, a phrase
or the whole sentence. For example, the span in
Example (4) in Section 2 includes strapped to his
body because that confirms the physical proximity
of the two entities. We have, however, not paid
much attention to the annotation of the span, and
will not report inter-annotator agreement on this
part of the annotation. The reason for this is that
we are interested in annotating the underlying se-
mantics; we will use machine learning to find the
linguistics clues to each type of social event, rather
than relying on the annotators? ability to deter-
mine these. Also note that we did not give precise
instructions on which entity mentions to choose
in case of multiple mentions of the same entity.
Again, this is because we are interested in anno-
tating the underlying semantics, and we will rely
on later analysis to determine which mentions par-
ticipate in signaling the annotated social events.
Figure 6: Snapshot of Callisto. Top screen has the text
from a document. Bottom screen has tabs for Entities, Entity
Mentions etc. An annotator selected text said, highlighted
in dark blue, as an event of type COG between Entities with
entity ID E1 and E9.
Both our annotators annotated 46 common doc-
uments. Out these, there was one document that
had no entity annotations, implying no social event
annotation. The average number of entities in the
remaining 45 documents was 6.82 per document,
and the average number of entity mentions per
document was 23.78. The average number of so-
cial events annotated per document by one anno-
25
tator was 3.43, whereas for the other annotator it
was 3.69. In the next section we present our inter-
annotator agreement calculations for these 45 doc-
uments.
5 Inter-annotator Agreement
Annotators consider all sentences that contain at
least two person entities (individuals or group),
but do not always consider all possible labels, or
annotation values. As represented in the decision
tree in Figure 5, many of the labels are conditional.
At each next depth of the tree, the number of in-
stances can become considerably pruned. Due to
the novelty of the annotation task, and the condi-
tional nature of the labels, we want to assess the
reliability of the annotation of each decision point.
For this, we report Cohen?s Kappa (Cohen, 1960)
for each independent decision. We use the stan-
dard formula for Cohen?s Kappa given by:
Kappa =
P (a)? P (e)
1? P (e)
where P (a) is probability of agreement and P (e)
is probability of chance agreement. These proba-
bilities can be calculated from the confusion ma-
trix represented as follows:
In addition, we present the confusion matrix for
each decision point to show the absolute number
of cases considered, and F-measure to show the
proportion of cases agreed upon. For most de-
cision points, the Kappa scores are at or above
the 0.67 threshold recommended by Krippen-
dorff (1980) with F-measures above 0.90. Where
Kappa is low, F-measure remains high. As dis-
cussed below, we conclude that the annotation
schema is reliable.
We note that in the ACE annotation effort, inter-
annotator agreement (IAA) was measured by a
single number, but this number did not take chance
agreement into account: it simply used the eval-
uation metric to compare systems against a gold
standard. Furthermore, this metric is composed
of distinct parts which were weighted in accor-
dance with research goals from year to year, mean-
ing that the results of applying the metric changed
from year to year. We have also performed an
ACE-style IAA evaluation, which we report at the
end of this section.
Figure 7 shows the results for the seven binary
decision points, considered separately. The num-
ber of the decision point in the table corresponds
to the decision points in Figure 4. The (flattened)
confusion matrices in column two present annota-
tor two?s choices by annotator one?s, with positive
agreement in the upper left (cell A) and negative
agreement in the lower right (cell D). In all cases
the cell values on the agreement diagonal (A, D)
are much higher than the cells for disagreement
(B, C). The upper left cell (A) of the matrix for
decision 1 represents the positive agreements on
the presence of a social event (N=133), and these
are the cases considered for decision 2. For the
remaining decisions, agreement is always unbal-
anced towards agreement on the positive cases,
with few negative cases. In the case of decision
4, for example, this reflects the inherent unlike-
lihood of the NONVERBAL-FAR event. In other
cases, it reflects a property of the genre. For ex-
ample, when we apply this annotation schema to
fiction, we find a much higher frequency of phys-
ically proximate events (PPR), corresponding to
the lower left cell (D) of the confusion matrix for
decision 6.
For decision 4 (NONVERBAL-NEAR) and 7
(PCR/COG), kappa scores are low but the con-
fusion matrices and high F-measures demonstrate
that the absolute agreement is very high. Kappa
measures the amount of agreement that would not
have occurred by chance, with values in [-1,1]. For
binary data and two annotators, values of -1 can
occur, indicating that the annotators have perfectly
non-random disagreements. The probability of an
annotation value is estimated by its frequency in
the data (the marginals of the confusion matrix).
It does not measure the actual amount of agree-
ment among annotators, as illustrated by the rows
for decisions 4 and 7. Because NONVERBAL-
FAR is chosen so rarely by either annotator (never
by annotator 2), the likelihood that both annota-
tors will agree on NONVERBAL-NEAR is close to
one. In this case, there is little room for agreement
above chance, hence the Kappa score of zero. We
should point out, however, that this skewness was
revealed from the annotated corpus. We did not
bias our annotators to look for a particular type of
relation.
The five cases of high Kappa and high F-
26
measure indicate aspects of the annotation where
annotators generally agree, and where the agree-
ment is unlikely to be accidental. We conclude that
these aspects of the annotation can be carried out
reliably as independent decisions. The two cases
of low Kappa and high F-measure indicate aspects
of the annotation where, for this data, there is rel-
atively little opportunity for disagreement.
Decision Point Confusion Matrix Kappa F1A B C D
1 (+/- Relation) 133 31 34 245 0.68 0.80
2 (1 or 2 way) 51 8 1 73 0.86 0.91
3 (Verbal/NonV) 40 4 0 7 0.73 0.95
4 (NonV-Near/Far) 6 0 1 0 0.00 0.92
5 (Verbal-Near/Far) 30 1 2 7 0.77 0.95
6 (+/- PPR) 71 0 1 1 0.66 0.99
7 (PCR/COG) 69 1 1 0 -0.01 0.98
Figure 7: This table presents the Inter-annotator agreement
measures. Column 1 is the decision point corresponding to
the decision tree. Column 2 represents a flattened confusion
matrix where A corresponds to top left corner, D corresponds
to the bottom right corner, B corresponds to top right corner
and C corresponds to the bottom left corner of the confusion
matrix. We present values for Cohen?s Kappa in column 3
and F-measure in the last column.
Now, we present a measure of % agreement
for our annotators by using the ACE evaluation
scheme.7 We considered one annotator to be the
gold standard and the other to be a system being
evaluated against the gold standard. For the cal-
culation of this measure we first take the union of
all event spans. As in the ACE evaluation scheme,
we associate penalties with each wrong decision
annotators take about the entities participating in
an event, type and sub-type of an event. Since
these penalties are not public, we assign our own
penalties. We choose penalties that are not biased
towards any particular event type or subtype. We
decide the penalty based on the number of options
an annotator has to consider before taking a cer-
tain decision. For example, we assign a penalty
of 0.5 if one annotator records an event which the
other annotator does not. If annotators disagree
on the relation type, the penalty is 0.25 because
there are four options to select from (INR, COG,
PPR, PCR). Similarly, we assign a penalty of 0.2
7http://www.itl.nist.gov/iad/mig//tests/ace/2007/doc/ace07-
evalplan.v1.3a.pdf
if the annotators disagree on the relation sub-types
(VERBAL-NEAR, VERBAL-FAR, NONVERBAL-
NEAR, NONVERBAL-FAR, No sub-type). We as-
sign a penalty of 0.5 if the annotators disagree on
the participating entities (incorporating the direc-
tionality in directed relations). Using these penal-
ties, we get % agreement of 69.74%. This is a high
agreement rate as compared to that of ACE?s event
annotation, which was reported to be 31.5% at the
ACE 2005 meeting.
6 Conclusion and Future Work
We have presented a new annotation scheme for
extracting social networks from text. We have
argued, social network created by the sender -
receiver links in Enron Email corpus can ben-
efit from social event links extracted from the
content of emails where people talk about their
?implicit? social relations. Our annotation task
is novel in that we are interested in the cogni-
tive states of people: who is aware of interact-
ing with whom, and who is aware of whom with-
out interacting. Though the task requires detec-
tion of events followed by conditional classifica-
tion of events into four types and subtypes, we
achieve high Kappa (0.66-0.86) and F-measure
(0.8-0.9). We also achieve a high global agree-
ment of 69.74% which is inspired by Automated
Content Extraction (ACE) inter-annotator agree-
ment measure. These measures indicate that our
annotations are reliable.
In future work, we will apply our annotation
effort to other genres, including fiction, and to
text from which larger social networks can be
extracted, such as extended journalistic reporting
about a group of people.
Please contact the second author of the paper
about the availability of the corpus.
Acknowledgments
This work was funded by NSF grant IIS-0713548.
We thank Dr. David Day for help with adapting
the annotation interface (Callisto) to our require-
ments. We would like to thank David Elson for
extremely useful discussions and feedback on the
annotation manual and the inter-annotator calcu-
lation scheme. We would also like to thank the
Natural Language Processing group at Columbia
University for their feedback on our classification
of social events.
27
References
Jacob Cohen. 1960. A coeffiecient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
David Day, Chad McHenry, Robyn Kozierok, and Lau-
rel Riek. 2004. Callisto: A configurable annotation
workbench. International Conference on Language
Resources and Evaluation.
Jana Diesner, Terrill L Frantz, and Kathleen M Car-
ley. 2005. Communication networks from the enron
email corpus it?s always about the people. enron is
no different. Computational & Mathematical Orga-
nization Theory, 11(3):201?228.
G Doddington, A Mitchell, M Przybocki, L Ramshaw,
S Strassel, and R Weischedel. 2004. The auto-
matic content extraction (ace) program?tasks, data,
and evaluation. LREC, pages 837?840.
P S Keila and D B Skillicorn. 2005. Structure in the
enron email dataset. Computational & Mathemati-
cal Organization Theory, 11 (3):183?199.
Bryan Klimt and Yiming Yang. 2004. Introducing
the enron corpus. In First Conference on Email and
Anti-Spam (CEAS).
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications.
Andrew McCallum, Xuerui Wang, and Andres
Corrada-Emmanuel. 2007. Topic and role discovery
in social networks with experiments on enron and
academic email. Journal of Artificial Intelligence
Research, 30 (1):249?272.
Nishith Pathak, Colin DeLong, Arindam Banerjee, and
Kendric Erickson. 2008. Social topic models for
community extraction. Proceedings of SNA-KDD.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J Stolfo. 2007. Automated social hi-
erarchy detection through email network analysis.
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, pages 109?117.
Stanley Wasserman and Katherine Faust. 1994. Social
Network Analysis: Methods and Applications. New
York: Cambridge University Press.
28
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47?55,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Anveshan: A Framework for Analysis of Multiple Annotators? Labeling
Behavior
Vikas Bhardwaj,
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
Columbia University
New York, NY, USA
vsb2108@columbia.edu
(becky@cs|ansaf@ccls).columbia.edu
Nancy Ide
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Abstract
Manual annotation of natural language to
capture linguistic information is essen-
tial for NLP tasks involving supervised
machine learning of semantic knowledge.
Judgements of meaning can be more or
less subjective, in which case instead of
a single correct label, the labels assigned
might vary among annotators based on the
annotators? knowledge, age, gender, intu-
itions, background, and so on. We intro-
duce a framework ?Anveshan,? where we
investigate annotator behavior to find out-
liers, cluster annotators by behavior, and
identify confusable labels. We also in-
vestigate the effectiveness of using trained
annotators versus a larger number of un-
trained annotators on a word sense annota-
tion task. The annotation data comes from
a word sense disambiguation task for pol-
ysemous words, annotated by both trained
annotators and untrained annotators from
Amazon?s Mechanical turk. Our results
show that Anveshan is effective in uncov-
ering patterns in annotator behavior, and
we also show that trained annotators are
superior to a larger number of untrained
annotators for this task.
1 Credits
This work was supported by a research supple-
ment to the National Science Foundation CRI
award 0708952.
2 Introduction
Manual annotation of language data in order to
capture linguistic knowledge has become increas-
ingly important for semantic and pragmatic an-
notation tasks. A very short list of a few such
tasks illustrates the range of types of annotation,
in varying stages of development: predicate ar-
gument structure (Palmer et al, 2005b), dialogue
acts (Hu et al, 2009), discourse structure (Carbone
et al, 2004), opinion (Wiebe and Cardie, 2005),
emotion (Alm et al, 2005). The number of ef-
forts to create corpus resources that include man-
ual annotations has also been growing. A common
approach in assessing the resulting manual anno-
tations is to report a single quantitative measure
reflecting the quality of the annotations, either a
summary statistic such as percent agreement, or
an agreement coefficient from the family of met-
rics that include Krippendorff?s alpha (Krippen-
dorff, 1980) and Cohen?s kappa (Cohen, 1960).
We present some new assessment methods to use
in combination with an agreement coefficient for
understanding annotator behavior when there are
multiple annotators and many annotation values.
Anveshan (Annotation Variance Estimation)1 is
a suite of procedures for analyzing patterns of
agreement and disagreement among annotators,
as well as the distributions of annotation values
across annotators. Anveshan thus makes it pos-
sible to explore annotator behavior in more detail.
Currently, it includes three types of analysis: inter-
annotator agreement (IA) among all subsets of an-
notators, leverage of annotation values for outlier
detection, and metrics for comparing annotators?
distributions of annotation values (e.g., Kullbach-
Liebler divergence).
As an illustration of the utility of Anveshan, we
compare two groups of annotators on the same an-
notation word sense annotation tasks: a half dozen
trained annotators and fourteen Mechanical Turk-
ers. Previous work has argued that it can be cost
effective to collect multiple labels from untrained
labelers at a low cost per label, and to combine
the multiple labels through a voting method, rather
than to collect single labels from highly trained la-
1Anveshan is a Sanskrit word which literally means
search or exploration.
47
belers (Snow et al, 2008; Sheng et al, 2008; Lam
and Stork, 2003). The tasks included in (Snow et
al., 2008), for example, include word sense an-
notation; in contrast to our case, where the av-
erage number of senses per word is 9.5, the one
word sense annotation task had three senses. We
find that the same half dozen trained annotators
can agree well or not on sense labels for poly-
semous words. When they agree less well, we
find that it is possible to distinguish between prob-
lems in the labels (e.g., confusable senses) and
systematic differences of interpretation among an-
notators. When we use twice the number of Me-
chanical Turkers as trained annotators for three of
our ten polysemous words, we find inconsistent re-
sults.
The next section of the paper presents the moti-
vation for Anveshan and its relevance to the word
sense annotation task, followed by a section on
related work. The word sense annotation data is
given in section 5. Anveshan is described in the
subsequent section, followed by the results of its
application to the two data sets. We discuss the
comparison of trained annotators and Mechanical
Turkers, as well as differences among words, in
section 7. Section 7 concludes with a short recap
of Anveshan in general, and its application to word
sense annotations in particular.
3 Beyond Interannotator Agreement (IA)
Assessing the reliability of an annotation typically
addresses the question of whether different anno-
tators (effectively) assign the same annotation la-
bels. Various measures can be used to compare
different annotators, including agreement coeffi-
cients such as Krippendorff?s alpha (Krippendorff,
1980). Extensive reviews of the properties of such
coefficients have been presented elsewhere, e.g.,
(Artstein and Poesio, 2008). Briefly, an agree-
ment produce values in the interval [-1,1] indicat-
ing how much of the observed agreement is above
(or below) agreement that would be predicted by
chance (value of 0). To measure reliability in this
way is to assume that for most of the instances in
the data, there is a single correct response. Here
we present the use of reliability metrics and other
measures for word sense annotation, and we as-
sume that in some cases there may not be a single
correct response. When annotators have less than
excellent agreement, we aim to examine possible
causes.
We take word sense to be a problematic anno-
tation to perform, thus requiring a deeper under-
standing of the conditions under which annotators
might disagree. The many reasons can only be
touched on here. For example, word senses are
not discrete, atomic units that can be delimited and
enumerated. While dictionaries and other lexical
resoures, such as WordNet (Miller et al, 1993) or
the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff
and Palmer, 2000)), do provide enumerations of
the senses for a given word, and their interrela-
tions (e.g., a list of senses, a tree of senses), it is
widely agreed that this is a convenient abstraction,
if for no other reason than the fact that words shift
meanings along with the communicative needs of
the groups of individuals who use them. The con-
text in which a word is used plays a significant role
in restricting the current sense. As a result, it is
often argued that the best representation for word
meaning would consist in clustering the contexts
in which words are used (Kilgarriff, 1997). Yet
even this would be insufficient because new com-
munities arise, new behaviors and artifacts emerge
along with them, hence new contexts of use and
new clusters. At the same time, contexts of use
and the senses that go along with them can fade
away (cf. the use of handbag discussed in (Kilgar-
riff, 1997) pertaining to disco dancing). Because
an enumeration of word senses is somewhat arti-
ficial, annotators might disagree on word senses
because they disagree on the boundaries between
one sense and another, just as professional lexi-
cographers do.
Apart from the artificiality of creating flat or
hierarchical sense inventories, the meanings of
words can vary in their subjectivity, due to differ-
ences in the perception or experience of individu-
als. This can be true for word senses that are inher-
ently relative, such as cold (as in, turn up the ther-
mostat, it?s too cold in here); or that derive their
meaning from cultural norms that may differ from
community to community, such as justice; or that
change as one grows older, e.g., whether a long
time to wait pertains to hours versus days.
Despite the arguments against using word sense
inventories, until they are replaced with an equally
convenient and more representative abstraction,
they are an extremely convenient computational
representation. We rely on WordNet senses, which
are presented to annotators with a gloss (defini-
tion) and with example uses. In order to better un-
48
derstand reasons for disagreement on senses, we
collect labels from multiple annotators. When an-
notators agree, having multiple annotators is re-
dundant. But when annotators disagree, having
multiple annotators is necessary in order to de-
termine whether the disagreement is due to noise
based on insufficiently clear sense definitions ver-
sus a systematic difference between individuals,
e.g., those who see a glass as half empty where
others see it as half full. To insure the opportu-
nity to observe how varied the labeling of a single
word can be, we collect word sense annotations
from multiple annotators. One potential benefit of
such investigation might be a better understanding
of how to model word meaning.
In sum, we hypothesize the following cases:
? Outliers: A small proportion of annotators
may assign senses in a manner that differs
markedly from the remaining annotators.
? Confusability of senses: If multiple annota-
tors assign multiple senses in an apparently
random fashion, it may be that the senses are
not sufficiently distinct.
? Systematic differences among subsets of an-
notators: If the same 50% of annotators al-
ways pick sense X where the remaining an-
notators always pick sense Y, it may be that
properties of the annotators, such as their age
cohort, account for the disagreement.
4 Related Work
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al, 2005a)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al, 2005a).
Differences in IA and system performance
across part-of-speech have been examined, as
in (Ng et al, 1999; Palmer et al, 2005a). Fac-
tors that have been proposed as affecting agree-
ment include whether annotators are allowed to as-
sign multilabels (Ve?ronis, 1998; Ide et al, 2002;
Passonneau et al, 2006), the number or granu-
larity of senses (Ng et al, 1999), merging of re-
lated senses (Snow et al, 2007), sense similar-
ity (Chugur et al, 2002), entropy (Diab, 2004;
Palmer et al, 2005a), and reactions times required
to distinguish senses (Klein and Murphy, 2002;
Ide and Wilks, 2006).
We anticipate that one of the ways in which the
data will be used will be to train machine learning
approaches to WSD. Noise in labeling and the im-
pact on machine learning has been discussed from
various perspectives. In (Reidsma and Carletta,
2008), it is argued that machine learning perfor-
mance does not vary consistently with interannota-
tor agreement. Through a simulation study, the au-
thors find that machine learning performance can
degrade or not with lower agreement, depending
on whether the disagreement is due to noise or sys-
tematic behavior. Noise has relatively little impact
compared with systematic disagreements. In (Pas-
sonneau et al, 2008), a similar lack of correla-
tion between interannotator agreement and ma-
chine learning performance is found in an empiri-
cal investigation.
5 Word Sense Annotation Data
5.1 Trained Annotator data
The Manually Annotated Sub-Corpus (MASC)
project (Ide et al, 2010) is creating a small,
representative corpus of American English written
and spoken texts drawn from the Open American
National Corpus (OANC).2 The MASC corpus
includes hand-validated or manual annotations
for a variety of linguistic phenomena. The first
MASC release, available as of May 2010, consists
of 82K words.3 One of the goals of MASC is
to support efforts to harmonize WordNet (Miller
et al, 1993) and FrameNet (Ruppenhofer et al,
2006), in order to bring the sense distinctions each
makes into better alignment.
We chose ten fairly frequent, moderately poly-
semous words for sense tagging. One hundred oc-
currences of each word were sense annotated by
five or six trained annotators. The ten words are
shown in Table 1, the words are grouped by part of
speech, with the number of WordNet senses, the
number of senses used by the trained annotators
(TAs), the number of annotators, and Alpha. We
call this the Trained annotator (TA) data.
We find that interannotator agreement (IA)
among half a dozen annotators varies depending
on the word. For ten words nearly balanced with
2http://www.anc.org
3http://www.anc.org/MASC/Home.html
49
Senses
Word-pos Avail. Used Ann Alpha
long-j 9 4 6 0.67
fair-j 10 6 5 0.54
quiet-j 6 5 6 0.49
time-n 10 8 5 0.68
work-n 7 7 5 0.62
land-n 11 9 6 0.49
show-v 12 10 5 0.46
tell-v 8 8 6 0.46
know-v 11 10 5 0.37
say-v 11 10 6 0.37
Table 1: Interannotator agreement on ten poly-
semous words: three adjectives, three nouns and
four verbs among trained annotators
respect to part of speech, we find a range of about
0.50 to 0.70 for nouns and adjectives, and about
0.37 to 0.46 for verbs. Table 1 shows the ten words
and the alpha scores for the same five or six an-
notators. The layout of the table illustrates both
that verbs have lower agreement than adjectives
or nouns, and that within each part of speech, an-
notators achieve varying levels of agreement, de-
pending on the word. The annotators, their level
of training, the number of sense choices, the anno-
tation tool, and other factors remain constant from
word to word. Thus we hypothesize that the differ-
ences in IA reflect differences in the degree of sub-
jectivity of the sense choices, the sense similarity,
or both. Anveshan is a data exploration framework
to help understand the differences in the ability of
the same annotators to agree well on sense anno-
tation for some words and not others.
As shown, annotators achieve respectable
agreement on long, time and work, and lower
agreement on the remaining words. Verbs have
lower agreement overall.
Figure 1 shows WordNet senses for long in the
form displayed to annotators, who used an annota-
tion GUI developed in Java. The sense number ap-
pears in the first column, followed by the glosses,
then sample phrases; only three senses are shown,
to conserve space. Note that annotators did not see
the WordNet synsets (sets of synonymous words)
for a given sense.
5.2 Mechanical Turk data
Amazon?s Mechanical Turk is a crowd-sourcing
marketplace where Human Intelligence Tasks
Senses
Word-pos Avail. Used Ann Alpha
long-j 9 9 14 0.15
fair-j 10 10 14 0.25
quiet-j 6 6 15 0.08
Table 2: Interannotator agreement on adjectives
among Mechanical Turk annotators
(HITs) such as sense annotation for words in a
sentence, can be set up and results from a large
number of annotators (or turkers) can be obtained
quickly. We used Mechanical Turk to obtain anno-
tations from 14 annotators on the set of adjectives
to analyze IA for a larger set of untrained annota-
tors.
The task was set up to get 150 occurrences an-
notated for each of the three adjectives: fair, long
and quiet, by 14 mechanical turk annotators each.
100 of these occurrences were the same as those
done by the trained annotators. For each word,
the 150 instances were divided into 15 HITs of 10
instances each. The average submit time of a HIT
was 200 seconds. We report the IA among the Me-
chanical Turk annotators using Krippendorff?s Al-
pha in Table 2. As shown, the turkers have poor
agreement, particularly on long and quiet, which
is at the chance level.
6 Anveshan
Anveshan: Annotation Variance Estimation, is
our approach to perform a more subtle analysis
of inter-annotator agreement. Anveshan uses sim-
ple statistical methods to achieve the three goals
identified in section 3: outlier detection, confus-
able senses, and distinct subsets of annotators that
agree with each other.
6.1 Method
This section uses the following notation to explain
Anveshan?s methodology:
We assume that we have n annotators annotat-
ingm senses. The probability of annotator a using
sense si is given by
Pa(S = si) =
count(si, a)
?m
j=1 count(sj , a)
where, count(si, a) is number of times si was
used by a.
50
1 primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time
or a duration as specified: ?a long life?; ?a long boring speech?; ?a long time?; ?a long friendship?;
?a long game?; ?long ago?; ?an hour long?
2 primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified:
?a long road?; ?a long distance?; ?contained many long words?; ?ten miles long?
3 of relatively great height: ?a race of long gaunt men? (Sherwood Anderson); ?looked out the long French windows?
Figure 1: Three of the WordNet senses for ?Long?
Anveshan uses the Kullbach-Liebler divergence
(KLD), Jensen-Shannon divergence (JSD) and
Leverage to compare probability distributions.
The KLD of two probability distributions P and
Q is given by:
KLD(P,Q) =
?
i
P (i) log
P (i)
Q(i)
JSD is a modified version of KLD, it is also
known as total divergence to the average, and is
given by:
JSD(P,Q) =
1
2
KLD(P,M) +
1
2
KLD(Q,M)
where
M = (P + Q)/2
We define Leverage Lev of probability distribu-
tion P over Q as:
Lev(P,Q) =
?
k
|P (k) ?Q(k)|
We now compute the following statistics:
? For each annotator ai, we compute Pai .
? We compute Pavg, which is (
?
i Pai)/n.
? We compute Lev(Pai , Pavg),?i
? Then we compute JSD(Pai , Paj ) ?(i, j),
where i, j ? n and i 6= j
? Lastly, we compute a distance measure for
each annotator, by computing the KLD be-
tween each annotator and the average of
the remaining annotators, i.e. we get
?i,Dai = KLD(Pai , Q), where Q =
(
?
j 6=i Paj )/(n? 1)
These statistics give us a deeper understanding
of annotator behavior. Looking at the sense us-
age probabilities, we can identify how frequently
senses are used by an annotator. We can see how
much an annotator deviates from the average sense
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
A107	 ? A101	 ? A103	 ? A102	 ? A105	 ? A108	 ?
Figure 2: Distance measure (KLD) for Annotators
of long in TA Data
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
A101	 ? A102	 ? A103	 ? A105	 ? A107	 ? A108	 ?
101	 ?
102	 ?
999	 ?
103	 ?
108	 ?
Figure 3: Sense Usage distribution for long by an-
notators in TA Data
usage distribution by looking at Leverage. JSD be-
tween two annotators gives us a measure of how
close they are to each other. KLD of an annota-
tor with the remaining annotators shows us how
different the annotator is from the rest. In the fol-
lowing section we show results, which illustrate
the effectiveness of Anveshan in identifying use-
ful patterns in the data from the trained annotators
(TAs) and Mechanical Turkers (MTs).
6.2 Results
We used Anveshan on all data from TAs and MTs.
We were successful in correctly identifying out-
liers on many words. Also, analyzing the sense
usage patterns and observing the JSD and KLD
scores gave us useful insights on annotator differ-
ences. In the figures for this section, the six TAs
are represented by their unique identifiers (A101,
A102, A103, A105, A107, A108). Word senses
are identified by adding 100 to the WordNet sense
51
Word Old Alpha Ann Dropped New Alpha
long 0.67 1 0.80
land 0.49 1 0.54
know 0.377 1 0.48
tell 0.45 2 0.52
say 0.37 2 0.44
fair 0.54 2 0.63
Table 3: Increase in IA score by dropping annota-
tors (TA Data)
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
A102	 ?
A105	 ?
Figure 4: Sense usage patterns of annotators ?102?
and ?105? for show in TA Data
number. An additional ?None of the Above? label
is represented as 999; annotators select this when
no sense applies, when the word occurs as part of
a large lexical unit (collocation) with a clearly dis-
tinct meaning, or when the sentence is not a cor-
rect example for other reasons (e.g., wrong part of
speech).
Figure 2 shows the distance measure (KLD) for
each annotator from the rest of the annotators for
the word long with respect to the probability for
each of the four senses used (cf. Table 1). It can
be clearly seen that annotator A108 is an outlier.
A108 differs in her excessive use of label 999, as
shown in Figure 3. Indeed, by dropping A108,
we see that the IA score (Alpha) jumps from 0.67
to 0.8 for long. Similar results were obtained
for annotations for other words as well. Table 3
shows the jump in IA score after outlier(s) were
dropped.
Anveshan helps us differentiate between noisy
disagreement versus systematic disagreement.
The word show with 5 annotators has a low
agreement score of 0.45. By looking at the
sense distributions for the various annotators,
and observing annotation preferences for each
annotator, we can see that annotators A102 and
A105 have similar behavior (Figure 4, with a
pairwise alpha of 0.52 versus 0.46 for all five
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
A107	 ?
A108	 ?
Figure 5: Sense usage patterns of annotators ?107?
and ?108? for show in TA Data
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
Overall	 ?
A101	 ?
Figure 6: Sense usage distribution of annotator
?101? vs. the average of all annotators for show
in TA Data
annotators), and annotators A107 and A108 have
similar behavior (Figure 5, with a pairwise alpha
of 0.53). In contrast, Annotator A101 has very
distinct preferences (Figure 6). This behavior
is captured by computing JSD scores among all
pairs of annotators. As can be seen in Figure 7,
the pairs A102-A105 and A107-A108 have very
low JSD values, indicating similarity in annotator
behavior. At the same time we also see the pairs
having A101 in them have a much higher JSD
score, which is attributed to the fact that A101
is different from everyone else. If we look at
corresponding Alpha scores, we see that pairs
having low JSD values have higher agreement
scores and vice versa.
Observing the sense usage distributions also
helps us identify confusable senses. For example,
Figure 8 shows us the differences in sense usage
patterns of A101, A103 and the average of all
annotators for the word say. We can see that
A101 and A103 deviate in distinct ways from the
average. A101 prefers sense 101 whereas A103
prefers sense 102. This indicates that sense 101
and 102 might be confusable. Sense 1 is given
as ?expressing words?; sense 2 as ?report or
maintain?.
52
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
A105	 ? A108	 ? A105	 ? A102	 ? A107	 ? A108	 ?
A102	 ? A107	 ? A101	 ? A101	 ? A101	 ? A101	 ?
JSD	 ?
Alpha	 ?
Figure 7: JSD and Alpha scores for pairs of anno-
tators for show in TA Data
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
Overall	 ? A101	 ? A103	 ?
102	 ?
101	 ?
108	 ?
103	 ?
Figure 8: Sense usage distribution for say in TA
Data for annotators ?101? and ?103?
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
A102	 ? A105	 ? A108	 ? A101	 ? A107	 ?
Figure 9: Distance measure (KLD) for annotators
of work in TA Data
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
1.2	 ?
Overall	 ? 	 ?	 ?A101	 ? 	 ?	 ?A102	 ? 	 ?	 ?A104	 ? 	 ?	 ?A107	 ? 	 ?	 ?A108	 ? 	 ?	 ?A111	 ? 	 ?	 ?A112	 ? 	 ?	 ?A115	 ? 	 ?	 ?A116	 ? 	 ?	 ?A117	 ? 	 ?	 ?A118	 ? 	 ?	 ?A119	 ? 	 ?	 ?A120	 ? 	 ?	 ?A121	 ?
106	 ?109	 ?999	 ?108	 ?103	 ?102	 ?101	 ?
Figure 10: Sense usage distribution among MTs
for long
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
1.2	 ?
A10
1-??TA
	 ?
A10
2-??TA
	 ?
A10
3-??TA
	 ?
A10
4-??TA
	 ?
A10
5-??TA
	 ?
A10
2-??M
T	 ?
A10
6-??M
T	 ?
A10
7-??M
T	 ?
A10
8-??M
T	 ?
A11
4-??M
T	 ?
105	 ?
999	 ?
102	 ?
101	 ?
Figure 11: Sense usage distribution among TAs
and MTs for fair
Anveshan not only helps us understand under-
lying patterns in annotator behavior and remove
noise from IA scores, but also helps identify
cases where there is no noise and no systematic
subsets of annotators that agree with each other.
An example can be seen in for the noun work. We
observed that the annotators do not have largely
different behavior, which is reflected in Figure 9.
As none of the annotators are significantly differ-
ent from the others, the KLD scores are low and
the plotted line does not have any steep rises, as
seen in Figure 2.
Similar to the results for TA data, Anveshan
was successful in identifying outliers in Mechan-
ical Turk data as well. In order to compare the
agreement among TAs and MTs, we looked at IA
scores of all subsets of annotators for the three ad-
jectives in the Mechanical Turk data. We observed
that MTs used much more senses than TAs for all
words and that there was a lot of noise in sense us-
age distribution. Figure 10 illustrates the sense us-
age statistics for long among MTs, for frequently
used senses.
We also looked at agreement scores among all
subsets of MTs to see if there are any subsets of
annotators who agree as much as TAs, and we ob-
served that for both long and quiet, there were no
53
subsets of MT annotators whose agreement was
comparable or greater than the same number of the
TAs, however for fair, we found one set of 5 an-
notators whose IA score (0.61) was greater than
the IA score (0.54) of trained annotators. We also
observed that among both these pairs of annota-
tors, the frequently used senses were the same, as
illustrated in Figure 11. Still, the two groups of an-
notators have sufficiently distinct sense usage that
the overall IA for the combined set drops to 0.43.
7 Conclusion and Future Work
For annotations on a subjective task, there are
cases where there is no single correct label. In
this paper, we presented Anveshan, an approach to
study annotator behavior and to explore datasets
with multiple annotators, and with a large set of
annotation values. Here we looked at data from
half a dozen trained annotators and fourteen un-
trained Mechanical Turkers on word sense anno-
tation for polysemous words. The analysis using
Anveshan provided many insights into sources of
disagreement among the annotators.
We learn that IA Scores do not give us a com-
plete picture and it is necessary to delve deeper
and study annotator behavior in order to identify
noise possibly due to sense confusability, to elim-
inate noise due to outliers, and to identify system-
atic differences where subsets of annotators have
much higher IA than the full set.
The results from Anveshan are encouraging and
the methodology can be readily extended to study
patterns in human behavior. We plan to extend
our work by looking at JSD scores of all subsets
of annotators instead of pairs, to identify larger
subsets of annotators who have similar behavior.
We also plan to investigate other statistical meth-
ods of outlier detection such as the orthogonalized
Gnanadesikan-Kettenring estimator.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In HLT ?05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 579?586, Morristown, NJ,
USA. Association for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Marco Carbone, Yaakov Gal, Stuart Shieber, and Bar-
bara Grosz. 2004. Unifying annotated discourse hi-
erarchies to create a gold standard. In Proceedings
of the 5th Sigdial Workshop on Discourse and Dia-
logue.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32?39,
Philadelphia.
Jacob Cohen. 1960. A coeffiecient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
Mona Diab. 2004. Relieving the data acquisition bot-
tleneck in word sense disambiguation. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 303?311.
Jun Hu, Rebecca J. Passonneau, and Owen Rambow.
2009. Contrasting the interaction structure of an
email and a telephone corpus: A machine learning
approach to annotation of dialogue function units.
In Proceedings of the 10th SIGDIAL on Dialogue
and Discourse.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations, pages 47?74, Dordrecht, The Netherlands.
Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002.
Sense discrimination with parallel corpora. In Pro-
ceedings of ACL?02 Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Direc-
tions, pages 54?60, Philadelphia.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden.
Adam Kilgarriff and Martha Palmer. 2000. Introduc-
tion to the special issue on senseval. Computers and
the Humanities, 34:1?2.
Adam Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference
on Language Resources and Evaluation (LREC),
pages 581?588, Granada.
Devra Klein and Gregory Murphy. 2002. Paper has
been my ruin: Conceptual relations of polysemous
words. Journal of Memory and Language, 47:548?
70.
54
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Chuck P. Lam and David G. Stork. 2003. Evaluating
classifiers by means of test data with noisy labels.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence (IJCAI-03), pages
513?518, Acapulco.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science
Laboratory (CSL) Report 43, Princeton University,
Princeton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop
On Standardizing Lexical Resources.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005a. Making fine-grained and coarse-
grained sense distinctions. Journal of Natural Lan-
guage Engineering, 13.2:137?163.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005b. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71?106.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a mul-
tilingual semantic annotation task. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC), pages 1951?1956,
Genoa, Italy.
Rebecca Passonneau, Tom Lippincott, Tae Yano, and
Judith Klavans. 2008. Relation between agreement
measures on human labeling and machine learning
performance: results from an art history domain. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC),
pages 2841?2848.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02Workshop
on Word Sense Disambiguation: Recent Successes
and Future Directions, pages 40?46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 81?87.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Comput. Linguist.,
34(3):319?326.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceeding of the 14th ACM SIG KDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 614?622, Las Vegas.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1005?
1014, Prague.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254?263, Honolulu.
Jean Ve?ronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities, page 2005.
55
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30?38,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Sentiment Analysis of Twitter Data
Apoorv Agarwal Boyi Xie Ilia Vovsha Owen Rambow Rebecca Passonneau
Department of Computer Science
Columbia University
New York, NY 10027 USA
{apoorv@cs, xie@cs, iv2121@, rambow@ccls, becky@cs}.columbia.edu
Abstract
We examine sentiment analysis on Twitter
data. The contributions of this paper are: (1)
We introduce POS-specific prior polarity fea-
tures. (2) We explore the use of a tree kernel to
obviate the need for tedious feature engineer-
ing. The new features (in conjunction with
previously proposed features) and the tree ker-
nel perform approximately at the same level,
both outperforming the state-of-the-art base-
line.
1 Introduction
Microblogging websites have evolved to become a
source of varied kind of information. This is due to
nature of microblogs on which people post real time
messages about their opinions on a variety of topics,
discuss current issues, complain, and express posi-
tive sentiment for products they use in daily life. In
fact, companies manufacturing such products have
started to poll these microblogs to get a sense of gen-
eral sentiment for their product. Many times these
companies study user reactions and reply to users on
microblogs. One challenge is to build technology to
detect and summarize an overall sentiment.
In this paper, we look at one such popular mi-
croblog called Twitter and build models for classify-
ing ?tweets? into positive, negative and neutral senti-
ment. We build models for two classification tasks:
a binary task of classifying sentiment into positive
and negative classes and a 3-way task of classi-
fying sentiment into positive, negative and neutral
classes. We experiment with three types of models:
unigram model, a feature based model and a tree
kernel based model. For the feature based model
we use some of the features proposed in past liter-
ature and propose new features. For the tree ker-
nel based model we design a new tree representa-
tion for tweets. We use a unigram model, previously
shown to work well for sentiment analysis for Twit-
ter data, as our baseline. Our experiments show that
a unigram model is indeed a hard baseline achieving
over 20% over the chance baseline for both classifi-
cation tasks. Our feature based model that uses only
100 features achieves similar accuracy as the uni-
gram model that uses over 10,000 features. Our tree
kernel based model outperforms both these models
by a significant margin. We also experiment with
a combination of models: combining unigrams with
our features and combining our features with the tree
kernel. Both these combinations outperform the un-
igram baseline by over 4% for both classification
tasks. In this paper, we present extensive feature
analysis of the 100 features we propose. Our ex-
periments show that features that have to do with
Twitter-specific features (emoticons, hashtags etc.)
add value to the classifier but only marginally. Fea-
tures that combine prior polarity of words with their
parts-of-speech tags are most important for both the
classification tasks. Thus, we see that standard nat-
ural language processing tools are useful even in
a genre which is quite different from the genre on
which they were trained (newswire). Furthermore,
we also show that the tree kernel model performs
roughly as well as the best feature based models,
even though it does not require detailed feature en-
gineering.
We use manually annotated Twitter data for our
30
experiments. One advantage of this data, over pre-
viously used data-sets, is that the tweets are col-
lected in a streaming fashion and therefore represent
a true sample of actual tweets in terms of language
use and content. Our new data set is available to
other researchers. In this paper we also introduce
two resources which are available (contact the first
author): 1) a hand annotated dictionary for emoti-
cons that maps emoticons to their polarity and 2)
an acronym dictionary collected from the web with
English translations of over 5000 frequently used
acronyms.
The rest of the paper is organized as follows. In
section 2, we discuss classification tasks like sen-
timent analysis on micro-blog data. In section 3,
we give details about the data. In section 4 we dis-
cuss our pre-processing technique and additional re-
sources. In section 5 we present our prior polarity
scoring scheme. In section 6 we present the design
of our tree kernel. In section 7 we give details of our
feature based approach. In section 8 we present our
experiments and discuss the results. We conclude
and give future directions of research in section 9.
2 Literature Survey
Sentiment analysis has been handled as a Natural
Language Processing task at many levels of gran-
ularity. Starting from being a document level classi-
fication task (Turney, 2002; Pang and Lee, 2004), it
has been handled at the sentence level (Hu and Liu,
2004; Kim and Hovy, 2004) and more recently at
the phrase level (Wilson et al, 2005; Agarwal et al,
2009).
Microblog data like Twitter, on which users post
real time reactions to and opinions about ?every-
thing?, poses newer and different challenges. Some
of the early and recent results on sentiment analysis
of Twitter data are by Go et al (2009), (Bermingham
and Smeaton, 2010) and Pak and Paroubek (2010).
Go et al (2009) use distant learning to acquire senti-
ment data. They use tweets ending in positive emoti-
cons like ?:)? ?:-)? as positive and negative emoti-
cons like ?:(? ?:-(? as negative. They build mod-
els using Naive Bayes, MaxEnt and Support Vec-
tor Machines (SVM), and they report SVM outper-
forms other classifiers. In terms of feature space,
they try a Unigram, Bigram model in conjunction
with parts-of-speech (POS) features. They note that
the unigram model outperforms all other models.
Specifically, bigrams and POS features do not help.
Pak and Paroubek (2010) collect data following a
similar distant learning paradigm. They perform a
different classification task though: subjective ver-
sus objective. For subjective data they collect the
tweets ending with emoticons in the same manner
as Go et al (2009). For objective data they crawl
twitter accounts of popular newspapers like ?New
York Times?, ?Washington Posts? etc. They re-
port that POS and bigrams both help (contrary to
results presented by Go et al (2009)). Both these
approaches, however, are primarily based on ngram
models. Moreover, the data they use for training and
testing is collected by search queries and is therefore
biased. In contrast, we present features that achieve
a significant gain over a unigram baseline. In addi-
tion we explore a different method of data represen-
tation and report significant improvement over the
unigram models. Another contribution of this paper
is that we report results on manually annotated data
that does not suffer from any known biases. Our
data is a random sample of streaming tweets unlike
data collected by using specific queries. The size
of our hand-labeled data allows us to perform cross-
validation experiments and check for the variance in
performance of the classifier across folds.
Another significant effort for sentiment classifica-
tion on Twitter data is by Barbosa and Feng (2010).
They use polarity predictions from three websites as
noisy labels to train a model and use 1000 manually
labeled tweets for tuning and another 1000 manu-
ally labeled tweets for testing. They however do
not mention how they collect their test data. They
propose the use of syntax features of tweets like
retweet, hashtags, link, punctuation and exclamation
marks in conjunction with features like prior polar-
ity of words and POS of words. We extend their
approach by using real valued prior polarity, and by
combining prior polarity with POS. Our results show
that the features that enhance the performance of our
classifiers the most are features that combine prior
polarity of words with their parts of speech. The
tweet syntax features help but only marginally.
Gamon (2004) perform sentiment analysis on
feeadback data from Global Support Services sur-
vey. One aim of their paper is to analyze the role
31
of linguistic features like POS tags. They perform
extensive feature analysis and feature selection and
demonstrate that abstract linguistic analysis features
contributes to the classifier accuracy. In this paper
we perform extensive feature analysis and show that
the use of only 100 abstract linguistic features per-
forms as well as a hard unigram baseline.
3 Data Description
Twitter is a social networking and microblogging
service that allows users to post real time messages,
called tweets. Tweets are short messages, restricted
to 140 characters in length. Due to the nature of this
microblogging service (quick and short messages),
people use acronyms, make spelling mistakes, use
emoticons and other characters that express special
meanings. Following is a brief terminology associ-
ated with tweets. Emoticons: These are facial ex-
pressions pictorially represented using punctuation
and letters; they express the user?s mood. Target:
Users of Twitter use the ?@? symbol to refer to other
users on the microblog. Referring to other users in
this manner automatically alerts them. Hashtags:
Users usually use hashtags to mark topics. This
is primarily done to increase the visibility of their
tweets.
We acquire 11,875 manually annotated Twitter
data (tweets) from a commercial source. They have
made part of their data publicly available. For infor-
mation on how to obtain the data, see Acknowledg-
ments section at the end of the paper. They collected
the data by archiving the real-time stream. No lan-
guage, location or any other kind of restriction was
made during the streaming process. In fact, their
collection consists of tweets in foreign languages.
They use Google translate to convert it into English
before the annotation process. Each tweet is labeled
by a human annotator as positive, negative, neutral
or junk. The ?junk? label means that the tweet can-
not be understood by a human annotator. A man-
ual analysis of a random sample of tweets labeled
as ?junk? suggested that many of these tweets were
those that were not translated well using Google
translate. We eliminate the tweets with junk la-
bel for experiments. This leaves us with an unbal-
anced sample of 8,753 tweets. We use stratified sam-
pling to get a balanced data-set of 5127 tweets (1709
tweets each from classes positive, negative and neu-
tral).
4 Resources and Pre-processing of data
In this paper we introduce two new resources for
pre-processing twitter data: 1) an emoticon dictio-
nary and 2) an acronym dictionary. We prepare
the emoticon dictionary by labeling 170 emoticons
listed on Wikipedia1 with their emotional state. For
example, ?:)? is labeled as positive whereas ?:=(? is
labeled as negative. We assign each emoticon a label
from the following set of labels: Extremely-positive,
Extremely-negative, Positive, Negative, and Neu-
tral. We compile an acronym dictionary from an on-
line resource.2 The dictionary has translations for
5,184 acronyms. For example, lol is translated to
laughing out loud.
We pre-process all the tweets as follows: a) re-
place all the emoticons with a their sentiment po-
larity by looking up the emoticon dictionary, b) re-
place all URLs with a tag ||U ||, c) replace targets
(e.g. ?@John?) with tag ||T ||, d) replace all nega-
tions (e.g. not, no, never, n?t, cannot) by tag ?NOT?,
and e) replace a sequence of repeated characters by
three characters, for example, convert coooooooool
to coool. We do not replace the sequence by only
two characters since we want to differentiate be-
tween the regular usage and emphasized usage of the
word.
Acronym English expansion
gr8, gr8t great
lol laughing out loud
rotf rolling on the floor
bff best friend forever
Table 1: Example acrynom and their expansion in the
acronym dictionary.
We present some preliminary statistics about the
data in Table 3. We use the Stanford tokenizer (Klein
and Manning, 2003) to tokenize the tweets. We use
a stop word dictionary3 to identify stop words. All
the other words which are found in WordNet (Fell-
baum, 1998) are counted as English words. We use
1http://en.wikipedia.org/wiki/List of emoticons
2http://www.noslang.com/
3http://www.webconfs.com/stop-words.php
32
Emoticon Polarity
:-) :) :o) :] :3 :c) Positive
:D C: Extremely-Positive
:-( :( :c :[ Negative
D8 D; D= DX v.v Extremely-Negative
: | Neutral
Table 2: Part of the dictionary of emoticons
the standard tagset defined by the Penn Treebank for
identifying punctuation. We record the occurrence
of three standard twitter tags: emoticons, URLs and
targets. The remaining tokens are either non English
words (like coool, zzz etc.) or other symbols.
Number of tokens 79,152
Number of stop words 30,371
Number of English words 23,837
Number of punctuation marks 9,356
Number of capitalized words 4,851
Number of twitter tags 3,371
Number of exclamation marks 2,228
Number of negations 942
Number of other tokens 9047
Table 3: Statistics about the data used for our experi-
ments.
In Table 3 we see that 38.3% of the tokens are stop
words, 30.1% of the tokens are found in WordNet
and 1.2% tokens are negation words. 11.8% of all
the tokens are punctuation marks excluding excla-
mation marks which make up for 2.8% of all tokens.
In total, 84.1% of all tokens are tokens that we ex-
pect to see in a typical English language text. There
are 4.2% tags that are specific to Twitter which in-
clude emoticons, target, hastags and ?RT? (retweet).
The remaining 11.7% tokens are either words that
cannot be found in WordNet (like Zzzzz, kewl) or
special symbols which do not fall in the category of
Twitter tags.
5 Prior polarity scoring
A number of our features are based on prior po-
larity of words. For obtaining the prior polarity of
words, we take motivation from work by Agarwal
et al (2009). We use Dictionary of Affect in Lan-
guage (DAL) (Whissel, 1989) and extend it using
WordNet. This dictionary of about 8000 English
language words assigns every word a pleasantness
score (? R) between 1 (Negative) - 3 (Positive). We
first normalize the scores by diving each score my
the scale (which is equal to 3). We consider words
with polarity less than 0.5 as negative, higher than
0.8 as positive and the rest as neutral. If a word is not
directly found in the dictionary, we retrieve all syn-
onyms from Wordnet. We then look for each of the
synonyms in DAL. If any synonym is found in DAL,
we assign the original word the same pleasantness
score as its synonym. If none of the synonyms is
present in DAL, the word is not associated with any
prior polarity. For the given data we directly found
prior polarity of 81.1% of the words. We find po-
larity of other 7.8% of the words by using WordNet.
So we find prior polarity of about 88.9% of English
language words.
6 Design of Tree Kernel
We design a tree representation of tweets to combine
many categories of features in one succinct conve-
nient representation. For calculating the similarity
between two trees we use a Partial Tree (PT) ker-
nel first proposed by Moschitti (2006). A PT ker-
nel calculates the similarity between two trees by
comparing all possible sub-trees. This tree kernel
is an instance of a general class of convolution ker-
nels. Convolution Kernels, first introduced by Haus-
sler (1999), can be used to compare abstract objects,
like strings, instead of feature vectors. This is be-
cause these kernels involve a recursive calculation
over the ?parts? of abstract object. This calculation
is made computationally efficient by using Dynamic
Programming techniques. By considering all possi-
ble combinations of fragments, tree kernels capture
any possible correlation between features and cate-
gories of features.
Figure 1 shows an example of the tree structure
we design. This tree is for a synthesized tweet:
@Fernando this isn?t a great day for playing the
HARP! :). We use the following procedure to con-
vert a tweet into a tree representation: Initialize the
main tree to be ?ROOT?. Then tokenize each tweet
and for each token: a) if the token is a target, emoti-
con, exclamation mark, other punctuation mark, or a
negation word, add a leaf node to the ?ROOT? with
33
VBGfor
EW
POSplayingPOS
STOP
NN dayPOS
EW EW
NN CAPS harp
EXC ||P||STOP
this
||T||
great
ROOT
NOTSTOP EW
is JJ
VB
GG forEW POS
VB
GG f orE
VBGf
orEW VBG ffoff ffrff
Figure 1: Tree kernel for a synthesized tweet: ?@Fernando this isn?t a great day for playing the HARP! :)?
the corresponding tag. For example, in the tree in
Figure 1 we add tag ||T || (target) for ?@Fernando?,
add tag ?NOT? for the token ?n?t?, add tag ?EXC?
for the exclamation mark at the end of the sentence
and add ||P || for the emoticon representing positive
mood. b) if the token is a stop word, we simply add
the subtree ? (STOP (?stop-word?))? to ?ROOT?. For
instance, we add a subtree corresponding to each of
the stop words: this, is, and for. c) if the token is
an English language word, we map the word to its
part-of-speech tag, calculate the prior polarity of the
word using the procedure described in section 5 and
add the subtree (EW (?POS? ?word? ?prior polarity?))
to the ?ROOT?. For example, we add the subtree
(EW (JJ great POS)) for the word great. ?EW? refers
to English word. d) For any other token <token>
we add subtree ?(NE (<token>))? to the ?ROOT?.
?NE? refers to non-English.
The PT tree kernel creates all possible subtrees
and compares them to each other. These subtrees
include subtrees in which non-adjacent branches be-
come adjacent by excising other branches, though
order is preserved. In Figure 1, we show some of
the tree fragments that the PT kernel will attempt to
compare with tree fragments from other trees. For
example, given the tree (EW (JJ) (great) (POS)), the
PT kernel will use (EW (JJ) (great) (POS)), (EW
(great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)),
(EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ),
(great), and (POS). This means that the PT tree ker-
nel attempts to use full information, and also ab-
stracts away from specific information (such as the
lexical item). In this manner, it is not necessary to
create by hand features at all levels of abstraction.
7 Our features
We propose a set of features listed in Table 4 for our
experiments. These are a total of 50 type of features.
We calculate these features for the whole tweet and
for the last one-third of the tweet. In total we get
100 additional features. We refer to these features as
Senti-features throughout the paper.
Our features can be divided into three broad cat-
egories: ones that are primarily counts of various
features and therefore the value of the feature is a
natural number ? N. Second, features whose value
is a real number ? R. These are primarily features
that capture the score retrieved from DAL. Thirdly,
features whose values are boolean ? B. These are
bag of words, presence of exclamation marks and
capitalized text. Each of these broad categories is
divided into two subcategories: Polar features and
Non-polar features. We refer to a feature as polar
if we calculate its prior polarity either by looking
it up in DAL (extended through WordNet) or in the
emoticon dictionary. All other features which are
not associated with any prior polarity fall in the Non-
polar category. Each of Polar and Non-polar features
is further subdivided into two categories: POS and
Other. POS refers to features that capture statistics
about parts-of-speech of words and Other refers to
all other types of features.
In reference to Table 4, row f1 belongs to the cat-
egory Polar POS and refers to the count of number
of positive and negative parts-of-speech (POS) in a
tweet, rows f2, f3, f4 belongs to the category Po-
34
lar Other and refers to count of number of negation
words, count of words that have positive and neg-
ative prior polarity, count of emoticons per polarity
type, count of hashtags, capitalized words and words
with exclamation marks associated with words that
have prior polarity, row f5 belongs to the category
Non-Polar POS and refers to counts of different
parts-of-speech tags, rows f6, f7 belong to the cat-
egory Non-Polar Other and refer to count of num-
ber of slangs, latin alphabets, and other words with-
out polarity. It also relates to special terms such as
the number of hashtags, URLs, targets and newlines.
Row f8 belongs to the category Polar POS and cap-
tures the summation of prior polarity scores of words
with POS of JJ, RB, VB and NN. Similarly, row f9
belongs to the category Polar Other and calculates
the summation of prior polarity scores of all words,
row f10 refers to the category Non-Polar Other and
calculates the percentage of tweet that is capitalized.
Finally, row f11 belongs to the category Non-
Polar Other and refers to presence of exclamation
and presence of capitalized words as features.
8 Experiments and Results
In this section, we present experiments and results
for two classification tasks: 1) Positive versus Nega-
tive and 2) Positive versus Negative versus Neutral.
For each of the classification tasks we present three
models, as well as results for two combinations of
these models:
1. Unigram model (our baseline)
2. Tree kernel model
3. 100 Senti-features model
4. Kernel plus Senti-features
5. Unigram plus Senti-features
For the unigram plus Senti-features model, we
present feature analysis to gain insight about what
kinds of features are adding most value to the model.
We also present learning curves for each of the mod-
els and compare learning abilities of models when
provided limited data.
Experimental-Set-up: For all our experiments we
use Support Vector Machines (SVM) and report av-
eraged 5-fold cross-validation test results. We tune
the C parameter for SVM using an embedded 5-fold
cross-validation on the training data of each fold,
i.e. for each fold, we first run 5-fold cross-validation
only on the training data of that fold for different
values of C. We pick the setting that yields the best
cross-validation error and use that C for determin-
ing test error for that fold. As usual, the reported
accuracies is the average over the five folds.
8.1 Positive versus Negative
This is a binary classification task with two classes
of sentiment polarity: positive and negative. We use
a balanced data-set of 1709 instances for each class
and therefore the chance baseline is 50%.
8.1.1 Comparison of models
We use a unigram model as our baseline. Re-
searchers report state-of-the-art performance for
sentiment analysis on Twitter data using a unigram
model (Go et al, 2009; Pak and Paroubek, 2010).
Table 5 compares the performance of three models:
unigram model, feature based model using only 100
Senti-features, and the tree kernel model. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the uni-
gram and the Senti-features by 2.58% and 2.66% re-
spectively. The 100 Senti-features described in Ta-
ble 4 performs as well as the unigram model that
uses about 10,000 features. We also experiment with
combination of models. Combining unigrams with
Senti-features outperforms the combination of ker-
nels with Senti-features by 0.78%. This is our best
performing system for the positive versus negative
task, gaining about 4.04% absolute gain over a hard
unigram baseline.
8.1.2 Feature Analysis
Table 6 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add
all non-polar features (rows f5, f6, f7, f10, f11 in Ta-
ble 4) and observe no improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures (rows f1, f8) and observe a gain of 3.49% over
the unigram baseline. We see an additional increase
in accuracy by 0.55% when we add other prior po-
larity features (rows f2, f3, f4, f9 in Table 4). From
35
N
Polar
POS # of (+/-) POS (JJ, RB, VB, NN) f1
Other # of negation words, positive words, negative words f2
# of extremely-pos., extremely-neg., positive, negative emoticons f3
# of (+/-) hashtags, capitalized words, exclamation words f4
Non-Polar
POS # of JJ, RB, VB, NN f5
Other # of slangs, latin alphabets, dictionary words, words f6
# of hashtags, URLs, targets, newlines f7
R
Polar
POS For POS JJ, RB, VB, NN,
?
prior pol. scores of words of that POS f8
Other
?
prior polarity scores of all words f9
Non-Polar Other percentage of capitalized text f10
B Non-Polar Other exclamation, capitalized text f11
Table 4: N refers to set of features whose value is a positive integer. They are primarily count features; for example,
count of number of positive adverbs, negative verbs etc. R refers to features whose value is a real number; for example,
sum of the prior polarity scores of words with part-of-speech of adjective/adverb/verb/noun, and sum of prior polarity
scores of all words. B refers to the set of features that have a boolean value; for example, presence of exclamation
marks, presence of capitalized text.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 71.35 1.95
Senti-features 71.27 0.65
Kernel 73.93 1.50
Unigram +
Senti-features
75.39 1.29
Kernel +
Senti-features
74.61 1.43
Table 5: Average and standard deviation for test accuracy
for the 2-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and tree kernel plus senti-features.
these experiments we conclude that the most impor-
tant features in Senti-features are those that involve
prior polarity of parts-of-speech. All other features
play a marginal role in achieving the best performing
system. In fact, we experimented by using unigrams
with only prior polarity POS features and achieved a
performance of 75.1%, which is only slightly lower
than using all Senti-features.
In terms of unigram features, we use Information
Gain as the attribute evaluation metric to do feature
selection. In Table 7 we present a list of unigrams
that consistently appear as top 15 unigram features
across all folds. Words having positive or negative
prior polarity top the list. Emoticons also appear as
important unigrams. Surprisingly though, the word
for appeared as a top feature. A preliminary analy-
Features Acc.
F1 Measure
Pos Neg
Unigram baseline 71.35 71.13 71.50
+ f5, f6, f7, f10, f11 70.1 69.66 70.46
+ f1, f8 74.84 74.4 75.2
+ f2, f3, f4, f9 75.39 74.81 75.86
Table 6: Accuracy and F1-measure for 2-way classifica-
tion task using Unigrams and Senti-features. All fi refer
to Table 4 and are cumulative.
Positive words love, great, good, thanks
Negative words hate, shit, hell, tired
Emoticons ||P || (positive emoticon),
||N || (negative emoticon)
Other for, ||U || (URL)
Table 7: List of top unigram features for 2-way task.
sis revealed that the word for appears as frequently
in positive tweets as it does in negative tweets. How-
ever, tweets containing phrases like for you and for
me tend to be positive even in the absence of any
other explicit prior polarity words. Owing to previ-
ous research, the URL appearing as a top feature is
less surprising because Go et al (2009) report that
tweets containing URLs tend to be positive.
8.1.3 Learning curve
The learning curve for the 2-way classification
task is in Figure 2. The curve shows that when lim-
36
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 162
64
66
68
70
72
74
76
Percentage of training data
Ac
cur
acy
 (%
)
 
 
Unigram
Unigram + Our Features
Tree Kernel
Figure 2: Learning curve for two-way classification task.
ited data is used the advantages in the performance
of our best performing systems is even more pro-
nounced. This implies that with limited amount of
training data, simply using unigrams has a critical
disadvantage, while both tree kernel and unigram
model with our features exhibit promising perfor-
mance.
8.2 Positive versus Negative versus Neutral
This is a 3-way classification task with classes
of sentiment polarity: positive, negative and neu-
tral. We use a balanced data-set of 1709 instances
for each class and therefore the chance baseline is
33.33%.
8.2.1 Comparison of models
For this task the unigram model achieves a gain
of 23.25% over chance baseline. Table 8 compares
the performance of our three models. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the un-
igram and the Senti-features model by 4.02% and
4.29% absolute, respectively. We note that this dif-
ference is much more pronounced comparing to the
two way classification task. Once again, our 100
Senti-features perform almost as well as the unigram
baseline which has about 13,000 features. We also
experiment with the combination of models. For
this classification task the combination of tree ker-
nel with Senti-features outperforms the combination
of unigrams with Senti-features by a small margin.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 56.58 1.52
Senti-features 56.31 0.69
Kernel 60.60 1.00
Unigram +
Senti-features
60.50 2.27
Kernel +
Senti-features
60.83 1.09
Table 8: Average and standard deviation for test accuracy
for the 3-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and Senti-features plus tree kernels.
This is our best performing system for the 3-way
classification task, gaining 4.25% over the unigram
baseline.
The learning curve for the 3-way classification
task is similar to the curve of the 2-way classifica-
tion task, and we omit it.
8.2.2 Feature Analysis
Table 9 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add all
non-polar features (rows f5, f6, f7, f10 in Table 4)
and observe an small improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures and observe a gain of 3.28% over the unigram
baseline. We see an additional increase in accuracy
by 0.64% when we add other prior polarity features
(rows f2, f3, f4, f9 in Table 4). These results are in
line with our observations for the 2-way classifica-
tion task. Once again, the main contribution comes
from features that involve prior polarity of parts-of-
speech.
Features Acc.
F1 Measure
Pos Neu Neg
Unigram baseline 56.58 56.86 56.58 56.20
+
f5, f6, f7, f10, f11
56.91 55.12 59.84 55
+ f1, f8 59.86 58.42 61.04 59.82
+ f2, f3, f4, f9 60.50 59.41 60.15 61.86
Table 9: Accuracy and F1-measure for 3-way classifica-
tion task using unigrams and Senti-features.
The top ranked unigram features for the 3-way
37
classification task are mostly similar to that of the
2-way classification task, except several terms with
neutral polarity appear to be discriminative features,
such as to, have, and so.
9 Conclusion
We presented results for sentiment analysis on Twit-
ter. We use previously proposed state-of-the-art un-
igram model as our baseline and report an overall
gain of over 4% for two classification tasks: a binary,
positive versus negative and a 3-way positive versus
negative versus neutral. We presented a comprehen-
sive set of experiments for both these tasks on manu-
ally annotated data that is a random sample of stream
of tweets. We investigated two kinds of models:
tree kernel and feature based models and demon-
strate that both these models outperform the unigram
baseline. For our feature-based approach, we do fea-
ture analysis which reveals that the most important
features are those that combine the prior polarity of
words and their parts-of-speech tags. We tentatively
conclude that sentiment analysis for Twitter data is
not that different from sentiment analysis for other
genres.
In future work, we will explore even richer lin-
guistic analysis, for example, parsing, semantic
analysis and topic modeling.
10 Acknowledgments
Agarwal and Rambow are funded by NSF grant
IIS-0713548. Vovsha is funded by NSF grant
IIS-0916200. We would like to thank NextGen
Invent (NGI) Corporation for providing us with
the Twitter data. Please contact Deepak Mit-
tal (deepak.mittal@ngicorportion.com) about ob-
taining the data.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis using
lexical affect scoring and syntactic n-grams. Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 24?32, March.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 36?44.
Adam Bermingham and Alan Smeaton. 2010. Classify-
ing sentiment in microblogs: is brevity an advantage is
brevity an advantage? ACM, pages 1833?1836.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. MIT Press.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. Proceedings of the
20th international conference on Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
M Hu and B Liu. 2004. Mining and summarizing cus-
tomer reviews. KDD.
S M Kim and E Hovy. 2004. Determining the sentiment
of opinions. Coling.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pages 423?430.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity analysis using sub-
jectivity summarization based on minimum cuts. ACL.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. ACL.
C M Whissel. 1989. The dictionary of Affect in Lan-
guage. Emotion: theory research and experience,
Acad press London.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase level sentiment analysis.
ACL.
38
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 248?258,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Embedded Wizardry
Rebecca J. Passonneau1, Susan L. Epstein2,3, Tiziana Ligorio3 and Joshua Gordon1
1Columbia University
New York, NY, USA
(becky|joshua)@cs.columbia.edu
2,3Hunter College
3The Graduate Center of the City University of New York
New York, NY, USA (susan.epstein@hunter|tligorio@gc).cuny.edu
Abstract
This paper presents a progressively challeng-
ing series of experiments that investigate clar-
ification subdialogues to resolve the words in
noisy transcriptions of user utterances. We fo-
cus on user utterances where the user?s spe-
cific intent requires little additional inference,
given sufficient understanding of the form. We
learned decision-making strategies for a dia-
logue manager from run-time features of our
spoken dialogue system and from observation
of human wizards we had embedded within it.
Results show that noisy ASR can be resolved
based on predictions from context about what
a user might say, and that dialogue manage-
ment strategies for clarifications of linguistic
form benefit from access to features from spo-
ken language understanding.
1 Introduction
Utterances have literal meaning derived from their
linguistic form, and pragmatic intent, the actions
speakers aim to achieve through words (Austin,
1962). Because the channel is usually not noisy
enough to impede communication, misunderstand-
ings that arise between adult human interlocutors
are more often due to confusions about intent, rather
than about words. Between humans and machines,
however, verbal interaction has a much higher rate
of linguistic misunderstandings because the channel
is noisy, and machines are not as adept at using spo-
ken language. It is difficult to arrive at accurate rates
for misunderstandings of form versus intent in hu-
man conversation, because the two types cannot al-
ways be distinguished (Schlangen and Fern?andez,
2005). However, one estimate of the rate of mis-
understandings of literal meaning between humans,
based on text transcripts of the British National Cor-
pus, is in the low range of 4% (Purver et al, 2001),
compared with a 30% estimate for human-computer
dialogue (Rieser and Lemon, 2011). The thesis
of our work is that misunderstandings of linguis-
tic form in human-machine dialogue are more ef-
fectively resolved through greater reliance on con-
text, and through closer integration of spoken lan-
guage understanding (SLU) with dialogue manage-
ment (DM). We investigate these claims by focusing
on noisy speech recognition for utterances where the
user?s specific intent requires little additional infer-
ence, given sufficient understanding of the form.
This paper presents three experiments that pro-
gressively address SLU methods to compensate for
poor automated speech recognition (ASR), and com-
plementary DM strategies. In two of the experi-
ments, human wizards are embedded in the spoken
dialogue system while run-time SLU features are
collected. Many wizard-of-Oz investigations have
addressed the noisy channel issue for SDS (Zollo,
1999; Skantze, 2003; Williams and Young, 2004;
Skantze, 2005; Rieser and Lemon, 2006; Schlangen
and Fern?andez, 2005; Rieser and Lemon, 2011).
Like them, we study how human wizards solve the
joint problem of interpreting users? words and in-
ferring users? intents. Our work differs in its ex-
ploration of the role context can play in the literal
interpretation of noisy language. We rely on knowl-
edge in the backend database to propose candidate
linguistic forms for noisy ASR.
Our principal results are that both wizards and our
248
SDS can achieve high accuracy interpretations, in-
dicating that predictions about what the user might
be saying can play a significant role in resolving
noise. We show it is possible to achieve low rates
of unresolved misunderstanding, even at word error
rates (WER) as poor as 50%-70%. We achieve this
through machine learned models of DM actions that
combine standard DM features with a rich number
and variety of SLU features. The learned models
predict DM actions to determine whether a reliable
candidate interpretation exists for a noisy utterance,
and if not, what action to take. The results support
an approach to DM design that integrates the two
problems of understanding form and intent.
The next sections present related work, our library
domain and our baseline SDS architecture. Subse-
quent sections discuss the SLU settings across the
three experiments, and present the experimental de-
signs and results, discussion and conclusion.
2 Related Work
Previous WOz studies of wizards? ability to pro-
cess noisy transcriptions of speaker utterances in-
clude the use of real (Skantze, 2003; Zollo, 1999)
or simulated ASR (Kruijff-Korbayova? et al, 2005;
Williams and Young, 2004). WOz studies that
directed their attention to the wizard include ef-
forts to predict: the wizard?s response when the
user is not understood (Bohus 2004); the wizard?s
use of multimodal clarification strategies (Rieser
and Lemon, 2006; Rieser and Lemon, 2011); and
the wizard?s use of application-specific clarification
strategies (Skantze, 2003; Skantze, 2005). WOz
studies that address real or simulated ASR reveal
that wizards can find ways to not respond to utter-
ances they fail to understand (Zollo, 1999; Skantze,
2003; Kruijff-Korbayova? et al, 2005; Williams and
Young, 2004). For example, they can prompt the
user for an alternative attribute of the same object.
Our work differs in that we address clarifications
about the words used, and rely on a rich set of SLU
features. Further, we compare behavior across wiz-
ards. Our SDS benefits from models of the most
skilled wizards.
To limit communication errors incurred by faulty
ASR, an SDS can rely on strategies to detect and re-
spond to incorrect recognition output (Bohus, 2004).
The SDS can repeatedly request user confirmation
to avoid misunderstanding, or ask for confirmation
using language that elicits responses from the user
that the system can handle (Raux and Eskenazi,
2004). When the user adds unanticipated informa-
tion in response to a system prompt, two-pass recog-
nition can rely on a concept-specific language model
to improve the recognition of the domain concepts
within the utterance containing unknown words, and
thereby achieve better recognition (Stoyanchev and
Stent, 2009). An SDS could take this approach one
step further and use context-specific language for in-
cremental understanding of noisy input throughout
the dialogue (Aist et al, 2007).
Current work on error recovery and grounding for
SDS assumes that the primary responsibility of a
dialogue management strategy is to understand the
user?s intent. Errors of understanding are addressed
by ignoring the utterances where understanding fail-
ures occur, asking users to repeat, or pursuing clari-
fications about intent. These strategies typically rely
on knowledge sources that follow the SLU stage.
The RavenClaw dialogue manager, which represents
domain-dependent (task-based) DM strategy as a
tree of goals, triggers error handling by means of a
single confidence score associated with the concepts
hypothesized to represent the user?s intent (Bohus
and Rudnicky, 2002; Bohus and Rudnicky, 2009).
Features for reinforcement learning of MDP-based
DM strategies include a few lexical features and a
measure of noise analogous to WER (Rieser and
Lemon, 2011). The WOz studies reported here yield
learned models of specific actions in response to
noisy input, such as whether to treat a candidate in-
terpretation as correct, or to pursue one of many pos-
sible clarification strategies, including clarifications
of form or intent. These models rely on relatively
large numbers of features from all phases of spoken
language understanding, as well as on typical dia-
logue management features.
3 CheckItOut
3.1 Domain
Our domain of investigation simulates book orders
from the Andrew Heiskell Braille and Talking Book
Library, part of the New York Public Library and the
Library of Congress. Patrons order books by tele-
249
phone during conversation with a librarian, and re-
ceive them by mail. Patrons typically have identify-
ing information for the books they seek, which they
get from monthly newsletters. In a corpus of eighty
two calls recorded at the library, we found that most
book requests by title were very faithful to the actual
title. Challenges to SLU in this domain include the
size of the database, the size of the vocabulary, and
the average sentence length.
While large databases have been used for inves-
tigations of phonological query expansion (Georgila
et al, 2003), much of the research on DM strategy
relies on relatively small databases. A recent study
of reinforcement learning of DM strategy modeled
as a Markov Decision Process reported in (Rieser
and Lemon, 2011) relies on a database of 438 items.
In (Gordon and Passonneau, 2011) we compared
the SLU challenges faced by CheckItOut and the
Let?s Go bus schedule information system, both of
which rely on the same architecture (Raux et al,
2005). The Let?s Go corpus contained 70 bus routes
names and 1300 place names, and a mean utterance
length of 4.4 words. The work reported here uses the
full 2007 version of Heiskell?s database of 71,166
books and 28,031 authors, and a sanitized version
of its 2007 patron database of 5,028 active patrons.
Authors and titles contribute 45,636 distinct words,
with a 10.43% overlap between the two. Average
book title length is 5.4 words; 26% of titles are 1-2
words, 44% are 3-5 words, 20% are 6 to 10. Con-
sequently, our domain has relatively long utterances.
The syntax of book titles is much richer than typical
SDS slot fillers, such as place or person names.
To achieve high-confidence SLU, we integrate
voice search into the SLU components of our two
SDS experiments (Wang et al, 2008).1 Our custom
voice search query relies on Ratcliff/Obershershelp
(R/O) pattern matching (Ratcliff and Metzener,
1988), the ratio of the number of matching charac-
ters to the total length of both strings. This simple
metric captures gross similarities without overfitting
to a specific application domain. The criteria for se-
lecting R/O derive from our first offline experiment,
described in Section 4.2.
For an experiment focused only on a single turn
1In concurrent work on a new SDS architecture, we use en-
sembles of SLU strategies (Gordon and Passonneau, 2011; Gor-
don et al, 2011).
(a) Baseline CheckItOut
(b) Embedded Wizard
Figure 1: CheckItOut information pipeline
exchange beginning with a user book request, we
queried the backend directly with the ASR string.
For a subsequent experiment on full dialogues, we
queried the backend with a modified ASR string, be-
cause the SDS architecture we used permits backend
queries to occur only during the dialogue manage-
ment phase, after natural language understanding.
The next section describes this architecture.
3.2 Architecture
CheckItOut, our baseline SDS, employs the Olym-
pus/RavenClaw architecture developed at Carnegie
Mellon University (CMU) (Raux et al, 2005; Bo-
hus and Rudnicky, 2009). SDS modules commu-
nicate via message passing, controlled by a central
hub. However, the information flow is largely a
pipeline, as depicted in Figure 1(a). The Pocket-
Sphinx recognizer (Huggins-Daines et al, 2006) re-
ceives acoustic data segmented by the audio man-
ager, and passes a single recognition hypothesis to
the Phoenix parser (Ward and Issar, 1994). Phoenix
sends one or more equivalently ranked semantic
parses to the Helios confidence annotator (Bohus
and Rudnicky, 2002), which selects a parse and as-
signs a confidence score. The Apollo interaction
manager (Raux and Eskenazi, 2007) monitors the
three SLU modules?the recognizer, the semantic
parser, and the confidence annotator?to determine
whether the user or SDS has the current turn. To
a limited degree, Apollo can override the early seg-
mentation decisions based solely on pause length.
250
Confidence-annotated concepts from the semantic
parse are passed to the RavenClaw DM, which de-
cides when to prompt the user, present information
to her, or query the backend database.
A wizard server communicates with other mod-
ules via the hub, as shown in Figure 1(b). For each
wizard experiment, we constructed a graphical user
interface (GUI). Wizard GUIs display information
for the wizard in a manageable form, and allow the
wizard to query the backend or select communica-
tive actions that result in utterances directed to the
user. Figure 1(b) shows an arrow from the speech
recognizer directly to the wizard: the recognition
string has been vetted by Apollo before it is dis-
played to the wizard.
4 Experiments and Results
The experiments reported here are an off-line pilot
study to identify book titles under worst case recog-
nition (Title Pilot), an embedded WOz study of a
single turn exchange involving book requests by ti-
tle (Turn Exchange), and an embedded WOz study
of dialogues where users followed scenarios that in-
cluded four books at a time (Full WOz). To evaluate
the impact of learned models of wizard actions from
the Full WOz wizard data, we evaluated CheckItOut
before and after the dialogue manager was enhanced
with wizard models for specific actions.
4.1 Experimental Settings
All three experiments use the full database for
search. To control for WER, the knowledge sources
for speech recognition and semantic parsing vary
across experiments. For each experiment, Table 1
indicates the acoustic model (AM) used, the num-
ber of hours of domain-specific spontaneous speech
used for AM adaptation, the number of titles used
to construct the language model (LM), the type of
LM, the type of grammar rules in the Phoenix book
title subgrammar, and average WER as measured by
Levenstein word edit distance (Levenshtein, 1996).
For the first two experiments, we used CMU?s
Open Source WSJ1 dictation AMs for wideband
(16kHz) microphone (dictation) speech. For Full
WOz we adapted narrowband (8kHz) WSJ1 dicta-
tion speech with about eight hours of data collected
from Turn Exchange and two hours of scripted spon-
taneous speech typical of CheckItOut dialogues.
Logios is a CMU toolkit for generating a pseudo-
corpus from a Phoenix grammar. It produces a set
of strings generated by Phoenix production rules,
which in turn are used to build an LM (Carnegie
Mellon University Speech Group, 2008). Before we
explain the three rightmost columns in Table 1, we
first briefly describe Phoenix, the Phoenix book title
subgrammar, and how we combine title strings with
a Logios pseudo-corpus.
Phoenix is a context-free grammar (CFG) parser
that produces one or more semantic frames per
parse. A semantic frame has slots, where each slot is
a concept with its own CFG productions (subgram-
mar). To accommodate noisy ASR, the parser can
skip words between frames or slots. Phoenix is well-
suited for restricted domains, where a frame repre-
sents a particular type of subdialogue (e.g., ordering
a plane ticket), and slots represent constrained con-
cepts (e.g., departure city, destination city). Phoenix
is not well-suited for book titles, which have a rich
vocabulary and syntax, and no obvious component
slots. The CFG rules for the Turn Exchange book ti-
tle subgrammar consisted of a verbatim rule for each
book title. Rules that consisted of a bag-of-words
(BOW; i.e., unordered) for each title proved to be
too unconstrained.2 In Turn Exchange, interpreta-
tion of ASR consisted primarily of voice search; the
highly constrained CFG rules (exact words in exact
order) had little impact on performance. For base-
line CheckItOut dialogues, and for Full WOz, we
required more constrained grammar rules that would
preserve Phoenix?s robustness to noise.
To avoid the brittleness of exact string CFG rules,
and the massive over-generation of BOW CFG rules,
we wrote a transducer that mapped dependency
parses of book titles to CFG rules. When ASR
words are skipped, book title parses can consist of
multiple slots. We used MICA, a broad-coverage
dependency grammar (Bangalore et al, 2009) to
parse the entire book title database. When a set
of titles is selected for an experiment, the corre-
sponding MICA parses are transduced to the rele-
vant CFG productions, and inserted into a Phoenix
grammar. Productions for the author subgrammar
2BOW Phoenix rules for book titles are used in a more re-
cent Olympus/RavenClaw system inspired in part by Check-
ItOut (Lee et al, 2010), with a database of 15,088 eBooks.
251
Exp. AM Adapted # Titles for LM LM Grammar rules WER
Title Pilot WSJ1 16kHz NA 500 unigram NA 0.76
Turn Exchange WSJ1 16kHz NA 7,500 trigram title strings 0.71
Full WOz WSJ1 8kHz 10 hr. 3,000 Logios + book data Mica-based 0.50 (est)
Table 1: SLU settings across experiments
consist largely of a first name slot followed by a last
name slot. The remaining portions of the Phoenix
CheckItOut grammar consist of subgrammars for
book request prefixes and affixes (e.g., ?I would like
the book called?), for confirmations and rejections,
phone numbers, book catalogue numbers, and mis-
cellaneous additional concepts. The set of subgram-
mars excluding the book title and author subgram-
mars (book requests, confirmations, and so on; the
grammar shell) are the same for all experiments.
The MICA-based book title grammar also provides
several features (e.g., number of slots in a parse) for
machine learning.
The Title Pilot LM consisted of unigram frequen-
cies of the 1400 word types from a random sample
(without replacement) of 500 titles. For Turn Ex-
change, a trigram LM was constructed from 7,500
titles randomly selected from the 19,708 titles that
remained after we eliminated one-word titles and ti-
tles with below average circulation. For Full WOz,
3,000 books were randomly selected from the full
book database (with no more than three titles by
the same author, and no one-word titles). Logios
was used on the grammar shell to generate an initial
pseudo-corpus, which was combined with the book
title and author strings to generate a full pseudo-
corpus for the trigram LM (denoted as ?Logios +
book data? in Table 1).
4.2 Title Pilot
The Title Pilot (Passonneau et al, 2009) was an of-
fline investigation of how reliance on prior knowl-
edge in the database might facilitate interpretation
of noisy ASR. It demonstrates that given the context
of things a user might say, ASR that is otherwise un-
intelligible becomes intelligible.
Three males each read 50 randomly selected ti-
tles from the LM subset of 500 (see Table 1). Their
average WER was 0.75, 0.83 and 0.69, respectively.
Three undergraduates (A, B, C) were each given one
of the sets of 50 recognition strings from a different
speaker. Each also received a plain text file listing all
the titles in the database, and word frequency statis-
tics for the book titles. Their task was to try to find
the correct title, and to provide a brief description of
their overall strategy.
A was accurate on 66.7% of the titles he matched,
B and C on 71.7%. We identified similar strate-
gies for A and B, including number of exact word
matches, types of exact word matches (e.g., content
words were favored over stop words), rarity of ex-
act word matches, and phonetic similarity. Analysis
of C?s responses showed dependency on number and
types of exact word matches, and on miscellaneous
strategies that could not be grouped. Through in-
spection, we determined that similarity in length and
number of words were important factors. From this
experiment, we concluded that humans are adept at
interpreting noisy ASR when provided with context;
that voice search (queries to the backend with ASR)
would prove useful, given an appropriate similarity
metric; and that there would likely always be uncer-
tain cases that might lead to false hits. As we discuss
below, two of seven Turn Exchange wizards were
fairly adept, and five of six Full WOz wizards were
very adept, at avoiding false hits from voice search.
4.3 Turn Exchange
The offline Title Pilot suggested that voice search
could lead to far fewer non-understandings, given
some predictions as to the actual words a noisy ASR
string might represent. The next experiment ad-
dressed, in real time, the question of what level of
accuracy might be achieved through an online im-
plementation of voice search for book requests by
title (Passonneau et al, 2010; Ligorio et al, 2010b).
We embedded wizards into the CheckItOut SDS to
present them with live ASR, and to collect runtime
recognition features. On the GUI, variations in the
display fonts for ASR and voice search returns cued
the wizard to gross differences in word-level recog-
nition confidence, and similarities between an ASR
string and each candidate returned by the search.
Learned models of wizard actions indicated that
252
recognition features such as acoustic model fit and
speech rate, along with various measures of sim-
ilarity between the ASR output string and candi-
date titles, number of books ordered thus far (Re-
centSuccess), and number of relatively close candi-
date matches, were useful in modeling the most ac-
curate wizards. These results show that DM strat-
egy for determing what actions to take, given an in-
terpretation of a user request, can depend on subtle
recognition metrics.
In Turn Exchange, users requested books by ti-
tle from embedded wizards. Speech input and out-
put was by microphone and headset, with wizards
and users seated in separate rooms, each using a dif-
ferent GUI. Seven undergraduates (one female and
six males, including two non-native speakers of En-
glish) participated as paid subjects. Each of the 21
possible pairs of students met for five trials. A trial
had two sessions. In the first, one student served as
wizard and the other as user for a session in which
the user requested 20 books by title. In the second
session, the students reversed roles. We collected
4,192 turn exchanges.
The GUI displayed the ASR corresponding to the
user utterance, with confident words in bolder font.
The wizard could query the backend with some or
all of the ASR. Voice search results displayed a sin-
gle candidate above a high R/O threshold with all
matching words in boldface, or three candidates of
moderate similarity with matching words in medium
bold, or five to ten candidates of lower similarity in
grayscale. There were four available wizard actions:
to offer a candidate title to the user in a confident
manner (through Text-to-Speech), to offer a title ten-
tatively, to select two or more candidates and ask a
free-form question about them (here the user would
hear the wizard?s speech), or to give up. The user in-
dicated whether an offered candidate was correct, or
indicated the quality and appropriateness of a wiz-
ard?s question. A prize would go to the wizard who
offered the most correct titles.
The top ranked search return was correct 65.24%
of the time. The two wizards who most often offered
the top ranked return (81% and 86% of the time)
both achieved 69.5% accuracy. The two best wiz-
ards (W4 and W5) could detect search returns that
did not contain the correct title, thus avoiding false
hits. On average, they offered the top return only
73% of the time and both achieved the highest accu-
racy (83.4%).
Several classification methods were used to pre-
dict the four wizard actions: firm offer, tentative of-
fer, question, and give up. Features (N=60) included
many ASR metrics, such as word-level confidence,
AM fit, and three measures of speech rate; various
measures of the average similarity or overlap be-
tween the ASR string and the candidate titles from
the R/O query; the dialogue history; the number of
candidates titles returned; and so on. The learned
classifiers, including C4.5 decision trees (Quinlan,
1993), all had similar performance. Learned trees
for W4 and W5 both had F measures of 0.85. De-
cision trees give a transparent view of the relative
importance of features; those nearer the root have
greater discriminatory power. Common features at
the tops of trees for all wizards were the type and
size of the query return, how often the wizard had
chosen the correct title in the last three title cycles,
the average of the maximum number of contiguous
exact word matches between the ASR string and the
candidate titles, and the Helios confidence score.
We trained an additional decision tree to learn
how W4 (the best wizard) chose between offering
a title versus asking a question (F=0.91 for making
an offer; F=0.68 for asking a question). The tree
is distinctive in that it splits at the root on a mea-
sure of speech rate. If the ASR is short (as mea-
sured both by the number of recognition frames and
the words), W4 asks a question if the query return
is not a single title, and either RecentSuccess=1 or
ContiguousWord-Match=0, and the acoustic model
score is low. Note that shorter titles are more con-
fusable. If the ASR is long, W4 asks a question
when ContiguousWordMatch=1, RecentSuccess=2,
and either CandidateDisplay = NoisyList, or Helios
Confidence is low, and there is a choice of titles.
4.4 Full WOz
The third experiment was a full WOz study demon-
strating that embedded wizards could achieve high
task success by relying on a large number of actions
that included clarifications of utterance form or in-
tent. Here we briefly report results on task success
and time on task in a comparision of baseline Check-
ItOut with an enhanced version, CheckItOut+, that
incorporates learned models of wizard actions. The
253
evaluation demonstrates improved performance with
more books ordered, more correct books ordered,
and less elapsed time per book, or per correct book.
For Full WOz (Ligorio et al, 2010a), CheckItOut
relied on VOIP (Voice over Internet Protocol) tele-
phony. Users interacted with the embedded wizards
by telephone, and wizards took over after Check-
ItOut answered the phone. After familiarization
with the task and GUI, nine wizards auditioned and
six were selected. There were ten users. Both groups
were evenly balanced for gender. Users were di-
rected to a website that presented scenarios for each
call. The scenario page gave the user a patron iden-
tity and phone number, and author, title and cata-
logue number information for four books they were
to order. Each user was to make at least fifteen calls
to each wizard; we recorded 913 usable calls.
A single trainer prepared the original nine wizard
volunteers one at a time. First, each trainee practiced
on data from the experiments described above. Next,
the trainer explained the wizard GUI and demon-
strated it, serving as wizard on a sample call. Fi-
nally, the trainee served as wizard on five test calls
with guidance from the trainer. The trainer chose the
six most skilled and motivated trainees as wizards.
The GUI had two screens, one for user login
and one for book requests. Users identified them-
selves by scenario phone number. The book re-
quest screen had a scrollable frame displaying the
ASR for each user utterance. Separate frames on
the GUI displayed the query return, dialogue history,
basic actions (e.g., querying the backend with a cus-
tom R/O query, or prompting the user for a book),
and auxiliary actions (e.g., removing a book from
the order in progress). Finally, wizards could select
among four types of dialogue acts: signals of non-
understanding, or clarifications about the ASR, the
book request or the query return. A dialogue act se-
lected by the wizard was passed to a template-based
natural language generator, and then to a Text-to-
Speech component. Due to their complexity, calls
could be time consuming. A clock on the GUI indi-
cated call duration; wizards were instructed to finish
the current book request and then terminate the call
after six minutes.
A wizard?s precision is the proportion of books
she offer that correctly match the user?s request; five
of the six wizards had precision over 90%. A wiz-
ard?s recall is the number of books in the scenario
that she correctly identified. The two best wizards,
WA and WB, had the highest recall, 63% and 67%
respectively.
The number of book requests per dialogue was
tallied automatically. Some dialogues were termi-
nated before all scenario books could be requested.
Also, a wizard who experienced problems with a
book request could abandon the current request and
prompt the user for a new book. The user could re-
sume the abandoned book request later in the dia-
logue. In such cases, the abandoned and resumed re-
quests for the same book would count as two distinct
book requests. Given these facts, the ratio of number
of correct books to number of book requests yields
only an approximate estimate of how many scenario
books were correctly identified. WA correctly iden-
tified 2.69 books per call from 3.64 requests per call,
yielding a total success rate of 73.9% per book re-
quest, and 67.25% per 4-book scenario. WB cor-
rectly identified 2.54 books per call from 4.44 re-
quests per call, yielding success rates of 57.21% per
request and 63.50% per 4-book scenario. WA and
WB had quite distinct strategies. WA persisted with
each book request and exploited a wide range of
the available GUI actions, with the greatest num-
ber of actions per book request among all wizards
(N=8.24). WB abandoned book requests early and
moved on to the next book request, exploited rela-
tively fewer GUI actions, and had the fewest actions
per book request (N=5.10).
From 163 features that characterize the ASR,
search, current user utterance, current turn ex-
change, current book request, and the entire dia-
logue, we learned models for three types of wiz-
ard actions: select a non-understanding prompt, per-
form a search, or select a prompt to disambiguate
among search returns. We used three machine learn-
ing methods for classification: decision trees, logis-
tic regression and support vector machines. Table 2
gives the accuracies and overall F measures for de-
cision trees that model WA and WB. (All learning
methods have similar performance.)
Of note here is the range of features that predict
when the best wizards selected a non-understanding,
shown in Table 3. In addition, the two models de-
pend partly on different features. Trees for the other
actions in Table 2 have similarly diverse features.
254
Wizard Action Acc F
A Non-Understanding 0.71 0.71
B Non-Understanding 0.73 0.73
A Disambiguate 0.80 0.81
B Disambiguate 0.86 0.87
A Search 0.94 0.95
B Search 0.93 0.94
Table 2: Performance of learned trees
To evaluate the benefit of learned models of wiz-
ard actions for SDS, we conducted two data collec-
tions where subjects placed calls following the same
types of scenarios used in Full WOz. For our base-
line evaluation of CheckItOut, 10 subjects were re-
cruited from Columbia University and Hunter Col-
lege. Each was to place a minimum of 50 calls over
a period of three days; 562 calls were collected. For
each call, subjects visited a web page that presented
a new scenario. Each scenario included mock patron
data for the caller to use (e.g., name, address and
phone number), a list of four books, and instructions
to request one book by catalogue number, one by
title, one by author, and one by any of those meth-
ods. At three points during their calls, subjects com-
pleted a user satisfaction survey containing eleven
questions adapted from (Hone and Graham, 2006).
CheckItOut+ is an enhanced version of our SDS
in which the DM was modified to include learned
models for three decisions. The first determines
whether the system should signal non-understanding
in response to the caller?s last utterance, and exe-
cutes before voice search would take place. The
second determines whether to perform voice search
with the ASR (i.e., before the parse, in contrast to
CheckItOut). The third executes after voice search,
and determines whether to offer the candidate with
the highest R/O score to the user. The evaluation
setup for CheckItOut+ also included 10 callers who
were to place 50 calls each; 505 calls were collected.
Here we report results that compare the number
of books ordered per call, the number of correct
books per call, the elapsed time per book ordered,
and elapsed time per correct book. T-tests show all
differences to be highly significant. (A full discus-
sion of the evaluation results will appear in future
publications.) Callers to CheckItOut+ nearly always
ordered four books (3.998), compared with 3.217 for
the baseline (p < 0.0001). There was an increase
of correct books in the order from 2.40 in the base-
Feature WA WB
# books ordered so far Y Y
% unparsed ASR words Y N
Avg. word confidence Y N
# explicit confirms in call Y Y
# MICA slots per concept Y N
# searches in call Y N
Most recent wizard action N Y
Most frequent concept in call N Y
Speech rate N Y
# user utts. this request N Y
# author searches in call Y Y
Normalized LM score this utt Y Y
Table 3: Features that predict wizards? non-
understanding
line to 2.70 in CheckItOut+ (p < 0.0001). The total
elapsed time per call increased by only 13 seconds
from 210.93 to 223.96 (p < 0.0175). Given that
CheckItOut+ callers ordered more books and more
correct books, CheckItOut+ performed much faster.
The elapsed time per ordered book decreased from
65.57 to 56.01 seconds, and decreased from 87.89 to
82.95 seconds per correct books.
5 Discussion
Spoken language understanding has been relatively
under-investigated in SDS design. Our experiments
suggest that tighter integration of all phases of SLU
with dialogue management can lead to more robust
system behavior. We illustrate here with an exam-
ple of WA?s strategic questioning in which a non-
understanding is avoided, and WA builds on partial
understanding to identify the user?s objective.
In response to ASR MARY .JO. EARTH, where
the ?.?s bracket an unconfident word, WA?s search
returned three authors with first name Mary, and
last names that had moderate character overlap with
.JO. EARTH. WA first asked whether the book
was by Mary O?Hara. When the user responded
negatively, WA confirmed the first name, prompted
for the last name, and got SURE as the ASR. WA fi-
nally confirmed that the book was by Mary Stewart.
Although it took four turns, WA was able to identify
the correct book.
In general, the Full WOz corpus contains a very
high proportion of wizard questions. In the to-
tal corpus of 20,415 caller utterances, there were
11,562 wizard questions. The types of questions
255
S1: What?s the next book?
U1: .BARBARA. THREE THREE
S2: Is the author barbara freethy?
U2: YES
S3: Would you like ?some kind of wonderful? by
BARBARA FREETHY?
U3: YES
(a) Example 1
S1: Sorry, I misunderstood. Can you repeat the author?
U1: DIDN?T I AM THE .GOLDEN. ARM
S2: Is the title ?THE man with THE golden ARM ??
U2: NO
S3: Sorry, I misunderstood. Can you repeat the title please?
U3: .A. .AMBLING. .THE. .GAME. .EDELMAN. STORY
S4: Is the title ?up and running the jami goldman STORY ??
U4: YES
(b) Example 2
Figure 2: Sample Clarification Subdialogues
wizard?s ask not only often lead to successful con-
cept identification, they also avoid prompting the
user to repeat what they said. Previous work has
presented results showing that the hyperarticulation
associated with user repetitions often leads users to
slow their speech, speak more loudly, and pronounce
words more carefully, which hurts recognition per-
formance (Hirschberg et al, 2004).
Figure 2 illustrates two clarification subdialogues
from CheckItOut+. The first illustrates how prior
knowledge about what a user might say provides
sufficient constraints to interpret ASR that would
otherwise be unintelligible. The first word in the
ASR for the caller?s first utterance is bracketed by
?.?, which again represents low word confidence.
The high confidence words THREE THREE are
phonologically and orthographically similar to the
actual author name, Freethy. Note that from the
caller?s point of view, the same question shown
in S3 could be motivated by confusion over the
words alone, as in this case, or confusion over the
words and multiple candidate referents (e.g., Bar-
bara Freethy versus Freeling).
The second clarification subdialogue illustrates
how confusions about the linguistic input can be
resolved through strategies that combine questions
about words and intents. The prompt at system turn
3 indicates that the system believes that the caller
provided a title in user turn 1, which is incorrect.
The caller responds with the title, however, which
provides an alternative means to guess the intended
book, Jami Goldman?s memoir Up and Running.
6 Conclusion
The studies reported here are premised on two hy-
potheses about the role spoken language understand-
ing plays in SDS design. First, prior knowledge
derived from the context in which a dialogue takes
place can yield predictions about the words a user
might produce, and that these predictions can play
a key role in interpreting noisy ASR. Here we have
used context derived from knowledge in the appli-
cation database. Similar results could follow from
predictions from other sources, such as an explicit
model of the alignment of linguistic representa-
tions proposed in the work of Pickering and Gar-
rod (e.g., (Pickering and Garrod, 2006). Second,
closer integration of spoken language understanding
and dialogue management affords a wider range of
clarification subdialogues.
Our results from the experiments reported here
support both hypotheses. Our first experiment
demonstrated that words obscured by very noisy
ASR (50% ? WER ? 75%) can be inferred by re-
liance on what might have been said, predictions
that came from the database of entities in the do-
main. We assume that an SDS that interacts well
when ASR quality is poor will perform all the better
when ASR quality is good. Our second experiment
demonstrated that two of five human wizards were
able to achieve high accuracy in on-line resolution
of noisy ASR, when presented with no more than ten
candidate matches. Run-time recognition features
not available to the wizards were nonetheless useful
in modeling the ability of the two best wizards to
avoid false hits. Our third experiment demonstrated
that wizards could achieve high task success on full
dialogues where callers requested four books, and
an enhancement of our baseline SDS with learned
models of three wizard actions led to improved task
success with less time per subtask. The variety of
features that contribute to learned models of wiz-
ard actions demonstrates the advantages of embed-
ded wizardry, as well as the benefit of DM clarifica-
tion strategies that include features from all phases
of SLU.
256
Acknowledgments
The Loqui project is funded by the National Science
Foundation under awards IIS-0745369, IIS-0744904
and IIS-084966. We thank those at Carnegie Mel-
lon University who helped us construct Check-
ItOut through tutorials and work sessions held at
Columbia University and Carnegie Mellon Univer-
sity, and who responded to numerous emails about
the Olympus/RavenClaw architecture and compo-
nent modules: Alex Rudnicky, Brian Langner,
David Huggins-Daines, and Antoine Raux. We also
thank the many undergraduates from Columbia Col-
lege, Barnard College, and Hunter College who as-
sisted with tasks that supported the implementation
of CheckItOut, including the telephony.
References
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental dialogue
system faster than and preferred to its nonincremental
counterpart. In COGSCI 2007, pages 779?74.
John L. Austin. 1962. How to Do Things with Words.
Oxford University Press, New York.
Srinivas Bangalore, Pierre B. Boullier, Alexis Nasr,
Owen Rambow, and Beno??it Sagot. 2009. Mica: a
probabilistic dependency parser based on tree insertion
grammars. In NAACL/HLT, pages 185?188.
Dan Bohus and Alex Rudnicky. 2002. Integrating multi-
ple knowledge sources for utterance-level confidence
anno-tation in the CMU Communicator spoken dia-
logue system. Technical Report CS-02-190, Carnegie
Mellon University, Department of Computer Science.
Dan Bohus and Alex Rudnicky. 2009. The RavenClaw
dialog management framework. Computer Speech and
Language, 23:332?361.
Dan Bohus. 2004. Error awareness and recovery in con-
versational spoken language interfaces. Ph.D. thesis,
Carnegie Mellon University, Computer Science.
Carnegie Mellon University Speech Group. 2008.
The Logios tool. https://cmusphinx.svn.
sourceforge.net/svnroot/cmusphinx/
trunk/logios.
Kallirroi Georgila, Kyrakos Sgarbas, Anastasios
Tsopanoglou, Nikos Fakotakis, and George Kokki-
nakis. 2003. A speech-based human-computer
interaction system for automating directory assistance
services. International Journal of Speech Technology,
Special Issue on Speech and Human-Computer
Interaction, 6:145?59.
Joshua Gordon and Rebecca J. Passonneau. 2011.
An evaluation framework for natural language under-
standing in spoken dialogue systems. In 7th LREC.
Joshua Gordon, Rebecca J. Passonneau, and Susan L. Ep-
stein. 2011. Helping agents help their users despite
imperfect speech recognition. In Proceedings of the
AAAI Spring Symposium 2011 (SS11): Help Me Help
You: Bridging the Gaps in Human-Agent Collabora-
tion.
Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43(1-2):155?75.
Kate S. Hone and Robert Graham. 2006. Towards a tool
for the subjective assessment of speech system inter-
faces (sassi). Natural Language Engineering, Special
ISsue on Best Practice in Spoken Dialogue Systems,
6(3-4):287?303.
David Huggins-Daines, Mohit Kumar, Arthur Chan,
Allen W. Black, Mosur Ravishankar, and Alex I. Rud-
nicky. 2006. PocketSphinx: A free, real-time contin-
uous speech recognition system for hand-led devices.
In Proceedings of ICASSP, volume I, pages 185?188.
Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-
stenberger, Verena Rieser, Tilman Becker, Michael
Kaisser, Peter Poller, and Jan Schehl. 2005. An ex-
periment setup for collecting data for adaptive output
planning in a multimodal dialogue system. In 10th
ENLG, pages 191?196.
Cheongjae Lee, Alexander Rudnicky, and Gary Geunbae
Lee. 2010. Let?s buy books: finding ebooks using
voice search. In IEEE-SLT 2010, pages 442?447.
Vladimir I. Levenshtein. 1996. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Tiziana Ligorio, Susan L. Epstein, and Rebecca J. Pas-
sonneau. 2010a. Wizards? dialogue strategies to han-
dle noisy speech recognition. In IEEE-SLT 2010.
Tiziana Ligorio, Susan L. Epstein, Rebecca J. Passon-
neau, and Joshua Gordon. 2010b. What you did
and didn?t mean: Noise, context and human skill. In
COGSCI 10.
Rebecca J. Passonneau, Susan L. Epstein, and Joshua
Gordon. 2009. Help me understand you: Address-
ing the speech recognition bottleneck. In Proceedings
of the AAAI Spring Symposium 2009 (SS09): Agents
that Learn from Human Teachers, pages 23?25.
Rebecca J. Passonneau, Susan L. Epstein, Tiziana Ligo-
rio, Joshua Gordon, and Pravin Bhutada. 2010. Learn-
ing about voice search for spoken dialogue systems. In
NAACL-HLT 2010, pages 840?848.
Martin J. Pickering and Simon Garrod. 2006. Alignment
as the basis for successful communication. Research
on Language and Communication, 4(2):203?228.
Matthew Purver, Jonathan Ginzburg, and Patrick Healey.
2001. On the means for clarification in dialogue.
In Proceedings of the 2nd SIGdial Workshop on Dis-
course and Dialogue, pages 116?125.
257
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
John W. Ratcliff and David Metzener. 1988. Pattern
matching: the gestalt approach.
Antoine Raux and Maxine Eskenazi. 2004. Non-native
users in the Let?s Go! spoken dialogue systems. In
HLT/NAACL, pages 217?224.
Antoine Raux and Maxine A. Eskenazi. 2007. A multi-
layer architecture for semi-synchronous event-driven
dialogue management. In ASRU 2007, pages 514?519.
Antoine Raux, Brian Langner, Allan W. Black, and Max-
ine Eskenazi. 2005. Let?s Go Public! taking a spoken
dialogue system to the real world. In Interspeech - Eu-
rospeech 2005, pages 885?888.
Verena Rieser and Oliver Lemon. 2006. Using ma-
chine learning to explore human multimodal clarifica-
tion strategies. In COLING/ACL, pages 659?666.
Verena Rieser and Oliver Lemon. 2011. Learning and
evaluation of dialogue strategies for new applications:
Empirical methods for optimization from small data
sets. Computational Linguistics, 37:153?96.
David Schlangen and Raquel Fern?andez. 2005. Speak-
ing through a noisy channel ? experiments on induc-
ing clarification behaviour in human-human diaogue.
In 8th Annual Converence of the International Speech
Communication Association (INTERSPEECH 2007),
pages 1266?1269.
Gabriel Skantze. 2003. Exploring human error handling
strategies: Implications for spoken dialogue systems.
In Proceedings of ISCA Tutorial and Research Work-
shop on Error Handling in Spoken Dialogue Systems,
pages 71?76.
Gabriel Skantze. 2005. Exploring human recovery
strategies: Implications for spoken dialogue systems.
Speech Communication, 45:325?41.
Svetlana Stoyanchev and Amanda Stent. 2009. Predict-
ing concept types in user corrections in dialog. In
EACL Workshop SRSL, pages 42?49.
Ye-Yi Wang, Yu Dong, Yun-Cheng Ju, and Alex Acero.
2008. An introduction to voice search. IEEE Signal
Processing Magazine: Special ISsue on Spoken Lan-
guage Technology, 25(3):28?38.
Wayne Ward and Sunil Issar. 1994. Recent improve-
ments in the CMU spoken language understanding
system. In Proceedings of the ARPA Human Language
Technology Workshop, pages 213?216.
Jason D. Williams and Steve Young. 2004. Characteriz-
ing task-oriented dialog using a simulated ASR chan-
nel. In ICSLP/Interspeech, pages 185?188.
Teresa Zollo. 1999. A study of human dialogue strate-
gies in the presence of speech recognition errors. In
Proceedings of the AAAI Fall Symposium on Psycho-
logical Models of Communication in Collaborative
Systems, pages 132?139.
258
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 266?271,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Learning to Balance Grounding Rationales for Dialogue Systems 
 
 
Joshua Gordon  Susan L. Epstein 
Department of Computer Science Department of Computer Science 
Rebecca J. Passonneau Hunter College and 
Center for Computational Learning Systems The Graduate Center of the City University 
of New York Columbia University 
New York, NY, USA New York, NY, USA 
(joshua|becky)@cs.columbia.edu susan.epstein@hunter.cuny.edu 
 
 
 
Abstract 
This paper reports on an experiment that 
investigates clarification subdialogues in 
intentionally noisy speech recognition. 
The architecture learns weights for mix-
tures of grounding strategies from exam-
ples provided by a human wizard 
embedded in the system. Results indicate 
that the architecture learns to eliminate 
misunderstandings reliably despite high 
word error rate. 
1 Introduction 
We seek to develop spoken dialogue systems 
(SDSs) that communicate effectively despite un-
certain input. Our thesis is that a task-oriented 
SDS can perform well despite a high degree of 
recognizer noise by relying on context. The SDS 
described here uses FORRSooth, a semi-
synchronous architecture under development for 
task-oriented human-computer dialogue. Our 
immediate goals are to reduce non-
understandings of user utterances (where the 
SDS produces no interpretation) and to eliminate 
misunderstandings (where the SDS misinterprets 
user utterances). The experiment recounted here 
investigates subdialogues consisting of an initial 
user response to a system prompt, and any sub-
sequent turns that might be needed to result in 
full understanding of the original response. Our 
principal finding is that a FORRSooth-based 
SDS learns to build on partial understandings 
and to eliminate misunderstandings despite noi-
sy ASR. 
A FORRSooth-based SDS is intended to inte-
ract effectively ?without the luxury of perfect 
components? (Paek and Horvitz, 2000), such as 
high-performance ASR. FORRSooth relies on 
portfolios of strategies for utterance interpreta-
tion and grounding, and learns to balance them 
from its experience. Its confidence in its inter-
pretations is dynamically calibrated against its 
past experience. At each user utterance, FORR-
Sooth selects grounding actions modulated to 
build upon partial interpretations in subsequent 
exchanges with the user. 
The experiment presented here bootstraps the 
SDS with human expertise. In a Wizard of Oz 
(WOz) study, a person (the wizard) replaces se-
lected SDS components. Knowledge is then ex-
tracted from the wizard?s behavior to improve 
the SDS. FORRSooth uses the Relative Support 
Weight Learning (RSWL) algorithm (Epstein and 
Petrovic, 2006) to learn weights that balance its 
individual strategies. Training examples for 
grounding strategies are based upon examples 
produced by an ablated wizard who was re-
stricted to the same information and actions as 
the system (Levin and Passonneau, 2006). 
Our domain is the Andrew Heiskell Braille 
and Talking Book Library. Heiskell?s patrons or-
der their books by telephone, during conversa-
tion with a librarian. The next section of this 
paper presents related work. Subsequent sections 
describe the weight learning, the SDS architec-
ture, and an experiment that challenges the ro-
bustness of utterance interpretation and 
grounding with intentionally noisy ASR. We 
266
conclude with a discussion of the results.  
2 Related Work  
Despite increasingly accurate ASR methods, di-
alogue systems often contend with noisy ASR, 
which can arise from performance phenomena 
such as filled pauses (er, um), false starts (fir- 
last name), or noisy transmission conditions. 
SDSs typically experience a higher WER when 
deployed. For example, the WER reported for 
Carnegie Mellon University?s Let?s Go Public! 
went from 17% under controlled conditions to 
68% in the field (Raux et al, 2005).  
To limit communication errors, an SDS can 
rely on strategies to detect and recover from in-
correct recognition output (Bohus, 2007). One 
such strategy, to ask the user to repeat a poorly 
understood utterance, can result in hyperarticula-
tion and decreased recognition (Litman, 
Hirschberg and Swerts, 2006). Prior work has 
shown that users prefer explicit confirmation 
over dialogue efficiency (fewer turns) (Litman 
and Pan, 1999). We hypothesize that this results 
from an inherent tradeoff between efficiency and 
user confidence. We assume that evidence of 
partial understanding increases user confidence 
more than evidence of non-understanding does. 
FORRSooth learns to ask more questions that 
build on partial information, and to make fewer 
explicit confirmations and requests to the user to 
repeat herself. 
While many techniques exist in the literature 
for semantic interpretation in task-oriented, in-
formation-seeking dialogue systems, there is no 
single preferred approach. SDSs rarely combine 
a portfolio of NLU (natural language under-
standing) resources. FORRSooth relies on ?mul-
tiple processes for interpreting utterances (e.g., 
structured parsing versus statistical techniques)? 
as in (Lemon, 2003). These range from voice 
search (querying a database directly with ASR 
results) to semantic parsing.  
Dialogue systems should ground their under-
standing of the user?s objectives. To limit com-
munication errors, an SDS can rely on strategies 
to detect and recover from incorrect recognition 
output (Bohus, 2007). In others? work, the 
grounding status of an utterance is typically bi-
nary (i.e., understood or not) (Allen, Ferguson 
and Stent, 2001; Bohus and Rudnicky, 
2005; Paek and Horvitz, 2000) or ternary (i.e., 
understood, misunderstood, not understood) 
(Bohus and Rudnicky, 2009). FORRSooth?s 
grounding decisions rely on a mixture of strate-
gies, are based on degrees of evidence (Bohus 
and Rudnicky, 2009; Roque and Traum, 2009), 
and disambiguate among candidate interpreta-
tions. Work in (DeVault and Stone, 2009) on 
disambiguation in task-oriented dialogue differs 
from ours in that it addresses genuine ambigui-
ties rather than noise resulting from inaccurate 
ASR.  
3 FORR and RSWL 
FORRSooth is based on FORR (FOr the Right 
Reasons), an architecture for learning and prob-
lem solving (Epstein, 1994). FORR uses se-
quences of decisions from multiple rationales to 
solve problems. Implementations have proved 
robust in game learning, simulated pathfinding, 
and constraint solving. FORR relies on an adap-
tive, hierarchical mixture of resource-bounded 
procedures called Advisors. Each Advisor em-
bodies a decision rationale. Advisors? opinions 
(comments) are combined to arrive at a decision. 
Each comment pairs an action with a strength 
that indicates some degree of support for or op-
position to that action. An Advisor can make 
multiple comments at once, and can base its 
comments upon descriptives. A descriptive is a 
shared data structure, computed on demand, and 
refreshed only when required. For each decision, 
FORR consults three tiers of Advisors, one tier 
at a time, until some tier reaches a decision.  
FORR learns weights for its tier-3 Advisors 
with RSWL. Relative support is a measure of the 
normalized difference between the comment 
strength (confidence) with which an Advisor 
supports an action compared to other available 
choices. RSWL learns Advisors? weights from 
their comments on training examples. The de-
gree of reinforcement (positive or negative) to 
an Advisor's weight is proportional to its 
strength and relative support for a decision. 
4 FORRSooth 
FORRSooth is a parallelized version of FORR. 
It models task-oriented dialogue with six FORR-
based services that operate concurrently: INTE-
267
RACTION, INTERPRETATION, SATISFACTION, 
GROUNDING, GENERATION, and DISCOURSE. 
These services interpret user utterances with re-
spect to system expectations, manage the con-
versational floor, and consider competing 
interpretations, partial understandings, and alter-
native courses of action. All services have 
access to the same data, represented by descrip-
tives. In this section, we present background on 
SATISFACTION and INTERPRETATION, and pro-
vide additional detail on GROUNDING.  
The role of SATISFACTION is to represent di-
alogue goals, and to progress towards those 
goals through spoken interaction. Dialogue goals 
are represented as agreements. An agreement is 
a subdialogue about a target concept (such as a 
specific book) whose value must be grounded 
through collaborative dialogue between the sys-
tem and the user (Clark and Schaefer, 1989). 
Agreements are organized into an agreement 
graph that represents dependencies among them. 
Task-based agreements are domain specific, 
while grounding agreements are domain inde-
pendent (cf. (Bohus, 2007)). An interpretation 
hypothesis represents the system?s belief that the 
value of a specific target (e.g., a full name or a 
first name) occurred in the user?s speech.  
The role of INTERPRETATION is to formulate 
hypotheses representing the meaning of what the 
user has said. INTERPRETATION relies on tier-3 
Advisors (essentially, mixtures of heuristics). 
Each Advisor constructs comments on speech 
recognition hypotheses. A comment is a seman-
tic concept (hypothesis) with an associated 
strength. More than one Advisor can vote for the 
same hypothesis. Confidence in any one hypo-
thesis is a function of votes, learned weights for 
Advisors, and comment strengths.  
In previous work, we showed that INTERPRE-
TATION Advisors can produce relatively reliable 
hypotheses given noisy ASR, with graceful de-
gradation  as recognition performance decreases 
(Gordon, Passonneau and Epstein, 2011). For 
example, at WER between 0.2 and 0.4, the con-
cept accuracy of the top hypothesis was 80%. 
That work left open how to decide whether to 
use the top INTERPRETATION hypothesis. Here 
FORRSooth learns how to assess its INTERPRE-
TATION confidence, and what grounding actions 
to take given different levels of confidence. 
Over the life of a FORRSooth SDS, INTER-
PRETATION produces hypotheses for the values 
of target concepts. FORRSooth records the mean 
and variance of the comment strengths for each 
INTERPRETATION hypothesis, and uses them to 
calculate INTERPRETATION?s merit. Merit 
represents FORRSooth?s INTERPRETATION con-
fidence as a dynamic, normalized estimate of the 
percentile in which the value falls. Merit compu-
tations improve initially with use of the SDS, 
and can then shift with the user population and 
the data. FORRSooth?s approach differs from 
supervised confidence annotation methods that 
learn a fixed confidence threshold from a corpus 
of human-machine dialogues (Bohus, 2007). 
The role of GROUNDING is to monitor the sys-
tem?s confidence in its interpretation of each us-
er utterance, to provide evidence to the user of 
its interpretation, and to elicit corroboration, fur-
ther information, or tacit agreement. To ground a 
target concept, FORRSooth considers one or 
more hypotheses for the value the user intended, 
and chooses a grounding action commensurate 
with its understanding and confidence.  
GROUNDING updates the agreement graph by 
adding grounding agreements to elicit confirma-
tions or rejections of target concepts, or to dis-
ambiguate among target concepts. A grounding 
agreement?s indicator target represents the ex-
pectation of a user response. Once a sufficiently 
confident INTERPRETATION hypothesis is bound 
to an indicator target, the grounding agreement 
executes side effects that strengthen or weaken 
the hypothesis being grounded. Recursive 
grounding (where the system grounds the user?s 
response to the system?s previous grounding ac-
tion) can result if the system?s expectation has 
not been met by the next system turn.  
GROUNDING makes two kinds of decisions, 
each with its own set of tier-3 Advisors. The 
first, commit bindings, indicates that the system 
is confident in the value of a target concept. In 
this experiment, decisions to commit to a value 
are irrevocable. The other kind of decision se-
lects the next grounding utterance for any target 
concepts that have not yet been bound. The deci-
sion to ground a target concept is made by tier-3 
Advisors that consider the distribution of hypo-
thesis merit, as well as the success or failure of 
the grounding actions taken thus far. 
268
5 FX2 
FX2 is a FORRSooth SDS constructed for the 
current experiment. The ten FX2 INTERPRETA-
TION Advisors are described in (Gordon, 
Passonneau and Epstein, 2011). Here we de-
scribe its GROUNDING actions and Advisors.  
FX2 can choose among six grounding actions. 
Given high confidence in a single interpretation, 
it commits to the binding of a target value with-
out confirmation. At slightly lower confidence 
levels, it chooses to implicitly confirm a target 
binding, with or without a hedge (e.g., the tag 
question ?right??). At even lower confidence, 
the grounding action is to explicitly confirm. 
Given competing interpretations with similarly 
high confidence, the grounding action is to dis-
ambiguate between the candidates. Finally, FX2 
can request the user to repeat herself. 
We give two examples of the twenty-three 
FX2 grounding Advisors. Given two interpreta-
tion hypotheses with similar confidence scores, a 
disambiguation Advisor votes to prompt the user 
to disambiguate between them. The strength for 
this grounding action is proportional to the ratio 
of the two hypotheses? scores. To avoid repeated 
execution of the same grounding action, one 
grounding Advisor votes against actions to re-
peat a prompt for the same target, especially if 
ASR confidence is low. In FX2, RSWL facili-
tates the use of multiple Advisors for INTERPRE-
TATION and GROUNDING by learning weights for 
them that reflect their relative reliability. We de-
scribe next how we collect training examples 
through an ablated wizard experiment. 
6 Experimental Design 
This experiment tests FX2?s ability to learn IN-
TERPRETATION and GROUNDING weights. In 
each dialogue, FX2 introduces itself, prompts 
the subject for her name or a book title, and then 
continues the dialogue until FX2 commits to a 
binding for the concept, or gives up. 
Four undergraduate native English speakers 
(two female, two male) participated. Speech in-
put and output was through a microphone head-
set. The PocketSphinx speech recognizer 
produced ASR output (Huggins-Daines et al, 
2006) with Wall-Street Journal dictation acous-
tic models adapted with ten hours of spontane-
ous speech. We built distinct trigram statistical 
language models for each type of agreement us-
ing names and titles from the Heiskell database. 
We collected three data sets, referenced here 
as baseline, wizard, and learning. Each had two 
agreement graphs: UserName seeks a grounded 
value for the patron's full name, and BookTitle 
seeks a grounded value for a book title. 120 di-
alogues were collected for each dataset.  
FX2 includes an optional wizard component. 
When active, the wizard component displays a 
GUI showing the current interpretation hypo-
theses for target concepts, along with their re-
spective merit. A screen shot for the wizard GUI 
appears in Figure 1. 
A wizard dialogue activates the wizard com-
ponent and uses INTERPRETATION as usual, but 
embeds a person (the wizard) in GROUNDING. 
The wizard?s purpose in this experiment is to 
provide training data for GROUNDING. After 
each user turn, the wizard makes two decisions 
based on data from the GUI: whether to consider 
any target as grounded, and which in a set of 
possible grounding actions to use next. The GUI 
displays what FX2 would choose for each deci-
sion; the wizard can either accept or override it. 
Ordinarily, a FORR-based system begins with 
uniform Advisor weights and learns more ap-
propriate values during its experience. Because 
correct interpretation and grounding are difficult 
tasks, however, we chose here to prime these 
weights and hypothesis merits using training ex-
amples collected during development. Develop-
ment data for INTERPRETATION included 200 
patron names, 400 book titles, and 50 indicator 
Figure 1. The wizard GUI displays hypotheses for a title from a user utterance. 
269
concepts. ASR output for each item, along with 
its correct value, became a training example. 
Development data for GROUNDING came from 
20 preliminary wizard dialogues. The develop-
ment data also served to prime hypothesis merit. 
Each subject had 30 dialogues with the sys-
tem for the baseline dataset. For the wizard data 
set, FX2 used the same primed weights and me-
rits as the baseline. The wizard?s grounding ac-
tions and the target graphs on which they were 
based were saved as training examples. Weights 
for GROUNDING Advisors were learned from the 
development data training examples and the 
training examples saved from the wizard data set 
together before collecting the learned data set.  
7 Results and Discussion 
We assess system performance as follows. A 
true positive (tp) here is a dialogue that made no 
grounding errors and successfully grounded the 
root task agreement; a false positive (fp) made at 
least one grounding error (where the system en-
tirely misunderstood the user). A false negative 
(fn) occurs when the system gives up on the 
task. Precision is tp/(tp+fp), recall is tp/(tp+fn), 
and F is their mean. We measure WER using 
Levenshtein edit distance (Levenshtein, 1966). 
Because the audio data is not yet transcribed, we 
estimated average WER from the speaker's first 
known utterance (n=360). Overall estimated 
WER was 66% (54% male, 78% female).  
An ideal system engages in dialogues that 
have high precision, high recall, and economical 
dialogue length (as measured by number of sys-
tem turns). Table 1 reports that data. There is a 
significant increase in precision across the three 
data sets, a small corresponding decrease in re-
call, and an overall gain in F measure. The pre-
cision demonstrated by the system during 
dialogues in the learned data set is as good or 
better than that reported for our best embedded 
human wizards in full dialogue experiments 
(Ligorio, Epstein and Passonneau, 2010).  
Table 2 shows the distribution of the system's 
grounding actions for the three data sets. The 
grounding actions in the learned data set are 
similar to the wizard?s; clearly, RSWL learned 
well. Figure 2 illustrates an effective learned 
grounding behavior in which GROUNDING votes 
to abandon the current grounding strategy on 
turn 5 in favor of an approach designed to elicit 
an utterance from the user more likely to be 
processed correctly by the speech recognizer.  
Our experiment suggests that misunderstand-
ings due to ASR errors can be significantly re-
duced by a close integration between spoken 
language understanding resources and grounding 
behavior, together with the use of fine-grained 
confidence measures. Despite intentionally noisy 
ASR with an estimated average WER of 66%, 
FX2 did not experience a single misunderstand-
ing during 120 clarification dialogues after it had 
trained with RSWL on examples provided by an 
embedded wizard. After training, the system's 
actions closely resembled the wizard's, indicat-
ing that the system?s grounding strategies were 
sufficiently expressive to approximate the wi-
zard's actions. FX2 accommodates a variety of 
independent spoken language understanding re-
sources. It relies on RSWL and on merit, a self-
normalizing estimate of the confidence percen-
tile in which an interpretation hypothesis falls.  
Turn Utterance ASR 
1 What title would you like?  
2 Family and Friends family .friends. 
3 
I have two guesses. The first is 
Family and Friends. The 
second is Family Happiness. 
Is it either of these? 
 
4 The first one .nest. .first. 
5 Let?s try something else. Is the full title Family and Friends?  
6 Yes yes 
Condition Precision Recall F Length 
Baseline 0.65 0.78 0.72 4.36 
Wizard 0.89 0.76 0.83 4.05 
Learned 1.00 0.71 0.86 3.86 
Condition Conf Disambig Repeat Other 
Baseline 0.23 0.19 0.50 0.08 
Wizard 0.09 0.50 0.35 0.06 
Learned 0.15 0.52 0.32 0.01 
Table 1. Performance across three data sets.  Table 2. Distribution of grounding actions. 
 
Figure 2. Example of learned GROUNDING behavior. 
The rightmost column is the top ASR hypothesis. 
Periods delimit unconfident words in the ASR. 
270
References 
James Allen, George Ferguson and Amanda Stent. 
2001. An architecture for more realistic 
conversational systems. Proc. 6th Int'l Conference 
on Intelligent User Interfaces. ACM: 1-8. 
Dan Bohus. 2007. Error awareness and recovery in 
conversational spoken language interfaces. Ph.D. 
thesis, Carnegie Mellon University, Pittsburgh,PA. 
Dan Bohus and Alexander I. Rudnicky. 2005. Error 
handling in the RavenClaw dialog management 
framework. Proc. Human Language Technology 
and Empirical Methods in Natural Language 
Processing, ACL: 225-232. 
Dan Bohus and Alexander I. Rudnicky. 2009. The 
RavenClaw dialog management framework: 
Architecture and systems. Comput. Speech Lang. 
23(3): 332-361. 
Herbert H. Clark and Edward F. Schaefer. 1989. 
Contributing to discourse. Cognitive Science 
13(2): 259 - 294. 
David Devault and Matthew Stone. 2009. Learning to 
interpret utterances using dialogue history. Proc. 
12th Conference of the European Chapter of the 
Association for Computational Linguistics. ACL: 
184-192. 
Susan L. Epstein. 1994. For the Right Reasons: The 
FORR Architecture for Learning in a Skill 
Domain. Cognitive Science 18(3): 479-511. 
Susan L. Epstein and Smiljana Petrovic. 2006. 
Relative Support Weight Learning for Constraint 
Solving. AAAI Workshop on Learning for Search: 
115-122. 
Joshua B. Gordon, Rebecca J. Passonneau and Susan 
L. Epstein. 2011. Helping Agents Help Their 
Users Despite Imperfect Speech Recognition. 
AAAI Symposium Help Me Help You: Bridging the 
Gaps in Human-Agent Collaboration. 
David Huggins-Daines, Mohit Kumar, Arthur Chan, 
Alan W. Black, Mosur Ravishankar and Alex I. 
Rudnicky. 2006. Pocketsphinx: A Free, Real-Time 
Continuous Speech Recognition System for Hand-
Held Devices. In Proc. IEEE ICASSP, 2006. 185-
188. 
Oliver Lemon. 2003. Managing dialogue interaction: 
A multi-layered approach. In Proc. 4th SIGDial 
Workshop on Discourse and Dialogue. 
Vladimir Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. So-
viet Physics Doklady. 10: 707-710. 
Esther Levin and Rebecca Passonneau. 2006. A WOz 
Variant with Contrastive Conditions. In Proc. of 
Interspeech 2006 Satelite Workshop: Dialogue on 
Dialogues. 
Tiziana Ligorio, Susan L. Epstein and Rebecca J. 
Passonneau. 2010. Wizards' dialogue strategies to 
handle noisy speech recognition. IEEE workshop 
on Spoken Language Technology (IEEE-SLT 
2010). Berkeley, CA. 
Diane Litman, Julia Hirschberg and Marc Swerts. 
2006. Characterizing and predicting corrections in 
spoken dialogue systems. Comput. Linguist. 32(3): 
417-438. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. 
Proc. 7th Int'l Conference on User Modeling. 
Springer-Verlag New York, Inc.: 55-64. 
Tim Paek and Eric Horvitz. 2000. Conversation as 
action under uncertainty. Proc. 16th Conference 
on Uncertainty in Artificial Intelligence, Morgan 
Kaufmann Publishers Inc.: 455-464. 
Rebecca J. Passonneau, Susan L. Epstein, Tiziana 
Ligorio, Joshua B. Gordon and Pravin Bhutada. 
2010. Learning about voice search for spoken 
dialogue systems. Human Language 
Technologies: NAACL 2010. ACL: 840-848. 
Antoine Raux, Brian Langner, Allan W. Black and 
Maxine Eskenazi. 2005. Let's Go Public! Taking a 
spoken dialog system to the real world. 
Interspeech 2005 (Eurospeech). Lisbon, Portugal. 
Antonio Roque and David Traum. 2009. Improving a 
virtual human using a model of degrees of 
grounding. Proc. IJCAI-2009. Morgan Kaufmann 
Publishers Inc.: 1537-1542.  
271
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 325?331,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
PARADISE-style Evaluation of a Human-Human Library Corpus 
Rebecca J. Passonneau 
Center for Computational 
Learning Systems 
Columbia University 
becky@cs.columbia.edu 
Irene Alvarado 
School of Engineering and  
Applied Science 
Columbia University 
ia2178@columbia.edu
Phil Crone 
Simon Jerome 
Columbia College 
Columbia University 
ptc2107@columbia.edu
sgj2111@columbia.edu
 
 
Abstract 
We apply a PARADISE-style evaluation to a 
human-human dialogue corpus that was  col-
lected to support the design of a spoken dialo-
gue system for library transactions.  The book 
request dialogue task we investigate is infor-
mational in nature: a book request is consi-
dered successful if the librarian is able to 
identify a specific book for the patron. 
PARADISE assumes that user satisfaction can 
be modeled as a regression over task success 
and dialogue costs.  The PARADISE model 
we derive includes features that characterize 
two types of qualitative features. The first has 
to do with the specificity of the communica-
tive goals, given a request for an item.  The 
second has to do with the number and location 
of overlapping turns, which can sometimes 
signal rapport between the speakers.  
1 Introduction 
The PARADISE method for evaluating task-based 
spoken dialogue systems (SDSs) assumes that user 
satisfaction can be modeled as a multivariate linear 
regression on measures of task success and dialo-
gue costs (Walker, et al 1998).  Dialogue costs 
address efficiency, such as length of time on task, 
and effort, such as number of times the SDS fails 
to understand an utterance and re-prompts the user.   
It has been used to compare subjects performing 
the same or similar tasks across distinct SDSs 
(Sanders, et al 2002). To our knowledge, it has not 
been applied to human-human dialogue.  
For human-human task-based dialogues, we 
hypothesized that user satisfaction would not be 
predicted well by measures of success and dialo-
gue costs alone. We expected that qualitative cha-
racteristics of human-human dialogue, such as the 
manner in which a dialogue goal is pursued, could 
counterbalance high dialogue costs. To test this 
hypothesis, we performed a PARADISE-like eval-
uation of a corpus of human-human library trans-
action dialogues that was originally collected to 
support the design of our SDS (Passonneau, et al 
2010).  The communicative task we examine is to 
identify a specific set of books of interest from the 
library?s holdings. This can be straightforward if 
the patron requests a book by catalogue number.  It 
can be complex if the patron does not have com-
plete bibliographic information, or if the request is 
non-specific. A book request is successful when 
the librarian identifies a specific book that ad-
dresses the patron?s request. 
Task success was predictive on a training set, 
but not on a held-out test set. Dialogue costs were 
less reliably predictive.  Two additional factors we 
found to be moderate predictors pertained to the 
number of book requests that were non-specific in 
nature, and the amount and location of overlapping 
turns. We refer to these as qualitative features. A 
non-specific book request can lead to a collabora-
tive identification of a specific book, and the costs 
incurred can be worth the effort. We speculate that 
overlapping turns during non-task-oriented subdia-
logue reflects positive rapport between the speak-
ers, while the role of overlapping turns during task-
oriented subdialogue is contingent on other charac-
teristics of the task, such as whether the goal is 
specific or non-specific. 
The three following sections discuss related 
work, our corpus, and our annotation procedures 
and reliability.  We then present how we measure 
325
user satisfaction, informational task success on 
book requests, and various dialogue costs.  This is 
followed by results of the application of 
PARADISE to the human-human corpus.  
2 Related Work 
It is commonly assumed that human-computer in-
teraction should closely resemble human-human 
interaction. For example, the originators of social 
presence theory  propose that media that more 
closely resemble face-to-face communication pro-
vide a higher degree of social presence, or aware-
ness of the communicative partner (Short, et al 
1976), which in turn leads  to communicative suc-
cess. A similar idea is seen in the origins of media 
richness theory (Daft and Lengel 1984), which de-
fines media with more ?richness? as having more 
communication cues, and thus enhancing task suc-
cess. A key component of this assumption is that, 
if computers are created with human-like qualities 
then people will view computers similarly to hu-
mans.  We hypothesize that human-machine dialo-
gue need not resemble human-human dialogue in 
all respects, thus we earlier proposed a method to 
investigate human-machine dialogue despite the 
large disparity in the spoken language processing 
abilities of humans versus machines (Levin and 
Passonneau 2006), and applied it work described in 
this proceedings (Gordon, et al 2011). Here, we 
apply PARADISE to human-human dialogue to 
facilitate comparison. 
Turn-taking in conversation has received a 
significant amount of attention. Early work ex-
amined the types of turn-taking attempts and the 
reasons why such attempts either succeed or fail 
(Beattie 1982). Recent research has focused on the 
acoustic, lexical, and discourse-relevant cues that 
indicate a transition between speakers (Be?u? 
2009, Gravano and Hirschberg 2009). More recent-
ly, turn-taking has been examined in the context of 
multi-tasking dialogues (Yang, et al 2011). The 
Loqui human-human dialogues often involve mul-
tiple tasks. We do not annotate who has the floor, 
but we do transcribe overlapping speech, where 
there may be competition for the turn  (see below). 
3 Loqui Human-Human Corpus 
Our baseline SDS, CheckItOut, is modeled on 
library transactions for the Andrew Heiskell Braille 
and Talking Book Library of New York City, and 
is part of the Library of Congress.   Patrons request 
books from librarians by telephone, and receive 
book orders (primarily in recorded format) by mail. 
Early in the project, we recorded 175 patron-
librarian calls at the Heiskell Library, 82 of which 
we identified to be primarily about book informa-
tion and book orders. These were transcribed with 
an XML transcription tool, and utterances were 
aligned with the speech signal. The total number of 
words is approximately 24,670, or about 300 
words per dialogue. Our transcription conventions 
are documented on our website.1 
To facilitate analysis of the interactive structure 
of many types of interaction, such as spontaneous 
spoken dialogue, email, and task-oriented dialogue, 
we previously developed Dialogue Function Unit 
(DFU) annotation (Hu, et al 2009).  The primary 
motivation was to capture information about adja-
cency pairs, sequences of communicative acts in 
which an initial utterance calls forth a responding 
one (Sacks, et al 1974). DFUs encode links be-
tween the elements of an adjacency pair, and a re-
stricted set of dialogue acts designed to generalize 
across genres of interaction.  Trained annotators 
applied DFU annotations to all 82 dialogues.   
To measure task success and dialogue costs, we 
developed an additional annotation process that 
builds on DFU annotation, as described next.  
4 TSC Annotation 
In our human-human corpus, each patron has a 
different set of goals. For most of the dialogues, at 
least some of the patron?s goals are to request 
books from the librarian.  Other goals include re-
questing an update to the patron?s profile informa-
tion, requesting new equipment for listening to 
recorded books, and so on. The three-step method 
developed for annotating task success, dialogue 
costs and qualitative features (TSC Annotation) 
consists of an annotation step to determine what 
tasks are being executed, and two tabulation steps. 
The 82 dialogues that had already been annotated 
for DFUs were then annotated for task success and 
dialogue costs.2 Three annotators were trained in 
the annotation over the course of several one-hour 
sessions, each of which was devoted to a different 
                                                          
1See resources link at http://www1.ccls.columbia.edu/~Loqui/. 
2 The guidelines are at http://www1.ccls.columbia.edu/ 
~Loqui/resources.html.   
326
sample dialogue. Pairs of annotators worked on 
each dialogue, with one annotator reviewing the 
other?s work. Disagreements were adjudicated, and 
interannotator agreement was measured on three 
dialogues. 
4.1 Annotation 
The annotation procedure starts by dividing a tran-
scription of a dialogue into a covering sequence of 
communicative tasks (Dialogue Task Units, or 
DTUs). Each DTU encompasses a complete idea 
with a single goal. It ends when both speakers have 
collaboratively closed the topic, per the notion of 
collaborative contributions to discourse found in 
(Clark and Schaefer 1989). Each DTU is labeled 
with its type.  The two types of DTUs of most re-
levance here are book requests (BRs; where a pa-
tron requests a book), and librarian proposals (LPs; 
where the librarian proposes a book for the patron). 
Each BR or LP is numbered.  Other DTU types 
include Inform (e.g., patron requests the librarian 
to provide a synopsis of a book), and Request-
Action (e.g., patron requests the librarian update 
the patron?s profile). After the DTUs have been 
annotated, success and task measures are tabulated 
for the book requests (BR and LP): the start and 
end lines, the specificity of the request (a request 
for any book by a given author is non-specific), 
and whether the task was successful. 
Figure 1 shows part of a book request DTU.  
The DTU in Figure 1 is unsuccessful; the librarian 
is unable to identify the book the patron seeks. 
Several DTUs might pertain to the same goal, pur-
sued in different ways.  For example, the DTU il-
lustrated here is the second of three in which the 
patron tries to request a book called The Dog 
Whisperer.  The dialogue contains 7 DTUs devoted 
to this request, which is ultimately successful.  
16.1.0? L? wh??wha??do?you?have?the?author???
? ? [Request?Info:?author?of?book]?
17.1.0? P? Cesar?Millan??
? ? [Inform:?author?is?Cesar?Millan]?
18.1.0? L? M?I?L?A?N??
? ? [Request?Info:?is?librarian's?spelling?correct]?
19.1.0? P? yes?
20.1.0? L? <non?speaking?librarian?activity>?
21.1.1? P? can?you?hold?on?just?{one?second}?
? ? [Request?Action:?can?librarian?hold]?
21.1.2? L? {sure?sure}?
? ? [Confirm]?
22.1.0? P? I?m?back?
23.1.1? L? I?m?sorry?I?m?not?seeing?anything?{by?him}?
? ? [Inform:?Nothing?by?this?author]?
23.1.2? P? {really}?
? ? [Request?Info:?yes/no]??
24.1.0? L? no?
? ? [Disconfirm]?
? ? BOOK?REQUEST?1.1?
Figure 1. Book request DTU 
Figure 1 also illustrates how we transcribe 
overlapping utterances. Each line in Figure 1 cor-
responds to an utterance, or in the case of overlap-
ping speech, to a time segment consisting of an 
utterance with some overlap. Patron utterance 
21.1.1 is transcribed as ending with overlapping 
speech (in curly braces) where the librarian is also 
speaking within the same time segment (21.1.2). 
This is followed by the patron?s utterance 22.1.0. 
The next time segment (23) also has an overlap, 
followed by the librarian?s turn 24.1.0. As a result, 
we can investigate the proportion of utterances in a 
dialogue or subdialogue with overlapping speech, 
and the types of segments where overlaps occur. 
4.3 Interannotator Agreement 
To assess interannotator agreement among the 
three annotators, we randomly selected dialogues 
from a set that had already been annotated until we 
identified three that had been annotated by distinct 
pairs of annotators. Each was then annotated by a 
different third annotator who had not been a mem-
ber of the original pair. Interannotator agreement 
on DTU boundaries and labels was measured using 
Krippendorff?s alpha (Krippendorff 1980). Alpha 
ranges from 0 for no agreement above chance pre-
diction, given the rate at which each annotation 
value is used, to 1 or -1, for perfect agreement or 
disagreement.   
The three dialogues had alpha values of 0.87, 
0.77 and 0.66, thus all well above agreement that 
could have resulted from chance. The dialogue 
with the highest agreement had 1 book request 
consisting of 2 DTUs. The first DTU had a non-
specific request for two books by a given author, 
that was later reformulated in the second DTU as a 
specific request--by author and titles--for the two 
books. The dialogue with the next highest agree-
ment had 12 specific book requests by catalogue 
number, and one DTU per book request. The di-
alogue with the lowest agreement had 5 book re-
quests, with one DTU per book request. Two were 
by catalogue number, one was by author, and one 
was by author and title. 
327
5. Perceived User Satisfaction 
An indirect measure of User Satisfaction for each 
dialogue was provided by two annotators who lis-
tened to the audio while reviewing the transcripts. 
The annotators completed a user satisfaction sur-
vey that was nearly identical to one used in an 
evaluation of CheckItOut, the SDS modeled on the 
library transactions; references to the system were 
replaced with the librarian. It contained ten ques-
tions covering the librarian?s clarity, friendliness, 
helpfulness, and ability to communicate. The anno-
tators rated the perceived response of the caller 
with regard to the survey questions. On a 1 to 5 
scale where 5 was the greatest satisfaction, the 
range was [3.8, 4.7], thus overall, patrons were 
perceived to be quite satisfied.  
6. Task Success 
The dialogue task investigated here is information-
al in nature, rather than a borrowing task. That is, a 
book request is considered successful if the libra-
rian is able to identify the specific book the caller 
is requesting, or if the librarian and patron are able 
to specify a book in the library?s holdings that the 
caller wants to borrow. The actual availability of 
the book is not relevant. Some patrons request a 
specific book, and provide alternative means to 
identify the book, such as catalogue number versus 
title. Some seek unspecified books by a particular 
author, or books in a given genre.  
We calculate task success as the ratio of suc-
cessfully identified books to requested books. The 
total number of books requested ranged from 1 to 
24.  Patron-initiated book requests as well as libra-
rian-initiated proposals are included in the tabula-
tion. In addition, we tabulate the number of 
specific book requests that change in the type of 
information provided (RC, title, author, genre, etc.) 
as well as the number of book requests that change 
in their specificity (non-specific to specific). Final-
ly, we tabulate how many of these changes lead to 
successful identifications of books.  
In general, task success was extremely high. 
More than 90% of book requests were successful; 
for 78% of the dialogues, all book requests were 
successful. This high success rate is to be expected, 
given that most callers are requesting specificc 
books they learn about from a library newsletter, or  
making non-specific requests that the librarian can 
satisfy. 
7. Dialogue Costs and Qualitative Features 
Along with two measures of task success (number 
of successfully identified books: Successful.ID; 
percent of requested books that are successfully 
identified: Percent.Successful), we have 48 meas-
ures of dialogue costs and qualitative features. The 
full list appears in column 1 of the table in Appen-
dix A. Dialogue costs consist of measures such as 
the total number of turns, the total number of turns 
in book requests, the total number of utterances, 
counts of interruptions and misunderstandings by 
either party, and so on. Qualitative features include 
extensive clarifications, the types of book request, 
and overlapping utterances.  
An extensive clarification serves to clarify 
some misunderstanding by the caller, and generally 
these segments take at least ten turns. 
We classify each book request into one of sev-
en types.  These are non-specific by author, non-
specific by genre, specific author, specific title, 
specific author and title, specific set, and specific 
catalogue number.  As shown in the Appendix, we 
also tabulate the total number of specific book re-
quests per dialogue (S.Total) and the total number 
of non-specific requests (NS.Total). 
We tabulate overlapping utterances in a varie-
ty of ways. The average number of overlapping 
utterances per dialogue is 13.9.  A breakdown of 
overlapping utterances into those that occur in 
book requests versus other types of DTU gives a 
mean of 4.36 for book requests compared with 
8.74 otherwise. We speculate that the difference 
results from the potential for overlapping utter-
ances to impede understanding when the utterance 
goals are to request and share information about 
books. In these contexts, overlap may reflect com-
petition for the floor. In contrast, overlapping ut-
terances at points in the dialogue that pertain to the 
social dimension may be more indicative of rap-
port between the patron and the librarian, as a ref-
lection of sharing the floor.  We do not attempt to 
distinguish overlaps with positive versus negative 
effects.  We do, however, tabulate overlapping 
speech in different types of DTUs, such as book 
request DTUs versus other DTUs. 
To illustrate the role of the qualitative fea-
tures, we discus one of the dialogues in our corpus 
that exemplifies a property of these human-human 
dialogues that we believe could inform SDS de-
sign: high user satisfaction can occur despite low 
328
success rate on the communicative tasks.   Dialo-
gue 4 had the lowest task success of all dialogues 
(62.5%), yet perceived user satisfaction was quite 
high (4.7).  This dialogue had a large number of 
book requests and librarian proposals, with a mix 
of requests for specific books by catalogue num-
ber, title, or author and title, along with non-
specific requests for works by given authors.  It 
also had a fairly high proportion of overlapping 
speech.  As we discuss next, both dimensions are 
represented in the quantitative PARADISE models 
for predicting user satisfaction. 
8. PARADISE Results 
PARADISE predicts user satisfaction as a linear 
combination of task success and cost variables. 
Here we apply PARADISE to the Loqui library 
corpus, and add qualitative features to task success 
and dialogue costs. Six of the dialogues had no 
book requests, thus did not exemplify the task, 
namely to identify books for the patron in the li-
brary?s holdings.  These six were eliminated.  
We split the data into independent training and 
test sets. From the 76 dialogues with book re-
quests, we randomly selected 50 for deriving a re-
gression model. These dialogues had a total of 211 
book requests (mean=4.22). We reserved 26 dialo-
gues for an independent test of how well the fea-
tures from the user satisfaction model on the 
training set predicted user satisfaction on the test 
set. The test set had 73 book requests (mean=2.81).  
To explore the data, we first did Analysis of 
Variance (ANOVA) tests on the 50 individual fea-
tures as predictors of perceived user satisfaction on 
the training set. Certain features that are typically 
predictive for SDSs were also predictive here.  
Those that were most predictive on their own in-
cluded the proportion of book requests successfully 
identified (Pct.Successful), and several cost meas-
ures such as total length in utterances, and the total 
number of interruptions and misunderstandings. 
However, other features that were predictive here 
that are not typical of human-machine dialogue 
were the number of utterances with overlapping 
speech (Simultaneous.Utterances), and the number 
of book requests that evolved from non-specific  to 
specific (Change.NS.to.S). 
Given the relatively small size of our corpus, 
and the large number of variables, we pruned the 
30 features from the trained model before using 
them to build a regression on the test set. All ana-
lyses were done in the R Statistical Package 
(http://www.r-project.org/). We used the R func-
tion step to apply the Akaike Information Crite-
rion to guide the search through the model space.  
The resulting model relies on 30 of the 50 va-
riables, and has a multiple R-squared of 0.9063 (p= 
0.0001342). Appendix A indicates the 30 features 
selected, and their p-values. For the pruned model, 
we selected half of the 30 features that contributed 
most to the best model found through the step 
function on the training set.  The pruned model had 
a multiple R-squared of 0.5334 (p=0.0075). When 
we used the same features on the test set, the R-
squared was 0.7866  (p=0.0416).  However, the 
significance of individual features differed in train-
ing versus test. Appendix A lists the 15 features 
and their p-values on the training and test sets.  
On the training data, the most significant fea-
tures were Pct.Successful, the total number of di-
alogue segments pertaining to book requests 
(including librarian proposals; BR.request.segs), 
and the total number of book requests (Total.BR). 
The number of non specific book requests that 
evolved into specific requests (Change.NS.to.S) 
and the number of utterances per turn (Utter-
ances.Turns) were marginally significant. 
On the test data, the most significant variables 
were the ratio of overlapping utterances in seg-
ments that were not about book requests to book 
request segments (noBRLP.Overlap.per.TotalRe-
questSegments), the total number of non-specific 
book requests (NS.Total), and the number of over-
lapping utterances (Overlap.Utterances). 
9. Conclusion 
The human-human corpus examined here is an ap-
propriate corpus to compare with human-machine 
dialogue, in that our SDS was modeled on the book 
requests in the human-human corpus.   The R2 val-
ues indicate that the regression models based on 
the 15 features fit the data well, yet the coefficients 
and probabilities are very different. In part, this is 
due to the large number of variables we investi-
gated, relative to the small size of the corpus.  
Nevertheless, the results presented here point to a 
number of dimensions of human-human dialogue 
that contribute to user satisfaction beyond those 
that are typically considered when evaluating hu-
man-machine dialogue.   
329
References 
Beattie, G. W. 1982. Turn-taking and interruption in 
political interviews: Margaret Thatcher and Jim 
Callaghan compared and contrasted. Semiotica, 39 (1-
2): 93-114. 
Be?u?, ?. 2009. Are we 'in sync': Turn-taking in 
collaborative dialogues. In 10th Interspeech, pp. 2167-
2170. 
Clark, H. H. and E. F. Schaefer 1989. Contributing to 
discourse. Cognitive Science, 13 259-294. 
Daft, R. L. and R. H. Lengel 1984. Information 
richness: A new approach to manager behavior and 
organization design. Research in Organizational 
Behavior, 6 191-233. 
Gordon, J., et al 2011. Learning to balance grounding 
rationales for dialogue systems. In 12th Annual SIGdial 
Meeting on Discourse and Dialogue (SIGdial 12).  
Gravano, A. and J. Hirschberg. 2009. Turn-yielding 
cues in task-oriented dialogue. In 10th Annual Meeting 
of SIGDIAL, pp. 253-261. 
Hu, J., et al 2009. Contrasting the interaction structure 
of an email and a telephone corpus: A machine learning 
approach to annotation of dialogue function units. In 
10th SIGDIAL on Dialogue and Discourse, pp. 357-
366. 
Krippendorff, K. 1980. Content Analysis: An 
Introduction to Its Methodology. Beverly Hills, CA: 
Sage Publications. 
Levin, E. and R. J. Passonneau. 2006. A WOz Variant 
with Contrastive Conditions. In Interspeech Satelite 
Workshop, Dialogue on Dialogues: Multidisciplinary 
Evaluation of Speech-based Interactive Systems.  
Passonneau, R. J., et al 2010. Learning About Voice 
Search for Spoken Dialogue Systems. In 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics (NAACL 
HLT 2010), pp. 840-848. 
Sacks, H., et al 1974. A simplest systematics for the 
organization of turn-taking for conversation. Language, 
50 (4): 696-735. 
Sanders, G. A., et al 2002. Effects of word error rate in 
the DARPA Communicator data during 2000 and 2001. 
International Journal of speech Technology, 7 293-309. 
Short, J., et al 1976. The social psychology of 
telecommunications. Chichester: John Wiley. 
Walker, M. A., et al 1998. Evaluating Spoken Dialogue 
Agents with PARADISE: Two Case Studies. Computer 
Speech and Language, 12 317-348. 
Yang, F., et al 2011. An Investigation of interruptions 
and resumptions in multi-tasking dialogues. 
Computational Linguistics, 37 (1): 75-104. 
 
  
 
 
 
 
 
 
 
 
 
  
330
Appendix A: Features 
 Variable Training 
Coeff. 
Training 
p-value 
Pruned 
Coeff 
Pruned 
p-value 
Test 
Coeff. 
Test 
p-value 
1 Successful.ID       
2 Pct.Successful 0.504001 0.005118 0.356516 0.01219 -0.04154 0.86744 
3 Change.NS.to.S 1.440471 0.023525 0.287376 0.05761 0.10284 0.22876 
4 Successful.NS.to.S -1.450301 0.048656     
5 Change.S.to.S       
6 Successful.S.to.S       
7 BR.request.segs -0.201228 0.119857 -0.147057 0.00837 0.02566 0.79277 
8 LP.request.segs 0.146464 0.073138     
9 Total.Request.Segments       
10 Total.BR 0.448858 0.001813 0.147945 0.01220 -0.09960 0.35796 
11 Segments.per.BR 0.296577 0.047333 0.123411 0.17907 -0.08707 0.59903 
12 NS.Author -0.216559 0.090830     
13 NS.Genre -0.138867 0.249339     
14 S.Title          
15 S.AuthorTitle       
16 S.Set -0.953284 6.61e-05     
17 S.RC -0.158897 0.104752     
18 S.Author       
19 S.Total       
20 NS.Total   0.013265 0.75986 -0.27280 0.00716 
21 Turns.in.BR       
22 Utterances    -0.005613 0.013967     
23 Interruptions 0.187876 0.002704 -0.050500 0.29683 -0.29078 0.05378 
24 Misunderstandings       
25 Simultaneous.Utterances -0.151491 0.001967 -0.008705 0.21024 0.02329 0.04179 
26 Extensive.Clarifications -0.181057 1.76e-05 -0.022723 0.25767 -0.08685 0.11608 
27 S.U.Conventional 0.142152 0.006168     
28 S.U.Inform 0.141891 0.001619     
29 S.U.Sidebar  0.107238 0.047303     
30 S.U.BR.RC 0.142538 0.006467     
31 S.U.BR.Title 0.245880 0.000415     
32 S.U.BR.Title.and.Author 0.136412 0.002581     
33 S.U.BR.Genre       
34 S.U.LP 0.176515 0.015598     
35 S.U.R.A. 0.171413 0.001459     
36 S.U.IR.IRA 0.166315 0.001994     
37 Utterances.Turns -0.392267 0.020190 -0.256307 0.08077 0.01731 0.95674 
38 Total.Turns.BR       
39 Turns.in.BR.BR -0.015623 0.093573     
40 BR.Utterances -8.875951 0.000603 -1.104338 0.55174 2.59438 0.33439 
41 NS.Total.per.BR 0.183761 0.177739 -0.102524 0.33547 0.31111 0.10004 
42 S.U.BRLP       
43 S.U.BRLP.per.BR          
44 S.U.BRLP.per.TotalRequestSegs       
45 S.U.nonBRLP       
46 S.U.nonBRLP.per.BR       
47 S.U.nonBRLP.per.TotalRequestSegs 0.024492 0.117363 0.007839 0.33727 -0.06000 0.00848 
48 S.nonRC       
49 S.nonRC.per.BR   -0.370227 0.064299 -0.062149 0.46085 -0.08072 0.47704 
50 S.nonRC.per.TotalRequestSegs       
 
331
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 257?260,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Semantic Specificity in Spoken Dialogue Requests
Ben Hixon
Hunter College of The
City University of New York
New York, NY, USA
shixon@hunter.cuny.edu
Rebecca J. Passonneau
Columbia University
New York, NY, USA
becky@cs.columbia.edu
Susan L. Epstein
Hunter College, and
The Graduate Center of The
City University of New York
New York, NY, USA
susan.epstein@hunter.cuny.edu
Abstract
Ambiguous or open-ended requests to a di-
alogue system result in more complex dia-
logues. We present a semantic-specificity met-
ric to gauge this complexity for dialogue sys-
tems that access a relational database. An ex-
periment where a simulated user makes re-
quests to a dialogue system shows that seman-
tic specificity correlates with dialogue length.
1 Introduction
A dialogue system (DS) and its users have asymmet-
ric knowledge. The DS has access to knowledge the
user is not privy to, and the user has intentions that
the DS attempts to recognize. When the user?s inten-
tions are difficult for her to specify fully, the user and
DS must collaborate to formulate the intention. The
thesis of this work is that a DS can assess the speci-
ficity of its knowledge with respect to the user inten-
tions it is designed to address. Our principal result
is that, for a DS that queries a relational database,
measures of the ambiguity of database attributes can
be used both to assess the scope of the DS?s task
and to guide its dialogue strategy. To demonstrate
our thesis, we have developed a semantic specificity
metric applicable to any DS that queries a relational
database. This metric measures the degree to which
one or more attributes can uniquely specify an item
in the database. Attributes whose values are more
often ambiguous have lower semantic specificity.
CheckItOut is a book request DS that references
a copy of the catalogue at the Heiskell Braille and
Talking Book Library with its 71,166 books (Epstein
et al, In Press). We focus on three book attributes:
AUTHOR, TITLE and CALL NUMBER. Only the lat-
ter is guaranteed to identify a unique book. Of the
64,907 distinct TITLE values, a large majority return
a unique book (N=59,236; 91.3%). Of the 28,045
distinct AUTHOR values, about two thirds return a
unique book (N=17,980; 64.1%).
Query return Distinct Distinct
size TITLE values AUTHOR values
1 59236 17980
2 5234 4377
3 345 1771
...
10 2 168
...
184 ? 1
Total 64907 28045
Table 1: When used as a query, many TITLE values return
unique books, but AUTHOR values are less specific.
To compare the specificity of TITLE and AUTHOR,
we calculated query return size, the number of dis-
tinct books in the Heiskell database returned by each
possible attribute value. Table 1 tallies how many
attribute values have the same query return size. TI-
TLE partitions the books into 10 subsets, where the
two most ambiguous TITLE values, Collected Sto-
ries and Sanctuary, each return 10 distinct books.
AUTHOR produces 89 subsets; its most ambiguous
value, Louis L?Amour, returns 184 distinct books.
Clearly, TITLE has higher specificity than AUTHOR.
After a survey of related work, this paper defines a
semantic specificity metric that is a weighted sum of
257
the number of query return sizes for one or more at-
tributes. We show through simulation that dialogue
length varies with semantic specificity for a DS with
a simple system-initiative dialogue strategy.
2 Related Work
Little work has been reported on measures of the
relationship between dialogue complexity and the
semantic structure of a DS application?s database.
Zadrozny (1995) proposes Q-Complexity, which
roughly corresponds to vocabulary size, and is es-
sentially the number of questions that can be asked
about a database. Pollard and Bierman (2000) de-
scribe a similar measure that considers the number
of bits required to distinguish every object, attribute,
and relationship in the semantic space.
Gorin et al (2000) distinguish between semantic
and linguistic complexity of calls to a spoken DS.
Semantic complexity is measured by inheritance re-
lations between call types, the number of type labels
per call, and how often calls are routed to human
agents. Linguistic complexity is measured by utter-
ance length, vocabulary size and perplexity.
Popescu et al (2003) identify a class of ?seman-
tically tractable? natural language questions that can
be mapped to an SQL query to return the question?s
unique correct answer. Ambiguous questions with
multiple correct answers are not considered seman-
tically tractable. Polifroni and Walker (2008) ad-
dress how to present informative options to users
who are exploring a database, for example, to choose
a restaurant. When a query returns many options,
their system summarizes the return using attribute
value pairs shared by many of the members.
3 Semantic Specificity
The database queried by a DS can be regarded as
the system?s knowledge. Consequently, the seman-
tic structure of the database and the way it is popu-
lated constrain the requests the system can address
and how much information the user must provide.
Intuitively, Table 1 shows that TITLE has a higher
semantic specificity than AUTHOR. Our goal is to
quantify the query ambiguity engendered by the in-
stantiation of any database table.
Often a user does not know in advance which
combination of attribute values uniquely communi-
cates her intent to the system. In addition, the DS
does not know what the user wants until it has of-
fered an item that the user confirms, whether ex-
plicitly or implicitly. The remainder of this section
defines the specificity of individual and multiple at-
tributes with respect to a set of database instances.
3.1 Specificity for Single Attributes
When a user requests information about one or more
entities, the request can map to many more database
instances than intended. Let I be a set of instances
(rows) in a database relation, and let ? be an attribute
of I with values V that occur in I . Denote by q(v, ?)
the query return size for v ? V on ?, the number of
instances of I returned by the query ? = v. When-
ever q(v, ?) = 1, the query returns exactly one in-
stance in I; attributes with more such values have
higher specificity. If q(v, ?) = 1 for every v, then ?
is maximally specific with respect to I .
Let Q? be the set of d? distinct query return sizes
q(v, ?) returned on I . We call Q? the query return
size partition for ?. Q? induces a partition of V
into subsets Vj , j ? Q? such that a query on every
value in a given subset returns the same number of
instances. Table 1 shows two such partitions. We
now define the specificity S(?, I) of attribute ?with
respect to I as a weighted sum of the sizes of the
subsets in the partition induced by ?, normalized by
|I|, the number of instances in I:
S(?, I) = 1|I|
?
j?Q?
w(j) ? |Vj | (1)
The weight function w in (1) addresses the num-
ber of distinct values in each subset of Q?. A larger
query return size indicates a more ambiguous at-
tribute, one less able to distinguish among instances
in I . To produce specificity values in the range [0, 1],
w(j) should decrease as j increases, but not penal-
ize any query that returns a single instance, that is,
w(1) = 1. The faster w decreases, the more it pe-
nalizes an ambiguous attribute. Here we take as w
the inverse of the query return size, w(j) = 1j .For our CheckItOut example, equation (1) scores
TITLE?s specificity as 0.871 and AUTHOR?s speci-
ficity much lower, at 0.300. This matches our intu-
ition. The third attribute with which a user can order
a book, CALL NUMBER, was designed as a primary
key and so has a perfect specificity of 1.000.
258
3.2 Specificity for Multiple Attributes
The specificity of a set ? = {?1, ?2, ..., ?k} of k at-
tributes on a set of instances I measures to what de-
gree a combination (one value for each attribute in
?) specifies a restricted set of instances in I . Let V
be the combinations for ? that occur in I , and let
q(v, ?) be the query return size for v ? V . Then
Q? , the set of d? distinct query return sizes, induces
a partition on V into subsets Vj , j ? Q? where com-
binations in the same subset return the same number
of instances. We take w(j, k) = 1jk to penalize am-biguity more heavily when there are more attributes.
Then the specificity of ? with respect to I is
S(?, I) = 1|I|
?
j?Q?
w(j, k) ? |Vj | (2)
Using this equation, the specificity of ? =
{TITLE, AUTHOR} is 0.880. Interestingly, this is not
much higher than the 0.871 TITLE specificity alone,
which indicates that, in this particular database
instantiation, AUTHOR has little ability to disam-
biguate a TITLE query. This is because many
?books? in the Heiskell catalog appear in two for-
mats, Braille and audio. This duplication creates an
ambiguity that is better resolved by prompting the
user for CALL NUMBER or FORMAT. In some cases,
a value for FORMAT might still result in ambiguity;
for example, different recorded readers produce dif-
ferent audio versions of the same title and author.
In contrast, the large difference between AUTHOR?s
very low specificity (0.300) and that of {TITLE, AU-
THOR} (0.880) suggests that, given an ambiguous
author, it would in general be a good strategy for the
DS to then prompt the user for the title.
Because specificity is a function of a database in-
stantiation, specificity can be used to guide dialogue
strategy. For the books in Heiskell?s catalogue that
cannot be uniquely identified by AUTHOR and TITLE
alone, it can be determined a priori that some book
requests cannot be disambiguated without additional
attribute values.
4 Specificity in Simulated Dialogues
A DS faced with an ambiguous query should enter
a disambiguation subdialogue. It can either offer a
list of matching instances that must be individually
confirmed or rejected by the user, or indicate to the
user that an attribute value is ambiguous and request
additional information. In general, a less specific
user request should increase the dialogue length.
We tested this hypothesis with a simulated user
that interacted with the CheckItOut book-order DS
through a text interface. As in our work where hu-
man subjects called CheckItOut, a new scenario for
each call included ?patron? identification data and a
list of four books with their authors, titles, and call
numbers. In each dialogue, the simulator received
a scenario, the DS confirmed the simulator?s ?iden-
tity,? and then the simulator requested the books.
The simulator uses a rule-based dialogue strat-
egy with some parameters, such as persistence in
re-ordering the same book, that can be set or ran-
domized to represent a set of real users more real-
istically. For this experiment, the simulator was set
to request books only by author, and to be willing to
engage in a disambiguation subdialogue of arbitrary
length. When the system cannot uniquely identify a
requested book, it begins a disambiguation subdia-
logue, an example of which is shown in Figure 1.
To avoid addressing information presentation issues
such as those explored in (Polifroni and Walker,
2008), CheckItOut followed a simple strategy of of-
fering each next candidate book in a query return,
and user studies with CheckItOut restricted query re-
turn size to a maximum of three books. For the sim-
ulations, we expect an inverse relationship between
specificity and dialogue length.
S: NEXT BOOK?
U: Dan Simmons
S: DID YOU SAY DAN SIMMONS?
U: Yes
S: WOULD YOU LIKE ENDYMION BY THIS AUTHOR?
U: No
S: WOULD YOU LIKE THE CROOK FACTORY
BY THIS AUTHOR?
U: Yes
Figure 1: Sample disambiguation subdialogue.
We randomly sampled Heiskell?s full set of
71,166 books to create five smaller instantiations
of 1000 books each. We deliberately sampled
at different frequencies within each subset of the
original partition induced by AUTHOR, so that
S(AUTHOR, T ) for instantiation T ranged from
0.3528 to 1.000. For each instantiation we simulated
25 dialogues. Conditions of relatively lower speci-
259
0.0 0.2 0.4 0.6 0.8 1.0Author specificity35
40
45
50
55
Mea
n di
alog
ue le
ngth
Figure 2: Dialogue length averaged across 25 simulated
dialogues for each run of 5 different attribute specificity
levels, shown with 95% confidence intervals.
ficity result in more dialogues like the one shown in
Figure 1, with multiple turn exchanges where the DS
offers the simulator different books by the requested
author. As specificity approaches 1.0, the first book
offered by the DS is more frequently the requested
book, so no disambiguation is required, and the min-
imum dialogue length of 43 turns is achieved. Fig-
ure 2 compares the mean dialogue length for each
sub-instantiation to its author specificity, and clearly
shows that dialogue length increases as author speci-
ficity decreases. The error bars shrink as specificity
increases because there is less variation in dialogue
length when there are fewer candidate books for
CheckItOut to offer.
5 Conclusion and Future Work
Semantic specificity has two important applications.
Because it predicts how likely a value for a database
attribute (or a combination for a set of attributes) is
to return a single database instance, semantic speci-
ficity can help formulate subdialogues with a prior-
ity order in which the DS should prompt users for
attributes. Because it is a predictor for dialogue
length, semantic specificity could also be used to
evaluate whether a DS dialogue strategy incurs the
expected costs. Of course, many factors other than
semantic specificity affect DS dialogue complexity,
particularly the relation between users? utterances
and the semantics of the database. In the examples
given here, the way users refer to books corresponds
directly to attribute values in the database. Other
domains may require a more complex procedure to
map between the semantics of the database and the
semantics of natural language expressions.
Finally, how well semantic specificity with re-
spect to a database instantiation predicts dialogue
length depends in part on how closely the database
attributes correspond to information that users can
readily provide. Here, AUTHOR and TITLE are con-
venient both for users and for the database seman-
tics. However, the maximally specific CALL NUM-
BER is often unknown to the user. For DSs where
the database attributes differ from those that can be
extracted from user utterances, we intend to explore
enhanced or additional metrics to predict dialogue
length and guide dialogue strategy.
Acknowledgments
We thank Julia Hirschberg for helpful comments,
and Eric Schneider for help with the user simulator.
National Science Foundation awards IIS-0745369,
IIS-0744904 and IIS-084966 funded this project.
References
Susan L. Epstein, Rebecca J. Passonneau, Tiziana Ligo-
rio, and Joshua Gordon. In Press. Data mining to sup-
port human-machine dialogue for autonomous agents.
In Proceedings of Agents and Data Mining Interaction
(ADMI 2011). Springer-Verlag.
A. L. Gorin, J. H. Wright, G. Riccardi, A. Abella, and
T. Alonso. 2000. Semantic information processing of
spoken language. In Proceedings of ATR Workshop on
MultiLingual Speech Communication, pages 13?16.
Joseph Polifroni and Marilyn Walker. 2008. Intensional
summaries as cooperative responses in dialogue: Au-
tomation and evaluation. In Proceedings of ACL-08:
HLT, pages 479?487, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Shannon Pollard and Alan W. Bierman. 2000. A measure
of semantic complexity for natural language systems.
In NAACL-ANLP 2000 Workshop: Syntactic and Se-
mantic Complexity in Natural Language Processing
Systems, pages 42?46, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th inter-
national conference on Intelligent user interfaces, IUI
?03, pages 149?157, New York, NY, USA. ACM.
Wlodek Zadrozny. 1995. Measuring semantic com-
plexity. In Moshe Koppel, Eli Shamir, and Martin
Golumbic, editors, Proceedings of the Fourth Bar Ilan
Symposium on Foundations of Artificial Intelligence
(BISFAI 1995), pages 245?254.
260
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 187?195,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
The Benefits of a Model of Annotation
Rebecca J. Passonneau
Center for Computational Learning Systems
Columbia University
becky@ccls.columbia.edu
Bob Carpenter
Department of Statistics
Columbia University
carp@alias-i.com
Abstract
This paper presents a case study of a
difficult and important categorical anno-
tation task (word sense) to demonstrate
a probabilistic annotation model applied
to crowdsourced data. It is argued that
standard (chance-adjusted) agreement lev-
els are neither necessary nor sufficient
to ensure high quality gold standard la-
bels. Compared to conventional agree-
ment measures, application of an annota-
tion model to instances with crowdsourced
labels yields higher quality labels at lower
cost.
1 Introduction
The quality of annotated data for computational
linguistics is generally assumed to be good enough
if a few annotators can be shown to be consistent
with one another. Metrics such as pairwise agree-
ment and agreement coefficients measure consis-
tency among annotators. These descriptive statis-
tics do not support inferences about corpus quality
or annotator accuracy, and the absolute values one
should aim for are debatable, as in the review by
Artstein and Poesio (2008). We argue that high
chance-adjusted inter-annotator agreement is nei-
ther necessary nor sufficient to ensure high qual-
ity gold-standard labels. Agreement measures re-
veal little about differences among annotators, and
nothing about the certainty of the true label, given
the observed labels from annotators. In contrast, a
probabilistic model of annotation supports statis-
tical inferences about the quality of the observed
and inferred labels.
This paper presents a case study of a particu-
larly thorny annotation task that is of widespread
interest, namely word-sense annotation. The items
that were annotated are occurrences of selected
words in their sentence contexts, and the annota-
tion labels are WordNet senses (Fellbaum, 1998).
The annotations, collected through crowdsourc-
ing, consist of one WordNet sense for each item
from up to twenty-five different annotators, giv-
ing each word instance a large set of labels. Note
that application of an annotation model does not
require this many labels for each item, and crowd-
sourced annotation data does not require a prob-
abilistic model. This case study, however, does
demonstrate a mutual benefit.
A highly certain ground truth label for each an-
notated instance is the ultimate goal of data anno-
tation. Many issues, however, make this compli-
cated for word sense annotation. The number of
different senses defined for a word varies across
lexical resources, and pairs of senses within a sin-
gle sense inventory are not equally distinct (Ide
and Wilks, 2006; Erk and McCarthy, 2009). A
previous annotation effort using WordNet sense la-
bels demonstrates a great deal of variation across
words (Passonneau et al, 2012b). On over 116
words, chance-adjusted agreement ranged from
very high to chance levels. As a result, the ground
truth labels for many words are questionable. On a
random subset of 45 of the same words, the crowd-
sourced data presented here (available as noted be-
low) yields a certainty measure for each ground
truth label indicating high certainty for most in-
stances.
2 Chance-Adjusted Agreement
Current best practice for collecting and curating
annotated data involves iteration over four steps,
or variations of them: 1) design or redesign the
annotation task, 2) write or revise guidelines in-
187
structing annotators how to carry out the task, pos-
sibly with some training, 3) have two or more an-
notators work independently to annotate a sample
of data, and 4) measure the interannotator agree-
ment on the data sample. Once the desired agree-
ment has been obtained, a gold standard dataset
is created where each item is annotated by one
annotator. As noted in the introduction, how
much agreement is sufficient has been much dis-
cussed (Artstein and Poesio, 2008; di Eugenio and
Glass, 2004; di Eugenio, 2000; Bruce and Wiebe,
1998). The quality of the gold standard is not ex-
plicitly measured. Nor is the accuracy of the an-
notators. Since there are many ways to be inaccu-
rate, and only one way to be accurate, it is assumed
that if annotators agree, then the annotation must
be accurate. This is often but not always correct.
If two annotators do not agree well, this method
does not identify whether one annotator is more
accurate than the other. For the individual items
they disagree on, no information is gained about
the true label.
To get a high level sense of the limitations of
agreement metrics, we briefly discuss how they
are computed and what they tell us. For a com-
mon notation, let i ? 1:I represent the set of all
items, j ? 1:J all the annotators, k ? 1:K all the
label classes in a categorical labeling scheme (e.g.,
word senses), and yi,j ? 1:K the observed labels
from annotator j for item i (assuming every anno-
tator labels every item exactly once; we relax this
restriction later).
Agreement: Pairwise agreement Am,n between
two annotators m,n ? 1:J is defined as the pro-
portion of items 1:I for which the annotators sup-
plied the same label,
Am,n = 1I
?I
i=1 I(yi,m = yi,n),
where the indicator function I(s) = 1 if s is true
and 0 otherwise. Am,n is thus the maximum like-
lihood estimate that annotator m and n will agree.
Pairwise agreement can be extended to the en-
tire pool of annotators by averaging over all
(J
2
)
pairs,
A = 1
(J2)
?J
m=1
?J
n=m+1Am,n.
By construction, Am,n ? [0, 1] and A ? [0, 1].
Pairwise agreement does not take into account
the proportion of observed annotation values from
1:K. As a simple expected chance of agreement, it
provides little information about the resulting data
quality.
Chance-Adjusted Agreement: An agreement
coefficient, such as Cohen?s ? (Cohen, 1960) or
Krippendorff?s ? (Krippendorff, 1980), measures
the proportion of observed agreements that are
above the proportion expected by chance. Given
an estimate Am,n of the probability that two an-
notators m,n ? 1:J will agree on a label and
an estimate of the probability Cm,n that they
will agree by chance, the chance-adjusted inter-
annotator agreement coefficient IAm,n ? [?1, 1]
is defined by
IAm,n =
Am,n?Cm,n
1?Cm,n
.
For Cohen?s ? statistic, chance agreement is de-
fined to take into account the prevalence of the
individual labels in 1:K. Specifically, it is de-
fined to be the probability that a pair of labels
drawn at random for two annotators agrees. There
are two common ways to define this draw. The
first assumes each annotator draws uniformly at
random from her set of labels. Letting ?j,k =
1
I
?I
i=1 I(yi,j = k) be the proportion of the label k
in annotator j?s labels, this notion of chance agree-
ment for a pair of annotators m,n is estimated as
the sum over 1:K of the products of their propor-
tions ?:
Cm,n =
?K
k=1 ?m,k ? ?n,k.
Another computation of chance agreement in wide
use assumes each annotator draws uniformly at
random from the pooled set of labels from all an-
notators (Krippendorff, 1980). Letting ?k be the
proportion of label k in the entire set of labels, this
alternative estimate, C ?m,n =
?K
k=1 ?
2
k, does not
depend on the identity of the annotators m and n.
An inter-annotator agreement statistic like ?
suffers from multiple shortcomings. (1) Agree-
ment statistics are intrinsically pairwise, although
one can compare to a voted consensus or aver-
age over multiple pairwise agreements. (2) In
agreement-based analyses, two wrongs make a
right; if two annotators both make the same mis-
take, they agree. If annotators are 80% accurate
on a binary task, chance agreement on the wrong
category occurs at a 4% rate. (3) Chance-adjusted
agreement reduces to simple agreement as chance
agreement approaches zero. When chance agree-
ment is high, even high-accuracy annotators can
188
have low chance-adjusted agreement. For ex-
ample, in a binary task with 95% prevalence of
one category, two 90% accurate annotators have
a chance-adjusted agreement of 0.9?(.95
2+.052)
1?(.952+.052) =
?.053. Thus high chance-adjusted inter-annotator
agreement is not a necessary condition for a high-
quality corpus. (4) Inter-annotator agreement
statistics implicitly assume annotators are unbi-
ased; if they are biased in the same direction, as we
show they are for the sense data considered here,
then agreement is an overestimate of their accu-
racy. In the extreme case, in a binary labeling task,
two adversarial annotators who always provide the
wrong answer have a chance-adjusted agreement
of 100%. (5) Item-level effects such as difficulty
can inflate levels of agreement-in-error. For ex-
ample, hard-to-identify names in a named-entity
corpus have correlated false negatives among an-
notators, leading to higher agreement-in-error than
would otherwise be expected. (6) Inter-annotator
agreement statistics are rarely computed with con-
fidence intervals, which can be quite wide even
under optimistic assumptions of no annotator bias
or item-level effects. In a sample of MASC word
sense data, 100 annotations by 80% accurate an-
notators produce a 95% interval for accuracy of
+/- 6%. Agreement statistics have even wider er-
ror bounds. This introduces enough uncertainty to
span the rather arbitrary decision boundaries for
acceptable agreement.
Model-Based Inference: In contrast to agreement
metrics, application of a model of annotation can
provide information about the certainty of param-
eter estimates. The model of annotation presented
in the next section includes as parameters the true
categories of items in the corpus, and also the
prevalence of each label in the corpus and each
annotator?s accuracies and biases by category.
3 A Probabilistic Annotation Model
A probabilistic model provides a recipe to ran-
domly ?generate? a dataset from a set of model
parameters and constants.1 The utility of a math-
ematical model lies in its ability to support mean-
ingful inferences from data, such as the true preva-
lence of a category. Here we apply the probabilis-
tic model of annotation introduced in (Dawid and
Skene, 1979); space does not permit detailed dis-
1In a Bayesian setting, the model parameters are them-
selves modeled as randomly generated from a prior distribu-
tion.
n iin jjn yn
1 1 1 4
2 1 3 1
3 192 17 5
...
...
...
...
Table 1: Table of annotations y indexed by word
instance ii and annotator jj.
cussion here of the inference process (this will be
provided in a separate paper that is currently in
preparation). Dawid and Skene used their model
to determine a consensus among patient histories
taken by multiple doctors. We use it to estimate
the consensus judgement of category labels based
on word sense annotations provided by multiple
Mechanical Turkers. Inference is driven by accu-
racies and biases estimated for each annotator on
a per-category basis.
Let K be the number of possible labels or cate-
gories for an item, I the number of items to anno-
tate, J the number of annotators, and N the total
number of labels provided by annotators, where
each annotator may label each instance zero or
more times. Each annotation is a tuple consist-
ing of an item ii ? 1:I , an annotator jj ? 1:J ,
and a label y ? 1:K. As illustrated in Table 1, we
assemble the annotations in a database-like table
where each row is an annotation, and the values in
each column are indices over the item, annotator,
and label. For example, the first two rows show
that on item 1, annotators 1 and 3 assigned labels
4 and 1, respectively. The third row says that for
item 192 annotator 17 provided label 5.
Dawid and Skene?s model includes parameters
? zi ? 1:K for the true category of item i,
? pik ? [0, 1] for the probability that an item is
of category k, subject to
?K
k=1 pik = 1, and
? ?j,k,k? ? [0, 1] for the probabilty that annota-
tor j will assign the label k? to an item whose
true category is k, subject to
?K
k?=1 ?j,k,k? =
1.
The generative model first selects the true cate-
gory for item i according to the prevalence of cat-
egories, which is given by a Categorical distribu-
tion,2
zi ? Categorical(pi).
2The probability of n successes inm trials has a binomial
distribution, with each trial (m=1) having a Bernoulli dis-
tribution. Data with more than two values has a multinomial
189
Word Pos Senses ? Agreement
curious adj 3 0.94 0.97
late adj 7 0.84 0.89
high adj 7 0.77 0.91
different adj 4 0.13 0.60
severe adj 6 0.05 0.32
normal adj 4 0.02 0.38
strike noun 7 0.89 0.93
officer noun 4 0.85 0.91
player noun 5 0.83 0.93
date noun 8 0.48 0.58
island noun 2 0.10 0.78
success noun 4 0.09 0.39
combination noun 7 0.04 0.73
entitle verb 3 0.99 0.99
mature verb 6 0.86 0.96
rule verb 7 0.85 0.90
add verb 6 0.55 0.72
help verb 8 0.26 0.58
transfer verb 9 0.22 0.42
ask verb 7 0.10 0.37
justify verb 5 0.04 0.82
Table 2: Agreement results for MASC words with
the three highest and lowest ? scores, by part of
speech, along with additional words discussed in
the text (boldface).
The observed labels yn are generated based on
annotator jj[n]?s responses ?jj[n], z[ii[n]] to items
ii[n] whose true category is zz[ii[n]],
yn ? Categorical(?jj[n], z[ii[n]]).
We use additively smoothed maximum likelihood
estimation (MLE) to stabilize inference. This is
equivalent to maximum a posteriori (MAP) estima-
tion in a Bayesian model with Dirichlet priors,
?j,k ? Dirichlet(?k) pi ? Dirichlet(?).
The unsmoothed MLE is equivalent to the MAP es-
timate when ?k and ? are unit vectors. For our
experiments, we added a tiny fractional count to
unit vectors, corresponding to a very small degree
of additive smoothing applied to the MLE.
4 MASC Word Sense Sentence Corpus
MASC (Manually Annotated SubCorpus) is a very
heterogeneous 500,000 word subset of the Open
American National Corpus (OANC) with 16 types
of annotation.3 MASC contains a separate word
sense sentence corpus for 116 words nearly evenly
distribution (a generalization of the binomial). Each trial then
results in one of k outcomes with a categorical distribution.
3Both corpora are available from http://www.anc.
org. The crowdsourced MASC words and labels will also
be available for download.
balanced among nouns, adjectives and verbs (Pas-
sonneau et al, 2012a). Each sentence is drawn
from the MASC corpus, and exemplifies a partic-
ular word form annotated for a WordNet sense.
To motivate our aim, which is to compare MASC
word sense annotations with the annotations we
collected through crowdsourcing, we review the
MASC word sense corpus and some of its limita-
tions.
College students from Vassar, Barnard, and
Columbia were trained to carry out the MASC word
sense annotation (Passonneau et al, 2012a). Most
annotators stayed with the project for two to three
years. Along with general training in the anno-
tation process, annotators trained for each word
on a sample of fifty sentences to become famil-
iar with the sense inventory through discussion
with Christiane Fellbaum, one of the designers
of WordNet, and if needed, to revise the sense
inventory for inclusion in subsequent releases of
WordNet. After the pre-annotation sample, an-
notators worked independently to label 1,000 sen-
tences for each word using an annotation tool that
presented the WordNet senses and example us-
ages, plus four variants of none of the above. Pas-
sonneau et al describe the training and annotation
tools in (2012b; 2012a). For each word, 100 of the
total sentences were annotated by three or four an-
notators for assessment of inter-annotator reliabil-
ity using pairwise agreement and Krippendorff?s
?.
The MASC agreement measures varied widely
across words. Table 2 shows for each part of
speech the words with the three highest and three
lowest ? scores, along with additional words ex-
emplified below (boldface).4 The ? values in col-
umn 2 range from a high of 0.99 (for entitle, verb,
3 senses) to a low of 0.02 (normal, adjective, 3
senses). Pairwise agreement (column 3) has simi-
larly wide variation. Passonneau et al (2012b) ar-
gue that the differences were due in part to the dif-
ferent words: each word is a new annotation task.
The MASC project deviated from the best prac-
tices described in section 2 in that there was no
iteration to achieve some threshold of agreement.
All annotators, however, had at least two phases
of training. Table 2 illustrates that annotators can
agree on words with many senses, but at the same
time, there are many words with low agreement.
4This table differs from a similar one Passonneau et al
give in (2012b) due to completion of more words and other
updates.
190
Even with high agreement, the measures reported
in Table 2 provide no information about word in-
stance quality.
5 Crowdsourced Word Sense Annotation
Amazon Mechanical Turk is a venue for crowd-
sourcing tasks that is used extensively in the NLP
community (Callison-Burch and Dredze, 2010).
Human Intelligence Tasks (HITs) are presented to
turkers by requesters. For our task, we used 45
randomly selected MASC words, with the same
sentences and WordNet senses the trained MASC
annotators used. Given our 1,000 instances per
word, for a category whose prevalence is as low
as 0.10 (100 examples expected), the 95% interval
for observed examples, assuming examples are in-
dependent, will be 0.10 ? 0.06. One of our future
goals for this data is to build item difficulty into the
annotation model, so we collected 20 to 25 labels
per item to get reasonable confidence intervals for
the true label. This will also sharpen our estimates
of the true category significantly, as estimated er-
ror goes down as 1/
?
n with n independent anno-
tations; confidence intervals must be expanded as
correlation among annotator responses increases
due to annotator bias or item-level effects such as
difficulty or subject matter.
In each HIT, turkers were presented with ten
sentences for each word, with the word?s senses
listed below each sentence. Each HIT had a short
paragraph of instructions indicating that turkers
could expect their time per HIT to decrease as their
familiarity with a word?s senses increased (we
wanted multiple annotations per turker per word
for tighter estimates of annotator accuracies and
biases).
To insure a high proportion of instances with
high quality inferred labels, we piloted the HIT de-
sign and payment regimen with two trials of two
and three words each, and discussed both with
turkers on the Turker Nation message board. The
final procedure and payment were as follows. To
avoid spam workers, we required turkers to have
a 98% lifetime approval rating and to have suc-
cessfully completed 20,000 HITs. Our HITs were
automatically approved after fifteen minutes. We
considered manual approval and programming a
more sophisticated approval procedure, but both
were deemed too onerous given the scope of
our task. Instead, we monitored performance of
turkers across HITs by comparing each individ-
ual turker?s labels to the current majority labels.
Turkers with very poor performance were warned
to take more care, or be blocked from doing fur-
ther HITs. Of 228 turkers, five were blocked, with
one subsequently unblocked. The blocked turker
data is included in our analyses and in the full
dataset, which will be released in the near future;
the model-based approach to annotation is effec-
tive at adjusting for inaccurate annotators.
6 Annotator Accuracy and Bias
Through maximum likelihood estimation of the
parameters of the Dawid and Skene model, an-
notators? accuracies and error biases can be esti-
mated. Figure 1a) shows confusion matrices in the
form of heatmaps that plot annotator responses by
the estimated true labels for four of the 57 annota-
tors who contributed labels for add-v (the affixes
-v and -n represent part of speech). This word
had a reliability of ?=0.56 for four trained MASC
annotators on 100 sentences and pairwise agree-
ment=0.73. Figure 1b) shows heatmaps for four of
the 49 annotators on help-v, which had a reliability
of ?=0.26 for the MASC annotators, with pairwise
agreement=0.58. As indicated in the figure keys,
darker cells have higher probabilities. Perfect ac-
curacy of annotator responses (agreement with the
inferred reference label) would yield black squares
on the diagonal, with all the off-diagonal squares
in white.
The two figures show that the turkers were
generally more accurate on add-v than on help-
v, which is consistent with the differences in the
MASC agreement on these two words. In contrast
to the knowledge gained from agreement metrics,
inference based on the annotation model provides
estimates of bias towards specific category values.
Figure 1a shows the bias of these annotators to
overuse WordNet sense 1 for help-v; bias appears
in the plots as an uneven distribution of grey boxes
off the main diagonal. Further, there were no as-
signments of senses 6 or 8 for this word. The fig-
ures provide a succinct visual summary that there
were more differences across the four annotators
for help-v than for add-v, with more bias towards
overuse of not only sense 1, but also senses 2 (an-
notators 8 and 41) and 3 (annotator 9). When an-
notator 8 uses sense 1, the true label is often sense
6, thus illustrating how annotators provide infor-
mation about the true label even from inaccurate
responses.
191
(a) Four of 57 annotators for add-v
(b) Four of 49 annotators for help-v
Figure 1: Heatmaps of annotators? accuracies and biases
For the 45 words, average accuracies per word
ranged from 0.05 to 0.86, with most words show-
ing a large spread. Examination of accuracies by
sense shows that accuracy was often highest for
the more frequent senses. Accuracy for add-v
ranged from 0.25 to 0.73, but was 0.90 for sense
1, 0.79 for sense 2, and much lower for senses
6 (0.29) and 7 (0.19). For help-v, accuracy was
best on sense 1 (0.73), which was also the most
frequent, but it was also quite good on sense 4
(0.64), which was much less frequent. Accuracies
on senses of help-v ranged from 0.11 (senses 5, 7,
and other) to 0.73 (sense 1).
7 Estimates for Prevalence and Labels
That the Dawid and Skene model allows an-
notators to have distinct biases and accuracies
should match the intuitions of anyone who has
performed annotation or collected annotated data.
The power of their parameterization, however,
shows up in the estimates their model yields for
category prevalence (rate of each category) and for
the true labels on each instance. Figure 2 con-
trasts five ways to estimate the sense prevalence
of MASC words, two of which are based on models
estimated via MLE. The MLE estimates each have
an associated probability, thus a degree of cer-
tainty, with more certain estimates derived from
the larger sets of crowdsourced labels (AMT MLE).
MASC Freq is a simple ratio. Majority voted labels
tend to be superior to single labels, but do not take
annotators? biases into account.
The plots for the four words in Figure 2 are or-
dered by their ? scores from four trained MASC
annotators (see Table 2). There is a slight trend
for the various estimates to diverge less on words
where agreement is higher. The notable result,
however, is that for each word, the plot demon-
strates one or more senses where the AMT MLE es-
timate differs markedly from all other estimates.
For add-v, the AMT MLE estimate for sense 1 is
much lower (0.51) than any of the other measures
(0.61-0.64). For date-n, the AMT MLE estimate for
sense 4 is much closer to the other estimates than
AMT Maj, which sugggests that some AMT an-
notators are baised against sense 4. The AMT MLE
estimates for senses 6 and 7 are quite distinct. For
help-v, the AMT MLE estimates for senses 1 and 6
are also very distinct. For ask-v, there are more
differences across all estimates for senses 2 and 4,
with the AMT MLE estimate neither the highest nor
the lowest.
The estimates of label quality on each item are
perhaps the strongest reason for turning to model-
based approaches to assess annotated data. For the
same four words discussed above, Table 3 shows
the proportion of all instances that had an esti-
mated true label where the label probability was
greater than or equal to 0.99. For these words with
? scores ranging from 0.10 (ask-v) to 0.55 (add-v),
the proportion of very high quality inferred true
labels ranges from 81% to 94%. Even for help-
v, of the remaining 19% of instances, 13% have
probabilities greater than 0.75. Table 3 also shows
192
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6
add-v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(a) add-v (? = 0.55, agreement=0.72)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense Sense
5
Sense
6
Sense
7
Sense
8
date -n MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(b) date-n (? = 0.48, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense
1
Sense
2
Sense
3
Sense
4
Sense
5
Sense
6
Sense
7
Sense
8
hel p -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(c) help-v (? = 0.26, agreement=0.58)
0.00
0.10
0.20
0.30
0.40
0.50
0.60
Other Sense 1 Sense 2 Sense 3 Sense 4 Sense 5 Sense 6 Sense 7
ask -v MASC Freq
MASC Maj
MASC MLE
AMT Maj
AMT MLE
(d) ask-v (? = 0.10, agreement=0.37)
Figure 2: Prevalence estimates for 4 MASC words; (MASC Freq) frequency of each sense in ? 1, 000
singly-annotated instances from the trained MASC annotators; (MASC Maj) frequency of majority vote
sense in ?100 instances annotated by four trained MASC annotators; (MASC MLE) estimated probability
of each sense in the same 100 instances annotated by four MASC annotators, using MLE; (AMT Maj)
frequency of each majority vote sense for ? 1000 instances annotated by ? 25 turkers; (AMT MLE)
estimated probability of each sense in the same ?1000 instances annotated by ?25 turkers, using MLE
Sense k ? 0.99 Prop.
0 9 0.01
1 461 0.48
2 135 0.14
3 107 0.11
4 50 0.05
5 50 0.05
6 93 0.10
SubTot 905 0.94
Rest 62 0.06
(a) add-v: 94%
Sense k ? 0.99 Prop.
0 19 0.02
1 68 0.07
2 19 0.02
3 83 0.09
4 173 0.18
5 190 0.20
6 133 0.14
7 236 0.25
8 5 0.01
SubTot 926 0.97
Rest 33 0.03
(b) date-n: 97%
Sense k ? 0.99 Prop.
0 0 0.00
1 279 0.30
2 82 0.09
3 201 0.21
4 24 0.03
5 0 0.00
6 169 0.18
7 0 0.00
8 5 0.01
SubTot 760 0.81
Rest 180 0.19
(c) help-v: 81%
Sense k ? 0.99 Prop.
0 6 0.01
1 348 0.36
2 177 0.18
3 9 0.01
4 251 0.26
5 0 0
6 0 0
7 6 0.01
8 6 0.01
SubTot 803 0.83
Rest 163 0.17
(d) ask-v: 83%
Table 3: Proportion of high quality labels per word
193
that the high quality labels for each word are dis-
tributed across many of the senses. Of the 45
words studied here, 22 had ? scores less than 0.50
from the trained annotators. For 42 of the same
45 words, 80% of the inferred true labels have a
probability higher than 0.99.
In contrast to current best practices, an annota-
tion model yields far more information about the
most essential aspect of annotation efforts, namely
how much uncertainty is associated with each gold
standard label, and how the uncertainty is dis-
tributed across other possible label categories for
each instance. An equally important benefit comes
from a comparison of the cost per gold standard
label. Over the course of a five-year period that
included development of the infrastructure, the
undergraduates who annotated MASC words were
paid an estimated total of $80,000 for 116 words
? 1000 sentences per word, which comes to a unit
cost of $0.70 per ground truth label. In a 12 month
period with 6 months devoted to infrastructure and
trial runs, we paid 224 turkers a total of $15,000
for 45 words? 1000 sentences per word, for a unit
cost of $0.33 per ground truth label. In short, the
AMT data cost less than half the trained annotator
data.
8 Related Work
The model proposed by Dawid and Skene (1979)
comes out of a long practice in epidemiology
to develop gold-standard estimation. Albert and
Dodd (2008) give a relevant discussion of dis-
ease prevalence estimation adjusted for accuracy
and bias of diagnostic tests. Like Dawid and
Skene (1979), Smyth (1995) used unsupervised
methods to model human annotation of craters on
images of Venus. In the NLP literature, Bruce
and Wiebe (1999) and Snow et al (2008) use
gold-standard data to estimate Dawid and Skene?s
model via maximum likelihood; Snow et al show
that combining noisy crowdsourced annotations
produced data of equal quality to five distinct pub-
lished gold standards. Rzhetsky et al (2009) and
Whitehill et al (2009) estimate annotation mod-
els without gold-standard supervision, but nei-
ther models annotator biases, which are criti-
cal for estimating true labels. Klebanov and
Beigman (2009) discuss censoring uncertain items
from gold-standard corpora. Sheng et al (2008)
apply similar models to actively select the next la-
bel to elicit from annotators. Smyth et al (1995),
Rogers et al (2010), and Raykar et al (2010)
all discuss the advantages of learning and evalu-
ation with probabilistically annotated corpora. By
now crowdsourcing is so widespread that NAACL
2010 sponsored a workshop on ?Creating Speech
and Language Data With Amazons Mechanical
Turk? and in 2011, TREC added a crowdsourcing
track.
9 Conclusion
The case study of word sense annotation presented
here demonstrates that in comparison to current
practice for assessment of annotated corpora, an
annotation model applied to crowdsourced labels
provides more knowledge and higher quality gold
standard labels at lower cost. Those who would
use the corpus for training benefit because they
can differentiate high from low confidence la-
bels. Cross-site evaluations of word sense dis-
ambiguation systems could benefit because there
are more evaluation options. Where the most
probable label is relatively uncertain, systems can
be penalized less for an incorrect but close re-
sponse (e.g., log loss). Systems that produce sense
rankings for each instance could be scored us-
ing metrics that compare probability distributions,
such as Kullbach-Leibler divergence (Resnik and
Yarowsky, 2000). Wider use of annotation mod-
els should lead to more confidence from users in
corpora for training or evaluation.
Acknowledgments
The first author was partially supported by from
NSF CRI 0708952 and CRI 1059312, and the
second by NSF CNS-1205516 and DOE DE-
SC0002099. We thank Shreya Prasad for data
collection, Mitzi Morris for feedback on the paper,
Marilyn Walker for advice on Mechanical Turk,
and Nancy Ide, Keith Suderman, Tim Brown and
Mitzi Morris for help with the sentence data.
References
Paul S. Albert and Lori E. Dodd. 2008. On esti-
mating diagnostic accuracy from studies with mul-
tiple raters and partial gold standard evaluation.
Journal of the American Statistical Association,
103(481):61?73.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
194
Rebecca F. Bruce and Janyce M. Wiebe. 1998. Word-
sense distinguishability and inter-coder agreement.
In Proceedings of Empirical Methods in Natural
Language Processing.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study of manual tagging.
Natural Language Engineering, 1(1):1?16.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society.
Series C (Applied Statistics), 28(1):20?28.
Barbara di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95?101.
Barbara di Eugenio. 2000. On the usage of kappa
to evaluate agreement on coding tasks. In Proceed-
ings of the Second International Conference on Lan-
guage Resources and Evaluation (LREC).
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In Word Sense Disambiguation: Al-
gorithms and Applications, pages 47?74. Springer
Verlag.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics, 35(4):495?503.
Klaus Krippendorff. 1980. Content analysis: An in-
troduction to its methodology. Sage Publications,
Beverly Hills, CA.
Rebecca J. Passonneau, Collin F. Baker, Christiane
Fellbaum, and Nancy Ide. 2012a. The MASC
word sense corpus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Rebecca J. Passonneau, Vikas Bhardwaj, Ansaf Salleb-
Aouissi, and Nancy Ide. 2012b. Multiplicity and
word sense: evaluating and learning from multi-
ply labeled word sense annotations. Language Re-
sources and Evaluation, 46(2):219?252.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297?
1322.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natu-
ral Language Engineering, 5(3):113?133.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistical Computing, 20:317?334.
Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology, 5(5):1?13.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of the Fourteenth ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjectively-labeled images of Venus. In
Advances in Neural Information Processing Systems
7, pages 1085?1092. MIT Press.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254?263, Honolulu.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Proceedings
of the 24th Annual Conference on Advances in Neu-
ral Information Processing Systems.
195
Proceedings of the SIGDIAL 2014 Conference, pages 228?237,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Aspectual Properties of Conversational Activities
Rebecca J. Passonneau and Boxuan Guan and Cho Ho Yeung
becky@ccls.columbia.edu and bg2469@columbia.edu and cy2277@columbia.edu
Columbia University, New York, NY, USA
Yuan Du
ydu@fb.com
Facebook, New York, NY, USA
Emma Conner
econner@oberlin.edu
Oberlin College, Oberlin, OH, USA
Abstract
Segmentation of spoken discourse into
distinct conversational activities has been
applied to broadcast news, meetings,
monologs, and two-party dialogs. This
paper considers the aspectual properties
of discourse segments, meaning how they
transpire in time. Classifiers were con-
structed to distinguish between segment
boundaries and non-boundaries, where the
sizes of utterance spans to represent data
instances were varied, and the locations
of segment boundaries relative to these in-
stances. Classifier performance was better
for representations that included the end of
one discourse segment combined with the
beginning of the next. In addition, classi-
fication accuracy was better for segments
in which speakers accomplish goals with
distinctive start and end points.
1 Introduction
People engage in dialogue to address a wide range
of goals. It has long been observed that discourse
can be structured into units that correspond to dis-
tinct goals and activities (Grosz and Sidner, 1986;
Passonneau and Litman, 1997). This is concep-
tually distinct from structuring discourse into the
topical units addressed in (Hearst, 1997). The
ability to recognize where distinct activities oc-
cur in spoken discourse could support offline ap-
plications to spoken corpora such as search (Ward
and Werner, 2013), summarization (Murray et al.,
2005), and question answering. Further, a deeper
understanding of the relation of conversational
activities to observable features of utterance se-
quences could inform the design of interactive sys-
tems for online applications such as information
gathering, service requests, tutoring, and compan-
ionship. Automatic identification of such units,
however, has been difficult to achieve. This pa-
per considers the aspectual properties of speak-
ers? conversational activities, meaning how they
transpire in time. We hypothesize that recognition
of a transition to a new conversational activity de-
pends on recognizing not only the start of a new
activity but also the end of the preceding one, on
the grounds that the relative contrast between end-
ings and beginnings might matter as much or more
than absolute characteristics consistent across all
beginnings or all endings. We further hypothesize
that transitions to certain kinds of conversational
activity may be easier to detect than others.
Following Austin?s view that speech constitutes
action of different kinds (Austin, 1962), we as-
sume that different kinds of communicative ac-
tion have different ways of transpiring in time,
just as other actions do. Conversational activities
that address objective goals, for example, can have
very well-demarcated beginnings and endings, as
when two people choose a restaurant to go to
for dinner. Conversational participants can, how-
ever, address goals that need not have a specific
resolution, such as shared complaints about the
lack of good Chinese restaurants. This distinction
between different kinds of actions that speakers
perform through their communicative behavior is
analogous to the distinction in linguistic semantics
pertaining to verbal aspect, between states, pro-
cesses and transition events (or accomplishments
and achievements) (Vendler, 1957) (Dowty, 1986).
States (e.g., being at a standstill) have no percep-
tible change from moment to moment; processes
(e.g., walking) have detectable differences in state
from moment to moment with no clearly demar-
cated change of state during the process; transition
events (e.g., starting to walk; walking to the end
of the block) involve a transition from one state or
process to another.
To investigate the aspectual properties of dis-
course segments, we constructed classifiers to de-
228
tect discourse segment boundaries based on fea-
tures of utterances. We considered the aspec-
tual properties of discourse segments in two ways.
First, to investigate the relative contribution of
features from segment endings versus beginnings,
we experimented with different sizes of utter-
ance sequences, and different locations of seg-
ment boundaries relative to these sequences. Sec-
ond, we considered different categories of seg-
ments, based on the speculation that segment tran-
sitions that are easier to recognize would be as-
sociated with conversational activities that have
a well-demarcated event structure, in constrast to
activities that involve goals to maintain or sustain
aspects of interaction.
The following section describes related work in
this area, as well as the difficulties in achieving
good performance. Most work on identification of
discourse segments (or other forms of discourse
structure in spoken interaction) depends on a prior
phase of annotation (e.g., (Galley et al., 2003; Pas-
sonneau and Litman, 1997)). We studied a corpus
of eighty-two transcribed and annotated telephone
dialogues between library patrons and librarians
that had been annotated with units analogous to
speech acts, and subsequently annotated with dis-
course segments comprised of these units. The an-
notation yielded eight distinct kinds of discourse
segment, where a segment results from a linear
segmentation of a discourse into strictly sequential
units. (While the segmentation is sequential, the
units can have hierarchical relations.) We found
that classifiers to detect segment boundaries per-
formed best with boundaries represented by fea-
tures of sequences of utterances that spanned the
end of one segment and the beginning of the next.
Error analysis indicated that performance was bet-
ter for boundaries that initiate conversational ac-
tivities with clear beginnings and endings.
2 Related Work
Segmentation of spoken language interaction into
distinct discourse units has been applied to meet-
ings as well as to two-party discourse using acous-
tic features, lexical features, and very heteroge-
neous features. In our previous work, we used
a very heterogeneous set of features to segment
monologues into units that had been identified
by annotators as corresonding to distinct inten-
tional units (Passonneau and Litman, 1997). Text
tiling (Hearst, 1997) has been applied to segmen-
tation of meetings into distinct agenda segments
using both prior and following context (Baner-
jee and Rudnicky, 2006). Results had high pre-
cision and low recall. We also find that recall is
more challenging than precision. Topic modeling
methods have also been applied to the identifica-
tion of topical segments in speech (Purver et al.,
2006) (Eisenstein and Barzilay, 2008), with im-
provements over earlier work on the ICSI meeting
corpus (Galley et al., 2003) (Malioutov and Barzi-
lay, 2006).
An analog of text tiling that uses acoustic pat-
terns rather than lexical items has been applied to
the segmentation of speech into stories using seg-
mental dynamic time warping (SDTW) (Park and
Glass, 2008). The method is based on the intuition
of aligning utterances by similar acoustic patterns,
possibly representing common words and phrases.
Results on TDT2 Mandarin Broadcast News cor-
pus were moderately good for short episodes with
F=0.71 beating the baseline for lexical text tiling
of 0.66, but poor on long episodes.
An alternative method of relying solely on
acoustic information has been applied to impor-
tance prediction at a very fine granularity (Ward
and Richart-Ruiz, 2013). Four basic classes
of prosodic features derived from PCA were
used (Ward and Vega, 2012): volume, pitch
height, pitch range and speaking rate cross various
widths of time intervals. The data was labeled by
annotators using an importance scale of 1 to 5, and
linear regression was used to predict the label for
instances consisting of frames. The method per-
formed well with a correlation of 0.82 and mean
average error of 0.75 (5-fold cross validation).
The identification of different kinds of units in
discourse is somewhat related to the notion of
genre identification, e.g. (Obin et al., 2010) (Ries
et al., 2000). Results from this area have been ap-
plied to segmentation of conversation by a combi-
nation of topic and style (Ries, 2002).
3 Data and Annotations
The corpus consists of recordings, transcripts and
annotations on the transcripts of a set of 82 calls
recorded in 2005 between patrons of the Andrew
Heiskell Braille and Talking Book Library of New
York City.
1
An annotation for dialog acts with a
1
The audio files and transcripts are available for download
from the Columbia University Data Commons. The annota-
tions and raw features will be released in the near future.
229
reduced set of dialog act types and adjacency pair
relations (Dialogue Function Units, DFUs) was
developed, originally for comparison of dialogues
across modalities (Hu et al., 2009). A subsequent
phase of annotation at the discourse level that
makes use of the dialog act annotation was later
applied. This later annotation, referred to as Task
Success and Cost Annotation (TSCA), was aimed
at identifying individual dialog tasks analogous to
those carried out by spoken dialog systems, to fa-
cilitate comparison of human-human dialog with
human-machine dialog. Interannotator reliability
of both annotations was measured using Krippen-
dorff?s alpha (Krippendorff, 1980) at levels of 0.66
and above for individual dialogues (Passonneau et
al., 2011). The corpus consists of 24,760 words,
or 302 words per dialog.
Briefly, the second phase of annotation involved
grouping DFUs into larger sequences in which
the participants continued to pursue a single co-
ordinated activity, and labeling the large discourse
units for their discourse function. The human an-
notation instructions avoided reference to overt
signals of dialog structure. Rather, annotators
were asked to judge the semantic and pragmatic
functions of utterances. The annotations have been
described in previous work (Hu et al., 2009; Pas-
sonneau et al., 2011); the annotation guidelines are
available online.
2
The location of a transition between one con-
versational activity and the next is represented as
occurring between adjacent utterances. There are
9,340 utterance in the corpus, or 114 per dialog.
About 10.6 percent of the utterances (994) start a
new discourse unit. Within each unit, the speak-
ers establish a conversational goal explicitly or im-
plicitly, and continue to address the goal until it
is achieved, suspended, or abandoned. The dis-
course segments were of the following seven cate-
gories, with an additional Other category for none
of the above (examples from the corpus are shown
after each segment category description; words
in brackets represent overlapping talk of the two
speakers):
? Conventional: The participants engage in
conventionalized behavior, e.g., greetings (at
the beginning of the call) or goodbyes (at the
end of the call).
2
See links at http://www1.ccls.columbia.
edu/
?
Loqui/resources.html for transcription
guidelines, and annotation manuals.
Librarian: andrew heiskell library
Librarian: how are you
Patron: good morning
Librarian: good morning
? Book-Request: The participants address a pa-
tron?s request for a book, which can be a spe-
cific book that first needs to be identified,
or which can be a non-specific request for a
book fitting some criterion (e.g., a mystery
the patron has not read before).
Patron: do you have any fannie flagg stories
Librarian: flag
Patron: yeah
Patron: F L A <Pause>
Patron: A G G I think it is
? Inform: One of the participants provides the
other with general information that does not
support a Book Request, e.g., the patron pro-
vides identifying information so the librarian
can pull up the patron?s record.
Patron: well I?ll call him again then
Patron: and I?ll get the name [today]
Librarian [talk] to him and call me back
Patron: <pause> i- i?ll call him
Patron: and then i?ll call you okay
Librarian: okay
? Librarian-Proposal: The participants address
the librarian?s suggestion of a specific book
or a kind of book that might meet the patron?s
desires.
Librarian: I have ellis but not bret
Patron: ah wa wa what do you have by him
Librarian: by cose
Librarian: C O S E
Librarian: I have the rage of a privileged class
Patron: that?s all right
? Request-Action: One of the participants asks
the other to perform an action, e.g., the pa-
tron asks that certain authors be added to the
patron?s list of preferences
Patron: also <pause> uh
Patron: <pause> of the favorite author list
Librarian: mmhm
Patron: would you um
Patron: remove t jefferson parker
Librarian: okay
230
? Information-Request: One of the participants
seeks information from the other, e.g., the pa-
tron wants to know if
Patron: this is the talking books right
Librarian: yes
Librarian: this is the library for the blind
? Sidebar: The librarian temporarily takes a
call from another Patron only long enough to
place the new caller on hold
Librarian: hold on one second
Librarian: Andrew Heiskell Library
Librarian: please hold
? Other
Of these seven kinds of discourse units, Book-
Requests and Librarian-Proposals are the most
clearly delimited by beginning and ending points.
At the beginning of a Book-Request, the patron
establishes that she wants a book, and the end is
identified by the mutual achievement of the librar-
ian and patron of either a successful resolution,
meaning the identification of a particular book in
the library?s collection that the patron will accept,
or a failure of the current attempt, which often
leads to a new revised book request. Librarian-
Proposals are very parallel to Book-Requests; the
difference is that the librarian makes a suggestion
of a specific book or kind of book which must be
identified for the patron, and which the patron then
accepts or rejects.
4 Experiments
The experiments to automatically identify the lo-
cations of the annotated discourse units apply ma-
chine learning to instances consisting of utterance
sequences that represent the two classes, presence
versus absence of a boundary. We hyothesize
that the enormous challenges for identifying dis-
course structure in human-machine dialogue can
be better addressed through complementary re-
liance on semantics and interaction structure (be-
havioral cues), and each can reinforce the other.
The main focus of the experiments reported here
is on data representation to address the questions,
what features of the context support the ability
to segment a dialogue into conversational activity
units, and how much context is necessary?
A disadvantage of the dataset is its relatively
small size, especially given the extreme skew with
0
0
1
1
2
 + First utterance of segment
+ Last utterance of segment      
S1P0
S1P1
- Any other utterance 
 - Any other utterance 
S2P2
S2P1
S2P0 + First 2 utterances of segment - Any other sequence of 2 utterances 
 - Any other sequence of 2 utterances 
 - Any other sequence of 2 utterances + Last utterance of one segment, first of next+ Last 2 utterances of segment
Figure 1: Schematic representation of instance
spans and labels. Bars on the left show the num-
ber of utterances (size) and position of segment
boundary (position) for five of the fourteen types
of instances. Positive and negative labels are
shown on the descriptions at the right.
the positive class consisting of only 10% of the in-
stances. On the other hand, the small size made
detailed annotation feasible, and the corpus is
well-suited to our research question in that it rep-
resents naturally occurring, spontaneous human-
human telephone discourse. Therefore. the man-
ner in which the dialogs evolve over time is en-
tirely natural. Our major question of interest is
how much of the time-course of the discourse is
required for a machine learner to identify the start
of a new discourse unit. To examine this question,
we vary two dimensions of the representation of
the instances for learning. The first is the number
of utterances around the location of the start of a
new discourse unit. The second is the set of fea-
tures to represent each instance, which as we will
see below, affects to some degree how many utter-
ances to include before and after the start of a new
discourse unit.
Four machine learning methods were tested us-
ing the Weka toolkit (Hall et al., 2009): Naive
Bayes, J48 Decision Trees, Logistic Regression
and Multilayer Perceptron. Of these, J48 had the
best and most consistent performance, which we
speculate is due to a combination of the small size
of the dataset, and non-linearity of the data. Be-
cause J48 is doing feature selection while building
the tree, it can identify different threshholds for
the same features, depending on the location in the
tree. All results reported here are for J48.
4.1 Labels and Instance Spans
We refer to a sequence of utterances, and a poten-
tial location of the onset of a discourse unit relative
to that sequence, as a span. We varied the num-
231
ber of utterances for each span from 1 to 4, and
the location of the start of a new unit to be at the
beginning of the first utterance, at the end of the
last utterance, or between any pair of utterances in
the span. For a single utterance, there will be two
types of instances, as shown in Figure 1. Each in-
stance type is represented as S<N>P<M> where
N is the number of utterances in the span and M is
how many utterances there are before the bound-
ary. S1P0 denotes size 1 spans with the boundary
at position 0; positively labeled instances repre-
sent the first utterance of a segment. S1P1 denotes
size 1 spans with the boundary at position 1; posi-
tively labeled instances represent the last utterance
of a segment. The experiments used all labelings
for spans from size 1 to 4, yielding 14 types of
instances. For multi-utterance spans that occur at
the beginning or end of a discourse, dummy utter-
ances are used to fill out the spans.
4.2 Features
We use three sets of features. A set we refer to
as discourse features consists of a mixed set of
acoustic features and lexicogrammatical features
that have been associated with discourse structure,
such as discourse cue words (Hirschberg and Lit-
man, 1993). Table 1 lists the 35 discourse features.
The second set is a bag-of-words (BOW) vector
representation, and the third is the combination of
the discourse and BOW features. We used alterna-
tive sets of features on the assumption that the per-
formance of a machine learner across the differ-
ent instance spans will vary, depending on the as-
pects of the utterance that the features capture. We
see some expected differences in performance be-
tween the discourse features and BOW, with BOW
benefitting more than the discourse features from
longer spans. Unexpectedly, we see no gain in
performance from the combination of both feature
sets.
The discourse features consist of acoustic fea-
tures, pause features, word and utterance length
features, proper noun features and speaker change.
The acoustic features and the (unfilled) pause lo-
cation and duration features were extracted using
Praat, a cross-platform tool for speech analysis.
The features pertaining to filled pauses (e.g., um,
uh) were extracted from the transcripts.
4.3 Conditions and Evaluation
The experimental conditions varied the feature set,
the selection of training data versus testing data,
and the fourteen kinds of instance spans and la-
bels. Three feature sets consisted of the discourse
features from Table 1 (discourse), bag-of-words
(bow), and the combination of the two (combo).
In all experiments, the data was randomly split
into 75% for training, and 25% for testing, us-
ing two methods to select instances. In random-
ization by dialog, all utterances from a single di-
alog were kept together and 75% of the dialogs
were selected for training. In randomization by
utterance, 75% of all utterances were randomly
selected for training, without regard to which di-
alog they came from. This was done to test the
hypothesis that the bow representation would be
more sensitive to changes of vocabulary across di-
alogs. The three feature sets, fourteen data rep-
resentations and two randomization methods yield
84 experimental conditions.
While N-fold cross-validation is a popular
method to estimate a classifier?s prediction error,
it is not a perfect substitute for isolating the train-
ing data from the test data (Ng, 1997). The cross-
validation estimate of prediction error is relatively
unbiased, but it can be highly variable (Efron and
Tibshirani, 1997)(Rodriguez et al., 2010). To
avoid the inherent risk of overfitting (Ng, 1997),
one recommendation is to use cross-validation to
compare models, and to reserve a test set to verify
that a selected classifier has superior generaliza-
tion (Rao and Fung, 2008). To assess whether per-
formance measures of different models are gen-
uinely different requires error bounds on the result,
which is not done with cross-validation. We per-
form train-test splits of the data to minimize over-
fitting, and bootstrap confidence intervals for each
classifier?s accuracy (and other metrics) in order to
measure the variance, and thereby assess whether
the performance error bounds of two conditions
are distinct.
5 Results
Given that for this data, the rate of segment
boundary instances (positive labels) is about 10%,
a baseline classifier that always predicts a non-
segment will have about 90% overall accuracy.
The baseline column in Table 2 shows the aver-
age accuracy that would be achieved by this sim-
ple baseline on the test data for a given run, along
with the bootstrapped confidence interval for this
baseline over the 50 runs. In the 84 experiments,
the baseline ranged from 90% (+/- 1%) to 89% (+/-
232
Interaction feature
1 Speaker whether there is a speaker switch between preceding utterance and current utterance
Acoustic features
2 Pitch MIN Minimum pitch of the utterance
3 Pitch MAX Maximum pitch of the utterance
4 Pitch MEAN Mean pitch of the utterance
5 Pitch STDV Standard deviation of the pitch of the utterance
6 Pitch RANGE Maximim pitch of the utterance less the minimum pitch
7 Pitch CHANGE Pitch MEAN of the current utterance less the Pitch MEAN of the preceding utterance
8 Intensity MIN Minimum intensity of the utterance
9 Intensity MAX Maximum intensity of the utterance
10 Intensity MEAN Mean intensity of the utterance
11 Intensity STDV Standard deviation of the intensity of the utterance
12 Intensity RANGE Intensity MAX less Intensity MIN
13 Intensity CHANGE Intensity MEAN of the current utterance less Intensity MEAN of preceding utterance
14 LR1 Utterance duration
15 LR1 Normalized Utterance duration normalized by each speaker independently
Lexical features
16 LR2 1 Word count
17 LR2 2 Word count normalized by speaker
18 LR3 1 Words per second
19 LR3 2 Words per second by speaker
20 LR4 Average word length
21 LR5 Maximum word length
22 LR6 1 Average frequency of characters in the utterance
23 LR6 2 Number of low frequency characters
24 IR Number of content words
25 PN 1 Number of named entities
26 PN 2 Whether the utterance contains a new named entity
Pause features
27 Pause DURT total duration of all pauses
28 Pause RATIO proportion utterance consisting of pauses
29 FP1 Presence of a filled pause at the beginning of an utterance
30 FP2 Presence of a filled pause at the end of an utterance
31 FP3 Presence of a filled pause in the middle of an utterance
32 P1 Presence of a pause tag at the beginning of an utterance
33 P2 Presence of a pause tag at the end of an utterance
34 P3 Presence of a pause tag in the middle of an utterance
Table 1: Discourse Features
1%). Crucially, however, the simple baseline will
fail to identify any of the members of the positive
class. Though it is difficult to beat the baseline
on overall accuracy, the question addressed here
is what level of accuracy is achieved on the pos-
itive class, while remaining relatively consistent
with the baseline on overall accuracy. It should
be noted that accuracy on the positive class is the
same as recall, or sensitivity (the term used in the
epidemiological literature). The worst perform-
ing classifier among the 84 (disc/utterance/ S1P4)
achieves 83% (+/- 1%) accuracy overall, or below
the baseline by 6%, with 11% accuracy on the pos-
itive class, 100% of which is a gain over the base-
line. By this standard, the best classifier of the 84
conditions (bow/dial/S4P1) matches the baseline
on overall accuracy, and achieves 50% (+/- 5%)
accuracy on the positive class, which far exceeds
the baseline. About half of the experimental con-
ditions meet the baseline and achieve at least 25%
accuracy on the positive class.
Overall accuracy, and accuracy on the positive
class, measure prediction error, but can be supple-
mented with additional metrics that facilitate anal-
ysis of the nature and cost of error types. As a sup-
plementary metric, we report average F-measure,
the harmonic mean of recall and precision, due to
its familiarity, and because it provides a sense of
how often a classifier incorrectly predicts the pos-
itive class. An F-measure close to accuracy on the
positive class indicates that precision is about the
same as recall, while a relatively higher F-measure
indicates that the precision is even higher than the
F-measure, and the converse is true when the F-
measure is lower than accuracy on the positive
class. Table 2 shows 32 classifiers with the high-
est measures of accuracy, accuracy on the positive
class, and F-measure. The confidence intervals for
accuracy on the positive class and F-measure are
rather wide, compared to those for overall accu-
233
Exp. Baseline (sd) Acc (sd) AccPos(Recall) (sd) F (sd) >
Acc
pos
> F
bow/dial/S4P1 0.89 (+/-0.010) 0.89 (+/-0.009) 0.42 (+/-0.082) 0.28 (+/-0.054) 22 11
bow/dial/S4P2 0.90 (+/-0.013) 0.89 (+/-0.010) 0.39 (+/-0.071) 0.26 (+/-0.064) 22 3
bow/utterance/S1P0 0.90 (+/-0.004) 0.90 (+/-0.005) 0.51 (+/-0.051) 0.26 (+/-0.034) 30 11
bow/utterance/S4P0 0.89 (+/-0.005) 0.88 (+/-0.006) 0.43 (+/-0.049) 0.26 (+/-0.040) 23 10
disc/dial/S2P1 0.90 (+/-0.009) 0.87 (+/-0.009) 0.32 (+/-0.059) 0.26 (+/-0.037) 4 10
bow/utterance/S4P3 0.89 (+/-0.006) 0.88 (+/-0.005) 0.41 (+/-0.050) 0.25 (+/-0.027) 22 11
combo/dial/S3P2 0.89 (+/-0.011) 0.86 (+/-0.010) 0.31 (+/-0.048) 0.25 (+/-0.031) 7 10
disc/dial/S4P3 0.90 (+/-0.008) 0.86 (+/-0.009) 0.30 (+/-0.041) 0.25 (+/-0.030) 4 10
combo/dial/S3P1 0.89 (+/-0.010) 0.86 (+/-0.011) 0.31 (+/-0.059) 0.25 (+/-0.038) 3 10
combo/dial/S4P2 0.89 (+/-0.013) 0.86 (+/-0.012) 0.30 (+/-0.044) 0.25 (+/-0.031) 4 10
combo/dial/S2P1 0.89 (+/-0.012) 0.87 (+/-0.010) 0.32 (+/-0.054) 0.25 (+/-0.033) 7 10
combo/dial/S4P3 0.90 (+/-0.007) 0.87 (+/-0.008) 0.29 (+/-0.044) 0.25 (+/-0.035) 4 10
disc/dial/S3P2 0.90 (+/-0.008) 0.87 (+/-0.008) 0.29 (+/-0.047) 0.25 (+/-0.040) 3 10
bow/utterance/S4P1 0.90 (+/-0.005) 0.89 (+/-0.004) 0.40 (+/-0.053) 0.25 (+/-0.020) 22 10
bow/dial/S4P3 0.90 (+/-0.007) 0.89 (+/-0.009) 0.39 (+/-0.072) 0.25 (+/-0.035) 22 10
disc/dial/S4P2 0.90 (+/-0.009) 0.86 (+/-0.009) 0.28 (+/-0.042) 0.25 (+/-0.030) 0 10
bow/dial/S1P0 0.90 (+/-0.009) 0.89 (+/-0.009) 0.48 (+/-0.065) 0.24 (+/-0.045) 28 0
combo/dial/S4P1 0.90 (+/-0.010) 0.86 (+/-0.010) 0.28 (+/-0.045) 0.24 (+/-0.034) 0 9
disc/dial/S3P1 0.89 (+/-0.011) 0.86 (+/-0.010) 0.29 (+/-0.046) 0.24 (+/-0.033) 2 9
bow/dial/S4P0 0.90 (+/-0.009) 0.88 (+/-0.011) 0.37 (+/-0.031) 0.24 (+/-0.040) 22 0
disc/dial/S4P1 0.90 (+/-0.009) 0.86 (+/-0.008) 0.27 (+/-0.041) 0.23 (+/-0.032) 0 3
bow/utterance/S4P2 0.89 (+/-0.007) 0.88 (+/-0.010) 0.39 (+/-0.044) 0.23 (+/-0.033) 22 0
combo/utterance/S2P0 0.89 (+/-0.005) 0.86 (+/-0.009) 0.27 (+/-0.041) 0.21 (+/-0.029) 0 0
disc/dial/S2P0 0.89 (+/-0.010) 0.86 (+/-0.009) 0.27 (+/-0.047) 0.20 (+/-0.027) 0 0
disc/utterance/S2P0 0.90 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.032) 0.20 (+/-0.024) 0 0
combo/utterance/S1P0 0.89 (+/-0.005) 0.88 (+/-0.006) 0.31 (+/-0.041) 0.20 (+/-0.026) 10 0
combo/utterance/S3P0 0.90 (+/-0.005) 0.86 (+/-0.008) 0.25 (+/-0.038) 0.20 (+/-0.033) 0 0
disc/utterance/S4P3 0.89 (+/-0.006) 0.86 (+/-0.009) 0.24 (+/-0.043) 0.20 (+/-0.033) 0 0
combo/utterance/S2P1 0.89 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.036) 0.20 (+/-0.023) 0 0
disc/utterance/S2P1 0.89 (+/-0.005) 0.86 (+/-0.007) 0.26 (+/-0.032) 0.20 (+/-0.022) 0 0
combo/utterance/S4P1 0.89 (+/-0.006) 0.85 (+/-0.008) 0.24 (+/-0.033) 0.20 (+/-0.027) 0 0
disc/utterance/S4P0 0.89 (+/-0.006) 0.85 (+/-0.009) 0.24 (+/-0.034) 0.20 (+/-0.024) 0 0
Table 2: Classification performance (with standard deviations in parentheses) of the best 40% of 84
J48 models trained on 75% of the data and tested on the remaining 25%, with bootstrapped confidence
intervals from 50 trials each.
racy. To draw comparisons among the classifiers
that take into account this variance, the two right-
most columns of the table indicate for each clas-
sifier how many other classifiers in the same ta-
ble the current classifier surpasses on mean accu-
racy of the positive class, or on mean F-measure.
Here, to surpass another classifier means the lower
bound of its confidence interval surpasses the up-
per bounds of other classifiers? confidence inter-
vals.
Table 2 shows that there is no one classifier that
surpasses all others on all measures. There are,
however, some clear trends. Regarding the num-
ber of utterances spanned by each data instance,
the table shows that of the 32 best performing clas-
sifiers, the majority (seventeen) have size 4 spans,
and all but three have spans longer than a single
utterance. This trend indicates that more context
leads to better accuracy overall and better accuracy
on the positive class. Regarding where the seg-
ment boundary is located relative to the span, the
majority of cases (twenty-two) locate the bound-
ary within the span, meaning that the span includes
one or more of the final utterances of a segment
and one or more of the initial utterances of the next
segment. The remaining cases involve spans that
include utterances only from the beginning of the
segment. There are no cases of higher perform-
ing classifiers that use spans from segment end-
ings. Among the classifiers in the top half of the
table, the best performing bow classifiers surpass
a larger number of the other classifiers on accu-
racy of the positive class. The best performing dis-
course or combination classifiers surpass a larger
number of other classifiers on F-measure. This
suggests that in general, the bow classifiers do bet-
ter on recall and the classifiers with discourse fea-
tures have higher precision.
The combination of BOW and discourse fea-
tures has a performance that differs little from the
discourse features alone, and does not do as well
as BOW S4P1. This result was unexpected, and
suggests that the bow and discourse feature sets
often identify nearly the same set of discourse
234
Discourse, Rand Dial, S4P3
Activity Type TP % FN %
Inform 7 (0.11) 56 (0.89)
Book Request 18 (0.32) 40 (0.68)
Librarian Proposal 4 (0.27) 11 (0.73)
Request-Action 0 (0.00) 6 (1.00)
Information-Request 6 (0.11) 47 (0.89)
Sidebar 1 (0.08) 11 (0.92)
Conventional 5 (0.17) 25 (0.83)
Total 37 (0.14) 230 (0.86)
BOW, Rand Dial, S4P2
Inform 7 (0.10) 70 (0.90)
Book Request 14 (0.20) 57 (0.80)
Librarian Proposal 1 (0.05) 20 (0.95)
Request-Action 0 (0.00) 5 (1.00)
Information-Request 8 (0.16) 42 (0.84)
Sidebar 0 (0.00) 13 (1.00)
Conventional 6 (0.23) 29 (0.77)
Total 37 (0.14) 230 (0.86)
Table 3: Error Analysis of the Positive Class
boundaries. Since the initial utterances of a seg-
ment seem to have features with greater predictive
power than the final utterances of a segment, and
since discourse cue words tend to occur in the first
utterance or so of a segment, it could be that dis-
course cue words explain the good performance
of both sets of features. This could be tested in fu-
ture work by restricting a BOW representation to
words other than discourse cue words.
To pursue in more detail the factors that influ-
ence accuracy on the positive class (recall), we
now turn to an error analysis of the kinds of dis-
course units associated with true positives versus
false negatives of the classifier?s confusion matrix.
Table 3 presents the results of an error analysis of
the two cells of the confusion matrix for a clas-
sifier?s results on the positive class, the true pos-
itives and the false negatives. We looked at the
breakdown of the seven kinds of discourse units
to see whether there were differences in the like-
lihood of a correct identification of a boundary,
depending on the kind of discourse unit in ques-
tion. Results are drawn from classifiers learned
under two conditions, S4P3 spans with discourse
features randomized by dialogue (disc/dial/S4P3)
and S4P3 spans with BOW features, randomized
by dialogue (bow/dial/S4P3). (Results from other
classifiers are very similar.) In both cases, Book-
Requests have a much higher probability of be-
ing among the true positives (32% for discourse,
20% for BOW) than for the positive class over-
all (14%). Conventional discourse units, where
the participants first make their greetings, or make
their final good byes, are also correctly identified
more often than the overall TP rate. Librarian Pro-
posals are identified well by the model using the
discourse features, but not by the one using the
BOW features. We speculate that this is because
Librarian Proposals typically present information
that is new to the discourse: often, the librarian
is making a suggestion to the patron based on in-
formation the librarian can see in the preference
field of the patron?s record, or in the patron?s past
borrowing behavior. We speculate that the vocab-
ulary in Librarian Proposals may be too variable
to be predictive. Information-Request units and
Inform units are also relatively difficult to identify
correctly.
6 Conclusion
The problem of identification of conversational ac-
tivities is a difficult one for machine processing for
many reasons. Like vision and speech, segmenta-
tion of the units is difficult because the units are
not discrete, objective, components of perception,
but instead are the result of abstraction. The exper-
iments presented here consider a novel explana-
tion for the difficulty of the task, which is that dis-
course units differ from each other regarding the
manner in which they evolve in time. The results
show that a data representation that includes utter-
ances from both the end of one unit and the begin-
ning of another improves performance. The tran-
sition between one conversational activity and an-
other takes place over the course of several utter-
ances, rather than occurring at an instant in time.
Error analysis indicates further that discourse units
that correspond to conversational activities with
clear end points that can be achieved have a higher
probability of being recognized correctly.
References
John L. Austin. 1962. How to do Things with Words:
The William James Lectures delivered at Harvard
University in 1955. Clarendon Press, Oxford.
Satanjeev Banerjee and Alexander I. Rudnicky. 2006.
A texttiling based approach to topic boundary detec-
tion in meetings. Technical report, Department of
Computer Science, Carnegie Mellon University.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: semantics or
pragmatics? Linguistics and Philosophy, 9(1):37?
61.
Bradley Efron and Robert Tibshirani. 1997. Im-
provements on cross-validation: The .632+ boot-
235
strap method. Journal of the American Statistical
Association, 92(438):548?560, June.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?08), pages 334?
343. Association for Computational Linguistics.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse
segmentation of multi-party conversation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 562?569,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204, July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1):10?18.
Marti A. Hearst. 1997. Texttiling: segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1).
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19:501?530.
Jun Hu, Rebecca J. Passonneau, and Owen Rambow.
2009. Contrasting the interaction structure of an
email and a telephone corpus: A machine learning
approach to annotation of dialogue function units.
In Proceedings of the SIGDIAL 2009 Conference,
pages 357?366, London, UK, September. Associa-
tion for Computational Linguistics.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Igor Malioutov and Regina Barzilay. 2006. Min-
imum cut model for spoken lecture segmentation.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, ACL-44, pages 25?32, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593?
596.
Andrew Y. Ng. 1997. Preventing overfitting of
cross-validation data. In Proceedings of the Four-
teenth International Conference on Machine Learn-
ing, ICML ?97, pages 245?253.
Nicolas Obin, Volker Dellwo, Anne Lacheret, and
Xavier Rodet. 2010. Expectations for discourse
genre identification: aprosodic study. In Inter-
speech, pages 3070?3073.
A.S. Park and J.R. Glass. 2008. Unsupervised pattern
discovery in speech. Audio, Speech, and Language
Processing, IEEE Transactions on, 16(1):186?197,
Jan.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103?139,
March.
Rebecca J. Passonneau, Irene Alvarado, Phil Crone,
and Simon Jerome. 2011. Paradise-style evaluation
of a human-human library corpus. In Proceedings
of the SIGDIAL 2011 Conference, pages 325?331,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Matthew Purver, Thomas L. Griffiths, and Joshua B.
Kording, Konrad P. andTenenbaum. 2006. Unsu-
pervised topic modelling for multi-party spoken dis-
course. In Proceedings of the 44th annual meet-
ing of the Association for Computational Linguistics
(ACL-44), pages 17?24. Association for Computa-
tional Linguistics.
R. Bharat Rao and Glenn Fung. 2008. On the dangers
of cross-validation. an experimental evaluation. In
SDM, pages 588?596. SIAM.
Klaus Ries, Lori Levin, Liza Valle, Alon Lavie, and
Alex Waibel. 2000. Shallow discourse genre an-
notation in callhome spanish. In Proceedings of In-
ternational Conference on Language Resources and
Evaluation (LREC). European Language Resources
and Evaluation (ELRA).
Klaus Ries. 2002. Segmenting conversations by
topic, initiative, and style. In AnniR. Coden, EricW.
Brown, and Savitha Srinivasan, editors, Information
Retrieval Techniques for Speech Applications, vol-
ume 2273 of Lecture Notes in Computer Science,
pages 51?66. Springer Berlin Heidelberg.
J.D. Rodriguez, A. Perez, and J.A. Lozano. 2010. Sen-
sitivity analysis of k-fold cross validation in predic-
tion error estimation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 32(3):569?575,
March.
Zeno Vendler. 1957. Verbs and times. Philosophical
Review, 66(2):143?160.
Nigel G. Ward and Karen A. Richart-Ruiz. 2013. Pat-
terns of importance variation in spoken dialog. In
SigDial.
Nigel G. Ward and Alejandro Vega. 2012. A bottom-
up exploration of the dimensions of dialog state in
spoken interaction. In SigDial.
236
Nigel G. Ward and Steven D. Werner. 2013. Using
dialog-activity similarity for spoken information re-
trieval. In Interspeech.
237
