Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Incremental Feature Selection and `1 Regularization
for Relaxed Maximum-Entropy Modeling
Stefan Riezler and Alexander Vasserman
Palo Alto Research Center
3333 Coyote Hill Road, Palo Alto, CA 94304
Abstract
We present an approach to bounded constraint-
relaxation for entropy maximization that corre-
sponds to using a double-exponential prior or `1 reg-
ularizer in likelihood maximization for log-linear
models. We show that a combined incremental fea-
ture selection and regularization method can be es-
tablished for maximum entropy modeling by a nat-
ural incorporation of the regularizer into gradient-
based feature selection, following Perkins et al
(2003). This provides an efficient alternative to stan-
dard `1 regularization on the full feature set, and
a mathematical justification for thresholding tech-
niques used in likelihood-based feature selection.
Also, we motivate an extension to n-best feature
selection for linguistic features sets with moderate
redundancy, and present experimental results show-
ing its advantage over `0, 1-best `1, `2 regularization
and over standard incremental feature selection for
the task of maximum-entropy parsing.1
1 Introduction
The maximum-entropy (ME) principle, which pre-
scribes choosing the model that maximizes the en-
tropy out of all models that satisfy given feature
constraints, can be seen as a built-in regularization
mechanism that avoids overfitting the training data.
However, it is only a weak regularizer that cannot
avoid overfitting in situations where the number of
training examples is significantly smaller than the
number of features. In such situations some fea-
tures will occur zero times on the training set and
receive negative infinity weights, causing the as-
signment of zero probabilities for inputs including
those features. Similar assignment of (negative) in-
finity weights happens to features that are pseudo-
minimal (or pseudo-maximal) on the training set
(see Johnson et al (1999)), that is, features whose
value on correct parses always is less (or greater)
1This research has been funded in part by contract
MDA904-03-C-0404 of the Advanced Research and Develop-
ment Activity, Novel Intelligence from Massive Data program.
than or equal to their value on all other parses. Also,
if large features sets are generated automatically
from conjunctions of simple feature tests, many fea-
tures will be redundant. Besides overfitting, large
feature sets also create the problem of increased
time and space complexity.
Common techniques to deal with these problems
are regularization and feature selection. For ME
models, the use of an `2 regularizer, corresponding
to imposing a Gaussian prior on the parameter val-
ues, has been proposed by Johnson et al (1999) and
Chen and Rosenfeld (1999). Feature selection for
ME models has commonly used simple frequency-
based cut-off, or likelihood-based feature induction
as introduced by Della Pietra et al (1997). Whereas
`2 regularization produces excellent generalization
performance and effectively avoids numerical prob-
lems, parameter values almost never decrease to
zero, leaving the problem of inefficient computa-
tion with the full feature set. In contrast, feature se-
lection methods effectively decrease computational
complexity by selecting a fraction of the feature
set for computation; however, generalization per-
formance suffers from the ad-hoc character of hard
thresholds on feature counts or likelihood gains.
Tibshirani (1996) proposed a technique based on
`1 regularization that embeds feature selection into
regularization such that both a precise assessment of
the reliability of features and the decision about in-
clusion or deletion of features can be done in the
same framework. Feature sparsity is produced by
the polyhedral structure of the `1 norm which ex-
hibits a gradient discontinuity at zero that tends to
force a subset of parameter values to be exactly
zero at the optimum. Since this discontinuity makes
optimization a hard numerical problem, standard
gradient-based techniques for estimation cannot be
applied directly. Tibshirani (1996) presents a spe-
cialized optimization algorithm for `1 regularization
for linear least-squares regression called the Lasso
algorithm. Goodman (2003) and Kazama and Tsujii
(2003) employ standard iterative scaling and con-
jugate gradient techniques, however, for regulariza-
tion a simplified one-sided exponential prior is em-
ployed which is non-zero only for non-negative pa-
rameter values. In these approaches the full fea-
ture space is considered in estimation, so savings
in computational complexity are gained only in ap-
plications of the resulting sparse models. Perkins
et al (2003) presented an approach that combines
`1 based regularization with incremental feature se-
lection. Their basic idea is to start with a model in
which almost all weights are zero, and iteratively
decide, by comparing regularized feature gradients,
which weight should be adjusted away from zero
in order to decrease the regularized objective func-
tion by the maximum amount. The `1 regularizer is
thus used directly for incremental feature selection,
which on the one hand makes feature selection fast,
and on the other hand avoids numerical problems
for zero-valued weights since only non-zero weights
are included in the model. Besides the experimental
evidence presented in these papers, recently a theo-
retical account on the superior sample complexity of
`1 over `2 regularization has been presented by Ng
(2004), showing logarithmic versus linear growth in
the number of irrelevant features for `1 versus `2
regularized logistic regression.
In this paper, we apply `1 regularization to log-
linear models, and motivate our approach in terms
of maximum entropy estimation subject to relaxed
constraints. We apply the gradient-based feature se-
lection technique of Perkins et al (2003) to our
framework, and improve its computational com-
plexity by an n-best feature inclusion technique.
This extension is tailored to linguistically motivated
feature sets where the number of irrelevant features
is moderate. In experiments on real-world data from
maximum-entropy parsing, we show the advantage
of n-best `1 regularization over `2, `1, `0 regulariza-
tion and standard incremental feature selection in
terms of better computational complexity and im-
proved generalization performance.
2 `p Regularizers for Log-Linear Models
Let p?(x|y) = e
?n
i=1 ?ifi(x,y)
?
x e
?n
i=1 ?ifi(x,y)
be a conditional
log-linear model defined by feature functions f and
log-parameters ?. For data {(xj , yj)}mj=1, the objec-
tive function to be minimized in `p regularization of
the negative log-likelihood L(?) is
C(?) = L(?) + ?p(?)
= ?
1
m
m?
j=1
ln p?(xj |yj) + ????
p
p
The regularizer family ?p(?) is defined by the
Minkowski `p norm of the parameter vector ?
raised to the pth power, i.e. ???pp =
?n
i=1 |?i|
p
.
The essence of this regularizer family is to penalize
overly large parameter values. If p = 2, the regu-
larizer corresponds to a zero-mean Gaussian prior
distribution on the parameters with ? corresponding
to the inverse variance of the Gaussian. If p = 0,
the regularizer is equivalent to setting a limit on the
maximum number of non-zero weights. In our ex-
periments we replace `0 regularization by the re-
lated technique of frequency-based feature cutoff.
`1 regularization is defined by the case where
p = 1. Here parameters are penalized in the sum
of their absolute values, which corresponds to ap-
plying a zero-mean Laplacian or double exponential
prior distribution of the form
p(?i) =
1
2?
e?
|?i|
?
with ? = 1? being proportional to the inverse stan-
dard deviation
?
2? . In contrast to the Gaussian, the
Laplacian prior puts more mass near zero (and in
the tails), thus tightening the prior by decreasing
the standard deviation ? provides stronger regular-
ization against overfitting and produces more zero-
valued parameter estimates. In terms of `1-norm
regularization, feature sparsity can be explained by
the following observation: Since every non-zero pa-
rameter weight incurs a regularizer penalty of ?|?i|,
its contribution to minimizing the negative log-
likelihood has to outweigh this penalty. Thus param-
eters values where the gradient at ? = 0 is
?
?
?
?
?L(?)
??i
?
?
?
? ? ? (1)
can be kept zero without changing the optimality of
the solution.
3 Bounded Constraint Relaxation for
Maximum Entropy Estimation
As shown by Lebanon and Lafferty (2001), in terms
of convex duality, a regularization term for the dual
problem corresponds to a ?potential? on the con-
straint values in the primal problem. For a dual
problem of regularized likelihood estimation for
log-linear models, the corresponding primal prob-
lem is a maximum entropy problem subject to re-
laxed constraints. Let H(p) denote the entropy with
respect to probability function p, and g : IRn ? IR
be a convex potential function, and p?[?] and p[?] be
expectations with respect to the empirical distribu-
tion p?(x, y) = 1m
?m
j=1 ?(xj , x)?(yj , y) and the
model distribution p(x|y)p?(y). The primal problem
can then be stated as
Maximize H(p)? g(c) subject to
p[fi]? p?[fi] = ci, i = 1, . . . , n
Constraint relaxation is achieved in that equality of
the feature expectations is not enforced, but a certain
amount of overshooting or undershooting is allowed
by a parameter vector c ? IRn whose potential is de-
termined by a convex function g(c) that is combined
with the entropy term H(p).
In the case of `2 regularization, the potential func-
tion for the primal problem is a quadratic penalty
of the form 12?
?
i c
2
i for ? = 1?2i , i = 1, . . . , n(Lebanon and Lafferty, 2001). In order to recover
the specific form of the primal problem for our case,
we have to start from the given dual problem. Fol-
lowing Lebanon and Lafferty (2001), the dual func-
tion for regularized estimation can be expressed in
terms of the dual function ?(p?,?) for the unregu-
larized case and the convex conjugate g?(?) of the
potential function g(c). In our case the negative of
?(p?,?) corresponds to the likelihood term L(?),
and the negative of the convex conjugate g?(?) is
the `1 regularizer. Thus our dual problem can be
stated as
?? = argmax
?
?(p?,?)? g
?(?)
= argmin
?
L(?) + ????11
Since for convex and closed functions, the con-
jugate of the conjugate is the original function, i.e.
g?? = g (Boyd and Vandenberghe, 2004), the poten-
tial function g(c) for the primal problem can be re-
covered by calculating the conjugate g?? of the con-
jugate g?(?) = ????11. In our case, we get
g??(c) = g(c) =
{
0 ?c?? ? ?
? otherwise (2)
where ?c?? = max{|c1|, . . . , |cn|}. A proof for
this proposition is given in the Appendix. The re-
sulting potential function g(c) is the indicator func-
tion on the interval [??, ?]. That is, it restricts the
allowable amount of constraint relaxation to at most
??. From this perspective, increasing ? means to al-
low for more slack in constraint satisfaction, which
in turn allows to fit a more uniform, less overfit-
ting distribution to the data. For features that are in-
cluded in the model, the parameter values have to be
adjusted away from zero to meet the constraints
|p[fi]? p?[fi]| ? ?, i = 1, . . . , n (3)
Initialization: Initialize selected features S to ?, and
zero-weighted features Z to the full feature set,
yielding the uniform distribution p?(0),S(0) .
n-best grafting: For steps t = 1, . . . , T ,
(1) for all features fi in Z(t?1), calculate
?
?
?
?
?
?L(?(t?1), S(t?1))
??i
?
?
?
?
?
> ?,
(2) S(t) := S(t?1) ?N (t) and Z(t) := Z(t?1) \
N (t) where N (t) is the set of n-best features
passing the test in (1),
(3) perform conjugate gradient optimization to
find the optimal model p??,S(t) where ? is
initialized at ?(t?1), and ?(t) := ?? =
argmax
?
C(?, S(t)).
Stopping condition: Stop if for all fi in Z(t?1):
?
?
?
?
?
?L(?(t?1), S(t?1))
??i
?
?
?
?
?
? ?
Figure 1: n-best gradient feature testing
For features that meet the constraints without pa-
rameter adjustment, parameter values can be kept at
zero, effectively discarding the features. Note that
equality of equations 3 and 1 connects the maxi-
mum entropy problem to likelihood regularization.
4 Standardization
Note that the ?p regularizer presented above penal-
izes the model parameters uniformly, correspond-
ing to imposing a uniform variance onto all model
parameters. This motivates a normalization of in-
put data to the same scale. A standard technique
to achieve this is to linearly rescale each feature
count to zero mean and standard deviation of one
over all training data. The same rescaling has to be
done for training and application of the model to un-
seen data. As we will see in the experimental evalua-
tion presented below, a standardization of input data
can also dramatically improve convergence behav-
ior in unregularized optimization . Furthermore, pa-
rameter values estimated from standardized feature
counts are directly interpretable to humans. Com-
bined with feature selection, interpretable parame-
ter weights are particularly useful for error analysis
of the model?s feature design.
5 Incremental n-best Feature Selection
The basic idea of the ?grafting? (for ?gradient fea-
ture testing?) algorithm presented by (Perkins et al,
2003) is to assume a tendency of `1 regularization
to produce a large number of zero-valued param-
eters at the function?s optimum, thus to start with
all-zero weights, and incrementally add features to
the model only if adjusting their parameter weights
away from zero sufficiently decreases the optimiza-
tion criterion. This idea allows for efficient, incre-
mental feature selection, and at the same time avoids
numerical problems caused by the discontinuity of
the gradient in `1 regularization. Furthermore, the
regularizer is incorporated directly into a criterion
for feature selection, based on the observation made
above: It only makes sense to add a feature to the
model if the regularizer penalty is outweighed by
the reduction in negative log-likelihood. Thus fea-
tures considered for selection have to pass the fol-
lowing test:
?
?
?
?
?L(?)
??i
?
?
?
? > ?
In the grafting procedure suggested by (Perkins
et al, 2003), this gradient test is applied to each fea-
ture, and at each step the feature passing the test
with maximum magnitude is added to the model.
Adding one feature at a time effectively discards
noisy and irrelevant features, however, the overhead
introduced by grafting can outweigh the gain in ef-
ficiency if there is a moderate number of noisy and
truly redundant features. In such cases, it is bene-
ficial to add a number of n > 1 features at each
step, where n is adjusted by cross-validation or on a
held-out data set. In the experiments on maximum-
entropy parsing presented below, a feature set of lin-
guistically motivated features is used that exhibits
only a moderate amount of redundancy. We will see
that for such cases, n-best feature selection consid-
erably improves computational complexity, and also
achieves slightly better generalization performance.
After adding n ? 1 features to the model in
a grafting step, the model is optimized with re-
spect to all parameters corresponding to currently
included features. This optimization is done by call-
ing a gradient-based general purpose optimization
routine for the regularized objective function. We
use a conjugate gradient routine for this purpose
(Minka, 2001; Malouf, 2002)2. The gradient of our
criterion with respect to a parameter ?i is:
?C(?)
??i
=
1
m
m?
k=1
?L(?)
??i
+ ? sign(?i)
2Note that despite gradient feature testing, the parameters
for some features can be driven to zero in conjugate gradient
optimization of the `1-regularized objective function. Care has
to be taken to catch those features and prune them explicitly to
avoid numerical instability.
The sign of ?i decides if ? is added or subtracted
from the gradient for feature fi. For a feature that
is newly added to the model and thus has weight
?i = 0, we use the feature gradient test to determine
the sign. If ?L(?)??i > ?, we know that
?C(?)
??i
> 0,
thus we let sign(?i) = ?1 in order to decrease C.
Following the same rationale, if ?L(?)??i < ?? we
set sign(?i) = +1. An outline of an n-best grafting
algorithm is given in Fig. 1.
6 Experiments
6.1 Train and Test Data
In the experiments presented in this paper, we eval-
uate `2, `1, and `0 regularization on the task of
stochastic parsing with maximum-entropy models
For our experiments, we used a stochastic parsing
system for LFG that we trained on section 02-21
of the UPenn Wall Street Journal treebank (Mar-
cus et al, 1993) by discriminative estimation of a
conditional maximum-entropy model from partially
labeled data (see Riezler et al (2002)). For esti-
mation and best-parse searching, efficient dynamic-
programming techniques over features forests are
employed (see Kaplan et al (2004)). For the setup
of discriminative estimation from partially labeled
data, we found that a restriction of the training data
to sentences with a relatively low ambiguity rate
was possible at no loss in accuracy compared to
training from all sentences. Furthermore, data were
restricted to sentences of which a discriminative
learner can possibly take advantage, i.e. sentences
where the set of parses assigned to the labeled string
is a proper subset of the parses assigned to the un-
labeled string. Together with a restriction to exam-
ples that could be parsed by the full grammar and
did not have to use a backoff mechanism of frag-
ment parses, this resulted in a training set of 10,000
examples with at most 100 parses. Evaluation was
done on the PARC 700 dependency bank3, which
is an LFG annotation of 700 examples randomly
extracted from section 23 of the UPenn WSJ tree-
bank. To tune regularization parameters, we split the
PARC 700 into a heldout and test set of equal size.
6.2 Feature Construction
Table 1 shows the 11 feature templates that were
used in our experiments to create 60, 109 features.
On the around 300,000 parses for 10,000 sentences
in our final training set, 10, 986 features were active,
resulting in a matrix of active features times parses
that has 66 million non-zero entries. The scale of
this experiment is comparable to experiments where
3http://www2.parc.com/istl/groups/nltt/fsbank/
Table 1: Feature templates
name parameters activation condition
Local Templates
cs label label constituent label is present in parse
cs adj label parent label, constituent child label is
child label child of constituent parent label
cs right branch constituent has right child
cs conj nonpar depth non-parallel conjuncts within depth levels
fs attrs attrs f-structure attribute is one of attrs
fs attr value attr, value attribute attr has value value
fs attr subsets attr sum of cardinalities of subsets of attr
lex subcat pred, args sets verb pred has one of args sets as arguments
Non-Local (Top-Down) Templates
cs embedded label, size chain of size constituents
labeled label embedded into one another
cs sub label ancestor label, constituent descendant label
descendant label is descendant of ancestor label
fs aunt subattr aunts, parents, one of descendants is descendant of one of
descendants parents which is a sister of one of aunts
much larger, but sparser feature sets are employed4.
The reason why the matrix of non-zeroes is less
sparse in our case is that most of our feature tem-
plates are instantiated to linguistically motivated
cases, and only a few feature templates encode all
possible conjunctions of simple feature tests. Re-
dundant features are introduced mostly by the lat-
ter templates, whereas the former features are gen-
eralizations over possible combinations of grammar
constants. We conjecture that feature sets like this
are typical for natural language applications.
Efficient feature detection is achieved by a com-
bination of hashing and dynamic programming on
the packed representation of c- and f-structures
(Maxwell and Kaplan, 1993). Features can be de-
scribed as local and non-local, depending on the size
of the graph that has to be traversed in their compu-
tation. For each local template one of the parame-
ters is selected as a key for hashing. Non-local fea-
tures are treated as two (or more) local sub-features.
Packed structures are traversed depth-first, visiting
each node only once. Only the features keyed on
the label of the current node are considered for
matching. For each non-local feature, the contexts
of matching subfeatures are stored at the respective
nodes, propagated upward in dynamic programing
fashion, and conjoined with contexts of other sub-
features of the feature. Fully matched features are
associated with the corresponding contexts resulting
in a feature-annotated and/or-forest. This annotated
4For example, Malouf (2002) reports a matrix of non-zeroes
that has 55 million entries for a shallow parsing experiment
where 260,000 features were employed.
and/or forest is exploited for dynamic programming
computation in estimation and best parse selection.
6.3 Experimental Results
Table 2 shows the results of an evaluation of five
different systems of the test split of the PARC 700
dependency bank. The presented systems are unreg-
ularized maximum-likelihood estimation of a log-
linear model including the full feature set (mle),
standardized maximum-likelihood estimation as de-
scribed in Sect. 4 (std), `0 regularization using
frequency-based cutoff, `1 regularization using n-
best grafting, and `2 regularization using a Gaus-
sian prior. All `p regularization runs use a standard-
ization of the feature space. Special regularization
parameters were adjusted on the heldout split, re-
sulting in a cutoff threshold of 16, and penaliza-
tion factors of 20 and 100 for `1 and `2 regular-
ization respectively, with an optimal choice of 100
features to be added in each n-best grafting step.
Performance of these systems is evaluated firstly
with respect to F-score on matching dependency re-
lations. Note that the F-score values on the PARC
700 dependency bank range between a lower bound
of 68.0% for averaging over all parses and an upper
bound of 83.6% for the parses producing the best
possible matches. Furthermore, compression of the
full feature set by feature selection, number of con-
jugate gradient iterations, and computation time (in
hours:minutes of elapsed time) are reported.5
5All experiments were run on one CPU of a dual processor
AMD Opteron 244 with 1.8GHz clock speed and 4GB of main
memory.
Table 2: F-score, compression, number of iterations,
and elapsed time for unregularized and standardized
maximum-likelihood estimation, and `0, `1, and `2
regularization on test split of PARC 700 dependency
bank.
mle std `0 `2 `1
F-score 77.9 78.1 78.1 78.9 79.3
compr. 0 0 18.4 0 82.7
cg its. 761 371 372 34 226
time 129:12 66:41 60:47 6:19 5:25
Unregularized maximum-likelihood estimation
using the full feature set exhibits severe overtraining
problems, as the relation of F-score to the number
of conjugate gradient iterations shows. Standard-
ization of input data can alleviate this problem by
improving convergence behavior to half the num-
ber of conjugate gradient iterations. `0 regulariza-
tion achieves its maximum on the heldout data for a
threshold of 16, which results in an estimation run
that is slightly faster than standardized estimation
using all features, due to a compression of the full
feature set by 18%. `2 regularization benefits from
a very tight prior (standard deviation of 0.1 corre-
sponding to penalty 100) that was chosen on the
heldout set. Despite the fact that no reduction of the
full feature set is achieved, this estimation run in-
creases the F-score to 78.9% and improves compu-
tation time by a factor of 20 compared to unregular-
ized estimation using all features. `1 regularization
for n-best grafting, however, even improves upon
this result by increasing the F-score to 79.3%, fur-
ther decreasing computation time to 5:25 hours, at a
compression of the full feature set of 83%.
77.5
78
78.5
79
79.5
1 10 100 1000 10000
10
100
1000
F-score Num CG Iterations
Features Added At Each Step
F-score
3
3
3
3
3
3
3
Num CG Iterations
+
+
+
+
+
+
+
Figure 2: n-best grafting with n of features added
at each step plotted against F-score on test set and
conjugate gradient iterations.
As shown in Fig. 2, for feature selection from lin-
guistically motivated feature sets with only a mod-
erate amount of truly redundant features, it is crucial
to choose the right number n of features to be added
in each grafting step. The number of conjugate gra-
dient iterations decreases rapidly in the number of
features added at each step, whereas F-score evalu-
ated on the test set does not decrease (or increases
slightly) until more than 100 features are added in
each step. 100-best grafting thus reduces estimation
time by a factor of 10 at no loss in F-score compared
to 1-best grafting. Further increasing n results in a
significant drop in F-score, while smaller n is com-
putationally expensive, and also shows slight over-
training effects.
Table 3: F-score, compression, number of itera-
tions, and elapsed time for gradient-based incre-
mental feature selection without regularization, and
with `2, and `1 regularization on test split of PARC
700 dependency bank.
mle-ifs `2-ifs `1
F-score 78.8 79.1 79.3
compr. 88.1 81.7 82.7
cg its. 310 274 226
time 6:04 6:56 5:25
In another experiment we tried to assess the rel-
ative contribution of regularization and incremental
feature selection to the `1-grafting technique. Re-
sults of this experiments are shown in Table 3. In
this experiment we applied incremental feature se-
lection using the gradient test described above to un-
regularized maximum-likelihood estimation (mle-
ifs) and `2-regularized maximum-likelihood estima-
tion (`2-ifs). Threshold parameters ? are adjusted
on the heldout set, in addition to and independent
of regularization parameters such as the variance
of the Gaussian prior. Results are compared to `1-
regularized grafting as presented above. For all runs
a number of 100 features to be added in each graft-
ing step is chosen. The best result for the mle-ifs run
is achieved at a threshold of 25, yielding an F-score
of 78.8%. This shows that incremental feature se-
lection is a powerful tool to avoid overfitting. A fur-
ther improvement in F-score to 79.1% is achieved
by combining incremental feature selection with the
`2 regularizer at a variance of 0.1 for the Gaussian
prior and a threshold of 15. Both runs provide ex-
cellent compression rates and convergence times.
However, they are still outperformed by the `1 run
that achieves a slight improvement in F-score to
79.3% and a slightly better runtime. Furthermore,
by integrating regularization naturally into thresh-
olding for feature selection, a separate thresholding
parameter is avoided in `1-based incremental fea-
ture selection.
A theoretical account of the savings in com-
putational complexity that can be achieved by n-
best grafting can be given as follows. Perkins et
al. (2003) assess the computational complexity for
standard gradient-based optimization with the full
feature set by ? cmp2? , for a multiple c of p line
minimizations for p derivatives over m data points,
each of which has cost ? . In contrast, for grafting,
the cost is assessed by adding up the costs for fea-
ture testing and optimization for s grafting steps as
? (msp+13cms
3)? . For n-best grafting as proposed
in this paper, the number of steps can be decom-
posed into s = n ? t for n features added at each
of t steps. This results in a cost of ? mtp for fea-
ture testing, and ? 13cmn
2t3? for optimization. If
we assume that t  n  s, this indicates consid-
erable savings compared to both 1-best grafting and
standard gradient-based optimization.
7 Discussion and Conclusion
A related approach to `1 regularization and
constraint-relaxation for maximum-entropy mod-
eling has been presented by Kazama and Tsujii
(2003). In this approach, constraint relaxation is
done by allowing two-sided inequality constraints
?Bi ? p?[fi]? p[fi] ? Ai, Ai, Bi > 0
in entropy maximization. The dual function is the
regularized likelihood function
1
m
m?
j=1
p???(xj |yj)?
n?
i=1
?iAi ?
n?
i=1
?iBi
where the two parameter vectors ? and ? replace
our parameter vector ?, and ?i, ?i ? 0. This reg-
ularizer corresponds to a simplification of double-
sided exponentials to a one-sided exponential dis-
tribution which is non-zero only for non-negative
parameters. The use of one-sided exponential pri-
ors for log-linear models has also been proposed
by Goodman (2003), however, without a motiva-
tion in a maximum entropy framework. The fact that
Kazama and Tsujii (2003) allow for lower and up-
per bounds of different size requires the parameter
space to be doubled in their approach. Furthermore,
similar to Goodman (2003), the requirement to work
with a one-sided strictly positive exponential dis-
tribution makes it necessary to double the feature
space to account for (dis)preferences in terms of
strictly positive parameter values. These are consid-
erable computational and implementational disad-
vantages of these approaches. More importantly, an
integration of `1 regularization into incremental fea-
ture selection was not considered.
Incremental feature selection has been proposed
firstly by Della Pietra et al (1997) in a likelihood-
based framework. In this approach, an approximate
gain in likelihood for adding a feature to the model
is used as feature selection criterion, and thresholds
on this gain are used as stopping criterion. Maxi-
mization of approximate likelihood gains and gra-
dient feature testing both are greedy approxima-
tions to the true gain in the objective function -
grafting can be seen as applying one iteration of
Newton?s method, where the weight of the newly
added feature is initialized at 0, to calculate the ap-
proximate likelihood gain. Efficiency and accuracy
of both approaches are comparable, however, the
grafting framework provides a well-defined mathe-
matical basis for feature selection and optimization
by incorporating selection thresholds naturally as
penalty factors of the regularizer. The idea of adding
n-best features in each selection step also has been
investigated earlier in the likelihood-based frame-
work (see for example McCallum (2003)). How-
ever, the possible improvements in computational
complexity and generalization performance due to
n-best selection were not addressed explicitly. Fur-
ther improvements of efficiency of grafting are pos-
sible by applying Zhou et al?s (2003) technique of
restricting feature selection in each step to the top-
ranked features from previous stages.
In sum, we presented an application of `1 regu-
larization to likelihood maximization for log-linear
models that has a simple interpretation as bounded
constraint relaxation in terms of maximum entropy
estimation. The presented n-best grafting method
does not require specialized algorithms or simplifi-
cations of the prior, but allows for an efficient, math-
ematically well-defined combination of feature se-
lection and regularization. In an experimental eval-
uation, we showed n-best grafting to outperform `0,
1-best `1, `2 regularization and standard incremen-
tal feature selection in terms of computational effi-
ciency and generalization performance.
References
Stephen Boyd and Lieven Vandenberghe. 2004.
Convex Optimization. Cambridge University
Press.
Stanley F. Chen and Ronald Rosenfeld. 1999.
A gaussian prior for smoothing maximum en-
tropy models. Technical Report CMU-CS-99-
108, Carnegie Mellon University, Pittsburgh, PA.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random
fields. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 19(4):380?393.
Joshua Goodman. 2003. Exponential priors
for maximum entropy models. Unpublished
Manuscript, Microsoft Research, Redmont, WA.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?99),
College Park, MD.
Ronald M. Kaplan, Stefan Riezler, Tracy H. King,
John T. Maxwell III, and Alexander Vasserman.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the Human
Language Technology conference / North Ameri-
can chapter of the Association for Computational
Linguistics annual meeting (HLT/NAACL?04),
Boston, MA.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Eval-
uation and extension of maximum entropy mod-
els with inequality constraints. In Proceedings of
EMNLP?03, Sapporo, Japan.
Guy Lebanon and John Lafferty. 2001. Boosting
and maximum likelihood for exponential models.
In Advances in Neural Information Processing 14
(NIPS?01), Vancouver.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
Proceedings of Computational Natural Language
Learning (CoNLL?02), Taipei, Taiwan.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of english: The Penn
treebank. Computational Linguistics, 19(2):313?
330.
John Maxwell and Ron Kaplan. 1993. The inter-
face between phrasal and functional constraints.
Computational Linguistics, 19(4):571?589.
Andrew McCallum. 2003. Efficiently inducing fea-
tures of conditional random fields. In Proceed-
ings of the 19th Conference on Uncertainty in Ar-
tificial Intelligence (UAI?03), Acapulco, Mexico.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of
Statistics, Carnegie Mellon University.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2
regularization, and rotational invariance. In Pro-
ceedings of the 21st International Conference on
Machine Learning, Banff, Canada.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremetal feature selection
by gradient descent in function space. Machine
Learning, 3:1333?1356.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and dis-
criminative estimation techniques. In Proceed-
ings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?02),
Philadelphia, PA.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. Journal of the Royal Sta-
tistical Society. Series B, 58(1):267?288.
Yaqian Zhou, Fuliang Weng, Lide Wu, and Hauke
Schmidt. 2003. A fast algorithm for feature se-
lection in conditional maximum entropy mod-
eling. In Proceedings of EMNLP?03, Sapporo,
Japan.
Appendix: Proof of Proposition 2
Following Boyd and Vandenberghe (2004), the con-
vex conjugate of function g : IRn ? IR is
g?(w) = sup
u
{
n?
i=1
wiui ? g(u)}
and the dual norm ? ? ?? of norm ? ? ? on IRn is
?w?? = sup
u
{
n?
i=1
wiui| ?u? ? 1} (4)
and the dual norm of the `1 norm is the `? norm
?w?? = ?w?? for ?u? = ?u?11 (5)
We show that the convex conjugate of
g(u) = ??u?11, for ? > 0
is g?(w) =
{
0 ?w?? ? ?
? otherwise
Proof. Let ?w?? ? ?, then
?
i wiui ?
?u?11?w?? (from 4 and 5)? ?u?11? (since ?w?? ?
?). Then ?i wiui ? ?u?11? ? 0 and u = 0 maxi-
mizes it with maximum value g?(w) = 0.
Let ?w?? > ?, then ?z s.t. ?z?11 ? 1 and?
i wizi > ? (from 4 and 5). For u = tz, let t ?
?, then
?
i wiui???u?
1
1 = t(
?
i wizi???z?
1
1) ?
? (since?i wizi? ??z?11 > 0), thus g?(w) = ?.
Identifying Chemical Names in Biomedical Text: 
An Investigation of the Substring Co-occurrence Based Approaches
Abstract 
We investigate various strategies for finding 
chemicals in biomedical text using substring 
co-occurrence information.  The goal is to 
build a system from readily available data 
with minimal human involvement.  Our 
models are trained from a dictionary of 
chemical names and general biomedical text. 
We investigated several strategies including 
Na?ve Bayes classifiers and several types of 
N-gram models.  We introduced a new way of 
interpolating N-grams that does not require 
tuning any parameters.  We also found the 
task to be similar to Language Identification. 
1 Introduction 
Chemical names recognition is one of the first tasks 
needed for building an information extraction system in 
the biomedical domain. Chemicals, especially organic 
chemicals, are one of the main agents in many processes 
and relationships such a system would need to find.  In 
this work, we investigate a number of approaches to the 
problem of chemical names identification.  We focus on 
approaches that use string internal information for 
classification, those based on the character co-
occurrence statistics within the strings that we would 
like to classify.  We would also like not to spend much 
time and effort to do manual annotation, and hence use 
readily publicly available data for training all the 
models.  Because of that, we would be satisfied with 
only moderate results. In the course of this 
investigation, we have found that N-gram methods work 
best given these restrictions on the models.   
Work has been done on a related task of named 
entity recognition (Bikel et al, 1999, Riloff, 1996, 
Cucerzan, 1999, and others).  The aim of the named 
entity task is usually set to find names of people, 
organizations, and some other similar entities in text.  
Adding features based on the internal substring patterns 
has been found useful by Cucerzan et al, 1999.  For 
finding chemicals, internal substring patterns are even 
more important source of information.  Many substrings 
of chemical names are very characteristic.  For example, 
seeing "methyl" as a substring of a word is a strong 
indicator of a chemical name.  The systematic chemical 
names are constructed from substrings like that, but 
even the generic names follow certain conventions, and 
have many characteristic substrings.   
In this work, character co-occurrence patterns are 
extracted from available lists of chemicals that have 
been compiled for other purposes.  We built models 
based on the difference between strings occurring in 
chemical names and strings that occur in other words.  
The use of only string internal information prevents us 
from disambiguating different word senses, but we 
accept this source of errors as a minor one. 
Classification based solely on string internal 
information makes the chemical names recognition task 
similar to language identification. In the language 
identification task, these patterns are used to detect 
strings from a different language embedded into text.  
Because chemicals are so different, we can view them 
as a different language, and borrow some of the 
Language Identification techniques.  Danning, 1994 was 
able to achieve good results using character N-gram 
models on language identification even on short strings 
(20 symbols long).  This suggests that his approach 
might be successful in chemical names identification 
setting. 
N-gram based methods were previously used for 
chemicals recognition. Wilbur et al, 1999 used all 
substrings of a fixed length N, but they combined the 
training counts in a Bayesian framework, ignoring non-
independence of overlapping substring.  They claimed 
good performance for their data, but this approach 
showed significantly lower performance than 
alternatives on our data.  See the results section for 
Alexander Vasserman 
Department of Computer and  
Information Science 
University of Pennsylvania 
Philadelphia, PA 19104 
avasserm@seas.upenn.edu 
more details.  The difference is that their data is 
carefully constructed to contain only chemicals and 
chemicals of all types in the test data, i.e. their training 
and testing data is in a very close correspondence. 
We on the other hand tried to use readily available 
chemical lists without putting much manual labor into 
their construction.  Most of our training data comes 
from a single source - National Cancer Institute website 
- and hence represents only a very specific domain of 
chemicals, while testing data is coming from a random 
sample from MEDLINE.  In addition, these lists were 
designed for use by human, and hence contain many 
comments and descriptions that are not easily separable 
for the chemical names themselves.  Several attempts on 
cleaning these out have been made. Most aggressive 
attempts deleted about half the text from the list.  While 
deleting many useful names, this improved the results 
significantly. 
While we found that N-grams worked best amoung 
the approaches we have tried, other approaches are also 
possible.  We did not explore the possibility of using 
substring as features to a generic classification 
algorithm, such as, for example, support  vector 
machines (Burges, 1998). 
2 Available Data 
In order to train a statistical model for recognizing 
chemicals a list of about 240 thousands entries have 
been download from National Cancer Institute website 
(freely available at dtp.nci.nih.gov).  Entries are unique 
names of about 45 thousands unique chemicals.  Each 
entry includes a name of a chemical possibly followed 
by alternative references and some comments.  This 
additional information had to be deleted in order to 
compute statistics from chemical names only.  While 
there were no clean separators between chemical names 
and the additional materials, several patterns were 
designed to clean up the list. Applying those patterns 
shrunk each entry on average by half.  This cleaning 
step has not produced perfect results in both leaving 
some unusable material in and deleting some useful 
strings, yet it improved the performance of all methods 
dramatically.  Cleaning the list by hand might have 
produced better results, but it would require more 
expertise and take a lot of time and would contradict the 
goal of building the system from readily available data. 
We used text from MEDLINE abstracts to model 
general biomedical language.  These were available as a 
part of the MEDLINE database of bibliographical 
records for papers in biomedical domain.  Records that 
had non-empty abstracts have been extracted.  From 
those 'title' and 'abstract' fields were taken and cleaned 
off from remaining XML tags. 
Both the list of chemical names (LCN) and the text 
corpus obtained from the MED LINE database (MED) 
were tokenized by splitting on the white spaces.  White 
space tokenization was used over other possible 
approaches, as the problem of tokenization is very hard 
for chemical names, because they contain a lot of 
internal punctuation.  We also wanted to avoid splitting 
chemical names into tokens that are too small, as they 
would contain very little internal information to work 
with.  The counts of occurrences of tokens in LCN and 
MD were used in all experiments to build models of 
chemical names and general biomedical text. 
 In addition, 15 abstracts containing chemical names 
were selected from the parts of MEDLINE corpus not 
used for the creation of the above list.  These abstracts 
have been annotated by hand and used as development 
and test sets. 
3 Classification Using Substring 
Importance Criteria 
3.1 Classification Approach 
Most obvious approach to this problem is to try to 
match the chemicals in the list against the text and label 
only the matches, i.e. chemicals that are known from the 
list.  This approach is similar to the memory-based 
baseline described by Palmer et al, 1997, where instead 
of using precompiled list they memorized all the entries 
that occurred in a training text. 
A natural extension of matching is a decision list.  
Each classification rule in the list checks if a substring is 
present in a token.  Matching can be viewed as just an 
extreme of this approach, where the strings selected into 
the decision list are the complete tokens from the LCN 
(including token boundary information).  Using other 
substrings increases recall, as non-exact matches are 
detected, and it also improves precision, as it decreases 
the number of error coming from noise in LCN. 
While decision list performs better than matching, its 
performance is still unsatisfactory.  Selecting only 
highly indicative substrings results in high precision, but 
very low recall.  Lowering the thresholds and taking 
more substrings decreases the precision without 
improving the recall much until the precision gets very 
low. 
The decision list approach makes each decision 
based on a single substring.  This forces us to select 
only substrings that are extreemly rare outside the 
chemical names. This in turn results in extremely low 
recall. An alternative would be to combine the 
information from multiple substrings into a single 
decision using Naive Bayes framework.  This would 
keep precision from dropping as dramatically when we 
increase the number of strings used in classification.  
We would like to estimate the probability of a token 
being a part of a chemical name given the token (string) 
p(c|s) .  Representing each string as a set of its substrings 
we need to estimate p(c|s1...sn).  Using Bayes Rule, we 
get 
)...ss|c)p(c)/p(...sp(s)...sp(c|s n1n1n1 =  (1) 
Assuming independence of substrings s1...sn and 
conditional independence of substrings s1...sn given c, 
we can rewrite: 
??
==
=
=
n
1i
i
n
1i
i
n1n1n1
)p(s/|c)p(sp(c) 
)...s|c)/p(s...sp(c)p(s)...sp(c|s
 (2) 
Now notice that for most applications we would like 
to be able to vary precision/recall tradeoff by setting 
some threshold t and classifying each string s a s  a 
chemical only if 
t)p(s/|c)p(sp(c) scp
n
1i
i
n
1i
i >= ??
==
)|(  (3) 
or 
'/ tp(c)t)p(s/|c)p(s
n
1i
i
n
1i
i =>??
==
  (4) 
This allows us to avoid estimation of p(c) 
(estimating p(c) is hard without any labeled text). We 
can estimate p(si|c) and p(si) from the LCN and MED 
respectively as 
tokens)(/#) containg tokens(#)( ii ssp =  (5) 
3.2 Substring Selection 
For this approach, we need to decide what set of 
substring {si} of s to use to represent s.  We would like 
to select a set of non-overlapping substrings to make the 
independence assumption more grounded (while it is 
clear that even non-overlapping substrings are not 
independent, assuming independence of overlapping 
substrings clearly causes major problems).  In order to 
do this we need some measure of usefulness of 
substrings.  We would like to select substrings that are 
both informative and reliable as features, i.e. the 
substrings fraction of which in LCN is different from 
the fraction of them in MED and which occur often 
enough in LCN.  Once this measure is defined, we can 
use dynamic programming algorithm similar to Viterbi 
decoding to select the set of non-overlapping substrings 
with maximum value. 
Kullback-Leibler divergence based measure  
If we view the substring frequencies as a distribution, 
we can ask the question which substrings account for 
the biggest contribution to Kullback-Leibler divergence 
(Cover et al 1991) between distribution given by LCN 
and that given by MED.  From this view it is reasonable 
to take p(si|c)*log(p(si|c)/p(si)) as a measure of value of 
a substring.  Therefore, the selection criterion would be 
tspcspcsp iii >))(/)|(log()|(   (6) 
where t is some threshold value.  Notice that this 
measure combines frequency of a substring in chemicals 
and the difference between frequencies of occurrences 
of the substring in chemicals and non-chemicals. 
A problem with this approach arises when either 
p(si|c) or p(si) is equal to zero.  In this case, this 
selection criterion cannot be computed, yet some of the 
most valuable strings could have p(si) equal to zero.  
Therefore, we need to smooth probabilities of the 
strings to avoid zero values.  One possibility is to 
include all strings si, such that p(si)=0 and p(si|c)>t', 
where t'<t is some new threshold needed to avoid 
selecting very rare strings.  It would be nice though not 
to introduce an additional parameter.  An alternative 
would be to reassign probabilities to all substrings and 
keep the selection criterion the same.  It could be done, 
for example, using Good-Turing smoothing (Good 
1953). 
Selection by significance testing 
A different way of viewing this is to say that we want to 
select all the substrings in which we are confident.  It 
can be observed that tokens might contain certain 
substrings that are strong indicators of them being 
chemicals.  Useful substrings are the ones that predict 
significantly different from the prior probability of 
being a chemical.  I.e. if the frequency of chemicals 
among all tokens is f(c), then s is a useful substring if 
the frequency of chemicals among tokens containing s 
f(c|s)  is significantly different from f(c).  We test the 
significance by assuming that f(c) is a good estimate for 
the prior probability of a token being a chemical p(c), 
and trying to reject the null hypothesis, that actual 
probability of chemicals among tokens that contain s is 
also p(c).  If the number of tokens containing s is n(s) 
and the number of chemicals containing s is c(s) , then 
the selection criterion becomes 
95.65.1))(1)(()(
)()()( z
cfcfsn
cfsnsc =>
-
-   (7) 
This formula is obtained by viewing occurrences of s 
as Bernoulli trials with probability p(c)  of the 
occurrence being a chemical and probability (1-p(c))  of 
the occurrence being non-chemical.  Distribution 
obtained by n(s) such trials can be approximated with 
the normal distribution with mean n(s)p(c) and variance 
n(s)p(c)(1-p(c)). 
4 Classification Using N-gram Models 
W e  can estimate probability of a string given class 
(chemical or non-chemical) as the probability of letters 
of the string based on a finite history.  
)()...|(
),...|(
)(/)()|()|(
01
01
cpsssp
csssp
SpcpcSpScp
i
ii
i
ii
?
?
-
-
=
=
   (8) 
where  S is the string to be classified and si are the 
letters of S. 
The N-gram approach has been a successful 
modeling technique in many other applications.  It has a 
number of advantages over the Bayesian approach.  In 
this framework we can use information from all 
substrings of a token, and not only sets of non-
overlapping ones.  There is no (incorrect) independence 
assumption, so we get a more sound probability model.  
As a practical issue, there has been a lot of work done 
on smoothing techniques for N-gram models (Chen et 
al., 1998), so it is easier to use them. 
4.1 Investigating Usefulness of Different N-gram 
Lengths 
As the first task in investigating N-gram models, we 
investigated usefulness of N-grams of different length.  
For each n, we constructed a model based on the 
substrings of this length only using Laplacian 
smoothing to avoid zero probability. 
Bn
nsssp
Bn
ncsssp
i
Ni
i
Ni
ii
i
Nic
i
Nic
ii
d
d
d
d
+
+?
+
+?
-
+-
+-
-
-
+-
+-
-
1
1
1
01
1
1
1
01
)...|(
),...|(
  (9) 
where N is the length of the N-grams, nii-N+1 and ncii-N+1 
are the number of occurrences of N-gram sisi-1...si-N-1 in 
MEDLINE and chemical list respectively, d  is the 
smoothing parameter, and B is the number of different 
N-grams of length N. 
The smoothing parameter was tuned for each n 
individually using the development data (hand 
annotated MEDLINE abstracts).  The results of these 
experiments showed that 3-grams and 4-grams are most 
useful.  While poor performance by longer N-grams was 
somewhat surprising, results indicated that overtraining 
might be an issue for longer N-grams, as the model they 
produce models the training data more precisely.  While 
unexpected, the result is similar to the conclusion in 
Dunning '94 for language identification task. 
4.2 Interpolated N-gram Models 
In many different tasks that use N-gram models, 
interpolated or back-off models have been proven 
useful.  The idea here is to use shorter N-grams for 
smoothing longer ones. 
m
n
n
n
n
n
sssp
m
n
n
n
n
n
csssp
i
i
i
Ni
i
Ni
Ni
Ni
i
Ni
N
ii
c
i
ic
i
Nic
i
Nic
Ni
Nic
i
Nic
N
ii
11
2
2
11
1
1
01
11
2
2
11
1
1
01
...
)...|(
...
),...|(
lll
lll
+++?
+++?
-
+-
+-
--
+-
+-
-
-
+-
+-
--
+-
+-
-
       (10) 
where lj's are the interpolation coefficients, m and mc 
are the total number of letters in MEDLINE and 
chemical list respectively.  lj can generally depend on 
si-1...si-N+1 , with the only constraint that all l j 
coefficients sum up to one.  One of the main question 
for interpolated models is learning the values for l's.  
Estimating N different l's for each context si-1...si-N+1 is 
a hard learning task by itself that requires a lot of 
development data.  There are two fundamentally 
different ways for dealing with this problem.  Often 
grouping different coefficients together and providing 
single value for each group, or imposing some other 
constraints on the coefficients is used to decrease the 
number of parameters.  The other approach is providing 
a theory for values of l's without tuning them on the 
development data (This is similar in spirit to Minimal 
Description Length approach).  We have investigated 
several different possibilities in both of these two 
approaches. 
4.3 Computing Interpolation Coefficients: Fixed 
Coefficients 
Equation (10) can be rewritten in a slightly different 
form:  
?
?
?
?
?
?
?
?
?
?
?
?
?
?
???
?
???
?
???
?
???
? +-
+-
+
-?
-
-
-
-
+-
+-
-
-
-
+-
+-
--
m
n
n
n
n
n
n
nsssp
i
i
i
i
i
i
N
i
Ni
i
Ni
N
N
i
ni
i
Ni
Nii
11
1
1
12
2
2
2
1
1
1
1
101
)1(...
)1(
)1()...|(
lll
l
l
l
        (11) 
This form states more explicitly that each N-gram 
model is smoothed by all lower models.  An extreme of 
the grouping approach is then to make all lj's equal, 
and tune this single parameter on the development data. 
4.4 Computing Interpolation Coefficients: 
Context Independent Coefficients 
Relaxing this constraint and going back to the original 
form of equation (10), we can make all lj's independent 
of their context, so we get only N parameters to tune.  
When N is small, this can be done even with relatively 
small development set.  We can do this by exploring all 
possible settings of these parameters in an N 
dimensional grid with small increment.  For larger N  we 
have to introduce an additional constraint that l j's 
should lie on some function of j with a smaller number 
of parameters.  We have used a quadratic function (2 
parameters, as one of them is fixed by the constraint that 
all lj's have to sum up to 1).  Using higher order of the 
function gives more flexibility, but introduces more 
parameters, which would require more development 
data to tune well.  The quadratic function seems to be a 
good trade off that provides enough flexibility, but does 
not introduce too many parameters. 
4.5 Computing Interpolation Coefficients: 
Confidence Based Coefficients 
The intuition for using interpolated models is that higher 
level N-grams give more information when they are 
reliable, but lower level N-grams are usually more 
reliable, as they normally occur more frequently.  We 
can formalize this intuition by computing the 
confidence of higher level N-grams and weight them 
proportionally.  We are trying to estimate p(si|si-1...si-
N+1) with the ratio nii-N+1 /ni-1i-N+1.  We can say that our 
observation in the training data was generated by ni-1i-
N+1 Bernoulli trials with outcomes either si or any other 
letter.  We consider si to be a positive outcome and any 
other letter would be a negative outcome.  Given this 
model we have nii-N+1 positive outcomes in ni-1i-N+1 
Bernoulli trials with probability of positive outcome 
p(si|si-1...si-N+1).  This means that the estimate given by 
nii-N+1 /ni-1i-N+1 has the confidence interval of binomial 
distribution approximated by normal given by 
)(2 2
4223
a
aa
zcc
zczc
I
+
+=                 (12) 
where c = ni-1i-N+1 . 
Since the true probability is within I of the estimate, 
the lower level models should not change the estimate 
given by the highest-level model by more than I.  This 
means that lN-1 in the equation (11) should be equal to 
I.  By recursing the argument we get 
)(2 2
4223
a
aal
zcc
zczc
I
jj
jj
jj +
+
==                (13) 
where cj = ni-1i-j+2  for j > 1 , and c1 = m. 
5 Evaluation and Results 
We performed cross validation experiments on 15 hand-
annotated MEDLINE abstracts described in section 
"Available Data".  Experiments were done by holding 
out each abstract, tuning model parameters on 14 
remaining abstracts, and testing on the held out one.  
Fifteen such experiments were performed.  The results 
of these experiments were combined by taking weighed 
geometric mean of precision results at each recall level.  
The results were weighted according to the number of 
positive examples in each file to ensure equal 
contribution from each example.  Figure 1 shows the 
resulting precision/recall curves. 
As we can see, the  N-gram approaches perform 
better than the other ones.  The interpolated model with 
quadratic coefficients needs a lot of development data, 
so it does not produce good results in our case.  Simple 
Laplacian smoothing needs less development data and 
produces much better results.  The model with 
confidence based coefficients works best.  The graph 
also shows the model introduced by Wilbur et al, 1999.  
It does not perform nearly as well on our data, even 
though it produces very good results on clean data they 
have used.  This (as well as some experiments we 
performed that have not been included into this work) 
suggests that quality of the training data has very strong 
effect on the model results. 
6 Conclusions and Future Work 
We have investigated a number of different approaches 
to chemical identification using string internal 
information.  We used readily available training data, 
and a small amount of human annotated text that was 
used primarily for testing.  We were able to achieve 
good performance on general biomedical text taken 
from MEDLINE abstracts.  N-gram models showed the 
best performance.  The specific details of parameter 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.
05
0.
10
0.
15
0.
20
0.
25
0.
30
0.
35
0.
40
0.
45
0.
50
0.
55
0.
60
0.
65
0.
70
0.
75
0.
80
0.
85
0.
90
0.
95
1.
00
Recall
P
r
e
c
i
s
i
o
n
Naive Bayes
Wilbur et al
Laplacian Smoothing
Quadratic Coefficients
Confidence-Based Coefficients
Fig. 1. Precision/Recall curves for Na?ve 
Bayes and N-gram based models  
tuning for these models produced small variations in the 
results.  We have also introduced a method for 
computing interpolated N-gram model parameters 
without any tuning on development data.  The results 
produced by this method were slightly better than those 
of other approaches.  We believe this approach 
performed better because only one parameter - the 
length of N-grams - needed to be tuned on the 
development data.  This is a big advantage when little 
development data is available.  In general, we 
discovered many similarities with previous work on 
language identification, which suggests that other 
techniques introduced for language identification may 
carry over well into chemicals identification. 
As a short term goal we would like to determine N-
gram interpolation coeficients by usefulness of the 
corresponding context for discrimination.  This would 
incorporate the same techinque as we used for Naive 
Bayes system, hopefully combining the advantage of 
both approaches  
There are other alternatives for learning a 
classification rule.  Recently using support vector 
machines (Burges 1998) have been a popular approach.  
More traditionally decision trees (Breiman et al 1984) 
have been used for simmilar tasks.  It would be 
interesting to try these aproaches for our task and 
compare them with Naive Bayes and N-gram 
approaches discussed here. 
One limitation of the current system is that it does 
not find the boundaries of chemicals, but only classifies 
predetermind tokens as being part of a chemical name 
or not.  The system can be improved by removing prior 
tokenization requirment, and attempting to identify 
chemical name boundaries based on the learned 
information. 
 In this work we explored just one dimention of 
possible features usefull for finding chemical names. 
We intent to incorporate other types of features 
including context based features with this work. 
References 
T. Dunning.  1994.  "Statistical identification of 
language".  Technical Report MCCS 94-273, New 
Mexico State University. 
S. F. Chen and J. Goodman.  1998.  ?An Empirical 
Study of Smoothing Techniques for Language 
Modeling,? TR-10-98, Computer Science Group, 
Harvard Univ., 1998. 
W. John Wilbur, George F. Hazard, Jr., Guy Divita, 
James G. Mork, Alan R. Aronson, Allen C. Browne.  
1999.  "Analysis of Biomedical Text for Chemical 
Names: A Comparison of Three Methods".  
Proceedings of AMIA Symposium 1999:181-5. 
Daniel M. Bikel, Richard Schwartz and Ralph M. 
Weischedel.  1999.  "An Algorithm that Learns 
What's in a Name", Machine Learning 
Ellen Riloff.  1996.  "Automatically Generating 
Extraction Patterns from Untagged Text", 
Proceedings of the Thirteenth National Conference 
on Artificial Intelligence (AAAI-96), pp. 1044-1049 
Silviu Cucerzan, David Yarowsky.  1999.  "Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence".  
Proceedings of 1999 Joint SIGDAT conference on 
EMNLP and VLC, University of Maryland, MD. 
D. D. Palmer, D. S. Day.  1997.  "A Statistical Profile of 
Named Entity Task". Proceedings of Fifth ACL 
Conference for Applied Natural Language Processing 
(ANLP -97), Washington D.C. 
I. Good.  1953.  "The population frequencies of species 
and the estimation of population parameters". 
Biometrika, v. 40, pp. 237-264 
C.J.C. Burges, 1998.  "A Tutorial on Support Vector 
Machines for Pattern Recognition," Data Mining and 
Knowledge Discovery, 2(2), pp. 955-974 
T. Cover and J. Thomas, 1991.  ?Elements of 
Information Theory?, Wiley, New York. 
 
L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. 
Stone, 1984.  "Classification and Regression Trees," 
Chapman & Hall, New York. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 737?744
Manchester, August 2008
Translating Queries into Snippets for Improved Query Expansion
Stefan Riezler and Yi Liu and Alexander Vasserman
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
{riezler,yliu,avasserm}@google.com
Abstract
User logs of search engines have recently
been applied successfully to improve var-
ious aspects of web search quality. In this
paper, we will apply pairs of user queries
and snippets of clicked results to train a
machine translation model to bridge the
?lexical gap? between query and document
space. We show that the combination of
a query-to-snippet translation model with
a large n-gram language model trained
on queries achieves improved contextual
query expansion compared to a system
based on term correlations.
1 Introduction
In recent years, user logs of search engines have at-
tracted considerable attention in research on query
clustering, query suggestions, query expansion, or
general web search. Besides the sheer size of these
data sets, the main attraction of user logs lies in
the possibility to capitalize on users? input, either
in form of user-generated query reformulations, or
in form of user clicks on presented search results.
However noisy, sparse, incomplete, and volatile
these data may be, recent research has presented
impressive results that are based on simply taking
the majority vote of user clicks as a signal for the
relevance of results.
In this paper we will apply user logs to the prob-
lem of the ?word mismatch? or ?lexical chasm?
(Berger et al, 2000) between user queries and
documents. The standard solution to this prob-
lem, query expansion, attempts to overcome this
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mismatch in query and document vocabularies by
adding terms with similar statistical properties to
those in the original query. This will increase the
chances of matching words in relevant documents
and also decrease the ambiguity of the overall
query that is inherent to natural language. A suc-
cessful approach to this problem is local feed-
back, or pseudo-relevance feedback (Xu and Croft,
1996), where expansion terms are extracted from
the top-most documents that were retrieved in an
initial retrieval round. Because of irrelevant results
in the initial retrieval, caused by ambiguous terms
or retrieval errors, this technique may cause expan-
sion by unrelated terms, leading to query drift. Fur-
thermore, the requirement of two retrieval steps is
computationally expensive.
Several approaches have been presented that de-
ploy user query logs to remedy these problems.
One set of approaches focuses on user reformu-
lations of queries that differ only in one segment
(Jones et al, 2006; Fonseca et al, 2005; Huang
et al, 2003). Such segments are then identified
as candidate expansion terms, and filtered by var-
ious signals such as cooccurrence in similar ses-
sions or log-likelihood ratio of original and ex-
pansion phrases. Other approaches focus on the
relation of queries and retrieval results, either by
deploying the graph induced by queries and user
clicks in calculating query similarity (Beeferman
and Berger, 2000; Wen et al, 2002; Baeza-Yates
and Tiberi, 2007), or by leveraging top results from
past queries to provide greater context in find-
ing related queries (Raghavan and Sever, 1995;
Fitzpatrick and Dent, 1997; Sahami and Heilman,
2006). Cui et al (2002) present an all together dif-
ferent way to deploy user clickthrough data by ex-
tracting expansion terms directly from clicked re-
sults. They claim significant improvements over
737
the local feedback technique of Xu and Croft
(1996).
Cui et al?s (2002) work is the closest to ours.
We follow their approach in extracting expansion
terms directly from clicked results, however, with a
focus on high precision of query expansion. While
expansion from the domain of document terms has
the advantage that expansion terms are guaranteed
to be in the search domain, expansion precision
may suffer from the noisy and indirect ?approval?
of retrieval results by user clicks. Thus expansion
terms from the document domain are more likely
to be generalizations, specifications, or otherwise
related terms, than terms extracted from query sub-
stitutions that resemble synonyms more closely.
Furthermore, if the model that learns to correlate
document terms to query terms is required to ig-
nore context in order to generalize, finding appro-
priate expansions for ambiguous query terms is
difficult.
Our approach is to look at the ?word mismatch?
problem as a problem of translating from a source
language of queries into a target language of docu-
ments, represented as snippets. Since both queries
and snippets are arguably natural language, sta-
tistical machine translation technology (SMT) is
readily applicable to this task. In previous work,
this has been done successfully for question an-
swering tasks (Riezler et al, 2007; Soricut and
Brill, 2006; Echihabi and Marcu, 2003; Berger et
al., 2000), but not for web search in general. Cui et
al.?s (2002) model is to our knowledge the first to
deploy query-document relations for direct extrac-
tion of expansion terms for general web retrieval.
Our SMT approach has two main advantages over
Cui et al?s model: Firstly, Cui et al?s model re-
lates document terms to query terms by using sim-
ple term frequency counts in session data, with-
out considering smoothing techniques. Our ap-
proach deploys a sophisticated machine learning
approach to word alignment, including smooth-
ing techniques, to map query phrases to snippet
phrases. Secondly, Cui et al?s model only indi-
rectly uses context information to disambiguate
expansion terms. This is done by calculating the
relationship of an expansion term to the whole
query by multiplying its contributions to all query
terms. In our SMT approach, contextual disam-
biguation is done by deploying an n-gram lan-
guage model trained on queries to decide about the
appropriateness of an expansion term in the con-
text of the rest of the query terms. As shown in
an experimental evaluation, together the orthogo-
nal information sources of a translation model and
a language model provide significantly better con-
textual query expansion than Cui et al?s (2002)
correlation-based approach.
In the following, we recapitulate the essentials
of Cui et al?s (2002) model, and contrast it with
our SMT-based query expansion system. Further-
more, we will present a detailed comparison of the
two systems on a real-world query expansion task.
2 Query-Document Term Correlations
The query expansion model of Cui et al (2002)
is based on the principle that if queries containing
one term often lead to the selection of documents
containing another term, then a strong relationship
between the two terms is assumed. Query terms
and document terms are linked via clicked docu-
ments in user sessions. Formally, Cui et al (2002)
compute the following probability distribution of
document words w
d
given query words w
q
from
counts over clicked documents D:
P (w
d
|w
q
) =
?
D
P (w
d
|D)P (D|w
q
) (1)
The first term in the righthandside of equation 1 is
a normalized tfidf weight of the the document term
in the clicked document, and the second term is the
relative cooccurrence of document and query term
in sessions.
Since equation 1 calculates expansion probabil-
ities for each term separately, Cui et al (2002)
introduce the following cohesion formula that re-
spects the whole query Q by aggregating the ex-
pansion probabilities for each query term:
CoWeight
Q
(w
d
) = ln(
?
w
q
?Q
P (w
d
|w
q
) + 1) (2)
In contrast to local feedback techniques (Xu
and Croft, 1996), Cui et al?s algorithm allows to
precompute term correlations offline by collecting
counts from query logs. This reliance on pure fre-
quency counting is both a blessing and a curse: On
the one hand it allows for efficient non-iterative es-
timation, on the other hand it makes the implicit
assumption that data sparsity will be overcome by
counting from huge datasets. The only attempt at
smoothing that is made in this approach is a recur-
rence to words in query context, using equation 2,
when equation 1 assigns zero probability to unseen
pairs.
738
3 Query-Snippet Translation
The SMT system deployed in our approach is
an implementation of the alignment template ap-
proach of Och and Ney (Och and Ney, 2004). The
basic features of the model consist of a translation
model and a language model which go back to the
noisy channel formulation of machine translation
in Brown et al (1993). Their ?fundamental equa-
tion of machine translation? defines the job of a
translation system as finding the English string
?
e
that is a translation of a foreign string f such that
?
e = argmax
e
P (e|f)
= argmax
e
P (f |e)P (e) (3)
Equation 3 allows for a separation of a language
model P (e), and a translation model P (f |e). Och
and Ney (2004) reformulate equation 3 as a lin-
ear combination of feature functions h
m
(e, f) and
weights ?
m
, including feature functions for trans-
lation models h
i
(e, f) = P (f |e) and language
models h
j
(e) = P (e):
?
e = argmax
e
M
?
m=1
?
m
h
m
(e, f) (4)
The translation model used in our approach is
based on the sequence of alignment models de-
scribed in Och and Ney (2003). The relationship of
translation model and alignment model for source
language string f = f
J
1
and target string e = e
I
1
is via a hidden variable describing an alignment
mapping from source position j to target position
a
j
:
P (f
J
1
|e
I
1
) =
?
a
J
1
P (f
J
1
, a
J
1
|e
I
1
) (5)
The alignment a
J
1
contains so-called null-word
alignments a
j
= 0 that align source words to the
empty word. The different alignment models de-
scribed in Och and Ney (2003) each parameter-
ize equation 5 differently so as to capture differ-
ent properties of source and target mappings. All
models are based on estimating parameters ? by
maximizing the likelihood of training data con-
sisting of sentence-aligned, but not word-aligned
strings {(f
s
, e
s
) : s = 1, . . . , S}. Since each sen-
tence pair is linked by a hidden alignment variable
a = a
J
1
, the optimal
?
? is found using unlabeled-
data log-likelihood estimation techniques such as
the EM algorithm (Dempster et al, 1977):
?
? = argmax
?
S
?
s=1
?
a
p
?
(f
s
,a|e
s
) (6)
The final translation model is calculated from rel-
ative frequencies of phrases, i.e. consecutive se-
quences of words occurring in text. Phrases are
extracted via various heuristics as larger blocks of
aligned words from best word alignments, as de-
scribed in Och and Ney (2004).
Language modeling in our approach deploys an
n-gram language model that assigns the following
probability to a string w
L
1
of words (see Brants et
al. (2007)):
P (w
L
1
) =
L
?
i=1
P (w
i
|w
i?1
1
) (7)
?
L
?
i=1
P (w
i
|w
i?1
i?n+1
) (8)
Estimation of n-gram probabilities is done by
counting relative frequencies of n-grams in a cor-
pus of user queries. Remedies against sparse data
problems are achieved by various smoothing tech-
niques, as described in Brants et al (2007).
For applications of the system to translate un-
seen queries, a standard dynamic-programming
beam-search decoder (Och and Ney, 2004) that
tightly integrates translation model and language
model is used. Expansion terms are taken from
those terms in the 5-best translations of the query
that have not been seen in the original query string.
In our opinion, the advantages of using an
alignment-based translation model to correlate
document terms with query terms, instead of rely-
ing on a term frequency counts as in equation 1, are
as follows. The formalization of translation mod-
els as involving a hidden alignment variable allows
us to induce a probability distribution that assigns
some probability of being translated into a target
word to every source word. This is a crucial step
towards solving the problem of the ?lexical gap?
described above. Furthermore, various additional
smoothing techniques are employed in alignment
to avoid overfitting and improved coping with rare
words (see Och and Ney (2003)). Lastly, estima-
tion of hidden-variable models can be based on
the well-defined framework of statistical estima-
tion via the EM algorithm.
Similar arguments hold for the language model:
N-gram language modeling is a well-understood
739
sentence source target
pairs words words
tokens 3 billion 8 billion 25 billion
avg. length - 2.6 8.3
Table 1: Statistics of query-snippet training data
for translation model.
problem, with a host of well-proven smoothing
techniques to avoid data sparsity problems (see
Brants et al (2007).)
In combination, translation model and language
model provide orthogonal sources of information
to the overall translation quality. While the trans-
lation model induces a smooth probability distri-
bution that relates source to target words, the lan-
guage model deploys probabilities of target lan-
guage strings to assess the adequacy of a target
word as a translation in context. Reliance on or-
dering information of the words in the context of a
source word is a huge advantage over the bag-of-
words aggregation of context information in Cui et
al?s (2002) model. Furthermore, in the SMT model
used in our approach, translation model and lan-
guage model are efficiently integrated in a beam-
search decoder.
In our application of SMT to query expansion,
queries are considered as source language sen-
tences and snippets of clicked result documents
as target sentences. A parallel corpus of sentence-
aligned data is created by pairing each query with
each snippet of its clicked results. Further adjust-
ments to system parameters were applied in or-
der to adapt the training procedure to this special
data set. For example, in order to account for the
difference in sentence length between queries and
snippets, we set the null-word probability to 0.9.
This allows us to improve precision of alignment
of noisy data by concentrating the alignment to a
small number of key words. Furthermore, extrac-
tion of phrases in our approach is restricted to the
intersection of alignments from both translation di-
rections, thus favoring precision over recall also in
phrase extraction. The only major adjustment of
the language model to the special case of query-
snippet translation is the fact that we train our n-
gram model on queries taken from user logs, in-
stead of on standard English text.
1-grams 2-grams 3-grams
9 million 1.5 billion 5 billion
Table 2: Statistics of unique query n-grams in lan-
guage model.
items disagreements
w/ agreement included
# items 102 125
mean item score 0.333 0.279
95% conf. int. [0.216, 0.451] [0.176, 0.381]
Table 3: Comparison of SMT-based expan-
sion with correlation-based expansion on 7-point
Likert-type scale.
4 Experimental Evaluation
4.1 Data
The training data for the translation model and
the correlation-based model consist of pairs of
queries and snippets for clicked results taken from
anonymized query logs. Using snippets instead of
full documents makes iterative training feasible
and also reduces noise considerably. This parallel
corpus of query-snippet pairs is fed into a standard
SMT training pipeline (modulo the adjustments to
word and phrase alignment discussed above). The
parallel corpus consists of 3 billion query-snippet
pairs that are input to training of word and phrase
alignment models. The resulting phrase translation
table that builds the basis of the translation model
consists 700 million query-snippet phrase transla-
tions. A collection of data statistics for the training
data is shown in table 1.
The language model used in our experiment is a
trigram language model trained on English queries
in user logs. N-grams were cut off at a minimum
frequency of 4. Data statistics for resulting unique
n-grams are shown in table 2.
4.2 Experimental Comparison
Our experimental setup for query expansion de-
ploys a real-world search engine, google.com, for
a comparison of expansions from the SMT-based
system and the correlation-based system. The ex-
perimental evaluation was done as direct compari-
son of search results for queries where both exper-
imental systems suggested expansion terms. Since
expansions from both experimental systems are
done on top of the same underlying search engine,
this allows us to abstract away from interactions
with the underlying system. The queries used for
evaluation were extracted randomly from 3+ word
740
query SMT-based expansions corr-based expansions score
applying U.S. passport passport - visa applying - home -1.0
configure debian to use dhcp debian - linux configure - configuring -1.0
configure - install
how many episodes of 30 rock? episodes - season how many episodes - tv -0.83
episodes - series many episodes - wikipedia
lampasas county sheriff department department - office department - home -0.83
sheriff - office
weakerthans cat virtue chords chords - guitar cat - tabs -0.83
chords - lyrics chords - tabs
chords - tab
Henry VIII Menu Portland, Maine menu - restaurant portland - six 1.3
menu - restaurants menu - england
ladybug birthday parties parties - ideas ladybug - kids 1.3
parties - party
political cartoon calvin coolidge cartoon - cartoons political cartoon - encyclopedia 1.3
top ten dining, vancouver dining - restaurants dining vancouver - 10 1.3
international communication communication - communications international communication - college 1.3
in veterinary medicine communication - skills
Table 4: SMT-based versus correlation-based expansions with mean item score.
queries in user logs in order to allow the systems
to deploy context information for expansion.
In order to evaluate Cui et al?s (2002)
correlation-based system in this setup, we required
the system to assign expansion terms to particu-
lar query terms. This could be achieved by using
a linear interpolation of scores in equation 2 and
equation 1. Equation 1 thus introduces a prefer-
ence for a particular query term to the whole-query
score calculated by equation 2. Our reimplementa-
tion uses unigram and bigram phrases in queries
and expansions. Furthermore, we use Okapi BM25
instead of tfidf in the calculation of equation 1 (see
Robertson et al (1998)).
Query expansion for the SMT-based system is
done by extracting terms introduced in the 5-best
list of query translations as expansion terms for the
respective query terms.
The evaluation was performed by three in-
dependent raters. The raters were given task-
specific rating guidelines, and were shown queries
and 10-best search results from both systems,
anonymized, and presented randomly on left or
right sides. The raters? task was to evaluate the re-
sults on a 7-point Likert-type
1
scale, defined as:
-1.5: much worse
-1.0: worse
-0.5: slightly worse
1
Likert?s (1932) original rating system is a 5-point scale
using integer scores 1 through 5. Our system uses average
scores over three raters for each item, and uses a 7-point in-
stead of a 5-point scale. See Dawes (2008) on the compara-
bility of 5-, 7-, or 10-point scales.
0: about the same
0.5: slightly better
1.0: better
1.5: much better
Results on 125 queries where both systems sug-
gested expansion terms are shown in table 3. For
each query, rating scores are averaged over the
scores assigned by three raters. The overall mean
item score for a comparison of SMT-based ex-
pansion against correlation-based expansion was
0.333 for 102 items with rater agreement, and
0.279 for 125 items including rater disagreements.
All result differences are statistically significant.
Examples for SMT-based and correlation-based
expansions are given in table 4. The first five ex-
amples are losses for the SMT-based system. In
the first example, passport is replaced by the re-
lated, but not synonymous term visa in the SMT-
based expansion. The second example is a loss for
SMT-based expansion because of a replacement of
the specific term debian by the more general term
linux. The correlation-based expansions tv 30 rock
in the third example, lampasas county sheriff home
in the fourth example, and weakerthans tabs in the
fifth example directly hit the title of relevant web
pages, while the SMT-based expansion terms do
not improve retrieval results. However, even from
these negative examples it becomes apparent that
the SMT-based expansion terms are clearly related
to the query terms, and for a majority cases this
has a positive effect. Such examples are shown in
741
(herbs , herbs) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herb) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , remedies) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , medicine) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , supplements) ( for , for) ( chronic , chronic) ( constipation , constipation)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , cooking)
(herbs , herbs) ( for , for) ( cooking , cooking) ( mexican , mexican)
(herbs , herbs) ( for , for) ( mexican , mexican) ( cooking , food)
(mexican , mexican) ( herbs , herbs) ( for , for) ( cooking , cooking)
(herbs , spices) ( for , for) ( mexican , mexican) ( cooking , cooking)
Table 5: Unique 5-best phrase-level translations of queries herbs for chronic constipation and herbs for
mexican cooking.
query terms n-best expansions
herbs com treatment encyclopedia
chronic interpret treating com
constipation interpret treating com
herbs for medicinal support women
for chronic com gold encyclopedia
chronic constipation interpret treating
herbs cooks recipes com
mexican recipes com cooks
cooking cooks recipes com
herbs for medicinal women support
for mexican cooks com allrecipes
Table 6: Correlation-based expansions for queries herbs for chronic constipation and herbs for mexican
cooking.
the second set of expansions. SMT-based expan-
sions such as henry viii restaurant portland, maine,
or ladybug birthday ideas, or top ten restaurants,
vancouver achieve a change in retrieval results that
does not result in a query drift, but rather in im-
proved retrieval results. In contrast, the terms in-
troduced by the correlation-based system are either
only vaguely related or noise.
5 Discussion
We attribute the experimental result of a signif-
icant preference for SMT-based expansions over
correlation-based expansions to the fruitful com-
bination of translation model and language model
provided by the SMT system. The SMT approach
can be viewed as a combined system that proposes
candidate expansion via the translation model, and
filters them by the language model. Thus we may
find a certain amount of non-sensical expansion
candidates at the phrase translation level. This can
be seen from inspecting table 7 which shows the
most probable phrase translations that are applica-
ble to the queries herbs for chronic constipation
and herbs for mexican cooking. The phrase table
includes identity translations and closely related
terms as most probable translations for nearly ev-
ery phrase, however, it also clearly includes noisy
and non-related terms. More importantly, an ex-
traction of expansion terms from the phrase table
alone would not allow to choose the appropriate
term for the given query context. This can be at-
tained by combining the phrase translations with a
language model: As shown in table 5, the 5-best
translations of the full queries attain a proper dis-
ambiguation of the senses of herbs by replacing
the term by remedies, medicine, and supplements
for the first query, and with spices for the second
query. Expansion terms highlighted in bold face.
The fact that the most probable translation for
the whole query mostly is the identity translation
can be seen as a feature, not as a bug, of the SMT-
based approach: By the option to prefer identity
translations or word reorderings over translations
of source words, the SMT model effectively can
choose not to generate any expansion terms. This
will happen if none of the candidate phrase trans-
lations fit with high enough probability into the
context of the whole query, as assessed by the lan-
guage model.
In contrast to the SMT model, the correlation-
based model cannot fall back onto the ordering in-
formation of the language model, but aggregates
information for the whole query from a bag-of-
words of query terms. Table 6 shows the top three
742
correlation-based expansion terms assigned to uni-
grams and bigrams in the queries herbs for chronic
constipation and herbs for mexican cooking. Ex-
pansion terms are chosen by overall highest weight
and shown in bold face. Relevant expansion terms
such as treatment or recipes that would disam-
biguate the meaning of herbs are in fact proposed
by the correlation-based model, however, the cohe-
sion score also promotes terms such as interpret or
com as best whole-query expansions, thus leading
to query drift.
6 Conclusion
We presented an approach to contextual query ex-
pansion that deploys natural language technology
in form of statistical machine translation. The key
idea of our approach is to consider the problem
of the ?lexical gap? between queries and docu-
ments from a linguistic point of view, and at-
tempt to bridge this gap by translating from the
query language into the document language. Us-
ing search engine user logs, we could extract large
amounts of parallel data of queries and snippets
from clicked documents. These data were used to
train an alignment-based translation model, and
an n-gram based language model. The same data
were used to train a reimplementation of Cui
et al?s (2002) term-correlation based query ex-
pansion system. An experimental comparison of
the two systems showed a considerable prefer-
ence for SMT-based expansions over correlation-
based expansion. Our explanation for this result
is the fruitful combination of the orthogonal in-
formation sources from translation model and lan-
guage model. While in the SMT approach expan-
sion candidates proposed by the translation model
are effectively filtered by ordering information
on the query context from the language model,
the correlation-based approach resorts to an in-
ferior bag-of-word aggregation of scores for the
whole query. Furthermore, each component of the
SMT model takes great care to avoid sparse data
problems by various sophisticated smoothing tech-
niques. In contrast, the correlation-based model re-
lies on pure counts of term frequencies.
An interesting task for future work is to dis-
sect the contributions of translation model and
language model, for example, by combining a
correlation-based system with a language model
filter. The challenge here is a proper integration of
n-gram lookup into correlation-based expansion.
References
Baeza-Yates, Ricardo and Alessandro Tiberi. 2007.
Extracting semantic relations from query logs. In
Proceedings of the 13th ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining
(KDD?07), San Jose, CA.
Beeferman, Doug and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
Berger, Adam L., Rich Caruana, David Cohn, Dayne
Freitag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In Proceedings of SIGIR?00, Athens, Greece.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?07), Prague, Czech Re-
public.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
Cui, Hang, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In Proceedings of WWW 2002, Hon-
olulu, Hawaii.
Dawes, John. 2008. Do data characteristics change ac-
cording to the number of scale points used? An ex-
periment using 5-point, 7-point and 10-point scales.
International Journal of Market Research, 50(1):61?
77.
Dempster, A. P., N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, 39(B):1?38.
Echihabi, Abdessamad and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL?03),
Sapporo, Japan.
Fitzpatrick, Larry and Mei Dent. 1997. Automatic
feedback using past queries: Social searching? In
Proceedings of SIGIR?97, Philadelphia, PA.
Fonseca, Bruno M., Paulo Golgher, Bruno Possas,
Berthier Ribeiro-Neto, and Nivio Ziviani. 2005.
Concept-based interactive query expansion. In Pro-
ceedings of the 14th Conference on Information and
Knowledge Management (CIKM?05), Bremen, Ger-
many.
743
Huang, Chien-Kang, Lee-Feng Chien, and Yen-Jen
Oyang. 2003. Relevant term suggestion in interac-
tive web search based on contextual information in
query session logs. Journal of the American Society
for Information Science and Technology, 54(7):638?
649.
Jones, Rosie, Benjamin Rey, Omid Madani, and Wi-
ley Greiner. 2006. Generating query substitutions.
In Proceedings of the 15th International World Wide
Web conference (WWW?06), Edinburgh, Scotland.
Likert, Rensis. 1932. A technique for the measurement
of attitudes. Archives of Psychology, 140:5?55.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Raghavan, Vijay V. and Hayri Sever. 1995. On the
reuse of past optimal queries. In Proceedings of SI-
GIR?95, Seattle, WA.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL?07), Prague, Czech Republic.
Robertson, Stephen E., Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Sahami, Mehran and Timothy D. Heilman. 2006. A
web-based kernel function for measuring the sim-
ilarity of short text snippets. In Proceedings of
the 15th International World Wide Web conference
(WWW?06), Edinburgh, Scotland.
Soricut, Radu and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web
Information Retrieval, 9:191?206.
Wen, Ji-Rong, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans-
actions on Information Systems, 20(1):59?81.
Xu, Jinxi and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Pro-
ceedings of SIGIR?96, Zurich, Switzerland.
herbs herbs
herbal
medicinal
spices
supplements
remedies
herbs for herbs for
herbs
herbs and
with herbs
herbs for chronic herbs for chronic
and herbs for chronic
herbs for
for for
for chronic for chronic
chronic
of chronic
for chronic constipation for chronic constipation
chronic constipation
for constipation
chronic chronic
acute
patients
treatment
chronic constipation chronic constipation
of chronic constipation
with chronic constipation
constipation constipation
bowel
common
symptoms
for mexican for mexican
mexican
the mexican
of mexican
for mexican cooking mexican food
mexican food and
mexican glossary
mexican mexican
mexico
the mexican
mexican cooking mexican cooking
mexican food
mexican
cooking
cooking cooking
culinary
recipes
cook
food
recipe
Table 7: Phrase translations applicable to source
strings herbs for chronic constipation and herbs
for mexican cooking.
744
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464?471,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation for Query Expansion in Answer Retrieval
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu
Google Inc., 1600 Amphitheatre Parkway, Mountain View, CA 94043
{riezler|avasserm|ioannis|vibhu|yliu}@google.com
Abstract
We present an approach to query expan-
sion in answer retrieval that uses Statisti-
cal Machine Translation (SMT) techniques
to bridge the lexical gap between ques-
tions and answers. SMT-based query ex-
pansion is done by i) using a full-sentence
paraphraser to introduce synonyms in con-
text of the entire query, and ii) by trans-
lating query terms into answer terms us-
ing a full-sentence SMT model trained on
question-answer pairs. We evaluate these
global, context-aware query expansion tech-
niques on tfidf retrieval from 10 million
question-answer pairs extracted from FAQ
pages. Experimental results show that SMT-
based expansion improves retrieval perfor-
mance over local expansion and over re-
trieval without expansion.
1 Introduction
One of the fundamental problems in Question An-
swering (QA) has been recognized to be the ?lexi-
cal chasm? (Berger et al, 2000) between question
strings and answer strings. This problem is mani-
fested in a mismatch between question and answer
vocabularies, and is aggravated by the inherent am-
biguity of natural language. Several approaches have
been presented that apply natural language process-
ing technology to close this gap. For example, syn-
tactic information has been deployed to reformu-
late questions (Hermjakob et al, 2002) or to re-
place questions by syntactically similar ones (Lin
and Pantel, 2001); lexical ontologies such as Word-
net1 have been used to find synonyms for question
words (Burke et al, 1997; Hovy et al, 2000; Prager
et al, 2001; Harabagiu et al, 2001), and statisti-
cal machine translation (SMT) models trained on
question-answer pairs have been used to rank can-
didate answers according to their translation prob-
abilities (Berger et al, 2000; Echihabi and Marcu,
2003; Soricut and Brill, 2006). Information retrieval
(IR) is faced by a similar fundamental problem of
?term mismatch? between queries and documents.
A standard IR solution, query expansion, attempts to
increase the chances of matching words in relevant
documents by adding terms with similar statistical
properties to those in the original query (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
In this paper we will concentrate on the task of
answer retrieval from FAQ pages, i.e., an IR prob-
lem where user queries are matched against docu-
ments consisting of question-answer pairs found in
FAQ pages. Equivalently, this is a QA problem that
concentrates on finding answers given FAQ docu-
ments that are known to contain the answers. Our
approach to close the lexical gap in this setting at-
tempts to marry QA and IR technology by deploy-
ing SMT methods for query expansion in answer
retrieval. We present two approaches to SMT-based
query expansion, both of which are implemented in
the framework of phrase-based SMT (Och and Ney,
2004; Koehn et al, 2003).
Our first query expansion model trains an end-
to-end phrase-based SMT model on 10 million
question-answer pairs extracted from FAQ pages.
1http://wordnet.princeton.edu
464
The goal of this system is to learn lexical correla-
tions between words and phrases in questions and
answers, for example by allowing for multiple un-
aligned words in automatic word alignment, and dis-
regarding issues such as word order. The ability to
translate phrases instead of words and the use of a
large language model serve as rich context to make
precise decisions in the case of ambiguous transla-
tions. Query expansion is performed by adding con-
tent words that have not been seen in the original
query from the n-best translations of the query.
Our second query expansion model is based on
the use of SMT technology for full-sentence para-
phrasing. A phrase table of paraphrases is extracted
from bilingual phrase tables (Bannard and Callison-
Burch, 2005), and paraphrasing quality is improved
by additional discriminative training on manually
created paraphrases. This approach utilizes large
bilingual phrase tables as information source to ex-
tract a table of para-phrases. Synonyms for query
expansion are read off from the n-best paraphrases
of full queries instead of from paraphrases of sep-
arate words or phrases. This allows the model to
take advantage of the rich context of a large n-gram
language model when adding terms from the n-best
paraphrases to the original query.
In our experimental evaluation we deploy a
database of question-answer pairs extracted from
FAQ pages for both training a question-answer
translation model, and for a comparative evalua-
tion of different systems on the task of answer re-
trieval. Retrieval is based on the tfidf framework
of Jijkoun and de Rijke (2005), and query expan-
sion is done straightforwardly by adding expansion
terms to the query for a second retrieval cycle. We
compare our global, context-aware query expansion
techniques with Jijkoun and de Rijke?s (2005) tfidf
model for answer retrieval and a local query expan-
sion technique (Xu and Croft, 1996). Experimen-
tal results show a significant improvement of SMT-
based query expansion over both baselines.
2 Related Work
QA has approached the problem of the lexical gap
by various techniques for question reformulation,
including rule-based syntactic and semantic refor-
mulation patterns (Hermjakob et al, 2002), refor-
mulations based on shared dependency parses (Lin
and Pantel, 2001), or various uses of the Word-
Net ontology to close the lexical gap word-by-word
(Hovy et al, 2000; Prager et al, 2001; Harabagiu
et al, 2001). Another use of natural language pro-
cessing has been the deployment of SMT models on
question-answer pairs for (re)ranking candidate an-
swers which were either assumed to be contained
in FAQ pages (Berger et al, 2000) or retrieved by
baseline systems (Echihabi and Marcu, 2003; Sori-
cut and Brill, 2006).
IR has approached the term mismatch problem by
various approaches to query expansion (Voorhees,
1994; Qiu and Frei, 1993; Xu and Croft, 1996).
Inconclusive results have been reported for tech-
niques that expand query terms separately by adding
strongly related terms from an external thesaurus
such as WordNet (Voorhees, 1994). Significant
improvements in retrieval performance could be
achieved by global expansion techniques that com-
pute corpus-wide statistics and take the entire query,
or query concept (Qiu and Frei, 1993), into account,
or by local expansion techniques that select expan-
sion terms from the top ranked documents retrieved
by the original query (Xu and Croft, 1996).
A similar picture emerges for query expansion
in QA: Mixed results have been reported for word-
by-word expansion based on WordNet (Burke et
al., 1997; Hovy et al, 2000; Prager et al, 2001;
Harabagiu et al, 2001). Considerable improvements
have been reported for the use of the local context
analysis model of Xu and Croft (1996) in the QA
system of Ittycheriah et al (2001), or for the sys-
tems of Agichtein et al (2004) or Harabagiu and
Lacatusu (2004) that use FAQ data to learn how to
expand query terms by answer terms.
The SMT-based approaches presented in this pa-
per can be seen as global query expansion tech-
niques in that our question-answer translation model
uses the whole question-answer corpus as informa-
tion source, and our approach to paraphrasing de-
ploys large amounts of bilingual phrases as high-
coverage information source for synonym finding.
Furthermore, both approaches take the entire query
context into account when proposing to add new
terms to the original query. The approaches that
are closest to our models are the SMT approach of
Radev et al (2001) and the paraphrasing approach
465
web pages FAQ pages QA pairs
count 4 billion 795,483 10,568,160
Table 1: Corpus statistics of QA pair data
of Duboue and Chu-Carroll (2006). None of these
approaches defines the problem of the lexical gap
as a query expansion problem, and both approaches
use much simpler SMT models than our systems,
e.g., Radev et al (2001) neglect to use a language
model to aid disambiguation of translation choices,
and Duboue and Chu-Carroll (2006) use SMT as
black box altogether.
In sum, our approach differs from previous work
in QA and IR in the use SMT technology for query
expansion, and should be applicable in both areas
even though experimental results are only given for
the restricted domain of retrieval from FAQ pages.
3 Question-Answer Pairs from FAQ Pages
Large-scale collection of question-answer pairs has
been hampered in previous work by the small sizes
of publicly available FAQ collections or by restricted
access to retrieval results via public APIs of search
engines. Jijkoun and de Rijke (2005) nevertheless
managed to extract around 300,000 FAQ pages
and 2.8 million question-answer pairs by repeatedly
querying search engines with ?intitle:faq?
and ?inurl:faq?. Soricut and Brill (2006) could
deploy a proprietary URL collection of 1 billion
URLs to extract 2.3 million FAQ pages contain-
ing the uncased string ?faq? in the url string. The
extraction of question-answer pairs amounted to a
database of 1 million pairs in their experiment.
However, inspection of the publicly available Web-
FAQ collection provided by Jijkoun and de Rijke2
showed a great amount of noise in the retrieved
FAQ pages and question-answer pairs, and yet the
indexed question-answer pairs showed a serious re-
call problem in that no answer could be retrieved for
many well-formed queries. For our experiment, we
decided to prefer precision over recall and to attempt
a precision-oriented FAQ and question-answer pair
extraction that benefits the training of question-
answer translation models.
2http://ilps.science.uva.nl/Resources/WazDah/
As shown in Table 1, the FAQ pages used in our
experiment were extracted from a 4 billion page
subset of the web using the queries ?inurl:faq?
and ?inurl:faqs? to match the tokens ?faq? or
?faqs? in the urls. This extraction resulted in 2.6
million web pages (0.07% of the crawl). Since not
all those pages are actually FAQs, we manually la-
beled 1,000 of those pages to train an online passive-
aggressive classificier (Crammer et al, 2006) in a
10-fold cross validation setup. Training was done
using 20 feature functions on occurrences question
marks and key words in different fields of web
pages, and resulted in an F1 score of around 90%
for FAQ classification. Application of the classifier
to the extracted web pages resulted in a classification
of 795,483 pages as FAQ pages.
The extraction of question-answer pairs from this
database of FAQ pages was performed again in a
precision-oriented manner. The goal of this step
was to extract url, title, question, and answers fields
from the question-answer pairs in FAQ pages. This
was achieved by using feature functions on punc-
tuations, HTML tags (e.g., <p>, <BR>), listing
markers (e.g., Q:, (1)), and lexical cues (e.g.,
What, How), and an algorithm similar to Joachims
(2003) to propagate initial labels across similar text
pieces. The result of this extraction step is a database
of about 10 million question answer pairs (13.3
pairs per FAQ page). A manual evaluation of 100
documents, containing 1,303 question-answer pairs,
achieved a precision of 98% and a recall of 82% for
extracting question-answer pairs.
4 SMT-Based Query Expansion
Our SMT-based query expansion techniques are
based on a recent implementation of the phrase-
based SMT framework (Koehn et al, 2003; Och and
Ney, 2004). The probability of translating a foreign
sentence f into English e is defined in the noisy chan-
nel model as
argmax
e
p(e|f) = argmax
e
p(f|e)p(e) (1)
This allows for a separation of a language model
p(e), and a translation model p(f|e). Translation
probabilities are calculated from relative frequencies
of phrases, which are extracted via various heuris-
tics as larger blocks of aligned words from best word
466
alignments. Word alignments are estimated by mod-
els similar to Brown et al (1993). For a sequence of
I phrases, the translation probability in equation (1)
can be decomposed into
p(f Ii |e
I
i ) =
I?
i=1
p(fi|ei) (2)
Recent SMT models have shown significant im-
provements in translation quality by improved mod-
eling of local word order and idiomatic expressions
through the use of phrases, and by the deployment
of large n-gram language models to model fluency
and lexical choice.
4.1 Question-Answer Translation
Our first approach to query expansion treats the
questions and answers in the question-answer cor-
pus as two distinct languages. That is, the 10 million
question-answer pairs extracted from FAQ pages are
fed as parallel training data into an SMT training
pipeline. This training procedure includes various
standard procedures such as preprocessing, sentence
and chunk alignment, word alignment, and phrase
extraction. The goal of question-answer translation
is to learn associations between question words and
synonymous answer words, rather than the trans-
lation of questions into fluent answers. Thus we
did not conduct discriminative training of feature
weights for translation probabilities or language
model probabilities, but we held out 4,000 question-
answer pairs for manual development and testing of
the system. For example, the system was adjusted
to account for the difference in sentence length be-
tween questions and answers by setting the null-
word probability parameter in word alignment to
0.9. This allowed us to concentrate the word align-
ments to a small number of key words. Furthermore,
extraction of phrases was based on the intersection
of alignments from both translation directions, thus
favoring precision over recall also in phrase align-
ment.
Table 2 shows unique translations of the query
?how to live with cat allergies? on the phrase-level,
with corresponding source and target phrases shown
in brackets. Expansion terms are taken from phrase
terms that have not been seen in the original query,
and are highlighted in bold face.
4.2 SMT-Based Paraphrasing
Our SMT-based paraphrasing system is based on the
approach presented in Bannard and Callison-Burch
(2005). The central idea in this approach is to iden-
tify paraphrases or synonyms at the phrase level by
pivoting on another language. For example, given
a table of Chinese-to-English phrase translations,
phrasal synonyms in the target language are defined
as those English phrases that are aligned to the same
Chinese source phrases. Translation probabilities for
extracted para-phrases can be inferred from bilin-
gual translation probabilities as follows: Given an
English para-phrase pair (trg, syn), the probability
p(syn|trg) that trg translates into syn is defined
as the joint probability that the English phrase trg
translates into the foreign phrase src, and that the
foreign phrase src translates into the English phrase
syn. Under an independence assumption of those
two events, this probability and the reverse transla-
tion direction p(trg|syn) can be defined as follows:
p(syn|trg) = max
src
p(src|trg)p(syn|src) (3)
p(trg|syn) = max
src
p(src|syn)p(trg|src)
Since the same para-phrase pair can be obtained
by pivoting on multiple foreign language phrases, a
summation or maximization over foreign language
phrases is necessary. In order not to put too much
probability mass onto para-phrase translations that
can be obtained from multiple foreign language
phrases, we maximize instead of summing over src.
In our experiments, we employed equation (3)
to infer for each para-phrase pair translation model
probabilities p?(syn|trg) and p??(trg|syn) from
relative frequencies of phrases in bilingual tables.
In contrast to Bannard and Callison-Burch (2005),
we applied the same inference step to infer also
lexical translation probabilities pw(syn|trg) and
pw?(trg|syn) as defined in Koehn et al (2003) for
para-phrases. Furthermore, we deployed features for
the number of words lw, number of phrases c? , a
reordering score pd , and a score for a 6-gram lan-
guage model pLM trained on English web data. The
final model combines these features in a log-linear
model that defines the probability of paraphrasing a
full sentence, consisting of a sequence of I phrases
467
qa-translation (how, how) (to, to) (live, live) (with, with) (cat, pet) (allergies, allergies)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, allergy)
(how, how) (to, to) (live, live) (with, with) (cat, cat) (allergies, food)
(how, how) (to, to) (live, live) (with, with) (cat, cats) (allergies, allergies)
paraphrasing (how, how) (to live, to live) (with cat, with cat) (allergies, allergy)
(how, ways) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live with, to live with) (cat, feline) (allergies, allergies)
(how to, how to) (live, living) (with cat, with cat) (allergies, allergies)
(how to, how to) (live, life) (with cat, with cat) (allergies, allergies)
(how, way) (to live, to live) (with cat, with cat) (allergies, allergies)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergens)
(how, how) (to live, to live) (with cat, with cat) (allergies, allergen)
Table 2: Unique n-best phrase-level translations of query ?how to live with cat allergies?.
as follows:
p(synI1|trg
I
1) = (
I?
i=1
p?(syni|trgi)
?? (4)
? p??(trgi|syni)
???
? pw(syni|trgi)
?w
? pw?(trgi|syni)
?w?
? pd(syni, trgi)
?d)
? lw(syn
I
1)
?l
? c?(syn
I
1)
?c
? pLM (syn
I
1)
?LM
For estimation of the feature weights ~? defined
in equation (4) we employed minimum error rate
(MER) training under the BLEU measure (Och,
2003). Training data for MER training were taken
from multiple manual English translations of Chi-
nese sources from the NIST 2006 evaluation data.
The first of four reference translations for each Chi-
nese sentence was taken as source paraphrase, the
rest as reference paraphrases. Discriminative train-
ing was conducted on 1,820 sentences; final evalua-
tion on 2,390 sentences. A baseline paraphrase table
consisting of 33 million English para-phrase pairs
was extracted from 1 billion phrase pairs from three
different languages, at a cutoff of para-phrase prob-
abilities of 0.0025.
Query expansion is done by adding terms intro-
duced in n-best paraphrases of the query. Table 2
shows example paraphrases for the query ?how to
live with cat allergies? with newly introduced terms
highlighted in bold face.
5 Experimental Evaluation
Our baseline answer retrieval system is modeled af-
ter the tfidf retrieval model of Jijkoun and de Ri-
jke (2005). Their model calculates a linear com-
bination of vector similarity scores between the
user query and several fields in the question-answer
pair. We used the cosine similarity metric with
logarithmically weighted term and document fre-
quency weights in order to reproduce the Lucene3
model used in Jijkoun and de Rijke (2005). For
indexing of fields, we adopted the settings that
were reported to be optimal in Jijkoun and de
Rijke (2005). These settings comprise the use of
8 question-answer pair fields, and a weight vec-
tor ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? for fields or-
dered as follows: (1) full FAQ document text, (2)
question text, (3) answer text, (4) title text, (5)-(8)
each of the above without stopwords. The second
field thus takes takes wh-words, which would typ-
ically be filtered out, into account. All other fields
are matched without stopwords, with higher weight
assigned to document and question than to answer
and title fields. We did not use phrase-matching or
stemming in our experiments, similar to Jijkoun and
de Rijke (2005), who could not find positive effects
for these features in their experiments.
Expansion terms are taken from those terms
in the n-best translations of the query that have
not been seen in the original query string. For
paraphrasing-based query expansion, a 50-best list
of paraphrases of the original query was used.
For the noisier question-answer translation, expan-
sion terms and phrases were extracted from a 10-
3http://lucene.apache.org
468
S2@10 S2@20 S1,2@10 S1,2@20
baseline tfidf 27 35 58 65
local expansion 30 (+ 11.1) 40 (+ 14.2) 57 (- 1) 63 (- 3)
SMT-based expansion 38 (+ 40.7) 43 (+ 22.8) 58 65
Table 3: Success rate at 10 or 20 results for retrieval of adequate (2) or material (1) answers; relative change
in brackets.
best list of query translations. Terms taken from
query paraphrases were matched with the same field
weight vector ?0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.2, 0.3? as
above. Terms taken from question-answer trans-
lation were matched with the weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3?, preferring an-
swer fields over question fields. After stopword
removal, the average number of expansion terms
produced was 7.8 for paraphrasing, and 3.1 for
question-answer translation.
The local expansion technique used in our exper-
iments follows Xu and Croft (1996) in taking ex-
pansion terms from the top n answers that were re-
trieved by the baseline tfidf system, and by incorpo-
rating cooccurrence information with query terms.
This is done by calculating term frequencies for ex-
pansion terms by summing up the tfidf weights of
the answers in which they occur, thus giving higher
weight to terms that occur in answers that receive
a higher similarity score to the original query. In
our experiments, expansion terms are ranked accord-
ing to this modified tfidf calculation over the top 20
answers retrieved by the baseline retrieval run, and
matched a second time with the field weight vector
?0.0, 1.0, 0.0, 0.0, 0.5, 0.2, 0.5, 0.3? that prefers an-
swer fields over question fields. After stopword re-
moval, the average number of expansion terms pro-
duced by the local expansion technique was 9.25.
The test queries we used for retrieval are taken
from query logs of the MetaCrawler search en-
gine4 and were provided to us by Valentin Jijk-
oun. In order to maximize recall for the comparative
evaluation of systems, we selected 60 queries that
were well-formed natural language questions with-
out metacharacters and spelling errors. However, for
one third of these well-formed queries none of the
five compared systems could retrieve an answer. Ex-
amples are ?how do you make a cornhusk doll?,
4http://www.metacrawler.com
?what is the idea of materialization?, or ?what does
8x certified mean?, pointing to a severe recall prob-
lem of the question-answer database.
Evaluation was performed by manual labeling of
top 20 answers retrieved for each of 60 queries for
each system by two independent judges. For the sake
of consistency, we chose not to use the assessments
provided by Jijkoun and de Rijke. Instead, the judges
were asked to find agreement on the examples on
which they disagreed after each evaluation round.
The ratings together with the question-answer pair
id were stored and merged into the retrieval results
for the next system evaluation. In this way consis-
tency across system evaluations could be ensured,
and the effort of manual labeling could be substan-
tially reduced. The quality of retrieval results was
assessed according to Jijkoun and de Rijke?s (2005)
three point scale:
? adequate (2): answer is contained
? material (1): no exact answer, but important in-
formation given
? unsatisfactory (0): user?s information need is
not addressed
The evaluation measure used in Jijkoun and de
Rijke (2005) is the success rate at 10 or 20 an-
swers, i.e., S2@n is the percentage of queries with
at least one adequate answer in the top n retrieved
question-answer pairs, and S1,2@n is the percentage
of queries with at least one adequate or material an-
swer in the top n results. This evaluation measure ac-
counts for improvements in coverage, i.e., it rewards
cases where answers are found for queries that did
not have an adequate or material answer before. In
contrast, the mean reciprocal rank (MRR) measure
standardly used in QA can have the effect of prefer-
ring systems that find answers only for a small set
of queries, but rank them higher than systems with
469
(1) query: how to live with cat allergies
local expansion (-): allergens allergic infections filter plasmacluster rhinitis introduction effective replacement
qa-translation (+): allergy cats pet food
paraphrasing (+): way allergens life allergy feline ways living allergen
(2) query: how to design model rockets
local expansion (-): models represented orientation drawings analysis element environment different structure
qa-translation (+): models rocket
paraphrasing (+): missiles missile rocket grenades arrow designing prototype models ways paradigm
(3) query: what is dna hybridization
local expansion (-): instructions individual blueprint characteristics chromosomes deoxyribonucleic information biological
genetic molecule
qa-translation (+): slides clone cdna sitting sequences
paraphrasing (+): hibridization hybrids hybridation anything hibridacion hybridising adn hybridisation nothing
(4) query: how to enhance competitiveness of indian industries
local expansion (+): resources production quality processing established investment development facilities institutional
qa-translation (+): increase industry
paraphrasing (+): promote raise improve increase industry strengthen
(5) query: how to induce labour
local expansion (-): experience induction practice imagination concentration information consciousness different meditation
relaxation
qa-translation (-): birth industrial induced induces
paraphrasing (-): way workers inducing employment ways labor working child work job action unions
Table 4: Examples for queries and expansion terms yielding improved (+), decreased (-), or unchanged (0)
retrieval performance compared to retrieval without expansion.
higher coverage. This makes MRR less adequate for
the low-recall setup of FAQ retrieval.
Table 3 shows success rates at 10 and 20 retrieved
question-answer pairs for five different systems. The
results for the baseline tfidf system, following Jijk-
oun and de Rijke (2005), are shown in row 2. Row
3 presents results for our variant of local expansion
by pseudo-relevance feedback (Xu and Croft, 1996).
Results for SMT-based expansion are given in row 4.
A comparison of success rates for retrieving at least
one adequate answer in the top 10 results shows rel-
ative improvements over the baseline of 11.1% for
local query expansion, and of 40.7% for combined
SMT-based expansion. Success rates at top 20 re-
sults show similar relative improvements of 14.2%
for local query expansion, and of 22.8% for com-
bined SMT-based expansion. On the easier task of
retrieving a material or adequate answer, success
rates drop by a small amount for local expansion,
and stay unchanged for SMT-based expansion.
These results can be explained by inspecting a few
sample query expansions. Examples (1)-(3) in Ta-
ble 4 illustrate cases where SMT-based query expan-
sion improves results over baseline performance, but
local expansion decreases performance by introduc-
ing irrelevant terms. In (4) retrieval performance is
improved over the baseline for both expansion tech-
niques. In (5) both local and SMT-based expansion
introduce terms that decrease retrieval performance
compared to retrieval without expansion.
6 Conclusion
We presented two techniques for query expansion in
answer retrieval that are based on SMT technology.
Our method for question-answer translation uses a
large corpus of question-answer pairs extracted from
FAQ pages to learn a translation model from ques-
tions to answers. SMT-based paraphrasing utilizes
large amounts of bilingual data as a new informa-
tion source to extract phrase-level synonyms. Both
SMT-based techniques take the entire query context
into account when adding new terms to the orig-
inal query. In an experimental comparison with a
baseline tfidf approach and a local query expansion
technique on the task of answer retrieval from FAQ
pages, we showed a significant improvement of both
SMT-based query expansion over both baselines.
Despite the small-scale nature of our current ex-
perimental results, we hope to apply the presented
techniques to general web retrieval in future work.
Another task for future work is to scale up the ex-
traction of question-answer pair data in order to
provide an improved resource for question-answer
translation.
470
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano.
2004. Learning to find answers to questions on
the web. ACM Transactions on Internet Technology,
4(2):129?162.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of (ACL?05), Ann Arbor, MI.
Adam L. Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer-finding. In
Proceedings of SIGIR?00, Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Robin B. Burke, Kristian J. Hammond, and Vladimir A.
Kulyukin. 1997. Question answering from
frequently-asked question files: Experiences with the
FAQ finder system. AI Magazine, 18(2):57?66.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yo-ram Singer. 2006. Online passive-
agressive algorithms. Machine Learning, 7:551?585.
Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006. An-
swering the question you wish they had asked: The im-
pact of paraphrasing for question answering. In Pro-
ceedings of (HLT-NAACL?06), New York, NY.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of (ACL?03), Sapporo, Japan.
Sanda Harabagiu and Finley Lacatusu. 2004. Strategies
for advanced question answering. In Proceedings of
the HLT-NAACL?04 Workshop on Pragmatics of Ques-
tion Answering, Boston, MA.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of (ACL?01),
Toulouse, France.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answering.
In Proceedings of TREC-11, Gaithersburg, MD.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Michael
Junk, and Chin-Yew Lin. 2000. Question answering
in webclopedia. In Proceedings of TREC 9, Gaithers-
burg, MD.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2001. IBM?s statistical question answering system. In
Proceedings of TREC 10, Gaithersburg, MD.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management (CIKM?05),
Bremen, Germany.
Thorsten Joachims. 2003. Transductive learning
via spectral graph partitioning. In Proceedings of
ICML?03, Washington, DC.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of (HLT-NAACL?03), Edmonton, Cananda.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Journal of Natural
Language Engineering, 7(3):343?360.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
(HLT-NAACL?03), Edmonton, Cananda.
John Prager, Jennifer Chu-Carroll, and Krysztof Czuba.
2001. Use of wordnet hypernyms for answering what-
is questions. In Proceedings of TREC 10, Gaithers-
burg, MD.
Yonggang Qiu and H. P. Frei. 1993. Concept based query
expansion. In Proceedings of SIGIR?93, Pittsburgh,
PA.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Weigo Fan, and John
Prager. 2001. Mining the web for answers to natu-
ral language questions. In Proceedings of (CIKM?01),
Atlanta, GA.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9:191?206.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SI-
GIR?94, Dublin, Ireland.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In Proceed-
ings of SIGIR?96, Zurich, Switzerland.
471
