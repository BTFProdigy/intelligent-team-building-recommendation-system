85
86
87
88
89
90
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87?91,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Query log analysis with GALATEAS LangLog
Marco Trevisan and Luca Dini
CELI
trevisan@celi.it
dini@celi.it
Eduard Barbu
Universita` di Trento
eduard.barbu@unitn.it
Igor Barsanti
Gonetwork
i.barsanti@gonetwork.it
Nikolaos Lagos
Xerox Research Centre Europe
Nikolaos.Lagos@xrce.xerox.com
Fre?de?rique Segond and Mathieu Rhulmann
Objet Direct
fsegond@objetdirect.com
mruhlmann@objetdirect.com
Ed Vald
Bridgeman Art Library
ed.vald@bridgemanart.co.uk
Abstract
This article describes GALATEAS
LangLog, a system performing Search Log
Analysis. LangLog illustrates how NLP
technologies can be a powerful support
tool for market research even when the
source of information is a collection of
queries each one consisting of few words.
We push the standard Search Log Analysis
forward taking into account the semantics
of the queries. The main innovation of
LangLog is the implementation of two
highly customizable components that
cluster and classify the queries in the log.
1 Introduction
Transaction logs become increasingly important
for studying the user interaction with systems
likeWeb Searching Engines, Digital Libraries, In-
tranet Servers and others (Jansen, 2006). Var-
ious service providers keep log files recording
the user interaction with the searching engines.
Transaction logs are useful to understand the user
search strategy but also to improve query sugges-
tions (Wen and Zhang, 2003) and to enhance
the retrieval quality of search engines (Joachims,
2002). The process of analyzing the transaction
logs to understand the user behaviour and to as-
sess the system performance is known as Transac-
tion Log Analysis (TLA). Transaction Log Anal-
ysis is concerned with the analysis of both brows-
ing and searching activity inside a website. The
analysis of transaction logs that focuses on search
activity only is known as Search Log Analysis
(SLA). According to Jansen (2008) both TLA
and SLA have three stages: data collection, data
preparation and data analysis. In the data collec-
tion stage one collects data describing the user
interaction with the system. Data preparation is
the process of loading the collected data in a re-
lational database. The data loaded in the database
gives a transaction log representation independent
of the particular log syntax. In the final stage
the data prepared at the previous step is analyzed.
One may notice that the traditional three levels
log analyses give a syntactic view of the infor-
mation in the logs. Counting terms, measuring
the logical complexity of queries or the simple
procedures that associate queries with the ses-
sions in no way accesses the semantics of queries.
LangLog system addreses the semantic problem
performing clustering and classification for real
query logs. Clustering the queries in the logs al-
lows the identification of meaningful groups of
queries. Classifying the queries according to a
relevant list of categories permits the assessment
of how well the searching engine meets the user
needs. In addition the LangLog system address
problems like automatic language identification,
Name Entity Recognition, and automatic query
translation. The rest of the paper is organized
as follows: the next section briefly reviews some
systems performing SLA. Then we present the
data sources the architecture and the analysis pro-
cess of the LangLog system. The conclusion sec-
tion concludes the article summarizing the work
and presenting some new possible enhancements
of the LangLog.
87
2 Related work
The information in the log files is useful in many
ways, but its extraction raises many challenges
and issues. Facca and Lanzi (2005) offer a sur-
vey of the topic. There are several commercial
systems to extract and analyze this information,
such as Adobe web analytics1, SAS Web Analyt-
ics2, Infor Epiphany3, IBM SPSS4. These prod-
ucts are often part of a customer relation manage-
ment (CRM) system. None of those showcases
include any form of linguistic processing. On the
other hand, Web queries have been the subject
of linguistic analysis, to improve the performance
of information retrieval systems. For example, a
study (Monz and de Rijke, 2002) experimented
with shallow morphological analysis, another (Li
et al 2006) analyzed queries to remove spelling
mistakes. These works encourage our belief that
linguistic analysis could be beneficial for Web log
analysis systems.
3 Data sources
LangLog requires the following information from
the Web logs: the time of the interaction, the
query, click-through information and possibly
more. LangLog processes log files which con-
form to the W3C extended log format. No other
formats are supported. The system prototype is
based on query logs spanning one month of inter-
actions recorded at the Bridgeman Art Library5.
Bridgeman Art library contains a large repository
of images coming from 8000 collections and rep-
resenting more than 29.000 artists.
4 Analyses
LangLog organizes the search log data into units
called queries and hits. In a typical search-
ing scenario a user submits a query to the con-
tent provider?s site-searching engine and clicks
on some (or none) of the search results. From
now on we will refer to a clicked item as a hit,
and we will refer to the text typed by the user as
the query. This information alone is valuable to
the content provider because it allows to discover
1http://www.omniture.com/en/products/analytics
2http://www.sas.com/solutions/webanalytics/index.html
3http://www.infor.com
4http://www-01.ibm.com/software/analytics/spss/
5http://www.bridgemanart.com
which queries were served with results that satis-
fied the user, and which queries were not.
LangLog extracts queries and hits from the log
files, and performs the following analyses on the
queries:
? language identification
? tokenization and lemmatization
? named entity recognition
? classification
? cluster analysis
Language information may help the content
provider decide whether to translate the content
into new languages.
Lemmatization is especially important in lan-
guages like German and Italian that have a rich
morphology. Frequency statistics of keywords
help understand what users want, but they are bi-
ased towards items associated with words with
lesser ortographic and morpho-syntactic varia-
tion. For example, two thousand queries for
?trousers?, one thousand queries for ?handbag?
and another thousand queries for ?handbags?
means that handbags are twice as popular as
trousers, although statistics based on raw words
would say otherwise.
Named entities extraction helps the content
provider for the same reasons lemmatization does.
Named entities are especially important because
they identify real-world items that the content
provider can relate to, while lemmas less often do
so. The name entities and the most important con-
cepts can be linked afterwards with resources like
Wikipedia which offer a rich specification of their
properties.
Both classification and clustering allow the
content provider to understand what kind of the
users look for and how this information is targeted
by means of queries.
Classification consists of classifying queries
into categories drawn from a classification
schema. When the schema used to classify
is different from the schema used in the con-
tent provider?s website, classification may provide
hints as to what kind of queries are not matched
by items in the website. In a similar way, cluster
analysis can be used to identify new market seg-
ments or new trends in the user?s behaviour. Clus-
88
ter analysis provide more flexybility than classifi-
cation, but the information it produces is less pre-
cise. Many trials and errors may be necessary be-
fore finding interesting results. One hopes that the
final clustering solution will give insights into the
patterns of users? searches. For example an on-
line book store may discover that one cluster con-
tains many software-related terms, altough none
of those terms is popular enough to be noticeable
in the statistics.
5 Architecture
LangLog consists of three subsystems: log ac-
quisition, log analysis, log disclosure. Periodi-
cally the log acquisition subsystem gathers new
data which it passes to the log analyses compo-
nent. The results of the analyses are then available
through the log disclosure subsystem.
Log acquisition deals with the acquisition and
normalization and anonymization of the data con-
tained in the content provider?s log files. The
data flows from the content provider?s servers to
LangLog?s central database. This process is car-
ried out by a series of Pentaho Data Integration6
procedures.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
Log disclosure is actually a collection of inde-
pendent systems that allow the content providers
to access their information and the analyses. Log
disclosure systems are also concerned with access
control and protection of privacy. The content
provider can access the output of LangLog using
AWStats, QlikView, or JPivot.
? AWStats7 is a widely used log analysis sys-
tem for websites. The logs gathered from the
websites are parsed by AWStats, which gen-
erates a complete report about visitors, vis-
its duration, visitor?s countries and other data
to disclose useful information about the visi-
tor?s behavior.
6http://kettle.pentaho.com
7http://awstats.sourceforge.net
? QlikView8 is a business intelligence (BI)
platform. A BI platform provides histori-
cal, current, and predictive views of busi-
ness operations. Usually such tools are used
by companies to have a clear view of their
business over time. In LangLog, QlickView
does not display sales or costs evolution over
time. Instead, it displays queries on the con-
tent provider?s website over time. A dash-
board with many elements (input selections,
tables, charts, etc.) provides a wide range of
tools to visualize the data.
? JPivot9 is a front-end for Mondrian. Mon-
drian10 is an Online Analytical Processing
(OLAP) engine, a system capable of han-
dling and analyzing large quantities of data.
JPivot allows the user to explore the output
of LangLog, by slicing the data along many
dimensions. JPivot allows the user to display
charts, export results to Microsoft Excel or
CSV, and use custom OLAP MDX queries.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
5.1 Language Identification
The system uses a language identification sys-
tem (Bosca and Dini, 2010) which offers language
identification for English, French, Italian, Span-
ish, Polish and German. The system uses four
different strategies:
? N-gram character models: uses the distance
between the character based models of the
input and of a reference corpus for the lan-
guage (Wikipedia).
? Word frequency: looks up the frequency of
the words in the query with respect to a ref-
erence corpus for the language.
? Function words: searches for particles
highly connoting a specific language (such
as prepositions, conjunctions).
8http://www.qlikview.com
9http://jpivot.sourceforge.net
10http://mondrian.pentaho.com
89
? Prior knowledge: provides a default guess
based on a set of hypothesis and heuristics
like region/browser language.
5.2 Lemmatization
To perform lemmatization, Langlog uses general-
purpose morpho-syntactic analysers based on the
Xerox Incremental Parser (XIP), a deep robust
syntactic parser (Ait-Mokhtar et al 2002). The
system has been adapted with domain-specific
part of speech disambiguation grammar rules, ac-
cording to the results a linguistic study of the de-
velopment corpus.
5.3 Named entity recognition
LangLog uses the Xerox named entity recogni-
tion web service (Brun and Ehrmann, 2009) for
English and French. XIP includes also a named
entity detection component, based on a combina-
tion of lexical information and hand-crafted con-
textual rules. For example, the named entity
recognition system was adapted to handle titles
of portraits, which were frequent in our dataset.
While for other NLP tasks LangLog uses the same
system for every content provider, named entity
recognition is a task that produces better analyses
when it is tailored to the domain of the content.
Because LangLog uses a NER Web service, it is
easy to replace the default NER system with a dif-
ferent one. So if the content provider is interested
in the development of a NER system tailored for
a specific domain, LangLog can accomodate this.
5.4 Clustering
We developed two clustering systems: one per-
forms hierarchical clustering, another performs
soft clustering.
? CLUTO: the hierarchical clustering system
relies on CLUTO411, a clustering toolkit.
To understand the main ideas CLUTO is
based on one might consult Zhao and
Karypis (2002). The clustering process pro-
ceeds as follows. First, the set of queries to
be clustered is partitioned in k groups where
k is the number of desired clusters. To do
so, the system uses a partitional clustering
algorithm which finds the k-way clustering
solution making repeated bisections. Then
11http://glaros.dtc.umn.edu/gkhome/views/cluto
the system arranges the clusters in a hierar-
chy by successively merging the most similar
clusters in a tree.
? MALLET: the soft clustering system we
developed relies on MALLET (McCallum,
2002), a Latent Dirichlet Allocation (LDA)
toolkit (Steyvers and Griffiths, 2007).
Our MALLET-based system considers that
each query is a document and builds a topic
model describing the documents. The result-
ing topics are the clusters. Each query is as-
sociated with each topic according to a cer-
tain strenght. Unlike the system based on
CLUTO, this system produces soft clusters,
i.e. each query may belong to more than one
cluster.
5.5 Classification
LangLog allows the same query to be classified
many times using different classification schemas
and different classification strategies. The result
of the classification of an input query is always a
map that assigns each category a weight, where
the higher the weight, the more likely the query
belongs to the category. If NER performs bet-
ter when tailored to a specific domain, classifi-
cation is a task that is hardly useful without any
customization. We need a different classification
schema for each content provider. We developed
two classification system: an unsupervised sys-
tem and a supervised one.
? Unsupervised: this system does not require
any training data nor any domain-specific
corpus. The output weight of each category
is computed as the cosine similarity between
the vector models of the most representa-
tive Wikipedia article for the category and
the collection of Wikipedia articles most rel-
evant to the input query. Our evaluation in
the KDD-Cup 2005 dataset results in 19.14
precision and 22.22 F-measure. For com-
parison, the state of the art in the competi-
tion achieved a 46.1 F-measure. Our system
could not achieve a similar score because it
is unsupervised, and therefore it cannot make
use of the KDD-Cup training dataset. In ad-
dition, it uses only the query to perform clas-
sification, whereas KDD-Cup systems were
also able to access the result sets associated
to the queries.
90
? Supervised: this system is based on the
Weka framework. Therefore it can use any
machine learning algorithm implemented in
Weka. It uses features derived from the
queries and from Bridgeman metadata. We
trained a Naive Bayes classifier on a set of
15.000 queries annotated with 55 categories
and hits and obtained a F-measure of 0.26.
The results obtained for the classification
are encouraging but not yet at the level of
the state of the art. The main reason for
this is the use of only in-house meta-data in
the feature computation. In the future we
will improve both components by providing
them with features from large resources like
Wikipedia or exploiting the results returned
by Web Searching engines.
6 Demonstration
Our demonstration presents:
? The setting of our case study: the Bridgeman
Art Library website, a typical user search,
and what is recorded in the log file.
? The conceptual model of the results of the
analyses: search episodes, queries, lemmas,
named entities, classification, clustering.
? The data flow across the parts of the system,
from content provider?s servers to the front-
end through databases, NLP Web services
and data marts.
? The result of the analyses via QlikView.
7 Conclusion
In this paper we presented the LangLog system,
a customizable system for analyzing query logs.
The LangLog performs language identification,
lemmatization, NER, classification and clustering
for query logs. We tested the LangLog system on
queries in Bridgeman Library Art. In the future
we will test the system on query logs in differ-
ent domains (e.g. pharmaceutical, hardware and
software, etc.) thus increasing the coverage and
the significance of the results. Moreover we will
incorporate in our system the session information
which should increase the precision of both clus-
tering and classification components.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod and Claude
Roux 2002. Robustness Beyond Shallowness: In-
cremental Deep Parsing. Journal of Natural Lan-
guage Engineering 8, 2-3, 121-144.
Alessio Bosca and Luca Dini. 2010. Language Identi-
fication Strategies for Cross Language Information
Retrieval. CLEF 2010 Working Notes.
C. Brun and M. Ehrmann. 2007. Adaptation of
a Named Entity Recognition System for the ES-
TER 2 Evaluation Campaign. In proceedings of
the IEEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering.
F. M. Facca and P. L. Lanzi. 2005. Mining interesting
knowledge from weblogs: a survey. Data Knowl.
Eng. 53(3):225241.
Jansen, B. J. 2006. Search log analysis: What is it;
what?s been done; how to do it. Library and Infor-
mation Science Research 28(3):407-432.
Jansen, B. J. 2008. The methodology of search log
analysis. In B. J. Jansen, A. Spink and I. Taksa (eds)
Handbook of Web log analysis 100-123. Hershey,
PA: IGI.
Joachims T. 2002. Optimizing search engines us-
ing clickthrough data. In proceedings of the 8th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining 133-142.
M. Li, Y. Zhang, M. Zhu, and M. Zhou. 2006. Ex-
ploring distributional similarity based models for
query spelling correction. In proceedings of In ACL
06: the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of
the ACL 10251032, 2006.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
C. Monz and M. de Rijke. 2002. Shallow Morpholog-
ical Analysis in Monolingual Information Retrieval
for Dutch, German and Italian. In Proceedings of
CLEF 2001. Springer
M. Steyvers and T. Griffiths. 2007. Probabilistic
Topic Models. In T. Landauer, D McNamara, S.
Dennis and W. Kintsch (eds), Handbook of Latent
Semantic Analysis, Psychology Press.
J. R. Wen and H.J. Zhang 2003. Query Clustering
in the Web Context. In Wu, Xiong and Shekhar
(eds) Information Retrieval and Clustering 195-
226. Kluwer Academic Publishers.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
91
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 54?62,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Structure-Preserving Pipelines for Digital Libraries
Massimo Poesio
University of Essex, UK and
Universit? di Trento, Italy
Eduard Barbu
Egon W. Stemle
Universit? di Trento, Italy
{massimo.poesio,eduard.barbu,egon.stemle}
@unitn.it
Christian Girardi
FBK-irst, Trento, Italy
cgirardi@fbk.eu
Abstract
Most existing HLT pipelines assume the input
is pure text or, at most, HTML and either ig-
nore (logical) document structure or remove
it. We argue that identifying the structure of
documents is essential in digital library and
other types of applications, and show that it
is relatively straightforward to extend existing
pipelines to achieve ones in which the struc-
ture of a document is preserved.
1 Introduction
Many off-the-shelf Human Language Technology
(HLT) pipelines are now freely available (examples
include LingPipe,1 OpenNLP,2 GATE3 (Cunning-
ham et al, 2002), TextPro4 (Pianta et al, 2008)),
and although they support a variety of document for-
mats as input, actual processing (mostly) takes no
advantage of structural information, i.e. structural
information is not used, or stripped off during pre-
processing. Such processing can be considered safe,
e.g. in case of news wire snippets, when processing
does not need to be aware of sentence or paragraph
boundaries, or of text being part of a table or a fig-
ure caption. However, when processing large doc-
uments, section or chapter boundaries may be con-
sidered an important segmentation to use, and when
working with the type of data typically found in dig-
ital libraries or historical archives, such as whole
1
http://alias-i.com/lingpipe/
2
http://incubator.apache.org/opennlp/
3
http://http://gate.ac.uk/
4
http://textpro.fbk.eu/
books, exhibition catalogs, scientific articles, con-
tracts we should keep the structure. At least three
types of problems can be observed when trying to
use a standard HLT pipeline for documents whose
structure cannot be easily ignored:
? techniques for extracting content from plain
text do not work on, say, bibliographic refer-
ences, or lists;
? simply removing the parts of a document that
do not contain plain text may not be the right
thing to do for all applications, as sometimes
the information contained in them may also be
useful (e.g., keywords are often useful for clas-
sification, bibliographic references are useful in
a variety of applications) or even the most im-
portant parts of a text (e.g., in topic classifica-
tion information provided by titles and other
types of document structure is often the most
important part of a document);
? even for parts of a document that still can be
considered as containing basically text?e.g.,
titles?knowing that we are dealing with what
we will call here non-paragraph text can be
useful to achieve good - or improve - perfor-
mance as e.g., the syntactic conventions used
in those type of document elements may be dif-
ferent - e.g., the syntax of NPs in titles can be
pretty different from that in other sections of
text.
In this paper we summarize several years of work
on developing structure-preserving pipelines for dif-
ferent applications. We discuss the incorporation of
54
document structure parsers both in pipelines which
the information is passed in BOI format (Ramshaw
and Marcus, 1995), such as the TEXTPRO pipeline
(Pianta et al, 2008), and in pipelines based on a
standoff XML (Ide, 1998). We also present sev-
eral distinct applications that require preserving doc-
ument structure.
The structure of the paper is as follows. We first
discuss the notion of document structure and previ-
ous work in extracting it. We then introduce our ar-
chitecture for a structure-preserving pipeline. Next,
we discuss two pipelines based on this general archi-
tecture. A discussion follows.
2 The Logical Structure of a Document
Documents have at least two types of structure5.
The term geometrical, or layout, structure, refers
to the structuring of a document according to its vi-
sual appearance, its graphical representation (pages,
columns, etc). The logical structure (Luong et al,
2011) refers instead to the content?s organization to
fulfill an intended overall communicative purpose
(title, author list, chapter, section, bibliography, etc).
Both of these structures can be represented as trees;
however, these two tree structures may not be mu-
tually compatible (i.e. representable within a single
tree structure with non-overlapping structural ele-
ments): e.g. a single page may contain the end of
one section and the beginning of the next, or a para-
graph may just span part of a page or column. In this
paper we will be exclusively concerned with logical
structure.
2.1 Proposals concerning logical structure
Early on the separation of presentation and content,
i.e. of layout and logical structure, was promoted by
the early adopters of computers within the typeset-
ting community; well-known, still widely used, sys-
tems include the LATEXmeta-package for electronic
typesetting. The importance of separating document
logical structure from document content for elec-
tronic document processing and for the document
creators lead to the ISO 8613-1:1989(E) specifica-
tion where logical structure is defined as the result
of dividing and subdividing the content of a docu-
5other structure types include e.g. (hyper)links, cross-
references, citations, temporal and spatial relationships
ment into increasingly smaller parts, on the basis of
the human-perceptible meaning of the content, for
example, into chapters, sections, subsections, and
paragraphs. The influential ISO 8879:1986 Stan-
dard Generalized Markup Language (SGML) spec-
ification fostered document format definitions like
the Open Document Architecture (ODA) and inter-
change format, CCITT T.411-T.424 / ISO 8613.
Even though the latter format never gained
wide-spread support, its technological ideas influ-
enced many of today?s formats, like HTML and
CSS as well as, the Extensible Markup Language
(XML), today?s successor of SGML. Today, the ISO
26300:2006 Open Document Format for Office Ap-
plications (ODF), and the ISO 29500:2008 Office
Open XML (OOXML) format are the important
XML-based document file formats.
For the work on digital libraries the Text Encod-
ing Initiative (TEI)6,most notably, developed guide-
lines specifying encoding methods for machine-
readable texts. They have been widely used, e.g. by
libraries, museums, and publishers.
The most common logical elements in such
proposals?chapters, sections, paragraphs, foot-
notes, etc.?can all be found in HTML, LATEX, or
any other modern text processor. It should be
pointed out however that many modern types of doc-
uments found on the Web do not fit this pattern:
e.g. blog posts with comments, and the structure of
reply threads and inner-linkings to other comments
cannot be captured; or much of wikipedia?s non-
paragraph text. (For an in depth comparison and
discussion of logical formats, and formal characteri-
zations thereof we suggest (Power et al, 2003; Sum-
mers, 1998).)
2.2 Extracting logical structure
Two families of methods have been developed to ex-
tract document structure. Older systems tend to fol-
low the template-matching paradigm. In this ap-
proach the assignment of the categories to parts of
the string is done by matching a sequence of hand
crafted templates against the input string S. An
instance of this kind of systems is DeLos (Deriva-
tion of Logical Structure) (Niyogi and Srihari, 1995)
which uses control rules, strategy rules and knowl-
6
http://www.tei-c.org
55
edge rules to derive the logical document structure
from a scanned image of the document. A more elab-
orate procedure for the same task is employed by
Ishitani (Ishitani, 1999). He uses rules to classify the
text lines derived from scanned document image and
then employs a set of heuristics to assign the classi-
fied lines to logical document components. The tem-
plate based approach is also used by the ParaTools,
a set of Perl modules for parsing reference strings
(Jewell, 2000). The drawback of the template based
approaches is that they are usually not portable to
new domains and are not flexible enough to accom-
modate errors. Domain adaptation requires the de-
vising of new rules many of them from scratch. Fur-
ther the scanned documents or the text content ex-
tracted from PDF have errors which are not easily
dealt with by template based systems.
Newer systems use supervised machine learning
techniques which are much more flexible but re-
quire training data. Extracting document structure
is an instance of (hierarchical) sequence labeling,
a well known problem which naturally arises in di-
verse fields like speech recognition, digital signal
processing or bioinformatics. Two kinds of machine
learning techniques are most commonly used for this
problem: Hidden Markov Models (HMM) and Con-
ditional Random Fields (CRF). A system for pars-
ing reference strings based on HMMs was developed
in (Hetzner, 2008) for the California Digital Library.
The system implements a first order HMM where the
set of states of the model are represented by the cat-
egories in C; the alphabet is hand built and tailored
for the task and the probabilities in the probability
matrix are derived empirically. The system obtains
an average F1 measure of 93 for the Cora dataset.
A better performance for sequence labeling is ob-
tained if CRF replaces the traditional HMM. The
reason for this is that CRF systems better tolerate
errors and they have good performance even when
richer features are not available. A system which
uses CRF and a series of post-processing rules for
both document logical structure identification and
reference string parsing is ParsCit (Councill et al,
2008). ParsCit comprises three sub-modules: Sect-
Label and ParseHead for document logical structure
identification and ParsCit for reference string pars-
ing. The system is built on top of the well known
CRF++ package.
The linguistic surface level, i.e. the linear order
of words, sentences, and paragraphs, and the hi-
erarchical, tree-like, logical structure also lends it-
self to parsing-like methods for the structure analy-
sis. However, the complexity of fostering, maintain-
ing, and augmenting document structure grammars
is challenging, and the notorious uncertainty of the
input demands for the whole set of stochastic tech-
niques the field has to offer ? this comes at a high
computing price; cf. e.g.,(Lee et al, 2003; Mao et
al., 2003). It is therefore not surprising that high-
throughput internet sites like CiteSeerX7 use a flat
text classifier (Day et al, 2007).8
3 Digital Libraries and Document
Structure Preservation
Our first example of application in which document
structure preservation is essential are digital libraries
(Witten et al, 2003). In a digital library setting, HLT
techniques can be used for a variety of purposes,
ranging from indexing the documents in the library
for search to classifying them to automatically ex-
tracting metadata. It is therefore becoming more and
more common for HLT techniques to be incorporated
in document management platforms and used to sup-
port a librarian when he / she enters a new document
in the library. Clearly, it would be beneficial if such
a pipeline could identify the logical structure of the
documents being entered, and preserve it: this infor-
mation could be used by the document management
platform to, for instance, suggest the librarian the
most important keywords, find the text to be indexed
or even summarized, and produce citations lists, pos-
sibly to be compared with the digital library?s list of
citations to decide whether to add them.
We are in the process of developing a Portal
for Research in the Humanities (Portale Ricerca
Umanistica-PRU). This digital library will eventu-
ally include research articles about the Trentino re-
gion from Archeology, History, and History of Art.
So far, the pipeline to be discussed next has been
used to include in the library texts from the Italian
archeology journal Preistoria Alpina. One of our
goals was to develop a pipeline that could be used
7
http://citeseerx.ist.psu.edu/
8Still, especially multimedia documents with their possible
temporal and spatial relationships might need more sophisti-
cated methods.
56
whenever a librarian uploads an article in this digital
library, to identify title, authors, abstract, keywords,
content, and bibliographic references from the arti-
cle. The implemented portal already incorporates in-
formation extraction techniques that are used to iden-
tify in the ?content? part of the output of the pipeline
temporal expressions, locations, and entities such
as archeological sites, cultures, and artifacts. This
information is used to allow spatial, temporal, and
entity-based access to articles.
We are in the process of enriching the portal so
that title and author information are also used to au-
tomatically produce a bibliographical card for the ar-
ticle that will be entered in the PRU Library Catalog,
and bibliographical references are processed in or-
der to link the article to related articles and to the
catalog as well. The next step will be to modify the
pipeline (in particular, to modify the Named Entity
Recognition component) to include in the library ar-
ticles from other areas of research in the Humanities,
starting with History. There are also plans to make
it possible for authors themselves to insert their re-
search articles and books in the Portal, as done e.g.,
in the Semantics Archive.9.
We believe the functionalities offered by this por-
tal are or will become pretty standard in digital li-
braries, and therefore that the proposals discussed in
this paper could find an application beyond the use
in our Portal. We will also see below that a docu-
ment structure-sensitive pipeline can find other ap-
plications.
4 Turning an Existing Pipeline into One
that Extracts and Preserves Document
Structure
Most freely available HLT pipelines simply elimi-
nate markup during the initial phases of processing
in order to eliminate parts of the document struc-
ture that cannot be easily processed by their mod-
ules (e.g., bibliographic references), but this is not
appropriate for the Portal described in the previous
section, where different parts of the output of the
pipeline need to be processed in different ways. On
the other end, it was not really feasible to develop
a completely new pipeline from scratch. The ap-
proach we pursued in this work was to take an exist-
9
http://semanticsarchive.net/
ing pipeline and turn it into one which extracts and
outputs document structure. In this Section we dis-
cuss the approach we followed. In the next Section
we discuss the first pipeline we developed according
to this approach; then we discuss how the approach
was adopted for other purposes, as well.
Incorporating a document structure extractor in a
pipeline requires the solution of two basic problems:
where to insert the module, and how to pass on doc-
ument structure information. Concerning the first
issue, we decided to insert the document structure
parser after tokenization but before sentence process-
ing. In regards to the second issue, there are at
present three main formats for exchanging informa-
tion between elements of an HLT pipeline:
? inline, where each module inserts information
in a pre-defined fashion into the file received as
input;
? tabular format as done in CONLL, where to-
kens occupy the first column and each new
layer of information is annotated in a separate
new column, using the so-called IOB format
to represent bracketing (Ramshaw and Marcus,
1995);
? standoff format, where new layers of informa-
tion are stored in separate files.
The two main formats used by modern HLT pipelines
are tabular format, and inline or standoff XML for-
mat. Even though we will illustrate the problem of
preserving document structure in a pipeline of the
former type the PRU pipeline itself supports tabular
format and inline XML (TEI compliant).
The solution we adopted, illustrated in Figure 1,
involves using sentence headers to preserve docu-
ment structure information. In most pipelines using
a tabular interchange information, the output of a
module consists of a number of sentences each of
which consists of
? a header: a series of lines with a hash character
# at the beginning;
? a set of tab-delimited lines representing tokens
and token annotations;
? an empty EOF line.
57
 
# FILE: 11
# PART: id1
# SECTION: title
# FIELDS: token tokenstart sentence pos lemma entity nerType
Spondylus 0 - SPN Spondylus O B-SITE
gaederopus 10 - YF gaederopus O O
, 20 - XPW , O O
gioiello 22 - SS gioiello O O
dell ' 31 - E dell ' O O
Europa 36 - SPN europa B-GPE B-SITE
preistorica 43 - AS preistorico O O
. 55 <eos > XPS full_stop O O
# FILE: 11
# PART: id2
# SECTION: author
# FIELDS: token tokenstart sentence pos lemma entity nerType
MARIA 0 - SPN maria B-PER O
A 6 - E a I-PER O
BORRELLO 8 - SPN BORRELLO I-PER O
& 17 - XPO & O O
. 19 <eos > XPS full_stop O O
(TEI compliant inline XML snippet :)
<text >
<body >
<div type=" section" xml:lang="it">
[...]
<p id="p2" type=" author">
<s id="p2s1"><name key="PER1" type=" person">MARIA A BORRELLO </name >&.</s>
</p>
</div >
</body >
</text >
 
Figure 1: Using sentence headers to preserve document structure information. For illustration, the TEI compliant
inline XML snippet of the second sentence has been added.
58
The header in such pipelines normally specifies only
the file id (constant through the file), the number of
the sentence within the file, and the columns (see
Figure 1). This format however can also be used
to pass on document structure information provided
that the pipeline modules ignore all lines beginning
with a hash, as these lines can then be used to pro-
vide additional meta information. We introduce an
additional tag, SECTION, with the following mean-
ing: a line beginning with # SECTION: specifies the
position in the document structure of the following
sentence. Thus for instance, in Figure 1, the line
# SECTION: title
specifies that the following sentence is a title.
5 An Pipeline for Research Articles in
Archeology
The pipeline currently in use in the PRU Portal
we are developing is based on the strategy just dis-
cussed. In this Section We discuss the pipeline in
more detail.
5.1 Modules
The pipeline for processing archaeological articles
integrates three main modules: a module for recov-
ering the logical structure of the documents, a mod-
ule for Italian and English POS tagging and a gen-
eral Name Entity Recognizer and finally, a Gazetteer
Based Name Entity Recognizer. The architecture of
the system is presented in figure 2. Each module
except the first one takes as input the output of the
previous module in the sequence.
1. Text Extraction. This module extracts the text
from PDF documents. Text extraction from
PDF is a notoriously challenging task. We ex-
perimented with many software packages and
obtained the best results with pdftotext. This is
a component of XPDF, an open source viewer
for PDF documents. pdftotext allows the extrac-
tion of the text content of PDF documents in a
variety of encodings. The main drawback of the
text extractor is that it does not always preserve
the original text order.
2. Language Identification. The archeology
repository contains articles written in one of
the two languages: Italian or English. This
module uses the TextCat language guesser10 for
guessing the language of sentences. The lan-
guage identification task is complicated by the
fact that some articles contain text in both lan-
guages: for example, an article written in En-
glish may have an Italian abstract and/or an Ital-
ian conclusion.
3. Logical Structure Identification. This mod-
ule extracts the logical structure of a document.
For example, it identifies important parts like
the title, the authors, the main headers, tables
or figures. For this task we train the SectLa-
bel component of ParsCit on the articles in the
archeology repository. Details on the training
process, the tag set and the performance of the
module are provided in section 5.2.
4. Linguistic Processing. A set of modules in the
pipeline then perform linguistic processing on
specific parts of the document (the Bibliogra-
phy Section is excluded for example). First En-
glish or Italian POS is carried out as appropri-
ate, followed by English or Italian NER. NER
adaptation techniques have been developed to
identify non-standard types entities that are im-
portant in the domain, such as Archeological
Sites and Archeological Cultures. (This work
is discussed elsewhere.)
5. Reference Parsing. This module relies on
the output of ParsCit software to update the
Archeology Database Bibliography table with
the parsed references for each article. First,
each parsed reference is corrected in an auto-
matic post processing step. Then, the module
checks, using a simple heuristic, if the entry al-
ready exists in the table and updates the table,
if appropriate, with the new record.
Finally, the documents processed by the pipeline
are indexed using the Lucene search engine.
5.2 Training the Logical Document Structure
Identifier
As mentioned in Section 5, we use ParsCit to find the
logical structure of the documents in the archeology
10
http://odur.let.rug.nl/~vannoord/TextCat/
59
Figure 2: The pipeline of the system for PDF article processing in the Archeology Domain
domain. ParsCit comes with general CRF trained
models; unfortunately, they do not perform well on
archeology documents. There are some particulari-
ties of archeology repository articles that require the
retraining of the models. First, as said before, the
text extracted from PDF is not perfect. Second, the
archeology articles contain many figures with bilin-
gual captions. Third, the articles have portions of
the texts in both languages: Italian and English. To
improve the parsing performance two models are
trained: the first model should capture the logical
documents structure for those documents that have
Italian as main language but might contain portions
in English (like the abstract or summary). The sec-
ond model is trained with documents that have En-
glish as main language but might contain fragments
in Italian (like abstract or summary).
The document structure annotation was per-
formed by a student in the archeology department,
and was checked by one of the authors. In total 55
documents have been annotated (35 with Italian as
main language, 20 with English as main Language).
The tagset used for the annotation was specifically
devised for archeology articles. However, as it can
be seen below most of the devised tags can also be
found in general scientific articles. In Table 1 we
present the tag set used for annotation. The column
"Tag Count" gives the number of each tag in the an-
notated documents.
In general the meaning of the tags is self-
explanatory with the possible exception of the
tag VolumeInfo, which reports information for vol-
ume the article is part of. An annotation exam-
ple using this tag is: "<VolumeInfo> Preistoria
Alpina v. 38 (2002) Trento 2002 ISSN 0393-0157
</VolumeInfo>". The volume information can be
further processed by extracting the volume number,
the year of the issue and the International Standard
Serial Number (ISSN). To asses the performance of
the trained models we performed a five fold cross-
validation. The results are reported in the table 2
and are obtained for each tag using the F1 measure
(1):
F1 =
2?P?R
P+R
(1)
The results obtained for the Archeology articles
are in line with those obtained by the authors of
ParsCit and reported in (Luong et al, 2011). The
tag categories for which the performance of the sys-
tem is bad are the multilingual tags (e.g. ItalianAb-
stract or Italian Summary in articles where the main
language is English). We will address this issue in
the future by adapting the language identifier to label
multilingual documents. We also noticed that many
mis-tagged titles, notes or section headers are split
on multiple lines after the text extraction stage. The
system performance might be further improved if a
pre-processing step immediately after the text extrac-
tion is introduced.
60
Tag Tag Count
ItalianFigureCaption 456
ItalianBodyText 347
EnglishFigureCaption 313
SectionHeader 248
EnglishTableCaption 58
ItalianTableCaption 58
Author 71
AuthorEmail 71
AuthorAddress 65
SubsectionHeader 50
VolumeInfo 57
Bibliography 55
English Summary 31
ItalianKeywords 35
EnglishKeywords 35
Title 55
ItalianSummary 29
ItalianAbstract 10
Table 25
EnglishAbstract 13
Note 18
Table 1: The tag set used for Archeology Article Annota-
tion.
6 Additional Applications for
Structure-Sensitive Pipelines
The pipeline discussed above can be used for a va-
riety of other types of documents?archeology doc-
uments from other collections, or documents from
other domains?by simply replacing the document
structure extractor. We also found however that the
pipeline is useful for a variety of other text-analysis
tasks. We briefly discuss these in turn.
6.1 Blogs and Microblogging platforms
Content creation platforms like blogs, microblogs,
community QA sites, forums, etc., contain user gen-
erated data. This data may be emotional, opin-
ionated, personal, and sentimental, and as such,
makes it interesting for sentiment analysis, opinion
retrieval, and mood detection. In their survey on
opinion mining and sentiment analysis Pang and Lee
(2008) report that logical structure can be used to uti-
lize the relationships between different units of con-
tent, in order to achieve a more accurate labeling;
Tag F1
ItalianFigureCaption 70
ItalianBodyText 90
EnglishFigureCaption 71
SectionHeader 90
EnglishTableCaption 70
ItalianTableCaption 75
Author 72
AuthorEmail 75
AuthorAddress 73
SubsectionHeader 65
VolumeInfo 85
Bibliography 98
English Summary 40
ItalianKeywords 55
EnglishKeywords 56
Title 73
ItalianSummary 40
ItalianAbstract 50
Table 67
EnglishAbstract 50
Note 70
Table 2: The Precision and Recall for the trained models.
e.g. the relationships between discourse participants
in discussions on controversial topics when respond-
ing are more likely to be antagonistic than to be re-
inforcing, or the way of quoting?a user can refer to
another post by quoting part of it or by addressing
the other user by name or user ID?in posts on politi-
cal debates hints at the perceived opposite end of the
political spectrum of the quoted user.
We are in the process of creating an annotated cor-
pus of blogs; the pipeline discussed in this paper
was easily adapted to pre-process this type of data
as well.
6.2 HTML pages
In the IR literature it has often been observed that
certain parts of document structure contain infor-
mation that is particularly useful for document re-
trieval. For instance, Kruschwitz (2003) automati-
cally builds domain models ? simple trees of related
terms ? from documents marked up in HTML to
assist users during search tasks by performing auto-
matic query refinements, and improves users? experi-
61
ence for browsing the document collection. He uses
term counts in different markup contexts like non-
paragraph text and running text, and markups like
bold, italic, underline to identify concepts and the
corresponding shallow trees. However, this domain-
independent method is suited for all types of data
with logical structure annotation and similar data
sources can be found in many places, e.g. corporate
intranets, electronic archives, etc.
6.3 Processing Wikipedia pages
Wikipedia, as a publicly available web knowledge
base, has been leveraged for semantic information
in much work, including from our lab. Wikipedia
articles consist mostly of free text, but also con-
tain different types of structured information, e.g. in-
foboxes, categorization and geo information, links
to other articles, to other wiki projects, and to exter-
nal Web pages. Preserving this information is there-
fore useful for a variety of projects.
7 Discussion and Conclusions
The main point of this paper is to argue that the field
should switch to structure-sensitive pipelines. These
are particularly crucial in digital library applications,
but novel type of documents require them as well.
We showed that such extension can be achieved
rather painlessly even in tabular-based pipelines pro-
vided they allow for meta-lines.
References
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of the Language Resources
and Evaluation Conference (LREC 08), May.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP
tools and applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics.
Min-Yuh Day, Richard Tzong-Han Tsai, Cheng-Lung
Sung, Chiu-Chen Hsieh, Cheng-Wei Lee, Shih-Hung
Wu, Kun-Pin Wu, Chorng-Shyong Ong, and Wen-Lian
Hsu. 2007. Reference metadata extraction using a hi-
erarchical knowledge representation framework. Deci-
sion Support Systems, 43(1):152?167, February.
Erik Hetzner. 2008. A simple method for citation meta-
data extraction using hidden markov models. In Pro-
ceedings of the 8th ACM/IEEE-CS joint conference
on Digital libraries, JCDL ?08, pages 280?284, New
York, NY, USA. ACM.
Nancy Ide. 1998. Corpus encoding standard: SGML
guidelines for encoding linguistic corpora. In Proceed-
ings of LREC, pages 463?70, Granada.
Yasuto Ishitani. 1999. Logical structure analysis of doc-
ument images based on emergent computation. In
Proceedings of International Conference on Document
Analysis and Recognition.
Michael Jewell. 2000. Paracite: An overview.
Udo Kruschwitz. 2003. An Adaptable Search System for
Collections of Partially Structured Documents. IEEE
Intelligent Systems, 18(4):44?52, July.
Kyong-Ho Lee, Yoon-Chul Choy, and Sung-Bae Cho.
2003. Logical structure analysis and generation for
structured documents: a syntactic approach. IEEE
transactions on knowledge and data engineering,
15(5):1277?1294, September.
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen
Kan. 2011. Logical structure recovery in scholarly
articles with rich document feature. Journal of Digital
Library Systems. Forthcoming.
Song Mao, Azriel Rosenfeld, and Tapas Kanungo. 2003.
Document Structure Analysis Algorithms: A Litera-
ture Survey.
Debashish Niyogi and Sargur N. Srihari. 1995.
Knowledge-based derivation of document logical
structure. In Proceedings of International Conference
on Document Analysis and Recognition, pages 472?
475.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008. The TextPro tool suite. In LREC, 6th edition of
the Language Resources and Evaluation Conference,
Marrakech (Marocco).
Richard Power, Donia Scott, and Nadjet Bouayad-Agha.
2003. Document Structure. Computational Linguis-
tics, 29(2):211?260, June.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using tranformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large Cor-
pora, pages 82?94.
Kristen M. Summers. 1998. Automatic discovery of log-
ical document structure. Ph.D. thesis, Cornell Univer-
sity.
Ian H. Witten, David Bainbridge, and David M. Nichols.
2003. How to build a digital library. Morgan Kauf-
mann.
62
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 11?19,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Open Book: a tool for helping ASD users? semantic comprehension
Eduard Barbu
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
ebarbu@ujaen.es
Maria Teresa Mart??n-Valdivia
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
maite@ujaen.es
Luis Alfonso Uren?a-Lo?pez
University of Jae?n
Paraje de Las Lagunillas
Jae?n, 23071, Spain
laurena@ujaen.es
Abstract
Persons affected by Autism Spectrum Dis-
orders (ASD) present impairments in so-
cial interaction. A significant percentile of
them have inadequate reading comprehension
skills. In the ongoing FIRST project we build
a multilingual tool called Open Book that
helps the ASD people to better understand the
texts. The tool applies a series of automatic
transformations to user documents to identify
and remove the reading obstacles to compre-
hension. We focus on three semantic compo-
nents: an Image component that retrieves im-
ages for the concepts in the text, an idiom de-
tection component and a topic model compo-
nent. Moreover, we present the personaliza-
tion component that adapts the system output
to user preferences.
1 Introduction
Autism Spectrum Disorders are widespread and af-
fect every 6 people in 10000 according to Autism
Europe site1. The disorder is chiefly characterized
by impairments in social interaction and by repet-
itive and stereotyped behaviour (Attwood, 2007).
People affected by ASD are not able to communi-
cate properly because they lack an adequate theory
of mind (Baron-Cohen, 2001). Therefore, they are
not able to infer the other persons? mental states:
beliefs, emotions or desires. This lack of empathy
prevents the people with ASD to have a fulfilled so-
cial life. Their inability to understand others leads
to the incapacity to communicate their wishes and
desires and to social marginalization.
1http://www.autismeurope.org/
The FIRST project seeks to make a small step
towards integration of ASD people in the informa-
tion society by addressing their reading comprehen-
sion ability. It is well known that many of the ASD
people have a wide range of language difficulties.
Psychological studies showed that they have prob-
lems understanding less common words (Gillispie,
2008), have difficulty comprehending polysemous
words (Fossett and Mirenda, 2006) and have trou-
bles dealing with figurative language (Douglas et al,
2011). The absence of good comprehension skills
impedes the ASD students to participate in curricu-
lum activities or to properly interact with their col-
leagues in chats or blogs. To enhance the reading
comprehension of ASD people we are developing a
software tool. It is built by partners in Academia and
Industry in close collaboration with teams of psy-
chologists and clinicians. It operates in a multilin-
gual setting and is able to process texts in English,
Spanish and Bulgarian languages. Based on litera-
ture research and on a series of studies performed
in the United Kingdom, Spain and Bulgaria with a
variety of autistic patients ranging from children to
adults the psychologists identified a series of obsta-
cles in reading comprehensions that the tool should
remove. From a linguistic point of view they can
be classified in syntactic obstacles (difficulty in pro-
cessing relative clauses, for example) and semantic
obstacles (difficulty in understanding rare or special-
ized terms or in comprehension of idioms, for exam-
ple). The tool applies a series of automatic transfor-
mations to user documents to identify and remove
the reading obstacles to comprehension. It also as-
sists the carers , persons that assist the ASD people
in every day life tasks, to correct the results of auto-
11
matic processing and prepare the documents for the
users. This paper will focus on three essential soft-
ware components related to semantic processing: a
software component that adds images to concepts
in the text, a software component that identifies id-
iomatic expressions and a component that computes
the topics of the document. Moreover, we present
the personalization component that adapts the sys-
tem output to user preferences. The rest of the paper
has the following structure: the next section briefly
presents other similar tools on the market. Section
3 presents a simple procedure for identifying the
obstacles ASD people have in reading comprehen-
sions. Section 4 shows the architecture of the seman-
tic processing components and the personalization
component. The last section draws the conclusions
and comments on the future work. Before present-
ing the main part of the article we make a brief note:
throughout the paper we will use whenever possible
the term ?user? instead of ASD people or patients.
2 Related Work
A number of software tools were developed to sup-
port the learning of ASD people. Probably the
most known one is Mind Reading2, a tool that
teaches human emotions using a library of 412 ba-
sic human emotions illustrated by images and video.
Other well known software is VAST-Autism3, a tool
that supports the understanding of linguistic units:
words, phrase and sentences by combining spoken
language and images. ?Stories about me? is an IPad
application4 that allows early learners to compose
stories about themselves. All these tools and others
from the same category are complementary to Open
Book. However, they are restricted to pre-stored
texts and not able to accommodate new pieces of
information. The main characteristics that sets aside
our tool is its scalability and the fact that it is the only
tool that uses NLP techniques to enhance text com-
prehension. Even if the carers correct the automatic
processing output, part of their work is automatized.
2http://www.jkp.com/mindreading/index.php
3http://a4cwsn.com/2011/03/vast-autism-1-core/
4http://www.limitedcue.com/our-apps/
3 Obstacles in text comprehension
Most of the automatic operations executed by the
Open Book tool are actually manually performed by
the carers. They simplify the parts of the text that are
difficult to understand. We compared the texts be-
fore and after the manual simplification process and
registered the main operations. The main simplifica-
tion operations ordered by frequency performed by
carers for 25 Spanish documents belonging to dif-
ferent genders: rent contracts, newspaper articles,
children literature, health care advices, are the fol-
lowing:
1. Synonymous (64 Operations). A noun or an ad-
jective is replaced by its less complex synonym.
2. Sentence Splitting (40 Operations). A long sen-
tence is split in shorter sentences or in a bullet
list.
3. Definition (34 Operations). A difficult term is
explained using Wikipedia or a dictionary.
4. Near Synonymous (33 Operations). The term
is replaced by a near synonym.
5. Image (27 Operations) A concept is illustrated
by an image.
6. Explanation (24 Operations). A sentence is
rewritten using different words.
7. Deletion (17 Operations). Parts of the sentence
are removed.
8. Coreference(17 Operations). A coreference
resolution is performed.
9. Syntactic Operation (9 Operations). A trans-
formation on the syntactic parse trees is per-
formed.
10. Figurative Language (9 Operations). An idiom
or metaphor is explained.
11. Summarization (3 Operations). The content of
a sentence or paragraph is summarized.
The most frequent operations with the exception
of Sentence Splitting are semantic in nature: replac-
ing a word with a synonym, defining the difficult
12
terms. The only obstacle that cannot be tackled au-
tomatically is Explanation. The Explanation entails
interpretation of the sentence or paragraph and can-
not be reduced to simpler operations.
A similar inventory has been done in English.
Here the most frequent operation are Sentence Split-
ting, Synonyms and Definition. The operations are
similar across English and Spanish but their ordering
differs slightly.
4 The Semantic System
In this paper we focus on three semantic compo-
nents meant to augment the reading experience of
the users. The components enhance the meaning
of documents assigning images to the representa-
tive and difficult concepts, detecting and explaining
the idiomatic expressions or computing the topics to
which the documents belong.
In addition to these components we present an-
other component called Personalization. Strictly
speaking, the personalization is not related to se-
mantic processing per se but, nevertheless, it has
an important role in the final system. Its role
is to aggregate the output of all software compo-
nents,including the three ones mentioned above, and
adapt it according to user?s needs.
All the input and output documents handled by
NLP components are GATE (Cunningham et al,
2011) documents. There are three reasons why
GATE documents are preferred: reusability, extensi-
bility and flexibility. A GATE document is reusable
because there are many software components devel-
oped both in academy and industry, most of them
collected in repositories by University of Sheffield,
that work with this format. A GATE document is
extensible because new components can add their
annotations without modifying previous annotations
or the content of the document. Moreover, in case
there is no dependence between the software com-
ponents the annotations can be added in parallel. Fi-
nally, a GATE document is flexible because it al-
lows the creation of various personalization work-
flows based on the specified attributes of the anno-
tations. The GATE document format is inspired by
TIPSTER architecture design5 and contains in ad-
dition to the text or multimedia content annotations
5http://www.itl.nist.gov/iaui/894.02/related projects/tipster/
grouped in Annotation Sets and features. The GATE
format requires that an annotation has the following
mandatory features: an id, a type and a span. The
span defines the starting and the ending offsets of
the annotation in the document text.
Each developed software component adds its an-
notations in separate name annotation sets. The
components are distributed and exposed to the out-
side world as SOAP web services. Throughout the
rest of the paper we will use interchangeably the
terms: component, software component and web
service.
For each semantic component we discuss:
? The reasons for its development. In general,
there are two reasons for the development of a
certain software component: previous studies
in the literature and studies performed by our
psychologists and clinicians. In this paper we
will give only motivations from previous stud-
ies because the discussion of our clinicians and
psychologist studies are beyond the purpose of
this paper.
? Its architecture. We present both the foreseen
characteristics of the component and what was
actually achieved at this stage but we focus on
the latter.
? The annotations it added. We discuss all the
features of the annotations added by each com-
ponent.
4.1 The Image Web Service
In her landmark book, ?Thinking in Pictures: My
Life with Autism?, Temple Grandin (1996), a scien-
tist affected by ASD, gives an inside testimony for
the importance of pictures in the life of ASD peo-
ple:
?Growing up, I learned to convert abstract ideas
into pictures as a way to understand them. I visu-
alized concepts such as peace or honesty with sym-
bolic images. I thought of peace as a dove, an Indian
peace pipe, or TV or newsreel footage of the signing
of a peace agreement. Honesty was represented by
an image of placing one?s hand on the Bible in court.
A news report describing a person returning a wallet
with all the money in it provided a picture of honest
behavior.?
13
Grandin suggests that not only the ASD people
need images to understand abstract concepts but that
most of their thought process is visual. Other studies
document the importance of images in ASD: Kana
and colleagues (2006) show that the ASD people use
mental imagery even for comprehension of low im-
agery sentences. In an autobiographic study Grandin
(2009) narrates that she uses language to retrieve
pictures from the memory in a way similar to an im-
age retrieval system.
The image component assigns images to concepts
in the text and to concepts summarizing the meaning
of the paragraphs or the meaning of the whole doc-
ument. Currently we are able to assign images to
the concepts in the text and to the topics computed
for the document. Before retrieving the images from
the database we need a procedure for identifying
the difficult concepts. The research literature helps
with this task, too. It says that our users have diffi-
culty understanding less common words (Lopez and
Leekam, 2003) and that they need word disambigua-
tion (Fossett and Mirenda, 2006).
From an architectural point of view the Image
Web Service incorporates three independent sub-
components:
? Document Indexing. The Document Index-
ing sub-component indexes the document con-
tent for fast access and stores all offsets of the
indexing units. The indexed textual units are
words or combinations of words (e.g., terms).
? Difficult Concepts Detection. The difficult
concepts are words or terms (e.g. named enti-
ties) disambiguated against comprehensive re-
sources: like Wordnet and Wikipedia. This
sub-component formalizes the notion ?difficult
to understand? for the users. It should be based
on statistical procedures for identifying rare
terms as well as on heuristics for evaluating the
term complexity from a phonological point of
view. For the time being the sub-component
searches in the document a precompiled list of
terms.
? Image Retrieval. This sub-component re-
trieves the images corresponding to difficult
concepts from image databases or from web
searching engines like Google and Bing.
The Image Web Service operates in automated
mode or in on-demand mode. In the automated
mode a document received by the Image Web Ser-
vice is processed according to the working flow in
Figure 1. In the on-demand mode the user high-
lights the concepts (s)he considers difficult and the
web service retrieves the corresponding image or set
of images. The difference between the two modes
of operations is that in the on-demand mode the dif-
ficult concept detection is performed manually.
Once the GATE document is received by the sys-
tem it is tokenized, POS (Part of Speech) tagged
and lemmatized (if these operations were not already
performed by other component) by a layer that is not
presented in Figure 1. Subsequently, the document
content is indexed by Document Indexing subcom-
ponent. For the time being the terms of the doc-
ument are disambiguated against Wordnet. The Im-
age Retrieval component retrieves the corresponding
images from the image database.
The current version uses the ImageNet Database
(Deng et al, 2009) as image database. The Ima-
geNet database pairs the synsets in Princeton Word-
net with images automatically retrieved from Web
and cleaned with the aid of Mechanical Turk. Be-
cause the wordnets for Spanish and Bulgarian are ei-
ther small or not publicly available future versions of
the Web Service will disambiguate the terms against
Wikipedia articles and retrieve the image illustrating
the article title. All annotations are added in ?Im-
ageAnnotationSet?. An annotation contains the fol-
lowing features:
? Image Disambiguation Confidence is the con-
fidence of the WSD (Word Sense Disambigua-
tion) algorithm in disambiguating a concept.
? Image URL represents the URL address of the
retrieved image
? Image Retrieval Confidence is the confidence
of assigning an image to a disambiguated con-
cept.
In the on-demand mode the images are also re-
trieved from Google and Bing Web Services and
the list of retrieved images is presented to the carer
and/or to the users. The carer or user selects the im-
age and inserts it in the appropriate place in the doc-
ument.
14
Figure 1: The Image Web Service.
4.2 The Idiom Detection Web Service
In the actual linguistic discourse and lexicographical
practice the term ?idiom? is applied to a fuzzy cat-
egory defined by prototypical examples: ?kick the
bucket?, ?keep tabs on?, etc. Because we cannot
provide definitions for idioms we venture to spec-
ify three important properties that characterize them
(Nunberg et al, 1994) :
? Conventionality.The meaning of idioms are not
compositional.
? Inflexibility. Idioms appear in a limited range
of syntactic constructions.
? Figuration. The line between idioms and
other figurative language is somewhat blurred
because other figurative constructions like
metaphors: ?take the bull by the horns? or hy-
perboles: ?not worth the paper it?s printed on?
are also considered idioms.
The figurative language in general and the id-
ioms in particular present particular problems for
our users as they are not able to grasp the meaning
of these expressions (Douglas et al, 2011). To facil-
itate the understanding of idiomatic expressions our
system identifies the expressions and provide defini-
tions for them.
The actual Idiom Web Service finds idiomatic ex-
pressions in the user submitted documents by simple
text matching. The final version of Idiom Web Ser-
vice will use a combination of trained models and
hand written rules for idiom detection. Moreover, it
is also envisaged that other types of figurative lan-
guage like metaphors could be detected. At the mo-
ment the detection is based on precompiled lists of
idioms and their definitions. Because the compo-
nent works by simple text matching, it is language
independent. Unlike the actual version of the Idiom
Web Service the final version should be both lan-
guage and domain dependent. The architecture of
this simple component is presented in Figure 2 .
Figure 2: The Idiom Web Service.
The GATE input document is indexed by the doc-
ument indexing component for providing fast ac-
cess to its content. For each language we compiled
list of idioms from web sources, dictionaries and
Wikipedia. All idiom annotations are added in the
?IdiomAnnotationSet?. An annotation contains the
following features:
? Idiom Confidence represents the confidence the
algorithm assigns to a particular idiom detec-
tion.
? Definition represents the definition for the ex-
tracted idiom.
4.3 The Topic Models Web Service
The mathematical details of the topics models are
somewhat harder to grasp but the main intuition be-
hind is easily understood. Consider an astrobiology
document. Most likely it will talk about at least three
topics: biology, computer models of life and astron-
omy. It will contain words like: cell, molecules, life
related to the biology topic; model, computer, data,
number related to computer models of life topic and
star, galaxy, universe, cluster related with astronomy
topic. The topic models are used to organize vast
15
collections of documents based on the themes or dis-
courses that permeate the collection. From a practi-
cal point of view the topics can be viewed as clus-
ters of words (those related to the three topics in the
example above are good examples) that frequently
co-occur in the collection. The main assumption be-
hind Latent Dirichlet Allocation (LDA) (Blei et al,
2003), the simplest topic model technique, is that
the documents in the collections were generated by a
random process in which the topics are drawn from
a given distribution of topics and words are drawn
from the topics themselves. The task of LDA and
other probabilistic topic models is to construct the
topic distribution and the topics (which are basically
probability distributions over words) starting with
the documents in the collection.
The Topic Models Web Service is based on an
implementation of LDA. It assigns topics to the
user submitted documents, thus informing about the
themes traversing the documents and facilitating the
browsing of the document repository. The topics
themselves perform a kind of summarization of doc-
uments showing, before actual reading experience,
what the document is about.
The architecture of the Topic Models Web Service
is presented in Figure 3.
Figure 3: The Topic Model Web Service.
Once a document is received it is first dispatched
to the Feature Extraction Module where it is POS
tagged and lemmatized and the relevant features are
extracted. As for training models, the features are
all nouns, name entities and verbs in the document.
Then the Topic Inferencer module loads the appro-
priate domain model and performs the inference and
assigns the new topics to the document. There are
three domains/genders that the users of our system
are mainly interested in: News, Health Domain and
Literature. For each of these domains we train topic
models in each of the three languages of the project.
Of course the system is easily extensible to other do-
mains. Adding a new model is simply a matter of
loading it in the system and modifying a configura-
tion file.
The output of the Web System is a document in
the GATE format containing the most important top-
ics and the most significant words in the topics. The
last two parameters can be configured (by default
they are set to 3 and 5 respectively). Unlike the an-
notations for the previous components the annota-
tion for Topic Model Web Service are not added for
span of texts in the original document. This is be-
cause the topics are not necessarily words belonging
to the original document. Strictly speaking the top-
ics are attributes of the original document and there-
fore they are added in the ?GateDocumentFeatures?
section. An example of an output document contain-
ing the section corresponding to the document topics
is given in Figure 4.
Figure 4: The GATE Document Representation of the
Computed Topic Model.
Currently we trained three topic models cor-
responding to the three above mentioned do-
mains/genres for the Spanish language:
? News. The corpus of news contains more
than 500.000 documents downloaded from the
web pages of the main Spanish newspapers (El
Mundo, El Pais, La Razon, etc. . . ). The topic
model is trained using a subset of 50.000 docu-
ments and 400 topics. The optimum number of
documents and topics will be determined when
16
the users test the component. However, one
constraint on the number of documents to use
for model training is the time required to per-
form the inference: if the stored model is too
big then the inference time can exceed the time
limit the users expect.
? Health Domain. The corpus contains 7168
Spanish documents about general health is-
sues (healthy alimentation, description of the
causes and treatments of common diseases,
etc.) downloaded from medlineplus portal. The
topic model is trained with all documents and
100 topics. In the future we will extend both
the corpus and the topic model.
? Literature. The corpus contains literature in
two genders: children literature (121 Spanish
translation of Grimm brothers stories) and 336
Spanish novels. Since for the time being the
corpus is quite small we train a topic model
with 20 topics just for the system testing pur-
poses.
For the English and the Bulgarian language we
have prepared corpora for each domain but we have
not trained a topic model yet. To create the training
model all corpora should be POS tagged, lemma-
tized and the name entities recognized. The features
for training the topic model are all nouns, name en-
tities and verbs in the corpora.
4.4 Personalization
The role of the Personalization Web Service is to
adapt the output of the system to the user?s expe-
rience. This is achieved by building both static and
dynamic user profiles. The static user profiles con-
tain a number of parameters that can be manually
set. Unlike the static profiles, the dynamic ones con-
tain a series of parameters whose values are learnt
automatically. The system registers a series of ac-
tions the users or carers perform with the text. For
example, they can accept or reject the decisions per-
formed by other software components. Based on
editing operations a dynamic user profile will be
built incrementally by the system. Because at this
stage of the project the details of the dynamic pro-
file are not yet fully specified we focus on the static
profile in this section.
The architecture of the Personalization compo-
nent is presented in Figure 5.
Figure 5: The Personalization Web Service.
In addition to the web services presented in the
previous sections (The Idiom Web Service and The
Image Web Service) the Personalization Web Ser-
vice receives input from Anaphora Web Service and
Syntax Simplification Web Service. The Anaphora
component resolves the pronominal anaphora and
the Syntax Simplification component identifies and
eliminates difficult syntactic constructions. The Per-
sonalization component aggregates the input from
all web services and based on the parameters speci-
fied in the static profile (the wheel in Figure 5) trans-
forms the aggregate document according to the user
preferences. The personalization parameters in the
static profile are the following:
1. Image Disambiguation Confidence. The image
annotation is dropped when the corresponding
concept disambiguation confidence is less than
the threshold.
2. Image Retrieval Confidence. The image an-
notation is dropped when the assigned image
is retrieved with a confidence lower than the
threshold.
3. Idiom Confidence. The idiom annotation is
dropped when the assigned idiom confidence is
less than the threshold.
4. Anaphora Confidence. The pronominal
anaphora annotations are dropped when the
anaphor is solved with a confidence less than
the threshold.
5. Anaphora Complexity. The parameter assess
the complexity of anaphors. If the anaphora
17
complexity score is less than the specified
threshold it drops the resolved pronominal
anaphora.
6. Syntactic Complexity. It drops all annotations
for which the syntactic complexity is less than
the threshold.
The user can also reject the entire output of a cer-
tain web service if he does not need the functionality.
For example, the user can require to display or not
the images, to resolve or not the anaphora, to sim-
plify the sentences or not, etc. In case the output of
a certain web service is desired the user can spec-
ify the minimum level of confidence accepted. Any
annotation that has a level of confidence lower than
the specified threshold will be dropped. In addition
to the parameters related to document content the
static profile includes parameters related to graphi-
cal appearance (e.g. fonts or user themes) that are
not discussed here.
5 Conclusions and further work
In this paper we presented three semantic compo-
nents to aid ASD people to understand the texts.
The Image Component finds, disambiguates and as-
signs Images to difficult terms in the text or re-
lated to the text. It works in two modes: auto-
mated or on-demand. In the automated mode a doc-
ument is automatically enriched with images. In
the on-demand mode the user highlights the con-
cepts (s)he considers difficult and the web service
retrieves the corresponding images. Further devel-
opment of this component will involve disambigua-
tion against Wikipedia and retrieval of images from
the corresponding articles. The Idiom Component
finds idioms and other figurative language expres-
sions in the user documents and provides definitions
for them. Further versions of the component will
go beyond simple matching and will identify other
categories of figurative language. The Topic Mod-
els component helps organizing the repository col-
lection by computing topics for the user documents.
Moreover it also offers a summarization of the doc-
ument before the actual reading experience. Finally
the Personalization component adapts the system
output to the user experience. Future versions of the
component will define dynamic user profiles in addi-
tion to the static user profiles in the current version.
Our hope is that the Open Book tool will be useful
for other parts of populations that have difficulties
with syntactic constructions or semantic processing,
too.
Acknowledgments
We want to thank the three anonymous reviewers
whose suggestions helped improve the clarity of this
paper. This work is partially funded by the European
Commission under the Seventh (FP7 - 2007-2013)
Framework Program for Research and Technologi-
cal Development through the FIRST project (FP7-
287607). This publication reflects only the views
of the authors, and the Commission cannot be held
responsible for any use which may be made of the
information contained therein.
References
Tony Attwood. 2007. The complete guide to Asperger
Syndrome. Jessica Kingsley Press.
Simon Baron-Cohen. 2001. Theory of mind and autism:
a review. Int Rev Ment Retard, 23:169?184.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
Jia Deng, Wei Dong, R. Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. 2009. ImageNet: A large-scale hierarchi-
cal image database. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on,
pages 248?255. IEEE, June.
K.H. Douglas, K.M. Ayres, J. Langone, and V.B. Bram-
lett. 2011. The effectiveness of electronic text and
pictorial graphic organizers to improve comprehension
related to functional skills. Journal of Special Educa-
tion Technology, 26(1):43?57.
Brenda Fossett and Pat Mirenda. 2006. Sight word
reading in children with developmental disabilities:
A comparison of paired associate and picture-to-text
matching instruction. Research in Developmental Dis-
abilities, 27(4):411?429.
William Matthew Gillispie. 2008. Semantic Process-
ing in Children with Reading Comprehension Deficits.
Ph.D. thesis, University of Kansas.
18
Temple Grandin. 1996. Thinking In Pictures: and Other
Reports from My Life with Autism. Vintage, October.
Temple Grandin. 2009. How does visual thinking work
in the mind of a person with autism? a personal ac-
count. Philosophical Transactions of the Royal So-
ciety B: Biological Sciences, 364(1522):1437?1442,
May.
Rajesh K. Kana, Timothy A. Keller, Vladimir L.
Cherkassky, Nancy J. Minshew, and Marcel Adam
Just. 2006. Sentence comprehension in autism:
Thinking in pictures with decreased functional con-
nectivity.
B. Lopez and S. R. Leekam. 2003. Do children with
autism fail to process information in context ? Journal
of child psychology and psychiatry., 44(2):285?300,
February.
Geoffrey Nunberg, Ivan Sag, and Thomas Wasow. 1994.
Idioms. Language.
19
