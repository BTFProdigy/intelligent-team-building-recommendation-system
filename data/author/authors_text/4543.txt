Unit Completion for a Computer-aided Translation 
System 
Ph i l ippe  Lang la i s ,  George  Foster  and  Guy  Lapa lme 
RAL I  / D IRO 
Universit6 de Montrea l  
C.P. 6128, succursale Centre-vi l le 
Montra l  (Qubec) ,  Canada,  H3C 3J7 
{ f elipe,f oster, lapalme }@iro. umontreal, ca 
Typing 
Abst ract  
This work is in the context of TRANSTYPE, a sys- 
tem that observes its user as he or she types a trans- 
lation and repeatedly suggests completions for the 
text already entered. The user may either accept, 
modify, or ignore these suggestions. We describe the 
design, implementation, and performance of a pro- 
totype which suggests completions of units of texts 
that are longer than one word. 
1 I n t roduct ion  
TRANSTYPE is part of a project set up to explore 
an appealing solution to Interactive Machine Trans- 
lation (IMT). In constrast to classical IMT systems, 
where the user's role consists mainly of assisting the 
computer to analyse the source text (by answering 
questions about word sense, ellipses, phrasal attach- 
ments, etc), in TRANSTYPE the interaction is direct- 
ly concerned with establishing the target ext. 
Our interactive translation system works as fol- 
lows: a translator selects a sentence and begins typ- 
ing its translation. After each character typed by 
the translator, the system displays a proposed com- 
pletion, which may either be accepted using a spe- 
cial key or rejected by continuing to type. Thus 
the translator remains in control of the translation 
process and the machine must continually adapt it- 
s suggestions in response to his or her input. We 
are currently undertaking a study to measure the 
extent o which our word-completion prototype can 
improve translator productivity. The conclusions of 
this study will be presented elsewhere. 
The first version of TrtANSTYPE (Foster et al, 
1997) only proposed completions for the current 
word. This paper deals with predictions which ex- 
tend to the next several words in the text. The po- 
tential gain from multiple-word predictions can be 
appreciated in the one-sentence translation task re- 
ported in table 1, where a hypothetical user saves 
over 60% of the keystrokes needed to produce a 
translation i a word completion scenario, and about 
85% in a "unit" completion scenario. 
In all the figures that follow, we use different fonts 
to differentiate he various input and output: italics 
are used for the source text, sans-serif for characters 
typed by the user and typewr i te r - l i ke  for charac- 
ters completed by the system. 
The first few lines of the table 1 give an idea of 
how TransType functions. Let us assume the unit s- 
cenario (see column 2 of the table) and suppose that 
the user wants to produce the sentence "Ce projet 
de loi est examin~ ~ la chambre des communes" as a 
translation for the source sentence "This bill is ex- 
amined in the house of commons". The first hypoth- 
esis that the system produces before the user enters 
a character is lo i  (law). As this is not a good guess 
from TRANSTYPE the user types the first character 
(c) of the words he or she wants as a translation. 
Taking this new input into account, TRANSTYPE 
then modifies its proposal so that it is compatible 
whith what the translator has typed. It suggests 
the desired sequence ce projet de Ioi, which the user 
can simply validate by typing a dedicated key. Con- 
tinuing in this way, the user and TRANSTYPE alter- 
nately contribute to the final translation. A screen 
copy of this prototype is provided in figure 1. 
2 The  Core  Eng ine  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator which 
assigns probabilistic scores to completion hypotheses 
and a generator which uses the evaluation function 
to select he best candidate for completion. 
2.1 The  Eva luator  
The evaluator is a function p(t\[t', s) which assigns to 
each target-text unit t an estimate of its probability 
given a source text s and the tokens t' which precede 
t in the current ranslation of s. 1 Our approach to 
modeling this distribution is based to a large extent 
on that of the IBM group (Brown et al, 1993), but 
it differs in one significant aspect: whereas the IB- 
M model involves a "noisy channel" decomposition, 
we use a linear combination of separate prediction- 
s from a language model p(tlt ~) and a translation 
model p(tls ). Although the noisy channel technique 
1We assume the existence of a determinist ic  procedure for 
tokenizing the target text. 
135 
This bill is examined in the house of commons 
word-completion task unit-completion task 
ce 
projet 
de 
Ioi 
est 
examin~ 
chambre 
des 
communes 
preL completions 
ce+ / lo i  ? C/' 
p+ /es t ?  p / ro je t  
d+ / t rbs  ? d/e 
I+ / t=~s ? I /o i  
e+ /de ? e / s t  
e+ /en ? e/xamin6 
~+ /par ? ~/ 1~ 
+ /chambre 
de+ /co,~unes ? d/e 
+ /communes 
? de/s 
pref. completions 
c-l- /loJ. ? c/e pro je t  de 1oi 
e+ /de ? e / s t  
ex+ /~ la  chambre des communes. 
+ /b l a  chambre des con~unes 
e/n ? ex /min~ 
Table 1: A one-sentence s ssion illustrating the word- and unit-completion tasks. The first column indicates 
the target words the user is expected to produce. The next two columns indicate respectively the prefixes 
typed by the user and the completions proposed by the system in a word-completion task. The last two 
columns provide the same information for the unit-completion task. The total number of keystrokes for 
both tasks is reported in the last line. + indicates the acceptance key typed by the user. A completion is
denoted by a/13 where a is the typed prefix and 13 the completed part. Completions for different prefixes 
are separated by ?. 
is powerful, it has the disadvantage that p(slt' , t) is 
more expensive to compute than p(tls ) when using 
IBM-style translation models. Since speed is cru- 
cial for our application, we chose to forego the noisy 
channel approach in the work described here. Our 
linear combination model is described as follows: 
pCtlt',s) = pCtlt') a(t ' ,s)  + pCtls) \[1 - exit',s)\] (1) 
? ~ ? ? v J 
language translation 
where a(t', s) E \[0, 1\] are context-dependent inter- 
polation coefficients. For example, the translation 
model could have a higher weight at the start of a 
sentence but the contribution of the language mod- 
el might become more important in the middle or 
the end of the sentence? A study of the weightings 
for these two models is described elsewhere? In the 
work described here we did not use the contribution 
of the language model (that is, a(t' ,  s) = O, V t', s). 
Techniques for weakening the independence as- 
sumptions made by the IBM models 1 and 2 have 
been proposed in recent work (Brown et al, 1993; 
Berger et al, 1996; Och and Weber, 98; Wang and 
Waibel, 98; Wu and Wong, 98). These studies report 
improvements on some specific tasks (task-oriented 
limited vocabulary) which by nature are very differ- 
ent from the task TRANSTYPE is devoted to. Fur- 
thermore, the underlying decoding strategies are too 
time consuming for our application? We therefore 
use a translation model based on the simple linear in- 
terpolation given in equation 2 which combines pre- 
dictions of two translation models - -  Ms and M~ - -  
both based on IBM-like model 2(Brown et al, 1993). 
Ms was trained on single words and Mu, described 
in section 3, was trained on both words and units. 
- -  _ (2 )  
word unit 
where Ps and Pu stand for the probabilities given re- 
spectively by Ms and M~. G(s) represents he new 
sequence of tokens obtained after grouping the to- 
kens of s into units. The grouping operator G is 
illustrated in table 2 and is described in section 3. 
2.2  The  Generator  
The task of the generator is to identify units that 
match the current prefix typed by the user, and pick 
the best candidate according to the evaluator. Due 
to time considerations, the generator introduces a
division of the target vocabulary into two parts: a 
small active component whose contents are always 
searched for a match to the current prefix, and a 
much larger passive part over (380,000 word form- 
s) which comes into play only when no candidates 
are found in the active vocabulary. The active part 
is computed ynamically when a new sentence is s- 
elected by the translator. It is composed of a few 
entities (tokens and units) that are likely to appear 
in the translation. It is a union of the best can- 
didates provided by each model Ms and M~ over 
the set of all possible target tokens (resp. units) 
that have a non-null translation probability of being 
translated by any of the current source tokens (resp. 
units). Table 2 shows the 10 most likely tokens and 
units in the active vocabulary for an example source 
sentence. 
136 
that.  is ? what .  the . p r ime,  minister . said 
? and .  i ? have.  outlined? what .  has .  
happened . since? then . .  
c' - est. ce -que ,  le- premier - ministre, a- 
d i t . , .e t . j ' ,  ai. r4sum4- ce. qui .s ' -  est- 
produit - depuis ? . 
g(s) that is what ? the prime minister said ? , and i 
? have . outlined ? what has happened ? since 
then ? . 
As 
A~ 
? ? ? es t  ? ce  ? m in i s t re  ? que .  e t  ? a ? p remier  
l i e  
ce  qu i  s' es t  p rodu i t  ? e t  je  - c '  es t  ce  que .  vo i l~  
ce  que  ? qu '  es t  - c '  es t  ? ,  e t  ? le p remier  min is t re  
d i sa i t  
Table 2: Role of the generator for a sample pair of 
sentences (t is the translation of s in our corpus). 
G(s) is the sequence of source tokens recasted by 
the grouping operator G. A8 indicates the 10 best 
tokens according to the word model, Au the 10 best 
units according to the unit model. 
3 Mode l ing  Un i t  Assoc ia t ions  
Automatically identifying which source words or 
groups of words will give rise to which target words 
or groups of words is a fundamental problem which 
remains open. In this work, we decided to proceed 
in two steps: a) monolingually identifying roups of 
words that would be better handled as units in a giv- 
en context, and b) mapping the resulting source and 
target units. To train our unit models, we used a 
segment of the Hansard corpus consisting of 15,377 
pairs of sentences, totaling 278,127 english token- 
s (13,543 forms) and 292,865 french tokens (16,399 
forms). 
3.1 F inding Monol ingual  Uni ts  
Finding relevant units in a text has been explored in 
many areas of natural anguage processing. Our ap- 
proach relies on distributional and frequency statis- 
tics computed on each sequence of words found in a 
training corpus. For sake of efficiency, we used the 
suffix array technique to get a compact representa- 
tion of our training corpus. This method allows the 
efficient retrieval of arbitrary length n-grams (Nagao 
and Mori, 94; Haruno et al, 96; Ikehara et al, 96; 
Shimohata et al, 1997; Russell, 1998). 
The literature abounds in measures that can help 
to decide whether words that co-occur are linguisti- 
cally significant or not. In this work, the strength of 
association of a sequence of words w\[ = w l , . . . ,  wn 
is computed by two measures: a likelihood-based one 
p(w'~) (where g is the likelihood ratio given in (Dun- 
ning, 93)) and an entropy-based one e(w'~) (Shimo- 
hata et al, 1997). Letting T stand for the training 
text and m a token: 
p(w~) = argming(w~, uS1  ) (3) 
ie\]l,n\[ 
e(w'~) = 0.5x  +k 
~rnlw,~meT h ( Ireq(w'~ m) k Ir~q(wT) \] 
Intuitively, the first measurement accounts for the 
fact that parts of a sequence of words that should 
be considered as a whole should not appear often by 
themselves. The second one reflects the fact that a 
salient unit should appear in various contexts (i.e. 
should have a high entropy score). 
We implemented a cascade filtering strategy based 
on the likelihood score p, the frequency f ,  the length 
l and the entropy value e of the sequences. A 
first filter (.~"1 (lmin, fmin, Pmin, emin)) removes any 
sequence s for which l (s) < lmin or p(s) < Pmin 
or e(s) < e,nin or f ( s )  < fmin.  A second filter 
(~'2) removes sequences that are included in pre- 
ferred ones. In terms of sequence reduction, apply- 
ing ~1 (2, 2, 5.0, 0.2) on the 81,974 English sequences 
of at least two tokens een at least twice in our train- 
ing corpus, less than 50% of them (39,093) were fil- 
tered: 17,063 (21%) were removed because of their 
low entropy value, 25,818 (31%) because of their low 
likelihood value. 
3.2 Mapping 
Mapping the identified units (tokens or sequences) to 
their equivalents in the other language was achieved 
by training a new translation model (IBM 2) us- 
ing the EM algorithm as described in (Brown et al, 
1993). This required grouping the tokens in our 
training corpus into sequences, on the basis of the 
unit lexicons identified in the previous tep (we will 
refer to the results of this grouping as the sequence- 
based corpus). To deal with overlapping possibilities, 
we used a dynamic programming scheme which opti- 
mized a criterion C given by equation 4 over a set S 
of all units collected for a given language plus all sin- 
gle words. G(w~) is obtained by returning the path 
that maximized B(n) .  We investigated several C- 
criteria and we found C~--a length-based measurc 
to be the most satisfactory. Table 2 shows an output 
of the grouping function. 
Oi l  i=o  
B( i )  = argmax 
/~\[1,i\[ ,w~_les ) + B( i  - I - 1) (4) 
0 i f j<=i  
with: Cl (w~)= j - - i  + l e lse 
137 
source unit (s) 
we have 1748 
we must 720 
this bill 640 
people of canada 282 
mr. speaker : 269 
what is happening 190 
of course , 178 
is it the pleasure of the house to 14 
adopt the 
the world 
child care 
the free trade agreement 
post-secondary education 
the first time 
the canadian aviation safety board 
the next five years 
the people of china 
f(8) target units (\[a,p\]) 
\[nous,0.49\] \[avons,0.41\] \[, nous avons,0.07\] 
\[nous devons,0.61\] \[ilrant,0.19\] [nous,0.14\] 
\[ce projet de 1oi,0.35\] \[projet de loi .,0.21\] [projet de loi,0.18\] 
\[les canadiens,0.26\] \[des canadiens,0.21\] \[la population,0.07\] 
\[m. le prdsident :,0.80\] [a,0.07\] \[h la,0.06\] 
Ice qui se passe,0.21\] Ice qui se,0.16\] [et,0.15\] 
\[dvidemment ,0.26\] \[naturellement,0.08\] \[bien stir,0.08\] 
\[plait-il h la chambre d' adopter,0.49\] \[la motion ?,0.42\] [motion 
?,0.04\] 
201 \[le monde,O.46\] [du monde,O.33\] lie monde entier,O.19\] 
86 lies garderies,O.59\] \[la garde d' enfants,O.23\] \[des services de 
garde d' enfants,O.13\] 
75 \[1' accord de libre-dchange,O.96\] \[la ddcision du gatt,O.04\] 
66 \[1' euseignement postsecondaize,O.75\] \[1' dducation postsec- 
ondaire,O.15\] \[des fonds,O.06\] 
62 \[la premiere fois,l.00\] 
36 lie bureau canadien de la s~urit~ adrienne,O.55\] \[du bureau cana- 
dien de la sdcurit~ adrienne,O.31\] \[1'un,O.14\] 
26 \[au cours des cinq prochaines ann~es,O.53\] \[cinq prochaines an- 
ndes,O.27\] \[25 milliards de d ollars,O.lO\] 
17 \[le peuple chinois,0.38\] \[la population chinoise,0.25\] \[les chi- 
nois,O.13\] 
Table 3: Bilingual associations. The first column indicates a source unit, the second one its frequency in the 
training corpus. The third column reports its 3-best ranked target associations (a being a token or a unit, 
p being the translation probability). The second half of the table reports NP-associations obtained after the 
filter described in the text. 
We investigated three ways of estimating the pa- 
rameters of the unit model. In the first one, El, 
the translation parameters are estimated by apply- 
ing the EM algorithm in a straightforward fashion 
over all entities (tokens and units) present at least 
twice in the sequence-based corpus 2. The two next 
methods filter the probabilities obtained with the Ez 
method. In E2, all probabilities p(tls ) are set to 0 
whenever s is a token (not a unit), thus forcing the 
model to contain only associations between source 
units and target entities (tokens or units). In E3 
any parameter of the model that involves a token 
is removed (that is, p(tls ) = 0 if t or s is a token). 
The resulting model will thus contain only unit as- 
sociations. In both cases, the final probabilities are 
renormalized. Table 3 shows a few entries from a 
unit model (Mu) obtained after 15 iterations of the 
EM-algorithm on a sequence corpus resulting from 
the application of the length-grouping criterion (dr) 
over a lexicon of units whose likelihood score is above 
5.0. The probabilities have been obtained by appli- 
cation of the method E2. 
We found many partially correct associations 
Cover the years/au fils des, we have/nous, etc) that 
illustrate the weakness of decoupling the unit iden- 
tification from the mapping problem. In most cas- 
2The entities een only once are mapped to a special "un- 
known" word 
es however, these associations have a lower proba- 
bility than the good ones. We also found few er- 
ratic associations (the first time/e'dtait, some hon. 
members/t, etc) due to distributional rtifacts. It is 
also interesting to note that the good associations 
we found are not necessary compositional in nature 
(we must/il Iaut, people of canada/les canadiens, of 
eourse/6videmment, etc). 
3.3 F i l ter ing  
One way to increase the precision of the mapping 
process is to impose some linguistic constraints on 
the sequences such as simple noun-phrase contraints 
(Ganssier, 1995; Kupiec, 1993; hua Chen and Chen, 
94; Fung, 1995; Evans and Zhai, 1996). It is also 
possible to focus on non-compositional compounds, 
a key point in bilingual applications (Su et al, 1994; 
Melamed, 1997; Lin, 99). Another interesting ap- 
proach is to restrict sequences to those that do not 
cross constituent boundary patterns (Wu, 1995; Fu- 
ruse and Iida, 96). In this study, we filtered for po- 
tential sequences that are likely to be noun phrases, 
using simple regular expressions over the associated 
part-of-speech tags. An excerpt of the association 
probabilities of a unit model trained considering on- 
ly the NP-sequences i given in table 3. Applying 
this filter (referred to as JrNp in the following) to the 
39,093 english sequences still surviving after previ- 
ous filters ~'1 and ~'2 removes 35,939 of them (92%). 
138 
model spared ok good nu u 
1 baseline - model 1 48.98 0 0 747 0 
2 basel ine - model 2 51.83 0 0 747 0 
3 E1 + ~'1(2, 2, 0, 0.2) 50.98 527 1702 5 626 
4 E1+~'1(2,2,5,0.2)  51.61 596 2149 5 658 
5 E1 + ~-~ (2, 2, 5, 0.2) + 9r2 51.72 633 2265 5 657 
6 E2 + ~'~(2,2,0,0.2) 51.39 514 1551 43 578 
7 ?2 + ~-~ (2, 2, 5, 0.2) 51.99 470 1889 46 614 
8 E2 + ~'~(2,2,5,0.2) + ~'2 52.12 493 1951 46 606 
9 E3 + ~-1(2, 2, 0, 0.2) 51.07 577 1699 43 588 
10 E2 + ~-1(2, 2, 5, 0.2) 51.47 629 2124 46 618 
11 E2+~'~(2 ,2 ,5 ,0 .2 )+~'2  51.68 665 2209 46 615 
12 ~1 -}- .~1 (2, 2, 5, 0.2) -}- .~2 -}- ~:NP 52.83 416 1302 4 564 
13 E2 + ~'1(2, 2, 5, 0.2) + ~NP 53.12 439 1031 228 425 
14 ?2 + ~'~ (2, 2, 5, 0.2) + 5r2 + ~'NP 53.16 458 1052 199 439 
15 ~3 -{- ~ : 0.4 -}- ~-1(2, 2, 5, 0.2) 4- .~NP 53.22 495 1031 228 425 
Table 4: Completion results of several translation models, spared: theoretical proportion of characters 
saved; ok: number of target units accepted by the user; good: number of target units that matched the 
expected whether they were proposed or not; nu: number of sentences for which no target unit was found 
by the translation model; u: number of sentences for which at least one helpful unit has been found by the 
model, but not necessarily proposed. 
More than half of the 3,154 remaining NP-sequences 
contain only two words. 
4 Resu l t s  
We collected completion results on a test corpus 
of 747 sentences (13,386 english tokens and 14,506 
french ones) taken from the Hansard corpus. These 
sentences have been selected randomly among sen- 
tences that have not been used for the training. 
Around 18% of the source and target words are not 
known by the translation model. 
The baseline models (line 1 and 2) are obtained 
without any unit model (i.e. /~ = 1 in equation 2). 
The first one is obtained with an IBM-like model 1 
while the second is an IBM-like model 2. We observe 
that for the pair of languages we considered, model 
2 improves the amount of saved keystrokes of almost 
3% compared to model 1. Therefore we made use of 
alignment probabilities for the other models. 
The three next blocks in table 4 show how the 
parameter estimation method affects performance. 
Training models under the C1 method gives the worst 
results. This results from the fact that the word- 
to-word probabilities trained on the sequence based 
corpus (predicted by Mu in equation 2) are less ac- 
curate than the ones learned from the token based 
corpus. The reason is simply that there are less oc- 
currences of each token, especially if many units are 
identified by the grouping operator. 
In methods C2 and C3, the unit model of equation 
2 only makes predictions pu(tls ) when s is a source u- 
nit, thus lowering the noise compared to method ?1. 
We also observe in these three blocks the influence 
of sequence filtering: the more we filter, the better 
the results. This holds true for all estimation meth- 
ods tried. In the fifth block of table 4 we observe 
the positive influence of the NP-filtering, especially 
when using the third estimation method. 
The best combination we found is reported in line 
15. It outperforms the baseline by around 1.5%. 
This model has been obtained by retaining all se- 
quences een at least two times in the training cor- 
pus for which the likelihood test value was above 5 
and the entropy score above 0.2 (5rl (2, 2, 5, 0.2)). In 
terms of the coverage of this unit model, it is in- 
teresting to note that among the 747 sentences of 
the test session, there were 228 for which the model 
did not propose any units at all. For 425 of the re- 
maining sentences, the model proposed at least one 
helpful (good or partially good) unit. The active vo- 
cabulary for these sentences contained an average of 
around 2.5 good units per sentence, of which only 
half (495) were proposed during the session. The 
fact that this model outperforms others despite it- 
s relatively poor coverage (compared to the others) 
may be explained by the fact that it also removes 
part of the noise introduced by decoupling the i- 
dentification of the salient units from the training 
procedure. Furthermore, as we mentionned earlier, 
the more we filter, the less the grouping scheeme 
presented in equation 4 remains necessary, thus re- 
ducing a possible source of noise. 
The fact that this model outperforms others, de- 
spite its relatively poor coverage, is due to the fact 
139 
E ich le r  C )pt lons  
l am p leased  to  t~ lce  ]par t  in  th i s  debate  tod  W . 
Us ing  rod  W "s techno log ies ,  i t  i s  poss ib le  fo r  a l l  C~m~dia~s  to  
reg is ter  the i r  votes  on  i s s t les  of  pub l i c  spend ing  and  pub l i c  
I )o r ro~v ing .  
II me fa l t  p la le l r  de  prendre  la paro le  au Jourd 'hu i  dana  le cadre  de  ?e  
d~bat .  
Gr~ice  & la  techno log le  moderne ,  toue  lea  Canad len= peuvent  6e  
prononcer  sur  le=;  quest ion= de  d6pen=e== et  d" e rnprunta  de  I" I~tat  . 
Not re  p 
Figure 1: Example of an i teraction i  TRANSTYPE with the source text in the top half of the screen. The 
target text is typed in the bottom half with suggestions given by the menu at the insertion point. 
that it also removes part of the noise that is intro- 
duced by dissociating the identification ofthe salient 
units from the training procedure. ~rthermore, as 
we mentioned earlier, the more we filter, the less the 
grouping scheme presented in equation 4 remains 
necessary, thus further reducing an other possible 
source of noise. 
5 Conclusion 
We have described a prototype system called 
TRANSTYPE which embodies an innovative ap- 
proach to interactive machine translation in which 
the interaction is directly concerned with establish- 
ing the target ext. We proposed and tested a mech- 
anism to enhance TRANSTYPE by having it predic- 
t sequences of words rather than just completions 
for the current word. The results show a modest 
improvement in prediction performance which will 
serve as a baseline for our future investigations. One 
obvious direction for future research is to revise our 
current strategy of decoupling the selection of units 
from their bilingual context. 
Acknowlegments 
TRANSTYPE is a project funded by the Natural Sci- 
ences and Engineering Research Council of Canada. 
We are undebted to Elliott Macklovitch and Pierre 
Isabelle for the fruitful orientations they gave to this 
work. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approach to natural language processing. Compu- 
tational Linguistics, 22(1):39-71. 
Peter F. Brown, Stephen A. Della Pietra, Vincen- 
t Della J. Pietra, and Robert L. Mercer. 1993. 
The mathematics of machine trmaslation: Pa- 
rameter estimation. Computational Linguistics, 
19(2):263-312, June. 
Ted Dunning. 93. Accurate methods for the statis- 
tics of surprise and coincidence. Computational 
Linguistics, 19(1):61-74. 
David A. Evans and Chengxiang Zhai. 1996. Noun- 
phrase analysis in unrestricted text for informa- 
tion retrieval. In Proceedings of the 34th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 17-24, Santa Cruz, California. 
George Foster, Pierre Isabelle, and Pierre Plamon- 
don. 1997. Target-text Mediated Interactive Ma- 
chine Translation. Machine Translation, 12:175- 
194. 
Pascale Fung. 1995. A pattern matching method for 
finding noun and proper noun translations from 
noisy parallel corpora. In Proceedings ofthe 33rd 
Annual Meeting of the Association for Compu- 
tational Linguistics, pages 236-243, Cambridge, 
Massachusetts. 
Osamu Furuse and Hitoshi Iida. 96. Incremen- 
140 
tal translation utilizing constituent boundray pat- 
terns. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
412-417, Copenhagen, Denmark. 
Eric Gaussier. 1995. Modles statistiques et patron- 
s morphosyntaxiques pour l'extraction de lcxiques 
bilingues. Ph.D. thesis, Universit de Paris 7, jan- 
vier. 
Masahiko Haruno, Satoru Ikehara, and Takefumi 
Yamazaki. 96. Learning bilingual collocations by 
word-level sorting. In Proceedings of the 16th In- 
ternational Conference On Computational Lin- 
guistics, pages 525-530, Copenhagen, Denmark. 
Kuang hua Chen and Hsin-Hsi Chen. 94. Extract- 
ing noun phrases from large-scale texts: A hybrid 
approach and its automatic evaluation. In Pro- 
ceedings of the 32nd Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 234- 
241, Las Cruces, New Mexico. 
Satoru Ikehara, Satoshi Shirai, and Hajine Uchino. 
96. A statistical method for extracting uinterupt- 
ed and interrupted collocations from very large 
corpora. In Proceedings of the 16th International 
Conference On Computational Linguistics, pages 
574-579, Copenhagen, Denmark. 
Julian Kupiec. 1993. An algorithm for finding noun 
phrase correspondences in bilingual corpora. In 
Proceedings of the 31st Annual Meeting of the 
Association for Computational Linguistics, pages 
17-22, Colombus, Ohio. 
Dekang Lin. 99. Automatic identification of non- 
compositional phrases. In Proceedings of the 37th 
Annual Meeting of the Association for Computa- 
tional Linguistics, pages 317-324, College Park, 
Maryland. 
I. Dan Melamed. 1997. Automatic discovery of non- 
compositional coumpounds in parallel data. In 
Proceedings of the 2nd Conference on Empirical 
Methods in Natural Language Processing, pages 
97-108, Providence, RI, August, lst-2nd. 
Makoto Nagao and Shinsuke Mori. 94. A new 
method of n-gram statistics for large number of 
n and automatic extraction of words and phrases 
from large text data of japanese. In Proceedings 
of the 16th International Conference On Com- 
putational Linguistics, volume 1, pages 611-615, 
Copenhagen, Denmark. 
Franz Josef Och and Hans Weber. 98. Improving 
statistical natural anguage translation with cate- 
gories and rules. In Proceedings of the 36th Annu- 
al Meeting of the Association for Computational 
Linguistics, pages 985-989, Montreal, Canada. 
Graham Russell. 1998. Identification of salient to- 
ken sequences. Internal report, RALI, University 
of Montreal, Canada. 
Sayori Shimohata, Toshiyuki Sugio, and Junji 
Nagata. 1997. Retrieving collocations by co- 
occurrences and word order constraints. In Pro- 
ceedings of the 35th Annual Meeting of the Asso- 
ciation for Computational Linguistics, pages 476- 
481, Madrid Spain. 
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 
1994. A corpus-based approach to automatic om- 
pound extraction. In Proceedings of the 32nd An- 
nual Meeting of the Association for Computation- 
al Linguistics, pages 242-247, Las Cruces, New 
Mexico. 
Ye-Yi Wang and Alex Waibel. 98. Modeling with 
structures in statistical machine translation. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, vol- 
ume 2, pages 1357-1363, Montreal, Canada. 
Dekai Wu and Hongsing Wong. 98. Machine trans- 
lation with a stochastic grammatical channel. In 
Proceedings of the 36th Annual Meeting of the 
Association for Computational Linguistics, pages 
1408-1414, Montreal, Canada. 
Dekai Wu. 1995. Stochastic inversion transduc- 
tion grammars, with application to segmentation, 
bracketing, and alignment of parallel corpora. In 
Proceedings of the International Joint Conference 
on Artificial Intelligence, volume 2, pages 1328- 
1335, Montreal, Canada. 
141 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 51?54
Manchester, August 2008
Scaling up Analogical Learning
Philippe Langlais
Universite? de Montre?al / Dept. I.R.O.
C.P. 6128, Que?bec, H3C3J7, Canada
felipe@iro.umontreal.ca
Franc?ois Yvon
Univ. Paris Sud 11 & LIMSI-CNRS
F-91401 Orsay, France
yvon@limsi.fr
Abstract
Recent years have witnessed a growing in-
terest in analogical learning for NLP ap-
plications. If the principle of analogical
learning is quite simple, it does involve
complex steps that seriously limit its ap-
plicability, the most computationally de-
manding one being the identification of
analogies in the input space. In this study,
we investigate different strategies for ef-
ficiently solving this problem and study
their scalability.
1 Introduction
Analogical learning (Pirrelli and Yvon, 1999) be-
longs to the family of lazy learning techniques
(Aha, 1997). It allows to map forms belong-
ing to an input space I into forms of an output
space O, thanks to a set of known observations,
L = {(i, o) : i ? I, o ? O}. I(u) and O(u)
respectively denote the projection of an observa-
tion u into the input space and output space: if
u ? (i, o), then I(u) ? i and O(u) ? o. For an
incomplete observation u ? (i, ?), the inference of
O(u) involves the following steps:
1. building E
I
(u) the set of analogical triplets
of I(u), that is E
I
(u) = {(s, v, w) ? L
3
:
[I(s) : I(v) = I(w) : I(u) ]}
2. building the set of solutions to the target equa-
tions formed by projecting source triplets:
E
O
(u) = {t ? O : [O(s) : O(v) = O(w) :
t ] ,?(s, v, w) ? E
I
(u)}
3. selecting candidates among E
O
(u).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
where [x : y = z : t ] denotes an analogical pro-
portion, that is a relation between these four items,
meaning that ?x is to y as z is to t?, in a sense to
be specified. See (Lepage, 1998) or (Stroppa and
Yvon, 2005) for possible interpretations.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
If the principle of analogical learning is quite
simple, it does involve complex steps that seriously
limit its applicability. As a matter of fact, we are
only aware of studies where analogical learning is
applied to restricted tasks, either because they ar-
bitrarily concentrate on words (Stroppa and Yvon,
2005; Langlais and Patry, 2007; Denoual, 2007)
or because they focus on limited data (Lepage and
Denoual, 2005; Denoual, 2007).
In this study, we investigate different strategies
for making step 1 of analogical learning tractable.
We propose a data-structure and algorithms that
allow to control the balance between speed and
recall. For very high-dimensional input spaces
(hundreds of thousand of elements), we propose
a heuristic which reduces computation time with a
limited impact on recall.
2 Identifying input analogical relations
2.1 Existing approaches
A brute-force approach for identifying the input
triplets that define an analogy with the incomplete
observation u = (t , ?) consists in enumerating
51
triplets in the input space and checking for an ana-
logical relation with the unknown form t :
E
I
(u) = { ?x, y, z? : ?x, y, z? ? I
3
,
[x : y = z : t ] }
This amounts to check o(|I|3) analogies, which is
manageable for toy problems only.
Langlais and Patry (2007) deal with an input
space in the order of tens of thousand forms (the
typical size of a vocabulary) using following strat-
egy for E
I
(u). It consists in solving analogical
equations [y : x = t : ? ] for some pairs ?x, y?
belonging to the neighborhood1 of I(u), denoted
N (t). Those solutions that belong to the input
space are the z-forms retained.
E
I
(u) = { ?x, y, z? : ?x, y? ? N (t)
2
,
[y : x = t : z] }
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t ] ? [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs ?x, y? sampled.
2.2 Exhaustive tree-count search
The strategy we propose here exploits a prop-
erty on character counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t ] ? |x|
c
+ |t |
c
= |y|
c
+ |z|
c
?c ? A
where A is the alphabet on which the forms are
built, and |x|
c
stands for the number of occur-
rences of character c in x . In the sequel, we de-
note C(?x, t?) = {?y, z? ? I2 : |x|
c
+ |t |
c
=
|y|
c
+ |z|
c
?c ? A} the set of pairs that satisfy
the count property with respect to ?x, t? .
The strategy we propose consists in first select-
ing an x-form in the input space. This enforces a
set of necessary constraints on the counts of char-
acters that any two forms y and z must satisfy for
[x : y = z : t ] to be true. By considering all forms
x in turn,2 we collect a set of candidate triplets for
t . A verification of those that define with t a anal-
ogy must then be carried out. Formally, we built:
E
I
(u) = { ?x, y, z? : x ? I,
?y, z? ? C(?x, t?),
[x : y = z : t ] }
1The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u) .
2Anagram forms do not have to be considered separately.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and if
(ii) we can efficiently identify the pairs ?y, z? that
satisfy a set of constraints on character counts. To
this end, we propose to organize the input space
thanks to a data structure called a tree-count (see
Section 3), which is easy to built and supports effi-
cient runtime retrieval.
2.3 Sampled tree-count search
As shown in (Langlais and Yvon, 2008), using
tree-count to constraint the search allows to ex-
haustively solve step 1 for reasonably large input
spaces. Computing analogies in very large input
space (hundreds of thousand forms) however re-
mains computationally demanding, as the retrieval
algorithm must be carried out o(I) times. In this
case, we propose to sample the x-forms:
E
I
(u) = { ?x, y, z? : x ? N (t),
?y, z? ? C(?x, t?),
[x : y = t : z] }
There is unfortunately no obvious way of se-
lecting a good subset N (t) of input forms, as
analogies does not necessarily entail the similar-
ity of ?diagonal? forms, as illustrated by the anal-
ogy [une pomme verte : des pommes vertes =
une voiture rouge : des voitures rouges], which
involves singular/plural commutations in French
nominal groups. In this situation, randomly se-
lecting a subset of the input space seems to be a
reasonable strategy (hereafter RAND).
For some analogies however, the first and
last forms share some sequences of charac-
ters. This is obvious in [dream : dreamer =
dreams : dreamers], but can be more subtle, as
in our first example [This guy drinks too much :
This boat sinks = These guys drank too much :
These boats sank ] where the diagonal terms
share some n-grams reminiscent of the number
(This/These) and tense (drink /drank ) commuta-
tions involved.
We thus propose a sampling strategy (hereafter
EV) which selects x-forms that share with t some
sequences of characters. To this end, input forms
are represented in a vector space whose dimen-
sions are frequent character n-grams, retaining the
k-most frequent n-grams, where n ? [min;max].
A form is thus encoded as a binary vector of
52
dimension k, in which ith coefficient indicates
whether the form contains an occurrence of the ith
n-gram.3 At runtime, we select the N forms that
are the closest to a given form t , according to a
distance4. Figure 1 illustrates some forms selected
by this process. For comparison purposes, we also
tested a sampling strategy which consists in select-
ing the x-forms that are closest to the source form
t , according to the usual edit-distance (hereafter
ED).
establish a report ? order to establish a ? has
tabled this report ? is about the report ? basis
of the report ? other problem is that ? problem
that arises ? problem is that those
Figure 1: The 8 nearest neighbors of to establish
a report in a vector space computed from an input
space of over a million phrases.
3 The tree-count data-structure
A tree-count is a tree which encodes a set of forms.
Nodes are labeled by an alphabetical symbol and
contain a (possibly empty) set of pointers to forms.
A vertice from a node n labeled c to a node m is
weighted by the count of c in the forms encoded
by m, that is, the set of forms that can be reached
from this node and its descendants. Thus, a path
in a tree-count represents a set of constraints on
the counts of the characters encountered along this
path. This structure allows for instance the identi-
fication of anagrams in a set of forms: it suffices to
search the tree-count for nodes that contain more
than one pointer to forms in the vocabulary.
An example of a tree-count is provided in Fig-
ure 2 for a small set of forms. The node double
circled in this figure is labeled by the character d
and encodes the 6 input forms that contain 1 oc-
currence of ?o? and 1 occurrence of ?s?. One form
is os , referenced by the pointer m , the other five
forms are found by descending the tree from this
node; among which gods and dogs , two anagrams
encoded by the leave which set of pointers is b, k.
3.1 Construction time
The construction of a tree-count from a set of
forms only needs an arbitrary order on the char-
acters of the alphabet. This is the order in which
we will encounter them while descending the
3Typical values are min=max=3 and k=20000.
4We used the Manhattan distance in this study.
?u
?
p
a,l
?a
n
?
g
b,k
c
? k
? y
f,i
g,h
 s
? l
e,j
dm
d
? m
? t? s
? o
0
1 2
1
1 1
1
1
1
0
0 1
1
1
21
1
20
1
Figure 2: The tree-count encoding the set:
{soup(a), gods(b), odds(c), sos(d), solo(e),
tokyo(f), moot(g), moto(h), kyoto(i), oslo(j),
dogs(k), opus(l), os(m), a(n)}. The character la-
beling a node is represented in a box; the counts of
each character labels each vertice. Roman letters
in nodes represent pointers to input forms; greek
symbols label internal nodes.
tree. The lack of space prevents us to report the
construction algorithm (see (Langlais and Yvon,
2008)), but it is important to note that it only in-
volves a simple traversal of the input forms and is
therefore time efficient. Also worth mentioning,
our construction procedure only stores necessary
nodes. This means that when enumerating char-
acters in order, we only store zero-count nodes as
required. As a result, the depth of a tree-count is
typically much lower than the size of the alphabet.
3.2 Retrieval time
The retrieval of C(?x, t?) can be performed by
traversing the tree-count while maintaining a fron-
tier, that is, the set of pairs of nodes in the tree-
count that satisfy the constraints on counts encoun-
tered so far. Imagine, for instance, that we are
looking for the pairs of forms that contain exactly
3 occurrences of characters o , 2 of characters s
and 1 character l , and no other character. Start-
ing from the root node labelled by o , there is only
one pair of nodes that satisfy the constraint on o:
the frontier is therefore {(?, ?)}. The constraint
on s leads to the frontier {(d, ?)} (since the count
of t must be null). Finally, descending this node
yields the frontier {(m, (e, j))}, which identifies
the pairs (os, solo) and (os, oslo) to be the only
53
ones satisfying the initial set of constraints.
The complexity of retrieval is mainly dominated
by the size of the frontier built while traversing a
tree-count. In practice, because of the sparsity of
the space we manipulate in NLP applications, re-
trieval is also a fast operation.
4 Checking for an analogy
Stroppa (2005) provides a dynamic programming
algorithm for checking that a quadruplet is an anal-
ogy, whose complexity is o(|x| ? |y| ? |z| ? |t |).5
Depending on the application, a large number of
calls to this algorithm must be performed during
step 1 of analogical learning. The following prop-
erty helps cutting down the computations:
[x : y = z : t ] ?
(x[1] ? {y[1], z[1]}) ? (t [1] ? {y[1], z[1]})
(x[$] ? {y[$], z[$]}) ? (t [$] ? {y[$], z[$]})
where ?[$] denotes the last character of ?. A simple
and efficient trick consists in calling the analogy
checking routine only for those triplets that pass
this test.
5 Discussion
We investigated the aforementioned search strate-
gies by translating 1 000 new words (resp. phrases)
thanks to a translation table populated with pairs of
words (resp. pairs of phrases). We studied the scal-
ability of each strategy by varying the size of the
transfer table (small, medium, large). Precise fig-
ures can be found in (Langlais and Yvon, 2008);
we summarize here the main outcomes.
On the word-task, we compared the tree-count
search strategy to the LP one. On the largest word-
set (84 000 input words), the former (exact) strat-
egy could find an average of 34 597 input analogies
for 964 test-words at an average response time of
1.2 seconds per word, while with the latter strat-
egy, an average of 56 analogies could be identified
for 890 test-words, in an average of 6.3 seconds.
On the sequence-task, where input spaces are
much larger, we compared the various sampling
strategies presented in Section 2.3. We set N, the
number of sampled input forms, to 103 for all
sampling strategies. On the medium size dataset
(293 000 input phrases), both ED and RAND per-
form badly compared to EV. With the two for-
mer filtering strategies, we could at best identify
5In this study, we used the definition of a formal analogy
provided by Stroppa and Yvon (2005). Lepage (1998) pro-
poses a less general definition, which is faster to check.
17 input analogies for 38% of the test-phrases (at
an average response time of 9 seconds), while with
EV, an average 46 analogies could be identified for
75% of the test-phrases (in 3 seconds on average).
Finally, we checked that the approach we pro-
posed scales to very large datasets (several mil-
lions of input phrases), which to the best of our
knowledge is simply out of the reach of existing
approaches. This opens up interesting prospects
for analogical learning, such as enriching a phrase-
based table of the kind being used in statistical ma-
chine translation.
Acknowledgment
This study has been accomplished while the first
author was visiting Te?le?com ParisTech.
References
Aha, David A. 1997. Editorial. Artificial Intelligence
Review, 11(1-5):7?10. Special Issue on Lazy Learn-
ing.
Denoual, Etienne. 2007. Analogical translation of
unknown words in a statistical machine translation
framework. In Machine Translation Summit, XI,
Copenhagen, Sept. 10-14.
Langlais, Philippe and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
EMNLP-CoNLL, pages 877?886, Prague, Czech Re-
public, June.
Langlais, Philippe and Franc?ois Yvon. 2008. Scaling
up analogies. Technical report, Te?le?com ParisTech,
France.
Lepage, Yves and ?Etienne Denoual. 2005. Purest
ever example-based machine translation: Detailed
presentation and assessment. Machine Translation,
29:251?282.
Lepage, Yves. 1998. Solving analogies on words: an
algorithm. In COLING-ACL, pages 728?734, Mon-
treal, Canada.
Pirrelli, Vitto and Franc?ois Yvon. 1999. The hidden
dimension: a paradigmatic view of data-driven NLP.
Journal of Experimental & Theroretical Artifical In-
telligence, 11:391?408.
Stroppa, Nicolas and Franc?ois Yvon. 2005. An ana-
logical learner for morphological analysis. In 9th
Conf. on Computational Natural Language Learning
(CoNLL), pages 120?127, Ann Arbor, MI, June.
Stroppa, Nicolas. 2005. De?finitions et caracte?risations
de mode`les a` base d?analogies pour l?apprentissage
automatique des langues naturelles. Ph.D. thesis,
ENST, Paris, France, Nov.
54
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 877?886, Prague, June 2007. c?2007 Association for Computational Linguistics
Translating Unknown Words by Analogical Learning
Philippe Langlais and Alexandre Patry
Dept. I.R.O.
Universite? de Montre?al
{felipe,patryale}@iro.umontreal.ca
Abstract
Unknown words are a well-known hindrance
to natural language applications. In particu-
lar, they drastically impact machine transla-
tion quality. An easy way out commercial
translation systems usually offer their users
is the possibility to add unknown words
and their translations into a dedicated lex-
icon. Recently, Stroppa and Yvon (2005)
have shown how analogical learning alone
deals nicely with morphology in different
languages. In this study we show that ana-
logical learning offers as well an elegant and
effective solution to the problem of identify-
ing potential translations of unknown words.
1 Introduction
Analogical reasoning has received some attention in
cognitive science and artificial intelligence (Gentner
et al, 2001). It has been for a long time a faculty as-
sessed in the so-called SAT Reasoning tests used in
the application process to colleges and universities
in the United States. Turney (2006) has shown that
it is possible to compute relational similarities in a
corpus in order to solve 56% of typical analogical
tests quizzed in SAT exams. The interested reader
can find in (Lepage, 2003) a particularly dense treat-
ment of analogy, including a fascinating chapter on
the history of the notion of analogy.
The concept of proportional analogy, denoted
[A : B = C : D ], is a relation between four
entities which reads: ?A is to B as C is to D?.
Among proportional analogies, we distinguish for-
mal analogies, that is, ones that arise at the graph-
ical level, such as [fournit : fleurit = fournie :
fleurie] in French or [believer : unbelievable =
doer : undoable] in English. Formal analogies are
often good indices for deeper analogies (Stroppa and
Yvon, 2005).
Lepage and Denoual (2005) presented the sys-
tem ALEPH, an intriguing example-based system
entirely built on top of an automatic formal anal-
ogy solver. This system has achieved state-of-the-
art performance on the IWSLT task (Eck and Hori,
2005), despite its striking purity. As a matter of
fact, ALEPH requires no distances between exam-
ples, nor any threshold.1 It does not even rely on
a tokenization device. One reason for its success
probably lies in the specificity of the BTEC corpus:
short and simple sentences of a narrow domain. It is
doubtful that ALEPH would still behave adequately
on broader tasks, such as translating news articles.
Stroppa and Yvon (2005) propose a very help-
ful algebraic description of a formal analogy and
describe the theoretical foundations of analogical
learning which we will recap shortly. They show
both its elegance and efficiency on two morphologi-
cal analysis tasks for three different languages.
Recently, Moreau et al (2007) showed that for-
mal analogies of a simple kind (those involving suf-
fixation and/or prefixation) offer an effective way to
extend queries for improved information retrieval.
In this study, we show that analogical learning
can be used as an effictive method for translating
unknown words or phrases. We found that our ap-
proach has the potential to propose a valid transla-
tion for 80% of ordinary unknown words, that is,
words that are not proper names, compound words,
or numerical expressions. Specific solutions have
been proposed for those token types (Chen et al,
1998; Al-Onaizan and Knight, 2002; Koehn and
Knight, 2003).
The paper is organized as follows. We first recall
1Some heuristics are applied for speeding up the system.
877
in Section 2 the principle of analogical learning and
describe how it can be applied to the task of enrich-
ing a bilingual lexicon. In Section 3, we present the
corpora we used in our experiments. We evaluate
our approach over two translation tasks in Section 4.
We discuss related work in Section 5 and give per-
spectives of our work in Section 6.
2 Analogical Learning
2.1 Principle
Our approach to bilingual lexical enrichment is an
instance of analogical learning described in (Stroppa
and Yvon, 2005). A learning set L = {L1, . . . , LN}
gathers N observations. A set of features computed
on an incomplete observation X defines an input
space. The inference task consists in predicting the
missing features which belong to an output space.
We denote I(X) (resp. O(X)) the projection of X
into the input (resp. output) space. The inference
procedure involves three steps:
1. Building EI(X) = {(A,B,C) ? L3 | [I(A) :
I(B) = I(C) : I(X)]}, the set of input stems2
of X , that is the set of triplets (A,B,C) which
form with X an analogical equation.
2. Building EO(X) = {Y | [O(A) : O(B) =
O(C) : Y ] ,?(A,B,C) ? EI(X)} the set of
solutions to the analogical equations obtained
by projecting the stems of EI(X) into the out-
put space.
3. Selecting O(X) among the elements of
EO(X).
This inference procedure shares similarities with
the K-nearest-neighbor (k-NN) approach. In partic-
ular, since no model of the training material is be-
ing learned, the training corpus needs to be stored
in order to be queried. On the contrary to k-NN,
however, the search for closest neighbors does not
require any distance, but instead relies on relational
similarities. This purity has a cost: while in k-NN
inference, neighbors can be found in time linear to
the training size, in analogical learning, this oper-
ation requires a computation time cubic in N , the
2In Turney?s work (Turney, 2006), a stem designates the first
two words of a proportional analogy.
number of observations. In many applications of in-
terest, including the one we tackle here, this is sim-
ply impractical and heuristics must be applied.
The first and second steps of the inference proce-
dure rely on the existence of an analogical solver,
which we sketch in the next section. One impor-
tant thing to note at this stage, is that an analogical
equation may have several solutions, some being le-
gitimate word-forms in a given language, others be-
ing not. Thus, it is important to select wisely the
generated solutions, therefore Step 3. In practice,
the inference procedure involves the computation of
many analogical equations, and a statistic as simple
as the frequency of a solution often suffices to sepa-
rate good from spurious solutions.
2.2 Analogical Solver
Lepage (1998) proposed an algorithm for comput-
ing the solutions of a formal analogical equation
[A : B = C : ? ]. We implemented a variant of
this algorithm which requires to compute two edit-
distance tables, one between A and B and one be-
tween A and C. Since we are looking for subse-
quences of B and C not present in A, insertion cost
is null. Once this is done, the algorithm synchro-
nizes the alignments defined by the paths of min-
imum cost in each table. Intuitively, the synchro-
nization of two alignments (one between A and B,
and one between A and C) consists in composing in
the correct order subsequences of the strings B and
C that are not in A. We refer the reader to (Lep-
age, 1998) for the intricacies of this process which
is illustrated in Figure 1 for the analogical equation
[even : usual = unevenly : ? ]. In this exam-
ple, there are 681 different paths that align even and
usual (with a cost of 4), and 1 path which aligns even
with unevenly (with a cost of 0). This results in 681
synchronizations which generate 15 different solu-
tions, among which only unusually is a legitimate
word-form.
In practice, since the number of minimum-cost
paths may be exponential in the size of the strings
being aligned, we consider the synchronization of
a maximum of M best paths in each edit-distance
table. The worst-case complexity of our analogical
solver is O([|A| ? (|B| + |C|)] + [M2 ? (|A| +
ins(B,C))]), where the first term corresponds to
the computation of the two edit-distance tables,
878
4 4 4 4 4 4 n 4 4 3 3 2 1 0 0 0
3 3 3 3 3 3 e 3 3 3 2 1 0 0 0 0
2 2 2 2 2 2 v 2 2 2 1 0 0 0 0 0
1 1 1 1 1 1 e 1 1 1 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
l a u s u / . u n e v e n l y
e v e n e v e n
u s u a l u n e v e n l y
?usua-un-l-ly
e v e n e v e n
u s u a l u n e v e n l y
?un-usu-a-l-ly
Figure 1: The top table reports the edit-distance ta-
bles computed between even and usual (left part),
and even and unevenly (right part). The bottom
part of the figure shows 2 of the 681 synchroniza-
tions computed while solving the equation [even :
usual = unevenly : ? ]. The first one corresponds to
the path marked in bold italics and leads to a spuri-
ous solution; the second leads to a legitimate solu-
tion and corresponds to the path shown as squares.
and the second one corresponds to the maximum
time needed to synchronize them. |X| denotes the
length, counted in characters of the string X , whilst
ins(B,C) stands for the number of characters of B
and C not belonging to A. Given the typical length
of the strings we consider in this study, our solver is
quite efficient.3
Stroppa and Yvon (2005) described a generaliza-
tion of this algorithm which can solve a formal ana-
logical equation by composing two finite-state trans-
ducers.
2.3 Application to Lexical Enrichment
Analogical inference can be applied to the task of
extending an existing bilingual lexicon (or transfer
table) with new entries. In this study, we focus on
a particular enrichment task: the one of translating
valid words or phrases that were not encountered at
training time.
A simple example of how our approach translates
unknown words is illustrated in Figure 2 for the (un-
3Several thousands of equations solved within one second.
Step 1 source (French) stems
[activite?s : activite? = futilite?s : futilite?]
[hostilite?s : hostilite? = futilite?s : futilite?] . . .
Step 2a projection by lexicon look-up
activite?s?actions hostilite??hostility
hostilite?s?hostilities activite??action
futilite?s?trivialities,gimmicks . . .
Step 2b target (English) resolution
[actions : action = gimmicks : ? ] ? gimmick
[hostilities : hostility = trivialities : ? ] ? triviality
[hobbies : hobby = trivialities : ? ] ? triviality
Step 3 selection of target candidates
?triviality, 2?, ?gimmick , 1?, . . .
Figure 2: Illustration of the analogical inference pro-
cedure applied to the translation of the unknown
French word futilite?.
known) French word futilite?. In this example, trans-
lations is inferred by commuting plural and singular
words. The inference process lazily captures the fact
that English plural nouns ending in -ies usually cor-
respond to singular nouns ending in -y.
Formally, we are given a training corpus L =
{?S1, T1?, . . . , ?SN , TN ?} which consists of a col-
lection of N bilingual lexicon entries ?Si, Ti?. The
input space is in our case the space of possible
source words, while the output space is the set of
possible target words. We define:
?X ? ?S ,T?, I(X) = S and O(X) = T
Given an unknown source word-form S, Step 1 of
the inference process consists in identifying source
stems which have S as a solution:4
EI(S) = {?i, j, k? ? [1, N ]
3 | [Si : Sj = Sk : S]}.
During Step 2a, each source stem belonging to
EI(S) is projected form by form into (potentially
several) stems in the output space, thanks to an op-
erator proj that will be defined shortly:
E?i ,j ,k ?(S) = {T | [U : V = W : T ]} where
(U, V,W ) ? (projL(Si)? projL(Sj)? projL(Sk)).
4All strings in a stem must be different, otherwise, it can be
shown that all source words would be considered.
879
During Step 2b, each solution to those output
stems is collected in EO(S) along with its associated
frequency:
EO(S) =
?
?i ,j ,k ??EI(S)
E?i ,j ,k ?(S).
Step 3 selects from EO(S) one or several solu-
tions. We use frequency as criteria to sort the gener-
ated solutions. The projection mechanism we resort
to in this study simply is a lexicon look-up:
projL(S) = {T | ?S, T ? ? L}.
There are several situations where this inference
procedure will introduce noise. First, both source
and target analogical equations can lead to spuri-
ous solutions. For instance, [show : showing =
eating : ? ] will erroneously produce eatinging. Sec-
ond, an error in the original lexicon may introduce
as well erroneous target word-forms. For instance,
when translating the German word proklamierung,
by making use of the analogy [formalisiert :
formalisierung = proklamiert : proklamierung],
the English equation [formalised : formalized =
sets : ? ] will be considered if it happens that
proklamiert?sets belongs to L; in which case, zets
will be erroneously produced.
We control noise in several ways. The source
word-forms we generate are filtered by imposing
that they belong to the input space. We also use a
(large) target vocabulary to eliminate spurious tar-
get word-forms (see Section 3). More importantly,
since we consider many analogical equations when
translating a word-form, spurious analogical solu-
tions tend to appear less frequently than ones arising
from paradigmatic commutations.
2.4 Practical Considerations
Searching for EI(S) is an operation which requires
solving a number of (source) analogical equations
cubic in the size of the input space. In many settings
of interest, including ours, this is simply not practi-
cal. We therefore resort to two strategies to reduce
computation time. The first one consists in using the
analogical equations in a generative mode. Instead
of searching through the set of stems ?Si, Sj , Sk?
that have for solution the unknown source word-
form S, we search for all pairs (Si, Sj) to the so-
lutions of [Si : Sj = S :?] that are valid word-forms
of the input space. Note that this is an exact method
which follows from the property (Lepage, 1998):
[A : B = C : D ] ? [B : A = D : C ]
This leaves us with a quadratic computation time
which is still intractable in our case. Therefore,
we apply a second strategy which consists in com-
puting the analogical equations [Si : Sj = S :?]
for the only words Si and Sj close enough to S.
More precisely, we enforce that Si ? v?(S) and that
Sj ? v?(Si) for a neighborhood function v?(A) of
the form:
v?(A) = {B | f(B,A) ? ?}
where f is a distance; we used the edit-distance in
this study (Levenshtein, 1966). Note that the second
strategy we apply is only a heuristic.
3 Resources
In this work, we are concerned with one concrete
problem a machine translation system must face:
the one of translating unknown words. We are fur-
ther focusing on the shared task of the workshop
on Statistical Machine Translation, which took place
last year (Koehn and Monz, 2006) and consisted in
translating Spanish, German, and French texts from
and to English. For some reasons, we restricted our-
selves to translating only into English. The training
material available is coming from the Europarl cor-
pus. The test material was divided into two parts.5
The first one (hereafter called test-in) is com-
posed of 2 000 sentences from European parliament
debates. The second part (called test-out) gath-
ers 1 064 sentences6 collected from editorials of the
Project Syndicate website.7 The main statistics per-
tinent to our study are summarized in Table 1.
A rough analysis of the 441 different unknown
words encountered in the French test sets reveals
that 54 (12%) of them contain at least one digit
(years, page numbers, law numbers, etc.), 83 (20%)
are proper names, 37 (8%) are compound words,
18 (4%) are foreign words (often Latin or Greek
5The participants were not aware of this.
6We removed 30 sentences which had encoding problems.
7http://www.project-syndicate.com
880
French Spanish German
test- in out in out in out
|unknown| 180 265 233 292 469 599
oov% 0.26 1.22 0.38 1.37 0.84 2.87
Table 1: Number of different (source) test words not
seen at training time, and out-of-vocabulary rate ex-
pressed as a percentage (oov%).
words), 7 words are acronyms, and 4 are tokeniza-
tion problems. The 238 other words (54%) are ordi-
nary words.
We considered different lexicons for testing our
approach. These lexicons were derived from the
training material of the shared task by training with
GIZA++ (Och and Ney, 2000) ?default settings?
two transfer tables (source-to-target and the reverse)
that we intersected to remove some noise.
In order to investigate how sensitive our approach
is to the amount of training material available, we
varied the size of our lexicon LT by considering dif-
ferent portions of the training corpus (T = 5 000,
10 000, 100 000, 200 000, and 500 000 pairs of sen-
tences). The lexicon trained on the full training ma-
terial (688 000 pairs of sentences), called Lref here-
after, is used for validation purposes. We kept (at
most) the 20 best associations of each source word
in these lexicons. In practice, because we intersect
two models, the average number of translations kept
for each source word is lower (see Table 2).
Last, we collected from various target texts (En-
glish here) we had at our disposal, a vocabulary set
V gathering 466 439 words, that we used to filter out
spurious word-forms generated by our approach.
4 Experiments
4.1 Translating Unknown Words
For the three translation directions (from Span-
ish, German, and French into English), we ap-
plied the analogical reasoning to translate the (non-
numerical) source words of the test material, absent
from LT . Examples of translations produced by ana-
logical inference are reported in Figure 3, sorted by
decreasing order of times they have been generated.
anti-agricole  (anti-farm,5) (anti-agricultural,3)
(anti-rural,3) (anti-farming,3) (anti-farmer,3)
fleurie  (flourishing,5) (flourished,4) (flourish,1)
futilite?  (trivialities,27) (triviality,14) (futile,9)
(meaningless,9) (futility,4) (meaninglessness,4)
(superfluous,2) (unwieldy,2) (unnecessary,2)
(uselessness,2) (trivially,1) (tie,1) (trivial,1)
butoir  (deadline,42) (deadlines,33) (blows,1)
court-circuitant  (bypassing,13) (bypass,12)
(bypassed,5) (bypasses,1)
xviie  (xvii,18) (sixteenth,3) (eighteenth,1)
Figure 3: Candidate translations inferred from
L200 000 and their frequency. The candidates re-
ported are those that have been intersected with V .
Translations in bold are clearly erroneous.
4.1.1 Baselines
We devised two baselines against which we com-
pared our approach (hereafter ANALOG). The first
one, BASE1, simply proposes as translations the tar-
get words in the lexicon LT which are the most simi-
lar (in the sense of the edit-distance) to the unknown
source word. Naturally, this approach is only appro-
priate for pairs of languages that share many cog-
nates (i.e., docteur ? doctor). The second base-
line, BASE2, is more sensible and more closely cor-
responds to our approach. We first collect a set of
source words that are close-enough (according to the
edit-distance) to the unknown word. Those source
words are then projected into the output space by
simple bilingual lexicon look-up. So for instance,
the French word demanda will be translated into the
English word request if the French word demande is
in LT and that request is one of its sanctioned trans-
lations.
Each of these baselines is tested in two variants.
The first one (id), which allows a direct comparison,
proposes as many translations as ANALOG does. The
second one (10) proposes the first 10 translations of
each unknown word.
4.1.2 Automatic Evaluation
Evaluating the quality of translations requires to
inspect lists of words each time we want to test a
variant of our approach. This cumbersome process
not only requires to understand the source language,
881
LT 5 000 10 000 50 000 100 000 200 000 500 000
p% r% p% r% p% r% p% r% p% r% p% r%
test-in
ANALOG 51.4 30.7 55.3 44.4 58.8 64.3 58.2 65.1 59.4 65.2 30.4 67.6
BASE1id 31.6 30.7 32.3 44.4 24.7 64.3 20.3 65.1 20.9 65.2 8.7 67.6
BASE2id 34.5 30.7 37.1 44.4 39.0 64.3 37.8 65.1 34.4 65.2 56.5 67.6
BASE110 26.7 100.0 28.3 100.0 23.9 100.0 20.0 100.0 16.6 100.0 11.8 100.0
BASE210 26.3 100.0 30.8 100.0 29.3 100.0 27.6 100.0 24.9 100.0 55.9 100.0
unk [3 171 , 9.1] [2 245 , 7.7] [754 , 4.0] [456 , 2.9] [253 , 2.0] [34 , 1.2]
test-out
ANALOG 52.8 28.9 55.3 42.5 52.9 68.8 54.7 74.6 55.7 81.0 43.3 88.2
BASE1id 28.0 28.9 29.0 42.5 27.3 68.8 23.1 74.6 26.8 81.0 22.7 88.2
BASE2id 32.9 28.9 35.0 42.5 32.5 68.8 35.9 74.6 40.8 81.0 59.1 88.2
BASE110 24.7 100.0 25.9 100.0 25.1 100.0 20.9 100.0 25.2 100.0 25.0 100.0
BASE210 21.7 100.0 26.4 100.0 27.2 100.0 29.4 100.0 33.6 100.0 57.9 100.0
unk [2 270 , 8.2] [1 701 , 6.9] [621 , 3.4] [402 , 2.4] [226 , 1.8] [76 , 1.4]
Table 2: Performance of the different approaches on the French-to-English direction as a function of the
number T of pairs of sentences used for training LT . A pair [n , t] in lines labeled by unk stands for the
number of words to translate, and the average number of their translations in Lref .
but happens to be in practice a delicate task. We
therefore decided to resort to an automatic evalua-
tion procedure which relies on Lref , a bilingual lex-
icon which entries are considered correct.
We translated all the words of Lref absent from
LT . We evaluated the different approaches by com-
puting response and precision rates. The response
rate is measured as the percentage of words for
which we do have at least one translation produced
(correct or not). The precision is computed in our
case as the percentage of words for which at least
one translation is sanctioned by Lref . Note that this
way of measuring response and precision is clearly
biased toward translation systems that can hypoth-
esize several candidate translations for each word,
as statistical systems usually do. The reason of this
choice was however guided by a lack of precision of
the reference we anticipated, a point we discuss in
Section 4.1.3.
The figures for the French-to-English direction
are reported in Table 2. We observe that the ratio
of unknown words that get a translation by ANA-
LOG is clearly impacted by the size of the lexicon
LT we use for computing analogies: the larger the
better. This was expected since the larger a lexicon
is, the higher the number of source analogies that
can be made and consequently, the higher the num-
ber of analogies that can be projected onto the out-
put space. The precision of ANALOG is rather stable
across variants and ranges between 50% to 60%.
The second observation we make is that the base-
lines perform worse than ANALOG in all but the
L500 000 cases. Since our baselines propose trans-
lations to each source word, their response rate is
maximum. Their precision, however, is an issue.
Expectedly, BASE1 is the worst of the two baselines.
If we arbitrarily fix the response rate of BASE2 to the
one of ANALOG, the former approach shows a far
lower precision (e.g., 34.4 against 59.4 for L200 000).
This not only indicates that analogical learning is
handling unknown words better than BASE2, but as
well, that a combination of both approaches could
potentially yield further improvements.
A last observation concerns the fact that ANALOG
performs equally well on the out-domain material.
This is very important from a practical point of view
and contrasts with some related work we discuss in
Section 5.
At first glance, the fact that BASE2 outperforms
ANALOG on the larger training size is disappoint-
ing. After investigations, we came to the conclusion
that this is mainly due to two facts. First, the num-
882
ber of unknown words on which both systems were
tested is rather low in this particular case (e.g., 34
for the in-domain corpus). Second, we noticed a de-
ficiency of the reference lexicon Lref for many of
those words. After all, this is not surprising since
the words unseen in the 500 000 pairs of training
sentences, but encountered in the full training cor-
pus (688 000 pairs) are likely to be observed only a
few times, therefore weakening the associations au-
tomatically acquired for these entries. We evaluate
that a third of the reference translations were wrong
in this setting, which clearly raises some doubts on
our automatic evaluation procedure in this case.
The performance of ANALOG across the three lan-
guage pairs are reported in Table 3. We observe a
drop of performance of roughly 10% (both in preci-
sion and response) for the German-to-English trans-
lation direction. This is likely due to the heuris-
tic procedure we apply during the search for stems,
which is not especially well suited for handling com-
pound words that are frequent in German.
We observe that for Spanish- and German-to-
English translation directions, the precision rate
tends to decrease for larger values of T . One ex-
planation for that is that we consider all analogies
equally likely in this work, while we clearly noted
that some are spurious ones. With larger training
material, spurious analogies become more likely.
French Spanish German
T p% r% p% r% p% r%
5 51.4 30.7 52.8 30.3 49.3 23.1
10 55.3 44.4 52.0 45.2 47.6 33.3
50 58.8 64.3 54.0 66.5 44.6 53.2
100 58.2 65.1 53.9 69.1 45.8 55.6
200 59.4 65.2 46.4 71.8 43.0 59.2
Table 3: Performance across language pairs mea-
sured on test-in. The number T of pairs of sen-
tences used for training LT is reported in thousands.
We measured the impact the translations produced
by ANALOG have on a state-of-the-art phrase-based
translation engine, which is described in (Patry et
al., 2006). For that purpose, we extended a phrase-
table with the first translation proposed by ANALOG
or BASE2 for each unknown word of the test ma-
terial. Results in terms of word-error-rate (WER)
and BLEU score (Papineni et al, 2002) are reported
in Table 4 for those sentences that contain at least
one unknown word. Small but consistent improve-
ments are observed for both metrics with ANALOG.
This was expected, since the original system sim-
ply leaves the unknown words untranslated. What
is more surprising is that the BASE2 version slightly
underperforms the baseline. The reason is that some
unknown words that should appear unmodified in
a translation, often get an erroneous translation by
BASE2. Forcing BASE2 to propose a translation
for the same words for which ANALOG found one,
slightly improves the figures (BASE2id).
French Spanish German
WER BLEU WER BLEU WER BLEU
base 61.8 22.74 54.0 27.00 69.9 18.15
+BASE2 61.8 22.72 54.2 26.89 70.3 18.05
+BASE2id 61.7 22.81 54.1 27.01 70.1 18.14
+ANALOG 61.6 22.90 53.7 27.27 69.7 18.30
sentences 387 452 814
Table 4: Translation quality produced by our phrase-
based SMT engine (base) with and without the
first translation produced by ANALOG, BASE2, or
BASE2id for each unknown word.
4.1.3 Manual Evaluation
As we already mentioned, the lexicon used as a
reference in our automatic evaluation procedure is
not perfect, especially for low frequency words. We
further noted that several words receive valid trans-
lations that are not sanctioned by Lref . This is for
instance the case of the examples in Figure 4, where
circumventing and fellow are arguably legitimate
translations of the French words contournant and
concitoyen, respectively. Note that in the second ex-
ample, the reference translation is in the plural form
while the French word is not.
Therefore, we conducted a manual evaluation of
the translations produced from L100 000 by ANA-
LOG and BASE2 on the 127 French words of the
corpus test-in8 unknown of Lref . Those are
the non-numerical unknown words the participat-
ing systems in the shared task had to face in the
8We did not notice important differences between test-in
and test-out.
883
contournant (49 candidates)
ANALOG  (circumventing,55) (undermining,20)
(evading,19) (circumvented,17) (overturning,16)
(circumvent,15) (circumvention,15) (bypass,13)
(evade,13) (skirt,12)
Lref  skirting, bypassing, by-pass, overcoming
concitoyen (24 candidates)
ANALOG  (citizens,26) (fellow,26) (fellow-
citizens,26) (people,26) (citizen,23) (fellow-
citizen,21) (fellows,5) (peoples,3) (civils,3) (fel-
lowship,2)
Lref  fellow-citizens
Figure 4: 10 best ranked candidate translations pro-
duced by ANALOG from L200 000 for two unknown
words and their sanctioned translations in Lref .
Words in bold are present in both the candidate and
the reference lists.
in-domain part of the test material. 75 (60%) of
those words received at least one valid translation
by ANALOG while only 63 (50%) did by BASE2.
Among those words that received (at least) one valid
translation, 61 (81%) were ranked first by ANA-
LOG against only 22 (35%) by BASE2. We fur-
ther observed that among the 52 words that did not
receive a valid translation by ANALOG, 38 (73%)
did not receive a translation at all. Those untrans-
lated words are mainly proper names (bush), foreign
words (munere), and compound words (rhe?nanie-
du-nord-westphalie), for which our approach is not
especially well suited.
We conclude from this informal evaluation that
80% of ordinary unknown words received a valid
translation in our French-to-English experiment, and
that roughly the same percentage had a valid trans-
lation proposed in the first place by ANALOG.
4.2 Translating Unknown Phrases
Our approach is not limited to translate solely un-
known words, but might serve as well to enrich
existing entries in a lexicon. For instance, low-
frequency words, often poorly handled by current
statistical methods, could receive useful translations.
This is illustrated in Figure 5 where we report the
best candidates produced by ANALOG for the French
word invite?es, which appears 7 times in the 200 000
invite?e (61 candidates)
ANALOG  (invited,135) (requested,92) (cal-
led,77) (urged,75) (guest,72) (asked,47) (re-
quest,43) (invites,27) (invite,26) (urge,26)
L200 000  asked, generate, urged
Figure 5: 10 best candidates produced by ANALOG
for the low-frequency French word invite?es and its
translations in L200 000.
first pairs of the training corpus. Interestingly, ANA-
LOG produced the candidate guest which corre-
sponds to a legitimate meaning of the French word
that was absent in the training data.
Because it can treat separators as any other char-
acter, ANALOG is not bounded to translate only
words. As a proof of concept, we applied analogical
reasoning to translate those source sequences of at
most 5 words in the test material that contain an un-
known word. Since there are many more sequences
than there are words, the input space in this exper-
iment is far larger, and we had to resort to a much
more aggressive pruning technique to find the stems
of the sequences to be translated.
expulsent  (expelling,36) (expel,31) (are ex-
pelling,23) (are expel,10)
focaliserai  (focus,10) (focus solely,9) (concen-
trate all,9) (will focus,9) (will placing,9)
de?passeront  (will exceed,4) (exceed,3) (will be
exceed,3) (we go beyond,2) (will be exceeding,2)
non-re?ussite de  (lack of success for,4) (lack of
success of,4) (lack of success,4)
que vous subissez  (you are experiencing,2)
Figure 6: Examples of translations produced by
ANALOG where the input (resp. output) space is
defined by the set of source (resp. target) word se-
quences. Words in bold are unknown.
We applied the automatic evaluation procedure
described in Section 4.1.2 for the French-to-English
translation direction, with a reference lexicon being
this time the phrase table acquired on the full train-
ing material.9 The response rate in this experiment is
particularly low since only a tenth of the sequences
9This model contains 1.5 millions pairs of phrases.
884
received (at least) a translation by ANALOG. Those
are short sequences that contain at most three words,
which clearly indicates the limitation of our prun-
ing strategy. Among those sequences that received
at least one translation, the precision rate is 55%,
which is consistent with the rate we measured while
translating words.
Examples of translations are reported in Figure 6.
We observe that single words are not contrived any-
more to be translated by a single word. This allows
to capture 1:n relations such as de?passeront?will
exceed, where the future tense of the French word is
adequately rendered by the modal will in English.
5 Related Work
We are not the first to consider the translation of un-
known words or phrases. Several authors have for
instance proposed approaches for translating proper
names and named entities (Chen et al, 1998; Al-
Onaizan and Knight, 2002). Our approach is com-
plementary to those ones.
Recently and more closely related to the approach
we described, Callison-Burch et al (2006) proposed
to replace an unknown phrase in a source sentence
by a paraphrase. Paraphrases in their work are ac-
quired thanks to a word alignment computed over
a large external set of bitexts. One important dif-
ference between their work and ours is that our ap-
proach does not require additional material.10 In-
deed, they used a rather idealistic set of large, ho-
mogeneous bitexts (European parliament debates) to
acquire paraphrases from. Therefore we feel our ap-
proach is more suited for translating ?low density?
languages and languages with a rich morphology.
Several authors considered as well the translation
of new words by relying on distributional colloca-
tional properties computed from a huge non-parallel
corpus (Rapp, 1999; Fung and Yee, 1998; Takaaki
and Matsuo, 1999; Koehn and Knight, 2002). Even
if admittedly non-parallel corpora are easier to ac-
quire than bitexts, this line of work is still heavily
dependent on huge external resources.
Most of the analogies made at the word level in
our study are capturing morphological information.
10We do use a target vocabulary list to filter out spurious
analogies, but we believe we could do without. The frequency
with which we generate a string could serve to decide upon its
legitimacy.
The use of morphological analysis in (statistical)
machine translation has been the focus of several
studies, (Nie?en, 2002) among the first. Depend-
ing on the pairs of languages considered, gains have
been reported when the training material is of mod-
est size (Lee, 2004; Popovic and Ney, 2004; Gold-
water and McClosky, 2005). Our approach does not
require any morphological knowledge of the source,
the target, or both languages. Admittedly, several
unsupervised morphological induction methodolo-
gies have been proposed, e.g., the recent approach
in Freitag (2005). In any case, as we have shown,
ANALOG is not bounded to treat only words, which
we believe to be at our advantage.
6 Discussion and Future Work
In this paper, we have investigated the appropri-
ateness of analogical learning to handle unknown
words in machine translation. On the contrary to
several lines of work, our approach does not rely on
massive additional resources but capitalizes instead
on an information which is inherently pertaining to
the language. We measured that roughly 80% of or-
dinary unknown French words can receive a valid
translation into English with our approach.
This work is currently being developed in several
directions. First, we are investigating why our ap-
proach remains silent for some words or phrases.
This will allow us to better characterize the limita-
tions of ANALOG and will hopefully lead us to de-
sign a better strategy for identifying the stems of a
given word or phrase. Second, we are investigat-
ing how a systematic enrichment of a phrase-transfer
table will impact a phrase-based statistical machine
translation engine. Last, we want to investigate the
training of a model that can learn regularities from
the analogies we are making. This would relieve us
from requiring the training material while translat-
ing, and would allow us to compare our approach
with other methods proposed for unsupervised mor-
phology acquisition.
Acknowledgement We are grateful to the anony-
mous reviewers for their useful suggestions and to
Pierre Poulin for his fruitful comments. This study
has been partially funded by NSERC.
885
References
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. of the 40th ACL, pages 400?408,
Philadelphia, Pennsylvania, USA.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proc. of HLT-NAACL, pages 17?
24, New York City, USA.
Hsin-Hsi Chen, Sheng-Jie Hueng, Yung-Wei Ding, and
Shih-Chung Tsai. 1998. Proper name translation
in cross-language information retrieval. In Proc. of
the 17th COLING, pages 232?236, Montreal, Que?bec,
Canada.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In International
Workshop on Spoken Language Translation (IWSLT),
Pittsburgh, Pennsylvania, USA.
Dayne Freitag. 2005. Morphology induction from term
clusters. In Proc. of the 9th CoNLL, pages 128?135,
Ann Arbor, Michigan, USA.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proc. of the 36th ACL, pages 414?420,
San Francisco, California, USA.
Dedre Gentner, Keith J. Holyoak, and Boicho N.
Konikov. 2001. The Analogical Mind. The MIT
Press, Cambridge, Massachusetts, USA.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological analy-
sis. In Proc. of HLT-EMNLP, pages 676?683, Van-
couver, British Columbia, Canada.
Philipp Koehn and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
Proc. of the ACL Workshop on Unsupervised Lexical
Acquisition, pages 9?16, Philadelphia, Pennsylvania,
USA.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proc. of the 10th EACL,
pages 187?193, Budapest, Hungary.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proc. of the HLT-NAACL Work-
shop on Statistical Machine Translation, pages 102?
121, New York City, USA.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. of HLT-NAACL,
Boston, Massachusetts, USA.
Yves Lepage and Etienne Denoual. 2005. ALEPH: an
EBMT system based on the preservation of propor-
tionnal analogies between sentences across languages.
In Proc. of IWSLT, Pittsburgh, Pennsylvania, USA.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proc. of COLING-ACL, pages 728?734,
Montreal, Que?bec, Canada.
Yves Lepage. 2003. De l?analogie rendant compte de la
commutation en linguistique. Ph.D. thesis, Universite?
Joseph Fourier, Grenoble, France.
Vladimir. I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. Sov.
Phys. Dokl., 6:707?710.
Fabienne Moreau, Vincent Claveau, and Pascale Se?billot.
2007. Automatic morphological query expansion us-
ing analogy-based machine learning. In Proc. of the
29th ECIR, Roma, Italy.
Sonja Nie?en. 2002. Improving Statistical Ma-
chine Translation using Morpho-syntactic Informa-
tion. Ph.D. thesis, RWTH, Aachen, Germany.
Franz-Joseph Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of the 38th ACL,
pages 440?447, Hong Kong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of the 40th ACL,
pages 311?318, Philadelphia, Pennsylvania, USA.
Alexandre Patry, Fabrizo Gotti, and Philippe Langlais.
2006. Mood at work: Ramses versus Pharaoh. In
Proc. of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 126?129, New York City,
USA.
Maja Popovic and Hermann Ney. 2004. Towards the
use of word stems and suffixes for statistical machine
translation. In Proc. of the 4th LREC, pages 1585?
1588, Lisbon, Portugal.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proc. of the 37th ACL, pages 519?526, Col-
lege Park, Maryland, USA.
Nicolas Stroppa and Franc?ois Yvon. 2005. An analog-
ical learner for morphological analysis. In Proc. of
the 9th CoNLL, pages 120?127, Ann Arbor, Michigan,
USA.
Tanaka Takaaki and Yoshihiro Matsuo. 1999. Extrac-
tion of translation equivalents from non-parallel cor-
pora. In Proc. of the 8th TMI, pages 109?119, Chester,
England.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416, Sept.
886
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 487?495,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improvements in Analogical Learning:
Application to Translating multi-Terms of the Medical Domain
Philippe Langlais
DIRO
Univ. of Montreal, Canada
felipe@iro.umontreal.ca
Franc?ois Yvon and Pierre Zweigenbaum
LIMSI-CNRS
Univ. Paris-Sud XI, France
{yvon,pz}@limsi.fr
Abstract
Handling terminology is an important
matter in a translation workflow. However,
current Machine Translation (MT) sys-
tems do not yet propose anything proactive
upon tools which assist in managing termi-
nological databases. In this work, we in-
vestigate several enhancements to analog-
ical learning and test our implementation
on translating medical terms. We show
that the analogical engine works equally
well when translating from and into a mor-
phologically rich language, or when deal-
ing with language pairs written in differ-
ent scripts. Combining it with a phrase-
based statistical engine leads to significant
improvements.
1 Introduction
If machine translation is to meet commercial
needs, it must offer a sensible approach to trans-
lating terms. Currently, MT systems offer at best
database management tools which allow a human
(typically a translator, a terminologist or even the
vendor of the system) to specify bilingual ter-
minological entries. More advanced tools are
meant to identify inconsistencies in terminological
translations and might prove useful in controlled-
language situations (Itagaki et al, 2007).
One approach to translate terms consists in us-
ing a domain-specific parallel corpus with stan-
dard alignment techniques (Brown et al, 1993) to
mine new translations. Massive amounts of par-
allel data are certainly available in several pairs
of languages for domains such as parliament de-
bates or the like. However, having at our disposal
a domain-specific (e.g. computer science) bitext
with an adequate coverage is another issue. One
might argue that domain-specific comparable (or
perhaps unrelated) corpora are easier to acquire,
in which case context-vector techniques (Rapp,
1995; Fung and McKeown, 1997) can be used
to identify the translation of terms. We certainly
agree with that point of view to a certain extent,
but as discussed by Morin et al (2007), for many
specific domains and pairs of languages, such re-
sources simply do not exist. Furthermore, the task
of translation identification is more difficult and
error-prone.
Analogical learning has recently regained some
interest in the NLP community. Lepage and De-
noual (2005) proposed a machine translation sys-
tem entirely based on the concept of formal anal-
ogy, that is, analogy on forms. Stroppa and
Yvon (2005) applied analogical learning to sev-
eral morphological tasks also involving analogies
on words. Langlais and Patry (2007) applied it to
the task of translating unknown words in several
European languages, an idea investigated as well
by Denoual (2007) for a Japanese to English trans-
lation task.
In this study, we improve the state-of-the-art of
analogical learning by (i) proposing a simple yet
effective implementation of an analogical solver;
(ii) proposing an efficient solution to the search is-
sue embedded in analogical learning, (iii) investi-
gating whether a classifier can be trained to recog-
nize bad candidates produced by analogical learn-
ing. We evaluate our analogical engine on the task
of translating terms of the medical domain; a do-
main well-known for its tendency to create new
words, many of which being complex lexical con-
structions. Our experiments involve five language
pairs, including languages with very different mor-
phological systems.
487
In the remainder of this paper, we first present
in Section 2 the principle of analogical learn-
ing. Practical issues in analogical learning are
discussed in Section 3 along with our solutions.
In Section 4, we report on experiments we con-
ducted with our analogical device. We conclude
this study and discuss future work in Section 5.
2 Analogical Learning
2.1 Definitions
A proportional analogy, or analogy for short, is a
relation between four items noted [x : y = z : t ]
which reads as ?x is to y as z is to t?. Among pro-
portional analogies, we distinguish formal analo-
gies, that is, those we can identify at a graphemic
level, such as [adrenergic beta-agonists, adren-
ergic beta-antagonists, adrenergic alpha-agonists,
adrenergic alpha-antagonists].
Formal analogies can be defined in terms of
factorizations1. Let x be a string over an alpha-
bet ?, a factorization of x, noted fx, is a se-
quence of n factors fx = (f1x, . . . , f
n
x ), such that
x = f1x  f
2
x  . . .  f
n
x , where  denotes the
concatenation operator. After (Stroppa and Yvon,
2005) we thus define a formal analogy as:
Definition 1 ?(x, y, z, t) ? ??
4
, [x : y = z : t] iff
there exist factorizations (fx, fy, fz, ft) ? (?
?d)4
of (x, y, z, t) such that, ?i ? [1, d], (f iy, f
i
z) ?{
(f ix, f
i
t ), (f
i
t , f
i
x)
}
. The smallest d for which this
definition holds is called the degree of the analogy.
Intuitively, this definition states that (x, y, z, t)
are made up of a common set of alternating sub-
strings. It is routine to check that it captures the
exemplar analogy introduced above, based on the
following set of factorizations:
fx ? (adrenergic bet, a-agonists)
fy ? (adrenergic bet, a-antagonists)
fz ? (adrenergic alph, a-agonists)
ft ? (adrenergic alph, a-antagonists)
As no smaller factorization can be found, the de-
gree of this analogy is 2. In the sequel, we call
an analogical equation an analogy where one item
(usually the fourth) is missing and we note it [x :
y = z : ? ].
1Factorizations of strings correspond to segmentations.
We keep the former term, to emphasize the genericity of the
definition, which remains valid for other algebraic structures,
for which factorization and segmentation are no longer syno-
mymous.
2.2 Analogical Inference
Let L = {(i, o) | i ? I, o ? O} be a learning set
of observations, where I (O) is the set of possible
forms of the input (output) linguistic system under
study. We denote I(u) (O(u)) the projection of u
into the input (output) space; that is, if u = (i, o),
then I(u) ? i and O(u) ? o. For an incomplete
observation u = (i, ?), the inference procedure is:
1. building EI(u) = {(x, y, z) ? L3 | [I(x) :
I(y) = I(z) : I(u) ]}, the set of input triplets
that define an analogy with I(u) .
2. building EO(u) = {o ? O | ?(x, y, z) ?
EI(u) s.t. [O(x) : O(y) = O(z) : o]} the set
of solutions to the equations obtained by pro-
jecting the triplets of EI(u) into the output
space.
3. selecting candidates among EO(u).
In the sequel, we distinguish the generator
which implements the first two steps, from the se-
lector which implements step 3.
To give an example, assume L contains
the following entries: (beeta-agonistit, adren-
ergic beta-agonists), (beetasalpaajat, adrenergic
beta-antagonists) and (alfa-agonistit, adrener-
gic alpha-agonists). We might translate the
Finnish term alfasalpaajat into the English term
adrenergic alpha-antagonists by 1) identifying
the input triplet: (beeta-agonistit, beetasalpaa-
jat, alfa-agonistit) ; 2) projecting it into the equa-
tion [adrenergic beta-agonists : adrenergic beta-
antagonists = adrenergic alpha-agonists : ? ]; and
solving it: adrenergic alpha-antagonists is one of
its solutions.
During inference, analogies are recognized in-
dependently in the input and the output space, and
nothing pre-establishes which subpart of one in-
put form corresponds to which subpart of the out-
put one. This ?knowledge? is passively captured
thanks to the inductive bias of the learning strat-
egy (an analogy in the input space corresponds to
one in the output space). Also worth mentioning,
this procedure does not rely on any pre-defined no-
tion of word. This might come at an advantage for
languages that are hard to segment (Lepage and
Lardilleux, 2007).
3 Practical issues
Each step of analogical learning, that is, search-
ing for input triplets, solving output equations and
488
selecting good candidates involves some practical
issues. Since searching for input triplets might in-
volve the need for solving (input) equations, we
discuss the solver first.
3.1 The solver
Lepage (1998) proposed an algorithm for solving
an analogical equation [x : y = z : ? ]. An
alignment between x and y and between x and z
is first computed (by edit-distance) as illustrated
in Figure 1. Then, the three strings are synchro-
nized using x as a backbone of the synchroniza-
tion. The algorithm can be seen as a deterministic
finite-state machine where a state is defined by the
two edit-operations being visited in the two tables.
This is schematized by the two cursors in the fig-
ure. Two actions are allowed: copy one symbol
from y or z into the solution and move one or both
cursors.
x: r e a d e r x: r e a d e r
y: r e a d a b l e z: d o e r
4 4
Figure 1: Illustration of the synchronization done
by the solver described in (Lepage, 1998).
There are two things to realize with this algo-
rithm. First, since several (minimal-cost) align-
ments can be found between two strings, several
synchronizations are typically carried out while
solving an equation, leading to (possibly many)
different solutions. Indeed, in adverse situations,
an exponential number of synchronizations will
have to be computed. Second, the algorithm fails
to deliver an expected form in a rather frequent
situation where two identical symbols align fortu-
itously in two strings. This is for instance the case
in our running example where the symbol d in
doer aligns to the one in reader, which puzzles the
synchronization. Indeed, dabloe is the only form
proposed to [reader : readable = doer : ? ], while
the expected one is doable. The algorithm would
have no problem, however, to produce the form
writable out of the equation [reader : readable =
writer : ? ].
Yvon et al (2004) proposed an analogical
solver which is not exposed to the latter prob-
lem. It consists in building a finite state transducer
which generates the solutions to [x : y = z : ? ]
while recognizing the form x.
Theorem 1 t is a solution to [x : y = z : ?] iff
t belongs to {y ? z}\x.
shuffle and complement are two rational op-
erations. The shuffle of two strings w and
v, noted w ? v, is the regular language con-
taining the strings obtained by selecting (with-
out replacement) alternatively in w and v, se-
quences of characters in a left-to-right man-
ner. For instance, spondyondontilalgiatis and
ondspondonylaltitisgia are two strings belong-
ing to spondylalgia ? ondontitis). The comple-
mentary set of w with respect to v, noted w\v, is
the set of strings formed by removing from w, in
a left-to-right manner, the symbols in v. For in-
stance, spondylitis and spydoniltis are belong-
ing to spondyondontilalgiatis \ ondontalgia.
Our implementation of the two rational operations
are sketched in Algorithm 1.
Because the shuffle of two strings may con-
tain an exponential number of elements with re-
spect to the length of those strings, building such
an automaton may face combinatorial problems.
Our solution simply consists in randomly sam-
pling strings in the shuffle set. Our solver, depicted
in Algorithm 2, is thus controlled by a sampling
size s, the impact of which is illustrated in Ta-
ble 1. By increasing s, the solver generates more
(mostly spurious) solutions, but also increases the
relative frequency with which the expected output
is generated. In practice, provided a large enough
sampling size,2 the expected form very often ap-
pears among the most frequent ones.
s nb (solution,frequency)
10 11 (doable,7) (dabloe,3) (adbloe,3)
102 22 (doable,28) (dabloe,21) (abldoe,21)
103 29 (doable,333) (dabloe,196) (abldoe,164)
Table 1: The 3-most frequent solutions generated
by our solver, for different sampling sizes s, for
the equation [reader : readable = doer : ? ]. nb
indicates the number of (different) solutions gen-
erated. According to our definition, there are 32
distinct solutions to this equation. Note that our
solver has no problem producing doable.
3.2 Searching for input triplets
A brute-force approach to identifying the input
triplets that define an analogy with the incom-
plete observation u = (t, ?) consists in enumerat-
ing triplets in the input space and checking for an
2We used s = 2 000 in this study.
489
function shuffle(y,z)
Input: ?y, z? two forms
Output: a random word in y ? z
if y =  then
return z
else
n? rand(1,|y|)
return y[1:n] . shuffle(z,y[n+1:])
function complementary(m,x,r,s)
Input: m ? y ? z, x
Output: the set m \ x
if (m = ) then
if (x = ) then
s? s ? r
else
complementary(m[2:],x,r.m[1],s)
if m[1] = x[1] then
complementary(m[2:],x[2:],r,s)
Algorithm 1: Simulation of the two rational op-
erations required by the solver. x[a:b] denotes the
sequence of symbols x starting from index a to
index b inclusive. x[a:] denotes the suffix of x
starting at index a.
analogical relation with t. This amounts to check
o(|I|3) analogies, which is manageable for toy
problems only. Instead, Langlais and Patry (2007)
proposed to solve analogical equations [y : x = t :
? ] for some pairs ?x, y? belonging to the neighbor-
hood3 of I(u), denotedN (t). Those solutions that
belong to the input space are the z-forms retained;
EI(u) = { ?x, y, z? : x ? N (t) , y ? N (x),
z ? [y : x = t : ? ] ? I }
This strategy (hereafter named LP) directly fol-
lows from a symmetrical property of an analogy
([x : y = z : t ] ? [y : x = t : z]), and reduces
the search procedure to the resolution of a number
of analogical equations which is quadratic with the
number of pairs ?x, y? sampled.
We found this strategy to be of little use for
input spaces larger than a few tens of thousands
forms. To solve this problem, we exploit a prop-
erty on symbol counts that an analogical relation
must fulfill (Lepage, 1998):
[x : y = z : t ]? |x|c + |t|c = |y|c + |z|c ?c ? A
3The authors proposed to sample x and y among the clos-
est forms in terms of edit-distance to I(u).
function solver(?x, y, z?, s)
Input: ?x, y, z?, a triplet, s the sampling size
Output: a set of solutions to [x : y = z : ? ]
sol? ?
for i? 1 to s do
?a, b? ? odd(rand(0, 1))? ?z, y? : ?y, z?
m ? shuffle(a,b )
c? complementary(m,x,,{})
sol? sol ? c
return sol
Algorithm 2: A Stroppa&Yvon flavored solver.
rand(a, b) returns a random integer between a
and b (included). The ternary operator ?: is to
be understood as in the C language.
where A is the alphabet on which the forms are
built, and |x|c stands for the number of occur-
rences of symbol c in x.
Our search strategy (named TC) begins by se-
lecting an x-form in the input space. This en-
forces a set of necessary constraints on the counts
of characters that any two forms y and z must sat-
isfy for [x : y = z : t ] to be true. By considering
all forms x in turn,4 we collect a set of candidate
triplets for t. A verification of those that define
with t an analogy must then be carried out. For-
mally, we built:
EI(u) = { ?x, y, z? : x ? I,
?y, z? ? C(?x, t?),
[x : y = z : t ] }
where C(?x, t?) denotes the set of pairs ?y, z?
which satisfy the count property.
This strategy will only work if (i) the number
of quadruplets to check is much smaller than the
number of triplets we can form in the input space
(which happens to be the case in practice), and
if (ii) we can efficiently identify the pairs ?y, z?
that satisfy a set of constraints on character counts.
To this end, we proposed in (Langlais and Yvon,
2008) to organize the input space into a data struc-
ture which supports efficient runtime retrieval.
3.3 The selector
Step 3 of analogical learning consists in selecting
one or several solutions from the set of candidate
forms produced by the generator. We trained in
a supervised manner a binary classifier to distin-
guish good translation candidates (as defined by
4Anagram forms do not have to be considered separately.
490
a reference) from spurious ones. We applied to
this end the voted-perceptron algorithm described
by Freund and Schapire (1999). Online voted-
perceptrons have been reported to work well in a
number of NLP tasks (Collins, 2002; Liang et al,
2006). Training such a classifier is mainly a matter
of feature engineering. An example e is a pair of
source-target analogical relations (r, r?) identified
by the generator, and which elects t? as a transla-
tion for the term t:
e ? (r, r?) ? ([x : y = z : t], [x? : y? = z? : t?])
where x?, y?, and z? are respectively the projections
of the source terms x, y and z. We investigated
many features including (i) the degree of r and r?,
(ii) the frequency with which a form is generated,5
(iii) length ratios between t and t?, (iv) likelihoods
scores (min, max, avg.) computed by a character-
based n-gram model trained on a large general cor-
pus (without overlap to DEV or TRAIN), etc.
4 Experiments
4.1 Calibrating the engine
We compared the two aforementioned searching
strategies on a task of identifying triplets in an
input space of French words for 1 000 randomly
selected test words. We considered input spaces
of various sizes. The results are reported in Ta-
ble 2. TC clearly outperforms LP by systemati-
cally identifying more triplets in much less time.
For the largest input space of 84 000 forms, TC
could identify an average of 746 triplets for 946
test words in 1.2 seconds, while the best compro-
mise we could settle with LP allows the identifi-
cation of 56 triplets on average for 889 words in
6.3 seconds on average. Note that in this exper-
iment, LP was calibrated for each input space so
that the best compromise between recall (%s) and
speed could be found. Reducing the size of the
neighborhood in LP improves computation time,
but significantly affects recall. In the following,
we only consider the TC search strategy.
4.2 Experimental Protocol
Datasets The data we used in this study comes
from the Medical Subject Headings (MeSH) the-
saurus. This thesaurus is used by the US National
Library of Medicine to index the biomedical sci-
5A form t? may be generated thanks to many examples.
s %s (s) s %s (s) s %s (s)
TC 34 83.1 0.2 261 94.1 0.5 746 96.4 1.2
LP 17 71.7 7.4 46 85.0 7.6 56 88.9 6.3
|I| 20 000 50 000 84 076
Table 2: Average number s of input analogies
found over 1 000 test words as a function of the
size of the input space. %s stands for the percent-
age of source forms for which (at least) one source
triplet is found; and (s) indicates the average time
(counted in seconds) to treat one form.
entific literature in the MEDLINE database.6 Its
preferred terms are called ?Main Headings?. We
collected pairs of source and target Main Head-
ings (TTY = ?MH?) with the same MeSH identi-
fiers (SDUI).
We considered five language pairs with three
relatively close European languages (English-
French, English-Spanish and English-Swedish), a
more distant one (English-Finnish) and one pair
involving different scripts (English-Russian).7
The material was split in three randomly se-
lected parts, so that the development and test ma-
terial contain exactly 1 000 terms each. The char-
acteristics of this material are reported in Table 3.
For the Finnish-English and Swedish-English lan-
guage pairs, the ratio of uni-terms in the Foreign
language (uf%) is twice the ratio of uni-terms in
the English counterpart. This is simply due to
the agglutinative nature of these two languages.
For instance, according to MeSH, the English
multi-term speech articulation tests corresponds
to the Finnish uni-term a?a?nta?miskokeet and to the
Swedish one artikulationstester. The ratio of out-
of-vocabulary forms (space-separated words un-
seen in TRAIN) in the TEST material is rather
high: between 36% and 68% for all Foreign-
to-English translation directions, but Finnish-to-
English, where surprisingly, only 6% of the word
forms are unknown.
Evaluation metrics For each experimental con-
dition, we compute the following measures:
Coverage the fraction of input words for which
the system can generate translations. If Nt words
receive translations among N , coverage is Nt/N .
6The MeSH thesaurus and its translations are included in
the UMLS Metathesaurus.
7Russian MeSH is normally written in Cyrillic, but some
terms are simply English terms written in uppercase Latin
script (e.g., ACHROMOBACTER for English Achromobac-
ter). We removed those terms.
491
TRAIN TEST DEV TEST
f nb uf% ue% nb uf% uf% oov%
FI 19 787 63.7 33.7 1 000 64.2 64.0 5.7
FR 17 230 29.8 29.3 1 000 30.8 28.3 36.3
RU 21 407 38.6 38.6 1 000 38.5 40.2 44.4
SP 19 021 31.1 31.1 1 000 31.7 33.3 36.6
SW 17 090 67.9 32.5 1 000 67.4 67.9 68.4
Table 3: Main characteristics of our datasets. nb
indicates the number of pairs of terms in a bi-
text, uf% (ue%) stands for the percentage of uni-
terms in the Foreign (English) part. oov% indi-
cates the percentage of out-of-vocabulary forms
(space-separated forms of TEST unseen in TRAIN).
Precision among the Nt words for which the
system proposes an answer, precision is the pro-
portion of those for which a correct translation is
output. Depending on the number of output trans-
lations k that one is willing to examine, a correct
translation will be output for Nk input words. Pre-
cision at rank k is thus defined as Pk = Nk/Nt.
Recall is the proportion of the N input words
for which a correct translation is output. Recall at
rank k is defined as Rk = Nk/N .
In all our experiments, candidate translations
are sorted in decreasing order of frequency with
which they were generated.
4.3 The generator
The performances of the generator on the 10
translation sessions are reported in Table 4.
The coverage of the generator varies between
38.5% (French-to-English) and 47.1% (English-
to-Finnish), which is rather low. In most cases, the
silence of the generator is due to a failure to iden-
tify analogies in the input space (step 1). The last
column of Table 4 reports the maximum recall we
can obtain if we consider all the candidates output
by the generator. The relative accuracy of the gen-
erator, expressed by the ratio ofR? to cov, ranges
from 64.3% (English-French) to 79.1% (Spanish-
to-English), for an average value of 73.8% over
all translation directions. This roughly means that
one fourth of the test terms with at least one solu-
tion do not contain the reference.
Overall, we conclude that analogical learning
offers comparable performances for all transla-
tion directions, although some fluctuations are ob-
served. We do not observe that the approach is
affected by language pairs which do not share the
Cov P1 R1 P100 R100 R?
? FI 47.1 31.6 14.9 57.7 27.2 31.9
FR 41.2 35.4 14.6 60.4 24.9 26.5
RU 46.2 40.5 18.7 69.9 32.3 34.8
SP 47.0 41.5 19.5 69.1 32.5 35.9
SW 42.8 36.0 15.4 66.8 28.6 31.9
? FI 44.8 36.6 16.4 66.7 29.9 33.2
FR 38.5 47.0 18.1 69.9 26.9 29.4
RU 42.1 49.4 20.8 70.3 29.6 32.3
SP 42.6 47.7 20.3 75.1 32.0 33.7
SW 44.6 40.8 18.2 69.5 31.0 32.9
Table 4: Main characteristics of the generator, as a
function of the translation directions (TEST).
same script (Russian/English). The best (worse)
case (as far as R? is concerned) corresponds to
translating into Spanish (French).
Admittedly, the largest recall andR? values re-
ported in Table 4 are disappointing. Clearly, for
analogical learning to work efficiently, enough lin-
guistic phenomena must be attested in the TRAIN
material. To illustrate this, we collected for the
Spanish-English language pair a set of medical
terms from the Medical Drug Regulatory Activi-
ties thesaurus (MedDRA) which contains roughly
three times more terms than the Spanish-English
material used in this study. This extra material al-
lows to raise the coverage to 73.4% (Spanish to
English) and 79.7% (English to Spanish), an abso-
lute improvement of more than 30%.
4.4 The selector
We trained our classifiers on the several millions
of examples generated while translating the devel-
opment material. Since we considered numerous
feature representations in this study, this implies
saving many huge datafiles on disk. In order to
save some space, we decided to remove forms that
were generated less than 3 times.8 Each classifier
was trained using 20 epochs.
It is important to note that we face a very unbal-
anced task. For instance, for the English to Finnish
task, the generator produces no less than 2.7 mil-
lions of examples, among which only 4 150 are
positive ones. Clearly, classifying all the examples
as negative will achieve a very high classification
accuracy, but will be of no practical use. There-
fore, we measure the ability of a classifier to iden-
8Averaged over all translation directions, this incurs an
absolute reduction of the coverage of 3.4%.
492
FI?EN FR?EN RU?EN SP?EN SW?EN
p r p r p r p r p r
argmax-f1 41.3 56.7 46.7 63.9 48.1 65.6 49.2 63.4 43.2 61.0
s-best 53.6 61.3 57.5 68.4 61.9 66.7 64.3 70.0 53.1 64.4
Table 5: Precision (p) and recall (r) of some classifiers on the TEST material.
tify the few positive forms among the set of candi-
dates. We measure precision as the percentage of
forms selected by the classifier that are sanctioned
by the reference lexicon, and recall as the percent-
age of forms selected by the classifier over the to-
tal number of sanctioned forms that the classifier
could possibly select. (Recall that the generator
often fails to produce oracle forms.)
The performance measured on the TEST mate-
rial of the best classifier we monitored on DEV
are reported in Table 5 for the Foreign-to-English
translation directions (we made consistent obser-
vations on the reverse directions). For compari-
son purposes, we implemented a baseline classi-
fier (lines argmax-f1) which selects the most-
frequent candidate form. This is the selector
used as a default in several studies on analogi-
cal learning (Lepage and Denoual, 2005; Stroppa
and Yvon, 2005). The baseline identifies between
56.7% to 65.6% of the sanctioned forms, at pre-
cision rates ranging from 41.3% to 49.2%. We
observe for all translation directions that the best
classifier we trained systematically outperforms
this baseline, both in terms of precision and recall.
4.4.1 The overall system
Table 6 shows the overall performance of the ana-
logical translation device in terms of precision, re-
call and coverage rates as defined in Section 4.2.
Overall, our best configuration (the one embed-
ding the s-best classifier) translates between
19.3% and 22.5% of the test material, with a preci-
sion ranging from 50.4% to 63.2%. This is better
than the variant which always proposes the most
frequent generated form (argmax-f1). Allowing
more answers increases both precision and recall.
If we allow up to 10 candidates per source term,
the analogical translator translates one fourth of
the terms (26.1%) with a precision of 70.9%, aver-
aged over all translation directions. The oracle
variant, which looks at the reference for select-
ing the good candidates produced by the genera-
tor, gives an upper bound of the performance that
could be obtained with our approach: less than
a third of the source terms can be translated cor-
rectly. Recall however that increasing the TRAIN
material leads to drastic improvements in cover-
age.
4.5 Comparison with a PB-SMT engine
To put these figures in perspective, we mea-
sured the performance of a phrase-based statisti-
cal MT (PB-SMT) engine trained to handle the
same translation task. We trained a phrase table
on TRAIN, using the standard approach.9 How-
ever, because of the small training size, and the
rather huge OOV rate of the translation tasks we
address, we did not train translation models on
word-tokens, but at the character level. There-
fore a phrase is indeed a sequence of charac-
ters. This idea has been successively investigated
in a Catalan-to-Spanish translation task by Vi-
lar et al (2007). We tuned the 8 coefficients of
the so-called log-linear combination maximized
at decoding time on the first 200 pairs of terms
of the DEV corpora. On the DEV set, BLEU
scores10 range from 67.2 (English-to-Finnish) to
77.0 (Russian-to-English).
Table 7 reports the precision and recall of both
translation engines. Note that because the SMT
engine always propose a translation, its precision
equals its recall. First, we observe that the preci-
sion of the SMT engine is not high (between 17%
and 31%), which demonstrates the difficulty of
the task. The analogical device does better for all
translation directions (see Table 6), but at a much
lower recall, remaining silent more than half of
the time. This suggests that combining both sys-
tems could be advantageous. To verify this, we
ran a straightforward combination: whenever the
analogical device produces a translation, we pick
it; otherwise, the statistical output is considered.
The gains of the resulting system over the SMT
alone are reported in column ?B. Averaged over
9We used the scripts distributed by Philipp Koehn to train
the phrase-table, and Pharaoh (Koehn, 2004) for producing
the translations.
10We computed BLEU scores at the character level.
493
FI?EN FR?EN RU?EN SP?EN SW?EN
k Pk Rk Pk Rk Pk Rk Pk Rk Pk Rk
argmax-f 1 41.3 17.3 46.7 16.8 47.8 18.6 48.7 19.2 43.4 18.1
10 61.6 25.8 62.8 22.6 61.7 24.0 69.3 27.3 62.1 25.9
s-best 1 53.5 20.8 56.9 19.3 58.5 20.3 63.2 22.5 50.4 21
10 69.4 27.0 69.0 23.4 71.8 24.9 78.4 27.9 65.7 27.4
oracle 1 100 30.5 100 26.3 100 28.5 100 30.6 100 29.5
Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).
all translation directions, BLEU scores increase on
TEST from 66.2 to 71.5, that is, an absolute im-
provement of 5.3 points.
? EN ? EN
Psmt ?B Psmt ?B
FI 20.2 +7.4 21.6 +6.4
FR 19.9 +5.3 17.0 +6.0
RU 24.1 +3.1 28.0 +6.4
SP 22.1 +4.9 26.4 +5.5
SW 25.9 +4.2 31.6 +3.2
Table 7: Translation performances on TEST. Psmt
stands for the precision and recall of the SMT en-
gine. ?B indicates the absolute gain in BLEU
score of the combined system.
We noticed a tendency of the statistical engine
to produce literal translations; a default the ana-
logical device does not show. For instance, the
Spanish term instituciones de atencio?n ambulato-
ria is translated word for word by Pharaoh into
institutions, atention ambulatory while analogical
learning produces ambulatory care facilities. We
also noticed that analogical learning sometimes
produces wrong translations based on morpholog-
ical regularities that are applied blindly. This is,
for instance, the case in a Russian/English exam-
ple where mouthal manifestations is produced, in-
stead of oral manifestations.
5 Discussion and future work
In this study, we proposed solutions to practical is-
sues involved in analogical learning. A simple yet
effective implementation of a solver is described.
A search strategy is proposed which outperforms
the one described in (Langlais and Patry, 2007).
Also, we showed that a classifier trained to se-
lect good candidate translations outperforms the
most-frequently-generated heuristic used in sev-
eral works on analogical learning.
Our analogical device was used to translate
medical terms in different language pairs. The
approach rates comparably across the 10 transla-
tion directions we considered. In particular, we
do not see a drop in performance when trans-
lating into a morphology rich language (such as
Finnish), or when translating into languages with
different scripts. Averaged over all translation di-
rections, the best variant could translate in first po-
sition 21% of the terms with a precision of 57%,
while at best, one could translate 30% of the terms
with a perfect precision. We show that the ana-
logical translations are of better quality than those
produced by a phrase-based engine trained at the
character level, albeit with much lower recall. A
straightforward combination of both approaches
led an improvement of 5.3 BLEU points over the
SMT alone. Better SMT performance could be
obtained with a system based on morphemes, see
for instance (Toutanova et al, 2008). However,
since lists of morphemes specific to the medical
domain do not exist for all the languages pairs we
considered here, unsupervised methods for acquir-
ing morphemes would be necessary, which is left
as a future work. In any case, this comparison is
meaningful, since both the SMT and the analogi-
cal device work at the character level.
This work opens up several avenues. First, we
will test our approach on terminologies from dif-
ferent domains, varying the size of the training
material. Second, analyzing the segmentation in-
duced by analogical learning would be interesting.
Third, we need to address the problem of com-
bining the translations produced by analogy into a
front-end statistical translation engine. Last, there
is no reason to constrain ourselves to translating
terminology only. We targeted this task in the first
place, because terminology typically plugs trans-
lation systems, but we think that analogical learn-
ing could be useful for translating infrequent enti-
ties.
494
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
M. Collins. 2002. Discriminative training methods for
hidden markov models: theory and experiments with
perceptron algorithms. In EMNLP, pages 1?8, Mor-
ristown, NJ, USA.
E. Denoual. 2007. Analogical translation of unknown
words in a statistical machine translation framework.
In MT Summit, XI, pages 10?14, Copenhagen.
Y. Freund and R. E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Mach.
Learn., 37(3):277?296.
P. Fung and K. McKeown. 1997. Finding terminology
translations from non-parallel corpora. In 5th An-
nual Workshop on Very Large Corpora, pages 192?
202, Hong Kong.
M. Itagaki, T. Aikawa, and X. He. 2007. Auto-
matic validation of terminology translation consis-
tency with statistical method. In MT Summit XI,
pages 269?274, Copenhagen, Denmark.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models.
In AMTA, pages 115?124, Washington, DC, USA.
P. Langlais and A. Patry. 2007. Translating unknown
words by analogical learning. In EMNLP-CoNLL,
pages 877?886, Prague, Czech Republic.
P. Langlais and F. Yvon. 2008. Scaling up analogi-
cal learning. In 22nd International Conference on
Computational Linguistics (COLING 2008), pages
51?54, Manchester, United Kingdom.
Y. Lepage and E. Denoual. 2005. ALEPH: an EBMT
system based on the preservation of proportion-
nal analogies between sentences across languages.
In International Workshop on Statistical Language
Translation (IWSLT), Pittsburgh, PA, October.
Y. Lepage and A. Lardilleux. 2007. The GREYC Ma-
chine Translation System for the IWSLT 2007 Eval-
uation Campaign. In IWLST, pages 49?53, Trento,
Italy.
Y. Lepage. 1998. Solving analogies on words: an algo-
rithm. In COLING-ACL, pages 728?734, Montreal,
Canada.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In 21st COLING and 44th ACL,
pages 761?768, Sydney, Australia.
E. Morin, B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining - using brain,
not brawn comparable corpora. In 45th ACL, pages
664?671, Prague, Czech Republic.
R. Rapp. 1995. Identifying word translation in non-
parallel texts. In 33rd ACL, pages 320?322, Cam-
bridge,Massachusetts, USA.
N. Stroppa and F. Yvon. 2005. An analogical learner
for morphological analysis. In 9th CoNLL, pages
120?127, Ann Arbor, MI.
K Toutanova, H. Suzuki, and A. Ruopp. 2008. Ap-
plying morphology generation models to machine
translation. In ACL-8 HLT, pages 514?522, Colom-
bus, Ohio, USA.
D. Vilar, J. Peter, and H. Ney. 2007. Can we trans-
late letters? In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 33?
39, Prague, Czech Republic, June.
F. Yvon, N. Stroppa, A. Delhay, and L. Miclet. 2004.
Solving analogical equations on words. Techni-
cal Report D005, E?cole Nationale Supe?rieure des
Te?le?communications, Paris, France, July.
495
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 755?762, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Translating with non-contiguous phrases
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman,
Eric Gaussier, Cyril Goutte, Kenji Yamada
Xerox Research Centre Europe
FirstName.FamilyName@xrce.xerox.com
Philippe Langlais
RALI/DIRO Universite? de Montre?al
felipe@iro.umontreal.ca
Arne Mauser
RWTH Aachen University
arne.mauser@rwth-aachen.de
Abstract
This paper presents a phrase-based statis-
tical machine translation method, based
on non-contiguous phrases, i.e. phrases
with gaps. A method for producing such
phrases from a word-aligned corpora is
proposed. A statistical translation model
is also presented that deals such phrases,
as well as a training method based on the
maximization of translation accuracy, as
measured with the NIST evaluation met-
ric. Translations are produced by means of
a beam-search decoder. Experimental re-
sults are presented, that demonstrate how
the proposed method allows to better gen-
eralize from the training data.
1 Introduction
Possibly the most remarkable evolution of recent
years in statistical machine translation is the step
from word-based models to phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Yamada
and Knight, 2002; Tillmann and Xia, 2003). While
in traditional word-based statistical models (Brown
et al, 1993) the atomic unit that translation operates
on is the word, phrase-based methods acknowledge
the significant role played in language by multi-
word expressions, thus incorporating in a statistical
framework the insight behind Example-Based Ma-
chine Translation (Somers, 1999).
However, Phrase-based models proposed so far
only deal with multi-word units that are sequences
of contiguous words on both the source and the tar-
get side. We propose here a model designed to deal
with multi-word expressions that need not be con-
tiguous in either or both the source and the target
side.
The rest of this paper is organised as follows. Sec-
tion 2 provides motivations, definition and extrac-
tion procedure for non-contiguous phrases. The log-
linear conditional translation model we adopted is
the object of Section 3; the method used to train
its parameters is described in Section 4. Section 5
briefly describes the decoder. The experiments we
conducted to asses the effectiveness of using non-
contiguous phrases are presented in Section 6.
2 Non-contiguous phrases
Why should it be a good thing to use phrases
composed of possibly non-contiguous sequences of
words? In doing so we expect to improve trans-
lation quality by better accounting for additional
linguistic phenomena as well as by extending the
effect of contextual semantic disambiguation and
example-based translation inherent in phrase-based
MT. An example of a phenomenon best described
using non-contiguous units is provided by English
phrasal verbs. Consider the sentence ?Mary switches
her table lamp off?. Word-based statistical mod-
els would be at odds when selecting the appropri-
ate translation of the verb. If French were the target
language, for instance, corpus evidence would come
from both examples in which ?switch? is translated
as ?allumer? (to switch on) and as ?e?teindre? (to
switch off). If many-to-one word alignments are not
allowed from English to French, as it is usually the
755
2 31
Pierre
Pierre
ne mange pas
does not eat
Figure 1: An example of a complex alignment asso-
ciated with different syntax for negation in English
and French.
case, then the best thing a word-based model could
do in this case would be to align ?off? to the empty
word and hope to select the correct translation from
?switch? only, basically a 50-50 bet. While han-
dling inseparable phrasal verbs such as ?to run out?
correctly, previously proposed phrase-based models
would be helpless in this case. A comparable behav-
ior is displayed by German separable verbs. More-
over, non-contiguous linguistic units are not limited
to verbs. Negation is formed, in French, by inserting
the words ?ne? and ?pas? before and after a verb re-
spectively. So, the sentence ?Pierre ne mange pas?
and its English translation display a complex word-
level alignment (Figure 1) current models cannot ac-
count for.
Flexible idioms, allowing for the insertion of lin-
guistic material, are other phenomena best modeled
with non-contiguous units.
2.1 Definition and library construction
We define a bi-phrase as a pair comprising a source
phrase and a target phrase: b = ?s?, t??. Each of the
source and target phrases is a sequence of words and
gaps (indicated by the symbol ?); each gap acts as
a placeholder for exactly one unspecified word. For
example, w? = w1w2?w3?? w4 is a phrase of length
7, made up of two contiguous words w1 and w2, a
first gap, a third word w3, two consecutive gaps and
a final word w4. To avoid redundancy, phrases may
not begin or end with a gap. If a phrase does not
contain any gaps, we say it is contiguous; otherwise
it is non-contiguous. Likewise, a bi-phrase is said to
be contiguous if both its phrases are contiguous.
The translation of a source sentence s is produced
by combining together bi-phrases so as to cover the
source sentence, and produce a well-formed target-
language sentence (i.e. without gaps). A complete
translation for s can be described as an ordered se-
quence of bi-phrases b1...bK . When piecing together
the final translation, the target-language portion t?1
of the first bi-phrase b1 is first layed down, then each
subsequent t?k is positioned on the first ?free? posi-
tion in the target language sentence, i.e. either the
leftmost gap, or the right end of the sequence. Fig-
ure 2 illustrates this process with an example.
To produce translations, our approach therefore
relies on a collection of bi-phrases, what we call a
bi-phrase library. Such a library is constructed from
a corpus of existing translations, aligned at the word
level.
Two strategies come to mind to produce non-
contiguous bi-phrases for these libraries. The first is
to align the words using a ?standard? word aligne-
ment technique, such as the Refined Method de-
scribed in (Och and Ney, 2003) (the intersection of
two IBM Viterbi alignments, forward and reverse,
enriched with alignments from the union) and then
generate bi-phrases by combining together individ-
ual alignments that co-occur in the same pair of sen-
tences. This is the strategy that is usually adopted in
other phrase-based MT approaches (Zens and Ney,
2003; Och and Ney, 2004). Here, the difference is
that we are not restricted to combinations that pro-
duce strictly contiguous bi-phrases.
The second strategy is to rely on a word-
alignment method that naturally produces many-to-
many alignments between non-contiguous words,
such as the method described in (Goutte et al,
2004). By means of a matrix factorization, this
method produces a parallel partition of the two texts,
seen as sets of word tokens. Each token therefore
belongs to one, and only one, subset within this par-
tition, and corresponding subsets in the source and
target make up what are called cepts. For example,
in Figure 1, these cepts are represented by the circles
numbered 1, 2 and 3; each cept thus connects word
tokens in the source and the target, regardless of po-
sition or contiguity. These cepts naturally constitute
bi-phrases, and can be used directly to produce a bi-
phrase library.
Obviously, the two strategies can be combined,
and it is always possible to produce increasingly
large and complex bi-phrases by combining together
co-occurring bi-phrases, contiguous or not. One
problem with this approach, however, is that the re-
sulting libraries can become very large. With con-
756
danser le tango
to tango
I do not want to tango anymore
I do not want anymore
doI want
Je ne veux plus danser le tango
Je
I
ne plus
veux
wantdo
not anymore
I
source =
bi?phrase 1 =
bi?phrase 2 =
bi?phrase 3 =
bi?phrase 4 =
target =
Figure 2: Combining bi-phrases to produce a translation.
tiguous phrases, the number of bi-phrases that can
be extracted from a single pair of sentences typically
grows quadratically with the size of the sentences;
with non-contiguous phrases, however, this growth
is exponential. As it turns out, the number of avail-
able bi-phrases for the translation of a sentence has
a direct impact on the time required to compute the
translation; we will therefore typically rely on vari-
ous filtering techniques, aimed at keeping only those
bi-phrases that are more likely to be useful. For ex-
ample, we may retain only the most frequently ob-
served bi-phrases, or impose limits on the number of
cepts, the size of gaps, etc.
3 The Model
In statistical machine translation, we are given a
source language input sJ1 = s1...sJ , and seek the
target-language sentence tI1 = t1...tI that is its most
likely translation:
t?I1 = argmaxtI1Pr(t
I
1|s
J
1 ) (1)
Our approach is based on a direct approximation
of the posterior probability Pr(tI1|sJ1 ), using a log-
linear model:
Pr(tI1|s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 )
)
In such a model, the contribution of each feature
function hm is determined by the corresponding
model parameter ?m; ZsJ1 denotes a normalization
constant. This type of model is now quite widely
used for machine translation (Tillmann and Xia,
2003; Zens and Ney, 2003)1.
Additional variables can be introduced in such a
model, so as to account for hidden characteristics,
and the feature functions can be extended accord-
ingly. For example, our model must take into ac-
count the actual set of bi-phrases that was used to
produce this translation:
Pr(tI1, b
K
1 |s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 , b
K
1 )
)
Our model currently relies on seven feature func-
tions, which we describe here.
? The bi-phrase feature function hbp: it rep-
resents the probability of producing tI1 using
some set of bi-phrases, under the assump-
tion that each source phrase produces a target
phrase independently of the others:
hbp(t
I
1, s
J
1 , b
K
1 ) =
K?
k=1
logPr(t?k|s?k) (2)
Individual bi-phrase probabilities Pr(t?k|s?k)
are estimated based on occurrence counts in the
word-aligned training corpus.
? The compositional bi-phrase feature function
hcomp: this is introduced to compensate for
1Recent work from Chiang (Chiang, 2005) addresses simi-
lar concerns to those motivating our work by introducing a Syn-
chronous CFG for bi-phrases. If on one hand SCFGs allow to
better control the order of the material inserted in the gaps, on
the other gap size does not seem to be taken into account, and
phrase dovetailing such as the one involving ?do ?want? and
?not ???anymore? in Fig. 2 is disallowed.
757
hbp?s strong tendency to overestimate the prob-
ability of rare bi-phrases; it is computed as in
equation (2), except that bi-phrase probabilities
are computed based on individual word transla-
tion probabilities, somewhat as in IBM model
1 (Brown et al, 1993):
Pr(t?|s?) =
1
|s?||t?|
?
t?t?
?
s?s?
Pr(t|s)
? The target language feature function htl: this
is based on a N -gram language model of the
target language. As such, it ignores the source
language sentence and the decomposition of
the target into bi-phrases, to focus on the actual
sequence of target-language words produced
by the combination of bi-phrases:
htl(t
I
1, s
J
1 , b
K
1 ) =
I?
i=1
logPr(ti|t
i?1
i?N+1)
? The word-count and bi-phrase count feature
functions hwc and hbc: these control the length
of the translation and the number of bi-phrases
used to produce it:
hwc(tI1, s
J
1 , b
K
1 ) = I hbc(t
I
1, s
J
1 , b
K
1 ) = K
? The reordering feature function
hreord(tI1, s
J
1 , b
K
1 ): it measures the amount of
reordering between bi-phrases of the source
and target sentences.
? the gap count feature function hgc: It takes as
value the total number of gaps (source and tar-
get) within the bi-phrases of bK1 , thus allowing
the model some control over the nature of the
bi-phrases it uses, in terms of the discontigui-
ties they contain.
4 Parameter Estimation
The values of the ? parameters of the log-linear
model can be set so as to optimize a given crite-
rion. For instance, one can maximize the likely-
hood of some set of training sentences. Instead, and
as suggested by Och (2003), we chose to maximize
directly the quality of the translations produced by
the system, as measured with a machine translation
evaluation metric.
Say we have a set of source-language sentences
S. For a given value of ?, we can compute the set of
corresponding target-language translations T . Given
a set of reference (?gold-standard?) translations R
for S and a function E(T,R) which measures the
?error? in T relative to R, then we can formulate the
parameter estimation problem as2:
?? = argmin?E(T,R)
As pointed out by Och, one notable difficulty with
this approach is that, because the computation of T
is based on an argmax operation (see eq. 1), it is not
continuous with regard to ?, and standard gradient-
descent methods cannot be used to solve the opti-
mization. Och proposes two workarounds to this
problem: the first one relies on a direct optimiza-
tion method derived from Powell?s algorithm; the
second introduces a smoothed (continuous) version
of the error function E(T,R) and then relies on a
gradient-based optimization method.
We have opted for this last approach. Och shows
how to implement it when the error function can be
computed as the sum of errors on individual sen-
tences. Unfortunately, this is not the case for such
widely used MT evaluation metrics as BLEU (Pa-
pineni et al, 2002) and NIST (Doddington, 2002).
We show here how it can be done for NIST; a simi-
lar derivation is possible for BLEU.
The NIST evaluation metric computes a weighted
n-gram precision between T and R, multiplied by
a factor B(S, T,R) that penalizes short translations.
It can be formulated as:
B(S, T,R) ?
N?
n=1
?
s?S In(ts, rs)
?
s?S Cn(ts)
(3)
where N is the largest n-gram considered (usually
N = 4), In(ts, rs) is a weighted count of common
n-grams between the target (ts) and reference (rs)
translations of sentence s, and Cn(ts) is the total
number of n-grams in ts.
To derive a version of this formula that is a con-
tinuous function of ?, we will need multiple trans-
lations ts,1, ..., ts,K for each source sentence s. The
general idea is to weight each of these translations
2For the sake of simplicity, we consider a single reference
translation per source sentence, but the argument can easily be
extended to multiple references.
758
by a factor w(?, s, k), proportional to the score
m?(ts,k|s) that ts,k is assigned by the log-linear
model for a given ?:
w(?, s, k) =
[
m?(ts,k|s)
?
k? m?(ts,k? |s)
]?
where ? is the smoothing factor. Thus, in
the smoothed version of the NIST function, the
term In(ts, rs) in equation (3) is replaced by?
k w(?, s, k)In(ts,k, rs), and the term Cn(ts) is
replaced by
?
k w(?, s, k)Cn(ts,k). As for the
brevity penalty factor B(S, T,R), it depends on
the total length of translation T , i.e.
?
s |ts|. In
the smoothed version, this term is replaced by
?
s
?
k w(?, s, k)|ts,k|. Note that, when ? ? ?,
then w(?, s, k) ? 0 for all translations of s, except
the one for which the model gives the highest score,
and so the smooth and normal NIST functions pro-
duce the same value. In practice, we determine some
?good? value for ? by trial and error (5 works fine).
We thus obtain a scoring function for which we
can compute a derivative relative to ?, and which can
be optimized using gradient-based methods. In prac-
tice, we use the OPT++ implementation of a quasi-
Newton optimization (Meza, 1994). As observed by
Och, the smoothed error function is not convex, and
therefore this sort of minimum-error rate training is
quite sensitive to the initialization values for the ?
parameters. Our approach is to use a random set of
initializations for the parameters, perform the opti-
mization for each initialization, and select the model
which gives the overall best performance.
Globally, parameter estimation proceeds along
these steps:
1. Initialize the training set: using random pa-
rameter values ?0, for each source sentence of
some given set of sentences S, we compute
multiple translations. (In practice, we use the
M -best translations produced by our decoder;
see Section 5).
2. Optimize the parameters: using the method de-
scribed above, we find ? that produces the best
smoothed NIST score on the training set.
3. Iterate: we then re-translate the sentences of S
with this new ?, combine the resulting multiple
translations with those already in the training
set, and go back to step 2.
Steps 2 and 3 can be repeated until the smooothed
NIST score does not increase anymore3.
5 Decoder
We implemented a version of the beam-search stack
decoder described in (Koehn, 2003), extended to
cope with non-contiguous phrases. Each transla-
tion is the result of a sequence of decisions, each of
which involves the selection of a bi-phrase and of a
target position. The final result is obtained by com-
bining decisions, as in Figure 2. Hypotheses, cor-
responding to partial translations, are organised in a
sequence of priority stacks, one for each number of
source words covered. Hypotheses are extended by
filling the first available uncovered position in the
target sentence; each extended hypotheses is then
inserted in the stack corresponding to the updated
number of covered source words. Each hypothesis is
assigned a score which is obtained as a combination
of the actual feature function values and of admissi-
ble heuristics, adapted to deal with gaps in phrases,
estimating the future cost for completing a transla-
tion. Each stack undergoes both threshold and his-
togram pruning. Whenever two hypotheses are in-
distinguishable as far as the potential for further ex-
tension is concerned, they are merged and only the
highest-scoring is further extended. Complete trans-
lations are eventually recovered in the ?last? priority
stack, i.e. the one corresponding to the total num-
ber of source words: the best translation is the one
with the highest score, and that does not have any
remaining gaps in the target.
6 Evaluation
We have conducted a number of experiments to eval-
uate the potential of our approach. We were par-
ticularly interested in assessing the impact of non-
contiguous bi-phrases on translation quality, as well
as comparing the different bi-phrase library contruc-
tion strategies evoked in Section 2.1.
3It can be seen that, as the set of possible translations for
S stabilizes, we eventually reach a point where the procedure
converges to a maximum. In practice, however, we can usually
stop much earlier.
759
6.1 Experimental Setting
All our experiments focused exclusively on French
to English translation, and were conducted using the
Aligned Hansards of the 36th Parliament of Canada,
provided by the Natural Language Group of the USC
Information Sciences Institute, and edited by Ulrich
Germann. From this data, we extracted three dis-
tinct subcorpora, which we refer to as the bi-phrase-
building set, the training set and the test set. These
were extracted from the so-called training, test-1
and test-2 portions of the Aligned Hansard, respec-
tively. Because of efficiency issues, we limited our-
selves to source-language sentences of 30 words or
less. More details on the evaluation data is presented
in Table 14.
6.2 Bi-phrase Libraries
From the bi-phrase-building set, we built a number
of libraries. A first family of libraries was based on
a word alignment ?A?, produced using the Refined
method described in (Och and Ney, 2003) (com-
bination of two IBM-Viterbi alignments): we call
these the A libraries. A second family of libraries
was built using alignments ?B? produced with the
method in (Goutte et al, 2004): these are the B li-
braries. The most notable difference between these
two alignments is that B contains ?native? non-
contiguous bi-phrases, while A doesn?t.
Some libraries were built by simply extracting the
cepts from the alignments of the bi-phrase-building
corpus: these are the A1 and B1 libraries, and vari-
ants. Other libraries were obtained by combining
cepts that co-occur within the same pair of sen-
tences, to produce ?composite? bi-phrases. For in-
stance, the A2 libraries contain combinations of 1
or 2 cepts from alignment A; B3 contains combina-
tions of 1, 2 or 3 cepts, etc.
Some libraries were built using a ?gap-size? filter.
For instance library A2-g3 contains those bi-phrases
obtained by combining 1 or 2 cepts from alignment
A, and in which neither the source nor the target
phrase contains more than 3 gaps. In particular, li-
brary B1-g0 does not contain any non-contiguous
bi-phrases.
4Preliminary experiments on different data sets allowed us
to establish that 800 sentences constituted an acceptable size
for estimating model parameters. With such a corpus, the esti-
mation procedure converges after just 2 or 3 iterations.
Finally, all libraries were subjected to the same
two filtering procedures: the first excludes all bi-
phrases that occur only once in the training corpus;
the second, for any given source-language phrase,
retains only the 20 most frequent target-language
equivalents. While the first of these filters typically
eliminates a large number of entries, the second only
affects the most frequent source phrases, as most
phrases have less than 20 translations.
6.3 Experiments
The parameters of the model were optimized inde-
pendantly for each bi-phrase library. In all cases,
we performed only 2 iterations of the training proce-
dure, then measured the performance of the system
on the test set in terms of the NIST and BLEU scores
against one reference translation. As a point of com-
parison, we also trained an IBM-4 translation model
with the GIZA++ toolkit (Och and Ney, 2000), using
the combined bi-phrase building and training sets,
and translated the test set using the ReWrite decoder
(Germann et al, 2001)5.
Table 2 describes the various libraries that were
used for our experiments, and the results obtained
for each.
System/library bi-phrases NIST BLEU
ReWrite 6.6838 0.3324
A1 238 K 6.6695 0.3310
A2-g0 642 K 6.7675 0.3363
A2-g3 4.1 M 6.7068 0.3283
B1-g0 193 K 6.7898 0.3369
B1 267 K 6.9172 0.3407
B2-g0 499 K 6.7290 0.3391
B2-g3 3.3 M 6.9707 0.3552
B1-g1 206 K 6.8979 0.3441
B1-g2 213 K 6.9406 0.3454
B1-g3 218 K 6.9546 0.3518
B1-g4 222 K 6.9527 0.3423
Table 2: Bi-phrase libraries and results
The top part of the table presents the results for
the A libraries. As can be seen, library A1 achieves
approximately the same score as the baseline sys-
tem; this is expected, since this library is essentially
5Both the ReWrite and our own system relied on a trigram
language model trained on the English half of the bi-phrase
building set.
760
Subset sentences source words target words
bi-phrase-building set 931,000 17.2M 15.2M
training set 800 11,667 10,601
test set 500 6726 6041
Table 1: Data sets.
made up of one-to-one alignments computed using
IBM-4 translation models. Adding contiguous bi-
phrases obtained by combining pairs of alignments
does gain us some mileage (+0.1 NIST)6. Again, this
is consistent with results observed with other sys-
tems (Tillmann and Xia, 2003). However, the addi-
tion of non-contiguous bi-phrases (A2-g3) does not
seem to help.
The middle part of Table 2 presents analogous re-
sults for the corresponding B libraries, plus the B1-
g0 library, which contains only those cepts from the
B alignment that are contiguous. Interestingly, in
the experiments reported in (Goutte et al, 2004),
alignment method B did not compare favorably to A
under the widely used Alignment Error Rate (AER)
metric. Yet, the B1-g0 library performs better than
the analogous A1 library on the translation task.
This suggests that AER may not be an appropriate
metric to measure the potential of an alignment for
phrase-based translation.
Adding non-contiguous bi-phrases allows another
small gain. Again, this is interesting, as it sug-
gests that ?native? non-contiguous bi-phrases are in-
deed useful for the translation task, i.e. those non-
contiguous bi-phrases obtained directly as cepts in
the B alignment.
Surprisingly, however, combining cepts from the
B alignment to produce contiguous bi-phrases (B2-
G0) does not turn out to be fruitful. Why this
is so is not obvious and, certainly, more experi-
ments would be required to establish whether this
tendency continues with larger combinations (B3-
g0, B4-g0...). Composite non-contiguous bi-phrases
produced with the B alignments (B2-g3) seem
to bring improvements with regard to ?basic? bi-
phrases (B1), but it is not clear whether these are
significant.
6While the differences in scores in these and other experi-
ments are relatively small, we believe them to be significant, as
they have been confirmed systematically in other experiments
and, in our experience, by visual inspection of the translations.
Visual examination of the B1 library reveals
that many non-contiguous bi-phrases contain long-
spanning phrases (i.e. phrases containing long se-
quences of gaps). To verify whether or not these
were really useful, we tested a series of B1 libraries
with different gap-size filters. It must be noted that,
because of the final histogram filtering we apply on
libraries (retain only the 20 most frequent transla-
tions of any source phrase), library B1-g1 is not
a strict subset of B1-g2. Therefore, filtering on
gap-size usually represents a tradeoff between more
frequent long-spanning bi-phrases and less frequent
short-spanning ones.
The results of these experiments appear in the
lower part of Table 2. While the differences in score
are small, it seems that concentrating on bi-phrases
with 3 gaps or less affords the best compromise.
For small libraries such as those under consideration
here, this sort of filtering may not be very important.
However, for higher-order libraries (B2, B3, etc.) it
becomes crucial, because it allows to control the ex-
ponential growth of the libraries.
7 Conclusions
In this paper, we have proposed a phrase-based sta-
tistical machine translation method based on non-
contiguous phrases. We have also presented a esti-
mation procedure for the parameters of a log-linear
translation model, that maximizes a smooth version
of the NIST scoring function, and therefore lends
itself to standard gradient-based optimization tech-
niques.
From our experiments with these new methods,
we essentially draw two conclusions. The first and
most obvious is that non-contiguous bi-phrases can
indeed be fruitful in phrase-based statistical machine
translation. While we are not yet able to character-
ize which bi-phrases are most helpful, some of those
that we are currently capable of extracting are well
suited to cover some short-distance phenomena.
761
The second conclusion is that alignment quality is
crucial in producing good translations with phrase-
based methods. While this may sound obvious, our
experiments shed some light on two specific aspects
of this question. The first is that the alignment
method that produces the most useful bi-phrases
need not be the one with the best alignment error
rate (AER). The second is that, depending on the
alignments one starts with, constructing increasingly
large bi-phrases does not necessarily lead to better
translations. Some of our best results were obtained
with relatively small libraries (just over 200,000 en-
tries) of short bi-phrases. In other words, it?s not
how many bi-phrases you have, it?s how good they
are. This is the line of research that we intend to
pursue in the near future.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their useful suggestions. 7
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast Decoding and Optimal Decoding
for Machine Translation. In Proceedings of ACL 2001,
Toulouse, France.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Proc.
ACL?04, pages 503?510.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
7This work was supported in part by the IST Programme
of the European Community, under the PASCAL Network of
Excellence, IST-2002-506778. This publication only reflects
the authors? views.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP 02), Philadel-
phia, PA.
J. C. Meza. 1994. OPT++: An Object-Oriented Class
Library for Nonlinear Optimization. Technical Report
SAND94-8225, Sandia National Laboratories, Albu-
querque, USA, March.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of ACL 2000, pages
440?447, Hongkong, China, October.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VCL 99), College Park,
MD.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL?03: 41st Ann. Meet.
of the Assoc. for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, USA.
Harold Somers. 1999. Review Article: Example-based
Machine Translation. Machine Translation, 14:113?
157.
Christoph Tillmann and Fei Xia. 2003. A phrase-based
unigram model for statistical machine translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. Improvements
in Phrase-Based Statistical Machine Translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
762
TransType: a Computer--Aided Translation Typing System 
Ph i l ippe  Lang la i s  and George  Foster  and Guy  Lapa lme 
RAL I /D IRO - -  Universit@ de Montr@al 
C.P. 6128, succursale Centre-ville 
H3C 3J7 Montr4al, Canada 
Phone:: +1 (514) 343-2145 
Fax: +1 (514) 343-5834 
email: {felipe, foster, lapalme}?iro, umontreal, ca 
Abst ract  
This paper describes the embedding of a sta- 
tistical translation system within a text editor 
to produce TRANSTYPE, a system that watches 
over the user as he or she types a translation and 
repeatedly suggests completions for the text al- 
ready entered. This innovative Embedded Ma- 
chine Translation system is thus a specialized 
means of helping produce high quality transla- 
tions. 
1 In t roduct ion  
TRANSTYPE is a project set up to explore an 
appealing solution to the problem of using In- 
teractive Machine Translation (IMT) as a tool 
for professional or other highly-skilled transla- 
tors. IMT first appeared as part of Kay's MIND 
system (Kay, 1973), where the user's role was 
to help the computer analyze the source text 
by answering questions about word sense, el- 
lipsis, phrasal attachments, etc. Most later 
work on IMT, eg (Blanchon, 1991; Brown and 
Nirenburg, 1990; Maruyama and Watanabe, 
1990; Whitelock et al, 1986), has followed in 
this vein, concentrating on improving the ques- 
tion/answer process by having less questions, 
more friendly ones, etc. Despite progress in 
these endeavors, systems of this sort are gen- 
erally unsuitable as tools for skilled trans\]\[ators 
because the user serves only as an advisor, with 
the MT components keeping overall control over 
the translation process. 
TRANSTYPE originated from the conviction 
that a better approach to IMT for competent 
translators would be to shift the focus of in- 
teraction from the meaning of the source text 
to the form of the target text. This would re- 
lieve the translator of the burden of having to 
provide explicit analyses of the source text and 
allow him to translate naturally, assisted by the 
machine whenever possible. 
In this approach, a translation emerges from 
a series of alternating contributions by human 
and machine. The machine's contributions are 
basically proposals for parts of the target text, 
while the translator's can take many forms, in- 
cluding pieces of target text, corrections to a 
previous machine contribution, hints about the 
nature of the desired translation, etc. In all 
cases, the translator remains directly in control 
of the process: the machine must respect he 
constraints implicit in his contributions, and he 
or she is free to accept, modify, or completely 
ignore its proposals. 
So TRANSTYPE is a specialized text editor 
with an embedded Machine translation engine 
as one of its components. In this project we 
had to address the following problems: how to 
interact with the user and how to find appro- 
priate multi-word units for suggestions that can 
be computed in real time. 
2 The  TransType  mode l  
2.1 User  V iewpo int  
Our interactive translation system is illustrated 
in figure 1 for an English to French translation. 
It works as follows: a translator selects a sen- 
tence and beg!ns typing its translation. After 
each character typed by the translator, the sys- 
tem displays a proposed completion, which may 
either be accepted using a special key or rejected 
by continuing to type. This interface is simple 
and its performance may be measured by the 
proportion of characters or keystrokes aved in 
typing a translation. Note that, throughout this 
process, the translator emains in control, and 
the machine must continually adapt its sugges- 
tions to the translator's input. This differs from 
the usual machine translation set-ups where it is 
the machine that produces the first draft which 
46 
? . ? . : ,  ? , ,  - - - - - - ~  
..... Fich:ier :::= ptions 
? : ' .  " - : . ' .  . . .  " . . :  . . . .  " .11 . . .  I t "  I: am:pleased:to: akepart:m this debate today. 
Usingitoday'S technologies,it:is possiblefOrall ~ad iaqs  to 
. . . .  . a . . . . .  borrowing:. 
. . . . .  ? . . . . . . . . .  :::i~ : 
d~batl.. 
GraCel ~i~la t~chnOIogiemoderne, tousles Can adiehs peuVent se 
prononcer:sur:lesquestions de:depenses: et~:d em runts:de/EZra 
Figure 1: Example of an interaction in TRANSTYPE with the source text in the top half of the 
screen. The target text is typed in the bottom half with suggestions given by the menu at the 
insertion point. 
then has to be corrected by the translator. 
The first version of TRANSTYPE (Foster et 
al., 1997) only proposed completions for the cur- 
rent word. We are now working on predictions 
which extend to the next several words in the 
text. The potential gain from multiple-word 
predictions (Langlais et al, 2000) can be ap- 
preciated in the one-sentence translation task 
reported in table 1, where a hypothetical user 
saves over 60% of the keystrokes needed to pro- 
duce a translation in a word completion sce- 
nario, and about 75% in a "unit" completion 
scenario 
2.2 System V iewpo int  
The core of TRANSTYPE is a completion engine 
which comprises two main parts: an evaluator 
which assigns probabilistic scores to completion 
47 
This bill is very similar to its companion bill which we dealt with yesterday 
in the house of commons 
word-completion task. unit-completion task 
pref. completions pref. completions 
C+ /loi ? c/e pro jet de loi ce 
pro jet 
de 
Ioi 
est 
tr~s 
semblable 
au 
pro jet 
de 
Ioi 
que 
nous 
avons 
examin4 
hier 
la 
chambre 
des 
communes 
ce+ / lOi" C/' 
p+ /est p/rojet 
d+ /tr~s d/e 
I+ /tr~s I/oi 
e+ /de e/st 
t+ /de ? t / r~s  
se+ /de ? s/es 
au+ /loi ? a/vec 
p+ /loi p/rojet 
d+ /loi ? d/e 
I+ /nous ? I/oi 
qu+ /nous ? q /u i  ? 
+ /nous 
av+ /nous 
ex+ /hier 
+ /hier 
se/mblable 
qu/e 
?  v/ons 
? e /n .  ex/amin~ 
~+ /b ie r  ? ~/ la  
+ /chambre 
de+ / communes  ? dle ? 
+ /communes 
de/s 
e+ 
t+ 
se+ 
a+ 
/de e/st 
/de .  t/r~s 
/de ? s/es se/mblable 
/loi ? a/u projet de loi sur 
qu+ /nous ? q /u i  ? qu/e 
+ /nous 
av+ /nous. a/vec, av/ons 
exa+ /& la chambre des communes 
e/n. ex/istence, exa/min~ 
h-F /& la chambre des communes 
h/let 
+ /& la chambre des communes 
106 char. 23 20 accept. 14 11 accept. -t- 1 correc. 
43 keyst rokes  26 keyst rokes  
Table h A one-sentence s ssion illustrating the word- and unit- completion tasks. The first col- 
umn indicates the target words the user is expected to produce. The next two columns indicate 
respectively the prefixes typed by the user and the completions made by the system under a word- 
completion task. The last two columns provide the same information for the unit-completion task. 
The total number of keystrokes for both tasks is reported in the last line. + indicates the accep- 
tance key typed by the user. A Completion is denoted by a/ f l  where a is the typed prefix and fl 
the completed part. Completions for different prefixes are separated by ? . 
hypotheses and a generator which uses the eval- 
uation function to select the best candidate for 
completion. 
2.2.1 The  eva luator  
The evaluator is a function p(t\[t', s) which as- 
signs to each target-text unit t an estimate of 
its probability given a source text s and the to- 
kens t' which precede t in the current ranslation 
of s. Our approach to modeling this distribu- 
tion is based to a large extent on that of the 
IBM group (Brown et al, 1993), but it diflhrs in 
one significant aspect: whereas the IBM model 
involves a "noisy channel" decomposition, we 
use a linear combination of separate predictions 
from a language model p(t\[t') and a transla- 
tion model p(t\[s). Although the noisy channel 
technique is powerful, it has the disadvantage 
that p(s\[t', t) is more expensive to compute than 
p(t\[s) when using IBM-style translation models. 
Since speed is crucial for our application, we 
chose to forego it in the work described here. 
Our linear combination model is fully described 
in (Langlais and Foster, 2000) but can be seen 
as follows: 
48 
p(tlt ' ,s ) = p(tlt' ) A(O(t',s)), (1) 
language 
+ p(tls)\[1-~(O(t',s))! 
translation 
where .~(O(t',s)) e \[0,1\] are context- 
dependent interpolation coefficients. O(t~,s) 
stands for any function which maps t~,s into a 
set of equivalence classes. Intuitively, ),(O(t r, s)) 
should be high when s is more informative than 
t r and low otherwise. For example, the trans- 
lation model could have a higher weight at the 
start of sentence but the contribution of the lan- 
guage model can become more important in the 
middle or the end of the sentence. 
2.2.2 The  language mode l  
We experimented with various simple linear 
combinations of four different French language 
models: a cache model, similar to the cache 
component in Kuhn's model (Kuhn and Mori, 
1990); a unigram model; a trielass model (Der- 
ouault and Merialdo, 1986); and an interpolated 
trigram (Jelinek, 1990). 
We opted for the trigram, which gave signifi- 
cantly better results than the other three mod- 
els. The trigram was trained on the Hansard 
corpus (about 50 million words), with 75% of 
the corpus used for relative-frequency parame- 
ter estimates, and 25% used to reestimate inter- 
polation coefficients. 
2.2.3 The  t rans la t ion  mode l  
Our translation model is based on the linear in- 
terpolation given in equation 2 which combines 
predictions of two translation models - -  Ms and 
Mu - -  both based on an IBM-like model 2 (see 
equation 3). Ms was trained on single words 
and Mu was trained on both words and units. 
p( tls) = Z pt( tls) ,+ (1 - Z).p2 ( (s ) ) 
word unit 
(2) 
where Ps and Pu stand for the probabilities 
given respectively by Ms and M~. ~(s) repre- 
sents the new sequence of tokens obtained after 
grouping the tokens of s into units. 
Both models are based on IBM translation 
model 2 (Brown et al, 1993) which has the 
49 
property that it generates tokens independently. 
The total probability of the ith target-text to- 
ken ti is just the average of the probabilities 
with which it is generated by each source text 
token sj; this is a weighted average that takes 
the distance from the generating token into ac- 
count: 
is1 
p(tils) = ~p( t i l s j )  a(jli, Is\[) 
j=O 
(3) 
where p(ti Is j) is a word-for-word translation 
probability, Isl is the length (counted in tokens) 
o f the  source segment s under translation, and 
a(jli , Is\]) is the a priori alignment probability 
that the target-text token at position i will be 
generated by the source text token at position 
j; this is equal to a constant value of 1~(Is I + 1) 
for model 1. This formula follows the conven- 
tion of (Brown et al, 1993) in letting so des- 
ignate the null state. We modified IBM model 
2 to account for invariant entities such as En- 
glish forms that almost invariably translate into 
French either verbatim or after having under- 
gone a predictable transformation e.g. numbers 
or dates. These forms are very frequent in the 
Hansard corpus. 
2.3 The Generator  
The task of the generator is to identify units 
matching the current prefix typed by the user, 
and pick the best candidate using the evalua- 
tion function. Given the real time constraints 
of an IMT system, we divided the French vocab- 
ulary into two parts: a small active component 
whose contents are always searched for a match 
to the current prefix, and a much larger passive 
part which comes into play only when no candi- 
dates are found in the active vocabulary. Both 
vocabularies are coded as tries. 
The passive vocabulary is a large dictionary 
containing over 380,000 word forms. The ac- 
tive part is computed ynamically when a new 
sentence is selected by the translator. It relies 
on the fact that a small number of words ac- 
count for most of the tokens in a text. It is 
composed of a few entities (tokens and units) 
that are likely to appear in the translation. In 
practice, we found that keeping 500 words and 
50 units yields good performance. 
3 Implementat ion  
From an implementation point of view, the core 
of TransType relies on a flexible object ori- 
ented architecture, which facilitates the integra- 
tion of any model that can predict units (words 
or sequence of words) from what has been al- 
ready typed and the source text being trans- 
lated. This part is written in C?+.  Statisti- 
cal translation and language models have been 
integrated among others into this architecture 
(Lapalme et al, 2000). 
The graphical user interface is implemented 
in Tcl/Tk, a multi-platform script language well 
suited to interfacing problems. It offers all the 
classical functions for text edition plus a pop-up 
menu which contains the more probable words 
or sequences of words that may complete the 
ongoing translation. The proposed completions 
are updated after each keystroke the translator 
enters. 
4 Evaluation 
We have conducted a theoretical evaluation of 
TransType on a word completion task, which 
assumes that a translator carefully observes 
each completion proposed by the system, and 
accepts it as soon as it is correct. Under 
these optimistic onditions, we have shown that 
TransType allows for the production of a trans- 
lation typing less than a third of its characters. 
In order to better grasp the usefulness of 
TRANSTYPE, we also performed a more prac- 
tical evaluation by asking ten translators to 
use the prototype for about one hour to trans- 
late isolated sentences. We first asked them to 
translate without any help from TRANSTYPE 
and then we compared their typing speed with 
TRANSTYPE suggestions turned on. Overall, 
translators liked the concept and found it very 
useful; they all liked the suggestions although 
it seemed to induce a literal style of transla- 
tion. We also asked them if they thought hat 
TRANSTYPE improved their typing speed and 
the majority of them said so; unfortunately the 
figures showed that none of them did so ... The 
typing rates are nevertheless quite good, given 
that the users were new to this environment and 
this style of looking at suggestions while trans- 
lating. But interestingly this practical ew~lua- 
tion confirmed our theoretical evaluation that a- 
translation can be produced with TRANSTYPE 
by typing less than 40% of the characters of a 
translation. Results of this evaluation and com- 
parisons with our theoretical figures are further 
described in (Foster et al, 2000). 
This experiment made us realize that this 
concept of real-time suggestions depends very 
much on the usability of the prototype; we had 
first developed a much simpler editor but its 
limitations were such that the translators found 
it unusable. So we are convinced that the user- 
interface aspects of this prototype should be 
thoroughly studied. But the TRANSTYPE ap- 
proach would be much more useful if it was 
combined with other text editing tasks related 
to translation: for example TRANSTYPE could 
format the translation in the same way as the 
source text, this would be especially useful for 
titles and tables; it would also be possible to 
localize automatically specific entities such as 
dates, numbers and amounts of money. It would 
also be possible to check that some translations 
given by the user are correct with respect with 
some normative usage of words or terminologi- 
cal coherence; these facilities are already part of 
TRANSCHECK, another computer aided transla- 
tion tool prototype developed in our laboratory 
(Jutras, 2000). 
5 Conc lus ion  
We have presented an innovative way of em- 
bedding machine translation by means of a pro- 
totype which implements an appealing interac- 
tive machine translation scenario where the in- 
teraction is mediated via the target text under 
production. Among other advantages, this ap- 
proach relieves the translator of the burden of 
source analyses, and gives him or her direct con- 
trol over the final translation without having to 
resort to post-edition. 
Acknowledgements  
TRANSTYPE is a project funded by the Natu- 
ral Sciences and Engineering Research Council 
of Canada. We are greatly indebted to Elliott 
Macklovitch and Pierre Isabelle for the fruitful 
orientations they gave to this work. 
References 
Herv6 Blanchon. 1991. Probl~mes de 
d@sambigffisation i teractive et TAO per- 
sonnelle. In L 'environnement Traductionnel, 
50 
Journ@es cientifiques du R@seau th@matique 
de recherche "Lexicologie, terminologie, 
traduction", pages 31-48, Mons, April. 
Ralf D. Brown and Sergei Nirenburg. 1990. 
Human-computer interaction for semantic 
disambiguation. In Proceedings off the Inter- 
national Conference on Computational Lin- 
guistics (COLING), pages 42-47, Helsinki, 
Finland, August. 
Peter F. Brown, Stephen A. Della Pietra, Vin- 
cent Della J. Pietra, and Robert L. Mercer. 
1993. The mathematics of machine transla- 
tion: Parameter estimation. Computational 
Linguistics, 19(2):263-312, June. 
A.-M. Derouault and B. Merialdo. 1986. Nat- 
ural language modeling for phoneme-to-text 
transcription. IEEE Transactions on Pattern 
Analysis and Machine Intelligence (PAMI), 
8 (6): 742-749, November. 
George Foster, Pierre Isabelle, and Pierre Pla- 
mondon. 1997. Target-text Mediated Inter- 
active Machine Translation. Machine Trans- 
lation, 12:175-194. 
George Foster, Philippe Langlais, Guy 
Lapalme, Dominique Letarte, Elliott 
Macklovitch, and S@bastien Sauv@. 2000. 
Evaluation of transtype, a computer-aided 
translation typing system: A comparison of 
a theoretical- and a user- oriented evaluation 
procedures. In Conference on Language 
Resources and Evaluation (LREC), page 8 
pages, Athens, Greece, June. 
Frederick Jelinek. 1990. Self-organized lan- 
guage modeling for speech recognition. In 
A. Waibel and K. Lee, editors, Readings in 
Speech Recognition, pages 450-506. Morgan 
Kaufmann, San Mateo, California. 
Jean-Marc Jutras. 2000. An automatic reviser: 
The TransCheck system. In Applied Natu- 
ral Language Processing 2000, page 10 pages, 
Seattle, Washington, May. 
Martin Kay. 1973. The MIND system. In 
R. Rustin, editor, Natural Language Process- 
ing, pages 155-188. Algorithmics Press, New 
York. 
Roland Kuhn and Renato De Mori. 1990. 
A cache-based natural language model for 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence 
(PAMI), 12(6):570-583, June. 
Philippe Langlais and George Foster. 2000. Us- 
ing context-dependent i terpolation to com- 
bine statistical language and translation 
models for interactive machine translation. 
In Computer-Assisted Information Retrieval, 
Paris, April. 
Philippe Langlais, George Foster, and Guy 
Lapalme. 2000. Unit completion for a 
computer-aided translation typing system. In 
Applied Natural Language Processing 2000, 
page 10 pages, Seattle, Washington, May. 
Guy Lapalme, George Foster, and Philippe 
Langlais. 2000. La programmation rient@e- 
objet pour le d~veloppement de modules de 
langages. In Christophe Dony and Houari A. 
Sahraoui, editors, LMO'O0 - Langages et 
modules ~ objets, pages 139-147, Mont St- 
Hilaire, Qu@bec, 27 Janvier. Hermes Science. 
Conference invit@e. 
Hiroshi Maruyama nd Hideo Watanabe. 1990. 
An interactive Japanese parser for machine 
translation. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics (COLING), pages 257-262, Helsinki, Fin- 
land, August. 
P. J. Whitelock, M. McGee Wood, B. J. Chan- 
dler, N. Holden, and H. J. Horsfall. 1986. 
Strategies for interactive machine transla- 
tion: the experience and implications of the 
UMIST Japanese project. In Proceedings of 
the International Conference on Computa- 
tional Linguistics (COLING), pages 329-334, 
Bonn, West Germany. 
51 
User-Friendly Text Prediction for Translators
George Foster and Philippe Langlais and Guy Lapalme
RALI, Universite? de Montre?al
{foster,felipe,lapalme}@iro.umontreal.ca
Abstract
Text prediction is a form of interactive
machine translation that is well suited to
skilled translators. In principle it can as-
sist in the production of a target text with
minimal disruption to a translator?s nor-
mal routine. However, recent evaluations
of a prototype prediction system showed
that it significantly decreased the produc-
tivity of most translators who used it. In
this paper, we analyze the reasons for this
and propose a solution which consists in
seeking predictions that maximize the ex-
pected benefit to the translator, rather than
just trying to anticipate some amount of
upcoming text. Using a model of a ?typ-
ical translator? constructed from data col-
lected in the evaluations of the prediction
prototype, we show that this approach has
the potential to turn text prediction into a
help rather than a hindrance to a translator.
1 Introduction
The idea of using text prediction as a tool for trans-
lators was first introduced by Church and Hovy as
one of many possible applications for ?crummy?
machine translation technology (Church and Hovy,
1993). Text prediction can be seen as a form of in-
teractive MT that is well suited to skilled transla-
tors. Compared to the traditional form of IMT based
on Kay?s original work (Kay, 1973)?in which the
user?s role is to help disambiguate the source text?
prediction is less obtrusive and more natural, allow-
ing the translator to focus on and directly control the
contents of the target text. Predictions can benefit
a translator in several ways: by accelerating typing,
by suggesting translations, and by serving as an im-
plicit check against errors.
The first implementation of a predictive tool for
translators was described in (Foster et al, 1997), in
the form of a simple word-completion system based
on statistical models. Various enhancements to this
were carried out as part of the TransType project
(Langlais et al, 2000), including the addition of a re-
alistic user interface, better models, and the capabil-
ity of predicting multi-word lexical units. In the fi-
nal TransType prototype for English to French trans-
lation, the translator is presented with a short pop-
up menu of predictions after each character typed.
These may be incorporated into the text with a spe-
cial command or rejected by continuing to type nor-
mally.
Although TransType is capable of correctly antic-
ipating over 70% of the characters in a freely-typed
translation (within the domain of its training cor-
pus), this does not mean that users can translate in
70% less time when using the tool. In fact, in a trial
with skilled translators, the users? rate of text pro-
duction declined by an average of 17% as a result
of using TransType (Langlais et al, 2002). There
are two main reasons for this. First, it takes time to
read the system?s proposals, so that in cases where
they are wrong or too short, the net effect will be to
slow the translator down. Second, translators do not
always act ?rationally? when confronted with a pro-
posal; that is, they do not always accept correct pro-
posals and they occasionally accept incorrect ones.
Many of the former cases correspond to translators
simply ignoring proposals altogether, which is un-
derstandable behaviour given the first point.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 148-155.
                         Proceedings of the Conference on Empirical Methods in Natural
This paper describes a new approach to text pre-
diction intended to address these problems. The
main idea is to make predictions that maximize the
expected benefit to the user in each context, rather
than systematically proposing a fixed amount of text
after each character typed. The expected benefit is
estimated from two components: a statistical trans-
lation model that gives the probability that a can-
didate prediction will be correct or incorrect, and a
user model that determines the benefit to the trans-
lator in either case. The user model takes into ac-
count the cost of reading a proposal, as well as the
random nature of the decision to accept it or not.
This approach can be characterized as making fewer
but better predictions: in general, predictions will
be longer in contexts where the translation model is
confident, shorter where it is less so, and absent in
contexts where it is very uncertain.
Other novel aspects of the work we describe here
are the use of a more accurate statistical translation
model than has previously been employed for text
prediction, and the use of a decoder to generate pre-
dictions of arbitrary length, rather than just single
words or lexicalized units as in the TransType pro-
totype. The translation model is based on the max-
imum entropy principle and is designed specifically
for this application.
To evaluate our approach to prediction, we simu-
lated the actions of a translator over a large corpus of
previously-translated text. The result is an increase
of over 10% in translator productivity when using
the predictive tool. This is a considerable improve-
ment over the -17% observed in the TransType trials.
2 The Text Prediction Task
In the basic prediction task, the input to the predictor
is a source sentence s and a prefix h of its translation
(ie, the target text before the current cursor position);
the output is a proposed extension x to h. Figure 1
gives an example. Unlike the TransType prototype,
which proposes a set of single-word (or single-unit)
suggestions, we assume that each prediction consists
of only a single proposal, but one that may span an
arbitrary number of words.
As described above, the goal of the predictor is
to find the prediction x? that maximizes the expected
s: Let us return to serious matters.
t:
h
? ?? ?
On va r
x?
? ?? ?
evenir aux choses se?rieuses.
x: evenir a`
Figure 1: Example of a prediction for English to
French translation. s is the source sentence, h is the
part of its translation that has already been typed,
x? is what the translator wants to type, and x is the
prediction.
benefit to the user:
x? = argmax
x
B(x,h, s), (1)
where B(x,h, s) measures typing time saved. This
obviously depends on how much of x is correct, and
how long it would take to edit it into the desired text.
A major simplifying assumption we make is that the
user edits only by erasing wrong characters from the
end of a proposal. Given a TransType-style interface
where acceptance places the cursor at the end of a
proposal, this is the most common editing method,
and it gives a conservative estimate of the cost at-
tainable by other methods. With this assumption,
the key determinant of edit cost is the length of the
correct prefix of x, so the expected benefit can be
written as:
B(x,h, s) =
l?
k=0
p(k|x,h, s)B(x,h, s, k), (2)
where p(k|x,h, s) is the probability that exactly k
characters from the beginning of x will be correct,
l is the length of x, and B(x,h, s, k) is the benefit
to the user given that the first k characters of x are
correct.
Equations (1) and (2) define three main problems:
estimating the prefix probabilities p(k|x,h, s), esti-
mating the user benefit function B(x,h, s, k), and
searching for x?. The following three sections de-
scribe our solutions to these.
3 Translation Model
The correct-prefix probabilities p(k|x,h, s) are
derived from a word-based statistical translation
model. The first step in the derivation is to con-
vert these into a form that deals explicitly with char-
acter strings. This is accomplished by noting that
p(k|x,h, s) is the probability that the first k charac-
ters of x are correct and that the k + 1th character
(if there is one) is incorrect. For k < l:
p(k|x,h, s) = p(xk1|h, s)? p(x
k+1
1 |h, s)
where xk1 = x1 . . . xk. If k = l, p(k|x,h, s) =
p(x|h, s). Also, p(x01) ? 1.
The next step is to convert string probabilities
into word probabilities. To do this, we assume
that strings map one-to-one into token sequences, so
that:
p(xk1|h, s) ? p(v1, w2, . . . , wm?1, um|h, s),
where v1 is a possibly-empty word suffix, each wi is
a complete word, and um is a possibly empty word
prefix. For example, if x in figure 1 were evenir aux
choses, then x141 would map to v1 = evenir, w2 =
aux, and u3 = cho. The one-to-one assumption is
reasonable given that entries in our lexicon contain
neither whitespace nor internal punctuation.
To model word-sequence probabilities, we apply
the chain rule:
p(v1, w2, . . . , wm?1, um|h, s) =
p(v1|h, s)
m?1?
i=2
p(wi|h, v1, w
i?1
2 , s)?
p(um|h, v1, w
m?1
2 , s). (3)
The probabilities of v1 and um can be expressed in
terms of word probabilities as follows. Letting u1
be the prefix of the word that ends in v1 (eg, r in
figure 1), w1 = u1v1, and h = h?u1:
p(v1|h, s) = p(w1|h?, s)/
?
w:w=u1v
p(w|h?, s),
where the sum is over all words that start with u1.
Similarly:
p(um|h?, w
m?1
1 , s) =
?
w:w=umv
p(w|h?, wm?11 , s). (4)
Thus all factors in (3) can be calculated from
probabilities of the form p(w|h, s) which give the
likelihood that a word w will follow a previous se-
quence of words h in the translation of s.1 This is
the family of distributions we have concentrated on
modeling.
Our model for p(w|h, s) is a log-linear combina-
tion of a trigram language model for p(w|h) and a
maximum-entropy translation model for p(w|s), de-
scribed in (Foster, 2000a; Foster, 2000b). The trans-
lation component is an analog of the IBM model 2
(Brown et al, 1993), with parameters that are op-
timized for use with the trigram. The combined
model is shown in (Foster, 2000a) to have signif-
icantly lower test corpus perplexity than the linear
combination of a trigram and IBM 2 used in the
TransType experiments (Langlais et al, 2002). Both
models supportO(mJV 3) Viterbi-style searches for
the most likely sequence of m words that follows h,
where J is the number of tokens in s and V is the
size of the target-language vocabulary.
Compared to an equivalent noisy-channel combi-
nation of the form p(t)p(s|t), where t is the tar-
get sentence, our model is faster but less accurate.
It is faster because the search problem for noisy-
channel models is NP-complete (Knight, 1999), and
even the fastest dynamic-programming heuristics
used in statistical MT (Niessen et al, 1998; Till-
mann and Ney, 2000), are polynomial in J?for in-
stance O(mJ4V 3) in (Tillmann and Ney, 2000). It
is less accurate because it ignores the alignment rela-
tion between s and h, which is captured by even the
simplest noisy-channel models. Our model is there-
fore suitable for making predictions in real time, but
not for establishing complete translations unassisted
by a human.
3.1 Implementation
The most expensive part of the calculation in equa-
tion (3) is the sum in (4) over all words in the vo-
cabulary, which according to (2) must be carried out
for every character position k in a given prediction
x. We reduce the cost of this by performing sums
only at the end of each sequence of complete tokens
in x (eg, after revenir and revenir aux in the above
example). At these points, probabilities for all pos-
sible prefixes of the next word are calculated in a
1Here we ignore the distinction between previous words that
have been sanctioned by the translator and those that are hy-
pothesized as part of the current prediction.
single recursive pass over the vocabulary and stored
in a trie for later access.
In addition to the exact calculation, we also ex-
perimented with establishing exact probabilities via
p(w|h, s) only at the end of each token in x, and as-
suming that the probabilities of the intervening char-
acters vary linearly between these points. As a re-
sult of this assumption, p(k|x,h, s) = p(xk1|h, s)?
p(xk+11 |h, s) is constant for all k between the end of
one word and the next, and therefore can be factored
out of the sum in equation (2) between these points.
4 User Model
The purpose of the user model is to determine the
expected benefit B(x,h, s, k) to the translator of a
prediction x whose first k characters match the text
that the translator wishes to type. This will depend
on whether the translator decides to accept or reject
the prediction, so the first step in our model is the
following expansion:
B(x,h, s, k) =
?
a?{0,1}
p(a|x,h, s, k)B(x,h, s, k, a),
where p(a|x,h, s, k) is the probability that the trans-
lator accepts or rejects x, B(x,h, s, k, a) is the ben-
efit they derive from doing so, and a is a random
variable that takes on the values 1 for acceptance and
0 for rejection. The first two quantities are the main
elements in the user model, and are described in fol-
lowing sections. The parameters of both were esti-
mated from data collected during the TransType trial
described in (Langlais et al, 2002), which involved
nine accomplished translators using a prototype pre-
diction tool for approximately half an hour each. In
all cases, estimates were made by pooling the data
for all nine translators.
4.1 Acceptance Probability
Ideally, a model for p(a|x,h, s, k) would take into
account whether the user actually reads the proposal
before accepting or rejecting it, eg:
p(a|x,h, s, k) =
?
r?{0,1}
p(a|r,x,h, s, k)p(r|x,h, s, k)
where r is a boolean ?read? variable. However, this
information is hard to extract reliably from the avail-
able data; and even if were obtainable, many of the
?60 ?50 ?40 ?30 ?20 ?10 0 10 20 30 40 50 600
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
pro
ba
bili
ty 
of 
ac
ce
pti
ng
gain (length of correct prefix ? length of incorrect suffix)
raw
smoothed
model
Figure 2: Probability that a prediction will be ac-
cepted versus its gain.
factors which influence whether a user is likely to
read a proposal?such as a record of how many pre-
vious predictions have been accepted?are not avail-
able to the predictor in our formulation. We thus
model p(a|x,h, s, k) directly.
Our model is based on the assumption that the
probability of accepting x depends only on what the
user stands to gain from it, defined according to the
editing scenario given in section 2 as the amount by
which the length of the correct prefix of x exceeds
the length of the incorrect suffix:
p(a|x,h, s, k) ? p(a|2k ? l),
where k?(l?k) = 2k? l is called the gain. For in-
stance, the gain for the prediction in figure 1 would
be 2? 7? 8 = 6. The strongest part of this assump-
tion is dropping the dependence on h, because there
is some evidence from the data that users are more
likely to accept at the beginnings of words. How-
ever, this does not appear to have a severe effect on
the quality of the model.
Figure 2 shows empirical estimates of p(a =
1|2k? l) from the TransType data. There is a certain
amount of noise intrinsic to the estimation proce-
dure, since it is difficult to determine x?, and there-
fore k, reliably from the data in some cases (when
the user is editing the text heavily). Nonetheless, it
is apparent from the plot that gain is a useful abstrac-
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 ac
cep
t (m
sec
s)
length of proposal (chars)
raw
least?squares fit
0 10 20 30 40 50 600
500
1000
1500
2000
2500
3000
3500
4000
av
er
ag
e t
ime
 to
 re
ject
 (m
sec
s)
length of proposal (chars)
raw
least?squares fit
Figure 3: Time to read and accept or reject proposals versus their length
tion, because the empirical probability of acceptance
is very low when it is less than zero and rises rapidly
as it increases. This relatively clean separation sup-
ports the basic assumption in section 2 that benefit
depends on k.
The points labelled smoothed in figure 2 were
obtained using a sliding-average smoother, and the
model curve was obtained using two-component
Gaussian mixtures to fit the smoothed empirical
likelihoods p(gain|a = 0) and p(gain|a = 1). The
model probabilities are taken from the curve at in-
tegral values. As an example, the probability of ac-
cepting the prediction in figure 1 is about .25.
4.2 Benefit
The benefit B(x,h, s, k, a) is defined as the typing
time the translator saves by accepting or rejecting
a prediction x whose first k characters are correct.
To determine this, we assume that the translator first
reads x, then, if he or she decides to accept, uses a
special command to place the cursor at the end of x
and erases its last l ? k characters. Assuming inde-
pendence from h, s as before, our model is:
B(x, k, a) =
{
?R1(x) + T (x, k)? E(x, k), a = 1
?R0(x), a = 0
where Ra(x) is the cost of reading x when it ul-
timately gets accepted (a= 1) or rejected (a= 0),
T (x, k) is the cost of manually typing xk1 , and
E(x, k) is the edit cost of accepting x and erasing
to the end of its first k characters.
A natural unit for B(x, k, a) is the number of
keystrokes saved, so all elements of the above equa-
tion are converted to this measure. This is straight-
forward in the case of T (x, k) and E(x, k), which
are estimated as k and l ? k + 1 respectively?for
E(x, k), this corresponds to one keystroke for the
command to accept a prediction, and one to erase
each wrong character. This is likely to slightly un-
derestimate the true benefit, because it is usually
harder to type n characters than to erase them.
As in the previous section, read costs are inter-
preted as expected values with respect to the proba-
bility that the user actually does read x, eg, assuming
0 cost for not reading, R0(x) = p(r=1|x)R?0(x),
where R?0(x) is the unknown true cost of reading
and rejecting x. To determine Ra(x), we measured
the average elapsed time in the TransType data from
the point at which a proposal was displayed to the
point at which the next user action occurred?either
an acceptance or some other command signalling a
rejection. Times greater than 5 seconds were treated
as indicating that the translator was distracted and
were filtered out. As shown in figure 3, read times
are much higher for predictions that get accepted, re-
flecting both a more careful perusal by the translator
and the fact the rejected predictions are often simply
ignored.2 In both cases there is a weak linear rela-
2Here the number of characters read was assumed to include
the whole contents of the TransType menu in the case of rejec-
tions, and only the proposal that was ultimately accepted in the
case of acceptances.
tionship between the number of characters read and
the time taken to read them, so we used the least-
squares lines shown as our models. Both plots are
noisy and would benefit from a more sophisticated
psycholinguistic analysis, but they are plausible and
empirically-grounded first approximations.
To convert reading times to keystrokes for the
benefit function we calculated an average time per
keystroke (304 milliseconds) based on sections of
the trial where translators were rapidly typing and
when predictions were not displayed. This gives an
upper bound for the per-keystroke cost of reading?
compare to, for instance, simply dividing the total
time required to produce a text by the number of
characters in it?and therefore results in a conser-
vative estimate of benefit.
To illustrate the complete user model, in the fig-
ure 1 example the benefit of accepting would be
7?2?4.2 = .8 keystrokes and the benefit of reject-
ing would be?.2 keystrokes. Combining these with
the acceptance probability of .25 gives an overall ex-
pected benefit B(x,h, s, k = 7) for this proposal of
0.05 keystrokes.
5 Search
Searching directly through all character strings x
in order to find x? according to equation (1) would
be very expensive. The fact that B(x,h, s) is non-
monotonic in the length of x makes it difficult to or-
ganize efficient dynamic-programming search tech-
niques or use heuristics to prune partial hypotheses.
Because of this, we adopted a fairly radical search
strategy that involves first finding the most likely se-
quence of words of each length, then calculating the
benefit of each of these sequences to determine the
best proposal. The algorithm is:
1. For each length m = 1 . . .M , find the best
word sequence:
w?m = argmax
w1:(w1=u1v), wm2
p(wm1 |h
?, s),
where u1 and h? are as defined in section 3.
2. Convert each w?m to a corresponding character
string x?m.
3. Output x? = argmaxm B(x?m,h, s), or the
empty string if all B(x?m,h, s) are non-
positive.
M average time maximum time
1 0.0012 0.01
2 0.0038 0.23
3 0.0097 0.51
4 0.0184 0.55
5 0.0285 0.57
Table 1: Approximate times in seconds to generate
predictions of maximum word sequence length M ,
on a 1.2GHz processor, for the MEMD model.
In all experiments reported below, M was set to a
maximum of 5 to allow for convenient testing. Step
1 is carried out using a Viterbi beam search. To
speed this up, the search is limited to an active vo-
cabulary of target words likely to appear in transla-
tions of s, defined as the set of all words connected
by some word-pair feature in our translation model
to some word in s. Step 2 is a trivial deterministic
procedure that mainly involves deciding whether or
not to introduce blanks between adjacent words (eg
yes in the case of la + vie, no in the case of l? +
an). This also removes the prefix u1 from the pro-
posal. Step 3 involves a straightforward evaluation
of m strings according to equation (2).
Table 1 shows empirical search timings for vari-
ous values of M , for the MEMD model described
in the next section. Times for the linear model are
similar. Although the maximum times shown would
cause perceptible delays for M > 1, these occur
very rarely, and in practice typing is usually not no-
ticeably impeded when using the TransType inter-
face, even at M = 5.
6 Evaluation
We evaluated the predictor for English to French
translation on a section of the Canadian Hansard
corpus, after training the model on a chronologi-
cally earlier section. The test corpus consisted of
5,020 sentence pairs and approximately 100k words
in each language; details of the training corpus are
given in (Foster, 2000b).
To simulate a translator?s responses to predic-
tions, we relied on the user model, accepting prob-
abilistically according to p(a|x,h, s, k), determin-
ing the associated benefit using B(x,h, s, k, a), and
advancing the cursor k characters in the case of an
config M
1 2 3 4 5
fixed -8.5 -0.4 -3.60 -11.6 -20.8
linear 6.1 9.40 8.8 8.1 7.8
exact 5.3 10.10 10.7 10.0 9.7
corr 5.8 10.7 12.0 12.5 12.6
best 7.9 17.90 24.5 27.7 29.2
fixed -11.5 -9.3 -15.1 -22.0 -28.2
exact 3.0 4.3 5.0 5.2 5.2
best 6.2 12.1 15.4 16.7 17.3
Table 2: Results for different predictor configura-
tions. Numbers give % reductions in keystrokes.
user M
1 2 3 4 5
superman 48.6 53.5 51.8 51.1 50.9
rational 11.7 17.8 17.2 16.4 16.1
real 5.3 10.10 10.7 10.0 9.7
Table 3: Results for different user simulations.
Numbers give % reductions in keystrokes.
acceptance, 1 otherwise. Here k was obtained by
comparing x to the known x? from the test corpus.
It may seem artificial to measure performance ac-
cording to the objective function for the predictor,
but this is biased only to the extent that it misrepre-
sents an actual user?s characteristics. There are two
cases: either the user is a better candidate?types
more slowly, reacts more quickly and rationally?
than assumed by the model, or a worse one. The
predictor will not be optimized in either case, but
the simulation will only overestimate the benefit in
the second case. By being conservative in estimating
the parameters of the user model, we feel we have
minimized the number of translators who would fall
into this category, and thus can hope to obtain real-
istic lower bounds for the average benefit across all
translators.
Table 2 contains results for two different trans-
lation models. The top portion corresponds to the
MEMD2B maximum entropy model described in
(Foster, 2000a); the bottom portion corresponds to
the linear combination of a trigram and IBM 2 used
in the TransType experiments (Langlais et al, 2002).
Columns give the maximum permitted number of
words in predictions. Rows show different predic-
tor configurations: fixed ignores the user model and
makes fixedM -word predictions; linear uses the lin-
ear character-probability estimates described in sec-
tion 3.1; exact uses the exact character-probability
calculation; corr is described below; and best gives
an upper bound on performance by choosing m in
step 3 of the search algorithm so as to maximize
B(x,h, s, k) using the true value of k.
Table 3 illustrates the effects of different compo-
nents of the user model by showing results for sim-
ulated users who read infinitely fast and accept only
predictions having positive benefit (superman); who
read normally but accept like superman (rational);
and who match the standard user model (real). For
each simulation, the predictor optimized benefits for
the corresponding user model.
Several conclusions can be drawn from these re-
sults. First, it is clear that estimating expected bene-
fit is a much better strategy than making fixed-word-
length proposals, since the latter causes an increase
in time for all values of M . In general, making ?ex-
act? estimates of string prefix probabilities works
better than a linear approximation, but the difference
is fairly small.
Second, the MEMD2B model significantly out-
performs the trigram+IBM2 combination, produc-
ing better results for every predictor configuration
tested. The figure of -11.5% in bold corresponds
to the TransType configuration, and corroborates the
validity of the simulation.3
Third, there are large drops in benefit due to read-
ing times and probabilistic acceptance. The biggest
cost is due to reading, which lowers the best possi-
ble keystroke reduction by almost 50% for M = 5.
Probabilistic acceptance causes a further drop of
about 15% for M = 5.
The main disappointment in these results is that
performance peaks at M = 3 rather than continu-
ing to improve as the predictor is allowed to con-
sider longer word sequences. Since the predictor
knows B(x,h, s, k), the most likely cause for this
is that the estimates for p(w?m|h, s) become worse
with increasing m. Significantly, performance lev-
3Although the drop observed with real users was greater at
about 20% (= 17% reduction in speed), there are many dif-
ferences between experimental setups that could account for
the discrepancy. For instance, part of the corpus used for the
TransType trials was drawn from a different domain, which
would adversely affect predictor performance.
els off at three words, just as the search loses di-
rect contact with h through the trigram. To correct
for this, we used modified probabilities of the form
?m p(w?m|h, s), where ?m is a length-specific cor-
rection factor, tuned so as to optimize benefit on a
cross-validation corpus. The results are shown in the
corr row of table 2, for exact character-probability
estimates. In this case, performance improves with
M , reaching a maximum keystroke reduction of
12.6% at M = 5.
7 Conclusion and Future Work
We have described an approach to text prediction for
translators that is based on maximizing the benefit
to the translator according to an explicit user model
whose parameters were set from data collected in
user evaluations of an existing text prediction proto-
type. Using this approach, we demonstrate in sim-
ulated results that our current predictor can reduce
the time required for an average user to type a text
in the domain of our training corpus by over 10%.
We look forward to corroborating this result in tests
with real translators.
There are many ways to build on the work de-
scribed here. The statistical models which are
the backbone of the predictor could be improved
by making them adaptive?taking advantage of the
user?s input?and by adding features to capture the
alignment relation between h and s in such a way as
to preserve the efficient search properties. The user
model could also be made adaptive, and it could be
enriched in many other ways, for instance so as to
capture the propensity of translators to accept at the
beginnings of words.
We feel that the idea of creating explicit user mod-
els to guide the behaviour of interactive systems is
likely to have applications in areas of NLP apart
from translators? tools. For one thing, most of the
approach described here carries over more or less
directly to monolingual text prediction, which is an
important tool for the handicapped (Carlberger et al,
1997). Other possibilities include virtually any ap-
plication where a human and a machine communi-
cate through a language-rich interface.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.
Pietra, and Robert L. Mercer. 1993. The mathematics
of Machine Translation: Parameter estimation. Com-
putational Linguistics, 19(2):263?312, June.
Alice Carlberger, Johan Carlberger, Tina Magnuson,
Sira E. Palazuelos-Cagigas, M. Sharon Hunnicutt, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: an evaluation study. In
Proceedings of the 2nd Workshop on NLP for Commu-
nication Aids, Madrid, Spain, July.
Kenneth W. Church and Eduard H. Hovy. 1993. Good
applications for crummy machine translation. Ma-
chine Translation, 8:239?258.
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text Mediated Interactive Machine
Translation. Machine Translation, 12:175?194.
George Foster. 2000a. Incorporating position infor-
mation into a Maximum Entropy / Minimum Di-
vergence translation model. In Proceedings of the
4th Computational Natural Language Learning Work-
shop (CoNLL), Lisbon, Portugal, September. ACL
SigNLL.
George Foster. 2000b. A Maximum Entropy / Minimum
Divergence translation model. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Hong Kong, October.
Martin Kay. 1973. The MIND system. In R. Rustin,
editor, Natural Language Processing, pages 155?188.
Algorithmics Press, New York.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion, 25(4).
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Unit completion for a computer-aided transla-
tion typing system. Machine Translation, 15(4):267?
294, December.
Philippe Langlais, Guy Lapalme, and Marie Loranger.
2002. TransType: From an idea to a system. Machine
Translation. To Appear.
S. Niessen, S. Vogel, H. Ney, and C. Tillmann. 1998.
A DP based search algorithm for statistical machine
translation. In Proceedings of the 36th Annual Meet-
ing of the ACL and 17th COLING 1998, pages 960?
967, Montre?al, Canada, August.
C. Tillmann and H. Ney. 2000. Word re-ordering and
DP-based search in statistical machine translation. In
Proceedings of the International Conference on Com-
putational Linguistics (COLING) 2000, Saarbrucken,
Luxembourg, Nancy, August.
 	

ffImproving a general-purpose Statistical Translation Engine by
Terminological lexicons
Philippe Langlais
RALI / DIRO / Universite? de Montre?al
C.P. 6128, succursale Centre-ville
Montre?al (Que?bec)
Canada, H3C 3J7
email:felipe@iro.umontreal.ca
Abstract
The past decade has witnessed exciting work
in the field of Statistical Machine Translation
(SMT). However, accurate evaluation of its po-
tential in real-life contexts is still a questionable
issue.
In this study, we investigate the behavior of
an SMT engine faced with a corpus far differ-
ent from the one it has been trained on. We
show that terminological databases are obvious
resources that should be used to boost the per-
formance of a statistical engine. We propose
and evaluate a way of integrating terminology
into a SMT engine which yields a significant re-
duction in word error rate.
1 Introduction
SMT mainly became known to the linguistic
community as a result of the seminal work of
Brown et al (1993b). Since then, many re-
searchers have invested effort into designing bet-
ter models than the ones proposed in the afore-
mentioned article and several new exciting ways
have been suggested to attack the problem.
For instance, Vogel et al (1996) succeeded in
overcoming the independence assumption made
by IBM models by introducing order-1 Hidden
Markov alignment models. Och et al (1999) de-
scribed an elegant way of integrating automat-
ically acquired probabilistic templates into the
translation process, and Nie?en and Ney (2001)
did the same for morphological information.
Radically different statistical models have
also been proposed. (Foster, 2000) investigated
maximum entropy models as an alternative to
the so-called noisy-channel approach. Very re-
cently, Yamada and Knight (2001) described a
model in which the noisy-channel takes as input
a parsed sentence rather than simple words.
While many of these studies include intensive
evaluation sections, it is not always easy to de-
termine exactly how well statistical translation
can do on a given task. We know that on a spe-
cific task of spoken language translation, Wang
(1998) provided evidence that SMT compared
favorably to a symbolic translation system; but
as mentioned by the author, the comparison was
not totally fair.
We do not know of any studies that describe
extensive experiments evaluating the adequacy
of SMT in a real translation environment. We
prefer not to commit ourselves to defining what
a real translation task is; instead, we adopt the
conservative point of view that a viable transla-
tion engine (statistical or not) is one that copes
with texts that may be very different in nature
from those used to train it.
This fairly general definition suggests that
adaptativity is a cornerstone of a successful
SMT engine. Curiously enough, we are not
aware of much work on adaptative SMT, de-
spite the tremendous amount of work done on
adaptative statistical language modeling.
In this paper, we propose to evaluate how
a statistical engine behaves when translating a
very domain specific text which is far different
from the corpus used to trained both our trans-
lation and language models. We first describe
our translation engine. In section 3, we quantify
and analyse the performance deterioration of an
SMT engine trained on a broad-based corpus
(the Hansard) when used to translate a domain
specific text (in this study, a manual for military
snipers). In section 4, We then suggest a sim-
ple but natural way of improving a broad-based
SMT engine; that is, by opening the engine to
available terminological resources. In section 5,
we report on the improvement we observed by
implementing our proposed approach. Finally,
in section 6 we discuss other approaches we feel
can lead to more robust translation.
2 Our statistical engine
2.1 The statistical models
In this study, we built an SMT engine designed
to translate from French to English, following
the noisy-channel paradigm first described by
(Brown et al, 1993b). This engine is based on
equation 1, where eI?1 stands for the sequence
of I? English target words to be found, given a
French source sentence of J words fJ1 :
eI?1 = argmax
I,eI1
P (eI1)
? ?? ?
language
. P (fJ1 |eI1)
? ?? ?
translation
(1)
To train our statistical models, we assembled
a bitext composed of 1.6 million pairs of sen-
tences that were automatically aligned at the
sentence level. In this experiment, every token
was converted into lowercase before training.
The language model we used is an interpo-
lated trigram we trained on the English sen-
tences of our bitext. The perplexity of the re-
sulting model is fairly low ? 65 ?, which actually
reflects the fact that this corpus contains many
fixed expressions (e.g pursuant to standing
order).
The inverted translation model we used is
an IBM2-like model: 10 iterations of IBM1-
training were run (reducing the perplexity of
the training corpus from 7776 to 90), followed
by 10 iterations of IBM2-training (yielding a
final perplexity of 54). We further reduced
the number of transfer parameters (originally
34 969 331) by applying an algorithm described
in Foster (2000); this algorithm basically filters
in the pairs of words with the best gain, where
gain is defined as the difference in perplexity ?
measured on a held-out corpus ? of a model
trained with this pair of words and a model
trained without. In this experiment, we worked
with a model containing exactly the first gain-
ranked million parameters. It is interesting to
note that by doing this, we not only save mem-
ory, and therefore time, but also obtain improv-
ments in terms of perplexity and overall perfor-
mance1.
1On a translation task from French to English on
2.2 The search algorithm
The maximum operation in equation 1, also
called search or decoding, involves a length
model. We assume that the length (counted in
words) of French sentences that translate an En-
glish sentence of a given length follow a normal
distribution.
We extended the decoder described by Nie?en
et al (1998) to a trigram language model. The
basic idea of this search algorithm is to expand
hypotheses along the positions of the target
string while progressively covering the source
ones. We refer the reader to the original paper
for the recursion on which it relies, and instead
give in Figure 1 a sketch of how a translation
is built. An hypothesis h is fully determined
by four parameters: its source (j) and target
(i) positions of the last word (e), and its cov-
erage (c). Therefore, the search space can be
represented as a 4-dimension table, each item
of which contains backtracking information (f
for the fertility of e, bj and bw for the source
position and the target word we should look at
to backtrack) and the hypothesis score (prob).
We know that better alignment models have
been proposed and extensively compared (Och
and Ney, 2000). We must however point
out that the performance we obtained on the
hansard corpus (see Section 3) is comparable
to the rates published elsewhere on the same
kind of corpus. In any case, our goal in this
study is to compare the behavior of a SMT en-
gine in both friendly and adverse situations. In
our view, the present SMT engine is suitable for
such a comparative study.
2.3 Tuning the decoder
The decoder has been tuned in several ways in
order to reduce its computations without detri-
mentally affecting the quality of its output. The
first thing we do when the decoder receives a
sentence is to compute what we call an active
vocabulary ; that is, a collection of words which
are likely to occur in the translation. This is
done by ranking for each source word the tar-
get words according to their non normalized
posterior likelihood (that is argmaxe p(f |e)p(e),
where p(e) is given by a unigram target lan-
guage model, and p(f |e) is given by the transfer
Hansard sentences, we observed a reduction in word er-
ror rate of more than 3% with the reduced model.
Input: f1 . . . fj . . . fJ
Initialize the search space table Space
Select a maximum target length: Imax
Compute the active vocabulary
// Fill the search table recursively:
for all target position i = 1, 2, . . . , Imax do
prune(i ? 1);
for all alive hyp. h = Space(i, j, c, e) do
uv ? History(h);
zones ? FreeSrcPositions(h);
bestWords ? NBestTgtWords(uv);
for all w in bestWords do
prob ? Score(h) + log p(w|uv);
setIfBetter(i, j, c, b, prob, 0, j, v);
for all free source position d do
s ? prob;
for all f ? [1, fmax] / d + f ? 1 is
free do
s+ = log a(i|d, J) + log t(fd|ei);
setIfBetter(i, d, c+f, w, s, f, j, w);
// Find and return the best hypothesis if any
maxs ? ??
for all i ? [1, Imax] do
for all alive hyp. h = Space(i, j, c, e) do
s ? Score(h) + log p(i|J);
if ((c == J) and (s > maxs)) then
maxs ? s
?maxi, maxj , maxe? ? ?i, j, e?
if (maxs! = ?) then
Return Space(maxi, maxj , J, maxe);
else
Failure
Output: e1 . . . ei . . . emaxi
Figure 1: Sketch of our decoder.
FreeSrcPositions returns the source posi-
tions not already associated to words of h;
NBestTgtWords returns the list of words
that are likely to follow the last bigram uv
preceeding e according to the language model;
and setIfBetter(i, j, c, e, p, f, bj , bw) is an
operator that memorizes an hypothesis if its
score (p) is greater than the hypothesis already
stored in Space(i, j, c, e). a and t stands for the
alignment and transfert distributions used by
IBM2 models.
probabilities of our inverted translation model)
and keeping for each source word at most a tar-
get words.
Increasing a raises the coverage of the active
vocabulary, but also slows down the translation
process and increases the risk of admitting a
word that has nothing to do with the transla-
tion. We have conducted experiments with var-
ious a-values, and found that an a-value of 10
offers a good compromise.
As mentioned in the block diagram, we also
prune the space to make the search tractable.
This is done with relative filtering as well as ab-
solute thresholding. The details of all the filter-
ing strategies we implemented are however not
relevant to the present study.
3 Performances of our SMT engine
3.1 Test corpora
In this section we provide a comparison of the
translation performances we measured on two
corpora. The first one (namely, the hansard)
is a collection of sentences extracted from a part
of the Hansard corpus we did not use for train-
ing. In particular, we did not use any specific
strategy to select these sentences so that they
would be closely related to the ones that were
used for training.
Our second corpus (here called sniper) is
an excerpt of an army manual on sniper train-
ing and deployment that was used in an EAR-
LIER study (Macklovitch, 1995). This corpus is
highly specific to the military domain and would
certainly prove difficult for any translation en-
gine not specifically tuned to such material.
3.2 Overall performance
In this section, we evaluate the performance of
our engine in terms of sentence- and word- error
rates according to an oracle translation2. The
first rate is the percentage of sentences for which
the decoder found the exact translation (that is,
the one of our oracle), and the word error rate
is computed by a Levenstein distance (count-
ing the same penalty for both insertion, dele-
tion and substitution edition operations). We
realize that these measures alone are not suffi-
cient for a serious evaluation, but we were re-
2Both corpora have been published in both French
and English, and we took the English part as the gold
standard.
luctant in this experiment to resort to manual
judgments, following for instance the protocol
described in (Wang, 1998). Actually a quick
look at the degradation in performance we ob-
served on sniper is so clear that we feel these
two rates are informative enough !
Table 1 summarizes the performance rates we
measured. The WER is close to 60% on the
hansard corpus and close to 74% on sniper;
source sentences in the latter corpus being
slightly longer on average (21 words). Not a
single sentence was found to be identical to the
gold standard translation on the sniper corpus
3.
corpus nbs |length| SER WER
hansard 1038 ?16.2, 7.8? 95.6 59.6
sniper 203 ?20.8, 6.8? 100 74.6
Table 1: Main characteristics of our test cor-
pora and global performance of our statistical
translator without any adjustments. |length|
reports the average length (counted in words)
of the source sentences and the standard de-
viation; nbs is the number of sentences in the
corpus.
3.3 Analyzing the performance drop
As expected, the poor performance observed on
the sniper text is mainly due to two reasons:
the presence of out of vocabulary (OOV) words
and the incorrect translations of terminological
units.
In the sniper corpus, 3.5% of the source to-
kens and 6.5% of the target ones are unknown
to the statistical models. 44% of the source sen-
tences and 77% of the target sentences contain
at least one unknown word. In the hansard
text, the OOV rates are much lower: around
0.5% of the source and target tokens are un-
known and close to 5% of the source and target
sentences contain at least one OOV words.
These OOV rates have a clear impact on
the coverage of our active vocabulary. On the
sniper text, 72% of the oracle tokens are in the
active vocabulary (only 0.5% of the target sen-
tences are fully covered); whilst on hansard,
3The full output of our translation sessions
is available at www-iro.umontreal.ca/?felipe/
ResearchOutput/Computerm2002
86% of the oracle?s tokens are covered (24% of
the target sentences are fully covered).
Another source of disturbance is the presence
of terminological units (TU) within the text to
translate. Table 2 provides some examples of
mistranslated TU from the sniper text. We
also observed that many words within termino-
logical units are not even known by the statisti-
cal models. Therefore accounting for terminol-
ogy is one of the ways that should be considered
to reduce the impact of OOV words.
< source term / oracle / translation>
<a?me / bore / heart>
<huile polyvalente / general purpose oil / oil
polyvalente>
<chambre / chamber / house of common>
<tireur d? e?lite / sniper / issuer of elite>
<la longueur de la crosse / butt length / the
length of the crosse>
Table 2: Examples of mistranslated terminolog-
ical entries of the sniper corpus.
4 Integrating non-probabilistic
terminological resources
Using terminological resources to improve the
quality of an automatic translation engine is not
at all a new idea. However, we know of very few
studies that actually investigated this avenue
in the field of statistical machine translation.
Among them, (Brown et al, 1993a) have pro-
posed a way to exploit bilingual dictionnaries at
training time. There may also be cases where
domain-specific corpora are available which al-
low for the training of specialized models that
can be combined with the general ones.
Another approach that would not require
such material at training time consists in de-
signing an adaptative translation engine. For
instance, a cache-based language model could
be used instead of our static trigram model.
However, the design of a truly adaptative trans-
lation model remains a more speculative enter-
prise. At the very least, it would require a fairly
precise location of errors in previously trans-
lated sentences; and we know from the AR-
CADE campaign on bilingual alignments, that
accurate word alignments are difficult to obtain
(Ve?ronis and Langlais, 2000). This may be even
more difficult in situations where errors will in-
volve OOV words.
We investigated a third option, which involves
taking advantage ? at run time ? of existing ter-
minological resources, such as Termium4. As
mentioned by Langlais et al (2001), one of
a translator?s first tasks is often terminological
research; and many translation companies em-
ploy specialized terminologists. Actually, aside
from the infrequent cases where, in a given
thematic context, a word is likely to have a
clearly preferred translation (e.g. bill/facture
vs bill/projet de loi), lexicons are often the
only means for a user to influence the transla-
tion engine.
Merging such lexicons at run time offers
a complementary solution to those mentioned
above and it should be a fruitful strategy in sit-
uations where terminological resources are not
available at training time (which may often be
the case). Unfortunately, integrating termino-
logical (or user) lexicons into a probabilistic en-
gine is not a straightforward operation, since
we cannot expect them to come with attached
probabilities. Several strategies do come to
mind, however. For instance, we could credit a
translation of a sentence that contains a source
lexicon entry in cases it contains an authorized
translation. But this strategy may prouve dif-
ficult to tune since decoding usually involves
many filtering strategies.
The approach we adopted consists in view-
ing a terminological lexicon as a set of con-
straints that are employed to reduce the search
space. For instance, knowing that sniper is a
sanctioned translation of tireur d?e?lite, we may
require that current hypotheses in the search
space associate the target word sniper with the
three source French words.
In our implementation, we had to slightly
modify the block diagram of Figure 1 in order
to: 1) forbid a given word ei from being asso-
ciated with a word belonging to a source ter-
minological unit, if it is not sanctioned by the
lexicon; and 2) add at any target position an
hypothesis linking a target lexicon entry to its
source counterpart. Whether these hypotheses
will survive intact will depend on constraints
imposed by the maximum operation (of equa-
tion 1) over the full translation.
The score associated with a target entry ei?i
4See http://www.termium.com/site/.
when linked to its source counterpart f j?j in the
latter case is given by:
?
k?[i,i?]
log p(ek|ek?2ek?1) + max
l?[j,j?]
log(a(k|l, J))
The rationale behind this equation is that
both the language (p) and the alignment (a)
models have some information that can help
to decide the appropriateness of an extension:
the former knows how likely it is that a word
(known or not) will follow the current history5;
and the latter knows to some extent where the
target unit should be (regardless of its identity).
In the absence of a better mechanism (e.g. a
cache-model should be worth a try) We hope
that this will be sufficient to determine the final
position of the target unit in a given hypothesis.
5 Results
We considered three terminological lexicons
whose characteristics are summarized in Table
3; they essentially differ in terms of number
of entries and therefore coverage of the text to
translate.
lexicon nb coverage SER WER
sniper-1 33 20/247 99 67.4
sniper-2 59 47/299 98 66.2
sniper-3 146 132/456 98 64.3
Table 3: Translation performance with differ-
ent terminological lexicons. nb is the number of
entries in the lexicon and coverage reports the
number of different source entries from the lex-
icon belonging to the text to translate and the
total number of their occurrences.
The first lexicon (namely sniper-1) contains
the 33 entries used in the study of terminological
consistency checking described in (Macklovitch,
1995). The second and third lexicons (namely
sniper-2 and sniper-3) contain those entries
plus other ones added manually after an incre-
mental inspection of the sniper corpus.
As can be observed from Table 3, introduc-
ing terminological lexicons into the translation
engine does improve performance, measured in
terms of WER, and this even with lexicons that
5Our trigram model has been trained to provide pa-
rameters such as p(UNK|ab).
Source le tireur d? e?lite voit simultane?ment les fils croise?s et l? image ( l? objectif ) .
Target the sniper sees the crosshairs and the image - target - at the same time .
without the gunman being same son sit and picture of the hon. members : agreed .
with the sniper simultaneously see the crosshairs and the image (objective . )
Source contro?le de la de?tente .
Target exercising trigger control .
without the control of de?tente .
with control of the trigger .
Table 4: Two examples of translation with and without a terminological lexicon; TU appear in
bold.
cover only a small portion of the text to trans-
late. With the narrow coverage lexicon, we ob-
serve an absolute reduction of 7%, and a reduc-
tion of 10% with the broader lexicon sniper-3.
This suggests that adding more entries into the
lexicon is likely to decrease WER. In another
study (Carl and Langlais, 2002), we investigated
whether an automatic procedure designed to de-
tect term variants could improve these perfor-
mances futher.
Table 4 provides two examples of translation
outputs, with and without the help of termino-
logical units. The first one clearly shows that
EVEN A few TU (two in this case) may sub-
stantially improve the quality of the translation
output; (the translation produced without the
lexicon was particularly poor in this very case.
Even though terminological lexicons do im-
prove the overall WER figure, a systematic in-
spection of the outputs produced with TU re-
veals that the translations are still less faithful
to the source text than the translations pro-
duced for the hansard text. OOV words re-
main a serious problem.
6 Discussion
In this study, we have shown that translat-
ing texts in specific domains with a general[-
]purpose statistical engine is difficult. This sug-
gests the need to implementing an adaptative
strategy. Among the possible scenarios, we have
shown that opening the engine to terminologi-
cal resources is a natural and efficient way of
softening the decoder.
In a similar vein, Marcu (2001) investigated
how to combine Example Based Machine Trans-
lation (EBMT) and SMT approaches. The au-
thor automatically derived from the Hansard
corpus what he calls a translation memory: ac-
tually a collection of pairs of source and target
word sequences that are in a translation rela-
tion according to the viterbi alignment run with
an IBM4 model that was also trained on the
Hansard corpus. This collection of phrases was
then merged with a greedy statistical decoder to
improve the overall performance of the system.
What this study suggests is that translation
memories collected from a given corpus can im-
prove the performance of a statistical engine
trained on the same corpus, which in itself is
an interesting result. A very similar study but
with weaker results is derscribed in (Langlais et
al., 2000), in the framework of the TransType
project. Besides the different metrics the au-
thors used, the discrepancy in performance in
these two studies may be explained by the na-
ture of the test corpora used. The test corpus
in the latter study was more representative of a
real translation task, while the test corpus that
Marcu used was a set of around 500 French sen-
tences of no more than 10 words.
Our present study is close in spirit to these
last two, except that we do not attack the prob-
lem of automatically acquiring bilingual lexi-
cons; instead, we consider it a part of the trans-
lator?s task to provide such lexicons. Actually,
we feel this may be one of the only ways a user
has of retaining some control over the engine?s
output, a fact that professional translators seem
to appreciate (Langlais et al, 2001).
As a final remark, we want to stress that we
see the present study as a first step toward the
eventual unification of EBMT and SMT, and in
this respect we agree with (Marcu, 2001). Po-
tentially, of course, EBMT can offer much more
than just a simple list of equivalences, like those
we used in this study. However, the basic ap-
proach we describe here still holds, as long as
we can extend the notion of constraint used in
this study to include non-consecutive sequences
of words. This is a problem we we plan to in-
vestigate in future research.
Acknowledgments
I am indebted to Elliott Macklovitch and
George Foster for the fruitful orientation they
gave to this work.
References
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Meredith J. Goldsmith,
Jan Hajic, Robert L. Mercer, and Surya
Mohanty. 1993a. But dictionaries are data
too. In Human Language Technology (HLT),
pages 202?205, Princeton, NJ, march.
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, and Robert L. Mer-
cer. 1993b. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Michael Carl and Philippe Langlais. 2002. To-
ward an intelligent terminology database as
a front-and backend for statistical machine
translation. In COMPUTERM 2002, Taipei.
George Foster. 2000. A Maximum Entropy /
Minimum Divergence translation model. In
Proceedings of the 38th Annual Meeting of the
ACL, pages 37?44, Hong Kong, October.
Philippe Langlais, George Foster, and Guy
Lapalme. 2000. Unit completion for a
computer-aided translation typing system. In
Proceedings of the 5th Conference on Applied
Natural Language Processing (ANLP), pages
135?141, Seattle, Washington, May.
Philippe Langlais, George Foster, and Guy La-
palme. 2001. Integrating bilingual lexicons in
a probabilistic translation assistant. In Pro-
ceedings of the 8th Machine Translation Sum-
mit, pages 197?202, Santiago de Compostela,
Galicia, Spain, September. IAMT.
Elliott Macklovitch. 1995. Can terminologi-
cal consistency be validated automatically
? Technical report, CITI/RALI, Montre?al,
Canada.
Daniel Marcu. 2001. Towards a unified ap-
proach to memory- and statistical-based ma-
chine translation. In Proceedings of the 39th
Annual Meeting of the ACL, pages 378?385,
Toulouse, France.
Sonja Nie?en and Hermann Ney. 2001. Toward
hierarchical models for statistical machine
translation of inflected languages. In Proceed-
ings of the Workshop on Data Driven Ma-
chine Translation yielded at the 39th Annual
Meeting of the ACL, pages 47?54, Toulouse,
France.
Sonja Nie?en, Stephan Vogel, Hermann Ney,
and Christoph Tillmann. 1998. A dp based
search algorithm for statistical machine trans-
lation. In Proceedings of the 36th Annual
Meeting of the ACL and the 17th COLING,
pages 960?966, Montre?al, Canada, August.
Franz Joseph Och and Hermann Ney. 2000.
A comparison of alignement models for sta-
tistical machine translation. In Proceed-
ings of the International Conference on
Computational Linguistics (COLING) 2000,
pages 1086?1090, Saarbrucken, Luxembourg,
Nancy, August.
Franz Josef Och, Christoph Tillman, and Her-
man Ney. 1999. Improved alignment models
for statistical machine translation. In Pro-
ceedings of the 4nd Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP), pages 20?28, College Park, Mary-
land.
Jean Ve?ronis and Philippe Langlais, 2000. Eval-
uation of parallel text alignment systems: The
ARCADE project, volume 13, chapter 19,
pages 369?388. Parallel Text Processing,
Kluwer.
Stephan Vogel, Hermann Ney, and Christoph
Tillmann. 1996. Hmm-based word aligne-
ment in statistical translation. In Proceedings
of the International Conference on Compu-
tational Linguistics (COLING) 1996, pages
836?841, Copenhagen, Denmark, August.
Ye-Yi Wang. 1998. Grammar Inference and
Statistical Machine Translation. Ph.D. the-
sis, CMU-LTI, Carnegie Mellon University.
Kenji Yamada and Kevin Knight. 2001. A
syntax-based statistical translation model. In
Proceedings of the 39th Annual Meeting of the
ACL, pages 531?538, Toulouse, France.
Statistical Translation Alignment
with Compositionality Constraints
Michel Simard and Philippe Langlais
Laboratoire de recherche applique?e en linguistique informatique (RALI)
De?partement d?informatique et de recherche ope?rationnelle
Universite? de Montre?al
C.P. 6128, succursale Centre-ville, Local 2241
Montre?al (Que?bec), Canada H3C 3J7
{simardm,felipe}@iro.umontreal.ca
Abstract
This article presents a method for aligning
words between translations, that imposes a
compositionality constraint on alignments pro-
duced with statistical translation models. Ex-
periments conducted within the WPT-03 shared
task on word alignment demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Since the pioneering work of the IBM machine trans-
lation team almost 15 years ago (Brown et al, 1990),
statistical methods have proven to be valuable tools in
approaching the automation of translation. Word align-
ments (WA) play a central role in the statistical modeling
process, and reliable WA techniques are crucial in acquir-
ing the parameters of the models (Och and Ney, 2000).
Yet, the very nature of these alignments, as defined in
the IBM modeling approach (Brown et al, 1993), lead
to descriptions of the correspondences between source-
language (SL) and target-language (TL) words of a trans-
lation that are often unsatisfactory, at least from a human
perspective.
One notion that is typically evacuated in the statisti-
cal modeling process is that of compositionality: a fun-
damental assumption in statistical machine translation is
that, ultimately, all the words of a SL segment S con-
tribute to produce all the words of its TL translation T , at
least to some degree. While this makes perfect sense from
a stochastic point of view, it contrasts with the hypothesis
at the basis of most (if not all) other MT approaches, as
well as with our natural intuitions about translation: that
individual portions of the SL text produce individual TL
portions autonomously, and that the final translation T is
obtained by somehow piecing together these TL portions.
In what follows, we show how re-integrating compo-
sitionality into the statistical translation word alignment
process leads to better alignments. We first take a closer
look at the ?standard? statistical WA techniques in section
2, and then propose a way of imposing a compositional-
ity constraint on these techniques in section 3. In section
4, we discuss various implementation issues, and finally
present the experimental results of this approach on the
WPT-03 shared task on WA in section 5.
2 Statistical Word Alignment
Brown et al (1993) define a word alignment as a vec-
tor a = a1...am that connects each word of a source-
language text S = s1...sm to a target-language word in
its translation T = t1...tn, with the interpretation that
word taj is the translation of word sj in S (aj = 0 is
used to denote words of s that do not produce anything in
T ).
The Viterbi alignment between source and target sen-
tences S and T is defined as the alignment a? whose prob-
ability is maximal under some translation model:
a? = argmaxa?APrM(a|S, T )
where A is the set of all possible alignments between S
and T , and PrM(a|S, T ) is the estimate of a?s probabil-
ity under model M, which we denote Pr(a|S, T ) from
hereon. In general, the size of A grows exponentially
with the sizes of S and T , and so there is no efficient way
of computing a? efficiently. However, under the indepen-
dence hypotheses of IBM Model 2, the Viterbi alignment
can be obtained by simply picking for each position i in
S, the alignment that maximizes t(si|tj)a(j, i,m, n), the
product of the model?s ?lexical? and ?alignment? proba-
bility estimates. This procedure can trivially be carried
out in O(mn) operations. Because of this convenient
property, we take the Viterbi-2 WA method (which we
later refer to as the V method) as the basis for the rest of
this work.
3 Compositionality
In IBM-style alignments, each SL token is connected to a
single (possibly null) TL token, typically the TL token
with which it has the most ?lexical affinities?, regard-
less of other existing connections in the alignment and,
more importantly, of the relationships it holds with other
SL tokens in its vicinity. In practice, this means that
some TL tokens can end up being connected to several
SL tokens, while other TL tokens are left unconnected.
This contrasts with alternative alignment models such as
those of Melamed (1998) and Wu (1997), which impose a
?one-to-one? constraint on alignments. Such a constraint
evokes the notion of compositionality in translation: it
suggests that each SL token operates independently in the
SL sentence to produce a single TL token in the TL sen-
tence, which then depends on no other SL token.
This view is, of course, extreme, and real-life transla-
tions are full of examples that show how this composi-
tionality principle breaks down as we approach the level
of word correspondences. Yet, if we can find a way of
imposing compositionality constraints on WA?s, at least
to the level where it applies, then we should obtain more
sensible results than with Viterbi alignments.
For instance, consider a procedure that splits both the
SL and TL sentences S and T into two independent parts,
in such a way as to maximise the probability of the two
resulting Viterbi alignments:
argmax?i,j,d?
?
???
???
d = 1 : Pr(a1|si1, t
j
1)
?Pr(a2|smi+1, t
n
j+1)
d = ?1 : Pr(a1|si1, t
n
j+1)
?Pr(a2|smi+1, t
j
1)
(1)
In the triple ?i, j, d? above, i represents a ?split point?
in the SL sentence S, j is the analog for TL sentence T ,
and d is the ?direction of correspondence?: d = 1 denotes
a ?parallel correspondence?, i.e. s1...si corresponds to
t1...tj and si+1...sm corresponds to tj+1...tn; d = ?1
denotes a ?crossing correspondence?, i.e. s1...si corre-
sponds to tj+1...tn and si+1...sm corresponds to t1...tj .
The triple ?I, J,D? produced by this procedure refers
to the most probable alignment between S and T , un-
der the hypothesis that both sentences are made up of
two independent parts (s1...sI and sI+1...sm on the one
hand, t1...tJ and tJ+1...tn on the other), that correspond
to each other two-by-two, following direction D. Such
an alignment suggests that translation T was obtained
by ?composing? the translation of s1...sI with that of
sI+1...sm.
In the above procedure, these ?composing parts? of
S and T are further assumed to be contiguous sub-
sequences of words. Once again, real-life translations are
full of examples that contradict this (negations in French
and particle verbs in German are two examples that im-
mediately spring to mind when aligning with English).
Yet, this contiguity assumption turns out to be very con-
venient, because examining pairings of non-contiguous
sequences would quickly become intractable. In con-
trast, the procedure above can find the optimal partition
in polynomial time.
The ?splitting? process described above can be re-
peated recursively on each pair of matching segments,
down to the point where the SL segment contains a sin-
gle token. (TL segments can always be split, even when
empty, because IBM-style alignments allow connecting
SL tokens to the ?null? TL token, which is always avail-
able.) This recursive procedure actually produces two
different outputs:
1. A parallel partition of S and T into m pairs of seg-
ments ?si, tkj ?, where each tkj is a (possibly null)
contiguous sub-sequence of T ; this partition can of
course be viewed as an alignment on the words of S
and T .
2. an IBM-style alignment, such that each SL and TL
token is linked to at most one token in the other lan-
guage: this alignment is actually the concatenation
of individual Viterbi alignments on the ?si, tkj ? pairs,
which connects each si to (at most) one of the tokens
in the corresponding tkj .
In this procedure, which we call Compositional WA (or
C for short), there are at least two problems. First, each
SL token finds itself ?isolated? in its own partition bin,
which makes it impossible to account for multiple SL to-
kens acting together to produce a TL sequence. Second,
the TL tokens that are not connected in the resulting IBM-
style alignment do not play any role in the computation
of the probability of the optimal alignment; therefore, the
pair ?si, tkj ? in which these ?superfluous? tokens end up
is more or less random.
To compensate in part for these, we propose using
two IBM-2 models to compute the optimal partition: the
?forward? (SL?TL) model, and the ?reverse? (TL?SL)
model. When examining a particular split ?i, j, d? for S
and T , we compute both Viterbi alignments, forward and
reverse, between all pairs of segments, and score each
pair with the product of the two alignments? probabili-
ties.
In this variant, which we call Combined Compositional
WA (CC), we can no longer allow ?empty? segments in
the TL, and so we stop the recursion as soon as either the
SL or TL segment contains a single token. The resulting
partition therefore consists in a series of 1-to-k or k-to-1
alignments, with k ? 1.
4 Implementation
The C and CC WA methods of section 3 were imple-
mented in a program called ralign (Recursive ? or RALI
? alignment, as you wish). As suggested above, this pro-
gram takes as input a pair of sentence-aligned texts, and
the parameters of two IBM-2 models (forward and re-
verse), and outputs WA?s for the given texts. This pro-
gram also implements plain Viterbi alignments, using the
forward (V) or reverse (RV) models, as well as what we
call the Reverse compositional WA (or RC), which is just
the C method using the reverse IBM-2 model.
The output format proposed for the WPT-03 shared
task on WA allowed participants to distinguish between
?sure? (S) and ?probable? (P) WA?s. We figured that our
alignment procedure implicitly incorporated a way of dis-
tinguishing between the two: within each produced pair
of segments, we marked as ?sure? all WA?s that were pre-
dicted by both (forward and reverse) Viterbi alignments,
and as ?probable? all the others.
The translation models for ralign were trained using
the programs of the EGYPT statistical translation toolkit
(Al-Onaizan et al, 1999). This training was done using
the data provided as part of the WPT-03 shared task on
WA (table 1). We thus produced two sets of models, one
for English and French (en-fr), and one for Romanian
and English (ro-en). All models were trained on both the
training and test datasets1. For en-fr, we considered all
words that appeared only once in the corpus to be ?un-
known words? (whittle option -f 2), so as to obtain de-
fault values of ?real? unknowns in the test corpus2. In the
case of ro-en, there was too little training data for this to
be beneficial, and so we chose to use all words.
English-French
corpus tokens (SL/TL) sentence pairs
training 20M/24M 1M
trial 772/832 37
test 8K/9K 447
Romanian-English
corpus tokens (SL/TL) sentence pairs
training 1M/1M 48K
trial 513/547 17
test 6K/6K 248
Table 1: WPT-03 shared task resources
We trained and tested a number of translation mod-
els before settling for this particular setup. All of these
1No cheating here: the test dataset did not contain reference
alignments
2This is necessary, even when training on the test corpus,
because the EGYPT toolkit?s training program (GIZA) ignores
excessively long sentences in the corpus.
tests were performed using the trial data provided for the
WPT-03 shared task.
5 Experimental Results
The different word-alignment methods described in sec-
tions 2 and 3 were run on the test corpora of the WPT-
03 shared task on alignment. Results were evaluated in
terms of alignment precision (P), recall (R), F-measure
and alignment error rate (AER) (Och and Ney, 2000). As
specified in the shared task description, all of these met-
rics were computed taking null-alignments into account
(i.e. tokens left unconnected in an alignment were actu-
ally counted as aligned to virtual word token ?0?). The
results of our experiments are reproduced in table 2.
We observe that imposing a ?contiguous composition-
ality? constraint (C and RC methods) allows for sub-
stantial gains with regard to plain Viterbi alignments (V
and RV respectively), especially in terms of precision
and AER (a slight decline in recall can be observed be-
tween the V and C methods on the ro-en corpus, but it
is not clear whether this is significant). These gains are
even more interesting when one considers that all pairs of
alignments (V and C, RV and RC) are obtained using ex-
actly the same data. This highlights both the deficiencies
of IBM Model-2 and the importance of compositionality.
Using both the forward and reverse models (CC) yields
yet more gains with regard to all metrics. This result is
interesting, because it shows the potential of the compo-
sitional alignment method for integrating various sources
of information.
With regard to language pairs, it is interesting to note
that all alignment methods produce figures that are sub-
stantially better in recall and worse in precision on the ro-
en data, compared to en-fr. Overall, ro-en alignments dis-
play significantly higher F-measures. This is surprising,
considering that the provided en-fr corpus contained 20
times more training material. This phenomenon is likely
due to the fact that the en-fr test reference contains much
more alignments per word (1.98 per target word) than the
ro-en (1.12). All alignment methods described here pro-
duce roughly between 1 and 1.25 alignments per target
words. This fact affects recall and F-measure figures pos-
itively on the ro-en test, while precision and AER (which
correlates strongly with precision in practice) are affected
inversely.
6 Conclusion
In this article, we showed how a compositionality con-
straint could be imposed when computing word align-
ments with IBM Models-2. Our experiments on the WPT-
03 shared task on WA demonstrated how this improves
the quality of resulting alignments, when compared to
standard Viterbi alignments. Our results also highlight
English-French Romanian-English
method P R F AER
V 0.6610 0.3387 0.4479 0.2700
RV 0.6260 0.3212 0.4245 0.2944
C 0.7248 0.3534 0.4751 0.2318
RC 0.7422 0.3586 0.4835 0.2152
CC 0.7756 0.3681 0.4992 0.1850
method P R F AER
V 0.5509 0.5442 0.5475 0.4524
RV 0.5409 0.5375 0.5391 0.4608
C 0.5818 0.5394 0.5597 0.4402
RC 0.5865 0.5415 0.5630 0.4369
CC 0.6361 0.5714 0.6020 0.3980
Table 2: Alignment results
the benefit of using both forward and reverse translation
models for this task.
One of the weaknesses of the proposed method is the
inability to produce many-to-many alignments. To allow
for such alignments, it would be necessary to establish a
?stopping condition? on the recursion process, so as to
prevent partitioning pairs of segments that display ?non-
compositional? phenomena in both SL and TL languages.
We have begun experimenting with various such mecha-
nisms. One of these is to stop the recursion as soon as the
pair of segments under consideration contains less than
two ?sure? alignments, i.e. connections predicted by both
the forward and reverse models. Another possibility is to
establish a threshold on the probability ?drop? incurred
by the optimal split on any given pair of segments. So
far, these experiments are inconclusive.
Another problem is with ?null? alignments, which the
program is also unable to account for. Currently, omis-
sions and insertions in translation find themselves incor-
porated into aligned segments. A simple way to deal with
this problem would be to exclude from the final alignment
links that are not predicted by either the forward or re-
verse Viterbi alignments. But early experiments with this
approach are unconvincing, and more elaborate filtering
mechanisms will probably be necessary.
Finally, IBM Model 2 is certainly not the state of the
art in statistical translation modeling. Thenagain, the
methods proposed here are not dependent on the underly-
ing translation model, and similar WA methods could be
based on more elaborate models, such as Models 3?5, or
the HMM-based models proposed by Och et al (1999)
for example. On the other hand, our compositional align-
ment method could be used during the training process
of higher-level models. Whether this would lead to better
estimates of the models? parameters remains to be seen,
but it is certainly a direction worth exploring.
References
[Al-Onaizan et al1999] Yaser Al-Onaizan, Jan Curin,
Michael Jahr, Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy, Noah H.
Smith, and David Yarowsky. 1999. Statistical Ma-
chine Translation - Final Report, JHU Workshop 1999.
Technical report, Johns Hopkins University.
[Brown et al1990] Peter F. Brown, John Cocke, Stephen
A. Della Pietra, Vincent J. Della Pietra, Fredrick Je-
linek, John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85,
June.
[Brown et al1993] Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert L. Mer-
cer. 1993. The Mathematics of Machine Transla-
tion: Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
[Melamed1998] I. Dan Melamed. 1998. Word-to-Word
Models of Translational Equivalence. Technical Re-
port 98-08, Dept. of Computer and Information Sci-
ence, University of Pennsylvania, Philadelphia, USA.
[Och and Ney2000] Franz Josef Och and Hermann Ney.
2000. Improved statistical alignment models. In Pro-
ceedings of the 38th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 440?447,
Hong-Kong, China, October.
[Och et al1999] Franz Josef Och, Christoph Tillmann,
and Hermann Ney. 1999. Improved Alignment Mod-
els for Statistical Machine Translation. In Proceedings
of the 4th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP)and 7th ACL Work-
shop on Very Large Corpora (WVLC), pages 20?28,
College Park, USA.
[Wu1997] Dekai Wu. 1997. Stochastic Inversion Trans-
duction Grammars and Bilingual Parsing of Parallel
Corpora. Computational Linguistics, 23(3):377?404,
September.
Work-in-Progress  project report : CESTA - Machine Translation Evaluation 
Campaign 
 
Widad Mustafa El Hadi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
mustafa@univ-lille3.fr  
Marianne Dabbadie 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
dabbadie@univ-lille3.fr 
Isma?l Timimi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
timimi@univ-lille3.fr  
Martin Rajman 
LIA 
Ecole 
Polytechnique 
F?d?rale de Lausanne 
B?t. INR 
CH-1015 Lausanne 
Switzerland 
martin.rajman@epfl.ch 
 
 
Philippe Langlais 
RALI / DIRO - 
Universit? de Montr?al 
C.P. 6128, 
succursale Centre-ville 
Montr?al (Qu?bec) - 
Canada, H3C 3J7 
felipe@IRO.UMontreal.
CA
Antony Hartley 
University of Leeds 
Centre for Translation 
Studies 
Woodhouse Lane 
LEEDS LS2 9JT 
UK 
a.hartley@leeds.ac.uk
Andrei Popescu Belis 
University of Geneva 40 
bvd du Pont d'Arve CH-
1211 Geneva 4 
Switzerland 
Andrei.Popescu-
Belis@issco.unige.ch  
 
Abstract 
CESTA, the first European Campaign 
dedicated to MT Evaluation, is a project 
labelled by the French Technolangue action. 
CESTA provides an evaluation of six 
commercial and academic MT systems using a 
protocol set by an international panel of 
experts. CESTA aims at producing reusable 
resources and information about reliability of 
the metrics. Two runs will be carried out: one 
using the system?s basic dictionary, another 
after terminological adaptation. Evaluation 
task, test material, resources, evaluation 
measures, metrics, will be detailed in the full 
paper. The protocol is the combination of a 
contrastive reference to: IBM ?BLEU? 
protocol (Papineni, K., S. Roukos, T. Ward 
and Z. Wei-Jing, 2001); ?BLANC? protocol 
derived from (Hartley, Rajman, 2002).; 
?ROUGE? protocol (Babych, Hartley, Atwell, 
2003). The results of the campaign will be 
published in a final report and be the object of 
two intermediary and final workshops. 
1 Introduction 
1.1 CESTA and the Technolangue Action in 
France 
 
This article is a collective paper written by the 
CESTA scientific committee that aims at 
presenting the CESTA evaluation campaign, a 
project labelled in 2002 by the French Ministry of 
Research and Education within the framework of 
the Technolangue call for projects and integrated 
to the EVALDA evaluation platform. It reports 
work in progress and therefore is the description of 
an on-going campaign for which system results are 
not yet available.  
 
In France, EVALDA is the new Evaluation 
platform, a joint venture between the French 
Ministry of Research and Technology and ELRA 
(European Language Resources and Evaluation 
Association, Paris, France). Within the framework 
of this initiative eight evaluation projets are being 
conducted:  ARCADE II: campagne d??valuation 
de l?alignement de corpus multilingues; CESART:
 campagne d'Evaluation de Syst?mes 
d?Acquisition de Ressources Terminologiques; 
CESTA : campagne d'Evaluation de Syst?mes de 
Traduction automatique; Easy: Evaluation des 
Analyseurs Syntaxiques du fran?ais; Campagne 
EQueR, Evaluation en question-r?ponse; 
Campagne ESTER, Evaluation de transcriptions 
d??missions radio; Campagne EvaSY, Evaluation 
en synth?se vocale; and Campagne MEDIA, 
Evaluation du dialogue hors et en contexte. 
 
Regarding evaluation, the objectives of the 
Action as Joseph Mariani pointed out in his 
presentation at the LREC 2002 conference are to: 
? Improve the present evaluation 
methodologies  
? Identify new (quantitative and qualitative) 
approaches for already evaluated 
technologies:  socio-technical and psycho-
cognitive aspects  
? Identify protocols for new technologies 
and applications  
? Identification of language resources 
relevant for evaluation (to promote the 
development of new linguistic resources 
for those languages and domains where 
they do not exist yet, or only exist in a 
prototype stage, or exist but cannot be 
made available to the interested users); 
 
The object of the CESTA campaign is twofold. 
It is on the one hand to provide an evaluation of 
commercial Machine Translation Systems and on 
the other hand, to work collectively on the setting 
of a new reusable Machine Translation Evaluation 
protocol that is both user oriented and accounts for 
the necessity to use semantic metrics in order to 
make available a high quality reusable machine 
translation protocol to system providers.  
 
1.2 Object of the campaign 
The object of the CESTA campaign is to 
evaluate technologies together with metrics, i.e. to 
contribute to the setting of a state of the art within 
the field of Machine Translation systems 
evaluation.  
1.3 CESTA user oriented protocol 
The campaign will last three years, starting 
from January 2003. A board of European 
experts are members of CESTA Scientific 
committee and have been working together in 
order to determine the protocol to use for the 
campaign. Six systems are being evaluated. 
Five of these systems are commercial MT 
systems and one is a prototype developed at 
the university of Montreal by the RALI 
research centre. Evaluation is carried out on 
text rather than sentences. Text approximate 
width will be 400 words. Two runs will be 
carried out. For industrial reasons, systems 
will be made anonymous. 
 
2 State-of-the-art in the field of Machine 
Translation evaluation 
 
In 1966, the ALPAC report draws light on the 
limits of Machine Translation systems. In 1979, 
the Van Slype report presented a study dedicated to 
Machine Translation metrics.  
 
In 1992, the JEIDA campaign puts the user at the 
center of evaluator?s preoccupation. JEIDA 
proposed to draw human measures on the basis of 
three questionnaires: 
? One destined to users (containing a 
hundred questions) 
? Other questionnaires are destined to 
system Machine translation systems 
editors (three different questionnaires),  
? And a set of other questionnaires reserved 
to Machine Translation systems 
developers.  
 
Scores are worked out on the background of 
fourteen categories of questions. From these 
scores, graphs are produced according to the 
answers obtained. A comparison of different 
graphs for each systems is used as a basis for 
systems classification. 
 
The first DARPA Machine Translation 
evaluation campaign (1992-1994) makes use of 
human judgments. It is a very expensive method 
but interesting however, as regards the reliability 
of the evaluation thus produced. This campaign is 
based on tests carried out from French, Spanish 
and Japanese as source languages and English as a 
target language. The measures used for each of the 
following criteria are:  
? Fidelity ? a proximity distance is worked 
out between a source sentence and a target 
sentence on a 1 to 5 scale. 
? Intelligibility, that corresponds to 
linguistic acceptability of a translation is 
measured on a 1 to 5 evaluation scale. 
? Informativeness: the test is carried out on 
reading of the target text alone. A 
questionnaire on text informative content 
is displayed allowing to work out a 
measure calculated on the basis of the 
percentage of good answers provided in 
system translation.  
 
In 1995, the OVUM report proposes to compare 
commercial Machine Translation systems on the 
basis of ten criteria. 
 
In 1996, the EAGLES report (EAGLES, 1999) 
sets new standards for Natural Language 
Processing software evaluation on the background 
of ISO 9126.  
 
Initiated in 1999, and coordinated by Pr Antonio 
Zampolli, the ISLE project is divided into three 
working groups, one being a Machine Translation 
group.  
 
Starting from ISO 9126 standard (King, 1999b), 
the aim of the project is to produce two taxonomies 
(c.f. section 3 of this article) and : 
? One defining quality subcriteria with the 
aim of refining the six criteria defined by 
ISO 9126 (i.e. functionality, reliability, 
user-friendliness, efficiency, maintenance 
portability) 
? The second one specifying use contexts 
that define the type of task induced the use 
of a by Machine Translation system, the 
types of users and input data. This 
taxonomy uses contextual parameters to 
select and order the quality criteria subject 
to evaluation. This taxonomy can be 
viewed and downloaded on the ISSCO 
website at the following address : 
http://www.issco.unige.ch/projects/isle/fe
mti/  
 
The second DARPA campaign (Papineni, K., S. 
Roukos, T. Ward and Z. Wei-Jing, 2001), making 
use of the IBM BLEU metric is mentioned in the 
CESTA protocol (c.f. section 8.1 of this article). 
 
3 User-oriented evaluations 
An emerging evaluation methodology in NLP 
technology focuses on quality requirements 
analysis. The needs and consequently the 
satisfaction of end-users, and this will depend on 
the tasks and expected results requirement 
domains, which we have identified as diagnostic 
quality dimensions. One of the most suitable 
methods in this type of evaluation is the adequacy 
evaluation that aims at finding out whether a 
system or product is adequate to someone?s needs 
(see Sparck-Jones & Gallier, 1996 and King, 1996 
among many others for a more detailed discussion 
of these issues). This approach encourages 
communication between users and developers. 
 
The definition of the CESTA evaluation 
protocol took into account the Framework for 
MT Evaluation in ISLE (FEMTI), available 
online. FEMTI offers the possibility to define 
evaluation requirements, then to select relevant 
'qualities', and the metrics commonly used to 
score them (cf. ISO/IEC 9126, 14598). The 
CESTA evaluation methodology is founded on 
a black box approach.  
 
CESTA evaluators considered a generic user, 
which is interested in general-purpose, ready-
to-use translations, preferably using an off-the-
shelf system. In addition, CESTA aims at 
producing reusable resources, and providing 
information about the reliability of the metrics 
(validation), while being cost-effective and 
fast.  
 
With these evaluation requirements in mind 
(FEMTI-1), it appears that the relevant 
qualities (FEMTI-2) are 'suitability', 'accuracy' 
and 'well-formedness'. Automated metrics best 
meet the CESTA needs for reusability, among 
which BLEU, X-score and D-score (chosen for 
internal reasons). Their validation requires the 
comparison of their scores with recognised 
human scores for the same qualities (e.g., 
human assessment of fidelity or fluency). 
'Efficiency', measured through post-editing 
time, was also discussed. For the evaluation, 
first a general-purpose dictionary could be 
used, then a domain-specific one. 
 
 
3.1 An approach based on use cases 
 
ISO 14598 directives for evaluators put forth as 
a prequisite for systems development the detailed 
identification of user needs that ought to be 
specified through the use case document. 
Moreover, conducting a full evaluation process 
involves going through the establishment of an 
evaluation requirements document. ISO 14598 
document specifies that quality requirements 
should be identified ?according to user needs, 
application area and experience, software integrity 
and experience, regulations, law, required 
standards, etc.?. 
 
The evaluation specification document is created 
using the Software Requirement Specifications 
(SRS) and the Use-Case document. The CESTA 
protocol relies on a use case that refers to a 
translation need grounded on basic syntactic 
correctness and simple understanding of a text, as 
required by information watch tasks for example, 
and excludes making a direct use of the text for 
post editing purposes.  
4 Two campaigns 
4.1 Specificities of the CESTA campaign 
Two campaigns are being organised : 
The first campaign is organised using a system?s 
default dictionary. After systems terminological 
adaptation a second campaign will be organised. 
Two studies previously carried out and presented 
respectively at the 2001 MT Summit (Mustafa El 
Hadi, Dabbadie, Timimi, 2001) and at the 2002 
LREC conference (Mustafa Mustafa El Hadi, 
Dabbadie, Timimi, 2002) allowed us to realise the 
gap in terms in terms of quality between results 
obtained on target text after terminological 
enrichment.  
 
4.2 First campaign 
The organisation of the campaign implies going 
through several steps : 
? Identification of potential participants 
? Original protocol readjustement, 
? The setting of a specific test tool that is 
currently being be implemented in 
conformity with protocol specifications 
validated by CESTA scientific 
committee. CESTA protocol 
specifications have been 
communicated to participants in 
particular as regards data formatting, 
test schedule, metrics and adaptation 
phase. For cost requirements, CESTA 
will not include a training phase. The 
first run will start during autumn 2004 
 
4.3 Second campaign 
The systems having already been tuned, an 
adaptation phase will not be carried out for the 
second campaign. However terminological 
adaptation will be necessary at this stage. The 
second series of tests being carried out on a 
thematically homogeneous corpus, the thematic 
domain only will be communicated to participants 
for terminological adaptation. For thematic  
adaptation, and in order to avoid system 
optimisation after the first series of tests, a new 
domain specific 200.000 word hiding corpus will 
be used.  
 
The terminological domain on which evaluation 
will be carried out will then have to be defined. 
This terminological domain will be communicated 
to participants but not the corpus used itself. On 
the other hand, participants will be asked to send 
organisers a written agreement by which they will 
commit themselves to provide organisers with any 
relevant information regarding system tuning and 
specific adaptations that have made on each of the 
participating MT systems, in order to allow the 
scientific committee to understand and analyse the 
origin of the potential system ranking changes. The 
second run will start during year 2005. 
 
Organisers have committed themselves not to 
publish the results between the two campaigns. 
 
After the training phase, the second campaign 
will take place. Participants will be given a fifteen 
days delay to send the results. An additional three 
months period will be necessary to carry out result 
analysis and prepare data publication and 
workshop organisation.  
 
CESTA scientific committee also decided in 
parallel with the two campaigns, to evaluate 
systems capacity to process formatted texts 
including images and HTML tags. Participants 
who do not wish to participate to this additional 
test have informed the scientific committee. Most 
of the time the reason is that their system is only 
capable of processing raw text. This is the case 
mainly for academic systems involved in the 
campaign, most of the commercial systems being 
nowadays able to process formatted text. 
 
5 Contrastive evaluation 
One of the particularities of the CESTA protocol 
is to provide a Meta evaluation of the automated 
metrics used for the campaign ? a kind of state of 
the art of evaluation metrics. The robustness of the 
metrics will be tested on minor language pairs 
through a contrastive evaluation against human 
judgement.  
 
The scientific committee has decided to use 
Arabic?French as a minor language pair. 
Evaluation on the minor language pair will be 
carried directly on two of the participating systems 
and using English as a pivotal language on the 
other systems. Translation through a pivotal 
language will then be the following : 
Arabic?English?French.  
 
Organiser are, of course, perfectly aware of the 
potential loss of quality provoked by the use of a 
pivotal language but recall however that, contrarily 
to the major language pair, evaluation carried out 
on the minor language pair through a pivotal 
system will not be used to evaluate these systems 
themselves, but metric robustness. Results of 
metric evaluation and systems evaluation will, of 
course, be obtained and disseminated separately. 
 
During the tests of the first campaign, the 
French?English system obtaining the best ranking 
will be selected to be used as a pivotal system for 
metrics robustness Meta evaluation.  
 
6 Test material 
The required material is a set of corpora as 
detailed in the following section and a test tool that 
will be implemented according to metrics 
requirements and under the responsibility of 
CESTA organisers. 
6.1 Corpus 
 
The evaluation corpus is composed of 50 texts, 
each text length is 400 words to be translated 
twice, considering that a translation already exists 
in the original corpus. The different corpora are 
provided by ELRA. The masking corpus has 
250.000 words and must be thematically 
homogeneous.  
 
 
For each language pair the following corpora 
will be used: 
 
Adaptation 
? This 200.000 ? 250.000 word corpus is a 
bilingual corpus. It is used to validate 
exchanges between organisers and 
participants and for system tuning.  
First Campaign 
? One 20.000 word evaluation corpus will be 
used (50 texts of 400 words each) 
? One 200.000 to 250.000 word masking 
corpus that hides the evaluation corpus. 
Second campaign 
? One new 20.000 word corpus will be used 
but it will have to be thematically 
homogeneous (on a specific domain that 
will be communicated to participants a few 
months before the run takes place) 
? One masking corpus similar to the 
previous one. 
 
Additional requirement 
The BLANC metric requires the use of a 
bilingual aligned corpus at document scale. 
 
Three human translations will be used for each 
of the evaluation source texts. Considering that the 
corpora used, already provide one official 
translations, only two additional human 
translations will be necessary. These translations 
will be carried out under the organisers 
responsibility. Within the framework of CESTA 
use cases, evaluation is not made in order to obtain 
a ready to publish target language translation, but 
rather to provide a foreign user a simple access to 
information within the limits of basic grammatical 
correctness, as already mentioned in this article.  
 
7 The BLEU, BLANC and ROUGE metrics 
 
Three types of metrics will be tested on the 
corpus, the CESTA protocol being the combination 
of a contrastive reference to three different 
protocols:  
 
7.1 The IBM ?BLEU? protocol (Papineni, K., 
S. Roukos, T. Ward and Z. Wei-Jing, 
2001). 
 
The IBM BLEU metric used by the DARPA for 
its 2001 evaluation campaign, uses co-occurrence 
measures based on N-Grams. The translation in 
English of 80 Chinese source documents by six 
different commercial Machine Translation 
systems, was submitted to evaluation. From a 
reference corpus of translations made by experts, 
this metric works out quality measures according 
to a distance calculated between an automatically 
produced translation and the reference translation 
corpus based on shared N-grams (n=1,2,3?). The 
results of this evaluation are then compared to 
human judgments. 
? NIST now offers an online evaluation of 
MT systems performance, i.e.:  
o A program that can be 
downloaded for research aims. 
The user then provides source 
texts and reference translations for 
a determined pair of languages. 
o An e-mail evaluation service, for 
more formal evaluations. Results 
can be obtained in a few minutes. 
 
7.2 The ?BLANC? protocol 
 
It is a metric derived from a study presented at 
the LREC 2002 conference (Hartley A., Rajman 
M., 2002). We only take into account a part of the 
protocol described in the referred paper, i.e. the X 
score, that corresponds to grammatical correctness.  
 
We will not give an exhaustive description of 
this experience and shall only detail the elements 
that are relevant to the CESTA evaluation protocol.  
 
The protocol has been tested on the following 
languages.  
? Source language: French 
? Target language: English 
? Source corpus : 100 texts ? domain : 
newspaper articles 
 
Human judgements for comparison referential: 
? 12 English monolingual students.  
? No human translation reference corpus. 
? Three criteria were tested: Fluency, 
Adequacy, Informativeness  
 
Six systems were submitted to evaluation : 
Candide (CD), Globalink (GL), MetalSystem 
(MS), Reverso (RV), Systran (SY), XS (XS) 
? Each of the systems is due to translate a 
hundred source texts ranging from 250 to 
300 words each. A corpus of 600 
translations is thus produced. 
? For each of the source texts, a corpus of 6 
translations is produced automatically. 
These translations are then regrouped by 
series of six texts.  
? According to the protocol initiated by 
(White & Forner, 2001) these series are 
then ranked by medium adequacy score. 
? Every 5 series, a series is extracted from 
the whole. Packs of twenty series of target 
translations are thus obtained and 
submitted to human evaluators.  
 
7.2.1 Evaluators? tasks 
? Each evaluator reads 10 series of 6 
translations i.e. 60 texts.  
? Each of these series is then read by six 
different evaluators 
? The evaluators must observe a ten minute 
compulsory break every two series.  
? The evaluators do not know that the texts 
have been translated automatically. 
 
The directive given to them is the following: 
? rank these six texts from best to worst. If 
you cannot manage to give a different ranking 
to two texts, regroup them under the same 
parenthesis and give them the same score, as in 
the following example : 4 [1 2] 6 [3 5].? 
The aim of this instruction is to produce 
rankings that are similar to the rankings attributed 
automatically.  
Human judgement that ranks from best to worse 
corresponds in reality to a set of the fluency, 
adequacy and Informativeness criteria that can be 
attributed to the texts translated automatically.  
 
7.2.2 Automatically generated scores 
? X-score : syntactic score 
? D-score : semantic score 
 
Within the framework of the CESTA 
evaluation campaign the scientific committee 
decided to make use of the X-score only, the 
semantic D-score having proved to be unstable 
and that it could be advantageously replaced 
by the a metric based on (Bogdan, B.; Hartley, 
A.; Atwell, 2003), a reformulation of the D-
score developed by (Rajman, M. and T. 
Hartley, 2001), and which we refer to as the 
ROUGE metric in this article. 
 
7.2.3 X-score: definition 
? This score corresponds to a grammaticality 
metric 
? Each of the texts is previously parsed with 
XELDA Xerox parser. 
? 22 types of syntactic dependencies 
identified through the corpus of automatic 
translations. 
? The syntactic profile of each source 
document is computed. This profile is then 
used to derive the X-score for each 
document, making use of the following 
formula: 
? X-score = (#RELSUBJ+#RELSUBJPASS-
#PADJ-#ADVADJ)  
 
 
7.3 The ?ROUGE? protocol  
 
This protocol, developed by Anthony Hartley in 
(Bogdan, B.; Hartley, A.; Atwell, 2003), is a 
semantic score. It is the result of a reformulation of 
the D-Score, the semantic score initiated through 
previous collaboration with Martin Rajman 
(Rajman, M. and T. Hartley, 2001), as explained in 
the previous section.  
 
The original idea on which this protocol is based 
relies on the fact that MT evaluation metrics that 
?are based on comparing the distribution of 
statistically significant words in corpora of MT 
output and in human reference translation 
corpora?.  
 
The method used to measure MT quality is the 
following:  a statistical model for MT output 
corpora and for a parallel corpus of human 
translations, each statistically significant word 
being highlighted in the corpus. On the other hand, 
a statistical significance score is given for each 
highlighted word. Then statistical models for MT 
target texts and human translations are compared, 
special attention being paid to words that are 
automatically marked as significant in MT outputs, 
whereas they do not appear to be marked as 
significant in human translations. These words are 
considered to be ?over generated?. The same 
operation is then carried out on ?under generated 
words?. At this stage, a third operation consists in 
the marking of the words equally marked as 
significant by the MT systems and the human 
translations. The overall difference is then 
calculated for each pair of texts in the corpora. 
Three measures specifying differences in statistical 
models for MT and human translations are then 
implemented : the first one aiming at avoiding 
?over generation?, the second one aiming at 
avoiding ?under generation? and the last one being 
a combination of these two measures. The average 
scores for each of the MT systems are then 
computed.  
 
As detailed in (Bogdan, B.; Hartley, A.; Atwell, 
2003): 
 
?1. The score of statistical significance is 
computed for each word (with absolute frequency 
? 2 in the particular text) for each text in the 
corpus, as follows: ( )
][
][][][
][ ln
corpallword
foundnottxtswordcorprestwordtextword
textword P
NPP
S
?
??? ??=
 
where: 
Sword[text] is the score of statistical significance for 
a particular word in a particular text 
Pword[text] is the relative frequency of the word in 
the text; 
Pword[rest-corp] is the relative frequency of the same 
word in the rest of the corpus, without this text; 
Nword[txt-not-found] is the proportion of texts in the 
corpus, where this word is not found (number of 
texts, where it is not found divided by number of 
texts in the corpus) 
Pword[all-corp] is the relative frequency of the word 
in the whole corpus, including this particular text 
 
2. In the second stage, the lists of statistically 
significant words for corresponding texts together 
with their Sword[text] scores are compared across 
different MT systems. Comparison is done in the 
following way: 
For all words which are present in lists of 
statistically significant words both in the human 
reference translation and in the MT output, we 
compute the sum of changes of their Sword[text] 
scores: ( )? ?= ].[].[. MTtextwordreferencetextworddifftext SSS  
The score Stext.diff is added to the scores of all 
"over-generated" words (words that do not appear 
in the list of statistically significant words for 
human reference translation, but are present in 
such list for MT output). The resulting score 
becomes the general "over-generation" score for 
this particular text: 
? ?? +=
textwords
textgeneratedoverworddifftexttextgenerationover SSS
.
][...
 
The opposite "under-generation" score for 
each text in the corpus is computed by adding 
Stext.dif and all Sword[text]  scores of "under-generated" 
words ? words present in the human reference 
translation, but absent from the MT output. 
?+=?
textwords
textatedundergenerworddifftexttextgenerationunder SSS
.
][...
 
It is more convenient to use inverted scores, 
which increases as the MT system improves. These 
scores, So.text and Su.text, could be interpreted as 
scores for ability to avoid "over-generation" and 
"under-generation" of statistically significant 
words. The combined (o&u) score is computed 
similarly to the F-measure, where Precision and 
Recall are equally important: 
textgenerationover
texto S
S
.
.
1
?
= ; 
 
textgenerationunder
textu S
S
.
.
1
?
= ;
 
textutexto
textutexto
textuo SS
SSS
..
..
.&
2
+=  
The number of statistically significant words 
could be different in each text, so in order to make 
the scores compatible across texts we compute the 
average over-generation and under-generation 
scores per each statistically significant word in a 
given text. For the otext score we divide So.text by the 
number of statistically significant words in the MT 
text, for the utext score we divide Su.text by the 
number of statistically significant words in the 
human (reference) translation: 
rdsInMTstatSignWo
texto
text n
So .= ;
 
rdsInHTstatSignWo
textu
text n
Su .= ; 
 
texttext
texttext
text uo
uoou +=
2&  
The general performance of an MT system for IE 
tasks could be characterised by the average o-
score, u-score and u&o-score for all texts in the 
corpus?. 
8 Time Schedule and result dissemination 
 
The CESTA evaluation campaign started in 
January 2003 after having been labeled by the 
French Ministry of Research. During year 2003 
CESTA scientific committee went through 
protocol detailed redefinition and specification and 
a time schedule was agreed upon.  
 
2004 first semester is being dedicated to corpus 
untagging and the programming of CESTA 
evaluation tool. Reference human translations will 
also have to be produced and the implemented 
evaluation tool submitted to trial and validation.  
 
After this preliminary work, the first run will 
start during autumn 2004. At the end of the first 
campaign, result analysis will be carried out. A 
workshop will then be organized for CESTA 
participants. Then the second campaign will take 
place at the end of Spring 2005, the terminological 
adaptation phase being scheduled on a five month 
scale. 
 
After carrying out result analysis and final report 
redaction, a public workshop will be organized and 
the results disseminated and subject to publication 
at the end of 2005.  
 
9 Conclusion 
 
CESTA is the first European Campaign 
dedicated to MT Evaluation. The results of the 
campaign will be published in a final report and be 
the object of an intermediary workshop between 
the two campaigns and a final workshop at the end 
of the campaign.  
 
It is a noticeable point that the CESTA campaign 
aims at providing a state of the art of automated 
metrics in order to ensure protocol reusability. The 
originality of the CESTA protocol lies in the 
combination and contrastive use of three different 
types of measures carried out in parallel with a 
Meta evaluation of the metrics. 
 
It is also important to note that CESTA aims at 
providing a black box evaluation of available 
Machine Translation technologies, rather than a 
comparison of systems and interfaces, that can be 
tuned to match a particular need. If systems had to 
be compared, the fact that these applications 
should be compared including all software lawyers 
and ergonomic properties, ought to be taken into 
consideration.  
 
Moreover apart from providing a state of the art 
through a Meta evaluation of the metrics used in its 
protocol, thanks to the setting of this original 
protocol that relies on the contrastive use of 
complementary metrics, CESTA aims at protocol 
reusability. One of the outputs of the campaign 
will be the creation of a Machine Translation 
evaluation toolkit that will be put at users and 
system developers? disposal.Acknowledgements 
References  
Besan?on, R. and Rajman, M., (2002). Evaluation 
of aVector Space similarity measure in a 
multilingual framework. Procs. 3rd 
International Conference on Language 
Resources and Evaluation, Las 
Palmas,Spain,.1252 
Bogdan, B.; Hartley, A.; Atwell E.; Statistical 
modelling of MT output corpora for 
Information Extraction Proceedings Corpus 
Linguistics 2003, Lancaster, UK, 28-31 
March 2003, pp. 62-70 
Chaudiron, S. Technolangue. In: 
http://www.apil.asso.fr/metil.htm, mars 2001 
Chaudiron, S. L??valuation des syst?mes de 
traitement de l?information textuelle : vers un 
changement de paradigmes, M?moire pour 
l?habilitation ? diriger des recherches en sciences 
de l?information, pr?sent? devant l?Universit? de 
Paris 10, Paris, novembre 2001 
Dabbadie, M, Mustafa El Hadi, W., Timimi, I. 
(2001). Setting a Methodology for Machine 
Translation Evaluation. In: Machine 
Translation Summit VIII, ISLE/EMTA, 
Santiago de Compestela, Spain, 18-23 
October 2001, pp. 49-54. 
Dabbadie, M., Mustafa El Hadi, W., Timimi, I., 
(2002). Terminological Enrichment for non-
Interactive MT Evaluation. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 6 ? 1878-
1884 
EAGLES-Evaluation-Workgroup. (1996). 
EAGLES evaluation of natural language 
processing systems. Final report, Center for 
Sprogteknologi, Denmark, October 1996. 
EAGLES (1999). EAGLES Reports (Expert 
Advisory Group on Language Engineering 
Standards)http://www.issco.unige.ch/project
s/eagles/ewg99. 
ISLE (2001). MT Evaluation Classification, 
Expanded Classification. 
http://www.isi.edu/natural-
language/mteval/2b-MT-classification.htm. 
ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) ? 
Information Technology ? Software 
Product Evaluation ? Quality 
Characteristics and Guidelines for Their Use. 
ISO/IEC, Geneva. 
ISO (1999). Standard ISO/IEC 9126-1 Information 
Technology ? Software Engineering ? 
Quality characteristics and sub-
characteristics. Software Quality 
Characteristics and Metrics - Part 1 
ISO (1999). Standard ISO/IEC 9126-2 Information 
Technology ? Software Engineering ? 
Software products Quality : External Metrics 
- Part 2 
ISO/IEC-14598. 1998-2001. ISO/IEC 14598 ? 
Information technology ? Software product 
evaluation ? Part 1: General overview 
(1999), Part 2: Planning and management 
(2000), Part 3: Process for developers 
(2000), Part 4: Process for acquirers (1999), 
Part 5: Process for evaluators (1998), Part 6: 
Documentation of evaluation modules 
(2001). ISO/IEC, Geneva. 
ISSCO (2001) Machine Translation Evaluation : 
An Invitation to Get Your Hands Dirty!, 
ISSCO, University of Geneva, Workshop 
organised by M. King (ISSCO) & F. Reed, 
(Mitre Corporation), April 19-24 2001. 
King (1999a) EAGLES Evaluation Working 
Group, report,http://www.issco.unige.ch/ 
projects/eagles. 
King, M. (1999b). ?ISO Standards as a Point of 
Departure for EAGLES Work in EELS 
Conference (European Evaluation of 
Language Systems), 12-13 April 1999. 
Mariani, Joseph. ?Language Technologies : 
Technolangue Action ?. Presentation. In: 
LREC'2002 International Strategy Panel17, Las 
Palmas, May 2002. 
Nomura, H. and J. Isahara. (1992). The JEIDA 
report on MT. In Workshop on MT 
Evaluation: Basis for Future Directions, San 
Diego, CA. Association for Machine 
Translation in the Americas (AMTA). 
Popescu-Belis, A. S. Manzi, and M. King. (2001). 
Towards a two-stage taxonomy for MT 
evaluation. In Workshop on MT Evaluation 
?Who did what to whom?? at Mt Summit 
VIII, pages 1?8, Santiago de Compostela, 
Spain. 
Rajman, M. and T. Hartley, (2001). Automatically 
predicting MT systems rankings compatible 
with Fluency, Adequacy or Informativeness 
scores. Procs. 4th ISLE Workshop on MT 
Evaluation, MT Summit VIII, 29-34. 
Rajman, M. and T. Hartley, (2002). Automatic 
ranking of MT systems. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 4 ? 1247-
1253 
Reeder, F., K. Miller, J. Doyon, and J. White, J. 
(2001). The naming of things and the 
confusion of tongues: an MT metric. Procs. 
4th ISLE Workshop on MT Evaluation, MT 
Summit VIII, 55-59. 
Sparck-Jones K., Gallier, J.R. (1996). Evaluating 
Natural Language Processing Systems: An 
Analysis and Review, Springer, Berlin. 
TREC, NIST Website, last updated, August 1st, 
2000, visited by the authors, 23-03-2003  
Vanni, M. and K. Miller (2001). Scaling the ISLE 
framework: validating tests of machine 
translation quality for multi-dimensional 
measurement. Procs. 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII, 21-27. 
VanSlype., G. (1979). Critical study of methods 
for evaluating the quality of MT. Technical 
Report BR 19142, European Commission / 
Directorate for General Scientific and 
Technical Information Management (DG 
XIII). 
V?ronis, J., Langlais, Ph. (2000). ARCADE: 
?valuation de syst?mes d'alignement de textes 
multilingues. In Chibout, K., Mariani, J., 
Masson, N., Neel, F. ?ds., (2000). Ressources et 
?valuation en ing?nierie de la langue, Duculot, 
Coll. Champs linguistiques, et Collection 
Universit?s Francophones (AUF). 
Adaptive Language and Translation Models
for Interactive Machine Translation
Laurent Nepveu, Guy Lapalme
Philippe Langlais
RALI/DIRO - Universite? de Montre?al,
C.P. 6128, succursale Centre-ville
Montre?al, Que?bec, Canada H3C 3J7
{nepveul,lapalme,felipe}
@iro.umontreal.ca
George Foster
Language Technologies Research Centre
National Research Council Canada
A-1330, 101 rue Saint-Jean Bosco,
Gatineau, Que?bec, Canada K1A 0R6
George.Foster@nrc-cnrc.gc.ca
Abstract
We describe experiments carried out with adaptive
language and translation models in the context of an
interactive computer-assisted translation program.
We developed cache-based language models which
were then extended to the bilingual case for a cache-
based translation model. We present the improve-
ments we obtained in two contexts: in a theoretical
setting, we achieved a drop in perplexity for the new
models and, in a more practical situation simulat-
ing a user working with the system, we showed that
fewer keystrokes would be needed to enter a trans-
lation.
1 Introduction
Cache-based language models were introduced by
Kuhn and de Mori (1990) for the dynamic adap-
tation of speech language models. These models,
inspired by the memory caches on modern com-
puter architectures, are motivated by the principle
of locality which states that a program tends to re-
peatedly use memory cells that are physically close.
Similarly, when speaking or writing, humans tend
to use the same words and phrase constructs from
paragraph to paragraph and from sentence to sen-
tence. This leads us to believe that, when processing
a document, the part of a document that is already
processed (e.g. for speech recognition, translation
or text prediction) gives us very useful information
for future processing in the same document or in
other related documents.
A cache-based language model is a language
model to which is added a smaller model trained
only on the history of the document being pro-
cessed. The history is usually the last N words or
sentences seen in the document.
Kuhn and de Mori (1990) obtained a drop in per-
plexity of nearly 68% when adding an unigram POS
(part-of-speech) cache on a 3g-gram model. Martin
and al. (1997) obtained a drop of nearly 21% when
adding a bigram cache to a trigram model. Clarkson
and Robertson (1997) also obtained similar results
with an exponentially decaying unigram cache.
The major problem with these theoretical results
is that they assume the correctness of the material
entering the cache. In practice, this assumption does
not always hold, and so a cache can sometimes do
more harm than good.
1.1 Interactive translation context
Over the last few years, an interactive machine
translation (IMT) system (Foster et al, 2002) has
been developed which, as the translator is typing,
suggests word and phrase completions that the user
can accept or ignore. The system uses a transla-
tion engine to propose the words or phrases which
it judges the most probable to be immediately typed.
This engine includes a translation model (TM) and
a language model (LM) used jointly to produce pro-
posals that are appropriate translations of source
words and plausible completions of the current text
in the target language. The translator remains in
control of the translation because what is typed by
the user is taken as a constraint to which the model
must continually adapt its completions. Experi-
ments have shown that the use of this system can
save about 50% of the keystrokes needed for enter-
ing a translation. As the translation and language
models are built only once, before the user starts to
work with the system, the translator is often forced
to repeatedly correct similar suggestions from the
system.
The interactive nature of this setup made us be-
lieve that it is a good prospect for dynamic adaptive
modeling. If the dynamic nature of the system can
be disadvantageous for static language and transla-
tion models, it is an incomparable advantage for a
cache based approach because human correction in-
tervenes before words go in the cache. As the trans-
lator is using the system to correctly enter his trans-
lation progressively, we can expect the theoretical
results presented in the literature to be obtainable in
practice in the IMT context.
The first advantage of dynamic adaptation would
be to help the translation engine make better predic-
tions, but it has a further psychological advantage:
as the translator works and potentially corrects the
proposals of the engine, the user would feel that the
software is learning from its errors.
The next section describes the models currently
embedded within our IMT prototype. Section 3 de-
scribes the cache-based adaptation we performed on
the target language model. In section 4, we present
the different types of adaptations we performed on
the translation model. Section 5 then puts the results
in the context of our IMT application. Section 6 dis-
cusses the implications of our experiments and sug-
gests some improvements that could be made to the
system.
2 Current IMT models
The word-based translation model embedded within
the IMT system has been designed by Foster (2000).
It is a Maximum Entropy/Minimum Divergence
(MEMD) translation model (Berger et al, 1996),
which mimics the parameters of the IBM model 2
(Brown et al, 1993) within a log-linear setting.
The resulting model (named MDI2B) is of the
following form, where h is the current target text,
s the source sentence being translated, s a particular
word in s and w the next word to be predicted:
p(w|h, s) =
q(w|h) exp(
?
s?s ?sw + ?AB)
Z(h, s)
(1)
The q distribution represents the prior knowledge
that we have about the true distribution and is mod-
eled by an interpolated trigram in this study. The
? coefficients are the familiar transfer or lexical pa-
rameters, and the ? ones can be understood as their
position dependent correction. Z is a normalizing
factor, the sum of the numerator for every w in the
target vocabulary.
Our baseline model used an interpolated trigram
of the following form as the q distribution:
p(w|h) = ?1(wi?2wi?1) ? ptri(wi|wi?2wi?1)
+ ?2(wi?2wi?1) ? pbi(wi|wi?1)
+ ?3(wi?2wi?1) ? puni(wi)
+ ?4(wi?2wi?1) ? 1|V |+1
where ?1(wi?2wi?1) + ?2(wi?2wi?1) +
?3(wi?2wi?1) + ?4(wi?2wi?1) = 1 and |V | + 1
is the size of the event space (including a special
unknown word).
As mentioned above, the MDI2B model is closely
related to the IBM2 model (Brown et al, 1988). It
contains two classes of features: word pair features
and positional features. The word pair feature func-
tions are defined as follows:
fst(w,h, s) =
{
1 if s ? s and t = w
0 otherwise
This function is on if the predicted word is t and s
is in the current source sentence. Each feature fst
has a corresponding weight ?st (for brevity, this is
defined to be 0 in equation 1 if the pair s, t is not
included in the model).
The positional feature functions are defined as
follows:
fA,B(w, i, s) =
J?
j=1
?[(i, j, J) ? A ? (sj , w) ? B ? j = ??sj ]
where ?[X] is 1 if X is true, otherwise 0; and ??sj
is the position of the occurrence of sj that is clos-
est to i according to an IBM2 model. A is a class
that groups positional (i, j, J) configurations having
similar IBM2 alignment probabilities, in order to re-
duce data sparseness. B is a class of word pairs
having similar weights ?st. Its purpose is to simu-
late the way IBM2 alignment probabilities modulate
IBM1 word-pair probabilities, by allowing the value
of the positional feature weight to depend on the
magnitude of the corresponding word-pair weight.
As with the word pair features, each fA,B has a cor-
responding weight ?AB .
Since feature selection is applied at training time
in order to improve speed, avoid overfitting, and
keep the model compact, the summation in the ex-
ponential term in (1) is only carried out over the set
of active pairs maintained by the model and not over
all pairs as might be inferred from the formulation.
To give an example of how the model works, if
the source sentence is the fruit I am eating is a ba-
nana and we are predicting the word banane follow-
ing the target words: Le fruit que je mange est une,
the active pairs involving banana would be (fruit,
banana) and (banane, banana) since, of all the pairs
(s, t) they would be the only ones kept by the fea-
ture selection algorithm1. The probability of banane
would therefore depend on the weights of those two
pairs, along with position weights which capture the
relative proximity of the words involved.
3 Language model adaptation
We implemented a first monolingual dynamic adap-
tation of this model by inserting a cache compo-
nent in its reference distribution, thus only affect-
ing the q distribution. We obtained similar results
1See (Foster, 2000) for the description of this algorithm.
as for classical ngram models: the unigram cache
model proved to be less efficient than the bigram
one, and the trigram cache suffered from sparsity.
We also tested a model where we interpolated the
three cache models to gain information from each
of the unigram, bigram, and trigram cache mod-
els. For completeness, this generalized model is de-
scribed in equation 2 under the usual constraints that?
i ?i(h) = 1 for all h.
p(w|h) = ?1(h) ? ptri(wi|wi?2wi?1)
+ ?2(h) ? pbi(wi|wi?1)
+ ?3(h) ? puni(wi)
+ ?4(h) ? 1|V |+1
+ ?5(h) ? ptric(wi|wi?2wi?1)
+ ?6(h) ? pbic(wi|wi?1)
+ ?7(h) ? punic(wi)
(2)
Those models were trained from splits of the
Canadian Hansard corpus. The base ngram model
was estimated with a 30M word split of the corpus.
The weighting coefficients of both the base trigram
and the cache models were estimated with an EM
algorithm trained with 1M words.
We tested our models, translating from English
to French, on two corpora of different types: the
first one hansard is a document taken from the
same large corpus that was used for training (the
testing and training corpora were exclusive splits).
The second one sniper, which describes the job
of a sniper, is from another domain characterized
by lexical and phrasal constructions very different
from those used to estimate the probabilities of our
models.
Table 1 shows the perplexity on the hansard
and the sniper corpora. Preliminary experiments
led us to two sizes of cache which seemed promis-
ing: 2000 and 5000 corresponding to the last 2000
and 5000 words seen during the processing of a doc-
ument. The BI column gives the results of the bi-
gram cache model and the 1+2+3 gives the results
of the interpolated cache model which included the
unigram, bigram and trigram cache.
The results show that our models improve the
base static model by 5% on documents supposedly
well known by the models and by more that 52%
on documents that are unknown to the model. Sec-
tion 5 puts these results in the perspective of our
actual IMT system. Note that he addition of a cache
component to a language model involves negligible
extra training time.
Taille BI ? 1+2+3 ?
base hansard=17.6584
2000 16.937 -4.1% 16.840 -4.6%
5000 16.903 -4.3% 16.777 -5.0%
base sniper=135.808
2000 73.936 -45.6% 67.780 -50.1%
5000 70.514 -48.1% 64.204 -52.7%
Table 1: Perplexities of the MDI2B model with a
cache component included in the reference distribu-
tion on the hansard and sniper corpora.
4 Translation model adaptation
With those excellent results in mind, we extended
the idea of dynamic adaptation to the bilingual case
which, to our knowledge, has never been tried be-
fore.
We developed a model called MDI2BCache
which is a MDI2B model to which we added a cache
component based on word pairs. Recall that, when
predicting a word w at a certain point in a document,
the probability depends on the weights of the pairs
(s, w) for each active word s in the current source
sentence. As the prediction of the words of the doc-
ument goes on, our model keeps in a cache each
active pair used for the prediction of each word. In
the example above, if the translator accepts the word
banane, then the two pairs (fruit, banana) and (ba-
nane, banana) will be added to the cache.
We added a new feature to the MEMD model to
take into account the presence of a certain pair in
the recent history of the processed document:
fcache st(w,h, s) =
?
????
????
1 if
?
??
??
s ? s,
t = w,
(s, t) ? cache
?st > p
0 otherwise
We added a threshold value p to the feature func-
tion because while analyzing the pair weights, we
discovered that low weight pairs are usually pairs of
utility words such as conjunctions and punctuation.
We also came to the conclusion that they are not the
kind of words we want to have in the cache, since
their presence in a sentence implies little about their
presence in the next.
The resulting model is of the form:
p(w|h, s) =
q(w|h)exp(
?
s?s ?sw + ?AB + ?sw)
Z(h, s)
Thus, every fcache sw has a corresponding weight
?sw for the calculation of the probability of w.
Size 0.3 ? 0.5 ? 0.7 ?
base One feature weight, no Viterbi orig perp=17.6584
1000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34%
2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34%
5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35%
10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35%
base One feature weight per pair, no Viterbi orig perp=17.6584
1000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29%
2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30%
5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29%
10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29%
base One feature weight, Viterbi orig perp=17.6584
1000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36%
2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39%
5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37%
10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38%
Table 2: MDI2BCache test perplexities. One feature weight, Viterbi alignment version.
4.1 Number of cache features
We implemented two versions of the model, one in
which we estimated only one cache feature weight
for the whole model and another in which we esti-
mated one cache feature weight for every word pair
in the model.
The first model is simpler and is easier to esti-
mate. The assumption is made that every pair in the
model has the same tendency to repeat itself.
The second model doubles the number of word-
pair parameters compared to MDI2B, and thus leads
to a linear increase in training time. Extra training
time is negligible in the first model.
4.2 Word alignment
One of the main difficulties of automatic MT is de-
termining which source word(s) translate to which
target word(s). It is very difficult to do this task
automatically, in part because it is also very diffi-
cult manually. If a pair of sentences are given to
10 translators for alignment, the results would likely
not be identical in all cases. As it is nearly impossi-
ble to determine such an alignment, most translation
models consider every source word to have an effect
on the translation of every target word.
This difficulty shows up in our cache-based
model. When adding word pairs to the cache, we
ideally would like to add only word pairs that were
really in a translation relation in the given sentence.
This is why we also implemented a version of our
model in which a word alignment is first carried out
in order to select good pairs to be added to the cache.
For this purpose, we computed a Viterbi alignment
based on an IBM model 2. This results in a subset of
the good active pairs to be added to the cache. The
Viterbi algorithm gives us a higher confidence level
that the pair of words added to the cache were really
in a translation relation. But it can also lead to word
pairs not added to the cache that should have been
added.
4.3 Results
Table 2 shows the results of the different configura-
tions of the MDI2BCache model. For every config-
uration we trained and tested on splits of the Cana-
dian Hansard with threshold values of 0.3, 0.5, and
0.7 and cache sizes of 1000, 2000, 5000, and 10000.
The top of the table is the version of the model with
only one feature weight without Viterbi alignment.
The middle of the table is the version with one fea-
ture weight per word pair without Viterbi alignment.
Finally, the bottom is for the version with only one
feature weight and a Viterbi alignment made prior
to adding pairs to the cache.
Threshold values of 0.3, 0.5, and 0.7 led to 75%,
50%, and 25% of the pairs considered for addition
to the cache respectively. The results show that the
threshold values of 0.5 and 0.7 are removing too
many pairs. The best results are obtained with a
threshold of 0.3 in all tests. Since the number of
pairs kept in the model appears to vary in proportion
to the threshold value, we did not consider it neces-
sary to use an automatic search algorithm to find an
optimal threshold value. The gain in performance
would have been negligible.
The results also show that having one feature
weight per word pair leads to lower results. This
can be explained by the fact that it is much more
Size 0.3 ? 0.5 ?
base MDI2B=135.808
1000 132.865 -2.17% 132.751 -2.25%
2000 132.771 -2.23% 132.752 -2.25%
5000 132.733 -2.26% 132.628 -2.34%
10000 132.997 -2.07% 132.674 -2.31%
Table 3: MDI2BCache test perplexities. One fea-
ture weight, Viterbi alignment version. Sniper test
difficult to estimate a weight for every pair that one
weight for all pairs. Since we use only thousands of
words in the cache, the training process suffers from
a poor data representation.
The Viterbi alignment seems to be helping the
models. The best results are obtained with the ver-
sion of our model with Viterbi alignment. However,
this gives only a 0.56% percent drop in perplexity.
We then tested our best configuration on the
sniper corpus. Table 3 shows the results. We
dropped threshold value 0.7 and tested only the
model with only one feature weight and a Viterbi
alignment.
Results show that our bilingual cache model
shows improvement (four times higher) in drop of
perplexity when used on documents very different
from the training corpus. In general, results give
lower perplexity than our base model showing that
the bilingual cache is helpful to the model, but the
results are not as good as that the ones obtained in
the unilingual case. Section 6 discusses these results
further.
5 Evaluation of IMT
As stated earlier, drops in perplexity are theoreti-
cal results that have been obtained previously in the
case of unilingual dynamic adaptation but for which
a corresponding level of practical success was rarely
attained because of the cache correctness problem.
To show that the interactive nature of our assisted-
translation application can really benefit from dy-
namic adaptation, we tested our models in a more
realistic translation context. This test consists of
simulating a translator using the IMT system as it
proposes words and phrases and accepting, correct-
ing or rejecting the proposals by trying to reproduce
a given target translation (Foster et al, 2002). The
metric used is the percentage of keystrokes saved
by the use of the system instead of having to type
directly all the target text.
For these simulations, we used only a 10K word
split of the hansard and of the sniper cor-
pus. The reason is that the IMT application poten-
Taille BI ? 1+2+3 ?
base hansard=27.435
2000 27.784 +1.3% 27.719 +1.0%
5000 27.837 +1.5% 27.821 +1.4%
base sniper=9.686
2000 11.404 +15.1% 11.294 +14.2%
5000 11.498 +15.8% 11.623 +16.7%
Table 4: Saved keystrokes raises for the MDI2B
model with cache component in the reference dis-
tribution on the hansard and sniper corpora.
0.3 ?
base hansard=27.4358
1000 27.557 +0.44%
2000 27.531 +0.35%
5000 27.488 +0.18%
10000 27.468 +0.12%
base sniper=9.686
1000 9.896 +2.17%
2000 10.023 +3.48%
5000 9.983 +3.07%
10000 9.957 +2.80%
Table 5: Saved keystrokes raises for the
MDI2BCache model with only one feature
weight and Viterbi alignment on the hansard and
sniper corpora.
tially proposes new completions after every charac-
ter typed by the user. For a 10K word document, it
needs to search about 1 million times for high prob-
ability words and phrases. This leads to relatively
long simulation times, even though predictions are
made at real time speeds.
Table 4 shows the results obtained with the
MDI2B model to which we added a cache compo-
nent for the reference interpolated trigram distribu-
tion.
We can see that the saved keystroke percentages
are proportional to the perplexity drops reported in
section 3. The use of our models raises the saved
keystrokes by nearly 1.5% in the case of well known
documents and by nearly 17% in the case of very
different documents. These are very interesting re-
sults for a potential professional use of TransType.
Table 5 shows an increase in the number of saved
keystrokes: 0.44% on the hansard and 3.5% on
the sniper corpora. Once again, the results are
not as impressive as the ones obtained for the mono-
lingual dynamic adaptation case.
6 Discussion
The results presented in section 3 on language
model adaptation confirmed what had been reported
in the literature: adding a cache component to a lan-
guage model leads to a drop in perplexity. More-
over, we were able to demonstrate that using a
cache-based language model inside a translation
model leads to better performance for the whole
translation model. We obtained drops in perplexity
of 5% on a corpus of the same type as the training
corpus and of 50% on a different one. These theo-
retical results lead to very good practical results. We
were able to increase the saved keystroke percent-
age by 1.5% on the similar corpus as the training
and by nearly 17% on the different corpus. These
results confirm our hypothesis that dynamic adapta-
tion with cache-based language model can be useful
in the context of IMT, particularly for new types of
texts.
Results presented in section 4 on translation
model adaptation show that our approach has led
to drops in perplexity although not as high as we
would have hoped. To understand these disappoint-
ing results, we analyzed the content of the cache for
different configurations of our MDI2BCache model.
base 0.3 viterbi + 0.3
(is,qu?) (to,afin) (offence,crime)
(.,sa) (was,a) (was,e?te?)
(this,,) (UNK,UNK) (very,tre`s)
(all,toutes) (piece,le?gislative) (today,aujourd?hui)
(have,du) (this,ce) (jobs,emploi)
(the,pour) (per,100) (concern,inquie?tude)
(on,du) (that,soient) (skin,peau)
(of,un) (,,,) (there,y)
(we,nous) (?,il) (government,le)
(the,du) (any,tout) (an,un)
18 68 86
Table 6: Cache sampling of different configurations
of MDI2BCache model.
Table 6 shows the results of our sampling. We
tested three model configurations. The first one, in
the first column, was the base MDI2BCache model
which adds all active pairs to the cache. The second
configuration, in the second column, was a thresh-
old value of 0.3 that brings about 75% of the pairs
being added to the cache. The last configuration was
a model with threshold value of 0.3 and a Viterbi
alignment made prior to the addition of pairs in the
cache. The three model configuration were with
only one feature weight. For all three configura-
tions, we took a sample of 10 pairs (shown in table
6) and a sample of 100 pairs. With the second sam-
ple, we manually analyzed each pair and counted
the number of pairs (shown in the last row of the ta-
ble) we believed were useful for the model (words
that are occasionally translations of one another).
The results obtained in section 4 seem to agree
with the current analysis. From left to right in the ta-
ble, the pairs seem to contain more information and
to be more appropriate additions to the cache. The
configuration with Viterbi alignment which contains
86 good pairs clearly seems to be the configuration
with the most interesting pairs.
The problem with such a cache-based translation
model seem to be similar to the balance between
precision and recall in information retrieval. On one
hand, we want to add in the cache every word pair
in which the two words are in translation relation in
the text. We further want to add only the pairs in
which the two words are really in translation rela-
tion in the text. It seems that with our base model,
we add most of the good pairs, but also a lot of bad
ones. With the Viterbi alignment and a threshold
value of 0.3, most of the pairs added are good ones,
but we are probably missing a number of other ap-
propriate ones. This comes back to the task of word
alignment, which is a very difficult task for comput-
ers (Mihalcea and Pedersen, 2003).
Moreover, we would want to add in the cache
only those words for which more than one transla-
tion is possible. For example, the pair (today, au-
jourd?hui), though it is a very useful pair for the
base model, is unlikely to help when added to the
cache. The reason is simple: they are two words
that are always translations of one another, so the
model will have no problem predicting them. This
ideal of precision and recall and of useful pairs in
the cache is obtained by our model with threshold
of 0.3, a Viterbi alignment and a cache size of 1000.
One disadvantage of our bilingual adaptive model
is the way it handles unknown words. In the cache-
based language model, the unknown words were
dealt with normally, i.e. they were added to the
cache and given a certain probability afterwards.
So, if an unknown word was seen in a certain sen-
tence and then later on, it would receive a proba-
bility mass of its own but not the one given to any
unknown word. By having its own probability mass
due to its presence in the cache, such previously un-
known word can be predicted by the model. In the
case of our MDI2BCache model, because we have
not yet implemented an algorithm for guessing the
translations of unknown words, they are simply rep-
resented within the model as UNK words, which
means that the model never learns them.
The results obtained with the sniper corpus
shows us that dynamic adaptation is also more help-
ful for documents that are little known to the model
in the bilingual context. The results are four times
better on the sniper corpus than on the Hansard
testing corpus.
Once again for the bilingual case, the practical
test results in the number of saved keystrokes agree
with the theoretical results of drops in perplexity.
This result shows that bilingual dynamic adaptation
also can be implemented in a practical context and
obtain results similar to the theoretical results.
All things considered, we believe that a cache-
based translation model shows a great potential
for bilingual adaptation and that greater perplexity
drops and keystroke savings could be obtained by
either reengineering the model or by improving the
MDI2BCache model.
6.1 Key improvements to the model
Following the analysis of the results obtained by our
model, we have pointed out some key improvements
that the model would need in order to get better re-
sults. In this list we focus on ways of improving
adaptation strategies for the current model, omitting
other obvious enhancements such as adding phrase
translations.
Unknown word processing Learning new words
would be a very important feature to add to
the model and would lead to better results. We
did not incorporate the processing of unknown
words in the MDI2BCache because the struc-
ture of model did not lend itself to this addi-
tion. Especially with documents such as the
sniper corpus, we believe that this could
be a key improvement for a dynamic adaptive
model.
Better alignment As mentioned before, the ulti-
mate goal for our cache is that it contains only
the pairs present in the perfect alignment. Bet-
ter performance from the alignment would lead
to pairs in the cache closer to this ideal. In this
study we computed Viterbi alignments from an
IBM model 2, because it is very efficient to
compute and also because for training MDI2B,
we do use the IBM model 2. We could consider
also more advanced word alignment models
(Och and Ney, 2000; Lin and Cherry, 2003;
Moore, 2001). To keep the alignment model
simple, we could still use an IBM model 2, but
with the compositionality constraint that has
been shown to give better word alignment than
the Viterbi one (Simard and Langlais, 2003).
Feature weights We implemented two versions of
our model: one with only one feature weight
and another with one feature weight for each
word pair. The second model suffered from
poor data representation and our training algo-
rithm wasn?t able to estimate good cache fea-
ture weights. We think that creating classes
of word pairs, such as it was done for posi-
tional alignment features, would lead to better
results. It would enable the model to take into
account the tendency that a pair has to repeat
itself in a document.
Relative weighting Another key improvement is
that changes to word-pair weights should be
relative to each source word. For example,
if (house, maison) is a pair in the cache, we
would like to favour maison over possible al-
ternatives such as chambre as a translation of
house. In the existing model this is done by
boosting the weight on (house,maison), which
has the undesirable side-effect of making mai-
son more important in the model than transla-
tions of other source words in the current sen-
tence which have not appeared in the cache.
One way of eliminating this behaviour would
be to learn negative weights on alternatives like
(house,chambre) which do not appear in the
cache.
We believe these improvements would better show
the potential of bilingual dynamic adaptation.
7 Conclusion
We have presented dynamic adaptive translation
models using cache-based implementations. We
have shown that monolingual dynamic adaptive
models exhibit good theoretical performance in a
bilingual translation context. We observed that
these theoretical results carry over to practical gains
in the context of an IMT application.
We have developed bilingual dynamic adaptation
through a cache-based translation model. Our re-
sults show the potential of bilingual dynamic adap-
tation. We have given explanations about why the
results obtained are not as high as hoped and pre-
sented some key improvements that should be made
to our model or should be taken into account in the
development of a new model.
We believe that this study reveals the potential for
adaptive interactive machine translation system and
we hope to read similar reports for other implemen-
tations of the same interactive scenario e.g. (Och et
al., 2003).
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy
approach to Natural Language Processing. Com-
putational Linguistics, 22(1):39?71.
Peter F. Brown, John Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Fredrick Jelinek,
Robert L. Mercer, and Paul Roossin. 1988. A
statistical approach to language translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING), pages 71?
76, Budapest, Hungary, August.
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of Machine Translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?312, June.
P.R. Clarkson and P.R. Robertson. 1997. Language
model adaptation using mixtures and an expo-
nentially decaying cache. In IEEE Int. Confer-
ence on Acoustics, Speech, and Signal Process-
ing, Munich.
George Foster, Philippe Langlais, and Guy La-
palme. 2002. User-friendly text prediction
for translators. In 2002 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP 2002), Philadelphia, July. TT2
TransType2.
George Foster. 2000. A Maximum Entropy / Mini-
mum Divergence translation model. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
37?42, Hong Kong, October.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recog-
nition. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 12(6):570?
583, June.
Dekang Lin and Colin Cherry. 2003. Proalign:
Shared task system description. In NAACL 2003
Workshop on Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 11?14, Edmonton Canada, May 31. TT2.
S.C. Martin, J. Liermann, and H. Ney. 1997. Adap-
tative topic-dependent language modelling using
word-based varigrams. In Eurospeech.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In Rada Mihal-
cea and Ted Pedersen, editors, HLT-NAACL 2003
Workshop: Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,
pages 1?10, Edmonton, Alberta, Canada, May
31. Association for Computational Linguistics.
Robert C. Moore. 2001. Towards a simple and
accurate statistical approach to learning transla-
tion relationships among words. In Workshop on
Data-driven Machine Translation, 39th Annual
Meeting and 10th Conference of the European
Chapter, pages 79?86, Toulouse, France. Asso-
ciation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceed-
ings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages
440?447, Hong Kong, October.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient
search for interactive statistical machine trans-
lation. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 387?
393, Budapest, Hungary, April. TT2.
Michel Simard and Philippe Langlais. 2003. Statis-
tical translation alignment with compositionality
constraints. In NAACL 2003 Workshop on Build-
ing and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond, pages 19?22, Ed-
monton Canada, May 31. TT2.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 75?78,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
NUKTI: English-Inuktitut Word Alignment System Description
Philippe Langlais, Fabrizio Gotti, Guihong Cao
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C 3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Machine Translation (MT) as well as
other bilingual applications strongly
rely on word alignment. Efficient align-
ment techniques have been proposed
but are mainly evaluated on pairs of
languages where the notion of word
is mostly clear. We concentrated our
effort on the English-Inuktitut word
alignment shared task and report on
two approaches we implemented and a
combination of both.
1 Introduction
Word alignment is an important step in exploiting
parallel corpora. When efficient techniques have
been proposed (Brown et al, 1993; Och and Ney,
2003), they have been mostly evaluated on ?safe?
pairs of languages where the notion of word is
rather clear.
We devoted two weeks to the intriguing task
of aligning at the word level pairs of sentences
of English and Inuktitut. We experimented with
two different approaches. For the first one, we re-
lied on an in-house sentence alignment program
(JAPA) where English and Inuktitut tokens were
considered as sentences. The second approach
we propose takes advantage of associations com-
puted between any English word and roughly any
subsequence of Inuktitut characters seen in the
training corpus. We also investigated the combi-
nation of both approaches.
2 JAPA: Word Alignment as a Sentence
Alignment Task
To adjust our systems, the organizers made avail-
able to the participants a set of 25 pairs of sen-
tences where words had been manually aligned.
A fast inspection of this material reveals that in
most of the cases, the alignment produced are
monotonic and involve cepts of n adjacent En-
glish words aligned to a single Inuktitut word.
Many sentence alignment techniques strongly
rely on the monotonic nature of the inherent align-
ment. Therefore, we conducted a first experi-
ment using an in-house sentence alignment pro-
gram called JAPA that we developed within the
framework of the Arcade evaluation campaign
(Langlais et al, 1998). The implementation de-
tails of this aligner can be found in (Langlais,
1997), but in a few words, JAPA aligns pairs of
sentences by first grossly aligning their words
(making use of either cognate-like tokens, or a
specified bilingual dictionary). A second pass
aligns the sentences in a way similar1 to the algo-
rithm described by Gale and Church (1993), but
where the search space is constrained to be close
to the one delimited by the word alignment. This
technique happened to be among the most accu-
rate of the ones tested during the Arcade exercise.
To adapt JAPA to our needs, we only did two
things. First, we considered single sentences as
documents, and tokens as sentences (we define
a token as a sequence of characters delimited by
1In our case, the score we seek to globally maximize by
dynamic programming is not only taking into account the
length criteria described in (Gale and Church, 1993) but also
a cognate-based one similar to (Simard et al, 1992).
75
1-1 0.406 4-1 0.092 4-2 0.015
2-1 0.172 5-1 0.038 5-2 0.011
3-1 0.123 7-1 0.027 3-2 0.011
Table 1: The 9 most frequent English-Inuktitut
patterns observed on the development set. A total
of 24 different patterns have been observed.
white space). Second, since in its default setting,
JAPA only considers n-m sentence-alignment pat-
terns with n,m ? [0, 2], we provided it with a new
pattern distribution we computed from the devel-
opment corpus (see Table 1). It is interesting to
note that although English and Inuktitut have very
different word systems, the length ratio (in char-
acters) of the two sides of the TRAIN corpus is
1.05.
Each pair of documents (sentences) were then
aligned separately with JAPA. 1-n and n-1
alignments identified by JAPA where output with-
out further processing. Since the word alignment
format of the shared task do not account directly
for n-m alignments (n,m > 1) we generated the
cartesian product of the two sets of words for all
these n-m alignments produced by JAPA.
The performance of this approach is reported
in Table 2. Clearly, the precision is poor. This
is partly explained by the cartesian product we re-
sorted to when n-m alignments were produced by
JAPA. We provide in section 4 a way of improving
upon this scenario.
Prec. Rec. F-meas. AER
22.34 78.17 34.75 74.59
Table 2: Performance of the JAPA alignment tech-
nique on the DEV corpus.
3 NUKTI: Word and Substring
Alignment
Martin et al (2003) documented a study in build-
ing and using an English-Inuktitut bitext. They
described a sentence alignment technique tuned
for the specificity of the Inuktitut language, and
described as well a technique for acquiring cor-
respondent pairs of English tokens and Inuktitut
substrings. The motivation behind their work was
to populate a glossary with reliable such pairs.
We extended this line of work in order to achieve
word alignment.
3.1 Association Score
As Martin et al (2003) pointed out, the strong ag-
glutinative nature of Inuktitut makes it necessary
to consider subunits of Inuktitut tokens. This is
reflected by the large proportion of token types
and hapax words observed on the Inuktitut side
of the training corpus, compared to the ratios ob-
served on the English side (see table 3).
Inutktitut % English %
tokens 2 153 034 3 992 298
types 417 407 19.4 27 127 0.68
hapax 337 798 80.9 8 792 32.4
Table 3: Ratios of token types and happax words
in the TRAIN corpus.
The main idea presented in (Martin et al, 2003)
is to compute an association score between any
English word seen in the training corpus and all
the Inuktitut substrings of those tokens that were
seen in the same region. In our case, we com-
puted a likelihood ratio score (Dunning, 1993) for
all pairs of English tokens and Inuktitut substrings
of length ranging from 3 to 10 characters. A max-
imum of 25 000 associations were kept for each
English word (the top ranked ones).
To reduce the computation load, we used a suf-
fix tree structure and computed the association
scores only for the English words belonging to the
test corpus we had to align. We also filtered out
Inuktitut substrings we observed less than three
times in the training corpus. Altogether, it takes
about one hour for a good desktop computer to
produce the association scores for one hundred
English words.
We normalize the association scores such that
for each English word e, we have a distribution of
likely Inuktitut substrings s:
?
s pllr(s|e) = 1.
3.2 Word Alignment Strategy
Our approach for aligning an Inuktitut sentence
of K tokens IK1 with an English sentence of N
tokens EN1 (where K ? N )2 consists of finding
2As a matter of fact, the number of Inuktitut words in
the test corpus is always less than or equal to the number of
English tokens for any sentence pair.
76
K ? 1 cutting points ck?[1,K?1] (ck ? [1, N ? 1])
on the English side. A frontier ck delimits adja-
cent English words Eckck?1+1 that are translation of
the single Inuktitut word Ik. With the convention
that c0 = 0, cK = N and ck?1 < ck, we can for-
mulate our alignment problem as seeking the best
word alignment A = A(IK1 |EN1 ) by maximizing:
A = argmax
cK1
K?
k=1
p(Ik|E
ck
ck?1+1)
?1 ? p(dk)
?2
(1)
where dk = ck?ck?1 is the number of English
words associated to Ik; p(dk) is the prior proba-
bility that dk English words are aligned to a single
Inuktitut word, which we computed directly from
Table 1; and ?1 and ?2 are two weighting coeffi-
cients.
We tried the following two approximations to
compute p(Ik|Eckck?1+1). The second one led to
better results.
p(Ik|E
ck
ck?1+1) '
?
??
??
maxckj=ck?1+1 p(Ik|Ej)
or
?ck
j=ck?1+1
p(Ik|Ej)
We considered several ways of computing the
probability that an Inuktitut token I is the transla-
tion of an English one E; the best one we found
being:
p(I|E) '
?
s?I
?pllr(s|E) + (1? ?)pibm2(s|E)
where the summation is carried over all sub-
strings s of I of 3 characters or more. pllr(s|E)
is the normalized log-likelihood ratio score de-
scribed above and pibm2(s|E) is the probability
obtained from an IBM model 2 we trained after
the Inuktitut side of the training corpus was seg-
mented using a recursive procedure optimizing a
frequency-based criterion. ? is a weighting coef-
ficient.
We tried to directly embed a model trained
on whole (unsegmented) Inuktitut tokens, but no-
ticed a degradation in performance (line 2 of Ta-
ble 4).
3.3 A Greedy Search Strategy
Due to its combinatorial nature, the maximiza-
tion of equation 1 was barely tractable. There-
fore we adopted a greedy strategy to reduce the
search space. We first computed a split of the En-
glish sentence into K adjacent regions cK1 by vir-
tually drawing a diagonal line we would observe
if a character in one language was producing a
constant number of characters in the other one.
An initial word alignment was then found by sim-
ply tracking this diagonal at the word granularity
level.
Having this split in hand (line 1 of Table 4), we
move each cutting point around its initial value
starting from the leftmost cutting point and going
rightward. Once a locally optimal cutting point
has been found (that is, maximizing the score of
equation 1), we proceed to the next one directly
to its right.
3.4 Results
We report in Table 4 the performance of different
variants we tried as measured on the development
set. We used these performances to select the best
configuration we eventually submitted.
variant Prec. Rec. F-m. AER
start (diag) 51.7 53.66 52.66 49.54
greedy (word) 61.6 63.94 62.75 35.93
greedy (best) 63.5 65.92 64.69 34.21
Table 4: Performance of several NUKTI align-
ment techniques measured on the DEV corpus.
It is interesting to note that the starting point
of the greedy search (line 1) does better than our
first approach. However, moving from this ini-
tial split clearly improves the performance (line
3). Among the greedy variants we tested, we no-
ticed that putting much of the weight ? on the
IBM model 2 yielded the best results. We also no-
ticed that p(dk) in equation 1 did not help (?2 was
close to zero). A character-based model might
have been more appropriate to the case.
4 Combination of JAPA and NUKTI
One important weakness of our first approach lies
in the cartesian product we generate when JAPA
produces a n-m (n,m > 1) alignment. Thus,
we tried a third approach: we apply NUKTI on
any n-m alignment JAPA produces as if this ini-
tial alignment were in fact two (small) sentences
to align, n- and m-word long respectively. We can
77
therefore avoid the cartesian product and select
word alignments more discerningly. As can be
seen in Table 5, this combination improved over
JAPA alone, while being worse than NUKTI alone.
5 Results
We submitted 3 variants to the organizers. The
performances for each method are gathered in Ta-
ble 5. The order of merit of each approach was
consistent with the performance we measured on
the DEV corpus, the best method being the NUKTI
one. Curiously, we did not try to propose any Sure
alignment but did receive a credit for it for two of
the variants we submitted.
variant T. Prec. Rec. F-m. AER
JAPA P 26.17 74.49 38.73 71.27
JAPA + S 9.62 67.58 16.84
NUKTI P 51.34 53.60 52.44 46.64
NUKTI S 12.24 86.01 21.43
p 63.09 65.87 64.45 30.6
Table 5: Performance of the 3 alignments we sub-
mitted for the TEST corpus. T. stands for the type
of alignment (Sure or Possible).
6 Discussion
We proposed two methods for aligning an
English-Inuktitut bitext at the word level and a
combination of both. The best of these meth-
ods involves computing an association score be-
tween English tokens and Inuktitut substrings. It
relies on a greedy algorithm we specifically de-
vised for the task and which seeks a local opti-
mum of a cumulative function of log-likelihood
ratio scores. This method obtained a precision
and a recall above 63% and 65% respectively.
We believe this method could easily be im-
proved. First, it has some intrinsic limitations,
as for instance, the fact that NUKTI only recog-
nizes 1-n cepts and do not handle at all unaligned
words. Indeed, our method is not even suited to
aligning English sentences with fewer words than
their respective Inuktitut counterpart. Second, the
greedy search we devised is fairly aggressive and
only explores a tiny bit of the full search. Last,
the computation of the association scores is fairly
time-consuming.
Our idea of redefining word alignment as a sen-
tence alignment task did not work well; but at the
same time, we adapted poorly JAPA to this task.
In particular, JAPA does not benefit here from all
the potential of the underlying cognate system be-
cause of the scarcity of these cognates in very
small sequences (words).
If we had to work on this task again, we would
consider the use of a morphological analyzer. Un-
fortunately, it is only after the submission dead-
line that we learned of the existence of such a tool
for Inuktitut3.
Acknowledgement
We are grateful to Alexandre Patry who turned
the JAPA aligner into a nicely written and efficient
C++ program.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?311.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1).
W. A. Gale and K. W. Church. 1993. A Program for
Aligning Sentences in Bilingual Corpora. In Com-
putational Linguistics, volume 19, pages 75?102.
P. Langlais, M. Simard, and J. Ve?ronis. 1998. Meth-
ods and Practical Issues in Evaluating Alignment
Techniques. In 36th annual meeting of the ACL,
Montreal, Canada.
P. Langlais. 1997. A System to Align Complex Bilin-
gual Corpora. QPSR 4, TMH, Stockholm, Sweden.
J. Martin, H. Johnson, B. Farley, and A. Maclach-
lan. 2003. Aligning and Using an English-Inuktitut
Parallel Corpus. In Building and using Parallel
Texts: Data Driven Machine Translation and Be-
yond, pages 115?118, Edmonton, Canada.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Compu-
tational Linguistics, 29:19?51.
M. Simard, G.F. Foster, and P. Isabelle. 1992. Using
Cognates to Align Sentences in Bilingual Corpora.
In Conference on Theoretical and Methodological
Issues in Machine Translation, pages 67?81.
3See http://www.inuktitutcomputing.ca/
Uqailaut/
78
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
RALI: SMT shared task system description
Philippe Langlais, Guihong Cao and Fabrizio Gotti
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Thanks to the profusion of freely avail-
able tools, it recently became fairly
easy to built a statistical machine trans-
lation (SMT) engine given a bitext. The
expectations we can have on the quality
of such a system may however greatly
vary from one pair of languages to an-
other. We report on our experiments
in building phrase-based translation en-
gines for the four pairs of languages we
had to consider for the SMT shared-
task.
1 Introduction
Machine translation is nowadays mature enough
that it is possible without too much effort to de-
vise automatically a statistical translation system
from just a parallel corpus. This is possible
thanks to the dissemination of valuable packages.
The performance of such a system may however
greatly vary from one pair of languages to an-
other. Indeed, there is no free lunch for system
developers, and if a black box approach can some-
times be good enough for some applications (we
can surely accomplish translation gisting with the
French-English and Spanish-English systems we
developed during this exercice), making use of
the output of such a system for, let?s say, qual-
ity translation is another kettle of fish (especially
in our case with the Finnish-English system we
ended-up with).
We devoted two weeks to the SMT shared task,
the aim of which was precisely to see how well
systems can do across different language families.
We began with a core system which is described
in the next section and from which we obtained
baseline performances that we tried to improve
upon.
Since the French- and Spanish-English sys-
tems produced output that were comprehensi-
ble enough1, we focussed on the two languages
whose translations were noticeably worse: Ger-
man and Finnish. For German, we tried to move
around words in order to mimic English word or-
der; and we tried to split compound words. This
is described in section 4. For the Finnish/English
pair, we tried to decompose Finnish words into
smaller substrings (see section 5).
In parallel to that, we tried to smooth a phrase-
based model (PBM) making use of WORDNET.
We report on this experiment in section 3. We de-
scribe in section 6 the final setting of the systems
we used for submitting translations and their of-
ficial results as computed by the organizers. Fi-
nally, we conclude our two weeks of efforts in
section 7.
2 The core system
We assembled up a phrase-based statistical engine
by making use of freely available packages. The
translation engine we used is the one suggested
within the shared task: PHARAOH (Koehn, 2004).
The input of this decoder is composed of a phrase-
based model (PBM), a trigram language model
and an optional set of coefficients and thresholds
1What we mean by this is nothing more than we were
mostly able to infer the original meaning of the source sen-
tence by reading its automatic translation.
137
pair WER SER NIST BLEU
fi-en 66.53 99.20 5.3353 18.73
de-en 60.70 98.40 5.8411 21.11
fr-en 53.77 98.20 6.4717 27.69
es-en 53.84 98.60 6.5571 28.08
Table 1: Baseline performances measured on the
500 top sentences of the DEV corpus in terms of
WER (word error rate), SER (sentence error rate),
NIST and BLEU scores.
which control the decoder.
For acquiring a PBM, we followed the ap-
proach described by Koehn et al (2003). In brief,
we relied on a bi-directional word alignment of
the training corpus to acquire the parameters of
the model. We used the word alignment pro-
duced by Giza (Och and Ney, 2000) out of an
IBM model 2. We did try to use the alignment
produced with IBM model 4, but did not notice
significant differences over our experiments; an
observation consistent with the findings of Koehn
et al (2003). Each parameter in a PBM can be
scored in several ways. We considered its rela-
tive frequency as well as its IBM-model 1 score
(where the transfer probabilities were taken from
an IBM model 2 transfer table). The language
model we used was the one provided within the
shared task.
We obtained baseline performances by tuning
the engine on the top 500 sentences of the devel-
opment corpus. Since we only had a few param-
eters to tune, we did it by sampling the parameter
space uniformly. The best performance we ob-
tained, i.e., the one which maximizes the BLEU
metric as measured by the mteval script2 is re-
ported for each pair of languages in Table 1.
3 Smoothing PBMs with WORDNET
Among the things we tried but which did not
work well, we investigated whether smoothing
the transfer table of an IBM model (2 in our case)
with WORDNET would produce better estimates
for rare words. We adapted an approach proposed
by Cao et al (2005) for an Information Retrieval
task, and computed for any parameter (ei, fj) be-
2http://www.nist.gov/speech/tests/mt/
mt2001/resource
longing to the original model the following ap-
proximation:
p?(ei|fj) ?
?
e?E
pwn(ei|e)? pn(e|fj)
where E is the English vocabulary, pn desig-
nates the native distribution and pwn is the proba-
bility that two words in the English side are linked
together. We estimated this distribution by co-
occurrence counts over a large English corpus3.
To avoid taking into account unrelated but co-
occurring words, we used WORDNET to filter in
only the co-occurrences of words that are in re-
lation according to WORDNET. However, since
many words are not listed in this resource, we had
to smooth the bigram distribution, which we did
by applying Katz smoothing (Katz, 1997):
pkatz(ei|e) =
{
c?(ei,e|W,L)P
ej
c(ej ,e|W,L)
if c(ei, e|W,L) > 0
?(e)pkatz(ei) otherwise
where c?(a, b|W,L) is the good-turing dis-
counted count of times two words a and b that are
linked together by a WORDNET relation, co-occur
in a window of 2 sentences.
We used this smoothed model to score the pa-
rameters of our PBM instead of the native trans-
fer table. The results were however disappoint-
ing for both the G-E and S-E translation direc-
tions we tested. One reason for that, may be
that the English corpus we used for computing
the co-occurrence counts is an out-of-domain cor-
pus for the present task. Another possible ex-
planation lies in the fact that we considered both
synonymic and hyperonymic links in WORDNET;
the latter kind of links potentially introducing too
much noise for a translation task.
4 The German-English task
We identified two major problems with our ap-
proach when faced with this pair of languages.
First, the tendency in German to put verbs at the
end of a phrase happens to ruin our phrase acqui-
sition process, which basically collects any box
of aligned source and target adjacent words. This
3For this, we used the English side of the provided train-
ing corpus plus the English side of our in-house Hansard bi-
text; that is, a total of more than 7 million pairs of sentences.
138
can be clearly seen in the alignment matrix of fig-
ure 1 where the verbal construction could clarify
is translated by two very distant German words
ko?nnten and erla?utern. Second, there are many
compound words in German that greatly dilute
the various counts embedded in the PBM table.
. . . . . . . . . . . . . ?
erla?utern . . . . . . . ? . . . . .
punkt . . . . . . . . . ? . . .
einen . . . . . . . . ? . " . .
mir . . . . . . . . . . . ? .
sie . . . . . ? . . . . . . .
oder . . . . ? . . . . . . . .
kommission . . . ? . . . . . . . . .
die . . ? . . . . . . . . . .
ko?nnten . . . . . . ? . . . . . .
vielleicht . ? . . . . . . . . . . .
NULL . . . . . . . . . . . . .
N p t c o y c c a p f m .
U e h o r o o l o o e
L r e m u u a i r
L h m l r n
English perhaps the commission or you could
clarify a point for me .
German vielleicht ko?nnten die kommission oder
sie mir einen punkt erla?utern .
Figure 1: Bidirectional alignment matrix. A cross
in this matrix designates an alignment valid in
both directions, while the " symbol indicates an
uni-directional alignment (for has been aligned
with einen, but not the other way round).
4.1 Moving around German words
For the first problem, we applied a memory-based
approach to move around words in the German
side in order to better synchronize word order
in both languages. This involves, first, to learn-
ing transformation rules from the training corpus,
second, transforming the German side of this cor-
pus; then training a new translation model. The
same set of rules is then applied to the German
text to be translated.
The transformation rules we learned concern a
few (five in our case) verbal constructions that
we expressed with regular expressions built on
POS tags in the English side. Once the locus
e
v
u of a pattern has been identified, a rule is col-
lected whenever the following conditions apply:
for each word e in the locus, there is a target word
f which is aligned to e in both alignment direc-
tions; these target words when moved can lead to
a diagonal going from the target word (l) associ-
ated to eu?1 to the target word r which is aligned
to ev+1.
The rules we memorize are triplets (c, i, o)
where c = (l, r) is the context of the locus and i
and o are the input and output German word order
(that is, the order in which the tokens are found,
and the order in which they should be moved).
For instance, in the example of Figure 1,
the Verb Verb pattern match the locus could
clarify and the following rule is acquired:
(sie einen, ko?nnten erla?utern,
ko?nnten erla?utern), a paraphrase of
which is: ?whenever you find (in this order)
the word ko?nnten and erla?utern in a German
sentence containing also (in this order) sie and
einen, move ko?nnten and erla?utern between sie
and einen.
A set of 124 271 rules have been acquired
this way from the training corpus (for a total of
157 970 occurrences). The most frequent rule ac-
quired is (ich herrn, mo?chte danken,
mo?chte danken), which will transform a sen-
tence like ?ich mo?chte herrn wynn fu?r seinen
bericht danken.? into ?ich mo?chte danken herrn
wynn fu?r seinen bericht.?.
In practice, since this acquisition process does
not involve any generalization step, only a few
rules learnt really fire when applied to the test ma-
terial. Also, we devised a fairly conservative way
of applying the rules, which means that in prac-
tice, only 3.5% of the sentences of the test corpus
where actually modified.
The performance of this procedure as measured
on the development set is reported in Table 2. As
simple as it is, this procedure yields a relative gain
of 7% in BLEU. Given the crudeness of our ap-
proach, we consider this as an encouraging im-
provement.
4.2 Compound splitting
For the second problem, we segmented German
words before training the translation models. Em-
pirical methods for compound splitting applied to
139
system WER SER NIST BLEU
baseline 60.70 98.40 5.8411 21.11
swap 60.73 98.60 5.9643 22.58
split 60.67 98.60 5.7511 21.99
swap+split 60.57 98.40 5.9685 23.10
Table 2: Performances of the swapping and the
compound splitting approaches on the top 500
sentences of the development set.
German have been studied by Koehn and Knight
(2003). They found that a simple splitting strat-
egy based on the frequency of German words was
the most efficient method of the ones they tested,
when embedded in a phrase-based translation en-
gine. Therefore, we applied such a strategy to
split German words in our corpora. The results
of this approach are shown in Table 2.
Note: Both the swapping strategy and the com-
pound splitting yielded improvements in terms of
BLEU score. Only after the deadline did we find
time to train new models with a combination of
both techniques; the results of which are reported
in the last line of Table 2.
5 The Finnish-English task
The worst performances were registered on the
Finnish-English pair. This is due to the aggluti-
native nature of Finnish. We tried to segment the
Finnish material into smaller units (substrings) by
making use of the frequency of all Finnish sub-
strings found in the training corpus. We main-
tained a suffix tree structure for that purpose.
We proceeded by recursively finding the most
promising splitting points in each Finnish token
of C characters FC1 by computing split(FC1 )
where:
split(F ji ) =
?
?
?
|F ji | if j ? i < 2
maxc?[i+2,j?2] |F
c
i |?
split(F jc+1) otherwise
This approach yielded a significant degradation
in performance that we still have to analyze.
6 Submitted translations
At the time of the deadline, the best translations
we had were the baselines ones for all the lan-
guage pairs, except for the German-English one
where the moving of words ranked the best. This
defined the configuration we submitted, whose re-
sults (as provided by the organizers) are reported
in Table 3.
pair BLEU p1/p2/p3/p4
fi-en 18.87 55.2/24.7/13.1/7.1
de-en 22.91 58.9/29.0/16.8/10.3
es-en 28.49 62.4/34.5/21.9/14.4
fr-en 28.89 62.6/34.7/22.0/14.6
Table 3: Results measured by the organizers for
the TEST corpus.
7 Conclusion
We found that, while comprehensible translations
were produced for pairs of languages such as
French-English and Spanish-English; things did
not go as well for the German-English pair and
especially not for the Finnish-English pair. We
had a hard time improving our baseline perfor-
mance in such a tight schedule and only man-
aged to improve our German-English system. We
were less lucky with other attempts we imple-
mented, among them, the smoothing of a trans-
fer table with WORDNET, and the segmentation
of the Finnish corpus into smaller units.
References
G. Cao, J. Nie, and J. Bai. 2005. Integrating Word
relationships into Language Models. In to appear
in Proc. of SIGIR.
S. Katz. 1997. Estimation of Probabilities from
Sparse Data for the Language Model Component of
a Speech Recognizer. IEEE Transactions on Acous-
tics Speech and Signal Processing, 35.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of HLT,
pages 127?133.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based SMT. In Proceedings of AMTA,
pages 115?124.
F.J. Och and H. Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL, pages
440?447, Hongkong, China.
140
Proceedings of the Workshop on Statistical Machine Translation, pages 39?46,
New York City, June 2006. c?2006 Association for Computational Linguistics
Phrase-Based SMT with Shallow Tree-Phrases
Philippe Langlais and Fabrizio Gotti
RALI ? DIRO
Universite? de Montre?al,
C.P. 6128 Succ. Centre-Ville
H3C 3J7, Montre?al, Canada
{felipe,gottif}@iro.umontreal.ca
Abstract
In this article, we present a translation
system which builds translations by glu-
ing together Tree-Phrases, i.e. associ-
ations between simple syntactic depen-
dency treelets in a source language and
their corresponding phrases in a target
language. The Tree-Phrases we use in
this study are syntactically informed and
present the advantage of gathering source
and target material whose words do not
have to be adjacent. We show that the
phrase-based translation engine we imple-
mented benefits from Tree-Phrases.
1 Introduction
Phrase-based machine translation is now a popular
paradigm. It has the advantage of naturally cap-
turing local reorderings and is shown to outper-
form word-based machine translation (Koehn et al,
2003). The underlying unit (a pair of phrases), how-
ever, does not handle well languages with very dif-
ferent word orders and fails to derive generalizations
from the training corpus.
Several alternatives have been recently proposed
to tackle some of these weaknesses. (Matusov et
al., 2005) propose to reorder the source text in or-
der to mimic the target word order, and then let a
phrase-based model do what it is good at. (Simard
et al, 2005) detail an approach where the standard
phrases are extended to account for ?gaps? either on
the target or source side. They show that this repre-
sentation has the potential to better exploit the train-
ing corpus and to nicely handle differences such as
negations in French and English that are poorly han-
dled by standard phrase-based models.
Others are considering translation as a syn-
chronous parsing process e.g. (Melamed, 2004;
Ding and Palmer, 2005)) and several algorithms
have been proposed to learn the underlying produc-
tion rule probabilities (Graehl and Knight, 2004;
Ding and Palmer, 2004). (Chiang, 2005) proposes
an heuristic way of acquiring context free transfer
rules that significantly improves upon a standard
phrase-based model.
As mentioned in (Ding and Palmer, 2005), most
of these approaches require some assumptions on
the level of isomorphism (lexical and/or structural)
between two languages. In this work, we consider
a simple kind of unit: a Tree-Phrase (TP), a com-
bination of a fully lexicalized treelet (TL) and an
elastic phrase (EP), the tokens of which may be in
non-contiguous positions. TPs capture some syntac-
tic information between two languages and can eas-
ily be merged with standard phrase-based engines.
A TP can be seen as a simplification of the treelet
pairs manipulated in (Quirk et al, 2005). In particu-
lar, we do not address the issue of projecting a source
treelet into a target one, but take the bet that collect-
ing (without structure) the target words associated
with the words encoded in the nodes of a treelet will
suffice to allow translation. This set of target words
is what we call an elastic phrase.
We show that these units lead to (modest) im-
provements in translation quality as measured by au-
tomatic metrics. We conducted all our experiments
39
on an in-house version of the French-English Cana-
dian Hansards.
This paper is organized as follows. We first define
a Tree-Phrase in Section 2, the unit with which we
built our system. Then, we describe in Section 3
the phrase-based MT decoder that we designed to
handle TPs. We report in Section 4 the experiments
we conducted combining standard phrase pairs and
TPs. We discuss this work in Section 5 and then
conclude in Section 6.
2 Tree-Phrases
We call tree-phrase (TP) a bilingual unit consisting
of a source, fully-lexicalized treelet (TL) and a tar-
get phrase (EP), that is, the target words associated
with the nodes of the treelet, in order. A treelet can
be an arbitrary, fully-lexicalized subtree of the parse
tree associated with a source sentence. A phrase can
be an arbitrary sequence of words. This includes
the standard notion of phrase, popular with phrased-
based SMT (Koehn et al, 2003; Vogel et al, 2003)
as well as sequences of words that contain gaps (pos-
sibly of arbitrary size).
In this study, we collected a repository of tree-
phrases using a robust syntactic parser called SYN-
TEX (Bourigault and Fabre, 2000). SYNTEX identi-
fies syntactic dependency relations between words.
It takes as input a text processed by the TREETAG-
GER part-of-speech tagger.1 An example of the out-
put SYNTEX produces for the source (French) sen-
tence ?on a demande? des cre?dits fe?de?raux? (request
for federal funding) is presented in Figure 1.
We parsed with SYNTEX the source (French) part
of our training bitext (see Section 4.1). From this
material, we extracted all dependency subtrees of
depth 1 from the complete dependency trees found
by SYNTEX. An elastic phrase is simply the list of
tokens aligned with the words of the corresponding
treelet as well as the respective offsets at which they
were found in the target sentence (the first token of
an elastic phrase always has an offset of 0).
For instance, the two treelets in Figure 2 will be
collected out of the parse tree in Figure 1, yielding
2 tree-phrases. Note that the TLs as well as the EPs
might not be contiguous as is for instance the case
1www.ims.uni-stuttgart.de/projekte/
corplex/.
a demande?
SUB
ll
ll
ll
ll
ll OBJ
YYY
YYY
YYY
YYY
YYY
YYY
on cre?dits
DET
ll
ll
ll
ll
ll ADJ
RR
RR
RR
RR
RR
des fe?de?raux
Figure 1: Parse of the sentence ?on a demande? des
cre?dits fe?de?raux? (request for federal funding). Note
that the 2 words ?a? and ?demande?? (literally ?have?
and ?asked?) from the original sentence have been
merged together by SYNTEX to form a single token.
These tokens are the ones we use in this study.
with the first pair of structures listed in the example.
3 The Translation Engine
We built a translation engine very similar to the sta-
tistical phrase-based engine PHARAOH described in
(Koehn, 2004) that we extended to use tree-phrases.
Not only does our decoder differ from PHARAOH by
using TPs, it also uses direct translation models. We
know from (Och and Ney, 2002) that not using the
noisy-channel approach does not impact the quality
of the translation produced.
3.1 The maximization setting
For a source sentence f , our engine incrementally
generates a set of translation hypotheses H by com-
bining tree-phrase (TP) units and phrase-phrase (PP)
units.2 We define a hypothesis in this set as h =
{Ui ? (Fi, Ei)}i?[1,u], a set of u pairs of source
(Fi) and target sequences (Ei) of ni and mi words
respectively:
Fi ? {fjin : j
i
n ? [1, |f |]}n?[1,ni]
Ei ? {elim : l
i
m ? [1, |e|]}m?[1,mi]
under the constraints that for all i ? [1, u], jin <
jin+1 ,?n ? [1, ni[ for a source treelet (similar con-
straints apply on the target side), and jin+1 = j
i
n +
1 ,?n ? [1, ni[ for a source phrase. The way the
hypotheses are built imposes additional constraints
between units that will be described in Section 3.3.
Note that, at decoding time, |e|, the number of words
2What we call here a phrase-phrase unit is simply a pair of
source/target sequences of words.
40
alignment:
a demande? ? request for, fe?de?raux ? federal,
cre?dits ? funding
treelets:
a demande?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
on cre?dits
cre?dits
q
q
q
q
q
q
q
M
M
M
M
M
M
M
des fe?de?raux
tree-phrases:
TL? {{on@-1} a_demande? {cre?dits@2}}
EP? |request@0||for@1||funding@3|
TL {{des@-1} cre?dits {fe?de?raux@1}}
EP |federal@0||funding@1|
Figure 2: The Tree-Phrases collected out of the
SYNTEX parse for the sentence pair of Figure 1.
Non-contiguous structures are marked with a star.
Each dependent node of a given governor token is
displayed as a list surrounding the governor node,
e.g. {governor {right-dependent}}. Along with the
tokens of each node, we present their respective off-
set (the governor/root node has the offset 0 by defi-
nition). The format we use to represent the treelets
is similar to the one proposed in (Quirk et al, 2005).
of the translation is unknown, but is bounded accord-
ing to |f | (in our case, |e|max = 2? |f |+ 5).
We define the source and target projection of a
hypothesis h by the proj operator which collects in
order the words of a hypothesis along one language:
projF (h) =
{
fp : p ?
?u
i=1{j
i
n}n?[1,ni]
}
projE(h) =
{
ep : p ?
?u
i=1{l
i
m}m?[1,mi]
}
If we denote by Hf the set of hypotheses that
have f as a source projection (that is, Hf = {h :
projF (h) ? f}), then our translation engine seeks
e? = projE(h?) where:
h? = argmax
h?Hf
s(h)
The function we seek to maximize s(h) is a log-
linear combination of 9 components, and might be
better understood as the numerator of a maximum
entropy model popular in several statistical MT sys-
tems (Och and Ney, 2002; Bertoldi et al, 2004; Zens
and Ney, 2004; Simard et al, 2005; Quirk et al,
2005). The components are the so-called feature
functions (described below) and the weighting co-
efficients (?) are the parameters of the model:
s(h) = ?pprf log ppprf (h) + ?p|h|+
?tprf log ptprf (h) + ?t|h|+
?ppibm log pppibm(h)+
?tpibm log ptpibm(h)+
?lm log plm(projE(h))+
?d d(h) + ?w|projE(h)|
3.2 The components of the scoring function
We briefly enumerate the features used in this study.
Translation models Even if a tree-phrase is a gen-
eralization of a standard phrase-phrase unit, for in-
vestigation purposes, we differentiate in our MT
system between two kinds of models: a TP-based
model ptp and a phrase-phrase model ppp. Both rely
on conditional distributions whose parameters are
learned over a corpus. Thus, each model is assigned
its own weighting coefficient, allowing the tuning
process to bias the engine toward a special kind of
unit (TP or PP).
We have, for k ? {rf, ibm}:
pppk(h) =
?u
i=1 ppp(Ei|Fi)
ptpk(h) =
?u
i=1 ptp(Ei|Fi)
with p?rf standing for a model trained by rel-
ative frequency, whereas p?ibm designates a non-
normalized score computed by an IBM model-1
translation model p, where f0 designates the so-
called NULL word:
p?ibm(Ei|Fi) =
mi?
m=1
ni?
n=1
p(elim |fjin) + p(ekim |f0)
Note that by setting ?tprf and ?tpibm to zero, we
revert back to a standard phrase-based translation
engine. This will serve as a reference system in the
experiments reported (see Section 4).
The language model Following a standard prac-
tice, we use a trigram target language model
plm(projE(h)) to control the fluency of the trans-
lation produced. See Section 3.3 for technical sub-
tleties related to their use in our engine.
41
Distortion model d This feature is very similar to
the one described in (Koehn, 2004) and only de-
pends on the offsets of the source units. The only
difference here arises when TPs are used to build a
translation hypothesis:
d(h) = ?
n?
i=1
abs(1 + F i?1 ? F i)
where:
F i =
{ ?
n?[1,ni] j
i
n/ni if Fi is a treelet
jini otherwise
F i = j
i
1
This score encourages the decoder to produce a
monotonous translation, unless the language model
strongly privileges the opposite.
Global bias features Finally, three simple fea-
tures help control the translation produced. Each
TP (resp. PP) unit used to produce a hypothesis
receives a fixed weight ?t (resp. ?p). This allows
the introduction of an artificial bias favoring either
PPs or TPs during decoding. Each target word pro-
duced is furthermore given a so-called word penalty
?w which provides a weak way of controlling the
preference of the decoder for long or short transla-
tions.
3.3 The search procedure
The search procedure is described by the algorithm
in Figure 3. The first stage of the search consists in
collecting all the units (TPs or PPs) whose source
part matches the source sentence f . We call U the
set of those matching units.
In this study, we apply a simple match policy that
we call exact match policy. A TL t matches a source
sentence f if its root matches f at a source position
denoted r and if all the other words w of t satisfy:
fow+r = w
where ow designates the offset of w in t.
Hypotheses are built synchronously along with
the target side (by appending the target material to
the right of the translation being produced) by pro-
gressively covering the positions of the source sen-
tence f being translated.
Require: a source sentence f
U ? {u : s-match(u, f)}
FUTURECOST(U)
for s? 1 to |f | do
S[s]? ?
S[0]? {(?, , 0)}
for s? 0 to |f | ? 1 do
PRUNE(S[s], ?)
for all hypotheses alive h ? S[s] do
for all u ? U do
if EXTENDS(u, h) then
h? ? UPDATE(u, h)
k ? |projF (h?)|
S[k]? S[k] ? {h?}
return argmaxh?S[|f |] ? : h? (ps, t, ?)
Figure 3: The search algorithm. The symbol ? is
used in place of assignments, while? denotes uni-
fication (as in languages such as Prolog).
The search space is organized into a set S of |f |
stacks, where a stack S[s] (s ? [1, |f |]) contains all
the hypotheses covering exactly s source words. A
hypothesis h = (ps, t, ?) is composed of its target
material t, the source positions covered ps as well as
its score ?. The search space is initialized with an
empty hypothesis: S[0] = {(?, , 0)}.
The search procedure consists in extending each
partial hypothesis h with every unit that can con-
tinue it. This process ends when all partial hypothe-
ses have been expanded. The translation returned is
the best one contained in S[|f |]:
e? = projE(argmax
h?S[|f |]
? : h? (ps, t, ?))
PRUNE ? In order to make the search tractable,
each stack S[s] is pruned before being expanded.
Only the hypotheses whose scores are within a frac-
tion (controlled by a meta-parameter ? which typi-
cally is 0.0001 in our experiments) of the score of
the best hypothesis in that stack are considered for
expansion. We also limit the number of hypotheses
maintained in a given stack to the top maxStack
ones (maxStack is typically set to 500).
Because beam-pruning tends to promote in a stack
partial hypotheses that translate easy parts (i.e. parts
42
that are highly scored by the translation and lan-
guage models), the score considered while pruning
not only involves the cost of a partial hypothesis so
far, but also an estimation of the future cost that will
be incurred by fully expanding it.
FUTURECOST ? We followed the heuristic de-
scribed in (Koehn, 2004), which consists in comput-
ing for each source range [i, j] the minimum cost
c(i, j) with which we can translate the source se-
quence f ji . This is pre-computed efficiently at an
early stage of the decoding (second line of the algo-
rithm in Figure 3) by a bottom-up dynamic program-
ming scheme relying on the following recursion:
c(i, j) = min
{
mink?[i,j[c(i, k) + c(k, j)
minu?U/us?fji =us
score(us)
where us stands for the projection of u on the tar-
get side (us ? projE(u)), and score(u) is com-
puted by considering the language model and the
translation components ppp of the s(h) score. The
future cost of h is then computed by summing the
cost c(i, j) of all its empty source ranges [i, j].
EXTENDS ? When we simply deal with standard
(contiguous) phrases, extending a hypothesis h by a
unit u basically requires that the source positions of
u be empty in h. Then, the target material of u is
appended to the current hypothesis h.
Because we work with treelets here, things are
a little more intricate. Conceptually, we are con-
fronted with the construction of a (partial) source
dependency tree while collecting the target mate-
rial in order. Therefore, the decoder needs to check
whether a given TL (the source part of u) is compati-
ble with the TLs belonging to h. Since we decided in
this study to use depth-one treelets, we consider that
two TLs are compatible if either they do not share
any source word, or, if they do, this shared word
must be the governor of one TL and a dependent in
the other TL.
So, for instance, in the case of Figure 2, the
two treelets are deemed compatible (they obviously
should be since they both belong to the same orig-
inal parse tree) because cre?dit is the governor
in the right-hand treelet while being the depen-
dent in the left-hand one. On the other hand, the
two treelets in Figure 4 are not, since pre?sident
is the governor of both treelets, even though mr.
le pre?sident supple?ant would be a valid
source phrase. Note that it might be the case that
the treelet {{mr.@-2} {le@-1} pre?sident
{supple?ant@1}} has been observed during
training, in which case it will compete with the
treelets in Figure 2.
pre?sident
mr.
pre?sident
q
q
q
q
q
q
q
M
M
M
M
M
M
M
le supple?ant
Figure 4: Example of two incompatible treelets.
mr. speaker and the acting speaker
are their respective English translations.
Therefore, extending a hypothesis containing a
treelet with a new treelet consists in merging the two
treelets (if they are compatible) and combining the
target material accordingly. This operation is more
complicated than in a standard phrase-based decoder
since we allow gaps on the target side as well. More-
over, the target material of two compatible treelets
may intersect. This is for instance the case for the
two TPs in Figure 2 where the word funding is
common to both phrases.
UPDATE ? Whenever u extends h, we add a
new hypothesis h? in the corresponding stack
S[|projF (h?)|]. Its score is computed by adding to
that of h the score of each component involved in
s(h). For all but the one language model compo-
nent, this is straightforward. However, care must be
taken to update the language model score since the
target material of u does not come necessarily right
after that of h as would be the case if we only ma-
nipulated PP units.
Figure 5 illustrates the kind of bookkeeping
required. In practice, the target material of
a hypothesis is encoded as a vector of triplets
{?wi, log plm(wi|ci), li?}i?[1,|e|max] where wi is the
word at position i in the translation, log plm(wi|ci)
is its score as given by the language model, ci de-
notes the largest conditioning context possible, and
li indicates the length (in words) of ci (0 means a
unigram probability, 1 a bigram probability and 2 a
trigram probability). This vector is updated at each
extension.
43
u
des f?d?rauxon a_demand? cr?dits
TL: {on@?1}  a_demand?  {cr?dits@2}
EP: request@0  for@1  funding@3
TL: {des@?1}  cr?dits  {f?d?raux@1}
EP: federal@0  funding@1
U B F Urequest for funding
cr?ditson a_demand? des f?d?raux
forrequest fundingU B T Tfederal
h
h?
S[3]
S[4]
u
Figure 5: Illustration of the language model up-
dates that must be made when a new target unit
(circles with arrows represent dependency links) ex-
tends an existing hypothesis (rectangles). The tag
inside each occupied target position shows whether
this word has been scored by a Unigram, a Bigram
or a Trigram probability.
4 Experimental Setting
4.1 Corpora
We conducted our experiments on an in-house ver-
sion of the Canadian Hansards focussing on the
translation of French into English. The split of this
material into train, development and test corpora is
detailed in Table 1. The TEST corpus is subdivided
in 16 (disjoints) slices of 500 sentences each that
we translated separately. The vocabulary is atypi-
cally large since some tokens are being merged by
SYNTEX, such as e?taient#finance?es (were
financed in English).
The training corpus has been aligned at the
word level by two Viterbi word-alignments
(French2English and English2French) that we
combined in a heuristic way similar to the refined
method described in (Och and Ney, 2003). The
parameters of the word models (IBM model 2) were
trained with the GIZA++ package (Och and Ney,
2000).
TRAIN DEV TEST
sentences 1 699 592 500 8000
e-toks 27 717 389 8 160 130 192
f-toks 30 425 066 8 946 143 089
e-toks/sent 16.3 (? 9.0) 16.3 (? 9.1) 16.3 (? 9.0)
f-toks/sent 17.9 (? 9.5) 17.9 (? 9.5) 17.9 (? 9.5)
e-types 164 255 2 224 12 591
f-types 210 085 2 481 15 008
e-hapax 68 506 1 469 6 887
f-hapax 90 747 1 704 8 612
Table 1: Main characteristics of the corpora used in
this study. For each language l, l-toks is the number
of tokens, l-toks/sent is the average number of to-
kens per sentence (? the standard deviation), l-types
is the number of different token forms and l-hapax
is the number of tokens that appear only once in the
corpus.
4.2 Models
Tree-phrases Out of 1.7 million pairs of sen-
tences, we collected more than 3 million different
kinds of TLs from which we projected 6.5 million
different kinds of EPs. Slightly less than half of
the treelets are contiguous ones (i.e. involving a se-
quence of adjacent words); 40% of the EPs are con-
tiguous. When the respective frequency of each TL
or EP is factored in, we have approximately 11 mil-
lion TLs and 10 million EPs. Roughly half of the
treelets collected have exactly two dependents (three
word long treelets).
Since the word alignment of non-contiguous
phrases is likely to be less accurate than the align-
ment of adjacent word sequences, we further filter
the repository of TPs by keeping the most likely EPs
for each TL according to an estimate of p(EP |TL)
that do not take into account the offsets of the EP or
the TL.
PP-model We collected the PP parameters by sim-
ply reading the alignment matrices resulting from
the word alignment, in a way similar to the one
described in (Koehn et al, 2003). We use an in-
house tool to collect pairs of phrases of up to 8
words. Freely available packages such as THOT
(Ortiz-Mart??nez et al, 2005) could be used as well
for that purpose.
44
Language model We trained a Kneser-Ney tri-
gram language model using the SRILM toolkit (Stol-
cke, 2002).
4.3 Protocol
We compared the performances of two versions of
our engine: one which employs TPs ans PPs (TP-
ENGINE hereafter), and one which only uses PPs
(PP-ENGINE). We translated the 16 disjoint sub-
corpora of the TEST corpus with and without TPs.
We measure the quality of the translation pro-
duced with three automatic metrics. Two error
rates: the sentence error rate (SER) and the word
error rate (WER) that we seek to minimize, and
BLEU (Papineni et al, 2002), that we seek to
maximize. This last metric was computed with
the multi-bleu.perl script available at www.
statmt.org/wmt06/shared-task/.
We separately tuned both systems on the DEV cor-
pus by applying a brute force strategy, i.e. by sam-
pling uniformly the range of each parameter (?) and
picking the configuration which led to the best BLEU
score. This strategy is inelegant, but in early experi-
ments we conducted, we found better configurations
this way than by applying the Simplex method with
multiple starting points. The tuning roughly takes
24 hours of computation on a cluster of 16 comput-
ers clocked at 3 GHz, but, in practice, we found that
one hour of computation is sufficient to get a con-
figuration whose performances, while subobptimal,
are close enough to the best one reachable by an ex-
haustive search.
Both configurations were set up to avoid distor-
tions exceeding 3 (maxDist = 3). Stacks were
allowed to contain no more than 500 hypotheses
(maxStack = 500) and we further restrained the
number of hypotheses considered by keeping for
each matching unit (treelet or phrase) the 5 best
ranked target associations. This setting has been
fixed experimentally on the DEV corpus.
4.4 Results
The scores for the 16 slices of the test corpus are re-
ported in Table 2. TP-ENGINE shows slightly better
figures for all metrics.
For each system and for each metric, we had
16 scores (from each of the 16 slices of the test cor-
pus) and were therefore able to test the statistical sig-
nicance of the difference between the TP-ENGINE
and PP-ENGINE using a Wilcoxon signed-rank test
for paired samples. This test showed that the dif-
ference observed between the two systems is signif-
icant at the 95% probability level for BLEU and sig-
nificant at the 99% level for WER and SER.
Engine WER% SER% BLEU%
PP 52.80 ? 1.2 94.32 ? 0.9 29.95 ? 1.2
TP 51.98 ? 1.2 92.83 ? 1.3 30.47 ? 1.4
Table 2: Median WER, SER and BLEU scores
(? value range) of the translations produced by the
two engines on a test set of 16 disjoint corpora of
500 sentences each. The figures reported are per-
centages.
On the DEV corpus, we measured that, on aver-
age, each source sentence is covered by 39 TPs (their
source part, naturally), yielding a source coverage of
approximately 70%. In contrast, the average number
of covering PPs per sentence is 233.
5 Discussion
On a comparable test set (Canadian Hansard texts),
(Simard et al, 2005) report improvements by adding
non-contiguous bi-phrases to their engine without
requiring a parser at all. At the same time, they also
report negative results when adding non-contiguous
phrases computed from the refined alignment tech-
nique that we used here.
Although the results are not directly comparable,
(Quirk et al, 2005) report much larger improve-
ments over a phrase-based statistical engine with
their translation engine that employs a source parser.
The fact that we consider only depth-one treelets in
this work, coupled with the absence of any particular
treelet projection algorithm (which prevents us from
training a syntactically motivated reordering model
as they do) are other possible explanations for the
modest yet significant improvements we observe in
this study.
6 Conclusion
We presented a pilot study aimed at appreciating the
potential of Tree-Phrases as base units for example-
based machine translation.
45
We developed a translation engine which makes
use of tree-phrases on top of pairs of source/target
sequences of words. The experiments we conducted
suggest that TPs have the potential to improve trans-
lation quality, although the improvements we mea-
sured are modest, yet statistically significant.
We considered only one simple form of tree in this
study: depth-one subtrees. We plan to test our en-
gine on a repository of treelets of arbitrary depth. In
theory, there is not much to change in our engine
to account for such units and it would offer an al-
ternative to the system proposed recently by (Liu et
al., 2005), which performs translations by recycling
a collection of tree-string-correspondence (TSC) ex-
amples.
References
Nicola Bertoldi, Roldano Cattoni, Mauro Cettolo, and
Marcello Federico. 2004. The ITC-irst statistical ma-
chine translation system for IWSLT-2004. In IWSLT,
pages 51?58, Kyoto, Japan.
Didier Bourigault and Ce?cile Fabre. 2000. Ap-
proche linguistique pour l?analyse syntaxique de cor-
pus. Cahiers de Grammaire, (25):131?151. Toulouse
le Mirail.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In 43rd ACL, pages
263?270, Ann Arbor, Michigan, USA.
Yuang Ding and Martha Palmer. 2004. Automatic learn-
ing of parallel dependency treelet pairs. In Proceed-
ings of the first International Joint Conference on NLP.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In 43rd ACL, pages 541?548, Ann
Arbor, Michigan, June.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL 2004, pages 105?112,
Boston, Massachusetts, USA, May 2 - May 7. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based SMT. In Proceedings of AMTA,
pages 115?124.
Zhanyi Liu, Haifeng Wang, and Hua Wu. 2005.
Example-based machine translation based on tsc and
statistical generation. In Proceedings of MT Summit
X, pages 25?32, Phuket, Thailand.
Evgeny Matusov, Stephan Kanthak, and Hermann Ney.
2005. Efficient statistical machine translation with
constraint reordering. In 10th EAMT, pages 181?188,
Budapest, Hongary, May 30-31.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In 42nd ACL, pages 653?660, Barcelona,
Spain.
Franz Joseph Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of ACL,
pages 440?447, Hongkong, China.
Franz Joseph Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the ACL,
pages 295?302.
Franz Joseph Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Daniel Ortiz-Mart??nez, Ismael Garcia?-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Pro-
ceedings of MT Summit X, pages 141?148, Phuket,
Thailand, Sep.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In 40th ACL, pages 311?
318, Philadelphia, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In 43rd ACL, pages 271?279, Ann Ar-
bor, Michigan, June.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne Mauser.
2005. Translating with non-contiguous phrases. In
HLT/EMNLP, pages 755?762, Vancouver, British
Columbia, Canada, Oct.
Andreas Stolcke. 2002. Srilm - an Extensible Language
Modeling Toolkit. In Proceedings of ICSLP, Denver,
Colorado, Sept.
Stephan Vogel, Ying Zhang, Fei Huang, Alicai Trib-
ble, Ashish Venugopal, Bing Zao, and Alex Waibel.
2003. The CMU Statistical Machine Translation Sys-
tem. InMachine Translation Summit IX, New Orleans,
Louisina, USA, Sep.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of the HLT/NAACL, pages 257?264, Boston,
MA, May.
46
Proceedings of the Workshop on Statistical Machine Translation, pages 126?129,
New York City, June 2006. c?2006 Association for Computational Linguistics
Mood at work: Ramses versus Pharaoh
Alexandre Patry, Fabrizio Gotti and Philippe Langlais
RALI ? DIRO
Universite? de Montre?al
{patryale,gottif,felipe}@iro.umontreal.ca
Abstract
We present here the translation system we
used in this year?s WMT shared task. The
main objective of our participation was
to test RAMSES, an open source phrase-
based decoder. For that purpose, we used
the baseline system made available by the
organizers of the shared task1 to build the
necessary models. We then carried out a
pair-to-pair comparison of RAMSES with
PHARAOH on the six different translation
directions that we were asked to perform.
We present this comparison in this paper.
1 Introduction
Phrase-based (PB) machine translation (MT) is now
a popular paradigm, partly because of the relative
ease with which we can automatically create an ac-
ceptable translation engine from a bitext. As a mat-
ter of fact, deriving such an engine from a bitext con-
sists in (more or less) gluing together dedicated soft-
ware modules, often freely available. Word-based
models, or the so-called IBM models, can be trained
using the GIZA or GIZA++ toolkits (Och and Ney,
2000). One can then train phrase-based models us-
ing the THOT toolkit (Ortiz-Mart??nez et al, 2005).
For their part, language models currently in use in
SMT systems can be trained using packages such as
SRILM (Stolcke, 2002) and the CMU-SLM toolkit
(Clarkson and Rosenfeld, 1997).
1www.statmt.org/wmt06/shared-task/
baseline.html
Once all the models are built, one can choose
to use PHARAOH (Koehn, 2004), an efficient full-
fledged phrase-based decoder. We only know of
one major drawback when using PHARAOH: its
licensing policy. Indeed, it is available for non-
commercial use in its binary form only. This
severely limits its use, both commercially and sci-
entifically (Walker, 2005).
For this reason, we undertook the design of a
generic architecture called MOOD (Modular Object-
Oriented Decoder), especially suited for instantiat-
ing SMT decoders. Two major goals directed our
design of this package: offering open source, state-
of-the-art decoders and providing an architecture to
easily build these decoders. This effort is described
in (Patry et al, 2006).
As a proof of concept that our framework (MOOD)
is viable, we attempted to use its functionalities to
implement a clone of PHARAOH, based on the com-
prehensive user manual of the latter. This clone,
called RAMSES, is now part of the MOOD distribu-
tion, which can be downloaded freely from the page
http://smtmood.sourceforge.net.
We conducted a pair-to-pair comparison between
the two engines that we describe in this paper. We
provide an overview of the MOOD architecture in
Section 2. Then we describe briefly RAMSES in Sec-
tion 3. The comparison between the two decoders in
terms of automatic metrics is analyzed in Section 4.
We confirm this comparison by presenting a man-
ual evaluation we conducted on an random sample
of the translations produced by both decoders. This
is reported in Section 5. We conclude in Section 6.
126
2 The MOOD Framework
A decoder must implement a specific combination of
two elements: a model representation and a search
space exploration strategy. MOOD is a framework
designed precisely to allow such a combination, by
clearly separating its two elements. The design of
the framework is described in (Patry et al, 2006).
MOOD is implemented with the C++ program-
ming language and is licensed under the Gnu Gen-
eral Public License (GPL)2. This license grants the
right to anybody to use, modify and distribute the
program and its source code, provided that any mod-
ified version be licensed under the GPL as well.
As explained in (Walker, 2005), this kind of license
stimulates new ideas and research.
3 MOOD at work: RAMSES
As we said above, in order to test our design, we
reproduced the most popular phrase-based decoder,
PHARAOH (Koehn, 2004), by following as faithfully
as possible its detailed user manual. The command-
line syntax RAMSES recognizes is compatible with
that of PHARAOH. The output produced by both
decoders are compatible as well and RAMSES can
also output its n-best lists in the same format as
PHARAOH does, i.e. in a format that the CARMEL
toolkit can parse (Knight and Al-Onaizan, 1999).
Switching decoders is therefore straightforward.
4 RAMSES versus PHARAOH
To compare the translation performances of both
decoders in a meaningful manner, RAMSES and
PHARAOH were given the exact same language
model and translation table for each translation ex-
periment. Both models were produced with the
scripts provided by the organizers. This means in
practice that the language model was trained using
the SRILM toolkit (Stolcke, 2002). The word align-
ment required to build the phrase table was pro-
duced with the GIZA++ package. A Viterbi align-
ment computed from an IBM model 4 (Brown et al,
1993) was computed for each translation direction.
Both alignments were then combined in a heuristic
way (Koehn et al, ). Each pair of phrases in the
2http://www.gnu.org/copyleft/gpl.html
model is given 5 scores, described in the PHARAOH
training manual.3
To tune the coefficients of the log-linear
combination that both PHARAOH and RAMSES
use when decoding, we used the organizers?
minimum-error-rate-training.perl
script. This tuning step was performed on the
first 500 sentences of the dedicated development
corpora. Inevitably, RAMSES differs slightly
from PHARAOH, because of some undocumented
embedded heuristics. Thus, we found appropriate
to tune each decoder separately (although with
the same material). In effect, each decoder does
slightly better (with BLEU) when it uses its own best
parameters obtained from tuning, than when it uses
the parameters of its counterpart.
Eight coefficents were adjusted this way: five for
the translation table (one for each score associated
to each pair of phrases), and one for each of the fol-
lowing models: the language model, the so-called
word penalty model and the distortion model (word
reordering model). Each parameter is given a start-
ing value and a range within which it is allowed to
vary. For instance, the language model coefficient?s
starting value is 1.0 and the coefficient is in the range
[0.5?1.5]. Eventually, we obtained two optimal con-
figurations (one for each decoder) with which we
translated the TEST material.
We evaluated the translations produced by both
decoders with the organizers? multi-bleu.perl
script, which computes a BLEU score (and displays
the n-gram precisions and brevity penalty used). We
report the scores we gathered on the test corpus of
2000 pairs of sentences in Table 1. Overall, both
decoders offer similar performances, down to the
n-gram precisions. To assess the statistical signifi-
cance of the observed differences in BLEU, we used
the bootstrapping technique described in (Zhang
and Vogel, 2004), randomly selecting 500 sentences
from each test set, 1000 times. Using a 95% con-
fidence interval, we determined that the small dif-
ferences between the two decoders are not statis-
tically significant, except for two tests. For the
direction English to French, RAMSES outperforms
PHARAOH, while in the German to English direc-
3http://www.statmt.org/wmt06/
shared-task/training-release-1.3.tgz
127
tion, PHARAOH is better. Whenever a decoder is
better than the other, Table 1 shows that it is at-
tributable to higher n-gram precisions; not to the
brevity penalty.
We further investigated these two cases by calcu-
lating BLEU for subsets of the test corpus sharing
similar sentence lengths (Table 2). We see that both
decoders have similar performances on short sen-
tences, but can differ by as much as 1% in BLEU on
longer ones. In contrast, on the Spanish-to-English
translation direction, where the two decoders offer
similar performances, the difference between BLEU
scores never exceeds 0.23%.
Expectedly, Spanish and French are much easier
to translate than German. This is because, in this
study, we did not apply any pre-processing strat-
egy that we know can improve performances, such
as clause reordering or compound-word splitting
(Collins et al, 2005; Langlais et al, 2005).
Table 2 shows that it does not seem much more
difficult to translate into English than from English.
This is surprising: translating into a morphologically
richer language should be more challenging. The
opposite is true for German here: without doing any-
thing specific for this language, it is much easier to
translate from German to English than the other way
around. This may be attributed in part to the lan-
guage model: for the test corpus, the perplexity of
the language models provided is 105.5 for German,
compared to 59.7 for English.
5 Human Evaluation
In an effort to correlate the objective metrics with
human reviews, we undertook the blind evaluation
of a sample of 100 pairwise translations for the three
Foreign language-to-English translation tasks. The
pairs were randomly selected from the 3064 trans-
lations produced by each engine. They had to be
different for each decoder and be no more than 25
words long.
Each evaluator was presented with a source sen-
tence, its reference translation and the translation
produced by each decoder. The last two were in ran-
dom order, so the evaluator did not know which en-
gine produced the translation. The evaluator?s task
was two-fold. (1) He decided whether one transla-
tion was better than the other. (2) If he replied ?yes?
D BLEU p1 p2 p3 p4 BP
es ? en
P 30.65 64.10 36.52 23.70 15.91 1.00
R 30.48 64.08 36.30 23.52 15.76 1.00
fr ? en
P 30.42 64.28 36.45 23.39 15.64 1.00
R 30.43 64.58 36.59 23.54 15.73 0.99
de ? en
P 25.15 61.19 31.32 18.53 11.61 0.99
R 24.49 61.06 30.75 17.73 10.81 1.00
en ? es
P 29.40 61.86 35.32 22.77 15.02 1.00
R 28.75 62.23 35.03 22.32 14.58 0.99
en ? fr
P 30.96 61.10 36.56 24.49 16.80 1.00
R 31.79 61.57 37.38 25.30 17.53 1.00
en ? de
P 18.03 52.77 22.70 12.45 7.25 0.99
R 18.14 53.38 23.15 12.75 7.47 0.98
Table 1: Performance of RAMSES and PHARAOH
on the provided test set of 2000 pairs of sentences
per language pair. P stands for PHARAOH, R for
RAMSES. All scores are percentages. pn is the n-
gram precision and BP is the brevity penalty used
when computing BLEU.
in test (1), he stated whether the best translation was
satisfactory while the other was not. Two evalua-
tors went through the 3 ? 100 sentence pairs. None
of them understands German; subject B understands
Spanish, and both understand French and English.
The results of this informal, yet informative exercise
are reported in Table 3.
Overall, in many cases (64% and 48% for subject
A and B respectively), the evaluators did not pre-
fer one translation over the other. On the Spanish-
and French-to-English tasks, both subjects slightly
preferred the translations produced by RAMSES. In
about one fourth of the cases where one translation
was preferred did the evaluators actually flag the se-
lected translation as significantly better.
6 Discussion
We presented a pairwise comparison of two de-
coders, RAMSES and PHARAOH. Although RAM-
SES is roughly twice as slow as PHARAOH, both de-
128
Test set [0,15] [16,25] [26,?[
en ? fr (P) 33.52 30.65 30.39
en ? fr (R) 33.78 31.19 31.35
de ? en (P) 29.74 24.30 24.76
de ? en (R) 29.85 23.92 23.78
es ? en (P) 34.23 28.32 30.60
es ? en (R) 34.46 28.39 30.40
Table 2: BLEU scores on subsets of the test corpus
filtered by sentence length ([min words, max words]
intervals), for Pharaoh and Ramses.
Preferred Improved
P R No P R
es ? en
subject A 13 16 71 6 1
subject B 23 31 46 3 8
fr ? en
subject A 18 19 63 5 3
subject B 20 21 59 8 8
de ? en
subject A 24 18 58 5 9
subject B 30 31 39 3 3
Total 128 136 336 30 32
Table 3: Human evaluation figures. The column
Preferred indicates the preference of the subject
(Pharaoh, Ramses or No preference). The column
Improved shows when a subject did prefer a trans-
lation and also said that the preferred translation was
correct while the other one was not.
coders offer comparable performances, according to
automatic and informal human evaluations.
Moreover, RAMSES is the product of clean frame-
work: MOOD, a solid tool for research projects. Its
code is open source and the architecture is modular,
making it easier for researchers to experiment with
SMT. We hope that the availability of the source
code and the clean design of MOOD will make it a
useful platform to implement new decoders.
Acknowledgments
We warmly thanks Elliott Macklovitch for his par-
ticipation in the manual annotation task. This work
has been partially funded by an NSERC grant.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263?311.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-cambridge toolkit. In Proc.
of Eurospeech, pages 2707?2710, Rhodes, Greece.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of the 43rd ACL, pages 531?540, Ann Arbor, MI.
K. Knight and Y. Al-Onaizan, 1999. A Primer on
Finite-State Software for Natural Language Process-
ing. www.isi.edu/licensed-sw/carmel.
P. Koehn, F. Joseph Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proc. of HLT, Edmon-
ton, Canada.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based SMT. In Proc. of the 6th AMTA, pages
115?124, Washington, DC.
P. Langlais, G. Cao, and F. Gotti. 2005. RALI: SMT
shared task system description. In 2nd ACL workshop
on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, MI.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proc. of ACL, pages 440?447,
Hongkong, China.
D. Ortiz-Mart??nez, I. Garcia?-Varea, and F. Casacuberta.
2005. Thot: a toolkit to train phrase-based statistical
translation models. In Proc. of MT Summit X, pages
141?148, Phuket, Thailand.
A. Patry, F. Gotti, and P. Langlais. 2006. MOOD
a modular object-oriented decoder for statistical ma-
chine translation. In Proc. of LREC, Genoa, Italy.
A. Stolcke. 2002. SRILM - an Extensible Language
Modeling Toolkit. In Proc. of ICSLP, Denver, USA.
D.J. Walker. 2005. The open ?a.i.? kitTM: General ma-
chine learning modules from statistical machine trans-
lation. In Workshop of MT Summit X, ?Open-Source
Machine Translation?, Phuket, Thailand.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proc. of the 10th TMI, Baltimore, MD.
129
Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 617?625,
Beijing, August 2010
Revisiting Context-based Projection Methods for
Term-Translation Spotting in Comparable Corpora
Audrey Laroche
OLST ? De?p. de linguistique et de traduction
Universite? de Montre?al
audrey.laroche@umontreal.ca
Philippe Langlais
RALI ? DIRO
Universite? de Montre?al
felipe@iro.umontreal.ca
Abstract
Context-based projection methods for
identifying the translation of terms in
comparable corpora has attracted a lot of
attention in the community, e.g. (Fung,
1998; Rapp, 1999). Surprisingly, none of
those works have systematically investi-
gated the impact of the many parameters
controlling their approach. The present
study aims at doing just this. As a test-
case, we address the task of translating
terms of the medical domain by exploit-
ing pages mined from Wikipedia. One in-
teresting outcome of this study is that sig-
nificant gains can be obtained by using an
association measure that is rarely used in
practice.
1 Introduction
Identifying translations of terms in comparable
corpora is a challenge that has attracted many re-
searchers. A popular idea that emerged for solv-
ing this problem is based on the assumption that
the context of a term and its translation share sim-
ilarities that can be used to rank translation candi-
dates (Fung, 1998; Rapp, 1999). Many variants of
this idea have been implemented.
While a few studies have investigated pattern
matching approaches to compare source and tar-
get contexts (Fung, 1995; Diab and Finch, 2000;
Yu and Tsujii, 2009), most variants make use of
a bilingual lexicon in order to translate the words
of the context of a term (often called seed words).
De?jean et al (2005) instead use a bilingual the-
saurus for translating these.
Another distinction between approaches lies in
the way the context is defined. The most com-
mon practice, the so-called window-based ap-
proach, defines the context words as those cooc-
curing significantly with the source term within
windows centered around the term.1 Some studies
have reported gains by considering syntactically
motivated co-occurrences. Yu and Tsujii (2009)
propose a resource-intensive strategy which re-
quires both source and target dependency parsers,
while Otero (2007) investigates a lighter approach
where a few hand coded regular expressions based
on POS tags simulate source parsing. The latter
approach only requires a POS tagger of the source
and the target languages as well as a small par-
allel corpus in order to project the source regular
expressions.
Naturally, studies differ in the way each co-
occurrence (either window or syntax-based) is
weighted, and a plethora of association scores
have been investigated and compared, the like-
lihood score (Dunning, 1993) being among the
most popular. Also, different similarity measures
have been proposed for ranking target context vec-
tors, among which the popular cosine measure.
The goal of the different authors who inves-
tigate context-projection approaches also varies.
Some studies are tackling the problem of iden-
tifying the translation of general words (Rapp,
1999; Otero, 2007; Yu and Tsujii, 2009) while
others are addressing the translation of domain
specific terms. Among the latter, many are trans-
lating single-word terms (Chiao and Zweigen-
baum, 2002; De?jean et al, 2005; Prochasson et
1A stoplist is typically used in order to prevent function
words from populating the context vectors.
617
al., 2009), while others are tackling the translation
of multi-word terms (Daille and Morin, 2005).
The type of discourse might as well be of con-
cern in some of the studies dedicated to bilingual
terminology mining. For instance, Morin et al
(2007) distinguish popular science versus scien-
tific terms, while Saralegi et al (2008) target pop-
ular science terms only.
The present discussion only focuses on a few
number of representative studies. Still, it is al-
ready striking that a direct comparison of them
is difficult, if not impossible. Differences in re-
sources being used (in quantities, in domains,
etc.), in technical choices made (similarity mea-
sures, context vector computation, etc.) and in ob-
jectives (general versus terminological dictionary
extraction) prevent one from establishing a clear
landscape of the various approaches.
Indeed, many studies provide some figures that
help to appreciate the influence of some param-
eters in a given experimental setting. Notably,
Otero (2008) studies no less than 7 similarity mea-
sures for ranking context vectors while comparing
window and syntax-based methods. Morin et al
(2007) consider both the log-likelihood and the
mutual information association scores as well as
the Jaccard and the cosine similarity measures.
Ideally, a benchmark on which researchers
could run their translation finder would ease the
comparison of the different approaches. However,
designing such a benchmark that would satisfy the
evaluation purposes of all the researchers is far too
ambitious a goal for this contribution. Instead, we
investigate the impact of some major factors influ-
encing projection-based approaches on a task of
translating 5,000 terms of the medical domain (the
most studied domain), making use of French and
English Wikipedia pages extracted monolingually
thanks to an information retrieval engine. While
the present work does not investigate all the pa-
rameters that could potentially impact results, we
believe it constitutes the most complete and sys-
tematic comparison made so far with variants of
the context-based projection approach.
In the remainder of this paper, we describe the
projection-based approach to translation spotting
in Section 2 and detail the parameters that directly
influence its performance. The experimental pro-
tocol we followed is described in Section 3 and
we analyze our results in Section 4. We discuss
the main results in the light of previous work and
propose some future avenues in Section 5.
2 Projection-based variants
The approach we investigate for identifying term
translations in comparable corpora is similar to
(Rapp, 1999) and many others. We describe in the
following the different steps it encompasses and
the parameters we are considering in the light of
typical choices made in the literature.
2.1 Approach
Step 1 A comparable corpus is constructed for
each term to translate. In this study, the source and
target corpora are sets of Wikipedia pages related
to the source term (S) and its reference transla-
tion (T ) respectively (see Section 3.1). The degree
of corpus preprocessing varies greatly from one
study to another. Complex linguistic tools such
as terminological extractors (Daille and Morin,
2005), parsers (Yu and Tsujii, 2009) or lemma-
tizers (Rapp, 1999) are sometimes used.
In our case, the only preprocessing that takes
place is the deletion of the Wikipedia symbols per-
taining to its particular syntax (e.g. [[ ]]).2 It is
to be noted that, for the sake of simplicity and gen-
erality, our implementation does not exploit inter-
language links nor structural elements specific to
Wikipedia documents, as opposed to (Yu and Tsu-
jii, 2009).
Step 2 A context vector vs for the source term
S is built (see Figure 1 for a made-up example).
This vector contains the words that are in the con-
text of the occurrences of S and are strongly cor-
related to S. The definition of ?context? is one of
the parameters whose best value we want to find.
Context length can be based on a number of units,
for instance 3 sentences (Daille and Morin, 2005),
windows of 3 (Rapp, 1999) or 25 words (Prochas-
son et al, 2009), etc. It is an important parame-
ter of the projection-based approach. Should the
context length be too small, we would miss words
that would be relevant in finding the translation.
On the other hand, if the context is too large, it
2We used a set of about 40 regular expressions to do this.
618
might contain too much noise. At this step, a sto-
plist made of function words is applied in order
to filter out context words and reduce noise in the
context vector.
Additionally, an association measure is used to
score the strength of correlation between S and
the words in its contexts; it serves to normalize
corpus frequencies. Words that have a high as-
sociation score with S are more prominent in the
context vector. The association measure is the sec-
ond important parameter we want to study. As al-
ready noted, most authors use the log-likelihood
ratio to measure the association between collo-
cates; some, like (Rapp, 1999), informally com-
pare the performance of a small number of associ-
ation measures, or combine the results obtained
with different association measures (Daille and
Morin, 2005).
Figure 1: Step 2
Step 3 Words in vs are projected into the target
language with the help of the bilingual seed lexi-
con (Figure 2). Each word in vs which is present
in the bilingual lexicon is translated, and those
translations define the projected context vector vp.
Words that are not found in the bilingual lexicon
are simply ignored. The size of the seed lexi-
con and its content are therefore two important
parameters of the approach. In previous studies,
seed lexicons vary between 16,000 (Rapp, 1999)
and 65,000 (De?jean et al, 2005) entries, a typical
size being around 20,000 (Fung, 1998; Chiao and
Zweigenbaum, 2002; Daille and Morin, 2005).
Figure 2: Step 3
Step 4 Context vectors vt are computed for each
candidate term in the target language corpus (Fig-
ure 3). The dimension of the target-vector space
is defined to be the one induced by the projec-
tion mechanism described in Step 3. The con-
text vector vt of each candidate term is computed
as in Step 2. Therefore, in Step 4, the parame-
ters of context definition and association measure
are important and take the same values as those
in Step 2. Note that in this study, on top of all
single terms, we also consider target bigrams as
potential candidates (99.5 % of our reference tar-
get terms are composed of at most two words).
As such, our method can handle complex terms
(of up to two words), as opposed to most previ-
ous studies, without having to resort to a separate
terminological extraction as in (Daille and Morin,
2005).
Figure 3: Step 4
Step 5 Context vectors vt are ranked in decreas-
ing order of their similarity with vp (Figure 4).
The similarity measure between context vectors
varies among studies: city-block measure (Rapp,
1999), cosine (Fung, 1998; Chiao and Zweigen-
baum, 2002; Daille and Morin, 2005; Prochasson
et al, 2009), Dice or Jaccard indexes (Chiao and
Zweigenbaum, 2002; Daille and Morin, 2005),
etc. It is among the parameters whose effect we
experimentally evaluate.
Figure 4: Step 5
2.2 Parameters studied
The five steps we described involve many param-
eters, the values of which can influence at varying
degrees the performance of a translation spotter.
In the current study, we considered the following
parameter values.
Context We considered contexts defined as the
current sentence or the current paragraph involv-
619
ing S. We also considered windows of 5 and 25
words on both sides of S.
Association measure Following the aforemen-
tioned studies, we implemented these popular
measures: pointwise mutual information (PMI),
log-likelihood ratio (LL) and chi-square (?2). We
also implemented the discounted log-odds (LO)
described by (Evert, 2005, p. 86) in his work on
collocation mining. To our knowledge, this asso-
ciation measure has not been used yet in transla-
tion spotting. It is computed as:
odds-ratiodisc = log (O11 +
1
2)(O22 + 12)
(O12 + 12)(O21 + 12)
where Oij are the cells of the 2?2 contingency
matrix of a word token s cooccurring with the
term S within a given window size.3
Similarity measure We implemented four mea-
sures: city-block, cosine, as well as Dice and Jac-
card indexes (Jurafsky and Martin, 2008, p. 666).
Our implementations of Dice and Jaccard are
identical to the DiceMin and JaccardMin similar-
ity measures reported in (Otero, 2008) and which
outperformed the other five metrics he tested.
Seed lexicon We investigated the impact of both
the size of the lexicon and its content. We started
our study with a mixed lexicon of around 5,000
word entries: roughly 2,000 of them belong to
the medical domain, while the other entries be-
long to the general language. We also considered
mixed lexicons of 7,000, 9,000 and 11,000 entries
(where 2,000 entries are related to the medical do-
main), as well as a 5,000-entry general language
only lexicon.
2.3 Cognate heuristic
Many authors are embedding heuristics in order
to improve their approach. For instance, Chiao
and Zweigenbaum (2002) propose to integrate a
reverse translation spotting strategy in order to im-
prove precision. Prochasson et al (2009) boost
the strength of context words that happen to be
transliterated in the other language. A somehow
3For instance, O21 stands for the number of windows
containing S but not s.
generalized version of this heuristic has been de-
scribed in (Shao and Ng, 2004).
In this work, we examine the performance
of the best configuration of parameters we
found, combined with a simple heuristic based
on graphic similarity between source and tar-
get terms, similar to the orthographic features in
(Haghighi et al, 2008)?s generative model. This
is very specific to our task where medical terms
often (but not always) share Latin or Greek roots,
such as microvillosite?s in French and microvilli in
English.
In this heuristic, translation candidates which
are cognates of the source term are ranked first
among the list of translation candidates. In our
implementation, two words are cognates if their
first four characters are identical (Simard et al,
1992). One interesting note concerns the word-
order mismatch typically observed in French and
English complex terms, such as in ADN mitochon-
drial (French) and mitochondrial DNA (English).
We do treat this case adequately.
3 Experimental protocol
In order to pinpoint the best configuration of val-
ues for the parameters identified in Section 2.2,
four series of experiments were carried out. In
all of them, the task consists of spotting transla-
tion candidates for each source language term us-
ing the resources4 described below. The quality of
the results is evaluated with the help of the metrics
described in Section 3.2.
3.1 Resources
Corpora The comparable corpora are made of
the (at most) 50 French and English Wikipedia
documents that are the most relevant to the source
term and to its reference translation respectively.
These documents are retrieved with the NLGbAse
Information Retrieval tool.5 The average token
count of all the 50-document corpora as well as
the average frequency of the source and target
terms in these corpora for our four series of ex-
periments are listed in Table 1.
4Our resources are available at http://olst.ling.
umontreal.ca/?audrey/coling2010/. They wereacquired as described in (Rubino, 2009).
5http://nlgbase.org/
620
Experiment
1 2 3 4
Tokenss 89,431 73,809 42,762 90,328
Tokenst 52,002 27,517 12,891 38,929
|S| 296 184 66 306
|T | 542 255 104 404
Table 1: 50-document corpora averages
The corpora are somewhat small (most corpora
in previous studies are made of at least a million
words). We believe this is more representative of
a task where we try to translate domain specific
terms. Some of the Wikipedia documents may
contain a handful of parallel sentences (Smith et
al., 2010), but this information is not used in our
approach. The construction of the corpus involves
a bias in that the reference translations are used
to obtain the most relevant target language docu-
ments. However, since our objective is to com-
pare the relative performance of different sets of
parameters, this does not affect our results. In
fact, as per (De?jean et al, 2005) (whose compa-
rable corpora are English and German abstracts),
the use of such an ?ideal? corpus is common (as in
(Chiao and Zweigenbaum, 2002), where the cor-
pus is built from a specific query).
Seed lexicon The mixed seed lexicon we use is
taken from the Heymans Institute of Pharmacol-
ogy?s Multilingual glossary of technical and pop-
ular medical terms.6 Random general language
entries from the FreeLang7 project are also in-
corporated into the lexicon for some of our exper-
iments.
Reference translations The test set is com-
posed of 5,000 nominal single and multi-word
pairs of French and English terms from the MeSH
(Medical Subject Heading) thesaurus.8
3.2 Evaluation metrics
The performance of each set of parameters in the
experiments is evaluated with Top N precision
(PN ), recall (RN ) and F-measure (FN ), as well
as Mean Average Precision (MAP). Precision is
6http://users.ugent.be/?rvdstich/
eugloss/welcome.html
7http://www.freelang.net/
8http://www.nlm.nih.gov/mesh/
the number of correct translations (at most 1 per
source term) divided by the number of terms for
which our system gave at least one answer; recall
is equal to the ratio of correct translations to the
total number of terms. F-measure is the harmonic
mean of precision and recall:
F-measure = 2? (precision? recall)(precision+ recall)
The MAP represents in a single figure the qual-
ity of a system according to various recall levels
(Manning et al, 2008, p. 147?148):
MAP(Q) = 1|Q|
j=1?
|Q|
1
mj
k=1?
mj
Precision(Rjk)
where |Q| is the number of terms to be trans-
lated, mj is the number of reference translations
for the jth term (always 1 in our case), and
Precision(Rjk) is 0 if the reference translation
is not found for the jth term or 1/r if it is (r is the
rank of the reference translation in the translation
candidates).
4 Experiments
In Experiment 1, 500 single and multi-word terms
must be translated from French to English using
each of the 64 possible configurations of these pa-
rameters: context definition, association measure
and similarity measure. In Experiment 2, we sub-
mit to the 8 best variants 1,500 new terms to de-
termine with greater confidence the best 2, which
are again tested on the last 3,000 of the test terms
(Experiment 3). In Experiment 4, using 1,350 fre-
quent terms, we examine the effects of seed lex-
icon size and specificity and we apply a heuristic
based on cognates.
4.1 Experiment 1
The results of the first series of experiments on
500 terms can be analysed from the point of view
of each of the parameters whose values varied
among 64 configurations (Section 2.2). The max-
imal MAP reached for each parametric value is
given in Table 2.
The most notable result is that, of the four as-
sociation measures studied, the log-odds ratio is
621
Param. Value Best MAP In config.
ass
oc
iat
ion LO 0.536 sentence cosineLL 0.413 sentence Dice
PMI 0.299 sentence city-block
?2 0.179 sentence Dice
sim
ila
rity cosine 0.536 sentence LODice 0.520 sentence LO
Jaccard 0.520 sentence LO
city-block 0.415 sentence LO
co
nte
xt sentence 0.536 cosine LOparagraph 0.460 cosine LO
25 words 0.454 cosine LO
5 words 0.361 Dice LO
Table 2: Best MAP in Experiment 1
significantly superior to the others in every vari-
ant. There is as much as 34 % difference be-
tween LO and other measures for Top 1 recall.
This is interesting since most previous works use
the log-likelihood, and none use LO. Our best re-
sults for LO (with cosine sentence) and LL (with
Dice sentence) are in Table 3. Note that the oracle
recall is 93 % (7 % of the source and target terms
were not in the corpus).
Assoc. R1 R20 P1 P20 F1 F20 MAP
LO 39.4 84.8 42.3 91.0 40.8 87.8 0.536
LL 29.0 75.2 31.3 81.0 30.1 78.0 0.413
Table 3: Best LO and LL configurations scores
Another relevant observation is that the param-
eters interact with each other. When the similar-
ity measure is cosine, PMI results in higher Top 1
F-scores than LL, but the Top 20 F-scores are bet-
ter with LL. PMI is better than LL when using
city-block as a similarity measure, but LL is better
than PMI when using Dice and Jaccard indexes.
?2 gives off the worst MAP in all but 4 of the 64
parametric configurations.
As for similarity measures, the Dice and Jac-
card indexes have identical performances, in ac-
cordance with the fact that they are equivalent
(Otero, 2008).9 Influences among parameters are
also observable in the performance of similarity
measures. When the association measure is LO,
the cosine measure gives slightly better Top 1 F-
9For this reason, whenever ?Dice? is mentioned from this
point on, it also applies to the Jaccard index.
scores, while the Dice index performs slightly bet-
ter with regards to Top 20 F-scores. Dice is better
when the association measure is LL, with a Top 1
F-score gain of about 15 % compared to the co-
sine.
Again, in the case of context definitions, rel-
ative performances depend on the other param-
eters and on the number of top translation can-
didates considered. With LO, sentence contexts
have the highest Top 1 F-measures, while Top 20
F-measures are highest with paragraphs, and 5-
word contexts are the worst.
4.2 Experiment 2
The best parametric values found in Experiment 1
were put to the test on 1,500 different test terms
for scale-up verification. Along with LO, which
was the best association measure in the previous
experiment, we used LL to double-check its rel-
ative inefficiency. For all of the 8 configurations
evaluated, LL?s recall, precision and MAP remain
worse than LO?s. In particular, LO?s MAP scores
with the cosine measure are more than twice as
high as LL?s (respectively 0.33 and 0.124 for sen-
tence contexts). As in Experiment 1, the Dice
index is significantly better for LL compared to
the cosine, but not for LO. In the case of LO,
sentence contexts have better Top 1 performances
than paragraphs, and vice versa for Top 20 per-
formances (see Table 4; oracle recall is 93.5 %).
Hence, paragraph contexts would be more useful
in tasks consisting of proposing candidate transla-
tions to lexicographers, while sentences would be
more appropriate for automatic bilingual lexicon
construction.
Ctx R1 R20 P1 P20 F1 F20 MAP
Sent. 23.1 63.9 27.8 76.6 25.23 69.68 0.336
Parag. 20.1 70.0 22.9 79.7 21.41 74.54 0.325
Table 4: LO Dice configuration scores
The cosine and Dice similarity measures have
similar performances when LO is used. Moreover,
we observe the effect of source and target term
frequencies in corpus. As seen in Table 1, these
frequencies are on average about half smaller in
Experiment 2 as they are in Experiment 1, which
results in significantly lower performances for all
622
8 variants. As Figure 5 shows for the variant
LO cosine sentence, terms that are more frequent
have a greater chance of being correctly translated
at better ranks.
Figure 5: Average rank of correct translation
according to average source term frequency
However, the relative performance of the differ-
ent parametric configurations still holds.
4.3 Experiment 3
In Experiment 3, we evaluate the two best config-
urations from Experiment 2 with 3,000 new terms
in order to verify the relative performance of the
cosine and Dice similarity measures. As Table 5
shows, cosine has slightly better Top 1 figures,
while Dice is a little better when considering the
Top 20 translation candidates. Therefore, as pre-
viously mentioned, the choice of similarity mea-
sure (cosine or Dice) should depend on the goal
of translation spotting. Note that the scores in Ex-
periment 3 are much lower than those of Experi-
ments 1 and 2 because of low term frequencies in
the corpus (see Table 1 and Figure 5). Also, oracle
recall is only 71.1 %.
Sim. R1 R20 P1 P20 F1 F20 MAP
Cosine 9.8 28.1 20.7 59.4 13.3 38.15 0.232
Dice 9.4 28.9 19.8 61.2 12.75 39.26 0.286
Table 5: LO sentence configuration scores
4.4 Experiment 4
In the last series of experiments, we examine the
influence of the bilingual seed lexicon specificity
and size, using the 1,350 terms which have source
and target frequencies ? 30 from the 1,500 and
3,000 sets used in Experiments 2 and 3 (oracle re-
call: 100 %). We tested the different lexicons (see
Section 2.2) on the 4 parametric configurations
made of sentence contexts, LO or LL association
measures, and cosine or Dice similarity measures.
Yet again, LO is better than LL. MAP scores for
LO in all variants are comprised in [0.466?0.489];
LL MAPs vary between 0.135 and 0.146 when the
cosine is used and between 0.348 and 0.380 when
the Dice index is used.
According to our results, translation spotting
is more accurate when the seed lexicon contains
(5,000) entries from both the medical domain
and general language instead of general language
words only, but only by a very small margin.
Table 6 shows the results for the configuration
LO cosine sentence. The fact that the difference
Lex. R1 R20 P1 P20 F1 F20 MAP
Gen. + med. 39.3 87.0 39.6 87.6 39.4 87.3 0.473
Gen. only 38.8 88.1 39.0 88.5 38.9 88.3 0.471
Table 6: LO cosine sentence configuration scores
is so small could be explained by our resources?
properties. The reference translations from MeSH
contain terms that are also used in other domains
or in the general language, e.g. terms from the
category ?people? (Ne?ve?ol and Ozdowska, 2006).
Wikipedia documents retrieved by using those ref-
erences may in turn not belong to the medical do-
main, in which case medical terms from the seed
lexicon are not appropriate. Still, the relatively
good performance of the general language-only
lexicon supports (De?jean et al, 2005, p. 119)?s
claim that general language words are useful when
spotting translations of domain specific terms,
since the latter can appear in generic contexts.
Lexicon sizes tested are 5,000 (the mixed lex-
icon used in previous experiments), 7,000, 9,000
and 11,000 entries. The performance (based on
MAP) is better when 7,000- and 9,000-entry lexi-
cons are used, because more source language con-
text words can be taken into account. However,
when the lexicon reaches 11,000, Top 1 MAP
scores and F-measures are slightly lower than
those obtained with the 7,000-entry one. This may
happen because the lexicon is increased with gen-
eral language words; 9,000 of the 11,000 entries
623
are not from the medical domain, making it harder
for the context words to be specific. It would be
interesting to study the specificity of context vec-
tors built from the source corpus. Still, the dif-
ferences in scores are small, as Table 7 shows
(see Table 6 for the results obtained with 5,000
entries). This is because, in our implementation,
context vector size is limited to 20, as in (Daille
and Morin, 2005), in order to reduce processing
time. The influence of context vector sizes should
be studied.
Lex. size R1 R20 P1 P20 F1 F20 MAP
7,000 41.5 88.8 41.6 89.1 41.5 88.9 0.488
9,000 40.9 89.3 41.1 89.7 41.0 89.5 0.489
11,000 40.1 89.8 40.2 90.1 40.1 89.9 0.484
Table 7: LO cosine sentence configuration scores
The parameters related to the seed lexicon do
not have as great an impact on the performance
as the choice of association measure does: the
biggest difference in F-measures for Experiment 4
is 2.9 %. At this point, linguistic-based heuris-
tics such as graphic similarity should be used
to significantly increase performance. We ap-
plied the cognate heuristic (Section 2.3) on the
Top 20 translation candidates given by the vari-
ant LO sentence 9,000-entry lexicon using cosine
and Dice similarity measures. Without the heuris-
tic, Top 1 performances are better with cosine,
while Dice is better for Top 20. Applying the cog-
nate heuristic makes the Top 1 precision go from
41.1 % to 55.2 % in the case of cosine, and from
39.6 % to 53.9 % in the case of Dice.
5 Discussion
Our results show that using the log-odds ratio as
the association measure allows for significantly
better translation spotting than the log-likelihood.
A closer look at the translation candidates ob-
tained when using LL, the most popular asso-
ciation measure in projection-based approaches,
shows that they are often collocates of the refer-
ence translation. Therefore, LL may fare better in
an indirect approach, like the one in (Daille and
Morin, 2005).
Moreover, we have seen that the cosine simi-
larity measure and sentence contexts give more
correct top translation candidates, at least when
LO is used. Indeed, the values of the different
parameters influence one another in most cases.
Parameters related to the seed lexicon (size, do-
main specificity) are not of great influence on the
performance, but this may in part be due to our
resources and the way they were built.
The highest Top 1 precision, 55.2 %, was
reached with the following parameters: sentence
contexts, LO, cosine and a 9,000-entry mixed lex-
icon, with the use of a cognate heuristic.
In future works, other parameters which in-
fluence the performance will be studied, among
which the use of a terminological extractor to treat
complex terms (Daille and Morin, 2005), more
contextual window configurations, and the use of
syntactic information in combination with lexical
information (Yu and Tsujii, 2009). It would also
be interesting to compare the projection-based
approaches to (Haghighi et al, 2008)?s genera-
tive model for bilingual lexicon acquisition from
monolingual corpora.
One latent outcome of this work is that
Wikipedia is surprisingly suitable for mining med-
ical terms. We plan to check its adequacy for
other domains and verify that LO remains a bet-
ter association measure for different corpora and
domains.
Acknowledgments
We are deeply grateful to Raphae?l Rubino who
provided us with the data material we have been
using in this study. We thank the anonymous re-
viewers for their suggestions.
References
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In 19th Inter-
national Conference on Computational Linguistics,
pages 1208?1212.
Daille, Be?atrice and Emmanuel Morin. 2005. French-
English terminology extraction from comparable
corpora. In 2nd International Joint Conference on
Natural Language Processing, pages 707?718.
De?jean, Herve?, E?ric Gaussier, Jean-Michel Renders,
and Fatiha Sadat. 2005. Automatic processing of
624
multilingual medical terminology: Applications to
thesaurus enrichment and cross-language informa-
tion retrieval. Artificial Intelligence in Medicine,
33(2):111?124. Elsevier Science, New York.
Diab, Mona and Steve Finch. 2000. A statistical word-
level translation model for comparable corpora. In
Proceedings of the Conference on Content-Based
Multimedia Information Access.
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Evert, Stefan. 2005. The Statistics of Word Cooccur-
rences. Word Pairs and Collocations. Ph.D. thesis,
Universita?t Stuttgart.
Fung, Pascale. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In 33rd Annual Meeting
of the Association for Computational Linguistics,
pages 236?243.
Fung, Pascale. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In 3rd Conference of the Associa-
tion for Machine Translation in the Americas, pages
1?17.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Human Language
Technology and Association for Computational Lin-
guistics, pages 771?779.
Jurafsky, Daniel and James H. Martin. 2008. Speech
and Language Processing. Prentice-Hall.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Morin, Emmanuel, Be?atrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining ? using brain, not brawn comparable cor-
pora. In 45th Annual Meeting of the Association for
Computational Linguistics, pages 664?671.
Ne?ve?ol, Aure?lie and Sylwia Ozdowska. 2006. Termi-
nologie me?dicale bilingue anglais/franc?ais: usages
cliniques et bilingues. Glottopol, 8.
Otero, Pablo Gamallo. 2007. Learning bilingual lexi-
cons from comparable English and Spanish corpora.
In Machine Translation Summit 2007, pages 191?
198.
Otero, Pablo Gamallo. 2008. Evaluating two different
methods for the task of extracting bilingual lexicons
from comparable corpora. In 1st Workshop Building
and Using Comparable Corpora.
Prochasson, Emmanuel, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Machine Translation Summit XII, pages 284?291.
Rapp, Reinhard. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 66?70.
Rubino, Raphae?l. 2009. Exploring context variation
and lexicon coverage in projection-based approach
for term translation. In Proceedings of the Stu-
dent Research Workshop associated with RANLP?
09, pages 66?70.
Saralegi, X., I. San Vicente, and A. Gurrutxaga. 2008.
Automatic extraction of bilingual terms from com-
parable corpora in a popular science domain. In
1st Workshop Building and Using Comparable Cor-
pora.
Shao, Li and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In 20th Inter-
national Conference on Computational Linguistics,
pages 618?624.
Simard, Michel, George Foster, and Pierre Isabelle.
1992. Using cognates to align sentences in bilin-
gual corpora. In 4th Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 67?81.
Smith, Jason R., Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
ACL, pages 403?411.
Yu, Kun and Junichi Tsujii. 2009. Bilingual dictionary
extraction from Wikipedia. In Machine Translation
Summit XII.
625
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 103?109,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RALI Machine Translation System for WMT 2010
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry and Philippe Langlais
RALI - Universite? de Montre?al
C.P. 6128, succursale Centre-ville
H3C 3J7, Montre?al, Que?bec, Canada
{huetstep,bourdaij,patryale,felipe}@iro.umontreal.ca
Abstract
We describe our system for the translation
task of WMT 2010. This system, devel-
oped for the English-French and French-
English directions, is based on Moses and
was trained using only the resources sup-
plied for the workshop. We report exper-
iments to enhance it with out-of-domain
parallel corpora sub-sampling, N-best list
post-processing and a French grammatical
checker.
1 Introduction
This paper presents the phrase-based machine
translation system developed at RALI in order
to participate in both the French-English and
English-French translation tasks. In these two
tasks, we used all the corpora supplied for the con-
straint data condition apart from the LDC Giga-
word corpora.
We describe its different components in Sec-
tion 2. Section 3 reports our experiments to sub-
sample the available out-of-domain corpora in or-
der to adapt the translation models to the news
domain. Section 4, dedicated to post-processing,
presents how N-best lists are reranked and how the
French 1-best output is corrected by a grammatical
checker. Section 5 studies how the original source
language of news acts upon translation quality. We
conclude in Section 6.
2 System Architecture
2.1 Pre-processing
The available corpora were pre-processed using
an in-house script that normalizes quotes, dashes,
spaces and ligatures. We also reaccentuated
French words starting with a capital letter. We
significantly cleaned up the parallel Giga word
corpus (noted as gw hereafter), keeping 18.1 M
of the original 22.5 M sentence pairs. For exam-
ple, sentence pairs with numerous numbers, non-
alphanumeric characters or words starting with
capital letters were removed.
Moreover, training material was tokenized with
the tool provided for the workshop and truecased,
meaning that the words occuring after a strong
punctuation mark were lowercased when they be-
longed to a dictionary of common all-lowercased
forms; the others were left unchanged. In order
to reduce the number of words unknown to the
translation models, all numbers were serialized,
i.e. mapped to a special unique token. The origi-
nal numbers are then placed back in the translation
in the same order as they appear in the source sen-
tence. Since translations are mostly monotonic be-
tween French and English, this simple algorithm
works well most of the time.
2.2 Language Models
We trained Kneser-Ney discounted 5-gram lan-
guage models (LMs) on each available corpus us-
ing the SRILM toolkit (Stolcke, 2002). These
LMs were combined through linear interpola-
tion: first, an out-of-domain LM was built from
Europarl, UN and gw; then, this model was
combined with the two in-domain LMs trained
on news-commentary and news.shuffled, which
will be referred to as nc and ns in the remainder
of the article. Weights were fixed by optimizing
the perplexity of a development corpus made of
news-test2008 and news-syscomb2009 texts.
In order to reduce the size of the LMs, we
limited the vocabulary of our models to 1 M
words for English and French. The words of
these vocabularies were selected from the com-
putation of the number of their occurences us-
ing the method proposed by Venkataraman and
Wang (2003). The out-of-vocabulary rate mea-
sured on news-test2009 and news-test2010
with a so-built vocabulary varies between 0.6 %
103
and 0.8 % for both English and French, while it
was between 0.4 % and 0.7 % before the vocabu-
lary was pruned.
To train the LM on the 48 M-sentence English
ns corpus, 32 Gb RAM were required and up to
16 Gb RAM, for the other corpora. To reduce the
memory needs during decoding, LMs were pruned
using the SRILM prune option.
2.3 Alignment and Translation Models
All parallel corpora were aligned with
Giza++ (Och and Ney, 2003). Our transla-
tion models are phrase-based models (PBMs)
built with Moses (Koehn et al, 2007) with the
following non-default settings:
? maximum sentence length of 80 words,
? limit on the number of phrase translations
loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized
reordering model scores were optimized on the
development corpus thanks to the MERT algo-
rithm (Och, 2003).
2.4 Experiments
This section reports experiments done on the
news-test2009 corpus for testing various config-
urations. In these first experiments, we trained
LMs and translation models on the Europarl cor-
pus.
Case We tested two methods to handle case. The
first one lowercases all training data and docu-
ments to translate, while the second one normal-
izes all training data and documents into their nat-
ural case. These two methods require a post-
processing recapitalization but this last step is
more basic for the truecase method. Training mod-
els on lowercased material led to a 23.15 % case-
insensitive BLEU and a 21.61 % case-sensitive
BLEU; from truecased corpora, we obtained a
23.24 % case-insensitive BLEU and a 22.13 %
case-sensitive BLEU. As truecasing induces an in-
crease of the two metrics, we built all our mod-
els in truecase. The results shown in the remain-
der of this paper are reported in terms of case-
insensitive BLEU which showed last year a bet-
ter correlation with human judgments than case-
sensitive BLEU for the two languages we con-
sider (Callison-Burch et al, 2009).
Tokenization Two tokenizers were tested: one
provided for the workshop and another we devel-
oped. They differ mainly in the processing of com-
pound words: our in-house tokenizer splits these
words (e.g. percentage-wise is turned into percent-
age - wise), which improves the lexical coverage of
the models trained on the corpus. This feature
does not exist in the WMT tool. However, us-
ing the WMT tokenizer, we measured a 23.24 %
BLEU, while our in-house tokenizer yielded a
lower BLEU of 22.85 %. Follow these results
prompted us to use the WMT tokenizer.
Serialization In order to test the effect of se-
rialization, i.e. the mapping of all numbers to
a special unique token, we measured the BLEU
score obtained by a PBM trained on Europarl for
English-French, when numbers are left unchanged
(Table 1, line 1) or serialized (line 2). These
results exhibit a slight decrease of BLEU when
serialization is performed. Moreover, if BLEU
is computed using a serialized reference (line 3),
which is equivalent to ignoring deserialization er-
rors, a minor gain of BLEU is observed, which
validates our recovering method. Since resorting
to serialization/deserialization yields comparable
performance to a system not using it, while reduc-
ing the model?s size, we chose to use it.
BLEU
no serialization 23.24
corpus serialization 23.13
corpus and reference serialization 23.27
Table 1: BLEU measured for English-French on
news-test2009 when training on Europarl.
LM Table 2 reports the perplexity measured on
news-test2009 for French (column 1) and En-
glish (column 3) LMs learned on different cor-
pora and interpolated using the development cor-
pus. We also provide the BLEU score (column 2)
for English-French obtained from translation mod-
els trained on Europarl and nc. As expected, us-
ing in-domain corpora (line 2) for English-French
led to better results than using out-of-domain data
(line 3). The best perplexities and BLEU score
are obtained when LMs trained on all the available
corpora are combined (line 4). The last three lines
exhibit how LMs perform when they are trained on
in-domain corpora without pruning them. While
the gzipped 5-gram LM (last line) obtained in
104
such a manner occupies 1.4 Gb on hard disk, the
gzipped pruned 5-gram LM (line 4) trained using
all corpora occupies 0.9 Gb and yields the same
BLEU score. This last LM was used in all the ex-
periments reported in the subsequent sections.
corpora
Fr En
ppl BLEU ppl
nc 327 22.44 454
nc + ns 125 25.69 166
Europarl + UN + Gw 156 24.91 225
all corpora 113 26.01 151
nc + ns (3g, unpruned) 138 25.32 -
nc + ns (4g, unpruned) 124 25.86 -
nc + ns (5g, unpruned) 120 26.04 -
Table 2: LMs perplexities and BLEU scores mea-
sured on news-test2009. Translation models
used here were trained on nc and Europarl.
3 Domain adaptation
As the only news parallel corpus provided for
the workshop contains 85k sentence pairs, we
must resort to other parallel out-of-domain cor-
pora in order to build reliable translation models.
If in-domain and out-of-domain LMs are usually
mixed with the well-studied interpolation tech-
niques, training translation models from data of
different domains has received less attention (Fos-
ter and Kuhn, 2007; Bertoldi and Federico, 2009).
Therefore, there is still no widely accepted tech-
nique for this last purpose.
3.1 Effects of the training data size
We investigated how increasing training data acts
upon BLEU score. Table 3 shows a high increase
of 2.7 points w.r.t. the use of nc alone (line 1)
when building the phrase table and the reordering
model from nc and either the 1.7 M-sentence-pair
Europarl (line 2) or a 1.7 M-sentence-pair cor-
pus extracted from the 3 out-of-domain corpora:
Europarl, UN and Gw (line 3). Training a PBM on
merged parallel corpora is not necessarily the best
way to combine data from different domains. We
repeated 20 times nc before adding it to Europarl
so as to have the same amount of out-of-domain
and in-domain material. This method turned out
to be less successful since it led to a minor 0.15
BLEU decrease (line 4) w.r.t. our previous system.
Following the motto ?no data is better than more
corpora En?Fr Fr?En
nc 23.29 23.23
nc + Europarl 26.01 -
nc + 1.7 M random pairs 26.02 26.68
20?nc + Europarl 25.86 -
nc + 8.7 M pairs (part 0) 26.44 27.65
nc + 8.7 M pairs (part 1) 26.68 27.46
nc + 8.7 M pairs (part 2) 26.54 27.50
3 models merged 26.86 27.56
Table 3: BLEU (in %) measured on news-
test2009 for English-French and French-English
when translations models and lexicalized reorder-
ing models are built using various amount of data
in addition to nc.
data?, a PBM was built using all the parallel cor-
pora at our disposal. Since the overall parallel sen-
tences were too numerous for our computational
resources to be simultaneously used, we randomly
split out-of-domain corpora into 3 parts of 8.7 M
sentence pairs each and then combined them with
nc. PBMs were trained on each of these parts
(lines 5 to 7), which yields respectively 0.5 and
0.8 BLEU gain for English-French and French-
English w.r.t. the use of 1.7 M out-of-domain sen-
tence pairs. The more significant improvement no-
ticed for the French-English direction is probably
explained by the fact that the French language is
morphologically richer than English. The 3 PBMs
were then combined by merging the 3 phrase ta-
bles. To do so, the 5 phrase table scores computed
by Moses were mixed using the geometric average
and a 6th score was added, which counts the num-
ber of phrase tables where the given phrase pair
occurs. We ended up with a phrase table contain-
ing 623 M entries, only 9 % and 4 % of them being
in 2 and 3 tables respectively. The resulting phrase
table led to a slight improvement of BLEU scores
(last line) w.r.t. the previous models, except for the
model trained on part 0 for French-English.
3.2 Corpus sub-sampling
Whereas using all corpora improves translation
quality, it requires a huge amount of memory and
disk space. We investigate in this section ways to
select sentence pairs among large out-of-domain
corpora.
Unknown words The main interest of adding
new training material relies on the finding of
words missing in the phrase table. According to
105
this principle, nc was extended with new sentence
pairs containing an unknown word (Table 4, line 2)
or a word that belongs to our LM vocabulary and
that occurs less than 3 times in the current cor-
pus (line 3). This resulted in adding 400 k pairs
in the first case and 950 k in the second one, with
BLEU scores close or even better than those ob-
tained with 1.7 M.
corpora En?Fr Fr?En
nc + 1.7 M random pairs 26.02 26.68
nc + 400k pairs (occ = 1) 25.67 -
nc + 950k pairs (occ = 3) 26.13 -
nc + Joshua sub-sampling 26.98 27.68
nc + IR (1-g q, w/ repet) 25.81 -
nc + IR (1-g q, no repet) 26.56 27.54
nc + IR (1,2-g q, w/ repet) 26.26 -
nc + IR (1,2-g q, no repet) 26.53 -
nc + 8.7 M pairs 26.68 27.65
+ IR score (1g q, no repet) 26.93 27.65
3 large models merged 26.86 27.56
+ IR score (1g q, no repet) 26.98 27.74
Table 4: BLEU measured on news-test2009 for
English-French and French-English using transla-
tion models trained on nc and a subset of out-of-
domain corpora.
Unknown n-grams We applied the sub-
sampling method available in the Joshua
toolkit (Li et al, 2009). This method adds a
new sentence pair when it contains new n-grams
(with 1 ? n ? 12) occurring less than 20 times in
the current corpus, which led us to add 1.5 M pairs
for English-French and 1.4 M for French-English.
A significant improvement of BLEU is observed
using this method (0.8 for English-French and
1.0 for French-English) w.r.t. the use of 1.7 M
randomly selected pairs. However, this method
has the major drawback of needing to build a new
phrase table for each document to translate.
Information retrieval Information retrieval
(IR) methods have been used in the past to sub-
sample parallel corpora (Hildebrand et al, 2005;
Lu? et al, 2007). These studies use sentences
belonging to the development and test corpora as
queries to select the k most similar source sen-
tences in an indexed parallel corpus. The retrieved
sentence pairs constitute a training corpus for
the translation models. In order to alleviate the
fact that a new PBM has to be learned for each
new test corpus, we built queries using sentences
contained in the monolingual ns corpus, leading
to the selection of sentence pairs stylistically
close to those in the news domain. The source
sentences of the three out-of-domain corpora
were indexed using Lemur.1 Two types of queries
were built from ns sentences after removing stop
words: the first one is limited to unigrams, the
second one contains both unigrams and bigrams,
with a weight for bigrams twice as high as for
unigrams. The interest of the latter query type is
based on the hypothesis that bigrams are more
domain-dependent than unigrams. Another choice
that needs to be made when using IR methods is
concerning the retention of redundant sentences
in the final corpus.
Lines 5 to 8 of Table 4 show the results obtained
when sentence pairs were gathered up to the size
of Europarl, i.e. 1.7 M pairs. 10 sentences were
retrieved per query in various configurations: with
or without bigrams inside queries, with or without
duplicate sentence pairs in the training corpus. Re-
sults demonstrate the interest of the approach since
the BLEU scores are close to those obtained us-
ing the previous tested method based on n-grams
of the test data. Taking bigrams into account does
not improve results and adding only once new sen-
tences is more relevant than duplicating them.
Since using all data led to even better perfor-
mances (see last line of Table 3), we used infor-
mation provided by the IR method in the PBMs
trained on nc + 8.7 M out-of-domain sentence
pairs or taking into account all the training ma-
terial. To this end, we included a new score in
the phrase tables which is fixed to 1 for entries
that are in the phrase table trained on sentences
retrieved with unigram queries without repetition
(see line 6 of Table 4), and 0 otherwise. Therefore,
this score aims at boosting the weight of phrases
that were found in sentences close to the news do-
main. The results reported in the 4 last lines of Ta-
ble 4 show minor but consistent gains when adding
this score. The outputs of the PBMs trained on
all the training corpus and which obtained the best
BLEU scores on news-test2009 were submitted
as contrastive runs. The two first lines of Table 5
report the results on this years?s test data, when
the score related to the retrieved corpus is incor-
porated or not. These results still exhibit a minor
improvement when adding this score.
1www.lemurproject.org
106
En?Fr Fr?En
BLEU BLEU-cased TER BLEU BLEU-cased TER
PBM 27.5 26.5 62.2 27.8 26.9 61.2
+IR score 27.7 26.6 62.1 28.0 27.0 61.0
+N-best list reranking 27.9 26.8 62.1 28.0 27.0 61.2
+grammatical checker 28.0 26.9 62.0 - - -
Table 5: Official results of our system on news-test2010.
4 Post-processing
4.1 N-best List Reranking
Our best PBM enhanced by IR methods was em-
ployed to generate 500-best lists. These lists were
reranked combining the global decoder score with
the length ratio between source and target sen-
tences, and the proportions of source sentence n-
grams that are in the news monolingual corpora
(with 1 ? n ? 5). Weights of these 7 scores are
optimized via MERT on news-test2009. Lines 2
and 3 of Table 5 provide the results obtained be-
fore and after N-best list reranking. They show a
tiny gain for all metrics for English-French, while
the results remain constant for French-English.
Nevertheless, we decided to use those translations
for the French-English task as our primary run.
4.2 Grammatical Checker
PBM outputs contain a significant number of
grammatical errors, even when LMs are trained
on large data sets. We tested the use of a gram-
matical checker for the French language: Antidote
RX distributed by Druide informatique inc.2 This
software was applied in a systematic way on the
first translation generated after N-best reranking.
Thus, as soon as the software suggests one or sev-
eral choices that it considers as more correct than
the original translation, the first proposal is kept.
The checked translation is our first run for English-
French.
Antidote RX changed at least one word in
26 % of the news-test2010 sentences. The most
frequent type of corrections are agreement errors,
like in the following example where the agreement
between the subject nombre (number) is correctly
made with the adjective coupe? (cut), thanks to the
full syntactic parsing of the French sentence.
Source: [...] the number of revaccinations could then be
cut [...]
Reranking: [...] le nombre de revaccinations pourrait
2www.druide.com
alors e?tre coupe?es [...]
+Grammatical checker: [...] le nombre de revacci-
nations pourrait alors e?tre coupe? [...]
The example below exhibits a good decision
made by the grammatical checker on the mood of
the French verb e?tre (to be).
Source: It will be a long time before anything else will be
on offer in Iraq.
Reranking: Il faudra beaucoup de temps avant que tout
le reste sera offert en Irak.
+Grammatical checker: Il faudra beaucoup de temps
avant que tout le reste soit offert en Irak.
A last interesting type of corrected errors con-
cerns negation. Antidote has indeed the capacity
to add the French particle ne when it is missing in
the expressions ne ... pas, ne ... plus, aucun ne, per-
sonne ne or rien ne. The results obtained using the
grammatical checker are reported in the last line
of Table 5. The automatic evaluation shows only a
minor improvement but we expect the changes in-
duced by this tool to be more significant for human
annotators.
5 Effects of the Original Source
Language of Articles on Translation
During our experiments, we found that translation
quality is highly variable depending on the origi-
nal source language of the news sentences. This
phenomenon is correlated to the previous work of
Kurokawa et al (2009) that showed that whether
or not a piece of text is an original or a trans-
lation has an impact on translation performance.
The main reason that explains our observations
is probably that the topics and the vocabulary of
news originally expressed in languages other than
French and English tend to differ more from those
of the training materials used to train PBM mod-
els for these two languages. In order to take into
account this phenomenon, MERT tuning was re-
peated for each original source language, using the
107
same PBM models trained on all parallel corpora
and incorporating an IR score.
Columns 1 and 3 of Table 5 display the BLEU
measured using our previous global MERT op-
timization made on 2553 sentence pairs, while
columns 2 and 4 show the results obtained when
running MERT on subsets of the development ma-
terial, made of around 700 sentence pairs each.
The BLEU measured on the whole 2010 test set
is reported in the last line. As expected, language-
dependent MERT tends to increase the LM weight
for English and French. However, an absolute
0.35 % BLEU decrease is globally observed for
English-French using this approach and a 0.21 %
improvement for French-English.
En?Fr Fr?En
MERT global lang dep global lang dep
Cz 21.95 21.45 21.84 21.85
En 30.80 29.84 33.73 35.00
Fr 37.59 36.96 31.59 32.62
De 16.60 16.73 17.41 17.76
Es 24.52 24.45 29.25 28.31
total 27.64 27.39 27.99 28.20
Table 6: BLEU scores measured on parts of
news-test2010 according to the original source
language.
6 Conclusion
This paper presented our statistical machine trans-
lation system developed for the translation task us-
ing Moses. Our submitted runs were generated
from models trained on all the corpora made avail-
able for the workshop, as this method had pro-
vided the best results in our experiments. This
system was enhanced using IR methods which
exploits news monolingual copora, N-best list
reranking and a French grammatical checker.
This was our first participation where such a
huge amount data was involved. Training models
on so many sentences is challenging from an engi-
neering point of view and requires important com-
putational resources and storage capacities. The
time spent in handling voluminous data prevented
us from testing more approaches. We suggest that
the next edition of the workshop could integrate
a task restraining the number of parameters in the
models trained.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In 4th EACL Workshop
on Statistical Machine Translation (WMT), Athens,
Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In 2nd ACL Workshop
on Statistical Machine Translation (WMT), Prague,
Czech Republic.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In 10th conference
of the European Association for Machine Transla-
tion (EAMT), Budapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In 45th Annual Meeting of the Association for
Computational Linguistics (ACL), Companion Vol-
ume, Prague, Czech Republic.
David Kurokawa, Cyril Goutte, and Pierre Isabelle.
2009. Automatic detection of translated text and
its impact on machine translation. In 12th Machine
Translation Summit, Ottawa, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In 4th
EACL Workshop on Statistical Machine Translation
(WMT), Athens, Greece.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Join Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Sapporo, Japan.
108
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In 7th International Con-
ference on Spoken Language Processing (ICSLP),
Denver, CO, USA.
Arnand Venkataraman and Wen Wang. 2003. Tech-
niques for effective vocabulary selection. In 8th Eu-
ropean Conference on Speech Communication and
Technology (Eurospeech), Geneva, Switzerland.
109
Identifying Parallel Documents from a Large Bilingual Collection of Texts:
Application to Parallel Article Extraction in Wikipedia.
Alexandre Patry
KeaText
845, Boulevard Dcarie, bureau 202
Saint-Laurent, Canada H4L 3L7
alexandre.patry@keatext.com
Philippe Langlais
DIRO/RALI
Universite? de Montre?al
Montre?al, Canada H3C3J7
felipe@iro.umontreal.ca
Abstract
While several recent works on dealing with
large bilingual collections of texts, e.g. (Smith
et al, 2010), seek for extracting parallel sen-
tences from comparable corpora, we present
PARADOCS, a system designed to recognize
pairs of parallel documents in a (large) bilin-
gual collection of texts. We show that this
system outperforms a fair baseline (Enright
and Kondrak, 2007) in a number of con-
trolled tasks. We applied it on the French-
English cross-language linked article pairs of
Wikipedia in order see whether parallel ar-
ticles in this resource are available, and if
our system is able to locate them. Accord-
ing to some manual evaluation we conducted,
a fourth of the article pairs in Wikipedia are
indeed in translation relation, and PARADOCS
identifies parallel or noisy parallel article pairs
with a precision of 80%.
1 Introduction
There is a growing interest within the Machine
Translation (MT) community to investigate compa-
rable corpora. The idea that they are available in
a much larger quantity certainly contributes to fos-
ter this interest. Still, parallel corpora are playing
a crucial role in MT. This is therefore not surprising
that the number of bitexts available to the commu-
nity is increasing.
Callison-Burch et al (2009) mined from institu-
tional websites the 109 word parallel corpus1 which
gathers 22 million pairs of (likely parallel) French-
English sentences. Tiedemann (2009) created the
1http://www.statmt.org/wmt10
Opus corpus,2 an open source parallel corpus gath-
ering texts of various sources, in several languages
pairs. This is an ongoing effort currently gathering
more than 13 Gigabytes of compressed files. The
Europarl corpus3 (Koehn, 2005) gathers no less than
2 Gigabytes of compressed documents in 20 lan-
guage pairs. Some other bitexts are more marginal in
nature. For instance, the novel 1984 of George Or-
wel has been organized into an English-Norvegian
bitext (Erjavec, 2004) and Beyaz Kale of Orhan Pa-
muk as well as Sofies Verden of Jostein Gaardner
are available for the Swedish-Turk language pair
(Megyesi et al, 2006).
A growing number of studies investigate the ex-
traction of near parallel material (mostly sentences)
from comparable data. Among them, Munteanu et
al. (2004) demonstrate that a classifier can be trained
to recognize parallel sentences in comparable cor-
pora mined from news collections. A number of
related studies (see section 5) have also been pro-
posed; some of them seeking to extract parallel sen-
tences from cross-language linked article pairs in
Wikipedia4 (Adafre and de Rijke, 2006; Smith
et al, 2010). None of these studies addresses specif-
ically the issue of discovering parallel pairs of arti-
cles in Wikipedia.
In this paper, we describe PARADOCS, a system
capable of mining parallel documents in a collec-
tion, based on lightweight content-based features ex-
tracted from the documents. On the contrary to other
systems designed to target parallel corpora (Chen
2http://opus.lingfil.uu.se/
3http://www.statmt.org/europarl/
4http://fr.wikipedia.org/
87
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 87?95,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
and Nie, 2000; Resnik and Smith, 2003), we do
not assume any specific naming conventions on file-
names or URLs.
The reminder of this article is organized as fol-
lows. In the next section, we describe our ap-
proach to mining parallel documents in a bilingual
collection of texts. We test our approach on the
Europarl corpus in section 3. We present in sec-
tion 4 the application of our system to a subpart
of the French-English articles of Wikipedia. We
describe related work in section 5, summarize our
work in section 6 and present future works in sec-
tion 7.
2 PARADOCS
In order to identify pairs of parallel documents in a
bilingual collection of texts, we designed a system,
named PARADOCS, which is making as few assump-
tions as possible on the language pair being consid-
ered, while still making use of the content of the doc-
uments in the collection. Our system is built on three
lightweight components. The first one searches for
target documents that are more likely parallel to a
given source document (section 2.1). The second
component classifies (candidate) pairs of documents
as parallel or not (section 2.2). The third component
is designed to filter out some (wrongly) recognized
parallel pairs, making use of collection-level infor-
mation (section 2.3).
2.1 Searching Candidate Pairs
In a collection containing n documents in a given
language, and m in another one, scoring each of the
n?m potential pairs of source-target documents be-
comes rapidly intractable. In our approach, we re-
sort to an information retrieval system in order to
select the target documents that are most likely par-
allel to a given source one. In order to do so, we
index target documents t in the collection thanks to
an indexing strategy ? that will be described shortly.
Then, for a source document s, we first index it, that
is, we compute ?(s), and query the retrieval engine
with ?(s), which in turn returns the N most simi-
lar target documents found in the collection. In our
experiments, we used the Lucene5 retrieval library.
5http://lucene.apache.org
We tested two indexing strategies: one reduces a
document to the sequence of hapax words it contains
(? ?hap), the other one reduces it to its sequence
of numerical entities (? ?num). Hapax words have
been found very useful in identifying parallel pairs
of documents (Enright and Kondrak, 2007) as well
as for word-aligning bitexts (Lardilleux and Lep-
age, 2007). Following Enright and Kondrak (2007),
we define hapax words as blank separated strings of
more than 4 characters that appear only once in the
document being indexed. Also, we define a numer-
ical entity as a blank separated form containing at
least one digit. It is clear from this description that
our indexing strategies can easily be applied to many
different languages.
2.2 Identifying candidate pairs
Each candidate pair delivered by Lucene, is classi-
fied as parallel or not by a classifier trained in a su-
pervised way to recognize parallel documents. Here
again, we want our classifier to be as agnostic as
possible to the pair of languages considered. This
is why we adopted very light feature extractors ?
which are built on three types of entities in docu-
ments: numerical entities (? ?num), hapax words
(? ?hap) and punctuation marks6 (? ?punc). For
each sequence of entities ?(s) and ?(t) of a source
document s and a target document t respectively, we
compute the three following features:
? the normalized edit-distance between the two
representations:
? = ed(?(s), ?(t))/max(|?(s)|, |?(t)|)
where |?(d)| stands for the size of the sequence
of entities contained in d. Intuitively, ? gives
the proportion of entities shared across docu-
ments,
? the total number of entities in the representation
of both documents:
|?(s)|+ |?(t)|
We thought this information might complement
the one of ? which is relative to the document?s
sequence length.
6We only considered the 6 following punctuation marks that
are often preserved in translation: .!?():
88
? A binary feature which fires whenever the pair
of documents considered receives the smaller
edit-distance among all the pairs of documents
involving this source document:
?(s, t) ={ 1 if ed (?(s), ?(t)) ? ed (?(s), ?(t?)) ? t?
0 otherwise
Intuitively, the target document considered is
more likely the good one if it has with the
source document the smallest edit distance.
Since we do compute edit-distance for all the
candidate documents pairs, this feature comes
at no extra computational cost.
We compute these three features for each se-
quence of entities considered. For instance, if we
represent a document according to its sequence of
numerical entities and its hapax words, we do com-
pute a total of 6 features.7
It is fair to say that our feature extraction strat-
egy is very light. In particular, it does not capitalize
on an existing bilingual lexicon. Preliminary exper-
iments with features making use of such a lexicon
turned out to be less successful, due to issues in the
coverage of the lexicon (Patry and Langlais, 2005).
To create and put to the test our classifier, we used
the free software package Weka (Hall et al, 2009),
written in Java.8 This package allows the easy ex-
perimentation of numerous families of classifiers.
We investigated logistic regression (logit), naive
bayes models (bayes), adaboost (ada), as well as
decision tree learning (j48).
2.3 Post-treatments
The classifiers we trained label each pair of docu-
ments independently of other candidate pairs. This
independence assumption is obviously odd and leads
to situations where several target documents are
paired to a given source document and vice-versa.
Several solutions can be applied; we considered two
simple ones in this work. The first one, hereafter
named nop, consists in doing nothing; therefore
leaving potential duplicates source or target docu-
ments. The second solution, called dup, filters out
7We tried with less success to compute a single set of fea-
tures from a representation considering all entities.
8www.cs.waikato.ac.nz/ml/weka/
pairs sharing documents. Another solution we did
not implement would require to keep from the set of
pairs concerning a given source document the one
with the best score as computed by our classifier. We
leave this as future work.
3 Controlled Experiments
We checked the good behavior of PARADOCS
in a controlled experimental setting, using the
Europarl corpus. This corpus is organized into
bitexts, which means that we have a ground truth
against which we can evaluate our system.
3.1 Corpus
We downloaded version 5 of the Europarl cor-
pus.9 Approximatively 6 000 documents are avail-
able in 11 languages (including English), that is, we
have 6 000 bitexts in 10 language pairs where En-
glish is one of the languages. The average number
of sentences per document is 273. Some documents
contain problems (encoding problems, files ending
unexpectedly, etc.). We did not try to cope with this.
In order to measure how sensible our approach is
to the size of the documents, we considered several
slices of them (from 10 to 1000 sentences). 10
3.2 Protocol
We tested several experimental conditions, varying
the language pairs considered (en-da, -de, -el, -es,
-fi, -fr, -it, -nl, -pt and -sv) as well as the doc-
ument length (10, 20, 30, 50, 70, 100 and 1 000
sentences). We also tested several system configu-
rations, varying the indexing strategy (num, hap),
the entities used for representing documents (hap,
num, num+hap, num+punc), the classifier used
(logit, ada, bayes, and j48), as well as the
post-filtering strategy (nop, dup). This means that
we conducted no less than 4 480 experiments.
Because we know which documents are paral-
lel, we can compute precision (percentage of iden-
tified parallel pairs that are truly parallel) and recall
(percentage of true parallel pairs identified) for each
configuration.
9http://www.statmt.org/europarl
10We removed the first sentences of each document, since
they may contain titles or other information that may artificially
ease pairing.
89
Since our approach requires to train a classifier,
we resorted in this experiment to a 5-fold cross-
validation procedure where we trained our classifiers
on 4/5 of the corpus and tested on the remaining part.
The figures reported in the reminder of this section
are averaged over the 5 folds. Also, all configura-
tions tested in this section considered the N = 20
most similar target documents returned by the re-
trieval engine for each source document.
3.3 Results
3.3.1 Search errors
We first measured search errors observed during
step 1 of our system. There are actually two types
of errors: one when no document is returned by
Lucene (nodoc) and one when none of the target
documents returned by the retrieval engine are sanc-
tioned ones (nogood). Figure 1 shows both error
types for the Dutch-English language pair, as a func-
tion of the document length.11 Clearly, search errors
are more important when documents are short. Ap-
proximatively a tenth of the source documents of (at
most) 100 sentences do not receive by Lucene any
target document. For smaller documents, this hap-
pens for as much as a third of the documents. Also,
it is interesting to note that in approximatively 6% of
the cases where Lucene returns target documents,
the good one is not present. Obviously we pay the
prize of our lightweight indexation scheme. In or-
der to increase the recall of our system, nodoc er-
rors could be treated by employing an indexing strat-
egy which would use more complex features, such
as sufficiently rare words (possibly involving a key-
word test, e.g. tf.idf). This is left as future work.
3.3.2 Best System configuration
In order to determine the factors which influence
the most our system, we varied the language pairs
(10 values) and the length of the documents (7 val-
ues) and counted the number of times a given sys-
tem configuration obtained the best f-measure over
the 70 tests we conducted. We observed that most
of the time, the configurations recording the best
f-measure are those that exploit numerical entities
(both at indexing time and feature extraction time).
Actually, we observed that computing features on
11Similar figures have been observed for other language
pairs.
nb. of sent.
errors %
 10
 15
 20
 25
 30
 35
 40
 45
 8  16  32  64  128  256  512  1024
nodoc + nogoodnodoc
Figure 1: Percentage of Dutch documents for which
Lucene returns no English document (nodoc), or no
correct document (nodoc+nogood) as a function of the
document size counted in sentences.
hapax words or punctuation marks on top of nu-
merical entities do not help much. One possible
explanation is that often, and especially within the
Europarl corpus, hapax words correspond to nu-
merical entities. Also, we noted that frequently, the
wining configuration is the one embedding a logistic
regression classifier, tightly followed by the decision
tree learner.
3.3.3 Sensitivity to the language pair
We also tested the sensibility of our approach to
the language pair being considered. Apart from the
fact that the French-English pair was the easiest to
deal with, we did not notice strong differences in
performance among language pairs. For documents
of at most 100 sentences, the worst f-measure (0.93)
is observed for the Dutch/English language pair,
while the best one (0.95) is observed for the French-
English pair. Slightly larger differences were mea-
sured for short documents.
nb. of sent.
gain %
 0
 5
 10
 15
 20
 25
 30
 35
 40
 8  16  32  64  128  256  512  1024
dadeelesfifritnlptsv
Figure 2: Absolute gains of the best variant of our sys-
tem over the approach described by Enright and Kon-
drak (2007).
90
3.3.4 Sanity check
We conducted a last sanity check by comparing
our approach to the one of (Enright and Kondrak,
2007). This approach simply ranks the candidate
pairs in decreasing order of the number of hapax
words they share. The absolute gains of our ap-
proach over theirs are reported in Figure 2, as a
function of the document length and the language
pair considered. Our system systematically outper-
forms the hapax approach of (Enright and Kondrak,
2007) regardless of the length of the documents and
the language pairs considered. An average absolute
gain of 13.6% in f-measure is observed for long doc-
uments, while much larger gains are observed for
shorter ones. It has to be noted, that our approach
requires to train a classifier, which makes it poten-
tially less useful in some situations. Also, we used
the best of our system in this comparison.
4 Experiments with Wikipedia
Many articles in Wikipedia are available in
several languages. Often, they are explicitly
marked as linked across languages. For instance,
the English article [Text corpus] is linked to the
French one [Corpus], but they are not transla-
tion of each other, while the English article [De-
cline of the Roman Empire] and the French one
[De?clin de l?empire romain d?Occident] are paral-
lel.12
4.1 Resource
During summer 2009, we collected all French-
English cross-language linked articles from
Wikipedia. A very straightforward pre-
processing stage involving simple regular expres-
sions removed part of the markup specific to this
resource. We ended up with 537 067 articles in
each language. The average length of the English
pages is 711 words, while the average for French is
445 words. The difference in length among linked
articles has been studied by Filatova (2009) on a
small excerpt of bibliographical articles describing
48 persons listed in the biography generation task
(Task 5) of DUC 2004.13
12At least they were at the time of redaction.
13http://duc.nist.gov/duc2004/tasks.html/
4.2 Parallelness of cross-language linked
article pairs in FR-EN Wikipedia.
In this experiment, we wanted to measure the pro-
portion of cross-language linked article pairs in
Wikipedia that are in translation relation. In or-
der to do so, we manually evaluated 200 pairs of arti-
cles in our French-English Wikipedia repository.
A web interface was developed in order to anno-
tate each pair, following the distinction introduced
by Fung and Cheung (2004): parallel indicates
sentence-aligned texts that are in translation relation;
noisy characterizes two documents that are never-
theless mostly bilingual translations of each other;
topic corresponds to documents which share sim-
ilar topics, but that are not translation of each oth-
ers and very-non that stands for rather unrelated
texts.
The results of the manual evaluation are reported
in the left column of table 1. We observe that a
fourth of the pairs of articles are indeed parallel or
noisy parallel. This figure quantifies the observa-
tion made by Adafre and de Rijke (2006) that while
some articles in Wikipedia tend to be translations
of each other, the majority of the articles tend to be
written independently of each other. To the best of
our knowledge, this is the first time someone is mea-
suring the degree of parallelness of Wikipedia at
the article level.
If our sample is representative (something which
deserves further investigations), it means that more
than 134 000 pairs of documents in the French-
English Wikipedia are parallel or noisy parallel.
We would like to stress that, while conducting
the manual annotation, we frequently found diffi-
cult to label pairs of articles with the classes pro-
posed by Fung and Cheung (2004). Often, we could
spot a few sentences translated in pairs that we rated
very-non or topic. Also, it was hard to be con-
sistent over the annotation session with the distinc-
tion made between those two classes. Many arti-
cles are divided into sub-topics, some of which be-
ing covered in the other article, some being not.
4.3 Parallelness of the article pairs identified
by PARADOCS
We applied PARADOCS to our Wikipedia collec-
tion. We indexed the French pages with the Lucene
91
Wikipedia PARADOCS
Type Count Ratio Count Ratio
very-non 92 46% 5 2.5%
topic 58 29% 34 17%
noisy 22 11% 39 19.5%
parallel 28 14% 122 61%
Total 200 200
Table 1: Manual analysis of 200 pairs cross-language
linked in Wikipedia (left) and 200 pairs of articles
judged parallel by our system (right).
toolkit using the num indexing scheme. Each En-
glish article was consequently transformed with the
same strategy before querying Lucene, which was
asked to return the N = 5 most similar French arti-
cles. We limited the retrieval to 5 documents in this
experiment in order to reduce computation time. As
a matter of fact, running our system on Wikipedia
took 1.5 days of computation on 8 nodes of a pen-
tium cluster. Most of this time was devoted to com-
pute edit-distance features.
Each candidate pair of articles was then labeled
as parallel or not by a classifier we trained to rec-
ognize parallel documents in an in-house collection
of French-English documents we gathered in 2009
from a website dedicated to Olympic games.14 Us-
ing a classifier trained on a different task gives us the
opportunity to see how our system would do if used
out-of-the-box. A set of 1844 pairs of documents
have been automatically aligned (at the document
level) thanks to heuristics on URL names; then man-
ually checked for parallelness. The best classifier
we developed on this collection (thanks to a 5-fold
cross-validation procedure) was a decision tree clas-
sifier (j48) which achieves an average f-measure of
90% (92.7% precision, and 87.4% recall). This is
the classifier we used in this experiment.
From the 537 067 English documents of our col-
lection, 106 896 (20%) did not receive any answer
from Lucene (nodoc). A total of 117 032 pairs of
documents were judged by the classifier as parallel.
The post-filtering stage (dup) eliminated slightly
less than half of them, leaving us with a total of
14http://www.olympic.org
61 897 pairs. We finally eliminated those pairs that
were not cross-language linked in Wikipedia. We
ended up with a set of 44 447 pairs of articles iden-
tified as parallel by our system.
Since there is no reference telling us which cross-
language linked articles in Wikipedia are indeed
parallel, we resorted to a manual inspection of a ran-
dom excerpt of 200 pairs of articles identified as par-
allel by our system. The sampling was done in a way
that reflects the distribution of the scores of the clas-
sifier over the pairs of articles identified as parallel
by our system.
The results of this evaluation are reported in the
right column of table 1. First, we observe that
20% (2.5+17) of the pairs identified as parallel by
our system are at best topic aligned. One explana-
tion for this is that topic aligned articles often share
numbers (such as dates), sometimes in the same or-
der, especially in bibliographies that are frequent in
Wikipedia. Clearly, we are paying the prize of
a lightweight content-oriented system. Second, we
observe that 61% of the annotated pairs were indeed
parallel, and that roughly 80% of them were parallel
or noisy parallel. Although PARADOCS is not as ac-
curate as it was on the Europarl corpus, it is still
performing much better than random.
4.4 Further analysis
We scored the manually annotated cross-language
linked pairs described in section 4.2 with our clas-
sifier. The cumulative distribution of the scores is
reported in table 2. We observe that 64% (100-
35.7%) of the parallel pairs are indeed rated as par-
allel (p ? 0.5) by our classifier. This percentage is
much lower for the other types of article pairs. On
the contrary, for very non-parallel pairs, the classi-
p ? 0.1 p ? 0.2 p < 0.5 avr.
very-non 1.1% 91.4% 92.5% 0.25
topic 1.7% 74.6% 78.0% 0.37
noisy 13.6% 77.3% 90.9% 0.26
parallel 7.1% 25.0% 35.7% 0.71
Table 2: Cumulative distribution and average score given
by our classifier to the 200 manually annotated pairs of
articles cross-language linked in Wikipedia.
92
fier assigns a score lower than 0.2 in more than 91%
of the cases. This shows that the score given by the
classifier correlates to some extent with the degree
of parallelness of the article pairs.
Among the 28 pairs of cross-language linked arti-
cle pairs manually labelled as parallel (see table 1),
only 2 pairs were found parallel by PARADOCS,
even if 18 of them received a score of 1 by the classi-
fier. This discrepancy is explained in part by the fil-
ter (dup) which is too drastic since it removes all the
pairs sharing one document. We already discussed
alternative strategies. The retrieval stage of our sys-
tem is as well responsible of some failures, espe-
cially since we considered the 5 first French docu-
ments returned by Lucene. We further inspected
the 10 (28-18) pairs judged parallel but scored by
our classifier as non parallel. We observed sev-
eral problems; the most frequent one being a fail-
ure of our pre-processing step which leaves unde-
sired blocs of text in one of the article, but not in
the other (recall we kept the preprocessing very ag-
nostic to the specificities of Wikipedia). These
blocs might be infoboxes or lists recapitulating im-
portant dates, or even sometimes HTML markup.
The presence of numerical entities in those blocs is
confounding the classifier.
5 Related Work
Pairing parallel documents in a bilingual collection
of texts has been investigated by several authors.
Most of the previous approaches for tackling this
problem capitalize on naming conventions (on file
URL names) for pairing documents. This is for in-
stance the case of PTMINER (Chen and Nie, 2000)
and STRAND (Resnik and Smith, 2003), two sys-
tems that are intended to mine parallel documents
over the Web. Since heuristics on URL names does
not ensure parallelness, other cues, such as the ratio
of the length of the documents paired or their HTML
structure, are further being used. Others have pro-
posed to use features computed after sentence align-
ing a candidate pair of documents (Shi et al, 2006),
a very time consuming strategy (that we tried with-
out success). Others have tried to use bilingual lex-
icons in order to compare document pairs; this is
for instance the case of the BITS system (Ma and
Liberman, 1999). Also, Enright and Kondrak (2007)
propose a very lightweight content-based approach
to pairing documents, capitalizing on the number of
hapax words they share. We show in this study, that
this approach can easily be outperformed.
Zhao and Vogel (2002) were among the first to
report experiments on harvesting comparable news
collections in order to extract parallel sentences.
With a similar goal, Munteanu et al (2004) pro-
posed to train in a supervised way (using some par-
allel data) a classifier designed to recognize paral-
lel sentences. They applied their classifier on two
monolingual news corpora in Arabic and English,
covering similar periods, and showed that the paral-
lel material extracted, when added to an in-domain
parallel training corpus of United Nation texts, im-
proved significantly an Arabic-to-English SMT sys-
tem tested on news data. Still, they noted that the
extracted material does not come close to the qual-
ity obtained by adding a small out-domain parallel
corpus to the in-domain training material. Different
variants of this approach have been tried afterwards,
e.g. (Abdul-Rauf and Schwenk, 2009).
To the best of our knowledge, Adafre and de Rijke
(2006) where the first to look at the problem of ex-
tracting parallel sentences from Wikipedia. They
compared two approaches for doing so that both
search for parallel sentence pairs in cross-language
linked articles. The first one uses an MT engine in
order to translate sentences of one document into the
language of the other article; then parallel sentences
are selected based on a monolingual similarity mea-
sure. The second approach represents each sentence
of a pair of documents in a space of hyperlink an-
chored texts. An initial lexicon is collected from the
title of the articles that are linked across languages
(they also used the Wikipedia?s redirect feature
to extend the lexicon with synonyms). This lexicon
is used for representing sentences in both languages.
Whenever the anchor text of two hyperlinks, one in
a source sentence, and one in a target sentence is
sanctioned by the lexicon, the ID of the lexicon en-
try is used to represent each hyperlink, thus making
sentences across languages sharing some representa-
tion. They concluded that the latter approach returns
fewer incorrect pairs than the MT based approach.
Smith et al (2010) extended these previous lines
of work in several directions. First, by training a
global classifier which is able to capture the ten-
93
dency of parallel sentences to appear in chunks. Sec-
ond, by applying it at large on Wikipedia. In
their work, they extracted a large number of sen-
tences identified as parallel from linked pairs of arti-
cles. They show that this extra materiel, when added
to the training set, improves a state-of-the-art SMT
system on out-domain test sets, especially when the
in-domain training set is not very large.
The four aforementioned studies implement some
heuristics in order to limit the extraction of paral-
lel sentences to some fruitful document pairs. For
news collections, the publication time can for in-
stance be used for narrowing down the search; while
for Wikipedia articles, the authors concentrate
on document pairs that are linked across languages.
PARADOCS could be used for narrowing the search
space down to a set of parallel or closely parallel
document pairs. We see several ways this could
help the process of extracting parallel fragments.
For one thing, we know that extracting parallel
sentences from a parallel corpus is something we
do well, while extracting parallel sentences from a
comparable corpus is a much riskier enterprise (not
even mentioning time issues). As a matter of fact,
Munteanu et al (2004) mentioned the inherent noise
present in pairs of sentences extracted from com-
parable corpora as a reason why a large set of ex-
tracted sentence pairs does not contribute to improve
an SMT system more that a small but highly specific
parallel dataset. Therefore, a system like ours could
be used to decide which sort of alignment technique
should be used, given a pair of documents. For an-
other thing, one could use our system to delimit a
set of fruitful documents to harvest in the first place.
The material acquired this way could then be used
to train models that could be employed for extract-
ing noisiest document pairs, hopefully for the sake
of the quality of the material extracted.
6 Conclusion
We have described a system for identifying paral-
lel documents in a bilingual collection. This system
does not presume specific information, such as file
(or URL) naming conventions, which can sometime
be useful for mining parallel documents. Also, our
system relies on a very lightweight set of content-
based features (basically numerical entities and pos-
sibly hapax words), therefore our claim of a lan-
guage neutral system.
We conducted a number of experiments on the
Europarl corpus in order to control the impact
of some of its hyper-parameters. We show that
our approach outperforms the fair baseline described
in (Enright and Kondrak, 2007). We also con-
ducted experiments in extracting parallel documents
in Wikipedia. We were satisfied by the fact that
we used a classifier trained on another task in this
experiment, but still got good results (a precision of
80% if we consider noisy parallel document pairs
as acceptable). We conducted a manual evalua-
tion of some cross-language linked article pairs and
found that 25% of those pairs were indeed paral-
lel or noisy parallel. This manually annotated data
that can be downloaded at http://www.iro.
umontreal.ca/?felipe/bucc11/.
7 Future Work
In their study on infobox arbitrage, Adar et al
(2009) noted that currently, cross-language links in
Wikipedia are essentially made by volunteers,
which explains why many such links are missing.
Our approach lends itself to locate missing links
in Wikipedia. Another extension of this line of
work, admittedly more prospective, would be to de-
tect recent vandalizations (modifications or exten-
sions) operated on one language only of a parallel
pair of documents.
Also, we think that there are other kinds of data
on which our system could be invaluable. This is
the reason why we refrained in this work to engi-
neer features tailored for a specific data collection,
such as Wikipedia. One application of our sys-
tem we can think of, is the organization of (pro-
prietary) translation memories. As a matter if fact,
many companies do not organize the flow of the doc-
uments they handle in a systematic way and there is
a need for tools able to spot texts that are in transla-
tion relation.
Acknowledgments
We are grateful to Fabienne Venant who participated
in the manual annotation we conducted in this study.
94
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On
the use of comparable corpora to improve smt per-
formance. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, EACL ?09, pages 16?23.
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-
ing Similar Sentences across Multiple Languages in
Wikipedia. In 11th EACL, pages 62?69, Trento, Italy.
Eytan Adar, Michael Skinner, and Daniel S. Weld. 2009.
Information arbitrage across multi-lingual wikipedia.
In Proceedings of the Second ACM International Con-
ference on Web Search and Data Mining, WSDM ?09,
pages 94?103.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Jiang Chen and Jian-Yun Nie. 2000. Parallel Web text
mining for cross-language IR. In RIAO, pages 62?67,
Paris, France.
Jessica Enright and Gregorz Kondrak. 2007. A Fast
Method for Parallel Document Identification. In
NAACL HLT 2007, Companion Volume, pages 29?32,
Rochester, NY.
Tomaz Erjavec. 2004. MULTEXT-East Version 3:
Multilingual Morphosyntactic Specifications, Lexi-
cons and Corpora. In LREC, Lisbon, Portugal.
Elena Filatova. 2009. Directions for exploiting asymme-
tries in multilingual wikipedia. In Third International
Cross Lingual Information Access Workshop, pages
30?37, Boulder, Colorado.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 57?63, Barcelona, Spain, July. Association for
Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11, Issue 1(10?18).
Philipp Koehn. 2005. Europarl: A multilingual corpus
for evaluation of machine translation. In 10th Machine
Translation Summit, Phuket, Thailand, sep.
Adrien Lardilleux and Yves Lepage. 2007. The con-
tribution of the notion of hapax legomena to word
alignment. In 3rd Language & Technology Conference
(LTC?07), pages 458?462, Poznan? Poland.
Xiaoyi Ma and Mark Liberman. 1999. Bits: A method
for bilingual text search over the web. In Machine
Translation Summit VII, Singapore, sep.
Beata Bandmann Megyesi, Eva Csato Johansson, and
Anna Sgvall Hein. 2006. Building a Swedish-Turkish
Parallel Corpus. In LREC, Genoa, Italy.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 265?272, Boston, Massachusetts, USA,
May. Association for Computational Linguistics.
Alexandre Patry and Philippe Langlais. 2005. Automatic
identification of parallel documents with light or with-
out linguistic resources. In 18th Annual Conference on
Artificial Intelligence (Canadian AI), pages 354?365,
Victoria, British-Columbia, Canada.
Philip Resnik and Noah A. Smith. 2003. The web as a
parallel corpus. Computational Linguistics, 29:349?
380. Special Issue on the Web as a Corpus.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining par-
allel data from the web. In Proceedings of the 21st
International Conference on Computational Linguis-
tics (COLING) and the 44th annual meeting of the As-
sociation for Computational Linguistics (ACL), pages
489?496, Sydney, Australia.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the NAACL, HLT ?10, pages 403?411.
Jo?rg Tiedemann. 2009. News from OPUS ? A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing, pages 237?248. John Benjamins,
Amsterdam/Philadelphia.
Bing Zhao and Stephan Vogel. 2002. Adaptive parallel
sentences mining from web bilingual news collection.
In Proceedings of the 2002 IEEE International Con-
ference on Data Mining, ICDM ?02, pages 745?748,
Maebashi City, Japan.
95
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 80?89,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Translating Government Agencies? Tweet Feeds:
Specificities, Problems and (a few) Solutions
Fabrizio Gotti, Philippe Langlais
{gottif,felipe}@iro.umontreal.ca
RALI-DIRO
Universite? de Montre?al
C.P. 6128, Succ Centre-Ville
Montre?al (Que?bec) Canada
H3C 3J7
Atefeh Farzindar
farzindar@nlptechnologies.ca
NLP Technologies Inc.
52 Le Royer
Montre?al
(Que?bec) Canada
H2Y 1W7
Abstract
While the automatic translation of tweets has
already been investigated in different scenar-
ios, we are not aware of any attempt to trans-
late tweets created by government agencies.
In this study, we report the experimental re-
sults we obtained when translating 12 Twitter
feeds published by agencies and organizations
of the government of Canada, using a state-of-
the art Statistical Machine Translation (SMT)
engine as a black box translation device. We
mine parallel web pages linked from the URLs
contained in English-French pairs of tweets in
order to create tuning and training material.
For a Twitter feed that would have been other-
wise difficult to translate, we report significant
gains in translation quality using this strategy.
Furthermore, we give a detailed account of the
problems we still face, such as hashtag trans-
lation as well as the generation of tweets of
legal length.
1 Introduction
Twitter is currently one of the most popular online
social networking service after Facebook, and is the
fastest-growing, with the half-a-billion user mark
reached in June 2012.1 According to Twitter?s blog,
no less than 65 millions of tweets are published each
day, mostly in a single language (40% in English).
This hinders the spread of information, a situation
witnessed for instance during the Arab Spring.
1http://semiocast.com/publications/
2012_07_30_Twitter_reaches_half_a_billion_
accounts_140m_in_the_US
Solutions for disseminating tweets in different
languages have been designed. One solution con-
sists in manually translating tweets, which of course
is only viable for a very specific subset of the ma-
terial appearing on Twitter. For instance, the non-
profit organization Meedan2 has been founded in or-
der to organize volunteers willing to translate tweets
written in Arabic on Middle East issues. Another
solution consists in using machine translation. Sev-
eral portals are facilitating this,3 mainly by using
Google?s machine translation API.
Curiously enough, few studies have focused on
the automatic translation of text produced within so-
cial networks, even though a growing number of
these studies concentrate on the automated process-
ing of messages exchanged on social networks. See
(Gimpel et al, 2011) for a recent review of some of
them.
Some effort has been invested in translating short
text messages (SMSs). Notably, Munro (2010) de-
scribes the service deployed by a consortium of vol-
unteer organizations named ?Mission 4636? during
the earthquake that struck Haiti in January 2010.
This service routed SMSs alerts reporting trapped
people and other emergencies to a set of volunteers
who translated Haitian Creole SMSs into English,
so that primary emergency responders could under-
stand them. In Lewis (2010), the authors describe
how the Microsoft translation team developed a sta-
tistical translation engine (Haitian Creole into En-
glish) in as little as 5 days, during the same tragedy.
2http://news.meedan.net/
3http://www.aboutonlinetips.com/
twitter-translation-tools/
80
Jehl (2010) addresses the task of translating En-
glish tweets into German. She concludes that the
proper treatment of unknown words is of the utmost
importance and highlights the problem of producing
translations of up to 140 characters, the upper limit
on tweet lengths. In (Jehl et al, 2012), the authors
describe their efforts to collect bilingual tweets from
a stream of tweets acquired programmatically, and
show the impact of such a collection on developing
an Arabic-to-English translation system.
The present study participates in the effort for the
dissemination of messages exchanged over Twitter
in different languages, but with a very narrow focus,
which we believe has not been addressed specifically
yet: Translating tweets written by government in-
stitutions. What sets these messages apart is that,
generally speaking, they are written in a proper lan-
guage (without which their credibility would pre-
sumably be hurt), while still having to be extremely
brief to abide by the ever-present limit of 140 char-
acters. This contrasts with typical social media texts
in which a large variability in quality is observed
(Agichtein et al, 2008).
Tweets from government institutions can also dif-
fer somewhat from some other, more informal so-
cial media texts in their intended audience and ob-
jectives. Specifically, such tweet feeds often attempt
to serve as a credible source of timely information
presented in a way that engages members of the
lay public. As such, translations should present a
similar degree of credibility, ease of understanding,
and ability to engage the audience as in the source
tweet?all while conforming to the 140 character
limits.
This study attempts to take these matters into ac-
count for the task of translating Twitter feeds emitted
by Canadian governmental institutions. This could
prove very useful, since more than 150 Canadian
agencies have official feeds. Moreover, while only
counting 34 million inhabitants, Canada ranks fifth
in the number of Twitter users (3% of all users) after
the US, the UK, Australia, and Brazil.4 This cer-
tainly explains why Canadian governments, politi-
cians and institutions are making an increasing use
of this social network service. Given the need of
4http://www.techvibes.com/blog/how-
canada-stacks-up-against-the-world-on-
twitter-2012-10-17
Canadian governmental institutions to disseminate
information in both official languages (French and
English), we see a great potential value in targeted
computer-aided translation tools, which could offer
a significant reduction over the current time and ef-
fort required to manually translate tweets.
We show that a state-of-the-art SMT toolkit, used
off-the-shelf, and trained on out-domain data is un-
surprisingly not up to the task. We report in Sec-
tion 2 our efforts in mining bilingual material from
the Internet, which proves eventually useful in sig-
nificantly improving the performance of the engine.
We test the impact of simple adaptation scenarios
in Section 3 and show the significant improvements
in BLEU scores obtained thanks to the corpora we
mined. In Section 4, we provide a detailed account
of the problems that remain to be solved, including
the translation of hashtags (#-words) omnipresent
in tweets and the generation of translations of legal
lengths. We conclude this work-in-progress and dis-
cuss further research avenues in Section 5.
2 Corpora
2.1 Bilingual Twitter Feeds
An exhaustive list of Twitter feeds published by
Canadian government agencies and organizations
can be found on the GOV.PoliTWiTTER.ca web
site.5 As of this writing, 152 tweet feeds are listed,
most of which are available in both French and En-
glish, in keeping with the Official Languages Act
of Canada. We manually selected 20 of these feed
pairs, using various exploratory criteria, such as
their respective government agency, the topics being
addressed and, importantly, the perceived degree of
parallelism between the corresponding French and
English feeds.
All the tweets of these 20 feed pairs were gathered
using Twitter?s Streaming API on 26 March 2013.
We filtered out the tweets that were marked by the
API as retweets and replies, because they rarely have
an official translation. Each pair of filtered feeds
was then aligned at the tweet level in order to cre-
ate bilingual tweet pairs. This step was facilitated
by the fact that timestamps are assigned to each
tweet. Since a tweet and its translation are typi-
5http://gov.politwitter.ca/directory/
network/twitter
81
Tweets URLs mis. probs sents
. HealthCanada
1489 995 1 252 78,847
. DFAIT MAECI ? Foreign Affairs and Int?l Trade
1433 65 0 1081 10,428
. canadabusiness
1265 623 1 363 138,887
. pmharper ? Prime Minister Harper
752 114 2 364 12,883
. TCS SDC ? Canadian Trade Commissioner Service
694 358 1 127 36,785
. Canada Trade
601 238 1 92 22,594
. PHAC GC ? Public Health Canada
555 140 0 216 14,617
. cida ca ? Canadian Int?l Development Agency
546 209 2 121 18,343
. LibraryArchives
490 92 1 171 6,946
. CanBorder ? Canadian Border matters
333 88 0 40 9,329
. Get Prepared ? Emergency preparedness
314 62 0 11 10,092
. Safety Canada
286 60 1 17 3,182
Table 1: Main characteristics of the Twitter and URL cor-
pora for the 12 feed pairs we considered. The (English)
feed name is underlined, and stands for the pair of feeds
that are a translation of one another. When not obvious,
a short description is provided. Each feed name can be
found as is on Twitter. See Sections 2.1 and 2.3 for more.
cally issued at about the same time, we were able to
align the tweets using a dynamic programming al-
gorithm miminizing the total time drift between the
English and the French feeds. Finally, we tokenized
the tweets using an adapted version of Twokenize
(O?Connor et al, 2010), accounting for the hashtags,
usernames and urls contained in tweets.
We eventually had to narrow down further the
number of feed pairs of interest to the 12 most pro-
lific ones. For instance, the feed pair PassportCan6
that we initially considered contained only 54 pairs
of English-French tweets after filtering and align-
ment, and was discarded because too scarce.
6https://twitter.com/PassportCan
Did you know it?s best to test for #radon in
the fall/winter? http://t.co/CDubjbpS
#health #safety
L?automne/l?hiver est le meilleur moment pour
tester le taux de radon.
http://t.co/4NJWJmuN #sante? #se?curite
Figure 1: Example of a pair of tweets extracted from the
feed pair HealthCanada .
The main characteristics of the 12 feed pairs we
ultimately retained are reported in Table 1, for a to-
tal of 8758 tweet pairs. The largest feed, in terms
of the number of tweet pairs used, is that of Health
Canada7 with over 1 489 pairs of retained tweets
pairs at the time of acquisition. For reference, that
is 62% of the 2 395 ?raw? tweets available on the
English feed, before filtering and alignment. An ex-
ample of a retained pair of tweets is shown in Fig-
ure 1. In this example, both tweets contain a short-
ened url alias that (when expanded) leads to web-
pages that are parallel. Both tweets also contain so-
called hashtags (#-words): 2 of those are correctly
translated when going from English to French, but
the hashtag #radon is not translated into a hashtag in
French, instead appearing as the plain word radon,
for unknown reasons.
2.2 Out-of-domain Corpora: Parliament
Debates
We made use of two different large corpora in or-
der to train our baseline SMT engines. We used the
2M sentence pairs of the Europarl version 7 corpus.8
This is a priori an out-of-domain corpus, and we did
not expect much of the SMT system trained on this
dataset. Still, it is one of the most popular parallel
corpus available to the community and serves as a
reference.
We also made use of 2M pairs of sentences we
extracted from an in-house version of the Canadian
Hansard corpus. This material is not completely out-
of-domain, since the matters addressed within the
Canadian Parliament debates likely coincide to some
degree with those tweeted by Canadian institutions.
The main characteristics of these two corpora are re-
ported in Table 2. It is noteworthy that while both
7https://twitter.com/HealthCanada
8http://www.statmt.org/europarl/
82
Corpus sents tokens types s length
hansard en 2M 27.1M 62.2K 13.6
hansard fr 2M 30.7M 82.2K 15.4
europarl en 2M 55.9M 94.5K 28.0
europarl fr 2M 61.6M 129.6K 30.8
Table 2: Number of sentence pairs, token and token types
in the out-of-domain training corpora we used. s length
stands for the average sentence length, counted in tokens.
corpora contain an equal number of sentence pairs,
the average sentence length in the Europarl corpus is
much higher, leading to a much larger set of tokens.
2.3 In-domain Corpus: URL Corpus
As illustrated in Figure 1, many tweets act as
?teasers?, and link to web pages containing (much)
more information on the topic the tweet feed typi-
cally addresses. Therefore, a natural way of adapt-
ing a corpus-driven translation engine consists in
mining the parallel text available at those urls.
In our case, we set aside the last 200 tweet pairs of
each feed as a test corpus. The rest serves as the url-
mining corpus. This is necessary to avoid testing our
system on test tweets whose URLs have contributed
to the training corpus.
Although simple in principle, this data collection
operation consists in numerous steps, outlined be-
low:
1. Split each feed pair in two: The last 200 tweet
pairs are set aside for testing purposes, the rest
serves as the url-mining corpus used in the fol-
lowing steps.
2. Isolate urls in a given tweet pair using our to-
kenizer, adapted to handle Twitter text (includ-
ing urls).
3. Expand shortened urls. For instance, the url
in the English example of Figure 1 would
be expanded into http://www.hc-sc.
gc.ca/ewh-semt/radiation/radon/
testing-analyse-eng.php, using the
expansion service located at the domain t.co.
There are 330 such services on the Web.
4. Download the linked documents.
5. Extract all text from the web pages, without tar-
geting any content in particular (the site menus,
breadcrumb, and other elements are therefore
retained).
6. Segment the text into sentences, and tokenize
them into words.
7. Align sentences with our in-house aligner.
We implemented a number of restrictions during
this process. We did not try to match urls in cases
where the number of urls in each tweet differed (see
column mis.?mismatches?in Table 1). The col-
umn probs. (problems) in Table 1 shows the count of
url pairs whose content could not be extracted. This
happened when we encountered urls that we could
not expand, as well as those returning a 404 HTTP
error code. We also rejected urls that were identi-
cal in both tweets, because they obviously could not
be translations. We also filtered out documents that
were not in html format, and we removed document
pairs where at least one document was difficult to
convert into text (e.g. because of empty content, or
problematic character encoding). After inspection,
we also decided to discard sentences that counted
less than 10 words, because shorter sentences are
too often irrelevant website elements (menu items,
breadcrumbs, copyright notices, etc.).
This 4-hour long operation (including download)
yielded a number of useful web documents and ex-
tracted sentence pairs reported in Table 1 (columns
URLs and sents respectively). We observed that the
density of url pairs present in pairs of tweets varies
among feeds. Still, for all feeds, we were able to
gather a set of (presumably) parallel sentence pairs.
The validity of our extraction process rests on the
hypothesis that the documents mentioned in each
pair of urls are parallel. In order to verify this, we
manually evaluated (a posteriori) the parallelness of
a random sample of 50 sentence pairs extracted for
each feed. Quite fortunately, the extracted material
was of excellent quality, with most samples contain-
ing all perfectly aligned sentences. Only canadabusi-
ness, LibraryArchives and CanBorder counted a sin-
gle mistranslated pair. Clearly, the websites of the
Canadian institutions we mined are translated with
great care and the tweets referring to them are metic-
ulously translated in terms of content links.
83
3 Experiments
3.1 Methodology
All our translation experiments were conducted with
Moses? EMS toolkit (Koehn et al, 2007), which in
turn uses gizapp (Och and Ney, 2003) and SRILM
(Stolcke, 2002).
As a test bed, we used the 200 bilingual tweets
we acquired that were not used to follow urls, as de-
scribed in Sections 2.1 and 2.3. We kept each feed
separate in order to measure the performance of our
system on each of them. Therefore we have 12 test
sets.
We tested two configurations: one in which an
out-of-domain translation system is applied (with-
out adaptation) to the translation of the tweets of
our test material, another one where we allowed the
system to look at in-domain data, either at training
or at tuning time. The in-domain material we used
for adapting our systems is the URL corpus we de-
scribed in section 2.3. More precisely, we prepared
12 tuning corpora, one for each feed, each contain-
ing 800 heldout sentence pairs. The same number of
sentence pairs was considered for out-domain tuning
sets, in order not to bias the results in favor of larger
sets. For adaptation experiments conducted at train-
ing time, all the URL material extracted from a spe-
cific feed (except for the sentences of the tuning sets)
was used. The language model used in our experi-
ments was a 5-gram language model with Kneser-
Ney smoothing.
It must be emphasized that there is no tweet mate-
rial in our training or tuning sets. One reason for this
is that we did not have enough tweets to populate our
training corpus. Also, this corresponds to a realistic
scenario where we want to translate a Twitter feed
without first collecting tweets from this feed.
We use the BLEU metric (Papineni et al, 2002)
as well as word-error rate (WER) to measure trans-
lation quality. A good translation system maximizes
BLEU and minimizes WER. Due to initially poor
results, we had to refine the tokenizer mentioned
in Section 2.1 in order to replace urls with serial-
ized placeholders, since those numerous entities typ-
ically require rule-based translations. The BLEU
and WER scores we report henceforth were com-
puted on such lowercased, tokenized and serialized
texts, and did not incur penalties that would have
train tune canadabusiness DFAIT MAECI
fr?en wer bleu wer bleu
hans hans 59.58 21.16 61.79 19.55
hans in 58.70 21.35 60.73 20.14
euro euro 64.24 15.88 62.90 17.80
euro in 63.23 17.48 60.58 21.23
en?fr wer bleu wer bleu
hans hans 62.42 21.71 64.61 21.43
hans in 61.97 22.92 62.69 22.00
euro euro 64.66 19.52 63.91 21.65
euro in 64.61 18.84 63.56 22.31
Table 3: Performance of generic systems versus systems
adapted at tuning time for two particular feeds. The tune
corpus ?in? stands for the URL corpus specific to the feed
being translated. The tune corpora ?hans? and ?euro? are
considered out-of-domain for the purpose of this experi-
ment.
otherwise been caused by the non-translation of urls
(unknown tokens), for instance.
3.2 Translation Results
Table 3 reports the results observed for the two main
configurations we tested, in both translation direc-
tions. We show results only for two feeds here:
canadabusiness, for which we collected the largest
number of sentence pairs in the URL corpus, and
DFAIT MAECI for which we collected very little
material. For canadabusiness, the performance of the
system trained on Hansard data is higher than that
of the system trained on Europarl (? ranging from
2.19 to 5.28 points of BLEU depending on the con-
figuration considered). For DFAIT MAECI , supris-
ingly, Europarl gives a better result, but by a more
narrow margin (? ranging from 0.19 to 1.75 points
of BLEU). Both tweet feeds are translated with
comparable performance by SMT, both in terms
of BLEU and WER. When comparing BLEU per-
formances based solely on the tuning corpus used,
the in-domain tuning corpus created by mining urls
yields better results than the out-domain tuning cor-
pus seven times out of eight for the results shown in
Table 3.
The complete results are shown in Figure 2, show-
ing BLEU scores obtained for the 12 feeds we con-
sidered, when translating from English to French.
Here, the impact of using in-domain data to tune
84
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
in out in out in out in out in out in out in out in out in out in out in out in out
Canada_Trade canadabusiness CanBorder cida_ca DFAIT_MAECI Get_Prepared HealthCanada LibraryArchives PHAC_GC pmharper Safety_Canada TCS_SDC
B
LE
U
 s
co
re
 
euro hans
train
corpus tune
Moyenne de bleu
direction
Figure 2: BLEU scores measured on the 12 feed pairs we considered for the English-to-French translation direction.
For each tweet test corpus, there are 4 results: a dark histogram bar refers to the Hansard training corpus, while a
lighter grey bar refers to an experiment where the training corpus was Europarl. The ?in? category on the x-axis
designates an experiment where the tuning corpus was in-domain (URL corpus), while the ?out? category refers to an
out-of-domain tuning set. The out-of-domain tuning corpus is Europarl or Hansard, and always matches the nature of
training corpora.
the system is hardly discernible, which in a sense
is good news, since tuning a system for each feed
is not practical. The Hansard corpus almost always
gives better results, in keeping with its status as a
corpus that is not so out-of-domain as Europarl, as
mentioned above. The results for the reverse trans-
lation direction show the same trends.
In order to try a different strategy than using only
tuning corpora to adapt the system, we also investi-
gated the impact of training the system on a mix of
out-of-domain and in-domain data. We ran one of
the simplest adaptation scenarios where we concate-
nated the in-domain material (train part of the URL
corpus) to the out-domain one (Hansard corpus) for
the two feeds we considered in Table 3. The results
are reported in Table 4.
We measured significant gains both in WER and
BLEU scores in conducting training time versus tun-
ing time adaptation, for the canadabusiness feed (the
largest URL corpus). For this corpus, we observe
an interesting gain of more than 6 absolute points in
BLEU scores. However, for the DFAIT MAECI (the
smallest URL corpus) we note a very modest loss in
translation quality when translating from French and
a significant gain in the other translation direction.
These figures could show that mining parallel sen-
tences present in URLs is a fruitful strategy for adapt-
ing the translation engine for feeds like canadabusi-
ness that display poor performance otherwise, with-
out harming the translation quality for feeds that per-
Train corpus WER BLEU
fr?en
hans+canbusiness 53.46 (-5.24) 27.60 (+6.25)
hans+DFAIT 60.81 (+0.23) 20.83 (-0.40)
en?fr
hans+canbusiness 57.07 (-4.90) 26.26 (+3.34)
hans+DFAIT 61.80 (-0.89) 24.93 (+2.62)
Table 4: Performance of systems trained on a concatena-
tion of out-of-domain and in-domain data. All systems
were tuned on in-domain data. Absolute gains are shown
in parentheses, over the best performance achieved so far
(see Table 3).
form reasonably well without additional resources.
Unfortunately, it suggests that retraining a system is
required for better performance, which might hinder
the deployment of a standalone translation engine.
Further research needs to be carried out to determine
how many tweet pairs must be used in a parallel URL
corpus in order to get a sufficiently good in-domain
corpus.
4 Analysis
4.1 Translation output
Examples of translations produced by the best sys-
tem we trained are reported in Figure 3. The first
translation shows a case of an unknown French word
(soumissionnez). The second example illustrates
85
a typical example where the hashtags should have
been translated but were left unchanged. The third
example shows a correct translation, except that the
length of the translation (once the text is detok-
enized) is over the size limit allowed for a tweet.
Those problems are further analyzed in the remain-
ing subsections.
4.2 Unknown words
Unknown words negatively impact the quality of
MT output in several ways. First, they typically ap-
pear untranslated in the system?s output (we deemed
most appropriate this last resort strategy). Sec-
ondly, they perturb the language model, which often
causes other problems (such as dubious word order-
ing). Table 5 reports the main characteristics of the
words from all the tweets we collected that were not
present in the Hansard train corpus.
The out-of-vocabulary rate with respect to token
types hovers around 33% for both languages. No
less than 42% (resp. 37%) of the unknown English
(resp. French) token types are actually hashtags. We
defer their analysis to the next section. Also, 15%
(resp. 10%) of unknown English token types are
user names (@user), which do not require transla-
tion.
English French
tweet tokens 153 234 173 921
tweet types 13 921 15 714
OOV types 4 875 (35.0%) 5 116 (32.6%)
. hashtag types 2 049 (42.0%) 1 909 (37.3%)
. @user types 756 (15.5%) 521 (10.2%)
Table 5: Statistics on out-of-vocabulary token types.
We manually analyzed 100 unknown token types
that were not hashtags or usernames and that did not
contain any digit. We classified them into a num-
ber of broad classes whose distributions are reported
in Table 6 for the French unknown types. A simi-
lar distribution was observed for English unknown
types. While we could not decide of the nature of
21 types without their context of use (line ?type),
we frequently observed English types, as well as
acronyms and proper names. A few unknown types
result from typos, while many are indeed true French
types unseen at training time (row labeled french ),
some of which being very specific (term). Amus-
ingly, the French verbal neologism twitter (to tweet)
is unknown to the Hansard corpus we used.
french 26 sautez, perforateurs , twitter
english 22 successful , beauty
?types 21 bumbo , tra
name 11 absorbica , konzonguizi
acronym 7 hna , rnc
typo 6 gazouilli , pendan
term 3 apostasie , sibutramine
foreign 2 aanischaaukamikw, aliskiren
others 2 francophonesURL
Table 6: Distribution of 100 unknown French token types
(excluding hashtags and usernames).
4.3 Dealing with Hashtags
We have already seen that translating the text in
hashtags is often suitable, but not always. Typically,
hashtags in the middle of a sentence are to be trans-
lated, while those at the end typically should not be.
A model should be designed for learning when to
translate an hashtag or not. Also, some hashtags are
part of the sentence, while others are just (semantic)
tags. While a simple strategy for translating hash-
tags consists in removing the # sign at translation
time, then restoring it afterwards, this strategy would
fail in a number of cases that require segmenting the
text of the hashtag first. Table 7 reports the per-
centage of hashtags that should be segmented before
being translated, according to a manual analysis we
conducted over 1000 hashtags in both languages we
considered. While many hashtags are single words,
roughly 20% of them are not and require segmenta-
tion.
4.4 Translating under size constraints
The 140 character limit Twitter imposes on tweets is
well known and demands a certain degree of conci-
sion even human users find sometimes bothersome.
For machine output, this limit becomes a challeng-
ing problem. While there exists plain?but inelegant?
workarounds9, there may be a way to produce tweet
translations that are themselves Twitter-ready. (Jehl,
9The service eztweets.com splits long tweets into smaller
ones; twitlonger.com tweets the beginning of a long message,
86
SRC: vous soumissionnez pour obtenir de gros contrats ? voici 5 pratiques exemplaires a` suivre . URL
TRA: you soumissionnez big contracts for best practices ? here is 5 URL to follow .
REF: bidding on big contracts ? here are 5 best practices to follow . URL
SRC: avis de #sante?publique : maladies associe?es aux #salmonelles et a` la nourriture pour animaux de com-
pagnie URL #rappel
TRA: notice of #sante?publique : disease associated with the #salmonelles and pet food #rappel URL
REF: #publichealth notice : illnesses related to #salmonella and #petfood URL #recall
SRC: des ha??tiens de tous les a?ges , milieux et me?tiers te?moignent de l? aide qu? ils ont rec?ue depuis le se?isme
. URL #ha??ti
TRA: the haitian people of all ages and backgrounds and trades testify to the assistance that they have received
from the earthquake #ha??ti URL .
REF: #canada in #haiti : haitians of all ages , backgrounds , and occupations tell of the help they received .
URL
Figure 3: Examples of translations produced by an engine trained on a mix of in- and out-of-domain data.
w. en fr example
1 76.5 79.9 intelligence
2 18.3 11.9 gender equality
3 4.0 6.0 africa trade mission
4 1.0 1.4 closer than you think
5 0.2 0.6 i am making a difference
6 ? 0.2 fonds aide victime se?cheresse
afrique est
Table 7: Percentage of hashtags that require segmentation
prior to translation. w. stands for the number of words
into which the hashtag text should be segmented.
2010) pointed out this problem and reported that
3.4% of tweets produced were overlong, when trans-
lating from German to English. The reverse direc-
tions produced 17.2% of overlong German tweets.
To remedy this, she tried modifying the way BLEU
is computed to penalize long translation during the
tuning process, with BLEU scores worse than sim-
ply truncating the illegal tweets. The second strategy
the author tried consisted in generating n-best lists
and mining them to find legal tweets, with encour-
aging results (for n = 30 000), since the number
of overlong tweets was significantly reduced while
leaving BLEU scores unharmed.
In order to assess the importance of the problem
for our system, we measured the lengths of tweets
that a system trained like hans+canbusiness in Ta-
ble 4 (a mix of in- and out-of-domain data) could
produce. This time however, we used a larger test set
and provides a link to read the remainder. One could also simply
truncate an illegal tweet and hope for the best...
counting 498 tweets. To measure the lengths of their
translations, we first had to detokenize the transla-
tions produced, since the limitation applies to ?nat-
ural? text only. For each URL serialized token, we
counted 18 characters, the average length of a (short-
ened) url in a tweet. When translating from French
to English, the 498 translations had lengths ranging
from 45 to 138 characters; hence, they were all legal
tweets. From English to French, however, the trans-
lations are longer, and range from 32 characters to
223 characters, with 22.5% of them overlong.
One must recall that in our experiments, no tweets
were seen at training or tuning time, which explains
why the rate of translations that do not meet the
limit is high. This problem deserves a specific treat-
ment for a system to be deployed. One interest-
ing solution already described by (Jehl, 2010) is to
mine the n-best list produced by the decoder in or-
der to find the first candidate that constitutes a legal
tweet. This candidate is then picked as the trans-
lation. We performed this analysis on the canad-
abusiness output described earlier, from English to
French. We used n =1, 5, 10, 20, 50, 100, 200, 500,
1000, 5000, 10000, 30000 and computed the result-
ing BLEU scores and remaining percentage of over-
long tweets. The results are shown in Figure 4. The
results clearly show that the n-best list does contain
alternate candidates when the best one is too long.
Indeed, not only do we observe that the percentage
of remaining illegal tweets can fall steadily (from
22.4% to 6.6% for n = 30 000) as we dig deeper into
the list, but also the BLEU score stays unharmed,
showing even a slight improvement, from an ini-
87
tial 26.16 to 26.31 for n = 30 000. This counter-
intuitive result in terms of BLEU is also reported
in (Jehl, 2010) and is probably due to a less harsh
brevity penalty by BLEU on shorter candidates.
2013-03-01
nbest list
n wer ser BLEU
1 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.159.35 97.79 0.26 6
5 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.559.3 97.79 0.2622
10 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1059.31 97.79 0.2623
20 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.2059.31 97.79 0.26 2
50 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.5059.34 97.79 0.2622
100 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.10059.27 97.79 0.2628
200 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.20059.23 97.79 0.2633
500 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.50059.24 97.79 0 263
1000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.100059.21 97.79 0.2634
5000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.500059.26 97.79 0.2635
10000 /u/gottif/proj/nlptech2012/data/tweets/correction/nbest/trans.1000059.26 97.79 0.2638
30000 /u/gottif/p j/nlptech2012/data/tweets/correction/nbest/trans.3000059.32 97.79 0.26 1
0%
5%
10%
15%
20%
25%
0.261
0.262
0.262
0.263
0.263
0.264
0.264
1 10 100 1000 10000 100000
%
 o
f o
ve
rlo
ng
 tw
ee
ts
 
BL
EU
 sc
or
e 
n-best list length (n) 
BLEU % overlong
Figure 4: BLEU scores and percentage of overlong
tweets when mining the n-best list for legal tweets, when
the first candidate is overlong. The BLEU scores (dia-
mond series) should be read off the left-hand vertical axis,
while the remaining percentage of illegal tweets (circle
series) should be read off the right-hand axis.
5 Discussion
We presented a number of experiments where we
translated tweets produced by Canadian govern-
ments institutions and organizations. Those tweets
have the distinguishing characteristic (in the Twitter-
sphere) of being written in proper English or French.
We show that mining the urls mentioned in those
tweets for parallel sentences can be a fruitful strat-
egy for adapting an out-of-domain translation engine
to this task, although further research could show
other ways of using this resource, whose quality
seems to be high according to our manual evalua-
tion. We also analyzed the main problems that re-
main to be addressed before deploying a useful sys-
tem.
While we focused here on acquiring useful cor-
pora for adapting a translation engine, we admit that
the adaptation scenario we considered is very sim-
plistic, although efficient. We are currently inves-
tigating the merit of different methods to adaptation
(Zhao et al, 2004; Foster et al, 2010; Daume III and
Jagarlamudi, 2011; Razmara et al, 2012; Sankaran
et al, 2012).
Unknown words are of concern, and should be
dealt with appropriately. The serialization of urls
was natural, but it could be extended to usernames.
The latter do not need to be translated, but reduc-
ing the vocabulary is always desirable when work-
ing with a statistical machine translation engine.
One interesting subcategories of out-of-vocabulary
tokens are hashtags. According to our analysis,
they require segmentation into words before being
translated in 20% of the cases. Even if they are
transformed into regular words (#radon?radon or
#genderequality?gender equality), however, it is
not clear at this point how to detect if they are used
like normally-occurring words in a sentence, as in
(#radon is harmful) or if they are simply tags added
to the tweet to categorize it.
We also showed that translating under size con-
straints can be handled easily by mining the n-best
list produced by the decoder, but only up to a point.
A remaining 6% of the tweets we analyzed in detail
could not find a shorter version. Numerous ideas
are possible to alleviate the problem. One could for
instance modify the logic of the decoder to penal-
ize hypotheses that promise to yield overlong trans-
lations. Another idea would be to manually in-
spect the strategies used by governmental agencies
on Twitter when attempting to shorten their mes-
sages, and to select those that seem acceptable and
implementable, like the suppression of articles or the
use of authorized abbreviations.
Adapting a translation pipeline to the very specific
world of governmental tweets therefore poses mul-
tiple challenges, each of which can be addressed in
numerous ways. We have reported here the results of
a modest but fertile subset of these adaptation strate-
gies.
Acknowledgments
This work was funded by a grant from the Natu-
ral Sciences and Engineering Research Council of
Canada. We also wish to thank Houssem Eddine
Dridi for his help with the Twitter API.
88
References
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding high-
quality content in social media. In Proceedings of
the 2008 International Conference on Web Search and
Data Mining, WSDM ?08, pages 183?194.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In 49th ACL, pages 407?412, Portland,
Oregon, USA, June.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP,
pages 451?459, Cambridge, MA, October.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL
(Short Papers), pages 42?47.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012. Twit-
ter translation using translation-based cross-lingual re-
trieval. In 7th Workshop on Statistical Machine Trans-
lation, pages 410?421, Montre?al, June.
Laura Jehl. 2010. Machine translation for twitter. Mas-
ter?s thesis, School of Philosophie, Psychology and
Language Studies, University of Edinburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondr?ej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. pages 177?
180.
William D. Lewis. 2010. Haitian creole: How to build
and ship an mt engine from scratch in 4 days, 17 hours,
& 30 minutes. In EAMT, Saint-Raphael.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation, Denver.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory Search and Topic
Summarization for Twitter. In William W. Cohen,
Samuel Gosling, William W. Cohen, and Samuel
Gosling, editors, ICWSM. The AAAI Press.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei J.
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th ACL, Jeju, Republic of Korea, jul.
Baskaran Sankaran, Majid Razmara, Atefeh Farzindar,
Wael Khreich, Fred Popowich, and Anoop Sarkar.
2012. Domain adaptation techniques for machine
translation and their evaluation in a real-world setting.
In Proceedings of 25th Canadian Conference on Arti-
ficial Intelligence, Toronto, Canada, may.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901?904, Denver, USA.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In 20th
COLING.
89
