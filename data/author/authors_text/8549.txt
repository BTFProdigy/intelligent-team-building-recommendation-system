Coling 2008: Companion volume ? Posters and Demonstrations, pages 189?192
Manchester, August 2008
Temporal Processing with the TARSQI Toolkit
Marc Verhagen
Brandeis University
Computer Science Dept.
Waltham, MA 02454-9110
marc@cs.brandeis.edu
James Pustejovsky
Brandeis University
Computer Science Dept.
Waltham, MA 02454-9110
jamesp@cs.brandeis.edu
Abstract
We present the TARSQI Toolkit (TTK),
a modular system for automatic temporal
and event annotation of natural language
texts. TTK identifies temporal expressions
and events in natural language texts, and
parses the document to order events and to
anchor them to temporal expressions.
1 Introduction
A keyword-based search is not sufficient to answer
temporally loaded questions like ?did Brazil win
the soccer world championship in 1970?? since
a boolean keyword search cannot distinguish be-
tween those documents where the event win is ac-
tually anchored to the year 1970 versus those that
are not. The TARSQI Project (Temporal Aware-
ness and Reasoning Systems for Question Inter-
pretation) focused on enhancing natural language
question answering systems so that temporally-
based questions about the events and entities in
news articles can be addressed. To explicitly mark
the needed temporal relations the project deliv-
ered a series of tools for extracting time expres-
sions, events, subordination relations and tempo-
ral relations (Verhagen et al, 2005; Mani et al,
2006; Saur?? et al, 2005; Saur?? et al, 2006a). But
although those tools performed reasonably well,
they were not integrated in a principled way.
This paper describes the TARSQI Toolkit
(TTK), which takes the TARSQI components and
integrates them into a temporal parsing framework.
The toolkit is different from the system described
in (Verhagen et al, 2005) in several major aspects:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1. the components were integrated in a toolkit
which, amongst others, split the parsing of
properties typical for a particular document
type from the temporal parsing of the text
2. a component was added that takes the re-
sults from the various components that gen-
erate temporal relations and merges them into
a consistent temporal graph
3. a new way of visualizing the results was used
In addition, some components were updated and
test suites with unit tests and regression tests were
added. In this paper, we focus on the merging of
temporal links and the visualization of temporal re-
lations.
There has been a fair amount of recent re-
search on extraction of temporal relations, includ-
ing (Chambers et al, 2007; Lapata and Lascarides,
2006; Bramsen et al, 2006; Bethard and Martin,
2007; Min et al, 2007; Pus?cas?u, 2007). However,
we are not aware of approaches that integrate tem-
poral relations from various sources in one consis-
tent whole.
All TTK components use the TimeML anno-
tation language (Pustejovsky et al, 2003; Puste-
jovsky et al, 2005). TimeML is an annotation
scheme for markup of events, times, and their
temporal relations in news articles. The TimeML
scheme flags tensed verbs, adjectives, and nomi-
nals with EVENT tags with various attributes, in-
cluding the class of event, tense, grammatical as-
pect, polarity (negative or positive), and any modal
operators which govern the event being tagged.
Time expressions are flagged with TIMEX3 tags,
an extension of the ACE 2004 TIMEX2 annotation
scheme (tern.mitre.org).
189
Subordination relations between events, as for
example between reporting events and the em-
bedded event reported on, are annotated with the
SLINK tag. For temporal relations, TimeML de-
fines a TLINK tag that links tagged events to other
events and/or times.
In section 2, we will give a short overview of the
toolkit. In section 3, we focus on the component
that merges TLINKs, and in section 4 we will dwell
on the visualization of temporal relations.
2 Overview of the toolkit
The overall architecture of TTK is illustrated in
figure 1 below. Input text is first processed by the
DocumentModel, which takes care of document-
level properties like encoding and meta tags. The
DocumentModel hands clean text to the other
components which are allowed to be more generic.
DocumentModel
PreProcessing
Text
GUTime Evita
Slinket
Temporal Processing
Temporal Parse
Figure 1: TTK Architecture
The preprocessor uses standard approaches to
tokenization, part-of-speech tagging and chunk-
ing. GUTime is a temporal expression tagger that
recognizes the extents and normalized values of
time expressions. Evita is a domain-independent
event recognition tool that performs two main
tasks: robust event identification and analysis of
grammatical features such as tense and aspect.
Slinket is an application developed to automat-
ically introduce SLINKs, which in TimeML spec-
ify subordinating relations between pairs of events,
and classify them into factive, counterfactive, evi-
dential, negative evidential, and modal, based on
the modal force of the subordinating event (Saur??
et al, 2006b). SLINKs are introduced by a well-
delimited subgroup of verbal and nominal predi-
cates (such as regret, say, promise and attempt),
and in most cases clearly signaled by a subordina-
tion context. Slinket thus relies on a combination
of lexical and syntactic knowledge.
The temporal processing stage includes three
modules that generate TLINKs: Blinker, S2T and
the TLink Classifier.
Blinker is a rule-based component that applies
to certain configurations of events and timexes. It
contains rule sets for the following cases: (i) event
and timex in the same noun phrase, (ii) events
and the document creation time, (iii) events with
their syntactically subordinated events, (iv) events
in conjunctions, (v) two main events in consecu-
tive sentences, and (vi) timexes with other timexes.
Each of these rule sets has a different flavor. For
example, the rules in (vi) simply calculate differ-
ences in the normalized ISO value of the timex
tag while the rules in (v) refer to the tense and
aspect values of the two events. Blinker is a re-
implementation and extension of GutenLink (Ver-
hagen et al, 2005).
S2T takes the output of Slinket and uses about a
dozen syntactic rules to map SLINKs onto TLINKs.
For example, one S2T rule encodes that in SLINKs
with reporting verbs where both events are in past
tense, the reporting event occurred after the event
reported on.
The TLink Classifier is a MaxEnt classifier
that identifies temporal relations between identi-
fied events in text. The classifier accepts its input
for each pair of events under consideration as a set
of features. It is trained on the TimeBank corpus
(see www.timeml.org).
Of the three TLINK generating components, S2T
derives a relatively small number of TLINKs, but
Blinker and the classifier are quite prolific. In
many cases the TLINKs derived by Blinker and the
classifier are inconsistent with each other. The sys-
tem in (Verhagen et al, 2005) used a simple voting
mechanism that favors TLINKs from components
that exhibit higher precision. In addition, if con-
fidence measures are available then these can be
used by the voting mechanism. However, this ap-
proach does not factor in consistency of temporal
relations: choosing the TLINKs with the highest
probability may result in TLINKs that are incon-
sistent. For example, say we have two TLINKs:
BEFORE(x,y) and BEFORE(y,z). And say we have
190
two competing TLINKs, derived by Blinker and
the classifier respectively: BEFORE(x,z) and BE-
FORE(z,x). If the second of these two has a higher
confidence, then we will end up with an inconsis-
tent annotation. In the following section we de-
scribe how in TTK this problem is avoided.
3 Link Merger
The link merger, together with the three TLINK-
generating components, is part of the temporal
processing module of TTK, as shown in the dia-
gram in figure 2 below.
Blinker
Classifier
S2T
Events 
and 
Times
ALinks
SLinks
Link Merging
SputLink
Figure 2: TTK Temporal Processing
The link merging component uses a greedy al-
gorithm to merge TLinks into a consistent whole.
First all links are ordered on their confidence score.
Currently these scores are either global or local.
Global confidence scores are derived from the ob-
served precision of the component that generated
the links. For example, links generated by S2T are
considered high precision and are always deemed
more reliable than links generated by the classifier.
Links generated by the classifier come with a con-
fidence score assigned by the classifier and these
scores are used to order all classifier links.
Merging proceeds by first creating a graph that
contains all events and time expressions as nodes,
but that has no constraints expressed on the edges.
Those constraints are added by the temporal links.
Links are ordered on confidence score and are
added one by one. Each time a link is added a con-
straint propagation component named Sputlink,
based on Allen?s interval algebra (Allen, 1983;
Verhagen, 2005), is applied. If a link cannot be
added because it is inconsistent with the constraint
already on the edge, then the link is skipped. The
result is a consistent annotation where high preci-
sion links are prefered over lower precision links.
4 Visualization
Providing a good visualization of a temporal graph
can be tricky. A table of temporal relations is
only useful for relations inside sentences. Full
graphs, like the ones generated by GraphViz
(http://www.graphviz.org/), do not make it that
much easier for the reader to quickly obtain a pic-
ture of the temporal structure of the document.
Timelines can be misleading because so many
events in a document cannot be ordered with re-
spect to a time stamp.
TTK uses a visualization scheme named TBox
(Verhagen, 2007). It uses left-to-right arrows,
box inclusion and stacking to encode temporal
precedence, inclusion, and simultaneity respec-
tively (see figure 3).
Figure 3: The TBox Representation
This visualization makes it easier to convey the
temporal content of a document since temporal re-
lations are strictly and unambiguously mapped to
specific ways of drawing them. And vice versa, a
particular way of positioning two events always in-
dicates the same temporal relation. Note that ver-
tical positioning does not imply any temporal rela-
tion.
5 Conclusion and Future Work
We have described TTK, a toolkit that integrates
several components that generate tags to mark
up events and time expressions, as well as non-
consuming tags that encode relations between
events and times. TTK includes a module that
combines potentially conflicting temporal rela-
tions into a consistent temporal graph of a docu-
ment, which can be succinctly displayed using the
TBox representation.
In current work, we are exploring how to split up
the task of temporal relation extraction into more
subtasks and write specialized components, both
rule-based and machine learning based, to extract
temporal relations for that task. The link merging
would then have many more input streams, each
with their own reported reliability.
The TARSQI Toolkit can be downloaded from
http://timeml.org/site/tarsqi/toolkit/.
191
Acknowledgments
The work reported in the paper was carried
out in the context of the AQUAINT program
and was funded under ARDA/DoD/IARPA grant
NBCHC040027.
References
Allen, James. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
Bethard, Steven and James H. Martin. 2007. CU-
TMP: Temporal relation classification using syntac-
tic and semantic features. In Proceedings of the
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 129?132, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Bramsen, Philip, Pawan Deshpande, Yoong Keok Lee-
and, and Regina Barzilay. 2006. Finding tempo-
ral order in discharge summaries. In Proceedings of
EMNLP.
Chambers, Nathanael, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 173?176, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Lapata, Mirella and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Mani, Inderjeet, Ben Wellner, Marc Verhagen,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 44th Annual Meeting of the Association for
Computational Linguistics, Sydney. ACL.
Min, Congmin, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to tem-
poral relation identification in news text. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 219?
222, Prague, Czech Republic, June. Association for
Computational Linguistics.
Pus?cas?u, Georgiana. 2007. WVALI: Temporal re-
lation identification by syntactico-semantic analy-
sis. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 484?487, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Pustejovsky, James, Jos?e Casta?no, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust Specification of
Event and Temporal Expressions in Text. In IWCS-5
Fifth International Workshop on Computational Se-
mantics.
Pustejovsky, James, Robert Knippen, Jessica Littman,
and Roser Saur??. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39:123?164, May.
Saur??, Roser, Robert Knippen, Marc Verhagen, and
James Pustejovsky. 2005. Evita: A robust event
recognizer for qa systems. In Proceedings of the
HLT/EMNLP 2005, pages 700?707.
Saur??, Roser, Marc Verhagen, and James Pustejovsky.
2006a. Annotating and recognizing event modal-
ity in text. In Proceedings of of the 19th Inter-
national FLAIRS Conference, FLAIRS 2006, Mel-
bourne Beach, Florida, USA.
Saur??, Roser, Marc Verhagen, and James Pustejovsky.
2006b. SlinkET: A partial modal parser for events.
In Proceedings of LREC 2006, Genoa, Italy.
Verhagen, Marc, Inderjeet Mani, Roser Sauri, Jes-
sica Littman, Robert Knippen, Seok Bae Jang,
Anna Rumshisky, John Phillips, and James Puste-
jovsky. 2005. Automating temporal annotation with
TARSQI. In Proceedings of the 43th Annual Meet-
ing of the Association for Computational Linguistics,
Ann Arbor, USA. Demo session.
Verhagen, Marc. 2005. Temporal closure in an annota-
tion environment. Language Resources and Evalua-
tion, 39:211?241, May.
Verhagen, Marc. 2007. Drawing TimeML relations
with TBox. In Katz, Graham, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, volume 4795
of Lecture Notes in Computer Science, pages 7?28.
Springer.
192
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 700?707, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Evita: A Robust Event Recognizer For QA Systems
Roser Saur?? Robert Knippen Marc Verhagen James Pustejovsky
Lab for Linguistics and Computation
Computer Science Department
Brandeis University
415 South Street, Waltham, MA 02454, USA
 roser,knippen,marc,jamesp@cs.brandeis.edu
Abstract
We present Evita, an application for rec-
ognizing events in natural language texts.
Although developed as part of a suite of
tools aimed at providing question answer-
ing systems with information about both
temporal and intensional relations among
events, it can be used independently as
an event extraction tool. It is unique in
that it is not limited to any pre-established
list of relation types (events), nor is it re-
stricted to a specific domain. Evita per-
forms the identification and tagging of
event expressions based on fairly simple
strategies, informed by both linguistic-
and statistically-based data. It achieves a
performance ratio of 80.12% F-measure.1
1 Introduction
Event recognition is, after entity recognition, one of
the major tasks within Information Extraction. It is
currently being succesfully applied in different ar-
eas, like bioinformatics and text classification. Rec-
ognizing events in these fields is generally carried
out by means of pre-defined sets of relations, possi-
bly structured into an ontology, which makes such
tasks domain dependent, but feasible. Event recog-
nition is also at the core of Question Answering,
1This work was supported by a grant from the Advanced
Research and Development Activity in Information Technology
(ARDA), a U.S. Government entity which sponsors and pro-
motes research of import to the Intelligence Community which
includes but is not limited to the CIA, DIA, NSA, NIMA, and
NRO.
since input questions touch on events and situations
in the world (states, actions, properties, etc.), as they
are reported in the text. In this field as well, the use
of pre-defined sets of relation patterns has proved
fairly reliable, particularly in the case of factoid type
queries (Brill et al, 2002; Ravichandran and Hovy,
2002; Hovy et al, 2002; Soubbotin and Soubbotin,
2002).
Nonetheless, such an approach is not sensitive to
certain contextual elements that may be fundamental
for returning the appropriate answer. This is for in-
stance the case in reporting or attempting contexts.
Given the passage in (1a), a pattern-generated an-
swer to question (1b) would be (1c). Similarly, dis-
regarding the reporting context in example (2) could
erroneously lead to concluding that no one from the
White House was involved in the Watergate affair.
(1) a. Of the 14 known ways to reach the summit, only
the East Ridge route has never been successfully
climbed since George Mallory and Andrew ?Sandy?
Irvine first attempted to climb Everest in 1924.
b. When did George Mallory and Andrew Irvine first
climb Everest?
c. #In 1924.
(2) a. Nixon claimed that White House counsel John Dean
had conducted an investigation into the Watergate
matter and found that no-one from the White House
was involved.
b. What members of the White House were involved in
the Watergate matter?
c. #Nobody.
Intensional contexts like those above are gener-
ated by predicates referring to events of attempting,
intending, commanding, and reporting, among oth-
ers. When present in text, they function as modal
700
qualifiers of the truth of a given proposition, as in
example (2), or they indicate the factuality nature
of the event expressed by the proposition (whether
it happened or not), as in (1) (Saur?? and Verhagen,
2005).
The need for a more sophisticated approach that
sheds some awareness on the specificity of certain
linguistic contexts is in line with the results ob-
tained in previous TREC Question Answering com-
petitions (Voorhees, 2002, 2003). There, a system
that attempted a minimal understanding of both the
question and the answer candidates, by translating
them into their logical forms and using an infer-
ence engine, achieved a notably higher score than
any surface-based system (Moldavan et al, 2002;
Harabagiu et al, 2003).
Non-factoid questions introduce an even higher
level of difficulty. Unlike factoid questions, there
is no simple or unique answer, but more or less sat-
isfactory ones instead. In many cases, they involve
dealing with several events, or identifying and rea-
soning about certain relations among events which
are only partially stated in the source documents
(such as temporal and causal ones), all of which
makes the pattern-based approach less suitable for
the task (Small et al, 2003, Soricut and Brill, 2004).
Temporal information in particular plays a signifi-
cant role in the context of question answering sys-
tems (Pustejovsky et al, forthcoming). The ques-
tion in (3), for instance, requires identifying a set
of events related to the referred killing of peasants
in Mexico, and subsequently ordering them along a
temporal axis.
(3) What happened in Chiapas, Mexico, after the killing of
45 peasants in Acteal?
Reasoning about events in intensional contexts,
or with event-ordering relations such as temporality
and causality, is a requisite for any open-domain QA
system aiming at both factoid and non-factoid ques-
tions. As a first step, this involves the identification
of all relevant events reported in the source docu-
ments, so that later processing stages can locate in-
tensional context boundaries and temporal relations
among these events.
In this article, we present Evita, a tool for recog-
nizing events in natural language texts. It has been
developed as part of a suite of tools aimed at provid-
ing QA systems with information about both tem-
poral and intensional relations between events; we
anticipate, however, that it will be useful for other
NLP tasks as well, such as narrative understanding,
summarization, and the creation of factual databases
from textual sources.
In the next section, we provide the linguistic foun-
dations and technical details of our event recognizer
tool. Section 3 gives the results and discusses them
in the context of the task. We conclude in section 4,
with an overview of Evita?s main achievements and
a brief discussion of future directions.
2 Evita, An Event Recognition Tool
Evita (?Events In Text Analyzer?) is an event recog-
nition system developed under the ARDA-funded
TARSQI research framework. TARSQI is devoted
to two complementary lines of work: (1) estab-
lishing a specification language, TimeML, aimed
at capturing the richness of temporal and event re-
lated information in language (Pustejovsky et al,
2003a, forthcoming), and (2) the construction of a
set of tools that perform tasks of identifying, tag-
ging, and reasoning about eventive and temporal in-
formation in natural language texts (Pustejovsky and
Gaizauskas, forthcoming, Mani, 2005; Mani and
Schiffman, forthcoming; Verhagen, 2004; Verhagen
et al, 2005; Verhagen and Knippen, forthcoming).
Within TARSQI?s framework, Evita?s role is locat-
ing and tagging all event-referring expressions in the
input text that can be temporally ordered.
Evita combines linguistic- and statistically-based
techniques to better address all subtasks of event
recognition. For example, the module devoted to
recognizing temporal information that is expressed
through the morphology of certain event expressions
(such as tense and aspect) uses grammatical infor-
mation (see section 2.4), whereas disambiguating
nouns that can have both eventive and non-eventive
interpretations is carried out by a statistical module
(section 2.3).
The functionality of Evita breaks down into two
parts: event identification and analysis of the event-
based grammatical features that are relevant for tem-
poral reasoning purposes. Both tasks rely on a pre-
processing step which performs part-of-speech tag-
701
ging and chunking, and on a module for cluster-
ing together chunks that refer to the same event.
In the following subsection we provide the linguis-
tic assumptions informing Evita. Then, subsections
2.2 to 2.5 provide a detailed description of Evita?s
different subcomponents: preprocessing, clustering
of chunks, event identification, and analysis of the
grammatical features associated to events.
2.1 Linguistic settings
TimeML identifies as events those event-denoting
expressions that participate in the narrative of a
given document and which can be temporally or-
dered. This includes all dynamic situations (punc-
tual or durative) that happen or occur in the text, but
also states in which something obtains or holds true,
if they are temporally located in the text. As a result,
generics and most state-denoting expressions are fil-
tered out (see Saur?? et al (2004) for a more exhaus-
tive definition of the criteria for event candidacy in
TimeML).
Event-denoting expressions are found in a wide
range of syntactic expressions, such as finite clauses
(that no-one from the White House was involved),
nonfinite clauses (to climb Everest), noun phrases
headed by nominalizations (the young industry?s
rapid growth, several anti-war demonstrations)
or event-referring nouns (the controversial war),
and adjective phrases (fully prepared).
In addition to identifying the textual extent of
events, Evita also analyzes certain grammatical fea-
tures associated with them. These include:
  The polarity (positive or negative) of the ex-
pression tells whether the referred event has
happened or not;
  Modality (as marked by modal auxiliaries may,
can, might, could, should, etc., or adverbials
like probably, likely, etc.) qualifies the denoted
event with modal information (irrealis, neces-
sity, possibility), and therefore has implications
for the suitability of statements as answers to
questions, in a parallel way to other intensional
contexts exemplified in (1-2);
  Tense and aspect provide crucial information
for the temporal ordering of the events;
  Similarly, the non-finite morphology of certain
verbal expressions (infinitival, present partici-
ple, or past participle) has been shown as useful
in predicting temporal relations between events
(Lapata and Lascarides, 2004). We also con-
sider as possible values here the categories of
noun and adjective.
  Event class distinguishes among states (e.g., be
the director of), general occurrences (walk),
reporting (tell), intensional (attempt), and per-
ception (observe) events. This classification
is relevant for characterizing the nature of the
event as irrealis, factual, possible, reported,
etc. (recall examples (1-2) above).
Despite the fact that modality, tense, aspect, and
non-finite morphology are typically verbal features,
some nouns and adjectives can also have this sort
of information associated with them; in particular,
when they are part of the predicative complement of
a copular verb (e.g., may be ready, had been a col-
laborator). A TimeML mark-up of these cases will
tag only the complement as an event, disregarding
the copular verb. Therefore, the modality, tense, as-
pect, and non-finite morphology information associ-
ated with the verb is incorporated as part of the event
identified as the nominal or adjectival complement.
Except for event class, the characterization of all
the features above relies strictly on surface linguistic
cues. Notice that this surface-based approach does
not provide for the actual temporal interpretation of
the events in the given context. The tense of a ver-
bal phrase, for example, does not always map in a
straightforward way with the time being referred to
in the world; e.g., simple present is sometimes used
to express future time or habituality. We handle the
task of mapping event features onto their semantics
during a later processing stage, not addressed in this
paper, but see Mani and Schiffman (forthcoming).
TimeML does not identify event participants, but
the event tag and its attributes have been designed
to interface with Named Entity taggers in a straight-
forward manner. In fact, the issue of argument link-
ing to the events in TimeML is already being ad-
dressed in the effort to create a unified annotation
with PropBank and NomBank (Pustejovsky et al
2005). A complete overview of the linguistic foun-
dations of TimeML can be obtained in Pustejovsky
et al (forthcoming).
702
2.2 Preprocessing
For the task of event recognition, Evita needs ac-
cess to part of speech tags and to the result of some
form of syntactic parsing. Section 2.1 above de-
tailed some of the different syntactic structures that
are used to refer to events. However, using a shal-
low parser is enough to retrieve event referring ex-
pressions, since they are generally conveyed by three
possible part of speech categories: verbs (go, see,
say), nouns (departure, glimpse, war), and adjec-
tives (upset, pregnant, dead).
Part of speech tags and phrase chunks are also
valuable for the identification of certain grammatical
features such as tense, non-finite morphology, or po-
larity. Finally, lexical stems are necessary for those
tasks involving lexical look-up. We obtain all such
grammatical information by first preprocessing the
input file using the Alembic Workbench tagger, lem-
matizer, and chunker (Day et al, 1997). Evita?s in-
put must be XML-compliant, but need not conform
to the TimeML DTD.
2.3 Event Recognition
Event identification in Evita is based on the notion
of event as defined in the previous section. Only lex-
ical items tagged by the preprocessing stage as either
verbs, nouns, or adjectives are considered event can-
didates.
Different strategies are used for identifying events
in these three categories. Event identification in
verbal chunks is based on lexical look-up, accom-
panied by minimal contextual parsing in order to
exclude weak stative predicates, such as ?be?, and
some generics (e.g., verbs with bare plural subjects).
For every verbal chunk in the text, Evita first ap-
plies a pattern-based selection step that distinguishes
among different kinds of information: the chunk
head, which is generally the most-right element of
verbal nature in the chunk, thus disregarding par-
ticles of different sort and punctuation marks; the
modal auxiliary sequence, if any (e.g., may have to);
the sequence of do, have, or be auxiliaries, mark-
ing for aspect, tense and voice; and finally, any item
expressing the polarity of the event. The last three
pieces of information will be used later, when iden-
tifying the event grammatical features (section 2.4).
Based on basic lexical inventories, the chunk may
then be rejected if the head belongs to a certain class.
For instance, copular verbs are generally disregarded
for event tagging, although they enter into a a pro-
cess of chunk clustering, together with their predica-
tive complement (see section 2.5).
The identification of nominal and adjectival
events is also initiated by the step of information se-
lection. For each noun and adjective chunk, their
head and polarity markers, if any, are distinguished.
Identifying events expressed by nouns involves
two parts: a phase of lexical lookup, and a disam-
biguation process. The lexical lookup aims at an ini-
tial filtering of candidates to nominal events. First,
Evita checks whether the head of the noun chunk is
an event in WordNet. We identified about 25 sub-
trees from WordNet where all synsets denote nom-
inal events. One of these, the largest, is the tree
underneath the synset that contains the word event.
Other subtrees were selected by analyzing events in
SemCor and TimeBank1.22 and mapping them to
WordNet synsets. One example is the synset with
the noun phenomenon. In some cases, exceptions
are defined. For example, a noun in a subset sub-
sumed by the phenomenon synset is not an event
if it is also subsumed by the synset with the noun
cloud (in other words, many phenomena are events
but clouds are not).
If the result of lexical lookup is inconclusive (that
is, if a nominal occurs in WN as both and event and
a non-event), then a disambiguation step is applied.
This process is based on rules learned by a Bayesian
classifier trained on SemCor.
Finally, identifying events from adjectives takes
a conservative approach of tagging as events only
those adjectives that were annotated as such in Time-
Bank1.2, whenever they appear as the head of a
predicative complement. Thus, in addition to the
use of corpus-based data, the subtask relies again on
a minimal contextual parsing capable of identifying
the complements of copular predicates.
2TimeBank1.2 is our gold standard corpus of around
200 news report documents from various sources, anno-
tated with TimeML temporal and event information. A
previous version, TimeBank1.1, can be downloaded from
http://www.timeml.org/. For additional information
see Pustejovsky et al (2003b).
703
2.4 Identification of Grammatical Features
Identifying the grammatical features of events fol-
lows different procedures, depending on the part
of speech of the event-denoting expression, and
whether the feature is explicitely realized by the
morphology of such expressions.
In event-denoting expressions that contain a ver-
bal chunk, tense, aspect, and non-finite morphology
values are directly derivable from the morphology of
this constituent, which in English is quite straight-
forward. Thus, the identification of these features is
done by first extracting the verbal constituents from
the verbal chunk (disregarding adverbials, punctua-
tion marks, etc.), and then applying a set of over 140
simple linguistic rules, which define different possi-
ble verbal phrases and map them to their correspond-
ing tense, aspect, and non-finite morphology values.
Figure 1 illustrates the rule for verbal phrases of fu-
ture tense, progressive aspect, which bear the modal
form have to (as in, e.g., Participants will have to
be working on the same topics):
[form in futureForm],
[form==?have?],
[form==?to?, pos==?TO?],
[form==?be?], [pos==?VBG?],
==>
[tense=?FUTURE?,
aspect=?PROGRESSIVE?,
nf morph=?NONE?]
Figure 1: Grammatical Rule
For event-denoting expressions containing no
verbal chunk, tense and aspect is established as
null (?NONE? value), and non-finite morphology is
?noun? or ?adjective?, depending on the part-of-
speech of their head.
Modality and polarity are the two remaining
morphology-based features identified here. Evita
extracts the values of these two attributes using ba-
sic pattern-matching techniques over the approapri-
ate verbal, nominal, or adjectival chunk.
On the other hand, the identification of event class
cannot rely on linguistic cues such as the morphol-
ogy of the expression. Instead, it requires a combi-
nation of lexical resource-based look-up and word
sense disambiguation. At present, this task has been
attempted only in a very preliminary way, by tagging
events with the class that was most frequently as-
signed to them in TimeBank1.2. Despite the limita-
tions of such a treatment, the accuracy ratio is fairly
good (refer to section 3).
2.5 Clustering of Chunks
In some cases, the chunker applied at the prepro-
cessing stage identifies two independent constituents
that contribute information about the same event.
This may be due to a chunker error, but it is also sys-
tematically the case in verbal phrases containing the
have to modal form or the be going to future form
(Figure 2).
<VG>
<VX><lex pos="VBD">had</lex></VX>
</VG>
<VG-INF>
<INF><lex pos="TO">to</lex>
<lex pos="VB">say</lex>
</INF>
</VG-INF>
Figure 2: have to VP
It may be also necessary in verbal phrases with
other modal auxiliaries, or with auxiliary forms of
the have, do, or be forms, in which the auxiliary part
is split off the main verb because of the presence of
an adverbial phrase or similar (Figure 3).
<VG>
<VX><lex pos="VBZ">has</lex></VX>
</VG>
<lex pos=",">,</lex>
<lex pos="IN">of</lex>
<NG>
<HEAD><lex pos="NN">course</lex></HEAD>
</NG>
<lex pos=",">,</lex>
<VG>
<VX><lex pos="VBD">tried</lex></VX>
</VG>
Figure 3: have V en VP
Constructions with copular verbs are another kind
of context which requires clustering of chunks, in
order to group together the verbal chunk corre-
sponding to the copular predicate and the non-verbal
chunk that functions as its predicative complement.
In all these cases, additional syntactic parsing is
needed for the tasks of event recognition and gram-
matical feature identification, in order to cluster to-
gether the two independent chunks.
704
The task of clustering chunks into bigger ones is
activated by specific triggers (e.g., a chunk headed
by an auxiliary form, or a chunk headed by the cop-
ular verb be) and carried out locally in the context of
that trigger. For each trigger, there is a set of gram-
matical patterns describing the possible structures it
can be a constituent of. The form have, for instance,
may be followed by an infinitival phrase to V, con-
stituting part of the modal form have to in the big-
ger verbal group have to V, as in Figure 2 above, or
it may also be followed by a past participle-headed
chunk, with which it forms a bigger verbal phrase
have V-en expressing perfective aspect (Figure 3).
The grammatical patterns established for each
trigger are written using the standard syntax of reg-
ular expressions, allowing for a greater expressive-
ness in the description of sequences of chunks (op-
tionality of elements, inclusion of adverbial phrases
and punctuation marks, variability in length, etc.).
These patterns are then compiled into finite state au-
tomata that work with grammatical objects instead
of string characters. Such an approach is based on
well-established techniques using finite-state meth-
ods (see for instance Koskenniemi, 1992; Appelt et
al. 1993; Karttunen et al, 1996; Grefenstette, 1996,
among others).
Evita sequentially feeds each of the FSAs for the
current trigger with the right-side part of the trigger
context (up to the first sentence boundary), which is
represented as a sequence of grammatical objects. If
one of the FSAs accepts this sequence or a subpart
of it, then the clustering operation is applied on the
chunks within the accepted (sub)sequence.
3 Results
Evaluation of Evita has been carried out by com-
paring its performance against TimeBank1.2. The
current performance of Evita is at 74.03% precision,
87.31% recall, for a resulting F-measure of 80.12%
(with  =0.5). These results are comparable to the
interannotation agreement scores for the task of tag-
ging verbal and nominal events, by graduate lin-
guistics students with only basic training (Table 1).3
By basic training we understand that they had read
3These figures are also in terms of F-measure. See Hripcsak
and Rothschild (2005) for the use of such metric in order to
quantify interannotator reliability.
the guidelines, had been given some additional ad-
vice, and subsequently annotated over 10 documents
before annotating those used in the interannotation
evaluation. They did not, however, have any meet-
ings amongst themselves in order to discuss issues
or to agree on a common strategy.
Category F-measure
Nouns 64%
Verbs 80%
Table 1: Interannotation Agreement
On the other hand, the Accuracy ratio (i.e., the
percentage of values Evita marked according to the
gold standard) on the identification of event gram-
matical features is as shown:
Feature Accuracy
polarity 98.26%
aspect 97.87%
modality 97.02%
tense 92.05%
nf morph 89.95%
class 86.26%
Table 2: Accuracy of Grammatical Features
Accuracy for polarity, aspect, and modality is op-
timal: over 97% in all three cases. In fact, we were
expecting a lower accuracy for polarity, since Evita
relies only on the polarity elements present in the
chunk containg the event, but does not take into ac-
count non-local forms of expressing polarity in En-
glish, such as negative polarity on the subject of a
sentence (as in Nobody saw him or in No victims
were found).
The slightly lower ratio for tense and nf morph is
in most of the cases due to problems from the POS
tagger used in the preprocessing step, since tense
and non-finite morphology values are mainly based
on its result. Some common POS tagging mistakes
deriving on tense and nf morph errors are, for in-
stance, identifying a present form as the base form
of the verb, a simple past form as a past participle
form, or vice versa. Errors in the nf morph value are
also due to the difficulty in distinguishing sometimes
between present participle and noun (for ing-forms),
or between past participle and adjective.
705
The lowest score is for event class, which never-
theless is in the 80s%. This is the only feature that
cannot be obtained based on surface cues. Evita?s
treatment of this feature is still very basic, and we
envision that it can be easily enhanced by exploring
standard word sense disambiguation techniques.
4 Discussion and Conclusions
We have presented Evita, a tool for recognizing and
tagging events in natural language text. To our
knowledge, this is a unique tool within the commu-
nity, in that it is not based on any pre-established
list of event patterns, nor is it restricted to a specific
domain. In addition, Evita identifies the grammat-
ical information that is associated with the event-
referring expression, such as tense, aspect, polarity,
and modality. The characterization of these features
is based on explicit linguistic cues. Unlike other
work on event recognition, Evita does not attempt
to identify event participants, but relies on the use of
entity taggers for the linking of arguments to events.
Evita combines linguistic- and statistically-based
knowledge to better address each particular subtask
of the event recognition problem. Linguistic knowl-
edge has been used for the parsing of very local and
controlled contexts, such as verbal phrases, and the
extraction of morphologically explicit information.
On the other hand, statistical knowledge has con-
tributed to the process of disambiguation of nomi-
nal events, following the current trend in the Word
Sense Disambiguation field.
Our tool is grounded on simple and well-known
technologies; namely, a standard preprocessing
stage, finite state techniques, and Bayesian-based
techniques for word sense disambiguation. In ad-
dition, it is conceived from a highly modular per-
spective. Thus, an effort has been put on separating
linguistic knowledge from the processing thread. In
this way we guarantee a low-cost maintainance of
the system, and simplify the task of enriching the
grammatical knowledge (which can be carried out
even by naive programmers such as linguists) when
additional data is obtained from corpus exploitation.
Evita is a component within a larger suite of tools.
It is one of the steps within a processing sequence
which aims at providing basic semantic information
(such as temporal relations or intensional context
boundaries) to applications like Question Answer-
ing or Narrative Understanding, for which text un-
derstanding is shown to be fundamental, in addition
to shallow-based techniques. Nonetheless, Evita can
also be used independently for purposes other than
those above.
Additional tools within the TimeML research
framework are (a) GUTime, a recognizer of tempo-
ral expressions which extends Tempex for TimeML
(Mani, 2005), (b) a tool devoted to the temporal or-
dering and anchoring of events (Mani and Schiff-
man, forthcoming), and (c) Slinket, an application
in charge of identifying subordination contexts that
introduce intensional events like those exemplified
in (1-2) (Verhagen et al, 2005). Together with these,
Evita provides capabilities for a more adequate treat-
ment of temporal and intensional information in tex-
tual sources, thereby contributing towards incorpo-
rating greater inferential capabilities to applications
within QA and related fields, a requisite that has
been shown necessary in the Introduction section.
Further work on Evita will be focused on two
main areas: (1) improving the sense disambiguation
of candidates to event nominals by experimenting
with additional learning techniques, and (2) improv-
ing event classification. The accuracy ratio for this
latter task is already fairly acceptable (86.26%), but
it still needs to be enhanced in order to guarantee an
optimal detection of subordinating intensional con-
texts (recall examples 1-2). Both lines of work will
involve the exploration and use of word sense dis-
ambiguation techniques.
References
Appelt, Douglas E., Jerry R. Hobbs, John Bear, David
Israel and Mabry Tyson 1993. ?FASTUS: A Finite-
state Processor for Information Extraction from Real-
world Text?. Proceedings IJCAI-93.
Brill, Eric, Susan Dumais and Michele Banko. 2002.
?An Analysis of the AskMSR Question Answering
System?. Proceedings of EMNLP 2002.
Day, David,, John Aberdeen, Lynette Hirschman, Robyn
Kozierok, Patricia Robinson and Marc Vilain. 1997.
?Mixed-Initiative Development of Language Process-
ing Systems?. Fifth Conference on Applied Natural
Language Processing Systems: 88?95.
Grefenstette, Gregory. 1996. ?Light Parsing as Finite-
State Filtering?. Workshop on Extended Finite State
Models of Language, ECAI?96.
706
Harabagiu, S., D. Moldovan, C. Clark, M. Bowden, J.
Williams and J. Bensley. 2003. ?Answer Mining
by Combining Extraction Techniques with Abductive
Reasoning?. Proceedings of the Text Retrieval Confer-
ence, TREC 2003: 375-382.
Hovy, Eduard, Ulf Hermjakob and Deepak Ravichan-
dran. 2002. A Question/Answer Typology with Sur-
face Text Patterns. Proceedings of the Second Inter-
national Conference on Human Language Technology
Research, HLT 2002: 247-251.
Hripcsak, George and Adam S. Rothschild. 2005.
?Agreement, the F-measure, and reliability in informa-
tion retrieval?. Journal of the American Medical Infor-
matics Association, 12: 296-298.
Karttunen, L., J-P. Chanod, G. Grefenstette and A.
Schiller. 1996. ?Regular Expressions for Language
Engineering?. Natural Language Engineering, 2(4).
Koskenniemi, Kimmo, Pasi Tapanainen and Atro Vouti-
lainen. ?Compiling and Using Finite-State Syntactic
Rules?. Proceedings of COLING-92: 156-162.
Lapata, Maria and Alex Lascarides 2004. Inferring
Sentence-Internal Temporal Relations. Proceedings of
HLT-NAACL 2004.
Mani, Inderjeet. 2005. Time Expression Tagger and
Normalizer. http://complingone.georgetown.edu/ lin-
guist/GU TIME DOWNLOAD.HTML
Mani, Inderjeet and Barry Schiffman. Forthcom-
ing. ?Temporally Anchoring and Ordering Events in
News?. James Pustejovsky and Robert Gaizauskas
(eds.) Event Recognition in Natural Language. John
Benjamins.
Moldovan, D., S. Harabagiu, R. Girju, P. Morarescu, F.
Lacatusu, A. Novischi, A. Badulescu and O. Bolohan.
2002. ?LCC Tools for Question Answering?. Proceed-
ings of the Text REtrieval Conference, TREC 2002.
Pustejovsky, J., J. Castan?o, R. Ingria, R. Saur??, R.
Gaizauskas, A. Setzer, and G. Katz. 2003a. TimeML:
Robust Specification of Event and Temporal Expres-
sions in Text. IWCS-5 Fifth International Workshop
on Computational Semantics.
Pustejovsky, James and Rob Gaizauskas (editors) (forth-
coming) Reasoning about Time and Events. John
Benjamins Publishers.
Pustejovsky, J., P. Hanks, R. Saur??, A. See, R.
Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D.
Day, L. Ferro and M. Lazo. 2003b. The TIME-
BANK Corpus. Proceedings of Corpus Linguistics
2003: 647-656.
Pustejovsky, J., B. Knippen, J. Littman, R. Saur?? (forth-
coming) Temporal and Event Information in Natural
language Text. Language Resources and Evaluation.
Pustejovsky, James, Martha Palmer and Adam Meyers.
2005. Workshop on Frontiers in Corpus Annotation
II. Pie in the Sky. ACL 2005.
Pustejovsky, J., R. Saur??, J. Castan?o, D. R. Radev, R.
Gaizauskas, A. Setzer, B. Sundheim and G. Katz.
2004. Representing Temporal and Event Knowledge
for QA Systems. Mark T. Maybury (ed.) New Direc-
tions in Question Answering. MIT Press, Cambridge.
Ravichandran, Deepak and Eduard Hovy. 2002. ?Learn-
ing Surface Text Patterns for a Question Answering
System?. Proceedings of the ACL 2002.
Saur??, Roser, Jessica Littman, Robert Knippen, Rob
Gaizauskas, Andrea Setzer and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Saur??, Roser and Marc Verhagen. 2005. Temporal Infor-
mation in Intensional Contexts. Bunt, H., J. Geertzen
and E. Thijse (eds.) Proceedings of the Sixth In-
ternational Workshop on Computational Semantics.
Tilburg, Tilburg University: 404-406.
Small, Sharon, Liu Ting, Nobuyuki Shimuzu and Tomek
Strzalkowski. 2003. HITIQA, An interactive question
answering system: A preliminary report. Proceedings
of the ACL 2003 Workshop on Multilingual Summa-
rization and Question Answering.
Soricut, Radu and Eric Brill. 2004. Automatic Ques-
tion Answering: Beyond the Factoid. HLT-NAACL
2004, Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: 57-64.
Soubbotin, Martin M. and Sergei M. Soubbotin. 2002.
?Use of Patterns for Detection of Answer Strings: A
Systematic Approach?. Proceedings of TREC-11.
Verhagen, Marc. 2004. Times Between the Lines. Ph.D.
thesis. Brandeis University. Waltham, MA, USA.
Verhagen, Marc and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. James Pustejovsky and Robert
Gaizauskas (eds.) Time and Event Recognition in Nat-
ural Language. John Benjamin Publications.
Verhagen, Marc, Inderjeet Mani, Roser Saur??, Robert
Knippen, Jess Littman and James Pustejovsky. 2005.
?Automating Temporal Annotation with TARSQI?.
Demo Session. Proceedings of the ACL 2005.
Voorhees, Ellen M. 2002. ?Overview of the TREC
2002 Question Answering Track?. Proceedings of the
Eleventh Text REtrieval Conference, TREC 2002.
Voorhees, Ellen M. 2003. ?Overview of the TREC 2003
Question Answering Track?. Proceedings of 2003
Text REtrieval Conference, TREC 2003.
707
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 81?84, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automating Temporal Annotation with TARSQI
Marc Verhagen?, Inderjeet Mani?, Roser Sauri?,
Robert Knippen?, Seok Bae Jang?, Jessica Littman?,
Anna Rumshisky?, John Phillips?, James Pustejovsky?
? Department of Computer Science, Brandeis University, Waltham, MA 02254, USA
{marc,roser,knippen,jlittman,arum,jamesp}@cs.brandeis.edu
? Computational Linguistics, Georgetown University, Washington DC, USA
{im5,sbj3,jbp24}@georgetown.edu
Abstract
We present an overview of TARSQI, a
modular system for automatic temporal
annotation that adds time expressions,
events and temporal relations to news
texts.
1 Introduction
The TARSQI Project (Temporal Awareness and
Reasoning Systems for Question Interpretation)
aims to enhance natural language question an-
swering systems so that temporally-based questions
about the events and entities in news articles can be
addressed appropriately. In order to answer those
questions we need to know the temporal ordering of
events in a text. Ideally, we would have a total order-
ing of all events in a text. That is, we want an event
like marched in ethnic Albanians marched Sunday
in downtown Istanbul to be not only temporally re-
lated to the nearby time expression Sunday but also
ordered with respect to all other events in the text.
We use TimeML (Pustejovsky et al, 2003; Saur?? et
al., 2004) as an annotation language for temporal
markup. TimeML marks time expressions with the
TIMEX3 tag, events with the EVENT tag, and tempo-
ral links with the TLINK tag. In addition, syntactic
subordination of events, which often has temporal
implications, can be annotated with the SLINK tag.
A complete manual TimeML annotation is not
feasible due to the complexity of the task and the
sheer amount of news text that awaits processing.
The TARSQI system can be used stand-alone
or as a means to alleviate the tasks of human
annotators. Parts of it have been intergrated in
Tango, a graphical annotation environment for event
ordering (Verhagen and Knippen, Forthcoming).
The system is set up as a cascade of modules
that successively add more and more TimeML
annotation to a document. The input is assumed to
be part-of-speech tagged and chunked. The overall
system architecture is laid out in the diagram below.
Input Documents
GUTime
Evita
SlinketGUTenLINK
SputLink
TimeML Documents
In the following sections we describe the five
TARSQI modules that add TimeML markup to news
texts.
2 GUTime
The GUTime tagger, developed at Georgetown Uni-
versity, extends the capabilities of the TempEx tag-
ger (Mani and Wilson, 2000). TempEx, developed
81
at MITRE, is aimed at the ACE TIMEX2 standard
(timex2.mitre.org) for recognizing the extents and
normalized values of time expressions. TempEx
handles both absolute times (e.g., June 2, 2003) and
relative times (e.g., Thursday) by means of a num-
ber of tests on the local context. Lexical triggers like
today, yesterday, and tomorrow, when used in a spe-
cific sense, as well as words which indicate a posi-
tional offset, like next month, last year, this coming
Thursday are resolved based on computing direc-
tion and magnitude with respect to a reference time,
which is usually the document publication time.
GUTime extends TempEx to handle time ex-
pressions based on the TimeML TIMEX3 standard
(timeml.org), which allows a functional style of en-
coding offsets in time expressions. For example, last
week could be represented not only by the time value
but also by an expression that could be evaluated to
compute the value, namely, that it is the week pre-
ceding the week of the document date. GUTime also
handles a variety of ACE TIMEX2 expressions not
covered by TempEx, including durations, a variety
of temporal modifiers, and European date formats.
GUTime has been benchmarked on training data
from the Time Expression Recognition and Normal-
ization task (timex2.mitre.org/tern.html) at .85, .78,
and .82 F-measure for timex2, text, and val fields
respectively.
3 EVITA
Evita (Events in Text Analyzer) is an event recogni-
tion tool that performs two main tasks: robust event
identification and analysis of grammatical features,
such as tense and aspect. Event identification is
based on the notion of event as defined in TimeML.
Different strategies are used for identifying events
within the categories of verb, noun, and adjective.
Event identification of verbs is based on a lexi-
cal look-up, accompanied by a minimal contextual
parsing, in order to exclude weak stative predicates
such as be or have. Identifying events expressed by
nouns, on the other hand, involves a disambigua-
tion phase in addition to lexical lookup. Machine
learning techniques are used to determine when an
ambiguous noun is used with an event sense. Fi-
nally, identifying adjectival events takes the conser-
vative approach of tagging as events only those ad-
jectives that have been lexically pre-selected from
TimeBank1, whenever they appear as the head of a
predicative complement. For each element identi-
fied as denoting an event, a set of linguistic rules
is applied in order to obtain its temporally relevant
grammatical features, like tense and aspect. Evita
relies on preprocessed input with part-of-speech tags
and chunks. Current performance of Evita against
TimeBank is .75 precision, .87 recall, and .80 F-
measure. The low precision is mostly due to Evita?s
over-generation of generic events, which were not
annotated in TimeBank.
4 GUTenLINK
Georgetown?s GUTenLINK TLINK tagger uses
hand-developed syntactic and lexical rules. It han-
dles three different cases at present: (i) the event
is anchored without a signal to a time expression
within the same clause, (ii) the event is anchored
without a signal to the document date speech time
frame (as in the case of reporting verbs in news,
which are often at or offset slightly from the speech
time), and (iii) the event in a main clause is anchored
with a signal or tense/aspect cue to the event in the
main clause of the previous sentence. In case (iii), a
finite state transducer is used to infer the likely tem-
poral relation between the events based on TimeML
tense and aspect features of each event. For ex-
ample, a past tense non-stative verb followed by a
past perfect non-stative verb, with grammatical as-
pect maintained, suggests that the second event pre-
cedes the first.
GUTenLINK uses default rules for ordering
events; its handling of successive past tense non-
stative verbs in case (iii) will not correctly or-
der sequences like Max fell. John pushed him.
GUTenLINK is intended as one component in a
larger machine-learning based framework for order-
ing events. Another component which will be de-
veloped will leverage document-level inference, as
in the machine learning approach of (Mani et al,
2003), which required annotation of a reference time
(Reichenbach, 1947; Kamp and Reyle, 1993) for the
event in each finite clause.
1TimeBank is a 200-document news corpus manually anno-
tated with TimeML tags. It contains about 8000 events, 2100
time expressions, 5700 TLINKs and 2600 SLINKs. See (Day
et al, 2003) and www.timeml.org for more details.
82
An early version of GUTenLINK was scored at
.75 precision on 10 documents. More formal Pre-
cision and Recall scoring is underway, but it com-
pares favorably with an earlier approach developed
at Georgetown. That approach converted event-
event TLINKs from TimeBank 1.0 into feature vec-
tors where the TLINK relation type was used as the
class label (some classes were collapsed). A C5.0
decision rule learner trained on that data obtained an
accuracy of .54 F-measure, with the low score being
due mainly to data sparseness.
5 Slinket
Slinket (SLINK Events in Text) is an application
currently being developed. Its purpose is to automat-
ically introduce SLINKs, which in TimeML specify
subordinating relations between pairs of events, and
classify them into factive, counterfactive, evidential,
negative evidential, and modal, based on the modal
force of the subordinating event. Slinket requires
chunked input with events.
SLINKs are introduced by a well-delimited sub-
group of verbal and nominal predicates (such as re-
gret, say, promise and attempt), and in most cases
clearly signaled by the context of subordination.
Slinket thus relies on a combination of lexical and
syntactic knowledge. Lexical information is used to
pre-select events that may introduce SLINKs. Pred-
icate classes are taken from (Kiparsky and Kiparsky,
1970; Karttunen, 1971; Hooper, 1975) and subse-
quent elaborations of that work, as well as induced
from the TimeBank corpus. A syntactic module
is applied in order to properly identify the subor-
dinated event, if any. This module is built as a
cascade of shallow syntactic tasks such as clause
boundary recognition and subject and object tag-
ging. Such tasks are informed from both linguistic-
based knowledge (Papageorgiou, 1997; Leffa, 1998)
and corpora-induced rules (Sang and De?je?an, 2001);
they are currently being implemented as sequences
of finite-state transducers along the lines of (A??t-
Mokhtar and Chanod, 1997). Evaluation results are
not yet available.
6 SputLink
SputLink is a temporal closure component that takes
known temporal relations in a text and derives new
implied relations from them, in effect making ex-
plicit what was implicit. A temporal closure compo-
nent helps to find those global links that are not nec-
essarily derived by other means. SputLink is based
on James Allen?s interval algebra (1983) and was in-
spired by (Setzer, 2001) and (Katz and Arosio, 2001)
who both added a closure component to an annota-
tion environment.
Allen reduces all events and time expressions to
intervals and identifies 13 basic relations between
the intervals. The temporal information in a doc-
ument is represented as a graph where events and
time expressions form the nodes and temporal re-
lations label the edges. The SputLink algorithm,
like Allen?s, is basically a constraint propagation al-
gorithm that uses a transitivity table to model the
compositional behavior of all pairs of relations. For
example, if A precedes B and B precedes C, then
we can compose the two relations and infer that A
precedes C. Allen allowed unlimited disjunctions of
temporal relations on the edges and he acknowl-
edged that inconsistency detection is not tractable
in his algebra. One of SputLink?s aims is to ensure
consistency, therefore it uses a restricted version of
Allen?s algebra proposed by (Vilain et al, 1990). In-
consistency detection is tractable in this restricted al-
gebra.
A SputLink evaluation on TimeBank showed that
SputLink more than quadrupled the amount of tem-
poral links in TimeBank, from 4200 to 17500.
Moreover, closure adds non-local links that were
systematically missed by the human annotators. Ex-
perimentation also showed that temporal closure al-
lows one to structure the annotation task in such
a way that it becomes possible to create a com-
plete annotation from local temporal links only. See
(Verhagen, 2004) for more details.
7 Conclusion and Future Work
The TARSQI system generates temporal informa-
tion in news texts. The five modules presented here
are held together by the TimeML annotation lan-
guage and add time expressions (GUTime), events
(Evita), subordination relations between events
(Slinket), local temporal relations between times and
events (GUTenLINK), and global temporal relations
between times and events (SputLink).
83
In the nearby future, we will experiment with
more strategies to extract temporal relations from
texts. One avenue is to exploit temporal regularities
in SLINKs, in effect using the output of Slinket as
a means to derive even more TLINKs. We are also
compiling more annotated data in order to provide
more training data for machine learning approaches
to TLINK extraction. SputLink currently uses only
qualitative temporal infomation, it will be extended
to use quantitative information, allowing it to reason
over durations.
References
Salah A??t-Mokhtar and Jean-Pierre Chanod. 1997. Sub-
ject and Object Dependency Extraction Using Finite-
State Transducers. In Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications. ACL/EACL-97 Workshop Proceed-
ings, pages 71?77, Madrid, Spain. Association for
Computational Linguistics.
James Allen. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
26(11):832?843.
David Day, Lisa Ferro, Robert Gaizauskas, Patrick
Hanks, Marcia Lazo, James Pustejovsky, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics.
Joan Hooper. 1975. On Assertive Predicates. In John
Kimball, editor, Syntax and Semantics, volume IV,
pages 91?124. Academic Press, New York.
Hans Kamp and Uwe Reyle, 1993. From Discourse to
Logic, chapter 5, Tense and Aspect, pages 483?546.
Kluwer Academic Publishers, Dordrecht, Netherlands.
Lauri Karttunen. 1971. Some Observations on Factivity.
In Papers in Linguistics, volume 4, pages 55?69.
Graham Katz and Fabrizio Arosio. 2001. The Anno-
tation of Temporal Information in Natural Language
Sentences. In Proceedings of ACL-EACL 2001, Work-
shop for Temporal and Spatial Information Process-
ing, pages 104?111, Toulouse, France. Association for
Computational Linguistics.
Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
Manfred Bierwisch and Karl Erich Heidolph, editors,
Progress in Linguistics. A collection of Papers, pages
143?173. Mouton, Paris.
Vilson Leffa. 1998. Clause Processing in Complex Sen-
tences. In Proceedings of the First International Con-
ference on Language Resources and Evaluation, vol-
ume 1, pages 937?943, Granada, Spain. ELRA.
Inderjeet Mani and George Wilson. 2000. Processing
of News. In Proceedings of the 38th Annual Meet-
ing of the Association for Computational Linguistics
(ACL2000), pages 69?76.
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring Temporal Ordering of Events in News.
Short Paper. In Proceedings of the Human Language
Technology Conference (HLT-NAACL?03).
Harris Papageorgiou. 1997. Clause Recognition in the
Framework of Allignment. In Ruslan Mitkov and
Nicolas Nicolov, editors, Recent Advances in Natural
Language Recognition. John Benjamins, Amsterdam,
The Netherlands.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In IWCS-5 Fifth
International Workshop on Computational Semantics.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Tjong Kim Sang and Erik Herve De?je?an. 2001. Introduc-
tion to the CoNLL-2001 Shared Task: Clause Identifi-
cation. In Proceedings of the Fifth Workshop on Com-
putational Language Learning (CoNLL-2001), pages
53?57, Toulouse, France. ACL.
Roser Saur??, Jessica Littman, Robert Knippen, Robert
Gaizauskas, Andrea Setzer, and James Puste-
jovsky. 2004. TimeML Annotation Guidelines.
http://www.timeml.org.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield, Sheffield, UK.
Marc Verhagen and Robert Knippen. Forthcoming.
TANGO: A Graphical Annotation Environment for
Ordering Relations. In James Pustejovsky and Robert
Gaizauskas, editors, Time and Event Recognition in
Natural Language. John Benjamin Publications.
Marc Verhagen. 2004. Times Between The Lines. Ph.D.
thesis, Brandeis University, Waltham, Massachusetts,
USA.
Marc Vilain, Henry Kautz, and Peter van Beek. 1990.
Constraint propagation algorithms: A revised report.
In D. S. Weld and J. de Kleer, editors, Qualitative Rea-
soning about Physical Systems, pages 373?381. Mor-
gan Kaufman, San Mateo, California.
84
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine Learning of Temporal Relations 
Inderjeet Mani??, Marc Verhagen?, Ben Wellner?? 
Chong Min Lee? and James Pustejovsky? 
?The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
?Department of Linguistics, Georgetown University 
37th and O Streets, Washington, DC 20036, USA 
?Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.edu 
Abstract 
This paper investigates a machine learn-
ing approach for temporally ordering and 
anchoring events in natural language 
texts. To address data sparseness, we 
used temporal reasoning as an over-
sampling method to dramatically expand 
the amount of training data, resulting in 
predictive accuracy on link labeling as 
high as 93% using a Maximum Entropy 
classifier on human annotated data. This 
method compared favorably against a se-
ries of increasingly sophisticated base-
lines involving expansion of rules de-
rived from human intuitions. 
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event.  
A wealth of prior research by (Passoneau 
1988), (Webber 1988), (Hwang and Schubert 
1992), (Kamp and Reyle 1993), (Lascarides and 
Asher 1993), (Hitzeman et al 1995), (Kehler 
2000) and others, has explored the different 
knowledge sources used in inferring the temporal 
ordering of events, including temporal adver-
bials, tense, aspect, rhetorical relations, prag-
matic conventions, and background knowledge. 
For example, the narrative convention of events 
being described in the order in which they occur 
is followed in (1), but overridden by means of a 
discourse relation, Explanation in (2).  
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him.  
In addition to discourse relations, which often 
require inferences based on world knowledge, 
the ordering decisions humans carry out appear 
to involve a variety of knowledge sources, in-
cluding tense and grammatical aspect (3a), lexi-
cal aspect (3b), and temporal adverbials (3c): 
(3a) Max entered the room. He had drunk a lot 
of wine.  
(3b) Max entered the room. Mary was seated 
behind the desk.  
(3c) The company announced Tuesday that 
third-quarter sales had fallen.  
Clearly, substantial linguistic processing may 
be required for a system to make these infer-
ences, and world knowledge is hard to make 
available to a domain-independent program. An 
important strategy in this area is of course the 
development of annotated corpora than can fa-
cilitate the machine learning of such ordering 
inferences. 
This paper 1  investigates a machine learning 
approach for temporally ordering events in natu-
ral language texts. In Section 2, we describe the 
annotation scheme and annotated corpora, and 
the challenges posed by them. A basic learning 
approach is described in Section 3. To address 
data sparseness, we used temporal reasoning as 
an over-sampling method to dramatically expand 
the amount of training data.  
As we will discuss in Section 5, there are no 
standard algorithms for making these inferences 
that we can compare against. We believe 
strongly that in such situations, it?s worthwhile 
for computational linguists to devote consider-
                                                 
1Research at Georgetown and Brandeis on this prob-
lem was funded in part by a grant from the ARDA 
AQUAINT Program, Phase II.  
753
able effort to developing insightful baselines. 
Our work is, accordingly, evaluated in compari-
son against four baselines: (i) the usual majority 
class statistical baseline, shown along with each 
result, (ii) a more sophisticated baseline that uses 
hand-coded rules (Section 4.1), (iii) a hybrid 
baseline based on hand-coded rules expanded 
with Google-induced rules (Section 4.2), and (iv) 
a machine learning version that learns from im-
perfect annotation produced by (ii) (Section 4.3).  
2 Annotation Scheme and Corpora 
2.1 TimeML 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their temporal rela-
tions in news articles. The TimeML scheme flags 
tensed verbs, adjectives, and nominals with 
EVENT tags with various attributes, including 
the class of event, tense, grammatical aspect, po-
larity (negative or positive), any modal operators 
which govern the event being tagged, and cardi-
nality of the event if it?s mentioned more than 
once. Likewise, time expressions are flagged and 
their values normalized, based on TIMEX3, an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme.  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given (3a), a 
TLINK tag orders an instance of the event of 
entering to an instance of the drinking with the 
relation type AFTER. Likewise, given the sen-
tence (3c), a TLINK tag will anchor the event 
instance of announcing to the time expression 
Tuesday (whose normalized value will be in-
ferred from context), with the relation 
IS_INCLUDED. These inferences are shown (in 
slightly abbreviated form) in the annotations in 
(4) and (5). 
(4) Max <EVENT eventID=?e1? 
class=?occurrence? tense=?past? as-
pect=?none?>entered</EVENT> the room. 
He <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?>had drunk</EVENT>a 
lot of wine.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
 (5) The company <EVENT even-
tID=?e1? class=?reporting? 
tense=?past? as-
pect=?none?>announced</EVENT> 
<TIMEX3 tid=?t2? type=?DATE? tempo-
ralFunction=?false? value=?1998-01-
08?>Tuesday </TIMEX3> that third-
quarter sales <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?> had fallen</EVENT>.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
<TLINK eventID=?e1? relatedTo-
TimeID=?t2? relType=?IS_INCLUDED?/> 
 
The anchor relation is an Event-Time TLINK, 
and the order relation is an Event-Event TLINK. 
TimeML uses 14 temporal relations in the 
TLINK RelTypes, which reduce to a disjunctive 
classification of 6 temporal relations RelTypes = 
{SIMULTANEOUS, IBEFORE, BEFORE, BE-
GINS, ENDS, INCLUDES}. An event or time is 
SIMULTANEOUS with another event or time if 
they occupy the same time interval. An event or 
time INCLUDES another event or time if the 
latter occupies a proper subinterval of the former. 
These 6 relations and their inverses map one-to-
one to 12 of Allen?s 13 basic relations (Allen 
1984)2. There has been a considerable amount of 
activity related to this scheme; we focus here on 
some of the challenges posed by the TLINK an-
notation, the part that is directly relevant to the 
temporal ordering and anchoring problems. 
2.2 Challenges 
The annotation of TimeML information is on a 
par with other challenging semantic annotation 
schemes, like PropBank, RST annotation, etc., 
where high inter-annotator reliability is crucial 
but not always achievable without massive pre-
processing to reduce the user?s workload. In Ti-
meML, inter-annotator agreement for time ex-
pressions and events is 0.83 and 0.78 (average of 
Precision and Recall) respectively, but on 
TLINKs it is 0.55 (P&R average), due to the 
large number of event pairs that can be selected 
for comparison. The time complexity of the hu-
man TLINK annotation task is quadratic in the 
number of events and times in the document. 
Two corpora have been released based on Ti-
meML: the TimeBank (Pustejovsky et al 2003) 
(we use version 1.2.a) with 186 documents and 
                                                 
2Of the 14 TLINK relations, the 6 inverse relations are re-
dundant. In order to have a disjunctive classification, SI-
MULTANEOUS and IDENTITY are collapsed, since 
IDENTITY is a subtype of SIMULTANEOUS. (Specifi-
cally, X and Y are identical if they are simultaneous and 
coreferential.) DURING and IS_INCLUDED are collapsed 
since DURING is a subtype of IS_INCLUDED that anchors 
events to times that are durations. IBEFORE (immediately 
before) corresponds to Allen?s MEETS. Allen?s OVER-
LAPS relation is not represented in TimeML. More details 
can be found at timeml.org. 
754
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. The TimeBank was developed in 
the early stages of TimeML development, and 
was partitioned across five annotators with dif-
ferent levels of expertise. The Opinion Corpus 
was developed very recently, and was partitioned 
across just two highly trained annotators, and 
could therefore be expected to be less noisy. In 
our experiments, we merged the two datasets to 
produce a single corpus, called OTC. 
Table 1 shows the distribution of EVENTs and 
TIMES, and TLINK RelTypes3 in the OTC. The 
majority class percentages are shown in paren-
theses. It can be seen that BEFORE and SI-
MULTANEOUS together form a majority of 
event-ordering (Event-Event) links, whereas 
most of the event anchoring (Event-Time) links 
are INCLUDES.  
 
12750 Events, 2114 Times 
Relation Event-Event Event-Time 
IBEFORE 131 15 
BEGINS 160 112 
ENDS 208 159 
SIMULTANEOUS 1528 77 
INCLUDES 950 3001 (65.3%) 
BEFORE 3170 (51.6%) 1229 
TOTAL 6147 4593 
Table 1. TLINK Class Distributions in OTC 
Corpus 
 
The lack of TLINK coverage in human anno-
tation could be helped by preprocessing, pro-
vided it meets some threshold of accuracy. Given 
the availability of a corpus like OTC, it is natural 
to try a machine learning approach to see if it can 
be used to provide that preprocessing. However, 
the noise in the corpus and the sparseness of 
links present challenges to a learning approach. 
3 Machine Learning Approach 
3.1 Initial Learner 
There are several sub-problems related to in-
ferring event anchoring and event ordering. Once 
a tagger has tagged the events and times, the first 
task (A) is to link events and/or times, and the 
second task (B) is to label the links. Task A is 
hard to evaluate since, in the absence of massive 
preprocessing, many links are ignored by the 
human in creating the annotated corpora. In addi-
                                                 
3The number of TLINKs shown is based on the number of 
TLINK vectors extracted from the OTC. 
tion, a program, as a baseline, can trivially link 
all tagged events and times, getting 100% recall 
on Task A. We focus here on Task B, the label-
ing task. In the case of humans, in fact, when a 
TLINK is posited by both annotators between the 
same pairs of events or times, the inter-annotator 
agreement on the labels is a .77 average of P&R. 
To ensure replicability of results, we assume per-
fect (i.e., OTC-supplied) events, times, and links.  
Thus, we can consider TLINK inference as the 
following classification problem: given an or-
dered pair of elements X and Y, where X and Y 
are events or times which the human has related 
temporally via a TLINK, the classifier has to as-
sign a label in RelTypes. Using RelTypes instead 
of RelTypes ?  {NONE} also avoids the prob-
lem of heavily skewing the data towards the 
NONE class.  
To construct feature vectors for machine 
learning, we took each TLINK in the corpus and 
used the given TimeML features, with the 
TLINK class being the vector?s class feature.  
For replicability by other users of these corpora, 
and to be able to isolate the effect of components, 
we used ?perfect? features; no feature engineer-
ing was attempted. The features were, for each 
event in an event-ordering pair, the event-class, 
aspect, modality, tense and negation (all nominal 
features); event string, and signal (a preposi-
tion/adverb, e.g., reported on Tuesday), which 
are string features, and contextual features indi-
cating whether the same tense and same aspect 
are true of both elements in the event pair. For 
event-time links, we used the above event and 
signal features along with TIMEX3 time features. 
For learning, we used an off-the-shelf Maxi-
mum Entropy (ME) classifier (from Carafe, 
available at sourceforge.net/projects/carafe). As 
shown in the UNCLOSED (ME) column in Ta-
ble 24, accuracy of the unclosed ME classifier 
does not go above 77%, though it?s always better 
than the majority class (in parentheses). We also 
tried a variety of other classifiers, including the 
SMO support-vector machine and the na?ve 
Bayes tools in WEKA (www.weka.net.nz). SMO 
performance (but not na?ve Bayes) was compa-
rable with ME, with SMO trailing it in a few 
cases (to save space, we report just ME perform-
ance). It?s possible that feature engineering could 
improve performance, but since this is ?perfect? 
data, the result is not encouraging.  
                                                 
4All machine learning results, except for ME-C in Table 4, 
use 10-fold cross-validation. ?Accuracy? in tables is Predic-
tive Accuracy. 
755
 
 
 UNCLOSED (ME) CLOSED (ME-C) 
 Event-Event Event-Time Event-Event Event-Time 
Accuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3) 
Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
IBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0 
BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36 
ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62 
SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63 
INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34 
BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83 
 
Table 2. Machine learning results using unclosed and closed data
 
3.2 Expanding Training Data using Tem-
poral Reasoning 
To expand our training set, we use a temporal  
closure component SputLink (Verhagen 2004), 
that takes known temporal relations in a text and  
derives new implied relations from them, in ef-
fect making explicit what was implicit. SputLink 
was inspired by (Setzer and Gaizauskas 2000) 
and is based on Allen?s interval algebra, taking 
into account the limitations on that algebra that 
were pointed out by (Vilain et al 1990). It is ba-
sically a constraint propagation algorithm that 
uses a transitivity table to model the composi-
tional behavior of all pairs of relations in a 
document. SputLink?s transitivity table is repre-
sented by 745 axioms. An example axiom:  
 
If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE 
 
Once the TLINKs in each document in the 
corpus are closed using SputLink, the same vec-
tor generation procedure and feature representa-
tion described in Section 3.1 are used. The effect 
of closing the TLINKs on the corpus has a dra-
matic impact on learning. Table 2, in the 
CLOSED (ME-C) column shows that accura-
cies for this method (called ME-C, for Maximum 
Entropy learning with closure) are now in the 
high 80?s and low 90?s, and still outperform the 
closed majority class (shown in parentheses).  
What is the reason for the improvement?5 One 
reason is the dramatic increase in the amount of 
training data. The more connected the initial un-
                                                 
5Interestingly, performance does not improve for SIMUL-
TANEOUS.  The reason for this might be due to the rela-
tively modest increase in SIMULTANEOUS relations from 
applying closure (roughly factor of 2). 
closed graph for a document is in TLINKs, the 
greater the impact in terms of closure. When the 
OTC is closed, the number of TLINKs goes up 
by more than 11 times, from 6147 Event-Event 
and 4593 Event-Time TLINKs to 91,157 Event-
Event and 29,963 Event-Time TLINKs. The 
number of BEFORE links goes up from 3170 
(51.6%) Event-Event and 1229 Event-Time 
TLINKs (26.75%) to 68585 (75.2%) Event-
Event and 18665 (62.3%) Event-Time TLINKs, 
making BEFORE the majority class in the closed 
data for both Event-Event and Event-Time 
TLINKs. There are only an average of 0.84 
TLINKs per event before closure, but after clo-
sure it shoots up to 9.49 TLINKs per event. 
(Note that as a result, the majority class percent-
ages for the closed data have changed from the 
unclosed data.) 
Being able to bootstrap more training data is 
of course very useful. However, we need to dig 
deeper to investigate how the increase in data 
affected the machine learning. The improvement 
provided by temporal closure can be explained 
by three factors:  (1) closure effectively creates a 
new classification problem with many more in-
stances, providing more data to train on; (2) the 
class distribution is further skewed which results 
in a higher majority class baseline (3) closure 
produces additional data in such a way as to in-
crease the frequencies and statistical power of 
existing features in the unclosed data, as opposed 
to adding new features.  For example, with un-
closed data, given A BEFORE B and B BE-
FORE C, closure generates A BEFORE C which 
provides more significance for the features re-
lated to A and C appearing as first and second 
arguments, respectively, in a BEFORE relation.  
In order to help determine the effects of the 
above factors, we carried out two experiments in 
which we sampled 6145 vectors from the closed 
756
data ? i.e. approximately the number of Event-
Event vectors in the unclosed data.  This effec-
tively removed the contribution of factor (1) 
above. The first experiment (Closed Class Dis-
tribution) simply sampled 6145 instances uni-
formly from the closed instances, while the sec-
ond experiment (Unclosed Class Distribution) 
sampled instances according to the same distri-
bution as the unclosed data. Table 3 shows these 
results.  The greater class distribution skew in the 
closed data clearly contributes to improved accu-
racy. However, when using the same class distri-
bution as the unclosed data (removing factor (2) 
from above), the accuracy, 76%, is higher than 
using the full unclosed data.  This indicates that 
closure does indeed help according to factor (3). 
4 Comparison against Baselines 
4.1 Hand-Coded Rules 
Humans have strong intuitions about rules for 
temporal ordering, as we indicated in discussing 
sentences (1) to (3). Such intuitions led to the 
development of pattern matching rules incorpo-
rated in a TLINK tagger called GTag. GTag 
takes a document with TimeML tags, along with 
syntactic information from part-of-speech tag-
ging and chunking from Carafe, and then uses 
187 syntactic and lexical rules to infer and label 
TLINKs between tagged events and other tagged 
events or times. The tagger takes pairs of 
TLINKable items (event and/or time) and 
searches for the single most-confident rule to 
apply to it, if any, to produce a labeled TLINK 
between those items. Each (if-then) rule has a 
left-hand side which consists of a conjunction of 
tests based on TimeML-related feature combina-
tions (TimeML features along with part-of-
speech and chunk-related features), and a right-
hand side which is an assignment to one of the 
TimeML TLINK classes.  
The rule patterns are grouped into several dif-
ferent classes: (i) the event is anchored with or 
without a signal to a time expression within the 
same clause, e.g., (3c), (ii) the event is anchored 
without a signal to the document date (as is often 
the case for reporting verbs in news), (iii) an 
event is linked to another event in the same sen-
tence, e.g., (3c), and (iv) the event in a main 
clause of one sentence is anchored with a signal 
or tense/aspect cue to an event in the main clause 
of the previous sentence, e.g., (1-2), (3a-b). 
The performance of this baseline is shown in 
Table 4 (line GTag). The top most accurate rule 
(87% accuracy) was GTag Rule 6.6, which links 
a past-tense event verb joined by a conjunction to 
another past-tense event verb as being BEFORE 
the latter (e.g., they traveled and slept the 
night ..): 
 
If sameSentence=YES && 
 sentenceType=ANY && 
 conjBetweenEvents=YES && 
 arg1.class=EVENT && 
 arg2.class=EVENT && 
 arg1.tense=PAST && 
 arg2.tense=PAST && 
 arg1.aspect=NONE && 
 arg2.aspect=NONE && 
 arg1.pos=VB && 
 arg2.pos=VB && 
 arg1.firstVbEvent=ANY && 
 arg2.firstVbEvent=ANY  
then infer relation=BEFORE 
 
The vast majority of the intuition-bred rules 
have very low accuracy compared to ME-C, with 
intuitions failing for various feature combina-
tions and relations (for relations, for example, 
GTag lacks rules for IBEFORE, STARTS, and 
ENDS). The bottom-line here is that even when 
heuristic preferences are intuited, those prefer-
ences need to be guided by empirical data, 
whereas hand-coded rules are relatively ignorant 
of the distributions that are found in data. 
4.2 Adding Google-Induced Lexical Rules 
One might argue that the above baseline is too 
weak, since it doesn?t allow for a rich set of lexi-
cal relations. For example, pushing can result in 
falling, killing always results in death, and so 
forth. These kinds of defeasible rules have been 
investigated in the semantics literature, including 
the work of Lascarides and Asher cited in Sec-
tion 1.  
However, rather than hand-creating lexical 
rules and running into the same limitations as 
with GTag?s rules, we used an empirically-
derived resource called VerbOcean (Chklovski 
and Pantel 2004), available at 
http://semantics.isi.edu/ocean. This resource con-
sists of lexical relations mined from Google 
searches. The mining uses a set of lexical and 
syntactic patterns to test for pairs of verb 
strongly associated on the Web in an asymmetric 
?happens-before? relation. For example, the sys-
tem discovers that marriage happens-before di-
vorce, and that tie happens-before untie.  
We automatically extracted all the ?happens-
before? relations from the VerbOcean resource at 
the above web site, and then automatically con-
verted those relations to GTag format, producing 
4,199 rules. Here is one such converted rule: 
757
 
If arg1.class=EVENT && 
   arg2.class=EVENT && 
   arg1.word=learn && 
   arg2.word=forget && 
then infer relation=BEFORE 
 
Adding these lexical rules to GTag (with mor-
phological normalization being added for rule 
matching on word features) amounts to a consid-
erable augmentation of the rule-set, by a factor of 
22. GTag with this augmented rule-set might be 
a useful baseline to consider, since one would 
expect the gigantic size of the Google ?corpus? to 
yield fairly robust, broad-coverage rules.  
What if both a core GTag rule and a VerbO-
cean-derived rule could both apply? We assume 
the one with the higher confidence is chosen. 
However, we don?t have enough data to reliably 
estimate rule confidences for the original GTag 
rules; so, for the purposes of VerbOcean rule 
integration, we assigned either the original Ver-
bOcean rules as having greater confidence than 
the original GTag rules in case of a conflict (i.e., 
a preference for the more specific rule), or vice-
versa.  
 The results are shown in Table 4 (lines 
GTag+VerbOcean). The combined rule set, un-
der both voting schemes, had no statistically sig-
nificant difference in accuracy from the original 
GTag rule set. So, ME-C beat this baseline as 
well.  
The reason VerbOcean didn?t help is again 
one of data sparseness, due to most verbs occur-
ring rarely in the OTC. There were only 19 occa-
sions when a happens-before pair from VerbO-
cean correctly matched a human BEFORE 
TLINK, of which 6 involved the same rule being 
right twice (including learn happens-before for-
get, a rule which students are especially familiar 
with!), with the rest being right just once. There 
were only 5 occasions when a VerbOcean rule 
incorrectly matched a human BEFORE TLINK, 
involving just three rules. 
 
 
 Closed Class Distribution UnClosed Class Distribution 
Relation Prec Rec F Accuracy Prec Rec F Accuracy 
IBEFORE 100.0 100.0 100.0 83.33 58.82 68.96 
BEGINS 0 0 0 72.72 50.0 59.25 
ENDS 66.66 57.14 61.53 62.50 50.0 55.55 
SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34 
INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53 
BEFORE 90.68 92.60 91.63 
87.20  
(72.03) 
84.09 84.61 84.35 
76.0 
(40.95)  
Table 3. Machine Learning from subsamples of the closed data 
 
Accuracy Baseline 
Event-Event Event-Time 
GTag 63.43 72.46 
GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02 
GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37 
GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59) 
Table 4. Accuracy of ?Intuition? Derived Baselines 
 
4.3 Learning from Hand-Coded Rules 
Baseline 
The previous baseline was a hybrid confi-
dence-based combination of corpus-induced 
lexical relations with hand-created rules for tem-
poral ordering. One could consider another obvi-
ous hybrid, namely learning from annotations 
created by GTag-annotated corpora. Since the 
intuitive baseline fares badly, this may not be 
that attractive. However, the dramatic impact of 
closure could help offset the limited coverage 
provided by human intuitions.   
Table 4 (line GTag+closure+ME-C) shows the 
results of closing the TLINKs produced by 
GTag?s annotation and then training ME from 
the resulting data. The results here are evaluated 
against a held-out test set. We can see that even 
after closure, the baseline of learning from un-
closed human annotations is much poorer than 
ME-C, and is in fact substantially worse than the  
majority class on event ordering.  
This means that for preprocessing new data 
sets to produce noisily annotated data for this 
classification task, it is far better to use machine-
learning from closed human annotations rather 
758
than machine-learning from closed annotations 
produced by an intuitive baseline. 
5 Related Work 
Our approach of classifying pairs independ-
ently during learning does not take into account 
dependencies between pairs.  For example, a 
classifier may label <X, Y> as BEFORE. Given 
the pair <X, Z>,  such a classifier has no idea if 
<Y, Z> has been classified as BEFORE, in 
which case, through closure, <X, Z> should be 
classified as BEFORE. This can result in the 
classifier producing an inconsistently annotated 
text. The machine learning approach of (Cohen 
et al 1999) addresses this, but their approach is 
limited to total orderings involving BEFORE, 
whereas TLINKs introduce partial orderings in-
volving BEFORE and five other relations. Future 
research will investigate methods for tighter in-
tegration of temporal reasoning and statistical 
classification. 
The only closely comparable machine-
learning approach to the problem of TLINK ex-
traction was that of (Boguraev and Ando 2005), 
who trained a classifier on Timebank 1.1 for 
event anchoring for events and times within the 
same sentence, obtaining an F-measure (for tasks 
A and B together) of 53.1. Other work in ma-
chine-learning and hand-coded approaches, 
while interesting, is harder to compare in terms 
of accuracy since they do not use common task 
definitions, annotation standards, and evaluation 
measures. (Li et al 2004) obtained 78-88% accu-
racy on ordering within-sentence temporal rela-
tions in Chinese texts. (Mani et al 2003) ob-
tained 80.2 F-measure training a decision tree on 
2069 clauses in anchoring events to reference 
times that were inferred for each clause. (Ber-
glund et al 2006) use a document-level evalua-
tion approach pioneered by (Setzer and Gai-
zauskas 2000), which uses a distinct evaluation 
metric. Finally, (Lapata and Lascarides 2004) use 
found data to successfully learn which (possibly 
ambiguous) temporal markers connect a main 
and subordinate clause, without inferring under-
lying temporal relations. 
In terms of hand-coded approaches, (Mani and 
Wilson 2000) used a baseline method of blindly 
propagating TempEx time values to events based 
on proximity, obtaining 59.4% on a small sample 
of 8,505 words of text. (Filatova and Hovy 2001) 
obtained 82% accuracy on ?timestamping? 
clauses for a single type of event/topic on a data 
set of 172 clauses. (Schilder and Habel 2001) 
report 84% accuracy inferring temporal relations 
in German data, and (Li et al 2001) report 93% 
accuracy on extracting temporal relations in Chi-
nese. Because these accuracies are on different 
data sets and metrics, they cannot be compared 
directly with our methods. 
Recently, researchers have developed other 
tools for automatically tagging aspects of Ti-
meML, including EVENT (Sauri et al 2005) at 
0.80 F-measure and TIMEX36 tags at 0.82-0.85 
F-measure. In addition, the TERN competition 
(tern.mitre.org) has shown very high (close to .95  
F-measures) for TIMEX2 tagging, which is fairly 
similar to TIMEX3. These results suggest the 
time is ripe for exploiting ?imperfect? features in 
our machine learning approach. 
6 Conclusion 
Our research has uncovered one new finding: 
semantic reasoning (in this case, logical axioms 
for temporal closure), can be extremely valuable 
in addressing data sparseness. Without it, per-
formance on this task of learning temporal rela-
tions is poor; with it, it is excellent. We showed 
that temporal reasoning can be used as an over-
sampling method to dramatically expand the 
amount of training data for TLINK labeling, re-
sulting in labeling predictive accuracy as high as 
93% using an off-the-shelf Maximum Entropy 
classifier. Future research will investigate this 
effect further, as well as examine factors that 
enhance or mitigate this effect in different cor-
pora. 
The paper showed that ME-C performed sig-
nificantly better than a series of increasingly so-
phisticated baselines involving expansion of 
rules derived from human intuitions. Our results 
in these comparisons confirm the lessons learned 
from the corpus-based revolution, namely that 
rules based on intuition alone are prone to in-
completeness and are hard to tune without access 
to the distributions found in empirical data.  
Clearly, lexical rules have a role to play in se-
mantic and pragmatic reasoning from language, 
as in the discussion of example (2) in Section 1. 
Such rules, when mined by robust, large corpus-
based methods, as in the Google-derived VerbO-
cean, are clearly relevant, but too specific to ap-
ply more than a few times in the OTC corpus.  
It may be possible to acquire confidence 
weights for at least some of the intuitive rules in 
GTag from Google searches, so that we have a 
                                                 
6http://complingone.georgetown.edu/~linguist/GU_TIME_
DOWNLOAD.HTML 
759
level field for integrating confidence weights 
from the fairly general GTag rules and the fairly 
specific VerbOcean-like lexical rules. Further, 
the GTag and VerbOcean rules could be incorpo-
rated as features for machine learning, along with 
features from automatic preprocessing.  
We have taken pains to use freely download-
able resources like Carafe, VerbOcean, and 
WEKA to help others easily replicate and 
quickly ramp up a system. To further facilitate 
further research, our tools as well as labeled vec-
tors (unclosed as well as closed) are available for 
others to experiment with. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
Anders Berglund, Richard Johansson and Pierre 
Nugues. 2006. A Machine Learning Approach to 
Extract Temporal Information from Texts in Swed-
ish and Generate Animated 3D Scenes.  Proceed-
ings of EACL-2006. 
Branimir Boguraev and Rie Kubota Ando. 2005. Ti-
meML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270, 1999. 
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
760
Proceedings of the Linguistic Annotation Workshop, pages 109?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
Combining Independent Syntactic and Semantic Annotation Schemes
Marc Verhagen, Amber Stubbs and James Pustejovsky
Computer Science Department
Brandeis University, Waltham, USA
{marc,astubbs,jamesp}@cs.brandeis.edu
Abstract
We present MAIS, a UIMA-based environ-
ment for combining information from var-
ious annotated resources. Each resource
contains one mode of linguistic annotation
and remains independent from the other re-
sources. Interactions between annotations
are defined based on use cases.
1 Introduction
MAIS is designed to allow easy access to a set of
linguistic annotations. It embodies a methodology
to define interactions between separate annotation
schemes where each interaction is based on a use
case. With MAIS, we adopt the following require-
ments for the interoperability of syntactic and se-
mantic annotations:
1. Each annotation scheme has its own philosophy
and is independent from the other annotations.
Simple and generally available interfaces pro-
vide access to the content of each annotation
scheme.
2. Interactions between annotations are not de-
fined a priori, but based on use cases.
3. Simple tree-based and one-directional merg-
ing of annotations is useful for visualization of
overlap between schemes.
The annotation schemes currently embedded in
MAIS are the Proposition Bank (Palmer et al,
2005), NomBank (Meyers et al, 2004) and Time-
Bank (Pustejovsky et al, 2003). Other linguis-
tics annotation schemes like the opinion annotation
(Wiebe et al, 2005), named entity annotation, and
discourse annotation (Miltsakaki et al, 2004) will
be added in the future.
In the next section, we elaborate on the first
two requirements mentioned above and present the
MAIS methodology to achieve interoperability of
annotations. In section 3, we present the XBank
Browser, a unified browser that allows researchers
to inspect overlap between annotation schemes.
2 Interoperability of Annotations
Our goal is not to define a static merger of all anno-
tation schemes. Rather, we avoid defining a poten-
tially complex interlingua and instead focus on how
information from different sources can be combined
pragmatically. A high-level schematic representa-
tion of the system architecture is given in figure 1.
PropBank NomBank TimeBank
PropBank NomBank TimeBank
annotation     initializers
interface interface interface
case-based 
interaction
case-based 
interaction
GUI GUI
Figure 1: Architecture of MAIS
109
The simple and extensible interoperability of
MAIS can be put in place using three components: a
unified environment that stores the annotations and
implements some common functionality, a set of an-
notation interfaces, and a set of case-based interac-
tions.
2.1 Unified Environment
All annotations are embedded as stand-off annota-
tions in a unified environment in which each annota-
tion has its own namespace. This unified environ-
ment takes care of some basic functionality. For
example, given a tag from one annotation scheme,
there is a method that returns tags from other anno-
tation schemes that have the same text extent or tags
that have an overlap in text extent. The unified envi-
ronment chosen for MAIS is UIMA, the open plat-
form for unstructured information analysis created
by IBM.1
UIMA implements a common data representation
named CAS (Common Analysis Structure) that pro-
vides read and write access to the documents being
analyzed. Existing annotations can be imported into
a CAS using CAS Initializers. UIMA also provides
a framework for Analysis Engines: modules that can
read from and write to a CAS and that can be com-
bined into a complex work flow.
2.2 Annotation Interfaces
In the unified environment, the individual annota-
tions are independent from each other and they are
considered immutable. Each annotation defines an
interface through which salient details of the anno-
tations can be retrieved. For example, annotation
schemes that encodes predicate-argument structure,
that is, PropBank and NomBank, define methods
like
args-of-relation(pred)
arg-of-relation(pred, arg)
relation-of-argument(arg)
Similarly, the interface for TimeBank includes
methods like
rel-between(eventi, eventj)
events-before(event)
event-anchorings(event)
1http://www.research.ibm.com/UIMA/
The arguments to these methods are not strings
but text positions, where each text position contains
an offset and a document identifier. Return values
are also text positions. All interfaces are required to
include a method that returns the tuples that match a
given string:
get-locations(string, type)
This method returns a set of text positions. Each
text position points to a location where the input
string occurs as being of the given type. For Time-
Bank, the type could be event or time, for Prop-
Bank and NomBank, more appropriate values are
rel or arg0.
2.3 Case-based Interactions
Most of the integration work occurs in the interac-
tion components. Specific interactions can be built
using the unified environment and the specified in-
terfaces of each annotation scheme.
Take for example, the use case of an entity chron-
icle (Pustejovsky and Verhagen, 2007). An entity
chronicle follows an entity through time, display-
ing what events an entity was engaged in, how these
events are anchored to time expressions, and how the
events are ordered relative to each other. Such an
application depends on three kinds of information:
identification of named entities, predicate-argument
structure, and temporal relations. Each of these de-
rive from a separate annotation scheme. A use case
can be built using the interfaces for each annotation:
? the named entity annotation returns the text
extents of the named entity, using the gen-
eral method get-locations(string,
type)
? the predicate-argument annotation (accessed
through the PropBank and NomBank inter-
faces) returns the predicates that go with a
named-entity argument, repeatedly using the
method relation-of-argument(arg)
? finally, the temporal annotation returns the tem-
poral relations between all those predicates,
calling rel-between(eventi, eventj)
on all pairs of predicates
110
Note that named entity annotation is not inte-
grated into the current system. As a stopgap mea-
sure we use a pre-compiled list of named entities
and feed elements of this list into the PropBank
and NomBank interfaces, asking for those text po-
sitions where the entity is expressed as an argu-
ment. This shows the utility of a general method
like get-locations(string, type).
Each case-based interaction is implemented using
one or more UIMA analysis engines. It should be
noted that the analysis engines used for the entity
chronicler do not add data to the common data repre-
sentation. This is not a principled choice: if adding
new data to the CAS is useful then it can be part of
the case-based interaction, but these added data are
not integrated into existing annotations, rather, they
are added as a separate secondary resource.2
The point of this approach is that applications can
be built pragmatically, using only those resources
that are needed. It does not depend on fully merged
syntactic and semantic representations. The entity
chronicle, for example, does not require discourse
annotation, opinion annotation or any other resource
except for the three discussed before. An a priori
requirement to have a unified representation intro-
duces complexities that go beyond what?s needed for
individual applications.
This is not to say that a unified representation is
not useful on its own, there is obvious theoretical
interest in thoroughly exploring how annotations re-
late to each other. But we feel that the unified repre-
sentation is not needed for most, if not all, practical
applications.
3 The XBank Browser
The unified browser, named the XBank Browser, is
intended as a convenience for researchers. It shows
the overlap between different annotations. Annota-
tions from different schemes are merged into one
XML representation and a set of cascading style
sheets is used to display the information.
2In fact, for the entity chronicle it would be useful to have
extra data available. The current implementation uses what?s
provided by the basic resources plus a few heuristics to super-
ficially merge data from separate documents. But a more in-
formative chronicle along the lines of (Pustejovsky and Verha-
gen, 2007) would require more temporal links than available in
TimeBank. These can be pre-compiled and added using a dedi-
cated analysis engine.
The XBank Browser does not adhere to the MAIS
philosophy that all resources are independent. In-
stead, it designates one syntactic annotation to pro-
vide the basic shape of the XML tree and requires
tags from other annotations to find landing spots in
the basic tree.
The Penn Treebank annotation (Marcus et al,
1993) was chosen to be the first among equals: it
is the starting point for the merger and data from
other annotations are attached at tree nodes. Cur-
rently, only one heuristic is used to merge in data
from other sources: go up the tree to find a Treebank
constituent that contains the entire extent of the tag
that is merged in, then select the head of this con-
stituent. A more sophisticated approach would con-
sist of two steps:
? first try to find an exact match of the imported
tag with a Treebank constituent,
? if that fails, find the constituent that contains
the entire tag that is merged in, and select this
constituent
In the latter case, there can be an option to select
the head rather than the whole constituent. In any
case, the attached node will be marked if its original
extent does not line up with the extent at the tree
node.
It should be noted that this merging is one-
directional since no attempt is made to change the
shape of the tree defined by the Treebank annota-
tion.
The unified browser currently displays markups
from the Proposition Bank, NomBank, TimeBank
and the Discourse Treebank. Tags from individual
schemes can be hidden as desired. The main prob-
lem with the XBank Browser is that there is only a
limited amount of visual clues that can be used to
distinguish individual components from each other
and cognitive overload restricts how many annota-
tion schemes can be viewed at the same time. Nev-
ertheless, the browser does show how a limited num-
ber of annotation schemes relate to each other.
All functionality of the browser can be accessed at
http://timeml.org/ula/. An idea of what
it looks like can be gleaned from the screenshot dis-
played in figure 2. In this figure, boxes represent
relations from PropBank or NomBank and shaded
111
Figure 2: A glimpse of the XBank Browser
backgrounds represent arguments. Superscripts are
indexes that identify relations, subscripts identify
what relation an argument belongs to. Red fonts
indicate events from TimeBank. Note that the real
browser is barely done justice by this picture be-
cause the browser?s use of color is not visible.
4 Conclusion
We described MAIS, an environment that imple-
ments interoperability between syntactic and seman-
tic annotation schemes. The kind of interoperabil-
ity proposed herein does not require an elaborate
representational structure that allows the interaction.
Rather, it relies on independent annotation schemes
with interfaces to the outside world that interact
given a specific use case. The more annotations
there are, the more interactions can be defined. The
complexity of the methodology is not bound by the
number of annotation schemes integrated but by the
complexity of the use cases.
5 Acknowledgments
The work reported in this paper was performed as
part of the project ?Towards a Comprehensive Lin-
guistic Annotation of Language?, and supported un-
der award CNS-0551615 of the National Science
Foundation.
References
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treeb. Computational
Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The nombank
project: An interim report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Annota-
tion, pages 24?31, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguistics.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The penn discourse treebank.
In Proceedings of the Language Resources and Evalu-
ation Conference, Lisbon, Portugal.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky and Marc Verhagen. 2007. Con-
structing event-based entity chronicles. In Proceed-
ings of the IWCS-7, Tilburg, The Netherlands.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003. The timebank corpus. In Pro-
ceedings of Corpus Linguistics, pages 647?656.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
112
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 13:
Evaluating Events, Time Expressions, and Temporal Relations
(TempEval-2)
James Pustejovsky
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
jamesp@cs.brandeis.edu
Marc Verhagen
Computer Science Department
Brandeis University
Waltham, Massachusetts, USA
marc@cs.brandeis.edu
Abstract
We describe the TempEval-2 task which is
currently in preparation for the SemEval-2010
evaluation exercise. This task involves iden-
tifying the temporal relations between events
and temporal expressions in text. Six distinct
subtasks are defined, ranging from identifying
temporal and event expressions, to anchoring
events to temporal expressions, and ordering
events relative to each other.
1 Introduction
Newspaper texts, narratives and other such texts de-
scribe events which occur in time and specify the
temporal location and order of these events. Text
comprehension, even at the most general level, in-
volves the capability to identify the events described
in a text and locate these in time. This capability is
crucial to a wide range of NLP applications, from
document summarization and question answering to
machine translation. As in many areas of NLP, an
open evaluation challenge in the area of temporal an-
notation will serve to drive research forward.
The automatic identification of all temporal re-
ferring expressions, events, and temporal relations
within a text is the ultimate aim of research in this
area. However, addressing this aim in a first evalua-
tion challenge was deemed too difficult and a staged
approach was suggested. The 2007 SemEval task,
TempEval (henceforth TempEval-1), was an initial
evaluation exercise based on three limited tasks that
were considered realistic both from the perspective
of assembling resources for development and test-
ing and from the perspective of developing systems
capable of addressing the tasks.
We are now preparing TempEval-2, a temporal
evaluation task based on TempEval-1. TempEval-2
is more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
2 TempEval-1
TempEval-1 consisted of three tasks:
A. determine the relation between an event and a
timex in the same sentence;
B. determine the relation between an event and the
document creation time;
C. determine the relation between the main events
of two consecutive sentences.
The data sets were based on TimeBank (Puste-
jovsky et al, 2003; Boguraev et al, 2007), a hand-
built gold standard of annotated texts using the
TimeML markup scheme.1 The data sets included
sentence boundaries, TIMEX3 tags (including the
special document creation time tag), and EVENT
tags. For tasks A and B, a restricted set of events
was used, namely those events that occur more than
5 times in TimeBank. For all three tasks, the re-
lation labels used were BEFORE, AFTER, OVER-
LAP, BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER
and VAGUE.2 For a more elaborate description of
TempEval-1, see (Verhagen et al, 2007; Verhagen
et al, 2009).
1See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog number
LDC2006T08.
2Which is different from the set of 13 labels from TimeML.
The set of labels for TempEval-1 was simplified to aid data
preparation and to reduce the complexity of the task.
112
There were six systems competing in TempEval-
1: University of Colorado at Boulder (CU-TMP);
Language Computer Corporation (LCC-TE); Nara
Institute of Science and Technology (NAIST); Uni-
versity of Sheffield (USFD); Universities of Wolver-
hampton and Allicante (WVALI); and XEROX Re-
search Centre Europe (XRCE-T).
The difference between these systems was not
large, and details of system performance, along with
comparisons and evaluation, are presented in (Ver-
hagen et al, 2009). The scores for WVALI?s hybrid
approach were noticeably higher than those of the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the highest scoring
systems are barely ahead of the rest of the field. Sim-
ilarly, for task C using strict scoring, there is no sys-
tem that clearly separates itself from the field. Inter-
estingly, the baseline is close to the average system
performance on task A, but for other tasks the sys-
tem scores noticeably exceed the baseline. Note that
the XRCE-T system is somewhat conservative in as-
signing TLINKS for tasks A and B, producing lower
recall scores than other systems, which in turn yield
lower f-measure scores. For task A, this is mostly
due to a decision only to assign a temporal relation
between elements that can also be linked by the syn-
tactic analyzer.
3 TempEval-2
The set of tasks chosen for TempEval-1 was by no
means complete, but was a first step towards a fuller
set of tasks for temporal parsing of texts. While the
main goal of the division in subtasks was to aid eval-
uation, the larger goal of temporal annotation in or-
der to create a complete temporal characterization of
a document was not accomplished. Results from the
first competition indicate that task A was defined too
generally. As originally defined, it asks to tempo-
rally link all events in a sentence to all time expres-
sions in the same sentence. A clearer task would
have been to solicit local anchorings and to sepa-
rate these from the less well-defined temporal rela-
tions between arbitrary events and times in the same
sentence. We expect both inter-annotator agree-
ment and system performance to be higher with a
more precise subtask. Thus, the set of tasks used
in TempEval-1 is far from complete and the tasks
could have been made more restrictive. As a re-
sult, inter-annotator agreement scores lag, making
precise evaluation more challenging.
The overall goal of temporal tagging of a text is to
provide a temporal characterization of a set of events
that is as complete as possible. If the annotation
graph of a document is not completely connected
then it is impossible to determine temporal relations
between two arbitrary events because these events
could be in separate subgraphs. Hence, for the cur-
rent competition, TempEval-2, we have enriched the
task description to bring us closer to creating such
a temporal characterization for a text. We have en-
riched the TempEval-2 task definition to include six
distinct subtasks:
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL. The possible values of
TYPE are TIME, DATE, DURATION, and SET;
the value of VAL is a normalized value as de-
fined by the TIMEX2 and TIMEX3 standards.
B. Determine the extent of the events in a text as
defined by the TimeML EVENT tag. In addi-
tion, determine the value of the features TENSE,
ASPECT, POLARITY, and MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same sen-
tence. For TempEval-2, this task is further re-
stricted by requiring that either the event syn-
tactically dominates the time expression or the
event and time expression occur in the same
noun phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically dominates
the other event. This refers to examples like
?she heard an explosion? and ?he said they
postponed the meeting?.
The complete TimeML specification assumes the
temporal interval relations as defined by Allen
(Allen, 1983) in Figure 1.
113
A 
B A EQUALS B 
A 
B A is BEFORE B;  B is AFTER A 
A 
B A MEETS B;  B is MET BY A 
A 
B A OVERLAPS B;  B is OVERLAPPED BY A 
A 
B A STARTS B;  B is STARTED BY A 
A 
B A FINISHES B;  B is FINISHED BY A 
A 
B A is DURING B;  B CONTAINS A 
Figure 1: Allen Relations
For this task, however, we assume a reduced sub-
set, as introduced in TempEval-1: BEFORE, AFTER,
OVERLAP, BEFORE-OR-OVERLAP, OVERLAP-OR-
AFTER and VAGUE. However, we are investigat-
ing whether for some tasks the more precise set of
TimeML relations could be used.
Task participants may choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants may choose one
or more of the five languages for which we provide
data: English, Italian, Chinese, Spanish, and Ko-
rean.
3.1 Extent of Time Expression
This task involves identification of the EXTENT,
TYPE, and VAL of temporal expressions in the text.
Times can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following:
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80?s
e. Later this afternoon
f. yesterday
The TYPE of the temporal extent must be identified.
There are four temporal types that will be distin-
guished for this task;
(2) a. Time: at 2:45 p.m.
b. Date: January 27, 1920, yesterday
c. Duration two weeks
d. Set: every Monday morning
The VAL attribute will assume values according to
an extension of the ISO 8601 standard, as enhanced
by TIMEX2.
(3) November 22, 2004
<TIMEX3 tid="t1" type="DATE"
value="2004-11-22"/>
3.2 Extent of Event Expression
The EVENT tag is used to annotate those elements in
a text that describe what is conventionally referred to
as an eventuality. Syntactically, events are typically
expressed as inflected verbs, although event nomi-
nals, such as ?crash? in killed by the crash, should
also be annotated as EVENTs.
In this task, event extents must be identified and
tagged with EVENT, along with values for the fea-
tures TENSE, ASPECT, POLARITY, and MODALITY.
Examples of these features are shown below:
(4) should have bought
<EVENT id="e1" pred="BUY" pos="VERB"
tense="PAST" aspect="PERFECTIVE"
modality="SHOULD" polarity="POS"/>
(5) did not teach
<EVENT id="e2" pred="TEACH" pos="VERB"
tense="PAST" aspect="NONE"
modality="NONE" polarity="NEG"/>
The specifics on the definition of event extent
will follow the published TimeML guideline (cf.
timeml.org).
3.3 Within-sentence Event-Time Anchoring
This task involves determining the temporal relation
between an event and a time expression in the same
sentence. This was present in TempEval-1, but here,
in TempEval-2, this problem is further restricted by
requiring that the event either syntactically domi-
nates the time expression or the event and time ex-
pression occur in the same noun phrase. For exam-
ple, the following constructions will be targeted for
temporal labeling:
114
(6) Mary taughte1 on Tuesday morningt1
OVERLAP(e1,t1)
(7) They cancelled the eveningt2 classe2
OVERLAP(e2,t2)
3.4 Neighboring Sentence Event-Event
Ordering
In this task, the goal is to identify the temporal re-
lation between two main events in consecutive sen-
tences. This task was covered in the previous com-
petition, and includes pairs such as that shown be-
low:
(8) The President spokee1 to the nation on Tuesday
on the financial crisis. He had conferrede2 with
his cabinet regarding policy the day before.
AFTER(e1,e2)
3.5 Sentence Event-DCT Ordering
This task was also included in TempEval-1 and re-
quires the identification of the temporal order be-
tween the matrix event of the sentence and the Docu-
ment Creation Time (DCT) of the article or text. For
example, the text fragment below specifies a fixed
DCT, relative to which matrix events from the two
sentences are ordered:
(9) DCT: MARCH 5, 2009
a. Most troops will leavee1 Iraq by August of
2010. AFTER(e1,dct)
b. The country defaultede2 on debts for that
entire year. BEFORE(e2,dct)
3.6 Within-sentence Event-Event Ordering
The final task involves identifying the temporal re-
lation between two events, where one event syntac-
tically dominates the other event. This includes ex-
amples such as those illustrated below.
(10) The students hearde1 a fire alarme2.
OVERLAP(e1,e2)
(11) He saide1 they had postponede2 the meeting.
AFTER(e1,e2)
4 Resources and Evaluation
4.1 Data
The development corpus will contain the following
data:
1. Sentence boundaries;
2. The document creation time (DCT) for each
document;
3. All temporal expressions in accordance with
the TimeML TIMEX3 tag;
4. All events in accordance with the TimeML
EVENT tag;
5. Main event markers for each sentence;
6. All temporal relations defined by tasks C
through F.
The data for the five languages are being prepared
independently of each other. We do not provide a
parallel corpus. However, annotation specifications
and guidelines for the five languages will be devel-
oped in conjunction with one other. For some lan-
guages, we may not use all four temporal linking
tasks. Data preparation is currently underway for
English and will start soon for the other languages.
Obviously, data preparation is a large task. For En-
glish and Chinese, the data are being developed at
Brandeis University under three existing grants.
For evaluation data, we will provide two data sets,
each consisting of different documents. DataSet1 is
for tasks A and B and will contain data item 1 and 2
from the list above. DataSet2 is for tasks C though
F and will contain data items 1 through 5.
4.2 Data Preparation
For all languages, annotation guidelines are defined
for all tasks, based on version 1.2.1 of the TimeML
annotation guidelines for English3. The most no-
table changes relative to the previous TimeML
guidelines are the following:
? The guidelines are not all presented in one doc-
ument, but are split up according to the seven
TempEval-2 tasks. Full temporal annotation
has proven to be a very complex task, split-
ting it into subtasks with separate guidelines for
3See http://www.timeml.org.
115
each task has proven to make temporal annota-
tion more manageable.
? It is not required that all tasks for temporal link-
ing (tasks C through F) use the same relation
set. One of the goals during the data prepara-
tion phase is to determine what kind of relation
set makes sense for each individual task.
? The guidelines can be different depending on
the language. This is obviously required be-
cause time expressions, events, and relations
are expressed differently across languages.
Annotation proceeds in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where a
judge resolves disagreements between the annota-
tors. We are expanding the annotation tool used for
TempEval-1, making sure that we can quickly an-
notate data for all tasks while making it easy for a
language to define an annotation task in a slightly
different way from another language. The Brandeis
Annotation Tool (BAT) is a generic web-based anno-
tation tool that is centered around the notion of an-
notation tasks. With the task decomposition allowed
by BAT, it is possible to flexibly structure the com-
plex task of temporal annotation by splitting it up in
as many sub tasks as seems useful. As such, BAT is
well-suited for TempEval-2 annotation. Comparison
of annotation speed with tools that do not allow task
decomposition showed that annotation with BAT is
up to ten times faster. Annotation has started for
Italian and English.
For all tasks, precision and recall are used as eval-
uation metrics. A scoring program will be supplied
for participants.
5 Conclusion
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations between
events and temporal expressions in text. Using
a subset of TimeML temporal relations, we show
how temporal relations and anchorings can be an-
notated and identified in five different languages.
The markup language adopted presents a descrip-
tive framework with which to examine the tempo-
ral aspects of natural language information, demon-
strating in particular, how tense and temporal infor-
mation is encoded in specific sentences, and how
temporal relations are encoded between events and
temporal expressions. This work paves the way to-
wards establishing a broad and open standard meta-
data markup language for natural language texts, ex-
amining events, temporal expressions, and their or-
derings.
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. 2007. Timebank evolution as a community
resource for timeml parsing. Language Resource and
Evaluation, 41(1):91?115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser Saur??,
Andrew See, Andrea Setzer, and Beth Sundheim.
2003. The TimeBank Corpus. Corpus Linguistics,
March.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proc. of the Fourth Int. Work-
shop on Semantic Evaluations (SemEval-2007), pages
75?80, Prague, Czech Republic, June. Association for
Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The tempeval challenge: identifying
temporal relations in text. Language Resources and
Evaluation.
116
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57?62,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 13: TempEval-2
Marc Verhagen
?
, Roser Saur??
?
, Tommaso Caselli
?
and James Pustejovsky
?
? Computer Science Department, Brandeis University, Massachusetts, USA
?Barcelona Media, Barcelona, Spain ? ILC-CNR, Pisa, Italy
marc@cs.brandeis.edu roser.sauri@barcelonamedia.org
tommaso.caselli@ilc.cnr.it jamesp@cs.brandeis.edu
Abstract
Tempeval-2 comprises evaluation tasks for
time expressions, events and temporal re-
lations, the latter of which was split up in
four sub tasks, motivated by the notion that
smaller subtasks would make both data
preparation and temporal relation extrac-
tion easier. Manually annotated data were
provided for six languages: Chinese, En-
glish, French, Italian, Korean and Spanish.
1 Introduction
The ultimate aim of temporal processing is the au-
tomatic identification of all temporal referring ex-
pressions, events and temporal relations within a
text. However, addressing this aim is beyond the
scope of an evaluation challenge and a more mod-
est approach is appropriate.
The 2007 SemEval task, TempEval-1 (Verhagen
et al, 2007; Verhagen et al, 2009), was an initial
evaluation exercise based on three limited tempo-
ral ordering and anchoring tasks that were consid-
ered realistic both from the perspective of assem-
bling resources for development and testing and
from the perspective of developing systems capa-
ble of addressing the tasks.
1
TempEval-2 is based on TempEval-1, but is
more elaborate in two respects: (i) it is a multilin-
gual task, and (ii) it consists of six subtasks rather
than three.
In the rest of this paper, we first introduce the
data that we are dealing with. Which gets us in
a position to present the list of task introduced by
TempEval-2, including some motivation as to why
we feel that it is a good idea to split up temporal
relation classification into sub tasks. We proceed
by shortly describing the data resources and their
creation, followed by the performance of the sys-
tems that participated in the tasks.
1
The Semeval-2007 task was actually known simply as
TempEval, but here we use Tempeval-1 to avoid confusion.
2 TempEval Annotation
The TempEval annotation language is a simplified
version of TimeML.
2
using three TimeML tags:
TIMEX3, EVENT and TLINK.
TIMEX3 tags the time expressions in the text and
is identical to the TIMEX3 tag in TimeML. Times
can be expressed syntactically by adverbial or
prepositional phrases, as shown in the following
example.
(1) a. on Thursday
b. November 15, 2004
c. Thursday evening
d. in the late 80?s
e. later this afternoon
The two main attributes of the TIMEX3 tag are
TYPE and VAL, both shown in the example (2).
(2) November 22, 2004
type="DATE" val="2004-11-22"
For TempEval-2, we distinguish four temporal
types: TIME (at 2:45 p.m.), DATE (January 27,
1920, yesterday), DURATION (two weeks) and SET
(every Monday morning). The VAL attribute as-
sumes values according to an extension of the ISO
8601 standard, as enhanced by TIMEX2.
Each document has one special TIMEX3 tag,
the Document Creation Time (DCT), which is in-
terpreted as an interval that spans a whole day.
The EVENT tag is used to annotate those ele-
ments in a text that describe what is conventionally
referred to as an eventuality. Syntactically, events
are typically expressed as inflected verbs, although
event nominals, such as ?crash? in killed by the
crash, should also be annotated as EVENTS. The
most salient event attributes encode tense, aspect,
modality and polarity information. Examples of
some of these features are shown below:
2
See http://www.timeml.org for language speci-
fications and annotation guidelines
57
(3) should have bought
tense="PAST" aspect="PERFECTIVE"
modality="SHOULD" polarity="POS"
(4) did not teach
tense="PAST" aspect="NONE"
modality="NONE" polarity="NEG"
The relation types for the TimeML TLINK tag
form a fine-grained set based on James Allen?s
interval logic (Allen, 1983). For TempEval, the
set of labels was simplified to aid data preparation
and to reduce the complexity of the task. We use
only six relation types including the three core re-
lations BEFORE, AFTER, and OVERLAP, the two
less specific relations BEFORE-OR-OVERLAP and
OVERLAP-OR-AFTER for ambiguous cases, and fi-
nally the relation VAGUE for those cases where no
particular relation can be established.
Temporal relations come in two broad flavours:
anchorings of events to time expressions and or-
derings of events. Events can be anchored to an
adjacent time expression as in examples 5 and 6 or
to the document creation time as in 7.
(5) Mary taught
e1
on Tuesday morning
t1
OVERLAP(e1,t1)
(6) They cancelled the evening
t2
class
e2
OVERLAP(e2,t2)
(7) Most troops will leave
e1
Iraq by August of
2010. AFTER(e1,dct)
The country defaulted
e2
on debts for that en-
tire year. BEFORE(e2,dct)
In addition, events can be ordered relative to
other events, as in the examples below.
(8) The President spoke
e1
to the nation on
Tuesday on the financial crisis. He had
conferred
e2
with his cabinet regarding pol-
icy the day before. AFTER(e1,e2)
(9) The students heard
e1
a fire alarm
e2
.
OVERLAP(e1,e2)
(10) He said
e1
they had postponed
e2
the meeting.
AFTER(e1,e2)
3 TempEval-2 Tasks
We can now define the six TempEval tasks:
A. Determine the extent of the time expressions
in a text as defined by the TimeML TIMEX3
tag. In addition, determine value of the fea-
tures TYPE and VAL.
B. Determine the extent of the events in a text
as defined by the TimeML EVENT tag. In
addition, determine the value of the features
CLASS, TENSE, ASPECT, POLARITY, and
MODALITY.
C. Determine the temporal relation between an
event and a time expression in the same
sentence. This task is further restricted by
requiring that either the event syntactically
dominates the time expression or the event
and time expression occur in the same noun
phrase.
D. Determine the temporal relation between an
event and the document creation time.
E. Determine the temporal relation between two
main events in consecutive sentences.
F. Determine the temporal relation between two
events where one event syntactically domi-
nates the other event.
Of these tasks, C, D and E were also defined for
TempEval-1. However, the syntactic locality re-
striction in task C was not present in TempEval-1.
Task participants could choose to either do all
tasks, focus on the time expression task, focus on
the event task, or focus on the four temporal rela-
tion tasks. In addition, participants could choose
one or more of the six languages for which we pro-
vided data: Chinese, English, French, Italian, Ko-
rean, and Spanish.
We feel that well-defined tasks allow us to struc-
ture the workflow, allowing us to create task-
specific guidelines and using task-specific anno-
tation tools to speed up annotation. More im-
portantly, each task can be evaluated in a fairly
straightforward way, contrary to for example the
problems that pop up when evaluating two com-
plex temporal graphs for the same document. In
addition, tasks can be ranked, allowing systems to
feed the results of one (more precise) task as a fea-
ture into another task.
Splitting the task into substask reduces the error
rate in the manual annotation, and that merging
the different sub-task into a unique layer as a post-
processing operation (see figure 1) provides better
58
Figure 1: Merging Relations
and more reliable results (annotated data) than do-
ing a complex task all at once.
4 Data Preparation
The data for the five languages were prepared in-
dependently of each other and do not comprise a
parallel corpus. However, annotation specifica-
tions and guidelines for the five languages were
developed in conjunction with one other, in many
cases based on version 1.2.1 of the TimeML an-
notation guidelines for English
3
. Not all corpora
contained data for all six tasks. Table 1 gives the
size of the training set and the relation tasks that
were included.
language tokens C D E F X
Chinese 23,000 X X X X
English 63,000 X X X X
Italian 27,000 X X X
French 19,000 X
Korean 14,000
Spanish 68,000 X X
Table 1: Corpus size and relation tasks
All corpora include event and timex annota-
tion. The French corpus contained a subcorpus
with temporal relations but these relations were
not split into the four tasks C through F.
Annotation proceeded in two phases: a dual
annotation phase where two annotators annotate
each document and an adjudication phase where
a judge resolves disagreements between the an-
notators. Most languages used BAT, the Brandeis
Annotation Tool (Verhagen, 2010), a generic web-
based annotation tool that is centered around the
notion of annotation tasks. With the task decom-
position allowed by BAT, it is possible to structure
the complex task of temporal annotation by split-
ting it up in as many sub tasks as seems useful. As
3
See http://www.timeml.org.
such, BAT was well-suited for TempEval-2 anno-
tation.
We now give a few more details on the English
and Spanish data, skipping the other languages for
reasons that will become obvious at the beginning
of section 6.
The English data sets were based on TimeBank
(Pustejovsky et al, 2003; Boguraev et al, 2007),
a hand-built gold standard of annotated texts us-
ing the TimeML markup scheme.
4
However, all
event annotation was reviewed to make sure that
the annotation complied with the latest guidelines
and all temporal relations were added according to
the Tempeval-2 relation tasks, using the specified
relation types.
The data released for the TempEval-2 Spanish
edition is a fragment of the Spanish TimeBank,
currently under development. Its documents are
originally from the Spanish part of the AnCora
corpus (Taul?e et al, 2008). Data preparation fol-
lowed the annotation guidelines created to deal
with the specificities of event and timex expres-
sions in Spanish (Saur?? et al, 2009a; Saur?? et al,
2009b).
5 Evaluation Metrics
For the extents of events and time expres-
sions (tasks A and B), precision, recall and the
f1-measure are used as evaluation metrics, using
the following formulas:
precision = tp/(tp + fp)
recall = tp/(tp + fn)
f -measure = 2 ? (P ? R)/(P + R)
Where tp is the number of tokens that are part
of an extent in both key and response, fp is the
number of tokens that are part of an extent in the
response but not in the key, and fn is the number
of tokens that are part of an extent in the key but
not in the response.
For attributes of events and time expressions
(the second part of tasks A and B) and for relation
types (tasks C through F) we use an even simpler
metric: the number of correct answers divided by
the number of answers.
4
See www.timeml.org for details on TimeML, Time-
Bank is distributed free of charge by the Linguistic
Data Consortium (www.ldc.upenn.edu), catalog num-
ber LDC2006T08.
59
6 System Results
Eight teams participated in TempEval-2, submit-
ting a grand total of eighteen systems. Some of
these systems only participated in one or two tasks
while others participated in all tasks. The distribu-
tion over the six languages was very uneven: six-
teen systems for English, two for Spanish and one
for English and Spanish.
The results for task A, recognition and normal-
ization of time expressions, are given in tables 2
and 3.
team p r f type val
UC3M 0.90 0.87 0.88 0.91 0.83
TIPSem 0.95 0.87 0.91 0.91 0.78
TIPSem-B 0.97 0.81 0.88 0.99 0.75
Table 2: Task A results for Spanish
team p r f type val
Edinburgh 0.85 0.82 0.84 0.84 0.63
HeidelTime1 0.90 0.82 0.86 0.96 0.85
HeidelTime2 0.82 0.91 0.86 0.92 0.77
JU CSE 0.55 0.17 0.26 0.00 0.00
KUL 0.78 0.82 0.80 0.91 0.55
KUL Run 2 0.73 0.88 0.80 0.91 0.55
KUL Run 3 0.85 0.84 0.84 0.91 0.55
KUL Run 4 0.76 0.83 0.80 0.91 0.51
KUL Run 5 0.75 0.85 0.80 0.91 0.51
TERSEO 0.76 0.66 0.71 0.98 0.65
TIPSem 0.92 0.80 0.85 0.92 0.65
TIPSem-B 0.88 0.60 0.71 0.88 0.59
TRIOS 0.85 0.85 0.85 0.94 0.76
TRIPS 0.85 0.85 0.85 0.94 0.76
USFD2 0.84 0.79 0.82 0.90 0.17
Table 3: Task A results for English
The results for Spanish are more uniform and
generally higher than the results for English.
For Spanish, the f-measure for TIMEX3 extents
ranges from 0.88 through 0.91 with an average of
0.89; for English the f-measure ranges from 0.26
through 0.86, for an average of 0.78. However,
due to the small sample size it is hard to make
any generalizations. In both languages, type de-
tection clearly was a simpler task than determining
the value.
The results for task B, event recognition, are given
in tables 4 and 5. Both tables contain results for
both Spanish and English, the first part of each ta-
ble contains the results for Spanish and the next
part the results for English.
team p r f
TIPSem 0.90 0.86 0.88
TIPSem-B 0.92 0.85 0.88
team p r f
Edinburgh 0.75 0.85 0.80
JU CSE 0.48 0.56 0.52
TIPSem 0.81 0.86 0.83
TIPSem-B 0.83 0.81 0.82
TRIOS 0.80 0.74 0.77
TRIPS 0.55 0.88 0.68
Table 4: Event extent results
The column headers in table 5 are abbrevia-
tions for polarity (pol), mood (moo), modality
(mod), tense (tns), aspect (asp) and class (cl). Note
that the English team chose to include modality
whereas the Spanish team used mood.
team pol moo tns asp cl
TIPSem 0.92 0.80 0.96 0.89 0.66
TIPSem-B 0.92 0.79 0.96 0.89 0.66
team pol mod tns asp cl
Edinburgh 0.99 0.99 0.92 0.98 0.76
JU CSE 0.98 0.98 0.30 0.95 0.53
TIPSem 0.98 0.97 0.86 0.97 0.79
TIPSem-B 0.98 0.98 0.85 0.97 0.79
TRIOS 0.99 0.95 0.91 0.98 0.77
TRIPS 0.99 0.96 0.67 0.97 0.67
Table 5: Event attribute results
As with the time expressions results, the sample
size for Spanish is small, but note again the higher
f-measure for event extents in Spanish.
Table 6 shows the results for all relation tasks, with
the Spanish systems in the first two rows and the
English systems in the last six rows. Recall that for
Spanish the training and test sets only contained
data for tasks C and D.
Interestingly, the version of the TIPSem sys-
tems that were applied to the Spanish data did
much better on task C compared to its English
cousins, but much worse on task D, which is rather
puzzling.
Such a difference in performance of the systems
could be due to differences in annotation accurate-
ness, or it could be due to some particularities of
how the two languages express certain temporal
60
team C D E F
TIPSem 0.81 0.59 - -
TIPSem-B 0.81 0.59 - -
JU CSE 0.63 0.80 0.56 0.56
NCSU-indi 0.63 0.68 0.48 0.66
NCSU-joint 0.62 0.21 0.51 0.25
TIPSem 0.55 0.82 0.55 0.59
TIPSem-B 0.54 0.81 0.55 0.60
TRIOS 0.65 0.79 0.56 0.60
TRIPS 0.63 0.76 0.58 0.59
USFD2 0.63 - 0.45 -
Table 6: Results for relation tasks
aspects, or perhaps the one corpus is more ho-
mogeneous than the other. Again, there are not
enough data points, but the issue deserves further
attention.
For each task, the test data provided the event
pairs or event-timex pairs with the relation type
set to NONE and participating systems would re-
place that value with one of the six allowed rela-
tion types. However, participating systems were
allowed to not replace NONE and not be penalized
for it. Those cases would not be counted when
compiling the scores in table 6. Table 7 lists those
systems that did not classify all relation and the
percentage of relations for each task that those sys-
tems did not classify.
team C D E F
TRIOS 25% 19% 36% 31%
TRIPS 20% 10% 17% 10%
Table 7: Percentage not classified
A comparison with the Tempeval-1 results from
Semeval-2007 may be of interest. Six systems
participated in the TempEval-1 tasks, compared
to seven or eight systems for TempEval-2. Table
8 lists the average scores and the standard devi-
ations for all the tasks (on the English data) that
Tempeval-1 and Tempeval-2 have in common.
C D E
tempeval-1 average 0.59 0.76 0.51
stddev 0.03 0.03 0.05
tempeval-2 average 0.61 0.70 0.53
stddev 0.04 0.22 0.05
Table 8: Comparing Tempevals
The results are very similar except for task D,
but if we take a away the one outlier (the NCSU-
joint score of 0.21) then the average becomes 0.78
with a standard deviation of 0.05. However, we
had expected that for TempEval-2 the systems
would score better on task C since we added the
restriction that the event and time expression had
to be syntactically adjacent. It is not clear why the
results on task C have not improved.
7 Conclusion
In this paper, we described the TempEval-2 task
within the SemEval 2010 competition. This task
involves identifying the temporal relations be-
tween events and temporal expressions in text. Us-
ing a subset of TimeML temporal relations, we
show how temporal relations and anchorings can
be annotated and identified in six different lan-
guages. The markup language adopted presents
a descriptive framework with which to examine
the temporal aspects of natural language informa-
tion, demonstrating in particular, how tense and
temporal information is encoded in specific sen-
tences, and how temporal relations are encoded
between events and temporal expressions. This
work paves the way towards establishing a broad
and open standard metadata markup language for
natural language texts, examining events, tempo-
ral expressions, and their orderings.
One thing that would need to be addressed in
a follow-up task is what the optimal number of
tasks is. Tempeval-2 had six tasks, spread out over
six languages. This brought about some logisti-
cal challenges that delayed data delivery and may
have given rise to a situation where there was sim-
ply not enough time for many systems to properly
prepare. And clearly, the shared task was not suc-
cessful in attracting systems to four of the six lan-
guages.
8 Acknowledgements
Many people were involved in TempEval-2. We
want to express our gratitude to the following key
contributors: Nianwen Xue, Estela Saquete, Lo-
tus Goldberg, Seohyun Im, Andr?e Bittar, Nicoletta
Calzolari, Jessica Moszkowicz and Hyopil Shin.
Additional thanks to Joan Banach, Judith
Domingo, Pau Gim?enez, Jimena del Solar, Teresa
Su?nol, Allyson Ettinger, Sharon Spivak, Nahed
Abul-Hassan, Ari Abelman, John Polson, Alexan-
dra Nunez, Virginia Partridge, , Amber Stubbs,
Alex Plotnick, Yuping Zhou, Philippe Muller and
61
Irina Prodanof.
The work on the Spanish corpus was supported
by a EU Marie Curie International Reintegration
Grant (PIRG04-GA-2008-239414). Work on the
English corpus was supported under the NSF-CRI
grant 0551615, ?Towards a Comprehensive Lin-
guistic Annotation of Language? and the NSF-
INT-0753069 project ?Sustainable Interoperabil-
ity for Language Technology (SILT)?, funded by
the National Science Foundation.
Finally, thanks to all the participants, for stick-
ing with a task that was not always as flawless and
timely as it could have been in a perfect world.
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and
Marc Verhagen. 2007. Timebank evolution as a
community resource for timeml parsing. Language
Resource and Evaluation, 41(1):91?115.
James Pustejovsky, David Day, Lisa Ferro, Robert
Gaizauskas, Patrick Hanks, Marcia Lazo, Roser
Saur??, Andrew See, Andrea Setzer, and Beth Sund-
heim. 2003. The TimeBank Corpus. Corpus Lin-
guistics, March.
Roser Saur??, Olga Batiukova, and James Pustejovsky.
2009a. Annotating events in spanish. timeml an-
notation guidelines. Technical Report Version
TempEval-2010., Barcelona Media - Innovation
Center.
Roser Saur??, Estela Saquete, and James Pustejovsky.
2009b. Annotating time expressions in spanish.
timeml annotation guidelines. Technical Report
Version TempEval-2010, Barcelona Media - Inno-
vation Center.
Mariona Taul?e, Toni Mart??, and Marta Recasens. 2008.
Ancora: Multilevel annotated corpora for catalan
and spanish. In Proceedings of the LREC 2008,
Marrakesh, Morocco.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proc. of the Fourth
Int. Workshop on Semantic Evaluations (SemEval-
2007), pages 75?80, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation.
Marc Verhagen. 2010. The Brandeis Annotation Tool.
In Language Resources and Evaluation Conference,
LREC 2010, Malta.
62
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 1?9, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions,
Events, and Temporal Relations
Naushad UzZaman?1, Hector Llorens?1, Leon Derczynski?,
Marc Verhagen?, James Allen? and James Pustejovsky?
?: University of Rochester, USA; ?: University of Alicante, Spain
?: Department of Computer Science, University of Sheffield, UK
?: Computer Science Department, Brandeis University, USA
1: Nuance Communications
naushad@cs.rochester.edu, hllorens@dlsi.ua.es, leon@dcs.shef.ac.uk
Abstract
Within the SemEval-2013 evaluation exercise, the
TempEval-3 shared task aims to advance research
on temporal information processing. It follows on
from TempEval-1 and -2, with: a three-part struc-
ture covering temporal expression, event, and tem-
poral relation extraction; a larger dataset; and new
single measures to rank systems ? in each task and in
general. In this paper, we describe the participants?
approaches, results, and the observations from the
results, which may guide future research in this area.
1 Introduction
The TempEval task (Verhagen et al, 2009) was added as a
new task in SemEval-2007. The ultimate aim of research
in this area is the automatic identification of temporal ex-
pressions (timexes), events, and temporal relations within
a text as specified in TimeML annotation (Pustejovsky et
al., 2005). However, since addressing this aim in a first
evaluation challenge was deemed too difficult a staged
approach was suggested.
TempEval (henceforth TempEval-1) was an initial
evaluation exercise focusing only on the categorization of
temporal relations and only in English. It included three
relation types: event-timex, event-dct,1 and relations be-
tween main events in consecutive sentences.
TempEval-2 (Verhagen et al, 2010) extended
TempEval-1, growing into a multilingual task, and con-
sisting of six subtasks rather than three. This included
event and timex extraction, as well as the three relation
tasks from TempEval-1, with the addition of a relation
task where one event subordinates another.
TempEval-3 (UzZaman et al, 2012b) is a follow-up
to TempEval 1 and 2, covering English and Spanish.
TempEval-3 is different from its predecessors in a few
respects:
1DCT stands for document creation time
Size of the corpus: the dataset used has about 600K
word silver standard data and about 100K word gold stan-
dard data for training, compared to around 50K word cor-
pus used in TempEval 1 and 2. Temporal annotation is
a time-consuming task for humans, which has limited
the size of annotated data in previous TempEval exer-
cises. Current systems, however, are performing close to
the inter-annotator reliability, which suggests that larger
corpora could be built from automatically annotated data
with minor human reviews. We want to explore whether
there is value in adding a large automatically created sil-
ver standard to a hand-crafted gold standard.
End-to-end temporal relation processing task: the
temporal relation classification tasks are performed from
raw text, i.e. participants need to extract their own events
and temporal expressions first, determine which ones to
link and then obtain the relation types. In previous Tem-
pEvals, gold timexes, events, and relations (without cate-
gory) were given to participants.
Temporal relation types: the full set of temporal re-
lations in TimeML are used, rather than the reduced set
used in earlier TempEvals.
Platinum test set: A new test dataset has been devel-
oped for this edition. It is based on manual annotations
by experts over new text (unseen in previous editions).
Evaluation: we report a temporal awareness score for
evaluating temporal relations, which helps to rank sys-
tems with a single score.
2 Data
In TempEval-3, we reviewed and corrected existing cor-
pora, and also released new corpora.
2.1 Reviewing Existing Corpora
We considered the existing TimeBank (Pustejovsky et al,
2003) and AQUAINT2 data for TempEval-3. TempEval-
2See http://timeml.org/site/timebank/timebank.html
1
Entity Agreement
Event 0.87
Event class 0.92
Timex 0.87
Timex value 0.88
Table 1: Platinum corpus entity inter-annotator agreement.
Corpus # of words Standard
TimeBank 61,418 Gold
AQUAINT 33,973 Gold
TempEval-3 Silver 666,309 Silver
TempEval-3 Eval 6,375 Platinum
TimeBank-ES Train 57,977 Gold
TimeBank-ES Eval 9,833 Gold
Table 2: Corpora used in TempEval-3.
1 and TempEval-2 had the same documents as TimeBank
but different relation types and events.
For both TimeBank and AQUAINT, we, (i) cleaned up
the formatting for all files making it easy to review and
read, (ii) made all files XML and TimeML schema com-
patible, (iii) added some missing events and temporal ex-
pressions. In TimeBank, we, (i) borrowed the events from
the TempEval-2 corpus and (ii) borrowed the temporal re-
lations from TimeBank corpus, which contains a full set
of temporal relations. In AQUAINT, we added the tem-
poral relations between event and DCT (document cre-
ation time), which was missing for many documents in
that corpus. These existing corpora comprised the high-
quality component of our training set.
2.2 New Corpora
We created two new datasets: a small, manually-
annotated set over new text (platinum); and a machine-
annotated, automatically-merged dataset based on out-
puts of multiple systems (silver).
The TempEval-3 platinum evaluation corpus was anno-
tated/reviewed by the organizers, who are experts in the
area. This process used the TimeML Annotation Guide-
lines v1.2.1 (Saur?? et al, 2006). Every file was anno-
tated independently by at least two expert annotators, and
a third was dedicated to adjudicating between annotations
and merging the final result. Some annotators based their
work on TIPSem annotation suggestions (Llorens et al,
2012b). The GATE Annotation Diff tool was used for
merging (Cunningham et al, 2013), a custom TimeML
validator ensured integrity,3 and CAVaT (Derczynski and
Gaizauskas, 2010) was used to determine various modes
of TimeML mis-annotation and inconsistency that are in-
expressable via XML schema. Post-exercise, that corpus
(TempEval-3 Platinum with around 6K tokens, on com-
pletely new text) is released for the community to review
3See https://github.com/hllorens/TimeML-validator
and improve.4 Inter-annotator agreement (measured with
F1, as per Hripcsak and Rothschild (2005)) and the num-
ber of annotation passes per document were higher than
in existing TimeML corpora, hence the name. Details are
given in Table 1. Attribute value scores are given based
on the agreed entity set. These are for exact matches.
The TempEval-3 silver evaluation corpus is a 600K
word corpus collected from Gigaword (Parker et
al., 2011). We automatically annotated this corpus
by TIPSem, TIPSem-B (Llorens et al, 2013) and
TRIOS (UzZaman and Allen, 2010). These systems were
retrained on the corrected TimeBank and AQUAINT cor-
pus to generate the original TimeML temporal relation
set. We then merged these three state-of-the-art sys-
tem outputs using our merging algorithm (Llorens et al,
2012a). In our selected merged configuration all entities
and relations suggested by the best system (TIPSem) are
added in the merged output. Suggestions from other sys-
tems (TRIOS and TIPSem-B) are added in the merged
output, only if they are also supported by another system.
The weights considered in our configuration are: TIPSem
0.36, TIPSemB 0.32, TRIOS 0.32.
For Spanish, Spanish TimeBank 1.0 corpus (Saur?? and
Badia, 2012) wads used. It is the same corpus that was
used in TempEval-2, with a major review of entity anno-
tation and an important improvement regarding temporal
relation annotation. For TempEval-3, we converted ES-
TimeBank link types to the TimeML standard types based
on Allen?s temporal relations (Allen, 1983).
Table 2 summarizes our released corpora, measured
with PTB-scheme tokens as words. All data produced
was annotated using a well-defined subset of TimeML,
designed for easy processing, and for reduced ambigu-
ity compared to standard TimeML. Participants were en-
couraged to validate their submissions using a purpose-
built tool to ensure that submitted runs were legible. We
called this standard TimeML-strict, and release it sepa-
rately (Derczynski et al, 2013).
3 Tasks
The three main tasks proposed for TempEval-3 focus on
TimeML entities and relations:
3.1 Task A (Timex extraction and normalization)
Determine the extent of the timexes in a text as defined
by the TimeML TIMEX3 tag. In addition, determine the
value of the features TYPE and VALUE. The possible
values of TYPE are time, date, duration, and set; VALUE
is a normalized value as defined by the TIMEX3 standard.
4In the ACL data and code repository, reference ADCR2013T001.
See also https://bitbucket.org/leondz/te3-platinum
2
3.2 Task B (Event extraction and classification)
Determine the extent of the events in a text as defined by
the TimeML EVENT tag and the appropriate CLASS.
3.3 Task ABC (Annotating temporal relations)
This is the ultimate task for evaluating an end-to-end sys-
tem that goes from raw text to TimeML annotation of
entities and links. It entails performing tasks A and B.
From raw text extract the temporal entities (events and
timexes), identify the pairs of temporal entities that have
a temporal link (TLINK) and classify the temporal re-
lation between them. Possible pair of entities that can
have a temporal link are: (i) main events of consecu-
tive sentences, (ii) pairs of events in the same sentence,
(iii) event and timex in the same sentence and (iv) event
and document creation time. In TempEval-3, TimeML
relation are used, i.e.: BEFORE, AFTER, INCLUDES, IS-
INCLUDED, DURING, SIMULTANEOUS, IMMEDIATELY
AFTER, IMMEDIATELY BEFORE, IDENTITY, BEGINS,
ENDS, BEGUN-BY and ENDED-BY.
In addition to this main tasks, we also include two extra
temporal relation tasks:
Task C (Annotating relations given gold entities)
Given the gold entities, identify the pairs of entities that
have a temporal link (TLINK) and classify the temporal
relations between them.
Task C relation only (Annotating relations given gold
entities and related pairs) Given the temporal entities
and the pair of entities that have a temporal link, classify
the temporal relation between them.
4 Evaluation Metrics
The metrics used to evaluate the participants are:
4.1 Temporal Entity Extraction
To evaluate temporal entities (events and temporal ex-
pressions), we need to evaluate, (i) How many entities are
correctly identified, (ii) If the extents for the entities are
correctly identified, and (iii) How many entity attributes
are correctly identified. We use classical precision and
recall for recognition.
How many entities are correctly identified: We evalu-
ate our entities using the entity-based evaluation with the
equations below.
Precision = |Sysentity?Refentity||Sysentity|
Recall = |Sysentity?Refentity||Refentity|
where, Sysentity contains the entities extracted by the
system that we want to evaluate, and Refentity contains
the entities from the reference annotation that are being
compared.
If the extents for the entities are correctly identified:
We compare our entities with both strict match and re-
laxed match. When there is a exact match between the
system entity and gold entity then we call it strict match,
e.g. ?sunday morning? vs ?sunday morning?. When there
is a overlap between the system entity and gold entity
then we call it relaxed match, e.g. ?sunday? vs ?sunday
morning?. When there is a relaxed match, we compare
the attribute values.
How many entity attributes are correctly identified: We
evaluate our entity attributes using the attribute F1-score,
which captures how well the system identified both the
entity and attribute (attr) together.
Attribute Recall =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Refentity|
Attribute Precision =
|{?x | x?(Sysentity?Refentity)?Sysattr(x)==Refattr(x)}|
|Sysentity|
Attribute F1-score = 2?p?rp+r
Attribute (Attr) accuracy, precision and recall can be
calculated as well from the above information.
Attr Accuracy = Attr F1 / Entity Extraction F1
Attr R = Attr Accuracy * Entity R
Attr P = Attr Accuracy * Entity P
4.2 Temporal Relation Processing
To evaluate relations, we use the evaluation metric pre-
sented by UzZaman and Allen (2011).5 This metric cap-
tures the temporal awareness of an annotation in terms
of precision, recall and F1 score. Temporal awareness
is defined as the performance of an annotation as identi-
fying and categorizing temporal relations, which implies
the correct recognition and classification of the tempo-
ral entities involved in the relations. Unlike TempEval-
2 relation score, where only categorization is evaluated
for relations, this metric evaluates how well pairs of enti-
ties are identified, how well the relations are categorized,
and how well the events and temporal expressions are ex-
tracted.
Precision =
|Sys?relation?Ref
+
relation|
|Sys?relation|
Recall =
|Ref?relation?Sys
+
relation|
|Ref?relation|
where, G+ is the closure of graph G and G? is the
reduced of graph G, where redundant relations are re-
moved.6
We calculate the Precision by checking the number
of reduced system relations (Sys?relation) that can be veri-
fied from the reference annotation temporal closure graph
(Ref+relation), out of number of temporal relations in the
5We used a minor variation of the formula, where we consider the
reduced graph instead of all system or reference relations. Details can
be found in Chapter 6 of UzZaman (2012).
6A relation is redundant if it can be inferred through other relations.
3
strict value
F1 P R F1 F1
HeidelTime-t 90.30 93.08 87.68 81.34 77.61
HeidelTime-bf 87.31 90.00 84.78 78.36 72.39
HeidelTime-1.2 86.99 89.31 84.78 78.07 72.12
NavyTime-1,2 90.32 89.36 91.30 79.57 70.97
ManTIME-4 89.66 95.12 84.78 74.33 68.97
ManTIME-6 87.55 98.20 78.99 73.09 68.27
ManTIME-3 87.06 94.87 80.43 69.80 67.45
SUTime 90.32 89.36 91.30 79.57 67.38
ManTIME-1 87.20 97.32 78.99 70.40 67.20
ManTIME-5 87.20 97.32 78.99 69.60 67.20
ManTIME-2 88.10 97.37 80.43 72.22 66.67
ATT-2 85.25 98.11 75.36 78.69 65.57
ATT-1 85.60 99.05 75.36 79.01 65.02
ClearTK-1,2 90.23 93.75 86.96 82.71 64.66
JU-CSE 86.38 93.28 80.43 75.49 63.81
KUL 83.67 92.92 76.09 69.32 62.95
KUL-TE3RunABC 82.87 92.04 75.36 73.31 62.15
ClearTK-3,4 87.94 94.96 81.88 77.04 61.48
ATT-3 80.85 97.94 68.84 72.34 60.43
FSS-TimEx 85.06 90.24 80.43 49.04 58.24
TIPSem (TE2) 84.90 97.20 75.36 81.63 65.31
Table 3: Task A - Temporal Expression Performance.
reduced system relations (Sys?relation). Similarly, we
calculate the Recall by checking the number of reduced
reference annotation relations (Ref?relation) that can be
verified from the system output?s temporal closure graph
(Sys+relation), out of number of temporal relations in the
reduced reference annotation (Ref?relation).
This metric evaluates Task ABC together. For Task C
and Task C - relation only, all the gold annotation entities
were provided and then evaluated using the above metric.
Our evaluation toolkit that evaluated TempEval-3 par-
ticipants is available online.7
5 Evaluation Results
The aim of this evaluation is to provide a meaningful re-
port of the performance obtained by the participants in
the tasks defined in Section 3.
Furthermore, the results include TIPSem as reference
for comparison. This was used as a pre-annotation system
in some cases. TIPSem obtained the best results in event
processing task in TempEval-2 and offered very compet-
itive results in timex and relation processing. The best
timex processing system in TempEval-2 (HeidelTime) is
participating in this edition as well, therefore we included
TIPSem as a reference in all tasks.
We only report results in main measures. Results are
divided by language and shown per task. Detailed scores
can be found on the task website.8
7See http://www.cs.rochester.edu/u/naushad/temporal
8See http://www.cs.york.ac.uk/semeval-2013/task1/
5.1 Results for English
5.1.1 Task A: Timexes
We had nine participants and 21 unique runs for tem-
poral expression extraction task, Task A. Table 3 shows
the results. Details about participants? approaches can be
found in Table 4.
We rank the participants for Task A on the F1 score
of most important timex attribute ? Value. To get the
attribute Value correct, a system needs to correctly nor-
malise the temporal expression. This score (Value F1)
captures the performance of extracting the timex and
identifying the attribute Value together (Value F1 = Timex
F1 * Value Accuracy).
Participants approached the temporal expression ex-
traction task with rule-engineered methods, machine
learning methods and also hybrid methods. For temporal
expression normalization (identifying the timex attribute
value), all participants used rule-engineered approaches.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: Competition was close for timex recogni-
tion and the best systems all performed within 1% of
each other. On our newswire corpus, statistical systems
(ClearTK) performed best at strict matching, and rule-
engineered system best at relaxed matching (NavyTime,
SUTime, HeidelTime).
Strategy: post-processing, on top of machine learning-
base temporal expression extraction, provided a statisti-
cally significant improvement in both precision and recall
(ManTIME).
Data: using the large silver dataset, alone or together
with human annotated data, did not give improvements in
performance for Task A. Human-annotated gold standard
data alone provided the best performance (ManTIME).
Data: TimeBank alone was better than TimeBank and
AQUAINT together for Task A (ClearTK).
Features: syntactic and gazetteers did not provide any
statistically significant increment of performance with re-
spect to the morphological features alone (ManTIME).
Regarding the two sub-tasks of timex annotation,
recognition and interpretation/normalisation, we noticed
a shift in the state of the art. While normalisation is
currently (and perhaps inherently) done best by rule-
engineered systems, recognition is now done well by a
variety of methods. Where formerly, rule-engineered
timex recognition always outperformed other classes of
approach, now it is clear that rule-engineering and ma-
chine learning are equally good at timex recognition.
5.1.2 Task B: Events
For event extraction (Task B) we had seven participants
and 10 unique runs. The results for this task can be found
in Table 6. We rank the participants for TaskB on the F1
score of most important event attribute ? Class. Class
4
Strategy System Training data Classifier used
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt
ClearTK-1, 2 TimeBank SVM, Logit
ClearTK-3, 4 TBAQ SVM, Logit
JU-CSE TBAQ CRF
ManTIME-1 TBAQ + TE3Silver CRF
ManTIME-3 TBAQ CRF
ManTIME-5 TE3Silver CRF
Temp : ESAfeature TBAQ MaxEnt
Temp : WordNetfeature TBAQ MaxEnt
TIPSem (TE2) TBAQ CRF
Rule-based FSS-TimEx (EN) None None
FSS-TimEx (ES) None None
HeidelTime-1.2, bf (EN) None None
HeidelTime-t (EN) TBAQ None
HeidelTime (ES) Gold None
NavyTime-1, 2 None None
SUTime None None
Hybrid KUL TBAQ + TE3Silver Logit + post-processing
KUL-TE3RunABC TBAQ +TE3Silver Logit + post-processing
ManTIME-2 TBAQ + TE3Silver CRF + post-processing
ManTIME-4 TBAQ CRF + post-processing
ManTIME-6 TE3Silver CRF + post-processing
Table 4: Automated approaches for TE3 Timex Extraction
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ATT-1, 2, 3 TBAQ + TE3Silver MaxEnt ms, ss
ClearTK-1, 2 TimeBank SVM, Logit ms
ClearTK-3, 4 TBAQ SVM, Logit ms
JU-CSE TBAQ CRF
KUL TBAQ +TE3Silver Logit ms, ls
KUL-TE3RunABC TBAQ +TE3Silver Logit ms, ls
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
Temp : ESAfeature TBAQ MaxEnt ms, ls, ss
Temp : WordNetfeature TBAQ MaxEnt ms, ls
TIPSem (TE2) TBAQ CRF/SVM ms, ls, ss
Rule-based FSS-TimEx (EN) None None ls, ms
FSS-TimEx (ES) None None ls, ms
Table 5: Automated approaches for Event Extraction
5
F1 P R class F1
ATT-1 81.05 81.44 80.67 71.88
ATT-2 80.91 81.02 80.81 71.10
KUL 79.32 80.69 77.99 70.17
ATT-3 78.63 81.95 75.57 69.55
KUL-TE3RunABC 77.11 77.58 76.64 68.74
ClearTK-3,4 78.81 81.40 76.38 67.87
NavyTime-1 80.30 80.73 79.87 67.48
ClearTK-1,2 77.34 81.86 73.29 65.44
NavyTime-2 79.37 80.52 78.26 64.81
Temp:ESAfeature 68.97 78.33 61.61 54.55
JU-CSE 78.62 80.85 76.51 52.69
Temp:WordNetfeature 63.90 78.90 53.69 50.00
FSS-TimEx 65.06 63.13 67.11 42.94
TIPSem (TE2) 82.89 83.51 82.28 75.59
Table 6: Task B - Event Extraction Performance.
F1 P R
ClearTK-2 30.98 34.08 28.40
ClearTK-1 29.77 34.49 26.19
ClearTK-3 28.62 30.94 26.63
ClearTK-4 28.46 29.73 27.29
NavyTime-1 27.28 31.25 24.20
JU-CSE 24.61 19.17 34.36
NavyTime-2 21.99 26.52 18.78
KUL-TE3RunABC 19.01 17.94 20.22
TIPSem (TE2) 42.39 38.79 46.74
Table 7: Task ABC - Temporal Awareness Evaluation (Task C
evaluation from raw text).
F1 captures the performance of extracting the event and
identifying the attribute Class together (Class F1 = Event
F1 * Class Accuracy).
All the participants except one used machine learning
approaches. Details about the participants? approaches
and the linguistic knowledge9 used to solve this problem,
and training data, are in Table 5.
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: All the high performing systems for event
extraction (Task B) are machine learning-based.
Data: Systems using silver data, along with the hu-
man annotated gold standard data, performed very well
(top three participants in the task ? ATT, KUL, KUL-
TE3RunABC). Additionally, TimeBank and AQUAINT
together performed better than just TimeBank alone
(NavyTime-1, ClearTK-3,4).
Linguistic Features: Semantic features (ls and ss) have
played an important role, since the best systems (TIPSem,
ATT1 and KUL) include them. However, these three are
not the only systems using semantic features.
9Abbreviations used in the table: TBAQ ? TimeBank + AQUAINT
corpus ms ? morphosyntactic information, e.g. POS, lexical informa-
tion, morphological information and syntactic parsing related features;
ls ?lexical semantic information, e.g. WordNet synsets; ss ? sentence-
level semantic information, e.g. Semantic Role labels.
F1 P R
ClearTK-2 36.26 37.32 35.25
ClearTK-4 35.86 35.17 36.57
ClearTK-1 35.19 37.64 33.04
UTTime-5 34.90 35.94 33.92
ClearTK-3 34.13 33.27 35.03
NavyTime-1 31.06 35.48 27.62
UTTime-4 28.81 37.41 23.43
JU-CSE 26.41 21.04 35.47
NavyTime-2 25.84 31.10 22.10
KUL-TE3RunABC 24.83 23.35 26.52
UTTime-1 24.65 15.18 65.64
UTTime-3 24.28 15.10 61.99
UTTime-2 24.05 14.80 64.20
TIPSem (TE2) 44.25 39.71 49.94
Table 8: Task C - TLINK Identification and Classification.
F1 P R
UTTime-1, 4 56.45 55.58 57.35
UTTime-3, 5 54.70 53.85 55.58
UTTime-2 54.26 53.20 55.36
NavyTime-1 46.83 46.59 47.07
NavyTime-2 43.92 43.65 44.20
JU-CSE 34.77 35.07 34.48
Table 9: Task C - relation only: Relation Classification.
5.1.3 Task C: Relation Evaluation
For complete temporal annotation from raw text (Task
ABC - Task C from raw text) and for temporal relation
only tasks (Task C, Task C relation only), we had five
participants in total.
For relation evaluation, we primarily evaluate on Task
ABC (Task C from raw text), which requires joint entity
extraction, link identification and relation classification.
The results for this task can be found in Table 7.
While TIPSem obtained the best results in task ABC,
especially in recall, it was used by some annotators to
pre-label data. In the interest of rigour and fairness, we
separate out this system.
For task C, for provided participants with entities and
participants identified: between which entity pairs a rela-
tion exists (link identification); and the class of that rela-
tion. Results are given in Table 8. We also evaluate the
participants on the relation by providing the entities and
the links (performance in Table 9) ? TIPSem could not be
evaluated in this setting since the system is not prepared
to do categorization only unless the relations are divided
as in TempEval-2. For these Task C related tasks, we had
only one new participant, who didn?t participate in Task
A and B: UTTime.
Identifying which pair of entities to consider for tem-
poral relations is a new task in this TempEval challenge.
The participants approached the problems in data-driven,
rule-based and also in hybrid ways (Table 1010). On
10New abbreviation in the table, e-attr ? entity attributes, e.g. event
class, tense, aspect, polarity, modality; timex type, value.
6
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit e-attr, ms
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit e-attr, ms
ClearTK-3 TBAQ SVM, Logit e-attr, ms
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit e-attr, ms
KULRunABC TBAQ SVM, Logit ms
Rule-based JU-CSE None None
UTTime-1, 2 ,3 None None
TIPSem (TE2) None None e-attr, ms, ls, ss
Hybrid NavyTime-1 TBAQ MaxEnt ms
NavyTime-2 TimeBank MaxEnt ms
UTTime-4 TBAQ Logit ms, ls, ss
UTTime-5 TBAQ + inverse relations Logit ms, ls, ss
Table 10: Automated approaches for TE3 TLINK Identification
Strategy System Training data Classifier used Linguistic
Knowledge
Data-driven ClearTK-1 TimeBank SVM, Logit ms, ls
ClearTK-2 TimeBank + Bethard et al (2007) SVM, Logit ms, ls
ClearTK-3 TBAQ SVM, Logit ms, ls
ClearTK-4 TBAQ + Muller?s inferences SVM, Logit ms, ls
JU-CSE TBAQ CRF
KULRunABC TBAQ SVM, Logit ms
NavyTime-1 TBAQ MaxEnt ms, ls
NavyTime-2 TimeBank MaxEnt ms, ls
UTTime-1,4, 2 TBAQ Logit ms, ls, ss
UTTime-3,5 TBAQ + inverse relations Logit ms, ls, ss
TIPSem (TE-2) TBAQ CRF/SVM ms, ls, ss
Table 11: Automated approaches for Relation Classification
the other hand, all the participants used data-driven ap-
proaches for temporal relations (Table 11).
Observations: We collected the following observa-
tions from the results and from participants? experiments.
Strategy: For relation classification, all participants
used partially or fully machine learning-based systems.
Data: None of the participants implemented their sys-
tems training on the silver data. Most of the systems use
the combined TimeBank and AQUAINT (TBAQ) corpus.
Data: Adding additional high-quality relations, either
Philippe Muller?s closure-based inferences or the verb
clause relations from Bethard et al (2007), typically in-
creased recall and the overall performance (ClearTK runs
two and four).
Features: Participants mostly used the morphosyntac-
tic and lexical semantic information. The best perform-
ing systems from TempEval-2 (TIPSem and TRIOS) ad-
ditionally used sentence level semantic information. One
participant in TempEval-3 (UTTime) also did deep pars-
ing for the sentence level semantic features.
Features: Using more Linguistic knowledge is impor-
tant for the task, but it is more important to execute it
properly. Many systems performed better using less lin-
guistic knowledge. Hence a system (e.g. ClearTK) with
basic morphosyntactic features is hard to beat with more
semantic features, if not used properly.
entity extraction
strict relaxed
F1 F1 P R value
HeidelTime 85.3 90.1 96.0 84.9 87.5
TIPSemB-F 82.6 87.4 93.7 81.9 82.0
FSS-TimEx 49.5 65.2 86.6 52.3 62.7
Table 12: Task A: Temporal Expression (Spanish).
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 57.6 89.8 42.4 24.9 - -
TIPSemB-F 88.8 91.7 86.0 57.6 41.0 36.3
Table 13: Task B: Event Extraction (Spanish).
Classifier: Across the various tasks, ClearTK tried
Mallet CRF, Mallet MaxEnt, OpenNLP MaxEnt, and LI-
BLINEAR (SVMs and logistic regression). They picked
the final classifiers by running a grid search over models
and parameters on the training data, and for all tasks, a
LIBLINEAR model was at least as good as all the other
models. As an added bonus, it was way faster to train
than most of the other models.
6 Evaluation Results (Spanish)
There were two participants for Spanish. Both partici-
pated in task A and only one of them in task B. In this
7
F1 P R
TIPSemB-F 41.6 37.8 46.2
Table 14: Task ABC: Temporal Awareness (Spanish).
entity extraction attributes
strict relaxed val type
F1 F1 P R F1 F1
HeidelTime 86.4 89.8 94.0 85.9 87.5 89.8
FSS-TimEx 42.1 68.4 86.7 56.5 48.7 65.8
TIPSem 86.9 93.7 98.8 89.1 75.4 88.0
TIPSemB-F 84.3 89.9 93.0 87.0 82.0 86.5
Table 15: Task A: TempEval-2 test set (Spanish).
case, TIPSemB-Freeling is provided as a state-of-the-art
reference covering all the tasks. TIPSemB-Freeling is the
Spanish version of TIPSem with the main difference that
it does not include semantic roles. Furthermore, it uses
Freeling (Padro? and Stanilovsky, 2012) to obtain the lin-
guistic features automatically.
Table 12 shows the results obtained for task A. As it
can be observed HeidelTime obtains the best results. It
improves the previous state-of-the-art results (TIPSemB-
F), especially in normalization (value F1).
Table 13 shows the results from event extraction. In
this case, the previous state-of-the-art is not improved.
Table 14 only shows the results obtained in temporal
awareness by the state-of-the-art system since there were
not participants on this task. We observe that TIPSemB-F
approach offers competitive results, which is comparable
to results obtained in TE3 English test set.
6.1 Comparison with TempEval-2
TempEval-2 Spanish test set is included as a subset of this
TempEval-3 test set. We can therefore compare the per-
formance across editions. Furthermore, we can include
the full-featured TIPSem (Llorens et al, 2010), which
unlike TIPSemB-F used the AnCora (Taule? et al, 2008)
corpus annotations as features including semantic roles.
For timexes, as can be seen in Table 15, the origi-
nal TIPSem obtains better results for timex extraction,
which favours the hypothesis that machine learning sys-
tems are very well suited for this task (if the training data
is sufficiently representative). However, for normaliza-
tion (value F1), HeidelTime ? a rule-engineered system ?
obtains better results. This indicates that rule-based ap-
proaches have the upper hand in this task. TIPSem uses
class tense aspect
F1 P R F1 F1 F1
FSS-TimEx 59.0 90.3 43.9 24.6 - -
TIPSemB-F 90.2 92.5 88.0 58.6 39.7 38.1
TIPSem 88.2 90.6 85.8 58.7 84.9 78.7
Table 16: Task B: TempEval-2 test set (Spanish).
a partly data-driven normalization approach which, given
the small amount of training data available, seemed less
suited to the task.
Table 16 shows event extraction performance in TE2
test set. TIPSemB-F and TIPSem obtained a similar per-
formance. TIPSemB-F performed better in extraction and
TIPSem better in attribute classification.
7 Conclusion
In this paper, we described the TempEval-3 task within
the SemEval 2013 exercise. This task involves identify-
ing temporal expressions (timexes), events and their tem-
poral relations in text. In particular participating systems
were required to automatically annotate raw text using
TimeML annotation scheme
This is the first time end-to-end systems are evalu-
ated with a new single score (temporal awareness). In
TempEval-3 participants had to obtain temporal relations
from their own extracted timexes and events which is a
very challenging task and was the ultimate evaluation aim
of TempEval. It was proposed at TempEval-1 but has not
been carried out until this edition.
The newly-introduced silver data proved not so useful
for timex extraction or relation classification, but did help
with event extraction. The new single-measure helped to
rank systems easily.
Future work could investigate temporal annotation in
specific applications. Current annotations metrics evalu-
ate relations for entities in the same consecutive sentence.
For document-level understanding we need to understand
discourse and pragmatic information. Temporal question
answering-based evaluation (UzZaman et al, 2012a) can
help us to evaluate participants on document level tempo-
ral information understanding without creating any addi-
tional training data. Also, summarisation, machine trans-
lation, and information retrieval need temporal annota-
tion. Application-oriented challenges could further re-
search in these areas.
From a TimeML point of view, we still haven?t tack-
led subordinate relations (TimeML SLINKs), aspectual
relations (TimeML ALINKs), or temporal signal anno-
tation (Derczynski and Gaizauskas, 2011). The critical
questions of which links to annotate, and whether the cur-
rent set of temporal relation types are appropriate for lin-
guistic annotation, are still unanswered.
Acknowledgments
We thank the participants ? especially Steven Bethard,
Jannik Stro?tgen, Nate Chambers, Oleksandr Kolomiyets,
Michele Filannino, Philippe Muller and others ? who
helped us to improve TempEval-3 with their valuable
feedback. The third author also thanks Aarhus Univer-
sity, Denmark who kindly provided facilities.
8
References
J. F. Allen. 1983. Maintaining knowledge about temporal in-
tervals. Communications of the ACM, 26(11):832?843.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007. Timelines
from text: Identication of syntactic temporal relations. In
Proceedings of IEEE International Conference on Semantic
Computing.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva.
2013. Getting More Out of Biomedical Documents with
GATE?s Full Lifecycle Open Source Text Analytics. PLoS
computational biology, 9(2):e1002854.
L. Derczynski and R. Gaizauskas. 2010. Analysing Temporally
Annotated Corpora with CAVaT. In Proceedings of the 7th
International Conference on Language Resources and Eval-
uation, pages 398?404.
L. Derczynski and R. Gaizauskas. 2011. A Corpus-based Study
of Temporal Signals. In Proceedings of the 6th Corpus Lin-
guistics Conference.
L. Derczynski, H. Llorens, and N. UzZaman. 2013. TimeML-
strict: clarifying temporal annotation. CoRR, abs/1304.
G. Hripcsak and A. S. Rothschild. 2005. Agreement, the f-
measure, and reliability in information retrieval. Journal of
the American Medical Informatics Association, 12(3):296?
298.
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem (En-
glish and Spanish): Evaluating CRFs and Semantic Roles in
TempEval-2. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 284?291. Association
for Computational Linguistics.
H. Llorens, N. UzZaman, and J. Allen. 2012a. Merging Tem-
poral Annotations. In Proceedings of the TIME Conference.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2012b. Au-
tomatic system for identifying and categorizing temporal re-
lations in natural language. International Journal of Intelli-
gent Systems, 27(7):680?703.
H. Llorens, E. Saquete, and B. Navarro-Colorado. 2013. Ap-
plying Semantic Knowledge to the Automatic Processing of
Temporal Expressions and Events in Natural Language. In-
formation Processing & Management, 49(1):179?197.
L. Padro? and E. Stanilovsky. 2012. Freeling 3.0: Towards wider
multilinguality. In Proceedings of the Language Resources
and Evaluation Conference (LREC 2012), Istanbul, Turkey,
May. ELRA.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword Fifth Edition. LDC catalog ref.
LDC2011T07.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al
2003. The TimeBank corpus. In Corpus Linguistics.
J. Pustejovsky, B. Ingria, R. Saur??, J. Castano, J. Littman,
R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The
specification language TimeML. The Language of Time: A
reader, pages 545?557.
R. Saur?? and T. Badia. 2012. Spanish TimeBank 1.0. LDC
catalog ref. LDC2012T12.
R. Saur??, J. Littman, B. Knippen, R. Gaizauskas, A. Setzer, and
J. Pustejovsky. 2006. TimeML Annotation Guidelines Ver-
sion 1.2.1.
M. Taule?, M. A. Mart?, and M. Recasens. 2008. Ancora: Mul-
tilevel annotated corpora for catalan and spanish. In Pro-
ceedings of the 6th International Conference on Language
Resources and Evaluation (LREC-2008).
N. UzZaman and J. Allen. 2010. TRIPS and TRIOS system for
TempEval-2: Extracting temporal information from text. In
Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 276?283. Association for Computational
Linguistics.
N. UzZaman and J. Allen. 2011. Temporal Evaluation. In Pro-
ceedings of The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies.
N. UzZaman, H. Llorens, and J. Allen. 2012a. Evaluating tem-
poral information understanding with temporal question an-
swering. In Proceedings of IEEE International Conference
on Semantic Computing.
N. UzZaman, H. Llorens, J. F. Allen, L. Derczynski, M. Ver-
hagen, and J. Pustejovsky. 2012b. TempEval-3: Evaluating
Events, Time Expressions, and Temporal Relations. CoRR,
abs/1206.5333.
N. UzZaman. 2012. Interpreting the Temporal Aspects of Lan-
guage. Ph.D. thesis, University of Rochester, Rochester, NY.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
J. Moszkowicz, and J. Pustejovsky. 2009. The TempEval
challenge: identifying temporal relations in text. Language
Resources and Evaluation, 43(2):161?179.
M. Verhagen, R. Saur??, T. Caselli, and J. Pustejovsky. 2010.
SemEval-2010 task 13: TempEval-2. In Proceedings of the
5th International Workshop on Semantic Evaluation, pages
57?62. Association for Computational Linguistics.
9
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 184?185,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Medstract ? The Next Generation
Marc Verhagen
Computer Science Department
Brandeis University, Waltham, USA
marc@cs.brandeis.edu
James Pustejovsky
Computer Science Department
Brandeis University, Waltham, USA
jamesp@cs.brandeis.edu
Abstract
We present MedstractPlus, a resource for min-
ing relations from the Medline bibliographic
database. It was built on the remains of Med-
stract, a previously created resource that in-
cluded a bio-relation server and an acronym
database. MedstractPlus uses simple and scal-
able natural language processing modules to
structure text and is designed with reusability
and extendibility in mind.
1 Introduction
In the late 1990s, the Medstract project (Pustejovsky
et al, 2002) set out to use common Natural Lan-
guage Processing techniques and employ them to
access relational information in Medline abstracts.
Medstract used a set of pipelined Python scripts
where all scripts operated on in-memory objects.
The output of this pipeline was a set of relations,
indexed by the PubMed identifier of the abstract in
which they appeared. A Perl script proposed poten-
tial acronyms using a set of regular expressions on
named entities in Medline abstracts. Both relations
and acronyms were fed into an Oracle database,
where access to these datasources was enabled by
a set of Perl CGI scripts. The code, however, was
not made public and was not maintained in any se-
rious fashion after 2004. Developers of the system
dispersed over the world and the Medstract server
fatally crashed in 2007.
Here, we describe the resurrection of Medstract.
One goal was that code should be open source and
that installation should not depend on idiosyncra-
cies of the developer?s machine, which was a prob-
lem with the inherited code base. Reusability and
extendability are ensured by following the princi-
ples embodied in the Linguistic Annotation Format
(LAF) (Ide and Romary, 2006). In LAF, source data
are untouched, annotations are grouped in layers that
can refer to each other and to the source, and each
layer is required to be mappable to a graph-like pivot
format. For MedstractPlus, each component is set
up to be independent from other layers, although of
course each layer may need access to certain types
of information in order to create non-trivial output.
This allows us to swap in alternative modules, mak-
ing it easier to experiment with different versions of
the tagger and chunker for example. We now pro-
ceed to describe the system in section 2 and finish
with the current status and future work in section 3.
2 System Design and Implementation
The general design of MedstractPlus is presented in
Figure 1. The Lemmatizer creates what LAF calls
the base-segmentation, a first layer of tokenized text
that is the input to processing modules associated
with other layers. The Lemmatizer incorporates a
Python version of the Brill Tagger, extended with
entries from the UMLS Thesaurus.
The Semantic Tagger is a group of components
using (i) regular expressions for finding simple types
like URLs, (ii) dictionary lookup in the UMLS type
and concept lists as well as other typed word lists,
(iii) off-the-shelf components like the Abner gene
tagger (http://pages.cs.wisc.edu/ bsettles/abner/) and
(iv) a statistical disambiguation model for genes
trained on the GENIA corpus.
184
Medline Abstracts Lemmatizer
GENIA Corpus
Acronyms
UMLS
RelationsSemantic Types
Semantic Tagger
- regular expressions
- dictionary lookup
- Abner tagger
- disambiguation
Relation Extraction
- shallow parser
     three-level YACC parser
- argument linker
Web InterfacePubMed
Figure 1: Overview of the MedstractPlus Architecture
The Relation Extraction component now contains
a three-level 59-rule YACC parser that, starting with
simple low-level chunking of noun and verb groups,
proceeds to add more complex noun phrases and
subordinated phrases. The argument linker produces
binary relations, using a finite-state machine that
runs on the data created by the shallow parser.
An advantage of this data-driven approach is that
processing can be split up. A complete run of Med-
stractPlus on all Medline abstracts would take ap-
proximately 30 days on a entry-level desktop. But
some relatively stable components like the Lemma-
tizer and the shallow parser (the latter being the most
time-consuming component) can be run just once
and subsequent runs can be restricted to those com-
ponents that were changed.
The Web Interface gives access to the types and
relations in a fairly standard way. In its current pro-
totype form, it allows a user to type in a gene and
then view all relations that the gene participates in.
Alternatively, a pair of genes can be given.
3 Current Status and Future Work
The basic architecture depicted in Figure 1 is in
place, but some components like the type disam-
biguator are in embryonic form. The web inter-
face and the source code are or will be available at
http://medstractplus.org.
Extensive additions to the basic typing and re-
lation extraction component groups are in progress
and the Relation Extraction component can be ex-
tended with specialized rule sets for specific rela-
tions like inhibit or phosphorylate. The interaction
with the PubMed server is now limited to providing
links. But the plan is that the MedstractPlus server
will also query PubMed for relation pairs in case its
own database provides little information. This ap-
proach can be extended to other relation servers like
Chilibot (http://www.chilibot.net/), thereby moving
towards a system than presents merged relations
from the MedstractPlus database as well as relations
from other servers.
Acknowledgments
This work was supported by the National Institutes
of Health, under grant number 5R01NS057484-04.
References
Nancy Ide and Laurent Romary. 2006. Representing lin-
guistic corpora and their annotations. In Proceedings
of the Fifth Language Resources and Evaluation Con-
ference (LREC), Genoa, Italy.
James Pustejovsky, Jose? Castan?o, Roser Saur??, Anna
Rumshisky, Jason Zhang, and Wei Luo. 2002. Med-
stract: Creating large-scale information servers for
biomedical libraries. In Proceedings of ACL?02.
185
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 34?43,
Dublin, Ireland, August 23rd 2014.
The Language Application Grid Web Service Exchange Vocabulary
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
James Pustejovsky
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
jamesp@cs.brandeis.edu
Keith Suderman
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
suderman@anc.org
Marc Verhagen
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
marc@cs.brandeis.edu
Abstract
In the context of the Linguistic Applications (LAPPS) Grid project, we have undertaken the def-
inition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce lin-
guistically annotated data. The goal is not to define a new set of terms, but rather to provide a
single web location where terms relevant for exchange among NLP tools are defined and pro-
vide a ?sameAs? link to all known web-based definitions that correspond to them. The WS-EV
is intended to be used by a federation of six grids currently being formed but is usable by any
web service platform. Ultimately, the WS-EV could be used for data exchange among tools in
general, in addition to web services.
1 Introduction
There is clearly a demand within the community for some sort of standard for exchanging annotated lan-
guage data among tools.
1
This has become particularly urgent with the emergence of web services, which
has enabled the availability of language processing tools that can and should interact with one another,
in particular, by forming pipelines that can branch off in multiple directions to accomplish application-
specific processing. While some progress has been made toward enabling syntactic interoperability via
the development of standard representation formats (e.g., ISO LAF/GrAF (Ide and Suderman, 2014;
ISO-24612, 2012), NLP Interchange Format (NIF) (Hellmann et al., 2013), UIMA
2
Common Analysis
System (CAS)) which, if not identical, can be trivially mapped to one another, semantic interoperability
among NLP tools remains problematic (Ide and Pustejovsky, 2010). A few efforts to create repositories,
type systems, and ontologies of linguistic terms (e.g., ISOCat
3
, OLiA
4
, various repositories for UIMA
type systems
5
, GOLD
6
, NIF Core Ontology
7
) have been undertaken to enable (or provide) a mapping
among linguistic terms, but none has yet proven to include all requisite terms and relations or be easy
to use and reference. General repositories such as Dublin Core
8
, schema.org, and the Friend of a Friend
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
See, for example, proceedings of the recent LREC workshop on ?Language Technology Service Platforms: Synergies,
Standards, Sharing? (http://www.ilc.cnr.it/ltsp2014/).
2
https://uima.apache.org/
3
http://www.isocat.org
4
http://nachhalt.sfb632.uni-potsdam.de/owl/
5
E.g., http://www.julielab.de/Resources/Software/UIMA+type+system-p-91.html
6
http://linguistics-ontology.org
7
http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/nif-core
8
http://dublincore.org
34
project
9
include some relevant terms, but they are obviously not designed to fully cover the kinds of
information found in linguistically annotated data.
In the context of the Linguistic Applications (LAPPS) Grid project (Ide et al., 2014), we have under-
taken the definition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce linguistically
annotated data. The work is being done in collaboration with ISO TC37 SC4 WG1 in order to ensure
full community engagement and input. The goal is not to define a new set of terms, but rather to provide
a single web location where terms relevant for exchange among NLP tools are defined and provide a
?sameAs? link to all known web-based definitions that correspond to them. A second goal is to define
relations among the terms that can be used when linguistic data are exchanged. The WS-EV is intended
to be used by a federation of grids currently being formed, including the Kyoto Language Grid
10
, the
Language Grid Jakarta Operation Center
11
, the Xinjiang Language Grid, the Language Grid Bangkok
Operation Center
12
, LinguaGrid
13
, MetaNET/Panacea
14
, and LAPPS, but is usable by any web service
platform. Ultimately, the WS-EV could be used for data exchange among tools in general, in addition to
web services.
This paper describes the LAPPS WS-EV, which is currently under construction. We first describe the
LAPPS project and then overview the motivations and principles for developing the WS-EV. Because
our goal is to coordinate with as many similar projects and efforts as possible to avoid duplication, we
also describe existing collaborations and invite other interested groups to provide input.
2 The Language Application Grid Project
The Language Application (LAPPS) Grid project is in the process of establishing a framework that
enables language service discovery, composition, and reuse, in order to promote sustainability, manage-
ability, usability, and interoperability of natural language Processing (NLP) components. It is based on
the service-oriented architecture (SOA), a more recent, web- oriented version of the pipeline architecture
that has long been used in NLP for sequencing loosely-coupled linguistic analyses. The LAPPS Grid
provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA
and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide
general support for service discovery, composition, and reuse.
The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College,
Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and
is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in
the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability
and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope
and goals to ongoing projects such as The Language Grid
15
, PANACEA/MetaNET
16
, LinguaGrid
17
, and
CLARIN
18
, which also provide web service access to basic NLP processing tools and resources and
enable pipelining these tools to create custom NLP applications and composite services such as question
answering and machine translation, as well as access to language resources such as mono- and multi-
lingual corpora and lexicons that support NLP. The transformative aspect of the LAPPS Grid is therefore
not the provision of a suite of web services, but rather that it orchestrates access to and deployment of
language resources and processing functions available from servers around the globe, and enables users
to easily add their own language resources, services, and even service grids to satisfy their particular
needs.
9
http://www.foaf-project.org
10
http://langrid.nict
11
http://langrid.portal.cs.ui.ac.id/langrid/
12
http://langrid.servicegrid-bangkok.org
13
http://www.linguagrid.org/
14
http://www.panacea-lr.eu
15
http://langrid.nict
16
http://panacea-lr.eu/
17
http://www.linguagrid.org/
18
http://www.clarin.eu/
35
The most distinctive innovation in the LAPPS Grid that is not included in other projects is the provision
of an open advancement (OA) framework (Ferrucci et al., 2009a) for component- and application-based
evaluation of NLP tools and pipelines. The availability of this type of evaluation service will provide an
unprecedented tool for NLP development that could, in itself, take the field to a new level of productivity.
OA involves evaluating multiple possible solutions to a problem, consisting of different configurations
of component tools, resources, and evaluation data, to find the optimal solution among them, and en-
abling rapid identification of frequent error categories, together with an indication of which module(s)
and error type(s) have the greatest impact on overall performance. On this basis, enhancements and/or
modifications can be introduced with an eye toward achieving the largest possible reduction in error rate
(Ferrucci et al., 2009; Yang et al., 2013). OA was used in the development of IBM?s Watson to achieve
steady performance gains over the four years of its development (Ferrucci et al., 2010); more recently,
the open-source OAQA project has released software frameworks which provide general support for
open advancement (Garduno et al., 2013; Yang et al., 2013), which has been used to rapidly develop
information retrieval and question answering systems for bioinformatics (Yang et al., 2013; Patel et al.,
2013).
The fundamental system architecture of the LAPPS Grid is based on the Open Service Grid Initiative?s
Service Grid Server Software
19
developed by the National Institute of Information and Communications
Technology (NICT) in Japan and used to implement Kyoto University?s Language Grid, a service grid
that supports multilingual communication and collaboration. Like the Language Grid, the LAPPS Grid
provides three main functions: language service registration and deployment, language service search,
and language service composition and execution. As noted above, the LAPPS Grid is instrumented
to provide relevant component-level measures for standard metrics, given gold-standard test data; new
applications automatically include instrumentation for component-level and end-to-end measurement,
and intermediate (component-level) I/O is logged to support effective error analysis.
20
The LAPPS
Grid also implements a dynamic licensing system for handling license agreements on the fly
21
, provides
the option to run services locally with high-security technology to protect sensitive information where
required, and enables access to grids other than those based on the Service Grid technology.
We have adopted the JSON-based serialization for Linked Data (JSON-LD) to represent linguistically
annotated data for the purposes of web service exchange. The JavaScript Object Notation (JSON) is a
lightweight, text-based, language-independent data interchange format that defines a small set of format-
ting rules for the portable representation of structured data. Because it is based on the W3C Resource
Definition Framework (RDF), JSON-LD is trivially mappable to and from other graph-based formats
such as ISO LAF/GrAF and UIMA CAS, as well as a growing number of formats implementing the
same data model. Most importantly, JSON- LD enables services to reference categories and definitions
in web-based repositories and ontologies or any suitably defined concept at a given URI.
The LAPPS Grid currently supports SOAP services, with plans to support REST services in the
near future. We provide two APIs: org.lappsgrid.api.DataSource, which provides data
to other services, and org.lappsgrid.api.WebService, for tools that annotate, transform, or
otherwise manipulate data from a datasource or another web service. All LAPPS services exchange
org.lappsgrid.api.Data objects consisting of a discriminator (type) that indicates how to inter-
pret the payload, and a payload (typically a utf-8 string) that consists of the JSON-LD representation.
Data converters included in the LAPPS Grid Service Engines map from commonly used formats to the
JSON-LD interchange format; converters are automatically invoked as needed to meet the I/O require-
ments of pipelined services. Some LAPPS services are pre-wrapped to produce and consume JSON-LD.
Thus, JSON-LD provides syntactic interoperability among services in the LAPPS Grid; semantic inter-
19
http://servicegrid.net
20
Our current user interface provides easy (re-)configuration of single pipelines; we are currently extending the interface
to allow the user to specify an entire range of pipeline configurations using configuration descriptors (ECD; (Yang et al.,
2013) to define a space of possible pipelines, where each step might be achieved by multiple components or services and each
component or service may have configuration parameters with more than one possible value to be tested. The system will then
automatically generate metrics measurements plus variance and statistical significance calculations for each possible pipeline,
using a service-oriented version of the Configuration Space Exploration (CSE) algorithm (Yang et al., 2013).
21
See (Cieri et al., 2014) for a description of how licensing issues are handled in the LAPPS Grid.
36
operability is provided by the LAPPS Web Service Exchange Vocabulary, described in the next section.
3 LAPPS Web Service Exchange Vocabulary
3.1 Motivation
The WS-EV addresses a relatively small but critical piece of the overall LAPPS architecture: it allows
web services to communicate about the content they deliver, such that the meaning?i.e., exactly what
to do with and/or how to process the data?is understood by the receiver. As such it performs the same
function as a UIMA type system performs for tools in a UIMA pipeline that utilize that type system,
or the common annotation labels (e.g., ?Token?, ?Sentence?, etc.) required for communication among
pipelined tools in GATE: these mechanisms provide semantic interoperability among tools as long as one
remains in either the UIMA or GATE world. To pipeline a tool whose output follows GATE conventions
with a tool that expects input that complies with a given UIMA type system, some mapping of terms and
structures is likely to be required.
22
This is what the WS-EV is intended to enable; effectively, it is a
meta-type-system for mapping labels assigned to linguistically annotated data so that they are understood
and treated consistently by tools that exchange them in the course of executing a pipeline or workflow.
Since web services included in LAPPS and federated grids may use any i/o semantic conventions, the
WS-EV allows for communication among any of them?including, for example, between GATE and
UIMA services
23
The ability to pipeline components from diverse sources is critical to the implementation of the OA
development approach described in the previous section, it must be possible for the developer to ?plug
and play? individual tools, modules, and resources in order to rapidly re-configure and evaluate new
pipelines. These components may exist on any server across the globe, consist of modules developed
within frameworks such as UIMA and GATE, and or be user-defined services existing on a local machine.
3.2 WS-EV Design
The WS-EV was built around the following design principles, which were compiled based on input from
the community:
1. The WS-EV will not reinvent the wheel. Objects and features defined in the WS-EV will be linked
to definitions in existing repositories and ontologies wherever possible.
2. The WS-EV will be designed so as to allow for easy, one-to-one mapping from terms designating
linguistic objects and features commonly produced and consumed by NLP tools that are wrapped
as web services. It is not necessary for the mapping to be object-to-object or feature-to-feature.
3. The WS-EV will provide a core set of objects and features, on the principle that ?simpler is better?,
and provide for (principled) definition of additional objects and features beyond the core to represent
more specialized tool input and output.
4. The WS-EV is not LAPPS-specific; it will not be governed by the processing requirements or
preferences of particular tools, systems, or frameworks.
5. The WS-EV is intended to be used only for interchange among web services performing NLP tasks.
As such it can serve as a ?pivot? format to which user and tool-specific formats can be mapped.
6. The web service provider is responsible for providing wrappers that perform the mapping from
internally-used formats to and/or from the WS-EV.
7. The WS-EV format should be compact to facilitate the transfer of large datasets.
22
Within UIMA, the output of tools conforming to different type systems may themselves require conversion in order to be
used together.
23
Figure 5 shows a pipeline in which both GATE and UIMA services are called; GATE-to-GATE and UIMA-to-UIMA
communication does not use the WS-EV, but it is used for communication between GATE and UIMA services, as well as other
services.
37
8. The WS-EV format will be chosen to take advantage, to the extent possible, of existing technologi-
cal infrastructures and standards.
As noted in the first principle, where possible the objects and features in the WS-EV are drawn from
existing repositories such as ISOCat and the NIF Core Ontology and linked to them via the owl:sameAs
property
24
or, where appropriate, rdfs:subClassOf
25
. However, many repositories do not include some
categories and objects relevant for web service exchange (e.g., ?token? and other segment descriptors),
do include multiple (often very similar) definitions for the same concept, and/or do not specify relations
among terms. We therefore attempted to identify a set of (more or less) ?universal? concepts by surveying
existing type systems and schemas ? for example, the Julie Lab and DARPA GALE UIMA type systems
and the GATE schemas for linguistic phenomena ? together with the I/O requirements of commonly
used NLP software (e.g., the Stanford NLP tools, OpenNLP, etc.). Results of the survey for token and
sentence identification and part-of-speech labeling
26
showed that even for these basic categories, no
existing repository provides a suitable set of categories and relations.
Perhaps more problematically, sources that do specify relations among concepts, such as the various
UIMA type systems and GATE?s schemas, vary widely in their choices of what is an object and what
is a feature; for example, some treat ?token? as an object (label) and ?lemma? and ?POStag? as asso-
ciated features, while others regard ?lemma? and/or ?POStag? as objects in their own right. Decisions
concerning what is an object and what is a feature are for the most part arbitrary; no one scheme is right
or wrong, but a consistent organization is required for effective web service interchange. The WS-EV
therefore defines an organization of objects and features for the purposes of interchange only. Where
possible, the choices are principled, but they are otherwise arbitrary. The WS-EV includes sameAs and
similarTo mappings that link to like concepts in other repositories where possible, thus serving primar-
ily to group the terms and impose a structure of relations required for web service exchange in one
web-based location.
In addition to the principles above, the WS-EV is built on the principle of orthogonal design, such that
there is one and only one definition for each concept. It is also designed to be very lightweight and easy
to find and reference on the web. To that end we have established a straightforward web site (the Web
Service Exchange Vocabulary Repository
27
), similar to schema.org, in order to provide web-addressable
terms and definitions for reference from annotations exchanged among web services. Our approach is
bottom-up: we have adopted a minimalist strategy of adding objects and features to the repository only
as they are needed as services are added to the LAPPS Grid. Terms are organized in a shallow ontology,
with inheritance of properties, as shown in Figure 1.
4 WS-EV and JSON-LD
References in the JSON-LD representation used for interchange among LAPPS Grid web services point
to URIs providing definitions for specific linguistic categories in the WS-EV. They also reference doc-
umentation for processing software and rules for processes such as tokenization, entity recognition, etc.
used to produce a set of annotations, which are often left unspecified in annotated resources (see for
example (Fokkens et al., 2013)). While not required for web service exchange in the LAPPS Grid, the
inclusion of such references can contribute to the better replication and evaluation of results in the field.
Figure 3 shows the information for Token, which defines the concept, identifies application types that
produce objects of this type, cross-references a similar concept in ISOCat, and provides the URI for use
in the JSON-LD representation. It also specifies the common properties that can be specified for a set
of Token objects, and the individual properties that can be associated with a Token object. There is no
requirement to use any or all of the properties in the JSON-LD representation, and we foresee that many
web services will require definition of objects and properties not included in the WS-EVR or elsewhere.
24
http://www.w3.org/TR/2004/REC-owl-semantics-20040210/#owl sameAs
25
http://www.w3.org/TR/owl-ref/#subClassOf-def
26
Available at http://www.anc.org/LAPPS/EP/Meeting-2013-09-26-Pisa/ep-draft.pdf
27
http://vocab.lappsgrid.org
38
Figure 1: Fragment of the WS-EV ontology (associated properties in gray)
We therefore provide mechanisms for (principled) definition of objects and features beyond the WS-
EVR. Two options exist: users can provide a URI where a new term or other documentation is defined,
or users may add a definition to the WS-EVR. In the latter case, service providers use the name space
automatically assigned to them at the time of registration, thereby avoiding name clashes and providing
a distinction between general categories used across services and more idiosyncratic categories.
Figure 2 shows a fragment of the JSON-LD representation that references terms in the WS-
EV. The context statement at the top identifies the URI that is to be prefixed to any unknown
name in order to identify the location of its definition. For the purposes of the example, the
text to be processed is given inline. Our current implementation includes results from each step
in a pipeline, where applicable, together with metadata describing the service applied in each step
(here, org.anc.lapps.stanford.SATokenizer:1.4.0) and identified by an internally-defined type (stan-
ford). The annotations include references to the objects defined in the WS-EV, in this example, To-
ken (defined at http://vocab.lappsgrid.org/Token) with (inherited) features id, start, end and specific
feature string, defined at http://vocab.lappsgrid.org/Token#id, http://vocab.lappsgrid.org/Token#start,
http://vocab.lappsgrid.org/Token#end, and http://vocab.lappsgrid.orgToken/#string, respectively. The
web page defining these terms is shown in Figure 3.
"@context" : "http://vocab.lappsgrid.org/",
"metadata" : { },
"text" : {
"@value" : "Some of the strongest critics of our welfare system..." }
"steps" : [ {
"metadata" : {
"contains" : {
"Token" : {
"producer" : "org.anc.lapps.stanford.SATokenizer:1.4.0",
"type" : "stanford"
}
}
},
"annotations" : [ {
"@type" : "Token",
"id" : "tok0",
"start" : 18,
"end" : 22,
"features" : {
string" : "Some" }
},
Figure 2: JSON-LD fragment referencing the LAPPS Grid WS-EV
39
Figure 3: Token definition in the LAPPS WS-EVR
4.1 Mapping to JSON-LD
As noted above in Section 1, existing schemes and systems for organizing linguistic information ex-
changed by NLP tools vary considerably. Figure 4 shows some variants for a few commonly used NLP
tools, which differ in terminology, structure, and physical format. To be used in the LAPPS Grid, tools
such as those in the list are wrapped so that their output is in JSON-LD format, which provides syntactic
interoperability, terms are mapped to corresponding objects in the WS-EV, and the object-feature rela-
tions reflect those defined in the WS-EV. Correspondingly, wrappers transduce the JSON-LD/WS-EV
representation to the format used internally by the tool on input. This way, the tools use their internal
format as usual and map to JSON-LD/WS-EV for exchange only.
40
Name Input Form Output Form Example
Stanford tagger pt n/a word pos opl box NN1
XML n/a XML inline <word id=?0? pos=?VB?>Let</word>
NaCTeM tagger pt n/a word/pos inline box/NN1
CLAWS (1) pt n/a word pos inline box NN1
CLAWS (2) pt n/a XML inline <w id=?2? pos=?NN1?>Type</w>
CST Copenhagen pt n/a word/pos inline box/NN1
TreeTagger pt? n/a word pos lem opl The DT the
TnT token opl word pos opl der ART
word (pos pr)+ opl Falkenstein NE 8.00 NN 1.99
Twitter NLP pt opl word pos conf opl smh G 0.9406
NLTK pt s, bls [(?word?, ?pos?)] inline [(?At?, ?IN?), (?eight?, ?CD?),]
OpenNLP splitter pt n/a sentences ospl I can?t tell you if he?s here.
OpenNLP tokenizer sent ospl tokens wss, ospl I can ?t tell you if he ?s here .
OpenNLP tagger token wss, ospl word pos ospl At IN eight CD o?clock JJ on IN
pt = plain text opl = one per line wss = white space separated
ospl = one sentence per line bps = blank line separated
Figure 4: I/O variants for common splitters, tokenizers, and POS taggers
For example, the Stanford POS tagger XML output format produces output like this:
<word id="0" pos="VB">Let</word>
This maps to the following JSON-LD/WS-EV representation:
{
"@type" : "Token",
"id" : 0",
"start" : 18,
"end" : 21,
"features" : {
"string" : "Let",
"pos" : "VB"
}
}
The Stanford representation uses the term ?word? as an XML element name, gives an id and pos
as attribute-value pairs, and includes the string being annotated as element content. For conversion to
JSON-LD/WS-EV, ?word? is mapped to ?Token?, the attributes id and pos map to features of the Token
object with the same names, and the element content becomes the value of the string feature. Because
the JSON-LD representation uses standoff annotation, the attributes start and end are added in order to
provide the offset location of the string in the original data.
Services that share a format other than JSON-LD need not map into and out of JSON-LD/WS-EV
when pipelined in the LAPPS Grid. For example, two GATE services would exchange GATE XML
documents, and two UIMA services would exchange UIMA CAS, as usual. This avoids unnecessary
conversion and at the same time allows including services (consisting of individual tools or composite
workflows) from other frameworks. Figure 5 gives an example of the logical flow in the LAPPS Grid,
showing conversions into and out of JSON-LD/WS-EV where needed.
Each service in the LAPPS Grid is required to provide metadata that specifies what kind of input is
required and what kind of output is produced. For example, any service as depicted in the flow diagram
in Figure 5 can require input of a particular format (gate, uima, json-ld) with specific content (tokens,
sentences, etc.). The LAPPS Grid uses the notion of discriminators to encode these requirements, and
the pipeline composer can use these discriminators to determine if conversions are needed and/or input
requirements are met. The discriminators refer to elements of the vocabulary.
5 Collaborations
The LAPPS Grid project is collaborating with several other projects in an attempt to harmonize the
development of web service platforms, and ultimately to participate in a federation of grids and ser-
vice platforms throughout the world. Existing and potential projects across the globe are beginning to
41
Figure 5: Logical flow through the LAPPS Grid (client-server communication not represented)
converge on common data models, best practices, and standards, and the vision of a comprehensive in-
frastructure supporting discovery and deployment of web services that deliver language resources and
processing components is an increasingly achievable goal. Our vision is therefore not for a monolithic
grid, but rather a heterogeneous configuration of federated grids that implement common strategies for
managing and inter-changing linguistic information, so that services on all of these grids are mutually
accessible.
To this end, the LAPPS Grid project has established a multi-way international collaboration among the
US partners and institutions in Asia, Australia, and Europe. The basis is a formal federation among the
LAPPS Grid, the Language Grid (Kyoto University, Japan), NECTEC (Thailand), grids operated by the
University of Indonesia and Xinjiang University (China), and LinguaGrid
28
, scheduled for implementa-
tion in January 2015. The connection of these six grids into a single federated entity will enable access
to all services and resources on any of these grids by users of any one of them and, perhaps most impor-
tantly, facilitate adding additional grids and service platforms to the federation. Currently, the European
META-NET initiative is committed to joining the federation in the near future.
In addition to the projects listed above, we are also collaborating with several groups on technical
solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD
format, and a corollary development of an ontology of web service types. These collaborators include
the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project,
and the Lider project
29
. We actively seek collaboration with others in order to move closer to achieving
a ?global laboratory? for language applications.
6 Conclusion
In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WS-
EV), which provides a terminology for a core of linguistic objects and features exchanged among NLP
tools that consume and produce linguistically annotated data. The goal is to bring the field closer to
achieving semantic interoperability among NLP data, tools, and services. We are actively working to both
engage with existing projects and teams and leverage available resources to move toward convergence
of terminology in the field for the purposes of exchange, as well as promote an environment (the LAPPS
Grid) within which the WS-EV can help achieve these goals.
28
http://www.linguagrid.org/
29
http://www.lider-project.eu
42
Acknowledgements
This work was supported by National Science Foundation grants NSF-ACI 1147944 and NSF-ACI
1147912.
References
Steve Cassidy, Dominique Estival, Timothy Jones, Denis Burnham, and Jared Burghold. 2014. The Alveo Virtual
Laboratory: A Web based Repository API. In Proceedings of the Ninth International Conference on Language
Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association
(ELRA).
Christopher Cieri, Denise DiPersio, , and Jonathan Wright. 2014. Intellectual property rights management with
web services. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT,
Dublin, Ireland, August.
David Ferrucci, Eric Nyberg, James Allan, Ken Barker, Eric Brown, Jennifer Chu-Carroll, Arthur Ciccolo, Pablo
Duboue, James Fan, David Gondek, Eduard Hovy, Boris Katz, Adam Lally, Michael McCord, Paul Morarescu,
Bill Murdock, Bruce Porter, John Prager, Tomek Strzalkowski, Chris Welty, and Wlodek Zadrozny. 2009.
Towards the Open Advancement of Question Answering Systems. Technical report, IBM Research, Armonk,
New York.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010.
Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59?79.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring
from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691?1701, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Elmer Garduno, Zi Yang, Avner Maiberg, Collin McCormack, Yan Fang, and Eric Nyberg. 2013. CSE Frame-
work: A UIMA-based Distributed System for Configuration Space Exploration Unstructured Information Man-
agement Architecture. In Peter Klgl, Richard Eckart de Castilho, and Katrin Tomanek, editors, UIMA@GSCL,
CEUR Workshop Proceedings, pages 14?17. CEUR-WS.org.
Sebastian Hellmann, Jens Lehmann, S?oren Auer, and Martin Br?ummer. 2013. Integrating nlp using linked data.
In 12th International Semantic Web Conference, 21-25 October 2013, Sydney, Australia.
Nancy Ide and James Pustejovsky. 2010. What Does Interoperability Mean, Anyway? Toward an Operational
Definition of Interoperability. In Proceedings of the Second International Conference on Global Interoperability
for Language Resources. ICGL.
Nancy Ide and Keith Suderman. 2014. The Linguistic Annotation Framework: A Standard for Annotation Inter-
change and Merging. Language Resources and Evaluation.
Nancy Ide, James Pustejovsky, Nicoletta Calzolari, and Claudia Soria. 2009. The SILT and FlaReNet international
collaboration for interoperability. In Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP,
August.
Nancy Ide, James Pustejovsky, Christopher Cieri, Eric Nyberg, Di Wang, Keith Suderman, Marc Verhagen, and
Jonathan Wright. 2014. The language application grid. In Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources
Association (ELRA).
ISO-24612. 2012. Language Resource Management - Linguistic Annotation Framework. ISO 24612.
Alkesh Patel, Zi Yang, Eric Nyberg, and Teruko Mitamura. 2013. Building an optimal QA system automatically
using configuration space exploration for QA4MRE?13 tasks. In Proceedings of CLEF 2013.
Zi Yang, Elmer Garduno, Yan Fang, Avner Maiberg, Collin McCormack, and Eric Nyberg. 2013. Building optimal
information systems automatically: Configuration space exploration for biomedical information systems. In
Proceedings of the CIKM?13.
43
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 53?59,
Dublin, Ireland, August 23rd 2014.
A Conceptual Framework of Online Natural Language Processing
Pipeline Application
Chunqi Shi, Marc Verhagen, James Pustejovsky
Brandeis University
Waltham, United States
{shicq, jamesp, marc}@cs.brandeis.edu
Abstract
This paper describes a conceptual framework that enables online NLP pipelined applications to
solve various interoperability issues and data exchange problems between tools and platforms;
e.g., tokenizers and part-of-speech taggers from GATE, UIMA, or other platforms. We propose
a restful wrapping solution, which allows for universal resource identification for data manage-
ment, a unified interface for data exchange, and a light-weight serialization for data visualization.
In addition, we propose a semantic mapping-based pipeline composition, which allows experts
to interactively exchange data between heterogeneous components.
1 Introduction
The recent work on open infrastructures for human language technology (HLT) research and develop-
ment has stressed the important role that interoperability should play in developing Natural Language
Processing (NLP) pipelines. For example, GATE (Cunningham et al., 2002), UIMA (Ferrucci and Lally,
2004), and NLTK (Loper and Bird, 2002) all allow integrating components from different categories
based on common XML, or object-based (e.g., Java or Python) data presentation. The major categories
of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer,
Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution,
etc. Pipelined NLP applications can be built by composing several components; for example, a text
analysis application such as ?relationship analysis from medical records? can be composed by Sentence
Splitter, Tokenizer, POS Tagger, NER, and Coreference Resolution components.
In addition to interoperability, the very availability of a component can also play an important role in
building online application based on distributed components, especially in tasks such as online testing
and judging new NLP techniques by comparing to existing components. For example, the Language Grid
(Ishida, 2006) addresses issues relating to accessing components from different locations or providers
based on Service-Oriented Architecture (SOAs) models. In this paper, we explore structural, conceptual
interoperability, and availability issues, and provide a conceptual framework for building online pipelined
NLP applications.
The conventional view of structural interoperability is that a common set of data formats and com-
munication protocols should be specified by considering data management, data exchange, and data
visualization issues. Data management determines how to access, store and locate sources of data. For
example, GATE provides pluggable document readers or writers and XML (with meta-data configura-
tion) serialization of reusable objected-based data. UIMA provides document or database readers and
writers and XMI serialization of common object-based data structures. The Language Grid provides Java
object serialization of data collections. Data exchange strategies describe how components communi-
cate their data. For example, GATE provides CREOLE (Collection of REusable Objects for Language
Engineering) data collections for data exchange. UIMA provides CAS (Common Analysis Structure),
and NLTK provides API modules for each component type. Similarly, the Language Grid provides LSI
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
53
(Language Service Interface) for a concrete ontology for a given language infrastructure. Data visu-
alization facilitates manual reading, editing and adjudication. For example, GATE and UIMA provide
XML-based viewers for selection, searching, matching and comparison functionality.
The conventional view of conceptual interoperability is that expert knowledge should be used in bridg-
ing heterogeneous components. For example, GATE provides integration plugins for UIMA, OpenNLP,
and Stanford NLP, where experts have already engineered the specific knowledge on conversion strate-
gies among these components. This leaves open the question of how one would ensure the interoperable
pipelining of new or never-before-seen heterogeneous components, for which experts have not encoded
bridge protocols.
In order to achieve an open infrastructure of online pipelined applications, we will argue two points
regarding the conceptual design, considering both interoperability and availability:
? Universal resource identification, a SQL-like data management, and a light-weight data serialization
should be added with structural interoperability in online infrastructure of distributed components.
? By verifying and modifying inconsistent ontology mappings, experts can interactively learn con-
ceptual interoperability for online heterogeneous components pipelines.
2 Data, Tool and Knowledge Types
Interoperability in building pipelined NLP applications is intended ensure the exchange of information
between the different NLP tools. For this purpose, existing infrastructures like GATE or UIMA have
paid a lot of attention to common entity based data exchanges between the tools. When exchanging
data between heterogeneous tools (e.g., the GATE tokenizer pipelined with the NLTK POS tagger),
the knowledge of how these different entity based NLP tools can work together becomes much more
important, because there might be exchange problems between heterogeneous data or tool information,
and we may need specific knowledge to fix them. Thus, when considering interoperability, the main flow
of information should be exchanged in the open infrastructure consisting of source data information,
NLP tools information, and the knowledge that allows the tools to work together.
What are the main entity types of data and tools in designing an open infrastructure for online NLP
pipeline applications? From an abstract view of how linguistic analysis is related to human knowledge,
there are the following: Morphological, Lexical, Syntactic, Semantic, Pragmatic tool classifications; and
Utterance, Phoneme, Morpheme, Token, Syntactic Structure, Semantic Interpretation, and Pragmatic In-
terpretation data classifications. (Manaris, 1998; Pustejovsky and Stubbs, 2013). From a concrete appli-
cation perspective, where tools are available for concrete text mining for communities such as OpenNLP,
Stanford CoreNLP and NLTK, there are classification tools such as Sentence Splitter, Tokenizer, POS
Tagger, Phrase Chunker, Shallow Parser, NER, Lemmatizer, Coreference; and data classifications such
as Document, Sentence, Annotation, and Feature (Cunningham et al., 2002).
Le
xic
al
Sy
nta
ctic
Sem
ant
ic
Pra
gm
atic
Mo
rph
olo
gic
al
Figure 1: A NLP pipeline can be a (sub)-process of an abstract five-step process
POS
 Tagg
ing
Noun
-ph
rase
 
chun
king
Le
mm
atiza
tio
n
NER
Sen
ten
ce S
pli
tter
 
Token
izati
on
Core
fere
nce
Resol
utio
n
Figure 2: An example NLP pipeline of a concrete six-step process
54
The knowledge types needed for designing an open infrastructure also can be seen abstractly or con-
cretely. Abstractly, an NLP pipeline should be part of the process of morphological, lexical, syntactic,
semantic to pragmatic processing (see Figure 1). From a concrete view, each component of an NLP
pipeline should have any requisite preprocessing. For example, tokenization is required preprocessing
for POS tagging (see Figure 2). Such knowledge for building NLP pipelines can be interactively deter-
mined by the NLP expert or preset as built-in pipeline models.
Knowledge
Tool
D ata
Document Format, Structure, Style 
Morphological
(Splitter, Tokenizer , POS Tagger) 
Meta-
Information 
of 
Knowledge,
Tool, and 
Data. 
Lexical & Syntactic
(Lemmatization, Chunking, Parsing)
Semantic
(N ER, Coreference )
Pragmatic
(Indexing, Retrieval)
Knowledge of Tool Requirements, Data 
Interpretation
Figure 3: Information for NLP pipeline application description
We can put the above analyzed data, tool, and knowledge types with their meta-information together as
the information required for describing an NLP pipeline application (see Figure 3). Regarding the docu-
ment format, structure and style, for example, the Text Encoding Initiative (TEI)
1
provides one standard
for text encoding and interchange, which also enables meta-information description. Concerning the
main part (see dashdotted-line part of Figure 3), it is generally referred to as the model of annotation.
For example, GATE has its own single unified model of annotation, which is organized in annotation
graphs. The arcs in the graph have a start node and an end node, an identifier, a type and a set of
features (Bontcheva et al., 2004). One standardization effort (Ide and Romary, 2004), the Linguistic
Annotation Framework (LAF) architecture is designed so that a pivot format, such as GrAF (Ide and
Suderman, 2007), can bridge various annotation collections. Another standardization effort, the Syntac-
tic Annotation Framework (SynAF) (Declerck, 2006), has evolved into the Morpho-syntactic annotation
framework (MAF) (Declerck, 2008), which is based on the TEI and designed as the XML serialization
for morpho-syntactic annotations. A NLP processing middleware, the Heart of Gold, treats XML stand-
off annotations for natively XML support, and provides XSLT-based online integration mechanism of
various annotation collections (Sch?afer, 2006). The UIMA specifies a UML-based data model of anno-
tation, which also has a unified XML serialization (Hahn et al., 2007). Differently from Heart of Gold?s
XSLT-based mechanism, the conversion tools that bridge GATE annotation and UIMA annotation use
GrAF as a pivot and are provided as GATE plugins and UIMA modules (Ide and Suderman, 2009).
Thus, while a pivot standard annotation model like GrAF seems very promising, popular annotation
models like those provided by GATE annotations (see Figure 4) or UIMA annotations (see Figure 4)
will continue to exist and evolve for a long time. As a result, more bridge strategies, like the conversion
plugin (module) of GATE (UIMA) and the XSLT-based middleware mechanism, will continue to be nec-
essary. In the following sections, we consider the issue of the continuing availability of such conversion
functions, and whether the current realization of those two conversion strategies is sufficient to bridge the
various annotations made available by linguistic experts, without further substantial engineering work.
3 Towards A Conceptual Design of Online Infrastructure
In this section, we discuss the conceptual design of online infrastructure, focusing on both the interop-
erability and availability of the tools. Concerning the latter, the Service-oriented architecture (SOA) is
1
http://www.tei-c.org/
55
<!-- GATE -->
<GateDocument>
<TextWithNodes>
<Node id="15"/>Sonnet<Node id="21"/>
</TextWithNodes>
<AnnotationSet>
<Annotation Id="18" Type="Token"
StartNode="15" EndNode="21">
<Feature>
<Name className="java.lang.String">length</Name>
<Value className="java.lang.String">6</Value>
</Feature>
<Feature>
<Name className="java.lang.String">category</Name>
<Value className="java.lang.String">NNP</Value>
</Feature>
<Feature>
<Name className="java.lang.String">kind</Name>
<Value className="java.lang.String">word</Value>
</Feature>
<Feature>
<Name className="java.lang.String">string</Name>
<Value className="java.lang.String">Sonnet</Value>
</Feature>
</Annotation>
</AnnotationSet>
</GateDocument>
<!-- UIMA -->
<xmi:XMI
xmlns:xmi="http://www.omg.org/XMI"
xmlns:opennlp=
"http:///org/apache/uima/examples/opennlp.ecore"
xmlns:cas="http:///uima/cas.ecore"
xmi:version="2.0">
<cas:Sofa
xmlns:cas="http:///uima/cas.ecore"
xmi:id="1" sofaNum="1" sofaID="_InitialView"
mimetype="text"
sofaString="Sonnet." />
<opennlp:Token
xmi:id="18" sofa="1"
begin="0" end="6"
posTag="NNP" />
<cas:View sofa="1"
members="18"/>
</xmi:XMI>
Figure 4: Examples of GATE XML annotation and UIMA XML annotation
a promising approach. For example, while the Language Grid infrastructure makes NLP tools highly
available (Ishida, 2006), it can still have limitations regarding interoperability issues. Generally, service
interfaces can be either operation-oriented which allows flexible operations with simple input/output
data, or resource-oriented which allows flexible input/output data with simple operations. The NLP
processing services of Language Grid are more or less operation-oriented, and lack a certain structural
flexibility for composing with each other. We present a resource-oriented view of NLP tools, which
should have universal resource identification for distributed reference, an SQL-like data management,
and a light-weight data serialization for online visualization. We propose Restful wrapping both data and
tools into Web services for this purpose.
Restful wrapping makes both data and tools easy-to-access and with a unified interface, enabling
structural interoperability between heterogeneous tools, assuming standoff annotation from various NLP
tools is applied. For example, if the NLP tools are wrapped into Restful services so that they are operated
through HTTP GET protocol, and the XML serialization of UIMA annotation is applied for input and
output, each NLP components will have the same interface and data structure.
Once an internationalized resource identifier (IRI) is given, all the input and output of tools can be
distributed and ubiquitously identified. Moreover, a PUT/GET/POST/DELETE protocol of restful data
management is equivalent to an SQL-like CRUD data management interface. For example, an IRI can
be defined by a location identifier and the URL of the data service (Wright, 2014).
In addition, a lightweight serialization of stand-off annotation can benefit the online visualization of
data, which will be easy for experts to read, judge, or edit. For example, the XML serialization of UIMA
annotation can be transferred into JSON serialization, which is preferred for online reading or editing.
NLP tool services will be available by applying restful wrapping (see Figure 5). However, structural
interoperability based on the restful wrapping is not enough for conceptual interoperability. For example,
if an OpenNLP tokenizer is wrapped using HTTP GET protocol and GATE annotation, but a Stanford
NLP POS tagger is wrapped using UIMA annotation, it will raise conceptual interoperability issues.
Based on the previously mentioned bridging strategies, a conversion service from GATE annotation to
UIMA annotation should work, or a transformation interaction with a XSLT-like service should work.
We would like to assume that the interaction and contribution of linguistic experts without online support
by engineers can solve this issue. But how can we design the interaction to take advantage of such expert
knowledge?
We present a semantic mapping-based composer for building an NLP pipeline application (see Fig-
56
NLP Pipeline Application
NLP Tool Service
Source Data
(Document, Database)
NLP Tool
( OpenNLP, Standard 
NLP,  NLTK, etc )
Restful Wrapping
1. International resource identifier (IRI) ID
2. Unified interface, GET/PUT/POST/DELETE
3. Self-description message like XML or JSON
Meta-Information
( Provider, License, 
Location)
Semantic Mapping based Composing
1. NLP tool service pipeline engine
2. Proxy service of interactive ontology mapping
Workflow Engine
( BPEL )
Stand-off Ontology
(Vocabulary) 
Meta-Information
(Process Requirements)
Figure 5: Conceptual design of online NLP pipeline application
ure 5). Conceptual interoperability requires the same vocabularies for the same concept of a standoff
annotation. Once we have the standoff ontology of annotation, we can perform automatic semantic map-
ping from NLP tool output to that ontology. The interaction from experts will be triggered once the
automatic semantic mapping has failed (see Figure 6). For example, both GATE and UIMA XML an-
notations could be transformed into JSON formation, which is easy to present as tree structure entities.
Based on these tree structure entities, automatic ontology mapping tools like UFOme, which identifies
correspondences among entities in different ontologies (Pirr?o and Talia, 2010), can be applied to build
up various mapping solutions. Knowledge from experts can also be applied interactively, and successful
mapping solutions can be stored for further reference and use.
<! -- GATE -- >
< GateDocument >
< TextWithNodes > 
<Node id="1 5 " />Sonnet<Node id="2 1 " />
</ TextWithNodes >
< AnnotationSet>
<Annotation Id=" 1 8 " Type="Token" 
StartNode ="1 5 " EndNode ="2 1 " >
<Feature>
<Name clas sName =" java.lang.String ">length</Name>
<Value clas sName =" java.lang.String ">6 </Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">category</Name>
<Value clas sName =" java.lang.String ">NNP</Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">kind</Name>
<Value clas sName =" java.lang.String ">word</Value>
</Feature>
<Feature>
<Name clas sName =" java.lang.String ">string</Name>
<Value clas sName =" java.lang.String ">Sonnet</Value>
</Feature>
</Annotation>
</ AnnotationSet>
</ GateDocument >
< ! - - UIMA -- >
< xmi:XM I
xmlns:xmi ="http://www.omg.org/XM I" 
xmlns:opennlp =
"http:///org/apache/uima/examples/opennlp.ecore" 
xmlns:cas ="http:/// uima/cas.ecore"
xmi:version ="2. 0">
< cas:Sofa
xmlns:cas ="http:/// uima/cas.ecore" 
xmi:id ="1" sofaNum ="1" sofaID ="_ InitialView " 
mimetype ="text" 
sofaString ="Sonnet." />
< opennlp:Token
xmi:id ="18"  sofa="1" 
begin="0" end="6" 
posTag ="NNP" />
< cas:View sofa="1" 
members="18"/ >
< / xmi:XM I >
Ontology 
Mapping
Vocabulary
Mapping 
Storage
@Id
@ StartNode
@ EndNode
Feature
Annotation
Length
Category
String
@id
@sofa
@begin
@end
Token
@ posTag
@Type
GATE Annotation
UIMA Annotation
Figure 6: Interactive ontology mapping of two different annotations of NLP tools (Tree structures are
learned from XML annotations in Figure 4 )
The semantic mapping will be interactively created by the experts, when heterogeneous components
with different data models are used in the NLP pipeline created by the end-users, who create the NLP
pipeline without consideration of components interoperability. It means that this semi-automatically
created semantic mapping separates acquiring the knowledge of tool requirements from end-users and
acquiring the knowledge of data interpretation from experts (see Figure 3). For example, the end-users
chooses two POS Taggers (OpenNLP and NLTK) and two NER tools (OpenNLP and Stanford NLP)
components in the NLP application of ?relationship analysis from medical records?. When NLTK POS
57
Tagger output are serialized in to JSON formats but cannot be directly used as the input of Stanford NLP
NER component which requires the UIMA annotation, a semantic mapping issue will be automatically
created and reported to experts. This NLTK POS Tagger JSON format output will be mapped into
the standoff ontology of annotation of POS Tagger. After that, this output will bridge with the UIMA
annotation of the Stanford NLP NER. This particular semantic mapping between JSON serialization of
a NLTK POS Tagger and the standoff ontology of annotation of POS Tagger, and between the standoff
ontology of annotation of POS Tagger and the UIMA annotation of Stanford NLP NER will be reused in
the NLP application created by other end-users.
Our conceptual framework does not exclusively rely on the above interoperability design. Our con-
ceptual framework (see Figure 5) should integrate existing knowledge of various annotation frameworks,
for example, the alignment knowledge from the Open Annotation models (Verspoor and Livingston,
2012) and the pivot bridge knowledge from the GrAF (Ide and Suderman, 2007) under the Linguistic
Annotation Framework (LAF). Thus, existing pivot conversion solutions and XSLT-based middleware
solutions can also be applied. Our interactive ontology mapping design provides a more flexible choice
for linguistic experts to build up NLP pipeline applications on top of heterogeneous components, without
online help from engineers. Below we present varying levels of online NLP applications, according to
what kind of extra support would be needed for composing different NLP components:
? Components are interoperable without extra data exchange issues. For example, tools are from the
same community (e.g., only using OpenNLP tools).
? Components are interoperable with existing solutions of data exchange issues. For example, tools
are from popular communities such as GATE plugins or UIMA modules.
? Components are interoperable with extra knowledge from experts. For example, tools are both from
popular communities and personal developments or inner group software.
? Components are interoperable with considerable effort from both experts and engineers. For exam-
ple, tools are developed under novel ontology designs.
According to these levels, our conceptual framework is targeted at the third level of interoperability
issues. Our proposal will generate a ontology mapping storage (see Figure 6), which we hope will
contribute to improving a standard annotation ontology.
4 Conclusion
In this paper, we have tried to present a conceptual framework for building online NLP pipeline applica-
tions. We have argued that restful wrapping based on the Service-Oriented Architecture and a semantic
mapping based pipeline composition benefit both the availability and interoperability of online pipeline
applications. By looking at the information surrounding the data, tools, and knowledge needed for NLP
components pipelines, we explained how experts can be limited in building online NLP pipeline applica-
tions without help from engineers, and our restful wrapping and interactive ontology mapping design can
help in such situations. Finally, we have described various levels of support needed for building online
NLP pipelines, and we believe that this study can contribute to further online implementations of NLP
applications.
Acknowledgements
This work was supported by National Science Foundation grants NSF-ACI 1147944.
References
Kalina Bontcheva, Valentin Tablan, Diana Maynard, and Hamish Cunningham. 2004. Evolving gate to meet new
challenges in language engineering. Nat. Lang. Eng., 10(3-4):349?373, September.
58
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A Framework
and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Computational Linguistics (ACL?02).
Thierry Declerck. 2006. Synaf: Towards a standard for syntactic annotation. In Proceedings of the Fifth In-
ternational Conference on Language Resources and Evaluation (LREC?06). European Language Resources
Association (ELRA).
Thierry Declerck. 2008. A framework for standardized syntactic annotation. In Bente Maegaard Joseph
Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair), Khalid Choukri,
editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
David Ferrucci and Adam Lally. 2004. Uima: An architectural approach to unstructured information processing
in the corporate research environment. Nat. Lang. Eng., 10(3-4):327?348, September.
Udo Hahn, Ekaterina Buyko, Katrin Tomanek, Scott Piao, John McNaught, Yoshimasa Tsuruoka, and Sophia
Ananiadou. 2007. An annotation type system for a data-driven nlp pipeline. In Proceedings of the Linguistic
Annotation Workshop, LAW ?07, pages 33?40, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Nancy Ide and Laurent Romary. 2004. International standard for a linguistic annotation framework. Nat. Lang.
Eng., 10(3-4):211?225, September.
Nancy Ide and Keith Suderman. 2007. Graf: A graph-based format for linguistic annotations. In Proceedings of
the Linguistic Annotation Workshop, LAW ?07, pages 1?8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nancy Ide and Keith Suderman. 2009. Bridging the gaps: Interoperability for graf, gate, and uima. In Pro-
ceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP ?09, pages 27?34, Stroudsburg, PA, USA.
Association for Computational Linguistics.
T. Ishida. 2006. Language grid: an infrastructure for intercultural collaboration. In Applications and the Internet,
2006. SAINT 2006. International Symposium on, pages 5 pp.?100, Jan.
Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. In Proceedings of the ACL-02 Work-
shop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational
Linguistics - Volume 1, ETMTNLP ?02, pages 63?70, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bill Manaris. 1998. Natural language processing: A human-computer interaction perspective.
Giuseppe Pirr?o and Domenico Talia. 2010. Ufome: An ontology mapping system with strategy prediction capa-
bilities. Data Knowl. Eng., 69(5):444?471, May.
James Pustejovsky and Amber Stubbs. 2013. Natural language annotation for machine learning. O?Reilly Media,
Sebastopol, CA.
Ulrich Sch?afer. 2006. Middleware for creating and combining multi-dimensional nlp markup. In Proceedings of
the 5th Workshop on NLP and XML: Multi-Dimensional Markup in Natural Language Processing, NLPXML
?06, pages 81?84, Stroudsburg, PA, USA. Association for Computational Linguistics.
Karin Verspoor and Kevin Livingston. 2012. Towards adaptation of linguistic annotations to scholarly annotation
formalisms on the semantic web. In Proceedings of the Sixth Linguistic Annotation Workshop, LAW VI ?12,
pages 75?84, Stroudsburg, PA, USA. Association for Computational Linguistics.
Jonathan Wright. 2014. Restful annotation and efficient collaboration. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources
and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA).
59
Proceedings of SADAATL 2014, pages 31?39,
Dublin, Ireland, August 24, 2014.
Extracting Aspects and Polarity from Patents 
 Peter Anick, Marc Verhagen and James Pustejovsky 
Computer Science Department 
Brandeis University 
Waltham, MA, United States 
Peter_anick@yahoo.com, marc@cs.brandeis.edu, 
jamesp@cs.brandeis.edu 
 
Abstract 
We describe an approach to terminology extraction from patent corpora that follows from a view of pa-
tents as ?positive reviews? of inventions.  As in aspect-based sentiment analysis, we focus on identify-
ing not only the components of products but also the attributes and tasks which, in the case of patents, 
serve to justify an invention?s utility.  These semantic roles (component, task, attribute) can serve as a 
high level ontology for categorizing domain terminology, within which the positive/negative polarity of 
attributes serves to identify technical goals and obstacles.  We show that bootstrapping using a very 
small set of domain-independent lexico-syntactic features may be sufficient for constructing domain-
specific classifiers capable of assigning semantic roles and polarity to terms in domains as diverse as 
computer science and health. 
1 Introduction 
Automated data mining of patents has had a long history of research, driven by the large volume of 
patents produced each year and the many tasks to which they are put to use, including prior art inves-
tigation, competitive analysis, and trend detection and forecasting (Tseng, 2007).  Much of this work 
has concentrated on bibliographic methods such as citation analysis, but text mining has also been 
widely explored as a way to assist analysts to characterize patents, discover relationships, and facilitate 
patent searches.  One of the indicators of new technology emergence is the coinage, adoption and 
spread of new terms; hence the identification and tracking of technical terminology over time is of par-
ticular interest to researchers designing tools to support analysts engaged in technology forecasting 
(e.g., Woon, 2009; deMiranda, 2006) 
For the most part, research into terminology extraction has either (1) focused on the identification of 
keywords within individual patents or corpora without regard to the roles played by the keywords 
within the text (e.g., Sheremetyeva, 2009) or, (2) engaged in fine-grained analysis of the semantics of 
narrow domains (e.g., Yang, 2008).  In this paper we strive towards a middle ground, using a high-
level classification suitable for all domains, inspired in part by recent work on sentiment analysis (Liu, 
2012). In aspect-based sentiment analysis, natural language reviews of specific target entities, such as 
restaurants or cameras, are analyzed to extract aspects, i.e., features of the target entities, along with 
the sentiment expressed toward those features.  In the restaurant domain, for example, aspects might 
include the breadth of the menu, quality of the service, preparation of the food, and cost. Aspects thus 
tend to capture the tasks that the entity is expected to perform and various dimensions and components 
related to those tasks.  Sentiment reflects the reviewer?s assessment of these aspects on a scale from 
negative to positive.   
A patent application is required by definition to do three things: describe an invention, argue for its 
novelty, and justify its utility.  The utility of a patent is typically defined by the accomplishment of a 
new task or an improvement to some existing task along one or more dimensions.  Thus, a patent can 
be thought of as a positive review of a product with respect to specific aspects of its task(s).  Indeed, 
the most commonly occurring verbs in patents include those indicative of components (?comprise?, 
?include?), attributes (?increase?, ?reduce?), and tasks (?achieve?, ?perform?).   Organizing keywords 
along these high-level distinctions, then, would allow patent analysts to explore terminological infor-
This work is licensed under a Creative Commons Attribution 4.0 International License.  Page numbers and proceedings foot-
er are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
31
mation from several different relevant perspectives.  Furthermore, given the interpretation of a patent 
as a positive review, it should be possible to identify the default polarity of measurable aspects in the 
context of a domain.  For example, if a patent makes a reference to increasing network bandwidth, 
then this should lend support to the notion that network bandwidth is not only a relevant attribute with-
in the patent?s domain but also a positive one.  Likewise, if a patent refers to reducing power con-
sumption, then we might interpret power consumption as an aspect with negative polarity.  For ana-
lysts trying to assess trends within a technology domain, tracking the occurrences of terms signifying 
tasks and attributes, along with their polarity, could help them characterize the changing goals and ob-
stacles for inventors over time. 
The US patent office receives over half a million patent applications a year.1  These are classified by 
subject matter within several standardized hierarchical schemes, which permits dividing up the corpus 
of patents both by application date and subfield (e.g., computer science, health, chemistry).  Since our 
goal is to support analysts across all domains, it is highly desirable to extract domain-specific aspects 
through semi-supervised machine learning rather than incur the cost of domain-specific knowledge 
engineering.  To this end, we employed a bootstrapping approach in which a small number of domain 
independent features was used to generate a much larger number of domain dependent features for 
classification.  We then applied na?ve Bayes classification in a two-step classification process: first 
distinguishing attributes, components and tasks; and then classifying the extracted attribute terms by 
their polarity. 
The paper is structured as follows.  In section 2, we describe the system architecture.  Section 3 
shows results for two domains (computer science and health).  In section 4, we present an evaluation 
of results and discuss issues and shortcomings of the current implementation.  In section 5, we present 
related research and in section 6, our conclusions and directions for future work. 
2 System architecture 
2.1 Corpus processing 
Our patent collection is a set of 7,101,711 US patents in XML-markup form from Lexis-Nexis.  We 
divided the collection into subcorpora by application year and high-level domain using the patents? 
classification within the USPTO hierarchy.  The XML markup was then used to extract the relevant 
portions of patents for further analysis.  These sections included title, abstract, background, summary, 
description and claims. References, other than those embedded in the sections above, were omitted, as 
they contain many entity types (people, publications, and organizations) that are not particularly useful 
for our current task.  The text of each section was extracted and broken into sentences by the Stanford 
tagger (Toutanova, 2003) which also tokenized and tagged each token with a part of speech tag.   
We then chunked adjacent tokens into simple noun phrase chunks of the form (ADJECTIVE)? 
(NOUN)* NOUN.2  We will hereafter refer to these chunks as terms.  The majority of these patent 
terms fall into one of three major categories:  
Components: the physical constituents or processes that make up an invention, as well as the ob-
jects impacted, produced by or used in the invention.   
Tasks: the activities which inventions, their components or beneficiaries perform or undergo.   
Attributes: the measureable dimensions of tasks and components mentioned in the patent. 
 
To generate features suitable for machine learning of these semantic categories, we used a small set 
of lexico-syntactic relationships, each defined with respect to the location of the term in a sentence: 
prev_V: the closest token tagged as a verb appearing to the left of the term, along with any preposi-
tions or particles in between.  (cached_in, prioritizing, deal_with) 
prev_VNpr: a construction of the form <verb><NP><prep> appearing to the left of the term.  Only 
the head noun in the NP is retained (inform|user|of, provides|list|of, causes|increase|in) 
prev_Npr: a construction of the form <noun><prep> appearing to the left of the term. (re-
striction_on, applicability_of, time_with) 
                                                 
1 http://www.uspto.gov/web/offices/ac/ido/oeip/taf/us_stat.htm 
2 We blocked a set of 246 general adjectival modifiers (e.g., other, suitable, preferred, entire, initial,?) from participating in 
terms. 
32
prev_Jpr: a construction of form <adjective> <prep> appearing to the left of the term. (free_from, 
desirable_in, unfamiliar_with) 
prev_J: a construction of form <adjective> <prep> appearing to the left of the term. (excessive, con-
siderable, easy) 
 
These features were designed to capture specific dependency relations between the term and its pre-
modifiers and dominant verbs, nouns, and adjective phrases.  We extracted the features using localized 
rules rather than create a full dependency parse.3  One additional feature internal to the term itself was 
also included: last_word.  This simply captured the head term of the noun phrase, which often carries 
generalizable semantic information about the phrase.  Each feature instance was represented as a string 
comprising a prefix (the feature type) and its value (a token or concatenation of tokens).   
 
2.2 Classification 
 
For each term appearing in a subcorpus, the collection of co-occurring features across all documents 
was assembled into a single weighted feature vector in which the weight captured the number of doc-
uments for which the feature occurred in conjunction with the given term.  We also calculated the 
document frequency for each term, as well as its ?domain specificity score?, a metric reflecting the 
relative frequency of the term in specialized vs. randomized corpora (see section 3).   
In order to avoid the need to create manually labeled training data for each patent domain, we em-
ployed bootstrapping, a form of semi-supervised learning in which a small number of labeled features 
or seed terms are used in an iterative fashion to automaticaly identify other likely diagnostic features 
or category exemplars.  Bootstrapping approaches have previously shown considerable promise in the 
construction of semantic lexicons (Riloff, 1999; Thelen, 2002, Ziering, 2013).  By surveying common 
prev_V features in a domain-independent patent subcorpus, we selected a small set of domain-
independent diagnostic lexico-syntactic features (?seed features?) that we felt were strong indicators 
for each of the three semantic categories.  The set of seed features for each category is shown below.  
Semantically equivalent inflectional variants were also included as features.   
 
Attribute: improve, optimize, increase, decrease, reduce 
Component: comprise, contain, encompass, incorporate, use, utilize, consist_of, assembled_of, com-
posed_of 
Task: accomplish, achieve, enhance, facilitate, assisting_in, employed_in, encounter_in, perform, 
used_for, utilized_for 
 
We then utilized these manually labeled generic features to bootstrap larger feature sets F for do-
main-specific subcorpora.  For each term t in a domain-specific subcorpus, we extracted all the manu-
ally labeled features that the term co-ocurred with. Any term which co-occurred with at least two la-
beled feature instances and for which all of its labeled features were of the same class was itself la-
beled with that class for subsequent use as a seed term s for estimating the parameters of a multinomial 
na?ve Bayes classifier (Manning et al, 2008).  Each seed term so selected was represented as a bag of 
its co-occurring features.   
 
The prior probability of each class and conditional probabilities of each feature given the class were 
estimated as follows, using Laplace ?add one? smoothing to eliminate 0 probabilities: 
 
 ?(  )   
      
     
 
 
 ?(   )   
     (   )   
     ( )       
 
                                                 
3 The compute time required to produce dependency parses for the quantity of data to be analyzed led to the choice of a 
?leaner? feature extraction method. 
33
 
where    is the set of seed terms with class label j, S is the set of all seed terms, count(f,c) is the count 
of co-occurrences of feature f with seed terms in class c, count(c) is the total number of feature co-
occurrences with seed terms in class c, and F is the set of all features (used for Laplace smoothing).  
Using the na?ve Bayes conditional independence assumption, the class of each term in a subcorpus 
was then computed by maximizing the product of the prior probability for a class and the product of 
the conditional probabilities of the term?s features: 
         
    
 ( ) ? (   )
   
 
 
Terms for which no diagnostic features existed were labeled as ?unknown?.   
Once the terms in a subcorpus were categorized as attribute, component, or task, the terms identi-
fied as attributes were selected as input to a second round of classification.4  We used the same boot-
strapping process as described for the first round, choosing a small set of features highly diagnostic of 
the polarity of attributes.  For positive polarity, the seed features were: increase, raise, maximize. For 
negative polarity: avoid, lower, decrease, deal_with, eliminate, minimize, reduce, resulting_from, 
caused_by.  Based on co-occurrence with these features, a set of terms was produced from which pa-
rameters for a larger set of features could be estimated, as described above.  We then used na?ve Bayes 
classification to label the full set of attribute terms. 
3 Results 
We present results from two domains, health and computer science, using a corpus consisting of all 
US patent applications submitted in the year 2002. The health subcorpus consisted of 19,800 docu-
ments, while the computer science subcorpus contained 51,058 documents.  A ?generic? corpus com-
posed of 38,482 patents randomly selected from all domains was also constructed for the year for use 
in computing a ?domain specificity score?.  This score was designed to measure the degree to which a 
term could be considered part of a specific domain?s vocabulary and was computed as the 
log(probability of term in domain corpus / probability of term in generic corpus).  For example, in 
computer science, the term encryption technology earned a domain specificity score of 4.132, while 
speed earned .783 and color garnered .022.  Using a combination of term frequency (# of documents a 
term occurs in within a domain) thresholds and domain specificity, one can extract subsets of terms 
with varying degrees of relevance within a collection.5 
 
3.1 Attribute/Component/Task (ACT) Classification 
The bootstrapping process generated 1,644 features for use in the health domain and 3,200 in com-
puter science. Kullback-Leibler divergence is a commonly used metric for comparing the difference 
between two probability distributions (Kullback and Leibler, 1951).  By computing Kullback-Leibler 
divergence    (    ) between the distribution P of classes predicted by each feature (i.e., the proba-
bility of the class given the feature alone based on the term seed set labels) and the prior class distribu-
tion Q, we could estimate the impact of individual features in the model.  Table 1 shows some of the 
domain-specific features in the health and computer science domains, along with the category each 
tended to select for.6   
Using the features generated by bootstrapping, the classifier was able to label 61% of the 1,335,240 
terms in health and 81% of the 1,391,402 terms in computer science.  The majority of unlabeled terms 
were extremely low frequency (typically 1).  Higher frequency unlabeled terms were typically from 
categories other than those under consideration here (e.g., john wiley, j. biochem, 2nd edition).  The 
distribution of category labels for the health and computer domains is shown in Table 2. 
                                                 
4 We found relatively little evidence of explicit sentiment targeted at component and task aspects in patents and therefore 
focused our polarity analysis on attributes.  
5 Similar to Velardi?s use of ?domain relevance? and ?consensus? (Velardi, 2001). 
6 Although it is possible to use KL-Divergence for feature selection, it is applied here solely for diagnostic purposes to verify 
that feature distributions match our intuitions with respect to the classification scheme. 
34
 
 
Table 1.  Features highly associated with classes (a[ttribute], c[omponent], t[ask]) in the health and com-
puter science domains, along with an example of a term co-occurring with each feature in some patent. 
Health                                                                              Computer Science 
Feature Class Term Feature Class Term 
prev_V=performed_during                  t biopsy prev_V=automates                t retrieval 
prev_V=undergone                     t angioplasty last_word=translation            t axis translation 
prev_V=suffer                            a hypertension prev_Npr=reduction_in         a power usage 
prev_Npr=monitoring_of           a alertness Prev_Npr=degradation_in                  a audio quality 
prev_V=binds_to                        c cytokines prev_V=displayed_on           c oscillograph 
prev_Npr=salts_of                      c saccharin last_word=information          c customer infor-
mation 
 
Table 2. Number and percentage of category labels for health and computer domains (2002) 
Category Health Computer Science 
attribute 88,860   (10.8 %) 56,389   (6.5%) 
component 680,034  (83.2%) 716,688  (83.2%) 
task 48,002  (5.8 %) 88,786   (10.3%) 
 
Tables 3a and 3b show examples of machine-labeled terms for the health and computer science do-
mains.  When terms were ranked by frequency, given a relatively relaxed domain specificity threshold 
(e.g., .05 for health), the top terms tended to capture broad semantic types relevant to the domain.   As 
this threshold was increased (e.g., to 1.0 for health), the terms increased in specialization within each 
class.7 As the table entries show, while the classification is not perfect, most terms fit the definitions of 
their respective classes.  Note that in the health domain in particular, many of the ?components? reflect 
objects acted upon by the invention, not just constituents of inventions themselves.  Symptoms and 
diseases are interpreted as attributes because they are often measured according to severity and are 
targets for reduction.  
 
Table 3a. Examples  of ACT category results for health domain at two levels of domain specificity (ds). 
Component 
(ds .05) 
  
(ds 1.0) 
Attribute 
(ds .05) 
  
(ds 1.0) 
Task 
(ds .05) 
  
(ds 1.0) 
patients, 
tissue, 
blood, 
diseases, 
drugs, 
skin, 
catheter, 
brain, 
tablets, 
organs 
mitral valve, 
arterial blood, 
small incisions, 
pulmonary 
veins, 
anterior cham-
ber, 
intraocular 
lens, 
ultrasound sys-
tem, 
ultrasound en-
ergy, 
adenosine tri-
phosphate, 
bone fragments 
disease, 
infection, 
symptoms, 
pain, 
efficacy, 
side effects, 
inflammation, 
severity, 
death, 
blood flow 
cosmetic prop-
erties, 
cardiac activity, 
urination, 
tissue tempera-
ture, 
gastric empty-
ing, 
arousal 
neurotransmitter 
release, 
atrial arrhyth-
mias, 
thrombogenicity 
ventricular pac-
ing 
treatment, 
administration, 
therapy, 
surgery, 
diagnosis, 
oral admin-
istration, 
implantation, 
stimulation, 
parenteral 
administration, 
surgical pro-
cedures 
invasive proce-
dure, 
ultrasound imag-
ing, 
systole, 
anastomosis, 
spinal fusion, 
tissue ablation, 
image, recon-
struction, 
cardiac pacing, 
mass analysis, 
spinal surgery 
 
  
                                                 
7 The domain specificity thresholds chosen here differ between domains in order to compensate for the influence of the size 
of each domain?s subcorpus on the terminology mix in the ?generic? domain corpus against which domain specificity is 
measured.   In the future, we plan to compensate directly for these size disparities in the score computation.  
35
Table 3b. Examples of ACT category results for computer domain at two levels of domain specificity. 
Component 
(ds 1.5) 
 
(ds 3.0) 
Attribute 
(ds 1.5) 
  
(ds 3.0) 
Task 
(ds 1.5) 
  
(ds 3.0) 
data, 
information, 
network, 
computer, 
users, 
memory, 
internet, 
software, 
program, 
processor 
web applica-
tions, 
object access 
protocol, 
loans, 
memory sub-
system, 
function call, 
obligations, 
source file, 
file formats, 
lender 
centralized 
database 
errors, 
security, 
real time, 
traffic, 
overhead, 
delays, 
latency, 
burden, 
sales, 
copyright, 
protection 
interest rate, 
resource utiliza-
tion, 
resource con-
sumption, 
temporal locali-
ty, 
system errors, 
transport layer 
security, 
performance 
bottleneck, 
processor ca-
pacity, 
cpu utilization, 
shannon limit 
access, 
communication, 
execution, 
implementation, 
communications, 
management, 
task, 
tasks, 
stores, 
collection 
network envi-
ronments, 
business activi-
ties, 
database access, 
server process, 
search operation, 
client 's request, 
backup opera-
tion, 
project man-
agement, 
program devel-
opment, 
document man-
agement 
 
3.2 Polarity Classification 
For the polarity classification task, the system assigned positive or negative polarity to 80,870 
health and 73,289 computer science attributes. While not all the system labeled attributes merited their 
designation as attributes, the large quantity so labeled in each domain illustrates the vast number of 
conditions and dimensions for which inventions are striving to ?move the needle? one way or the oth-
er, relative to attributes in the domain.  Examples of the system?s polarity decisions are shown in Ta-
ble 4.  The system?s labels suggest that the default polarity of attributes in both domains is nearly 
evenly split. 
 
Table 4.  Examples of (pos)itive and (neg)ative polarity terms in health and computer science domains 
Domain # attributes % of total Examples 
health   
          pos 
43807 54% ambulation, hemodynamic performance, atrial rate, antico-
agulant activity, coaptation, blood oxygen saturation 
 
          neg 
37063 46% bronchospasm, thrombogenicity, ventricular pacing, with-
drawal symptoms, fibrin formation, cardiac dysfunction 
computer 
science                     
          pos 
32291 44% transport layer security, processor capacity, cpu utilization, 
routability, network speeds, microprocessor performance 
 
          neg 
40998 56% identity theft, deadlocks, system overhead, memory frag-
mentation, risk exposure, bus contention, software devel-
opment costs, network latencies, data entry errors 
 
4 Evaluation and discussion 
In order to evaluate the classification output, we first selected a subset of terms within each domain 
as candidates for evaluation based on the twin criteria of document frequency and domain specificity.  
That is, we wished to concentrate on terms with sufficient presence in the corpus as well as terms that 
were likely to express concepts of particular relevance to the domain.  Using a frequency threshold of 
10 this yielded 19,088 terms for the health corpus and 35,220 for computer science with domain speci-
ficity scores above .05 and 1.5 respectively.  For each domain, two judges annotated approximately 
150 random term instances with ACT judgments and approximately 100 machine-labeled attributes for 
polarity. The annotation tool displayed each term along with five random sentences from the corpus 
that contained the term, and asked the judge to choose the best label, given the contexts provided.  An 
36
?other? option was available if the term fit none of the target categories.  For the polarity task, the 
?other? label included cases where the attribute was neutral, could not be assigned a polarity, or was 
improperly assigned the category ?attribute?.   An adjudicated gold standard was compared to system 
labels to measure precision and recall, as shown in table 5.  
 
Table 5a. Health domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .70 .44 .54 
 component .76 1.0 .86 
 task .86 .29 .43 
Polarity  positive  .53 .85 .65 
                 negative .77 .93 .84 
 
 Table 5b. Computer domain: precision, recall and F-score for ACT and polarity classification tasks 
Task      Category Precision Recall F-score 
ACT attribute .80 .62 .70 
 component .86 .96 .90 
 task .43 .33 .38 
Polarity  positive  .67 .88 .76 
                 negative .75 .86 .80 
 
 Although the size of the evaluation set is small, we can make some observations from this sample. 
Precision in most cases is strong, which is important for the intended use of this data to characterize 
trends along each dimension using terminology statistics over time.  The lower scores for tasks within 
the ACT classification may reflect the fact that the distinction between component and task is not al-
ways clear cut.  The term ?antivirus protection?, for example, describes a task but it is classified by the 
system as a component because it occurs with features like ?prev_V=distribute? and 
?prev_V=provided_with?, which outweigh the contribution of the feature ?last_word=protection? to 
select for the type task.  To capture such cases of role ambiguity, it may be reasonable to assign some 
terms to multiple classes when the conditional probabilities for the two most probable classes are very 
close (as they are in this case).  It may also be possible to integrate other forms of evidence, such as 
syntactic coordination patterns (Zierning, 2013) to refine system decisions. 
 One shortcoming of the current polarity classifier is that it does not attempt to identify attributes for 
which the polarity is neutral or dependent upon further context within the domain.  For example, the 
attribute ?body weight gain? is labeled as a negative.  However, in the context of premature birth or 
cancer recovery, it may be actually be a positive attribute.  Testing whether an attribute co-occurs with 
conflicting features (e.g., prev_V=increase and prev_V=decrease) could help spot such cases. 
5 Related work 
Text mining from patents has focused on identifying domain keywords and terminology for analyt-
ics (Tseng, 2007).  Velardi?s (2001) approach, using statistics to determine domain relevance and con-
sensus is very similar to that adopted here. We have also drawn inspiration from sentiment analysis, 
proposing an ontology for patents that reflects their review-like qualities (Liu, 2012).  Most relevant is 
the work on discovering aspects and opinions relating to a particular subject such as a camera or res-
taurant (Kobayashi, 2007).  There are many subtleties that have been studied in opinion mining re-
search that we have finessed in our research here, such as detecting implicit sentiment and attributes 
not expressed as noun phrases.  Wilson et al (2005, 2009) addressed the larger problem of determining 
contextual polarity for subjective expressions in general, putting considerable effort into the compila-
tion of subjectivity clues and annotations.  In contrast, our aim was to test whether we could substan-
tially reduce the annotation effort when the task is focused on polarity labeling of attributes within pa-
tents.  We hypothesized that the specialized role of patents might permit a more lightweight approach 
amenable to bootstrapping from a very small set of annotations and feature types.   
37
Bootstrapping has been successfully applied to developing semantic lexicons containing a variety of 
concept types (Riloff, 1999; Thelen, 2002).  It is often applied iteratively to learn new discriminative 
features after a set of high probability categorized terms are identified during an earlier round.  While 
this increases recall, it also runs the risk of semantic drift if some terms are erroneously labeled.  Giv-
en that the majority of unlabeled terms after a single round in our system are either extremely low fre-
quency or not relevant to our ontology, we have not felt a need to run multiple iterations.  Zierning 
(2013) used bootstrapping to identify instances of the classes substance and disease in patents, exploit-
ing the tendency of syntactic coordination to relate noun phrases of the same semantic type.  Given the 
general nature of coordination, a similar approach could be used to find corroborating evidence for the 
classifications that our system produces. 
  
6 Conclusion 
We have described an approach to text data mining from patents that strikes a middle ground be-
tween undifferentiated keywords and rich, domain specific ontologies.  Motivated by the interpretation 
of patents as ?positive reviews?, we have made use of generic lexico-syntactic features common 
across patent domains to bootstrap domain-specific classifiers capable of organizing terms according 
to their roles as components, tasks and attributes with polarity.  Although the majority of keywords in 
a domain are categorized as components, the ontology puts tasks and attributes on an equal footing 
with components, thereby shifting the emphasis from devices and processes to the goals, obstacles and 
targets of inventions, information which could be valuable for analysts attempting to detect trends and 
make forecasts. In addition to more rigorous evaluation and tuning, future research directions include 
testing the approach across a wider range of technology domains, incorporation into time series analy-
sis for forecasting, and mining relationships between terms from different categories to provide an 
even richer terminological landscape for analysts to work with. 
Acknowledgements 
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via 
Department of Interior National Business Center (DoI/NBC) contract number D11PC20154. The U.S. 
Government is authorized to reproduce and distribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein 
are those of the authors and should not be interpreted as necessarily representing the official policies 
or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. 
References 
de Miranda, G. M. Coelho, Dos, and L. F. Filho. (2006) Text mining as a valuable tool in foresight exercises: A 
study on nanotechnology.  Technological Forecasting and Social Change, 73(8):1013?1027. 
Kobayashi, N., Inui, K. and Matsumoto, Y. (2007) Extracting aspect-evaluation and aspect-of relations in opin-
ion mining, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing 
and Computational Natural Language Learning, Prague, Czech Republic, pp. 1065?1074. 
Kullback, S. and Leibler, R. (1951). "On Information and Sufficiency". Annals of Mathematical Statistics 22 (1): 
79?86. 
Liu, B. (2012): Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies, 
Morgan & Claypool Publishers. 
Manning, C., Raghavan, P. and Sch?tze, H. (2008) Introduction to Information Retrieval.  Cambridge University 
Press. 
Riloff, E. and Jones, R. (1999) Learning dictionaries for information extraction by multi-level bootstrapping. In 
Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications 
of Artificial Intelligence Conference, pp. 474?479. 
Riloff, E. and Shepherd, J. (1997) A corpus-based approach for building semantic lexicons. In Proceedings of the 
Second Conference on Empirical Methods in Natural Language Processing, pp. 117?124. 
38
Shih, M.J., Liu, D.R., and Hsu, M.L. (2008) Mining Changes in Patent Trends for Competitive Intelligence. 
PAKDD 2008: 999-1005. 
Sheremetyeva S. 2009. An Efficient Patent Keyword Extractor As Translation Resource Proceedings of the 3rd 
Workshop on Patent Translation in conjunction with MT-Summit XII Ottawa, Canada. 
Thelen, M. and Riloff, E.  (2002) A bootstrapping method for learning semantic lexicons using extraction pattern 
contexts. In Proceedings of the Conference on Empirical Methods in Natural Language. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. (2003) Feature-Rich Part-of-Speech Tagging with a Cyclic 
Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259. 
Tseng, Y.-H., Lin, C.-J., and Lin, Y.-I. (2007). Text mining techniques for patent analysis. Information Pro-
cessing & Management, 43(5):1216 ? 1247. 
Velardi, P., Fabriani, P. and Missikoff, M. (2001) FOIS '01 Proceedings of the international conference on For-
mal Ontology in Information Systems - Volume 2001, pp. 270-284. 
Wilson, T., Wiebe, J and Hoffmann, P. (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment 
Analysis. Joint Human Language Technology Conference and the Conference on Empirical Methods in Natu-
ral Language Processing (HLT-EMNLP-2005). 
 Wilson, T., Wiebe, J and Hoffmann, P. (2009). Recognizing Contextual Polarity: an exploration of features for 
phrase-level sentiment analysis. Computational Linguistics 35(3). 
Woon, W. L., Henschel, A., and Madnick, S. (2009) A Framework for Technology Forecasting and Visualiza-
tion.  Working Paper CISL# 2009-11 , Massachusetts Institute of Technology.  
Yang, S.Y., Lin, S.Y., Lin, S. N., Lee, C. F., Cheng, S. L., and Soo, V. W. (2008) Automatic extraction of se-
mantic relations from patent claims.  International Journal of Electronic Business Management, Vol. 6, No. 1, 
pp. 45-54 (2008) 45. 
Ziering, P., van der Plas, L. and Sch?tze, H. (2013) Bootstrapping Semantic Lexicons for Technical Domains. In 
Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 844?848, Nago-
ya, Japan, October 2013. Asian Federation of Natural Language Processing. 
 
 
 
39
