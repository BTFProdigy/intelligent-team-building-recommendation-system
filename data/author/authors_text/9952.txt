Multi-Document Summarization By Sentence Extraction 
Jade Goldstein* Vibhu Mittal t Jaime Carbonell* Mark Kantrowitzt 
jade@cs.cmu.edu mittal@jprc.com jgc@cs.cmu.edu mkant@jprc.com 
*Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
U.S.A. 
tJust Research 
4616 Henry Street 
Pittsburgh, PA 15213 
U.S.A. 
Abstract 
This paper discusses a text extraction approach to multi- 
document summarization that builds on single-document 
summarization methods by using additional, available in-, 
formation about the document set as a whole and the 
relationships between the documents. Multi-document 
summarization differs from single in that the issues 
of compression, speed, redundancy and passage selec- 
tion are critical in the formation of useful summaries. 
Our approach addresses these issues by using domain- 
independent techniques based mainly on fast, statistical 
processing, a metric for reducing redundancy and maxi- 
mizing diversity in the selected passages, and a modular 
framework to allow easy parameterization for different 
genres, corpora characteristics and user requirements. 
1 Introduction 
With the continuing growth of online information, it 
has become increasingly important to provide improved 
mechanisms to find and present extual information ef- 
fectively. Conventional IR systems find and rank docu- 
ments based on maximizing relevance to the user query 
(Salton, 1970; van Rijsbergen, 1979; Buckley, 1985; 
Salton, 1989). Some systems also include sub-document 
relevance assessments and convey this information to the 
user. More recently, single document summarization sys- 
tems provide an automated generic abstract or a query- 
relevant summary (TIPSTER, 1998a). i However, large- 
scale IR and summarization have not yet been truly in- 
tegrated, and the functionality challenges on a summa- 
rization system are greater in a true IR or topic-detection 
context (Yang et al, 1998; Allan et al, 1998). 
Consider the situation where the user issues a search 
query, for instance on a news topic, and the retrieval sys- 
tem finds hundreds of closely-ranked documents in re- 
sponse. Many of these documents are likely to repeat 
much the same information, while differing in certain 
i Most of these were based on statistical techniques applied to var- 
ious document entities; examples include frait, 1983; Kupiec et al, 
1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al, 1995; 
Shaw, 1995; Aon? et al, 1997; Boguraev and Kennedy, 1997; Hovy 
and Lin, 1997; Mitra et al, 1997; Teufel and Moens, 1997; Barzilay 
and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mor- 
tbn, 1998; Radev and McKeown, 1998; Strzalkowski etal., 1998). 
parts. Summaries of the individual documents would 
help, but are likely to be very similar to each other, un- 
less the summarization system takes into account other 
summaries that have already been generated. Multi- 
document summarization - capable of summarizing ei- 
ther complete documents sets, or single documents in the 
context of previously summarized ones - are likely to 
be essential in such situations. Ideally, multi-document 
summaries should contain the key shared relevant infor- 
mation among all the documents only once, plus other 
information unique to some of the individual documents 
that are directly relevant to the user's query. 
Though many of the same techniques used in single- 
document summarization can also be used in multi- 
document summarization, there are at least four signif- 
icant differences: 
1. The degree of redundancy in information contained 
within a group of topically-related articles is much 
higher than the degree of redundancy within an arti- 
cle, as each article is apt to describe the main point 
as well as necessary shared background. Hence 
anti-redundancy methods are more crucial. 
2. A group of articles may contain a temporal dimen- 
sion, typical in a stream of news reports about an 
unfolding event. Here later information may over- 
ride earlier more tentative or incomplete accounts. 
3. The compression ratio (i.e. the size of the summary 
with respect o the size of the document set) will 
typically be much smaller for collections of dozens 
or hundreds of topically related documents than 
for single document summaries. The SUMMAC 
evaluation (TIPSTER, 1998a) tested 10% compres- 
sion summaries, but in our work summarizing 200- 
document clusters, we find that compression to the 
1% or 0.1% level is required. Summarization be- 
comes significantly more difficult when compres- 
sion demands increase. 
4. The co-reference problem in summarization 
presents even greater challenges for multi- 
document han for single-document summariza- 
tion (Baldwin and Morton, 1998). 
This paper discusses an approach to multi-document 
summarization that builds on previous work in single- 
40 
I 
i 
l 
i 
i 
I 
! 
I 
i 
i 
l 
I 
I 
! 
I 
I 
! 
I, 
I 
document summarization by using additional, available 
information about the document set as a whole, the re- 
lationships between the documents, as well as individual 
documents. 
2 Background and Related Work 
Generating an effective summary requires the summa- 
rizer to select, evaluate, order and aggregate items of 
information according to their relevance to a particular 
subject or purpose. These tasks can either be approx- 
imated by IR techniques or done in greater depth with 
fuller natural language processing. Most previous work 
in summarization has attempted todeal with the issues by 
focusing more on a related, but simpler, problem. With 
text-span deletion the system attempts o delete "less im- 
portant" spans of text from the original document; the 
text that remains is deemed a summary. Work on auto- 
mated document summarization by text span extraction 
dates back at least to work at IBM in the fifties (Luhn, 
1958). Most of the work in sentence xtraction applied 
statistical techniques (frequency analysis, variance anal- 
ysis, etc.) to linguistic units such as tokens, names, 
anaphora, etc. More recently, other approaches have 
investigated the utility of discourse structure (Marcu, 
1997), the combination of information extraction and 
language generation (Klavans and Shaw, 1995; McKe- 
own et al, 1995), and using machine learning to find 
patterns in text (Teufel and Moens, 1997; Barzilay and 
Elhadad, 1997; Strzalkowski et al, 1998). 
Some of these approaches tosingle document summa- 
rization have been extended to deal with multi-document 
summarization (Mani and Bloedern, 1997; Goldstein and 
Carbonell, 1998; TIPSTER, 1998b; Radev and McKe- 
own, 1998; Mani and Bloedorn, 1999; McKeown et al, 
.!999; Stein et al, 1999). These include comparing tem- 
plates filled in by extracting information - using special- 
ized, domain specific knowledge sources - from the doc- 
"ument, and then generating natural language summaries 
from the templates (Radev and McKeown, 1998), com-- 
? paring named-entities - extracted using specialized lists 
- between documents and selecting the most relevant 
section (TIPSTER, 1998b), finding co-reference chains 
in the document set to identify common sections of inter- 
est (TIPSTER, 1998b), or building activation etworks 
of related lexical items (identity mappings, synonyms, 
hypernyms, etc.) to extract text spans from the document 
set (Mani and Bloedern, 1997). Another system (Stein et 
al., 1999) creates a multi-document summary from mul- 
tiple single document summaries, an approach that can 
be sub-optimal in some cases, due to the fact that the 
process of generating the final multi-document summary 
takes as input he individual summaries and not the com- 
plete documents. (Particularly if the single-document 
summaries can contain much overlapping information.) 
The Columbia University system (McKeown et al, 1999) 
creates amulti-document summary using machine learn- 
ing and statistical techniques to identify similar sections 
41 
and language generation to reformulate the summary. 
The focus of our approach is a multi-document system 
that can quickly summarize large clusters of similar doc- 
uments (on the order of thousands) while providing the 
key relevant useful information or pointers to such in- 
formation. Our system (1) primarily uses only domain- 
independent techniques, based mainly on fast, statistical 
processing, (2) explicitly deals with the issue of reducing 
redundancy without eliminating potential relevant infor- 
mation, and (3) contains parameterized modules, so that 
different genres or corpora characteristics an be taken 
into account easily. 
3 Requirements for Multi-Document 
Summarization 
There are two types of situations in which multi- 
document summarization would be useful: (1) the user 
is faced with a collection of dis-similar documents and 
wishes to assess the information landscape contained in 
the collection, or (2) there is a collection of topically- 
related ocuments, extracted from a larger more diverse 
collection as the result of a query, or a topically-cohesive 
cluster. In the first case, if the collection is large enough, 
it only makes ense to first cluster and categorize the doc- 
uments (Yang et al, 1999), and then sample from, or 
summarize ach cohesive cluster. Hence, a "summary" 
would constitute of a visualization of the information 
landscape, where features could be clusters or summaries 
thereof. In the second case, it is possible to build a syn- 
thetic textual summary containing the main point(s) of 
the topic, augmented with non-redundant background in- 
formation and/or query-relevant elaborations. This is the 
focus of our work reported here, including the necessity 
to eliminate redundancy among the information content 
of multiple related ocuments. 
Users' information seeking needs and goals vary 
tremendously. When a group of three people created a
multi-document summarization of 10 articles about he 
Microsoft Trial from a given day, one summary focused 
on the details presented in court, one on an overall gist 
of the day's events, and the third on a high level view of 
the goals and outcome of the trial. Thus, an ideal multi- 
document summarization would be able to address the 
different levels of detail, which is difficult without natu- 
ral language understanding. An interface for the summa- 
rization system needs to be able to permit he user to en- 
ter information seeking oals, via a query, a background 
interest profile and/or a relevance feedback mechanism. 
Following is a list of requirements for multi-document 
summarization: 
? clustering: The ability to cluster similar documents 
and passages to find related information. 
? coverage: The ability to find and extract he main 
points across documents. 
? anti-redundancy: The ability to minimize redun- 
dancy between passages in the summary. 
*. summary cohesion criteria: The ability to combine 
text passages in a useful manner for the reader.-This 
may include: 
- document ordering: All text segments of high- 
est ranking document, hen all segments from 
the next highest ranking document, etc. 
- news-story principle (rank ordering):present 
the most relevant and diverse information first 
so that the reader gets the maximal information 
content even if they stop reading the summary. 
- topic-cohesion: Group together the passages 
by topic clustering using passage similarity cri- 
teria and present the information by the cluster" 
centroid passage rank. 
- t ime line ordering: Text passages ordered 
based on the occurrence of events in time. 
* coherence: Summaries generated should be read- 
able and relevant to the user. 
. context: Include sufficient context so that the sum- 
mary is understandable to the reader. 
? identification of source inconsistencies: Articles of- 
ten have errors (such as billion reported as million, 
etc.); multi-document summarization must be able 
to recognize and report source inconsistencies. 
? summary updates: A new multi-document summary 
must take into account previous ummaries in gen- 
erating new summaries. In such cases, the system 
needs to be able to track and categorize vents. 
? effective user interfaces: 
- Attributability: The user needs to be able to 
easily access the source of a given passage. 
This could be the single document summary. 
- Relationship: The user needs to view related 
passages to the text passage shown, which can 
highlight source inconsistencies. 
- Source Selection: The user needs to be able to 
,- select or eliminate various sources. For exam- 
ple, the user may want to eliminate information 
from some less reliable foreign news reporting 
sources. 
- Context: The user needs to be able to zoom 
in on the context surrounding the chosen pas- 
sages. 
- Redirection: The user should be able to high- 
light certain parts of the synthetic summary 
and give a command to the system indicating 
that these parts are to be weighted heavily and 
that other parts are to be given a lesser weight. 
4 Types of Multi-Document Summarizers 
In the previous ection we discussed the requirements 
for a multi-document summarization system. Depend- 
ing on a user's information seeking goals, the user may 
want to create summaries that contain primarily the com- 
mon portions of the documents (their intersection) or an 
overview of the entire cluster of documents (a sampling. 
of the space that the documents span). A user may also 
want to have a highly readable summary, an overview of 
pointers (sentences or word lists) to further information, 
? or a combination of the two. Following is a list of  var- 
ious methods of creating multi-document summaries by 
extraction: 
1. Summary from Common Sections of Documents: 
Find the important relevant parts that the cluster of 
documents have in common (their intersection) and 
use that as a summary. 
2. Summary from Common Sections and Unique Sec- 
tions of Documents: Find the important relevant 
parts that the cluster of documents have in common 
and the relevant parts that are unique and use that as 
a summary. 
3. Centroid Document Summary: Create a single doc- 
ument summary from the centroid ocument in the 
? cluster. 
4. Centroid Document plus Outliers Summary: Cre- 
ate a single document summary from the centroid 
document in the cluster and add some representa- 
tion from outlier documents (passages or keyword 
extraction) to provide a fuller coverage of the docu- 
ment set. 2 
5. Latest Document plus Outliers Summary: Create 
a single document summary from the latest time 
stamped ocument in the cluster (most recent in- 
formation) and add some representation f outlier 
documents o provide a fuller coverage of the docu- 
ment set. 
6. Summary from Common Sections and Unique Sec- 
tions of Documents with Time Weighting Factor: 
Find the important relevant parts that the cluster of 
documents have in common and the relevant parts 
that are unique and weight all the information by 
the time sequence of the documents in which they 
appear and use the result as a summary. This al- 
lows the more recent, often updated information to 
be more likely to be included in the summary. 
There are also much more complicated types of sum- 
mary extracts which involve natural anguage process- 
ing and/or understanding. These types of summaries in- 
clude: (1) differing points of view within the document 
collection, (2) updates of information within the doc- 
ument collection, (3) updates of information from the 
document collection with respect o an already provided 
summary, (4) the development of an event or subtopic of 
2This is similar to the approach ofTextwise fHPSTER, 1998b), 
whose multi-document summary consists of the most relevant para- 
graph and specialized word lists. 
42 
I 
I 
I 
I 
l 
I 
I 
I 
I 
I 
I 
i 
an event (e.g., death tolls) over time, and (5) a compara- 
tive development of an event. 
Naturally, an ideal multi-document summary would 
include a natural language generation component to cre- 
ate cohesive readable summaries (Radev and McKeown, 
1998; McKeown et al, 1999). Our current focus is on 
the extraction of the relevant passages. 
5 System Design 
In the previous ections we discussed the requirements 
and types of multi-document summarization systems. 
This section discusses our current implementation of
a multi-document summarization system which is de- 
signed to produce summaries that emphasize "relevant 
novelty." Relevant novelty is a metric for minimizing re- 
dundancy and maximizing both relevance and diversity. 
A first approximation tomeasuring relevant novelty is to 
measure relevance and novelty independently and pro- 
vide a linear combination as the metric. We call this lin- 
ear combination "marginal relevance" .-- i.e., a text pas- 
sage has high marginal relevance if it is both relevant to 
the query and useful for a summary, while having mini- 
mal similarity to previously selected passages. Using this 
metric one can maximize marginal relevance in retrieval 
and summarization, hence we label our method "maxi- 
mal marginal relevance" (MMR) (Carboneli and Gold- 
stein, 1998). 
The Maximal Marginal Relevance Multi-Document 
(MMR-MD) metric is defined in Figure 1. Sirnl and 
Sire2 cover some of the properties that we discussed in 
Section 3. 3 
: For Sirnl, the first term is the cosine similarity metric 
for query and document. The second term computes a
coverage score for the passage by whether the passage 
is in one or more clusters and the size of the cluster. 
The third term reflects the information content of the pas- 
.sage by taking into account both statistical and linguis- 
tic features for summary inclusion (such as query expan- 
.sion, position of the passage in the document and pres- 
ence/absence of named-entities in the passage). The final 
term indicates the temporal sequence of the document in 
the collection allowing for more recent information to 
have higher weights. 
For Sire2, the first term uses the cosine similarity met- 
ric to compute the similarity between the passage and 
previously selected passages. (This helps the system to 
minimize the possibility of including passages similar to 
ones already selected.) The second term penalizes pas- 
sages that are part of clusters from which other passages 
have already been chosen. The third term penalizes doc- 
uments from which passages have already been selected; 
however, the penalty is inversely proportional to docu- 
ment length, to allow the possibility of longer documents 
3Sirnn and Sirn2 as previously defined in MMR for single- 
document summarization contained only the first term of each equa- 
tion: 
43 
contributing more passages. These latter two terms allow 
for a fuller coverage of the clusters and documents. 
Given the above definition, MMR-MD incrementally 
computes the standard relevance-ranked list- plus some 
additional scoring factors - when the parameter A= 1, and 
computes a maximal diversity ranking among the pas- 
sages in the documents when A=0. For intermediate val- 
ues of A in the interval \[0,1 \], a linear combination of both 
criteria is optimized. In order to sample the information 
space in the general vicinity of the query, small values of 
can be used; to focus on multiple, potentially overlap- 
ping or reinforcing relevant passages, A can be set to a 
value closer to 1. We found that a particularly effective 
search strategy for document retrieval is to start with a 
small A (e.g., A = .3) in order to understand the informa- 
tion space in the region of the query, and then to focus 
on the most important parts using a reformulated query 
(possibly via relevance feedback) and a larger value of 
(e.g., A = .7) (Carboneli and Goldstein, 1998). 
Our multi-document summarizer works as follows: 
? Segment he documents into passages, and index 
them using inverted indices (as used by the IR 
engine). Passages may be phrases, sentences, n- 
sentence chunks, or paragraphs. 
? Identify the passages relevant o the query using 
cosine similarity with a threshold below which the 
passages are discarded. 
? Apply the MMR-MD metric as defined above. De- 
pending on the desired length of the summary, se- 
lect a number of passages to compute passage re- 
dundancy using the cosine similarity metric and use 
the passage similarity scoring as a method of clus- 
tering passages. Users can select he number of pas- 
sages or the amount of compression. 
? Reassemble the selected passages into a summary 
document using one of the summary-cohesion cri- 
teria (see Section 3). 
The results reported in this paper are based on the use 
of the SMART search engine (Buckley, 1985) to compute 
cosine similarities (with a SMART weighting of lnn  for 
both queries and passages), stopwords eliminated from 
the indexed ata and stemming turned on. 
6 Discussion 
The TIPSTER evaluation corpus provided several sets of 
topical clusters to which we applied MMR-MD summa- 
rization. As an example, consider a set of 200 apartheid- 
related news-wire documents from the Associated Press 
and the Wall Street Journal, spanning the period from 
1988 to 1992. We used the TIPSTER provided topic de- 
scription as the query. These 200 documents were on 
an average 31 sentences in length, with a total of 6115 
sentences. We used the sentence as our summary unit. 
Generating a summary 10 sentences long resulted in a 
MMR-MD ~ Arg max \[A(Siml (Pii, Q, Cij, Di, D)) - (1 - A) max Sirn2 (Pij, Pnm, C, S, Di))\] 
Pij ER\S t - P,=.. ES 
Sire1 (P,.j, Q, Cij, Di, D) = wl *(Pij'Q)+w2*coverage(Pij, Cij)+wa*content(Pij)+w4*tirne_sequenee(Di, D) 
Sim2 ( Pij, Pare, C, S, Di ) = tOa * ( f f  i j  " Pnm) + rob * clusters_selected( (7ij, S) + we * documents_selected( Di , S) 
~ov~r~ge(Pi~,C) = ~ wk * Ikl 
kECi./ 
eonlent(Pij) = ~ wtvp,(W) 
WEPij 
tirnesiarap( D,,a=tim, ) - timestamp( Di ) 
time_sequ_ence ( Di, D) = timestamp( Dmaxtime ) - tiraestamp( D,nintime ) 
clusters_selected(C~, S) = IC~ n L.J cv=l 
v,w:P,,,~ES 
documents_selected(Di, S) = ~ = 
where 
Sire1 is the similarity metric for relevance ranking 
Sim~ is the anti-redundancy metric 
D is a document collection 
P is the passages from the documents in that collection (e.g., ~ j  is passage j from document Di) 
Q is a query or user profile 
R = IR(D, P, Q, 8), i.e., the ranked list of passages from documents retrieved by an IR system, given D, P, Q and a 
' relevance threshold O, below which it will not retrieve passages (O can be degree of match or number of passages) 
._5" is the subset of passages in R already selected 
R\S  is the set difference, i.e., the set of as yet unselected passages in R 
' C is the set of passage clusters for the set of documents 
(7vw is the subset of clusters of (7 that contains passage Pvw 
(7~ is the subset of clusters that contain passages from document D~ 
Ikl is the number of passages in the individual cluster k
IC~,~ N Cijl is the number of clusters in the intersection of (7,,,nand(Tij 
wi..are weights for the terms, which can be optimized 
W is a word in the passage/~j 
type is a particular type of word, e.g., city name 
IOil is the length of document i. 
Figure l: Definition of multi-document summarization algorithm - MMR-MD 
i 
I 
I 
I 
i 
I 
! 
I 
I 
I 
i 
! 
i 
sentence compression ratio of 0.2% and a character com- 
pression of 0.3%, approximately two orders of magni- 
tude different with compression ratios used in single doc- 
ument summarization. The results of summarizing this 
document set with a value of A set to I (effectively query 
relevance, but no MMR-MD) and A set to 0.3 (both query 
relevance and MMR-MD anti-redundancy) are shown in 
Figures 2 and 3 respectively. The summary in Figure 2 
clearly illustrates the need for reducing redundancy and 
maximizing novel information. 
Consider for instance, the summary shown in Figure 2. 
The fact that the ANC is fighting to overthrow the gov- 
44 
i. wsJg10204-0176:1 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
2. AP880803-0082:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
3. AP880803-0080:25 Three Canadian anti-apartheid groups issued a statement urging the government to sever 
diplomatic and economic links with South Africa and aid the African National Congress, the banned group fighting 
the white-dominated government in South Africa. 
4. AP880802-0165:23 South Africa says the ANC, the main black group fighting to overthrow South Africa's white 
government, has seven major military bases in Angola, and the Pretoria government wants those bases closed 
down. 
5. AP880212-0060:14 ANGOP quoted the Angolan statement as saying the main causes of confict in the region 
are South Africa's "illegal occupation" of Namibia, South African attacks against its black-ruled neighbors and 
its alleged creation of armed groups to carry out "terrorist a~tivities" in those countries, and the denial of political 
rights to the black majority in South Africa. 
6. AP880823-0069:17 The ANC is the main guerrilla group fighting to overthrow the South African government 
and end apartheid, the system of racial segregation i which South Africa's black majority has no vote in national 
affairs. 
7. AP880803-0158:26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed down. 
8. AP880613-0126:15 The ANC is fighting to topple the South African government and its policy of apartheid, 
under which the nation's 26 million blacks have no voice in national affairs and the 5 million whites control the 
economy and dominate government. 
9. AP880212-0060:13 The African National Congress i the main rebel movement fighting South Africa's white-led 
government and SWAPO is a black guerrilla group fighting for independence for Namibia, which is administered 
by South Africa. 
I0. WSJ870129-0051:1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African 
National Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use 
of violence in the struggle against apartheid. 
Figure 2: Sample multi-document summary with A = 1, news-story-principle ordering (rank order) 
? ernment is mentioned seven times (sentences #2,-#4,#6- 
#9),"which constitutes 70% of the sentences in the sum- 
mary. Furthermore, sentence #3 is an exact duplicate of  
sentence #2, and sentence #7 is almost identical to sen- 
tence #4. In contrast, the summary in Figure 3, generated 
using MMR-MD with a value of A set to 0.3 shows sig- 
nificant improvements in eliminating redundancy. The 
fact that the ANC is fighting to overthrow the govern- 
ment is mentioned only twice (sentences #3,#7), and one 
of these sentences has additional information in it. The 
new summary retained only three of  the sentences from 
the earlier summary. 
Counting clearly distinct propositions in both cases, 
yields a 60% greater information content for the MMR- 
MD case, though both summaries are equivalent in 
length. 
When these 200 documents were added to a set of 4 
other topics of 200 documents, yielding a document-set 
with 1000 documents, the query relevant multi-document 
summarization system produced exactly the same re- 
suits. 
We are currently working on constructing datasetsfor 
experimental evaluations of multi-document summariza- 
tion. In order to construct these data sets, we attempted 
to categorize user's information seeking goals for multi- 
document summarization (see Section 3). As can be seen 
in Figure 2, the standard IR technique of using a query to 
extract relevant passages i no longer sufficient for multi- 
document summarization due to redundancy. In addi- 
tion, query relevant extractions cannot capture temporal 
sequencing. The data sets will allow us to measure the 
effects of these, and other features, on multi-document 
summarization quality. 
Specifically, we are constructing sets of 10 documents, 
? which either contain a snapshot of  an event from mul- 
tiple sources or the unfoldment of an event over time. 
45 
I 
I 1. WSJ870129-0051 1 Secretary of State George Shultz, in a meeting with Oliver Tambo, head of the African Na- 
tional Congress, voiced concerns about Soviet influence on the black South African group and the ANC's use of 
violence in the struggle against apartheid. 
2. wsJgg0422-0133 44 (See related story: "ANC: Apartheid' s Foes - The Long Struggle: The ANC Is Banned, 
But It Is in the Hearts of a Nation's Blacks - -  In South Africa, the Group Survives Assassinations, Government 
Crackdowns n The Black, Green and Gold" - WSJ April 22, 1988) 
3. AP880803-0158 26 South Africa says the ANC, the main black group fighting to overthrow South Africa's white- 
led government, has seven major military bases in Angola, and it wants those bases closed own. 
4. AP880919-0052  But activist clergymen from South Africa said the pontiff should have spoken out more force- 
fully against their white-minority government's policies of apartheid, under which 26 million blacks have no say 
in national affairs. 
5. AP890821-0092 10 Besides ending the emergency and lifting bans on anti- apartheid groups and individual ac- 
tivists, the Harare summit's conditions included the removal of all troops from South Africa's black townships, 
releasing all political prisoners and ending political trials and executions, and a government commitment tofree 
political discussion. 
6. wsJg00503-0041 1  Pretoria and the ANC remain'far ap~t ontheir vision s for a post-apartheid South Africa: 
The ANC wants a simple one-man, one-vote majority rule system, while the government claims that will lead to 
black domination and insists on constitutional protection of the rights of minorities, including the whites. 
7. WSJ900807-0037 1 JOHANNESBURG, South Africa - The African National Congress uspended its 30-year 
armed struggle against he whiie minority government, clearing the way for the start of negotiations over a new 
constitution based on black-white power sharing. 
8. WSJ900924-011920 The African National Congress, South Africa's main black liberation group, forged its sanc- 
tions strategy as a means of pressuring the government toabandon white-minority rule. 
9. WSJ910702-0053 36 At a, meeting in South Africa this week, the African National Congress, the major black 
group, is expected to take a tough line again st the white-rnn government. 
10. wsJg10204-01761 CAPE TOWN, South Africa - President EW. de Klerk's proposal to repeal the major pillars 
of apartheid rew a generally positive response from black leaders, but African National Congress leader Nelson 
Mandela called on the international community to continue conomic sanctions against South Africa until the 
government takes further steps. 
Figure 3: Sample multi-document summary with A = 0.3, time-line ordering 
From these sets we are performing two types of exper- 
iments. In the first, we are examining how users put 
sentences into pre-defined clusters and how they create 
sentence based multi-document summaries. The result 
will also serve as a gold standard for system generated 
summaries - do our systems pick the same summary sen- 
tences as humans and are they picking sentences from 
the same clusters as humans? The second type Of exper- 
iment is designed to determine how users perceive the 
output summary quality. In this experiment, users are 
asked to rate the output sentences from the summarizer 
as good, okay or bad. For the okay or bad sentences, 
they are asked to provide a summary sentence from the 
document set that is "better", i.e., that makes a better set 
of  sentences to represent the information content of  the 
document set. We are comparing our proposed summa- 
rizer #6 in Section 4 to summarizer #1, the common por- 
tions of  the document sets with no anti-redundancy and 
summarizer #3, single document summary of  a centroid 
document using our single document summarizer (Gold- 
stein et al, 1999). 
7 Conc lus ions  and  Future  Work  
This paper presented a statistical method of  generating 
extraction based multi-document summaries. I t  builds 
upon previous work in single-document summarization 
and takes into account some of the major differences be- 
tween single-document and multi-document summariza- 
tion: (i) the need to carefully eliminate redundant infor- 
mation from multiple documents, and achieve high com- 
pression ratios, (ii) take into account information about 
document and passage similarities, and weight different 
passages accordingly, and (iii) take temporal information 
into account. 
Our approach differs from others in several ways: it 
is completely domain-independent, is based mainly on 
fast, statistical processing, it attempts to maximize the 
novelty of the information being selected, and different 
46 
I 
I 
I 
I 
I 
I 
! 
I 
! 
I 
! 
! 
I 
i 
I 
I 
! 
I 
! 
I 
I 
! 
I 
I 
genres or corpora characteristics an be taken into ac- 
count easily. Since our system is not based on the use of 
sophisticated natural language understanding or informa- 
tion extraction techniques, ummaries lack co-reference 
resolution, passages may be disjoint from one another, 
and in some cases may have false implicature. 
In future work, we will integrate work on multi- 
document summarization with work on clustering to pro- 
vide summaries for clusters produced by topic detection 
and tracking. We also plan to investigate how to gen- 
erate coherent temporally based event summaries. We 
will also investigate how users can effectively use multi- 
document summarization through interactive interfaces 
to browse and explore large document sets. 
References 
James Allan, Jaime Carbonell, George Doddington,, 
Jonathan Yamron, and Yiming Yang. 1998. Topic de- 
tection and tracking pilot study: Final report. In Pro- 
ceedings of the DARPA Broadcast News Transcription 
and Understanding Workshop. 
Chinatsu Aone, M. E. Okurowski, J. Gorlinsky, and 
B. Larsen. 1997. A scalable summarization sys- 
tem using robust NLP. In Proceedings of the 
ACL'97/EACL'97 Workshop on Intelligent Scalable 
Text Summarization, pages 66-73, Madrid, Spain. 
Breck Baldwin and Thomas S. Morton. 1998. Dy- 
namic coreference-based summarization. I Proceed- 
ings of the Third Conference on Empirical Methods in 
Natural Language Processing (EMNLP-3), Granada, 
Spain, June. 
Regina Barzilay and Michael Elhadad. 1997. Using lex- 
ical chains for text summarization. In Proceedings of 
the ACL'97/EACL'97 Workshop on Intelligent Scal- 
able Text Summarization, pages 10-17, Madrid, Spain. 
Branimir Boguraev and Chris Kennedy. 1997. Salience 
based content characterization f text documents. In 
Proceedings of the ACL'97/EACL'97 Workshop on 
Intelligent Scalable Text Summarization, pages 2-9,. 
Madrid, Spain. 
Chris Buckley. 1985. Implementation f the SMART in- 
formation retrieval system. Technical Report TR 85- 
686, Cornell University. 
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for reordering 
documents and producing summaries. In Proceedings 
of SIGIR-98, Melbourne, Australia, August. 
Jade Goldstein and Jaime Carbonell. 1998. The use 
of mmr and diversity-based reranking in document 
reranking and summarization. In Proceedings of the 
14th Twente Workshop on Language Technology in 
Multimedia Information Retrieval, pages 152-166, 
Enschede, the Netherlands, December. 
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and 
? Jaime G. Carbonell. 1999. Summarizing Text Doc- 
uments: Sentence Selection and Evaluation Metrics. 
Irf Proceedings of the 22nd International ACM SIGIR 
Conference on Research and Development in Informa- 
tion Retrieval (S1G1R-99), pages 121-128, Berkeley, 
CA. 
Eduard Hovy and Chin-Yew Lin. 1997. Automated text 
summarization i SUMMARIST. In ACUEACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 18-24, Madrid, Spain, July. 
Judith L. Klavans and James Shaw. 1995. Lexical se- 
mantics in summarization. I  Proceedings of the First 
Annual Workshop of the IFIP Working Group FOR 
NLP and KR, Nantes, France, April. 
Julian M. Kupiec, Jan Pedersen, and Francine Chen. 
1995. A trainable document summarizer. In Proceed- 
ings of the 18th Annual Int. ACM/SIG1R Coaference 
on Research and Development in IR, pages 68-73, 
Seattle, WA, July. 
P. H. Luhn. 1958. Automatic reation of literature ab- 
stracts. IBM Journal, pages 159-165. 
Inderjeet Mani and Eric Bloedern. 1997. Multi- 
document summarization by graph search and merg- 
ing. In Proceedings of AAA1-97, pages 622--628. 
AAAI. 
Inderjeet Mani and Eric Bloedom. 1999. Summarizing 
similarities and differences among related ocuments. 
Information Retrieval, 1:35-67. 
Daniel'Marcu. 1997. From discourse structures to text 
summaries. In Proceedings of the ACL'97/EACL'97 
Workshop on Intelligent Scalable Text Summarization, 
pages 82-88, Madrid, Spain. 
Kathleen McKeown, Jacques Robin, and Karen Kukich. 
1995. Designing and evaluating a new revision-based 
model for summary generation. Info. Proc. and Man- 
agement, 31 (5). 
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas- 
siloglou, Regina Barzilay, and Eleazar Eskin. 1999. 
Towards Multidocument Summarization by Reformu- 
lation: Progress and Prospects. In Proceedings of 
AAAI-99, pages 453--460, Orlando, FL, July. 
Mandar Mitra, Amit Singhal, and Chris Buckley. 1997. 
Automatic text summarization by paragraph extrac- 
tion. In ACL/EACL-97 Workshop on Intelligent Scal- 
able Text Summarization, pages 31-36, Madrid, Spain, 
July. 
Chris D. Paice. 1990. Constructing literature abstracts 
by computer: Techniques and prospects. Info. Proc. 
and Management, 26:171-186. 
Dragomir Radev and Kathy McKeown. 1998. Generat- 
ing natural language summaries from multiple online 
sources. Compuutational Linguistics. 
Gerald Salttm. 1970. Automatic processing of foreign 
language docuemnts. Journal of American Society for 
Information Sciences, 21:187-194. 
Gerald Salton. 1989. Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of Informa- 
tion by Computer. Addison-Wesley. 
47 
James Shaw. 1995. Conciseness through aggregation i  
text generation. In Proceedings of 33rd Association 
for Computational Linguistics, pages 329-331. 
Gees C. Stein, Tomek Strzalkowski, and G. Bowden 
Wise. 1999. Summarizing Multiple Documents Us- 
ing Text Extraction and Interactive Clustering. In Pro- 
ceedings of PacLing-99: The Pacific Rim Conference 
on Computational Linguistics, pages 200-208, Water- 
loo, Canada. 
Tomek Strzalkowski, Jin Wang, and Bowden Wise. 
1998. A robust practical text summarization system. 
In AAAI Intelligent Text Summarization Workshop, 
pages 26-30, Stanford, CA, March. 
J. I. Tait. 1983. Automatic Summarizing of English 
Texts. Ph.D. thesis, University of Cambridge, Cam- 
bridge, UK. 
Simone Teufel and Marc Moens. 1997. Sentence x- 
traction as a classification task. In ACL/EACL-97 
Workshop on Intelligent Scalable Text Summarization, 
pages 58-65, Madrid, Spain, July. 
TIPSTER. 1998a. Tipster text phase III 18-month work- 
shop notes, May. Fairfax, VA. 
TIPSTER. 1998b. Tipster text phase III 24-month work- 
shop notes, October. Baltimore, MD. 
Charles J. van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
Yiming Yang, Tom Pierce, and Jaime 13. Carbonell. 
1998. A study on retrospective and on-line event de- 
tection. In Proceedings of the 21th Ann lnt ACM SI- 
G1R Conference on Research and Development inIn- 
formation Retrieval (SIGIR'98), pages 28-36. 
:Yiming Yang, Jaime G. Carbonell, Ralf D. Brown, 
Tom Pierce, Brian T. Archibald, and Xin Liu. 1999. 
Learning approaches for topic detection and tracking 
. news events. IEEE Intelligent Systems, Special Issue 
on Applications of Intelligent Information Retrieval, 
14(4):32-43, July/August. 
48 
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 336?344,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Person Identification from Text and Speech Genre Samples 
Jade Goldstein-Stewart 
U.S. Department of Defense 
 
jadeg@acm.org 
Ransom Winder 
The MITRE Corporation 
Hanover, MD, USA 
rwinder@mitre.org 
Roberta Evans Sabin 
Loyola University 
Baltimore, MD, USA 
res@loyola.edu 
 
Abstract 
 
In this paper, we describe experiments con-
ducted on identifying a person using a novel 
unique correlated corpus of text and audio 
samples of the person?s communication in six 
genres.  The text samples include essays, 
emails, blogs, and chat.  Audio samples were 
collected from individual interviews and group 
discussions and then transcribed to text.  For 
each genre, samples were collected for six top-
ics.  We show that we can identify the com-
municant with an accuracy of 71% for six fold 
cross validation using an average of 22,000 
words per individual across the six genres.  
For person identification in a particular genre 
(train on five genres, test on one), an average 
accuracy of 82% is achieved.  For identifica-
tion from topics (train on five topics, test on 
one), an average accuracy of 94% is achieved.  
We also report results on identifying a per-
son?s communication in a genre using text ge-
nres only as well as audio genres only.  
1 Introduction 
Can one identify a person from samples of 
his/her communication?  What common patterns 
of communication can be used to identify 
people?  Are such patterns consistent across va-
rying genres? 
People tend to be interested in subjects and 
topics that they discuss with friends, family, col-
leagues and acquaintances.  They can communi-
cate with these people textually via email, text 
messages and chat rooms.  They can also com-
municate via verbal conversations.  Other forms 
of communication could include blogs or even 
formal writings such as essays or scientific ar-
ticles.  People communicating in these different 
?genres? may have different stylistic patterns and 
we are interested in whether or not we could 
identify people from their communications in 
different genres. 
The attempt to identify authorship of written 
text has a long history that predates electronic 
computing.  The idea that features such as aver-
age word length and average sentence length 
could allow an author to be identified dates to 
Mendenhall (1887).  Mosteller and Wallace 
(1964) used function words in a groundbreaking 
study that identified authors of The Federalist 
Papers.  Since then many attempts at authorship 
attribution have used function words and other 
features, such as word class frequencies and 
measures derived from syntactic analysis, often 
combined using multivariable statistical tech-
niques.  
Recently, McCarthy (2006) was able to diffe-
rentiate three authors? works, and Hill and Prov-
ost (2003), using a feature of co-citations, 
showed that they could successfully identify 
scientific articles by the same person, achieving 
85% accuracy when the person has authored over 
100 papers.  Levitan and Argamon (2006) and 
McCombe (2002) further investigated authorship 
identification of The Federalist Papers (three 
authors).   
The genre of the text may affect the authorship 
identification task.  The attempt to characterize 
genres dates to Biber (1988) who selected 67 
linguistic features and analyzed samples of 23 
spoken and written genres.  He determined six 
factors that could be used to identify written text.  
Since his study, new ?cybergenres? have 
evolved, including email, blogs, chat, and text 
messaging.  Efforts have been made to character-
ize the linguistic features of these genres (Baron, 
2003; Crystal, 2001; Herring, 2001; Shepherd 
and Watters, 1999; Yates, 1996).  The task is 
complicated by the great diversity that can be 
exhibited within even a single genre.  Email can 
be business-related, personal, or spam; the style 
336
can be tremendously affected by demographic 
factors, including gender and age of the sender.  
The context of communication influences lan-
guage style (Thomson and Murachver, 2001; 
Coupland, et al, 1988).  Some people use ab-
breviations to ease the efficiency of communica-
tion in informal genres ? items that one would 
not find in a formal essay.  Informal writing may 
also contain emoticons (e.g., ?:-)? or ???) to 
convey mood. 
Successes have been achieved in categorizing 
web page decriptions (Calvo, et al, 2004) and 
genre determination (Goldstein-Stewart, et al, 
2007; Santini 2007).  Genders of authors have 
been successfully identified within the British 
National Corpus (Koppel, et al, 2002). In 
authorship identification, recent research has fo-
cused on identifying authors within a particular 
genre: email collections, news stories, scientific 
papers, listserv forums, and computer programs 
(de Vel, et al, 2001; Krsul and Spafford, 1997; 
Madigan, et al, 2005; McCombe, 2002).  In the 
KDD Cup 2003 Competitive Task, systems at-
tempted to identify successfully scientific articles 
authored by the same person.  The best system 
(Hill and Provost, 2003) was able to identify 
successfully scientific articles by the same per-
son 45% of the time; for authors with over 100 
papers, 85% accuracy was achieved. 
Are there common features of communication 
of an individual across and within genres?  Un-
doubtedly, the lack of corpora has been an impe-
diment to answering this question, as gathering 
personal communication samples faces consider-
able privacy and accessibility hurdles.  To our 
knowledge, all previous studies have focused on 
individual communications in one or possibly 
two genres.  
To analyze, compare, and contrast the com-
munication of individuals across and within dif-
ferent modalities, we collected a corpus consist-
ing of communication samples of 21 people in 
six genres on six topics.  We believe this corpus 
is the first attempt to create such a correlated 
corpus.   
From this corpus, we are able to perform expe-
riments on person identification.  Specifically, 
this means recognizing which individual of a set 
of people composed a document or spoke an ut-
terance which was transcribed.  We believe using 
text and transcribed speech in this manner is a 
novel research area.  In particular, the following 
types of experiments can be performed: 
- Identification of person in a novel genre 
(using five genres as training) 
- Identification of person in a novel topic 
(using five topics as training) 
- Identification of person in written genres, 
after training on the two spoken genres 
- Identification of person in spoken genres, 
after training on the written genres 
- Identification of person in written genres, 
after training on the other written genres 
In this paper, we discuss the formation and 
statistics of this corpus and report results for 
identifying individual people using techniques 
that utilize several different feature sets.  
2 Corpus Collection 
Our interest was in the research question: can a 
person be identified from their writing and audio 
samples?  Since we hypothesize that people 
communicate about items of interest to them 
across various genres, we decided to test this 
theory.  Email and chat were chosen as textual 
genres (Table 1), since text messages, although 
very common, were not easy to collect.  We also 
collected blogs and essays as samples of textual 
genres.  For audio genres, to simulate 
conversational speech as much as possible, we 
collected data from interviews and discussion 
groups that consisted of sets of subjects 
participating in the study.  Genres labeled ?peer 
give and take? allowed subjects to interact. 
Such a collection of genres allows us to 
examine both conversational and non-
conversational genres, both written and spoken 
modalities, and both formal and informal writing 
with the aim of contrasting and comparing 
computer-mediated and non-computer-mediated 
genres as well as informal and formal genres. 
 
Genre 
Com-
puter-
me-
diated 
Peer 
Give 
and 
Take 
Mode 
Con 
versa-
tional 
Au-
dience 
Email yes no text yes ad-
dressee 
Essay No no text no unspec 
Inter-
view 
No no speech yes inter-
viewer 
Blog yes yes text no world 
Chat yes yes text yes group 
Dis-
cussion 
No yes speech yes group 
Table 1.  Genres 
 
In order to ensure that the students could pro-
duce enough data, we chose six topics that were 
controversial and politically and/or socially rele-
337
vant for college students from among whom the 
subjects would be drawn.  These six topics were 
chosen from a pilot study consisting of twelve 
topics, in which we analyzed the amount of in-
formation that people tended to ?volunteer? on 
the topics as well as their thoughts about being 
able to write/speak on such a topic.  The six top-
ics are listed in Table 2.  
 
Topic Question 
Church Do you feel the Catholic Church 
needs to change its ways to adapt to 
life in the 21st Century? 
Gay Marriage While some states have legalized gay 
marriage, others are still opposed to 
it.  Do you think either side is right or 
wrong? 
Privacy Rights Recently, school officials prevented a 
school shooting because one of the 
shooters posted a myspace bulletin.  
Do you think this was an invasion of 
privacy? 
Legalization of 
Marijuana 
The city of Denver has decided to 
legalize small amounts of marijuana 
for persons over 21.  How do you feel 
about this? 
War in Iraq The controversial war in Iraq has 
made news headlines almost every 
day since it began.  How do you feel 
about the war? 
Gender  
Discrimination 
Do you feel that gender discrimina-
tion is still an issue in the present-day 
United States? 
Table 2. Topics 
 
The corpus was created in three phases 
(Goldstein-Stewart, 2008).  In Phase I, emails, 
essays and interviews were collected.  In Phase 
II, blogs and chat and discussion groups were 
created and samples collected.  For blogs, sub-
jects blogged over a period of time and could 
read and/or comment on other subjects? blogs in 
their own blog.  A graduate research assistant 
acted as interviewer and discussion and chat 
group moderator. 
Of the 24 subjects who completed Phase I, 7 
decided not to continue into Phase II.  Seven 
additional students were recruited for Phase II.  
In Phase III, these replacement students were 
then asked to provide samples for the Phase I 
genres.  Four students fully complied, resulting 
in a corpus with a full set of samples for 21 
subjects, 11 women and 10 men. 
All audio recordings, interviews and discus-
sions, were transcribed.  Interviewer/moderator 
comments were removed and, for each discus-
sion, four individual files, one for each partici-
pant?s contribution, were produced. 
Our data is somewhat homogeneous: it sam-
ples only undergraduate university students and 
was collected in controlled settings.  But we be-
lieve that controlling the topics, genres, and de-
mographics of subjects allows the elimination of 
many variables that effect communicative style 
and aids the identification of common features. 
3 Corpus Statistics 
3.1 Word Count 
The mean word counts for the 21 students per 
genre and per topic are shown in Figures 1 and 2, 
respectively.  Figure 1 shows that the students 
produced more content in the directly interactive 
genres ? interview and discussion (the spoken 
genres) as well as chat (a written genre). 
 
 
Figure 1.  Mean word counts for gender and genre 
 
 
Figure 2.  Mean word counts for gender and topic 
338
 
The email genre had the lowest mean word 
count, perhaps indicating that it is a genre in-
tended for succinct messaging. 
3.2 Word Usage By Individuals 
We performed an analysis of the word usage of 
individuals.  Among the top 20 most frequently 
occurring words, the most frequent word used by 
all males was ?the?.  For the 11 females, six most 
frequently used ?the?, four used ?I?, and one 
used ?like?.  Among abbreviations, 13 individu-
als used ?lol?.  Abbreviations were mainly used 
in chat.  Other abbreviations were used to vary-
ing degrees such as the abbreviation ?u?.  Emoti-
cons were used by five participants.   
4 Classification 
4.1 Features 
Frequencies of words in word categories were 
determined using Linguistic Inquiry and Word 
Count (LIWC).  LIWC2001 analyzes text and 
produces 88 output variables, among them word 
count and average words per sentence.  All oth-
ers are percentages, including percentage of 
words that are parts of speech or belong to given 
dictionaries (Pennebaker, et al, 2001).  Default 
dictionaries contain categories of words that in-
dicate basic emotional and cognitive dimensions 
and were used here.  LIWC was designed for 
both text and speech and has categories, such 
negations, numbers, social words, and emotion.  
Refer to LIWC (www.liwc.net) for a full descrip-
tion of categories.  Here the 88 LIWC features 
are denoted feature set L. 
From the original 24 participants? documents 
and the new 7 participants? documents from 
Phase II, we aggregated all samples from all ge-
nres and computed the top 100 words for males 
and for females, including stop words.  Six 
words differed between males and females.  Of 
these top words, the 64 words with counts that 
varied by 10% or more between male and female 
usage were selected.  Excluded from this list 
were 5 words that appeared frequently but were 
highly topic-specific: ?catholic?, ?church?, ?ma-
rijuana?, ?marriage?, and ?school.? 
Most of these words appeared on a large stop 
word list (www.webconfs.com/stop-words.php).  
Non-stop word terms included the word ?feel?, 
which was used more frequently by females than 
males, as well as the terms ?yea? and ?lot? (used 
more commonly by women) and ?uh? (used 
more commonly by men).  Some stop words 
were used more by males (?some?, ?any?), oth-
ers by females (?I?, ?and?).  Since this set mainly 
consists of stop words, we refer to it as the func-
tional word features or set F. 
The third feature set (T) consisted of the five 
topic specific words excluded from F. 
The fourth feature set (S) consisted of the stop 
word list of 659 words mentioned above. 
The fifth feature set (I) we consider informal 
features.  It contains nine common words not in 
set S: ?feel?, ?lot?, ?uh?, ?women?, ?people?, 
?men?, ?gonna?, ?yea? and ?yeah?.  This set alo 
contains the abbreviations and emotional expres-
sions ?lol?, ?ur?, ?tru?, ?wat?, and ?haha?. Some 
of the expressions could be characteristic of par-
ticular individuals. For example the term ?wat? 
was consistently used by one individual in the 
informal chat genre. 
Another feature set (E) was built around the 
emoticons that appeared in the corpus.  These 
included ?:)?, ?:(?, ?:-(?, ?;)?, ?:-/?, and ?>:o)?. 
For our results, we use eight feature set com-
binations: 1. All 88 LIWC features (denoted L); 
2. LIWC and functional word features, (L+F); 3. 
LIWC plus all functional word features and the 
topic words (L+F+T); 4. LIWC plus all function-
al word features and emoticons (L+F+E); 5. 
LIWC plus all stop word features (L+S); 6. 
LIWC plus all stop word and informal features 
(L+S+I); 7. LIWC supplemented by informal, 
topic, and stop word features, (L+S+I+T).  Note 
that, when combined, sets S and I cover set F. 
4.2 Classifiers 
Classification of all samples was performed us-
ing four classifiers of the Weka workbench, ver-
sion 3.5 (Witten and Frank, 2005).  All were 
used with default settings except the Random 
Forest classifier (Breiman, 2001), which used 
100 trees.  We collected classification results for 
Na?ve-Bayes, J48 (decision tree), SMO (support 
vector machine) (Cortes and Vapnik, 1995; Platt, 
1998) and RF (Random Forests) methods. 
5 Person Identification Results 
5.1 Cross Validation Across Genres 
To identify a person as the author of a text, six 
fold cross validation was used.  All 756 samples 
were divided into 126 ?documents,? each con-
sisting of all six samples of a person?s expression 
in a single genre, regardless of topic.  There is a 
baseline of approximately 5% accuracy if ran-
domly guessing the person.  Table 3 shows the 
339
accuracy results of classification using combina-
tions of the feature sets and classifiers. 
The results show that SMO is by far the best 
classifier of the four and, thus, we used only this 
classifier on subsequent experiments.  L+S per-
formed better alone than when adding the infor-
mal features ? a surprising result. 
Table 4 shows a comparison of results using 
feature sets L+F and L+F+T.  The five topic 
words appear to grant a benefit in the best trained 
case (SMO). 
Table 5 shows a comparison of results using 
feature sets L+F and L+F+E, and this shows that 
the inclusion of the individual emoticon features 
does provide a benefit, which is interesting con-
sidering that these are relatively few and are typ-
ically concentrated in the chat documents. 
 
Feature SMO RF100 J48 NB 
L 52 30 15 17 
L+F 60 44 21 25 
L+S 71 42 19 33 
L+S+I 71 39 17 33 
L+S+I+T 71 40 17 33 
Table 3. Person identification accuracy (%) using six 
fold cross validation 
 
Feature SMO RF100 J48 NB 
L+F 60 44 21 25 
L+F+T 67 40 21 25 
Table 4. Accuracy (%) using six fold cross validation 
with and without topic word features (T) 
 
Feature SMO RF100 J48 NB 
L+F 60 44 21 25 
L+F+E 65 41 21 25 
Table 5. Accuracy (%) using six fold cross validation 
with and without emoticon features (E) 
5.2 Predict Communicant in One Genre 
Given Information on Other Genres 
The next set of experiments we performed was to 
identify a person based on knowledge of the per-
son?s communication in other genres.  We first 
train on five genres, and we then test on one ? a 
?hold out? or test genre.   
Again, as in six fold cross validation, a total of 
126 ?documents? were used: for each genre, 21 
samples were constructed, each the concatena-
tion of all text produced by an individual in that 
genre, across all topics.  Table 6 shows the re-
sults of this experiment.  The result of 100% for 
L+F, L+F+T, and L+F+E in email was surpris-
ing, especially since the word counts for email 
were the lowest.  The lack of difference in L+F 
and L+F+E results is not surprising since the 
emoticon features appear only in chat docu-
ments, with one exception of a single emoticon 
in a blog document (?:-/?), which did not appear 
in any chat documents.  So there was no emoti-
con feature that appeared across different genres. 
 
SMO HOLD OUT (TEST GENRE) 
Features A B C D E S I 
L 60 76 52 43 76 81 29 
L+F 75 81 57 48 100 90 71 
L+F+T 76 86 62 52 100 86 71 
L+F+E 75 81 57 48 100 90 71 
L+S 82 81 67 67 86 90 100 
L+S+I 79 86 52 57 86 90 100 
L+S+I+T 81 86 52 67 90 90 100 
Table 6.  Person identification accuracy (%) training 
with SMO on 5 genres and testing on 1. A=Average 
over all genres, B=Blog, C=Chat, D=Discussion, 
E=Email, S=Essay, I=Interview 
 
Train Test L+F L+F+T 
CDSI Email 67 95 
BDSI Email 71 52 
BCSI Email 76 100 
BCDI Email 57 90 
BCDS Email 57 81 
Table 7. Accuracy (%) using SMO for predicting 
email author after training on 4 other genres. B=Blog, 
C=Chat, D=Discussion, S=Essay, I=Interview 
 
We attempted to determine which genres were 
most influential in identifying email authorship, 
by reducing the number of genres in its training 
set.  Results are reported in Table 7.  The differ-
ence between the two sets, which differ only in 
five topic specific word features, is more marked 
here.  The lack of these features causes accuracy 
to drop far more rapidly as the training set is re-
duced.  It also appears that the chat genre is im-
portant when identifying the email genre when 
topical features are included.  This is probably 
not just due to the volume of data since discus-
sion groups also have a great deal of data.  We 
need to investigate further the reason for such a 
high performance on the email genre. 
The results in Table 6 are also interesting for 
the case of L+S (which has more stop words than 
L+F).  With this feature set, classification for the 
interview genre improved significantly, while 
that of email decreased.  This may indicate that 
the set of stop words may be very genre specific 
? a hypothesis we will test in future work.  If this 
in indeed the case, perhaps certain different sets 
340
of stop words may be important for identifying 
certain genres, genders and individual author-
ship.  Previous results indicate that the usage of 
certain stop words as features assists with identi-
fying gender (Sabin, et al, 2008). 
Table 6 also shows that, using the informal 
words (feature set I) decreased performance in 
two genres: chat (the genre in which the abbrevi-
ations are mostly used) and discussion.  We plan 
to run further experiments to investigate this.  
The sections that follow will typically show the 
results achieved with L+F and L+S features.  
 
Train\Test B C D E S I 
Blog 100 14 14 76 57 5 
Chat 24 100 29 38 19 10 
Discussion 10 5 100 5 10 29 
Email 43 10 5 100 48 0 
Essay 67 5 5 33 100 5 
Interview 5 5 5 5 5 100 
Table 8. Accuracy (%) using SMO for predicting per-
son between genres after training on one genre using 
L+F features 
 
Table 8 displays the accuracies when the L+F 
feature set of single genre is used for training a 
model tested on one genre.  This generally sug-
gests the contribution of each genre when all are 
used in training.  When the training and testing 
sets are the same, 100% accuracy is achieved.  
Examining this chart, the highest accuracies are 
achieved when training and test sets are textual.  
Excluding models trained and tested on the same 
genre, the average accuracy for training and test-
ing within written genres is 36% while the aver-
age accuracy for training and testing within spo-
ken genres is 17%.  Even lower are average ac-
curacies of the models trained on spoken and 
tested on textual genres (9%) and the models 
trained on textual and tested on spoken genres 
(6%). This indicates that the accuracies that fea-
ture the same mode (textual or spoken) in train-
ing and testing tend to be higher. 
Of particular interest here is further examina-
tion of the surprising results of testing on email 
with the L+F feature set. Of these tests, a model 
trained on blogs achieved the highest score, per-
haps due to a greater stylistic similarity to email 
than the other genres.  This is also the highest 
score in the chart apart from cases where train 
and test genres were the same.  Training on chat 
and essay genres shows some improvement over 
the baseline, but models trained with the two 
spoken genres do not rise above baseline accura-
cy when tested on the textual email genre. 
5.3 Predict Communicant in One Topic 
Given Information on Five Topics 
This set of experiments was designed to deter-
mine if there was no training data provided for a 
certain topic, yet there were samples of commu-
nication for an individual across genres for other 
topics, could an author be determined? 
 
SMO HOLD OUT (TEST TOPIC) 
Features Avg Ch Gay Iraq Mar Pri Sex 
L+F 87 81 95 86 95 100 67 
L+F+T 65 76 71 86 29 62 67 
L+F+E 87 81 95 86 95 95 67 
L+S 94 95 95 81 100 100 95 
Table 9.  Person identification accuracy (%) training 
with SMO on 5 topics and testing on 1. Avg = Aver-
age over all topics: Ch=Catholic Church, Gay=Gay 
Marriage, Iraq=Iraq War, Mar=Marijuana Legaliza-
tion, Pri=Privacy Rights, Sex=Sex Discrimination 
 
Again a total of 126 ?documents? were used: 
for each topic, 21 samples were constructed, 
each the concatenation of all text produced by an 
individual on that topic, across all genres.  One 
topic was withheld and 105 documents (on the 
other 5 topics) were used for training.  Table 9 
shows that overall the L+S feature set performed 
better than either the L+F or L+F+T sets.  The 
most noticeable differences are the drops in the 
accuracy when the five topic words are added, 
particularly on the topics of marijuana and priva-
cy rights.  For L+F+T, if ?marijuana? is withheld 
from the topic word features when the marijuana 
topic is the test set, the accuracy rises to 90%.  
Similarly, if ?school? is withheld from the topic 
word features when the privacy rights topic is the 
test set, the accuracy rises to 100%.  This indi-
cates the topic words are detrimental to deter-
mining the communicant, and this appears to be 
supported by the lack of an accuracy drop in the 
testing on the Iraq and sexual discrimination top-
ics, both of which featured the fewest uses of the 
five topic words.  That the results rise when us-
ing the L+S features shows that more features 
that are independent of the topic tend to help dis-
tinguish the person (as only the Iraq set expe-
rienced a small drop using these features in train-
ing and testing, while the others either increased 
or remained the same).  The similarity here of the 
results using L+F features when compared to 
L+F+E is likely due to the small number of emo-
ticons observed in the corpus (16 total exam-
ples).  
341
5.4 Predict Communicant in a Speech Ge-
nre Given Information on the Other  
One interesting experiment used one speech ge-
nre for training, and the other speech genre for 
testing.  The results (Table 10) show that the ad-
ditional stop words (S compared to F) make a 
positive difference in both sets.  We hypothesize 
that the increased performance of training with 
discussion data and testing on interview data is 
due to the larger amount of training data availa-
ble in discussions.  We will test this in future 
work. 
 
Train Test L+F L+S 
Inter Disc 5 19 
Disc Inter 29 48 
Table 10.  Person identification accuracy (%) training 
and testing SMO on spoken genres 
5.5 Predict Authorship in a Textual Genre 
Given Information on Speech Genres  
Train Test L+F L+S 
Disc+Inter Blog 19 24 
Disc+Inter Chat 5 14 
Disc+Inter Email 5 10 
Disc+Inter Essay 10 29 
Table 11.  Person identification accuracy (%) training 
SMO on spoken genres and testing on textual genres 
 
Table 11 shows the results of training on speech 
data only and predicting the author of the text 
genre.  Again, the speech genres alone do not do 
well at determining the individual author of the 
text genre.  The best score was 29% for essays. 
5.6 Predict Authorship in a Textual Genre 
Given Information on Other Textual 
Genres  
Table 12 shows the results of training on text 
data only and predicting authorship for one of the 
four text genres.  Recognizing the authors in chat 
is the most difficult, which is not surprising since 
the blogs, essays and emails are more similar to 
each other than the chat genre, which uses ab-
breviations and more informal language as well 
as being immediately interactive. 
 
Train Test L+F L+S 
C+E+S Blog 76 86 
B+E+S Chat 10 19 
B+C+S Email 90 81 
B+C+E Essay 90 86 
Table 12.  Person identification accuracy (%) train-
ing and testing SMO on textual genres 
5.7 Predict Communicant in a Speech Ge-
nre Given Information on Textual Ge-
nres 
Training on text and classifying speech-based 
samples by author showed poor results.  Similar 
to the results for speech genres, using the text 
genres alone to determine the individual in the 
speech genre results in a maximum score of 29% 
for the interview genre (Table 13). 
 
Train Test L+F L+S 
B+C+E+S Discussion 14 23 
B+C+E+S Interview 14 29 
Table 13. Person identification accuracy (%) training 
SMO on textual genres and testing on speech genres 
5.8 Error Analysis 
Results for different training and test sets vary 
considerably.  A key factor in determining which 
sets can successfully be used to train other sets 
seems to be the mode, that is, whether or not a 
set is textual or spoken, as the lowest accuracies 
tend to be found between genres of different 
modes.  This suggests that how people write and 
how they speak may be somewhat distinct. 
Typically, more data samples in the training 
tends to increase the accuracy of the tests, but 
more features does not guarantee the same result.  
An examination of the feature sets revealed fur-
ther explanations for this apart from any inherent 
difficulties in recognizing authors between sets.  
For many tests, there is a tendency for the same 
person to be chosen for classification, indicating 
a bias to that person in the training data.  This is 
typically caused by features that have mostly, but 
not all, zero values in training samples, but have 
many non-zero values in testing.  The most 
striking examples of this are described in 5.3, 
where the removal of certain topic-related 
features was found to dramatically increase the 
accruacy.  Targetted removal of other features 
that have the same biasing effect could increase 
accuracy. 
While Weka normalizes the incoming features 
for SMO, it was also discovered that a simple 
initial normalization of the feature sets by 
dividing by the maximum or standardization by 
subtracting the mean and dividing by the 
standard deviation of the feature sets could 
increase the accuracy across the different tests.  
6 Conclusion 
In this paper, we have described a novel unique 
corpus consisting of samples of communication 
342
of 21 individuals in six genres across six topics 
as well as experiments conducted to identify a 
person?s samples within the corpus.  We have 
shown that we can identify individuals with rea-
sonably high accuracy for several cases: (1) 
when we have samples of their communication 
across genres (71%), (2) when we have samples 
of their communication in specific genres other 
than the one being tested (81%), and (3) when 
they are communicating on a new topic (94%). 
For predicting a person?s communication in 
one text genre using other text genres only, we 
were able to achieve a good accuracy for all 
genres (above 76%) except chat.  We believe this 
is because chat, due to its ?real-time 
communication? nature is quite different from 
the other text genres of emails, essays and blogs. 
Identifying a person in one speech genre after 
training with the other speech genre had lower 
accuracies (less than 48%).  Since these results 
differed significantly, we hypothesize this is due 
to the amount of data available for training ? a 
hypothesis we plan to test in the future.  
Future plans also include further investigation 
of some of the suprising results mentioned in this 
paper as well investigation of stop word lists 
particular to communicative genres.  We also 
plan to investigate if it is easier to identify those 
participants who have produced more data 
(higher total word count) as well as perform a 
systematic study the effects of the number of 
words gathered on person identificaton. 
?n addition, we plan to investigate the efficacy 
of using other features besides those available in 
LIWC, stopwords and emoticons in person 
identification.  These include spelling errors, 
readability measures, complexity measures, 
suffixes, and content analysis measures. 
References 
Naomi S. Baron. 2003. Why email looks like speech. 
In J. Aitchison and D. M. Lewis, editors, New Me-
dia Language. Routledge, London, UK. 
Douglas Biber. 1988. Variation across speech and 
writing. Cambridge University Press, Cambridge, 
UK. 
Leo Breiman. 2001. Random forests. Technical Re-
port for Version 3, University of California, Berke-
ley, CA. 
Rafael A. Calvo, Jae-Moon Lee, and Xiaobo Li. 2004. 
Managing content with automatic document classi-
fication. Journal of Digital Information,  5(2). 
Corinna Cortes and Vladimir Vapnik. 1995. Support 
vector networks. Machine Learning, 20(3):273-
297. 
Nikolas Coupland, Justine Coupland, Howard Giles, 
and Karen L. Henwood. 1988. Accommodating the 
elderly: Invoking and extending a theory, Lan-
guage in Society, 17(1):1-41. 
David Crystal. 2001. Language and the Internet. 
Cambridge University Press, Cambridge, UK. 
Olivier de Vel, Alison Anderson, Malcolm Corney, 
George Mohay. 2001. Mining e-mail content for 
author identification forensics, In SIGMOD: Spe-
cial Section on Data Mining for Intrusion Detec-
tion and Threat Analysis. 
Jade Goldstein-Stewart, Gary Ciany, and Jaime Car-
bonell. 2007. Genre identification and goal-focused 
summarization, In Proceedings of the ACM 16th 
Conference on Information and Knowledge Man-
agement (CIKM) 2007, pages 889-892. 
Jade Goldstein-Stewart, Kerri A. Goodwin, Roberta 
E. Sabin, and Ransom K. Winder. 2008. Creating 
and using a correlated corpora to glean communic-
ative commonalities. In LREC2008 Proceedings, 
Marrakech, Morocco. 
Susan Herring. 2001. Gender and power in online 
communication. Center for Social Informatics, 
Working Paper, WP-01-05.  
Susan Herring. 1996. Two variants of an electronic 
message schema. In Susan Herring, editor, Com-
puter-Mediated Communication: Linguistic, Social 
and Cross-Cultural Perspectives. John Benjamins, 
Amsterdam, pages 81-106. 
Shawndra Hill and Foster Provost. 2003. The myth of 
the double-blind review? Author identification us-
ing only citations. SIGKDD Explorations. 
5(2):179-184. 
Moshe Koppel, Shlomo Argamon, and Anat Rachel 
Shimoni. 2002. Automatically categorizing written 
texts by author gender. Literary and Linguistic 
Computation. 17(4):401-412. 
Ivan Krsul and Eugene H. Spafford. 1997.  Author-
ship analysis: Identifying the author of a program. 
Computers and Security 16(3):233-257. 
Shlomo Levitan and Shlomo Argamon. 2006.  Fixing 
the federalist: correcting results and evaluating edi-
tions for automated attribution. In Digital Humani-
ties, pages 323-328, Paris. 
LIWC, Linguistic Inquiry and Word Count. 
http://www.liwc.net/ 
David Madigan, Alexander Genkin, David Lewis, 
Shlomo Argamon, Dmitriy Fradkin, and Li Ye. 
2005. Author identification on the large scale. 
Proc. of the Meeting of the Classification Society 
of North America.  
343
Philip M. McCarthy, Gwyneth A. Lewis, David F. 
Dufty, and Danielle S. McNamara. 2006. Analyz-
ing writing styles with Coh-Metrix, In Proceedings 
of AI Research Society International Conference 
(FLAIRS), pages 764-769. 
Niamh McCombe. 2002. Methods of author identifi-
cation, Final Year Project, Trinity College, Ireland. 
Thomas C. Mendenhall. 1887. The characteristic 
curves of composition. Science, 9(214):237-249. 
Frederick Mosteller and David L. Wallace. 1964.  
Inference and Disputed Authorship: The Federal-
ist. Addison-Wesley, Boston. 
James W. Pennebaker, Martha E. Francis, and Roger 
J. Booth. 2001. Linguistic Inquiry and Word Count 
(LIWC): LIWC2001. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ. 
John C. Platt. 1998. Using sparseness and analytic QP 
to speed training of support vector machines. In M. 
S. Kearns, S. A. Solla, and D. A. Cohn, editors, 
Advances in Neural Information Processing Sys-
tems 11. MIT Press, Cambridge, Mass. 
Roberta E. Sabin, Kerri A. Goodwin, Jade Goldstein-
Stewart, and Joseph A. Pereira. 2008. Gender dif-
ferences across correlated corpora: preliminary re-
sults. FLAIRS Conference 2008, Florida, pages 
207-212. 
Marina Santini.  2007.  Automatic Identification of 
Genre in Web Pages. Ph.D., thesis,  University of 
Brighton, Brighton, UK.  
Michael Shepherd and Carolyn Watters. 1999. The 
functionality attribute of cybergenres. In Proceed-
ings of the 32nd Hawaii International Conf. on 
System Sciences (HICSS1999), Maui, HI. 
Rob Thomson and Tamar Murachver. 2001. Predict-
ing gender from electronic discourse. British Jour-
nal of Social Psychology. 40(2):193-208. 
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques 
(Second Edition). Morgan Kaufmann, San Francis-
co, CA. 
Simeon J. Yates. 1996. Oral and written linguistic 
aspects of computer conferencing: a corpus based 
study. In Susan Herring, editor, Computer-
mediated Communication: Linguistic, Social, and 
Cross-Cultural Perspectives. John Benjamins, 
Amsterdam, pages 29-46. 
344
