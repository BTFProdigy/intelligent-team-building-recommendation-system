Extended Models  and Tools for High-performance Part-of-speech 
Tagger 
Masayuki Asahara and Yuji Matsumoto 
Graduate School of Information Science, Nara Institute of Science and Technology 
8916-5, Taka.yama-cho, Ikoma-shi, Nara, 630-0101, Japan 
{masayu-a,matsu}@is. a ist -nara,  ac. jp 
Abst rac t  
Statistical part-of-st)eeeh(POS) taggers achieve high 
accuracy and robustness when based oil large, scale 
maimally tagged eorl)ora. Ilowever, enhancements 
of the learning models are necessary to achieve bet- 
ter 1)erforma.nce. We are develol)ing a learning 
tool for a Jalmnese morphological analyzer called 
Ch, aScn. Currently we use a fine-grained POS tag 
set with about 500 tags. To al)l)ly a normal tri- 
gram model on the tag set, we need unrealistic size 
of eorl)ora. Even, for a hi-gram model, we ean- 
no~, 1)ret)are a ll loderate size of an mmotated cor- 
pus, when we take all the tags as distinct. A usual 
technique to Col)e with such fine-grained tags is to 
reduce the size of the tag set 1)y grouping the set 
of tags into equivalence classes. We introduce the 
concept of position-wise 9rouping where the tag set 
is t)artitioned into dill'el'lint equivalence classes at 
each t)osition in the. conditional 1)rohabilities in the 
Markov Model. Moreover, to eoi)e with the data 
Sl)arsen(?ss prot)lem caused 1) 3, exceptional t)henon> 
ena, we introduce several other techniques uch as 
word-level statistics, smoothing of word-level an(l 
P()S-level statistics and a selective tri-gram model. 
To help users determine probabilistic 1)arameters, we 
introduce an error-driven method for the pm'mneter 
selection. We then give results of exl)eriments to see 
the effect of the tools applied to an existing Jat)anese 
morphological nalyzer. 
1 Introduction 
Along with the increasing awfilability of mmotated 
eorl)ora, a number of statistic P()S tat ters  have 
been developed which achieve high accuracy and ro- 
bustness. On the other hand, there is still continu- 
ing demand for the iinprovement of learning lnod- 
els when sufficient quantity of annotated corpora 
are not available in the users domains or languages. 
Flexible tools for easy tuning of leanfing models 
are in demand. We present such tools in this pa- 
per. Our tools are originally intended for use with 
the Japanese morphological nalyzer, ChaSen (Mat- 
sumoto et al, 1999), which at present is a statis- 
tical tagger based on the w~riable memory length 
Marker Model (lion et al, 1.994). We first give a 
brief overview of the features of the learning tools. 
The t)art-of-speech tag set we use is a slightly 
modified version of tile IPA POS tag set (RWCP, 
2000) with about 500 distinct POS tags. The real 
tag set is even larger since some words are treated 
as distim't P()S tags. The size of the tag set is unre- 
alistic for buihting tri-grmn rules and even bi-gram 
rules which take all the tags as distinct. The usual 
technique for coping with such fine-grained tags is to 
reduce the size of the tag set by groul)ing the set of 
tags into equivalence classes (Jelinek, 1.998). We in- 
troduce the concept of position-wise grouping where 
the tag set is partitioned into different equivalence 
(;lasses at each position in the conditiolml probabili- 
lies in the Marker Model. This feature is especially 
useflfl for ,lapanese language analysis ince Jal)anese 
is a highly (:onjugated language, where conjugation 
fOl'lllS have a great etfeet on the succeeding mor- 
1)homes, trot have little to do with the t)receding nlor- 
phemes. Moreover, in colloquial anguage, a number 
of eolltrael;ed expressio11s are eoinmon, where two or 
more morphemes are central:ted into a single word. 
The contracted word behaves as belonging to dif- 
ferent t)arts-of-st)eech by connecting to the previous 
word or to the next word. Position-wise grouping en- 
ables users to grouI) such words differently according 
to the positions in which they appear. 
Data sparseness i  always a serious problem when 
dealing with a large tag set. Since it is unrealistic to 
adopt a simple POS tri-gram model to our tag set, 
we base our model on a hi-gram model and augment 
it with selective tri-grams. By selective tri-gram, we 
mean that only special contexts are conditioned by 
tri-gram model and are mixed with the ordinary bi- 
grmn model. We also incorporate some smoothing 
techniques for coping with the data sparseness prob- 
lel l l .  
By eolnbining these methods, we constructed the 
learning tools for a high-lmrformance statistical mor- 
phological analyzer that are able to learn the prob- 
ability i)arameters with only a moderate size tagged 
('orl)us. 
The rest of this paper is structured as follows. 
21 
Section 2 discusses the basic concet)ts of tile statisti- 
cal morphological nalysis and some problems of the 
statistical approach. Section 3 presents the charac- 
teristics of the our learning tools. Section 4 reports 
the result of some experiments and tile accuracy of 
the tagger in several settings. Section 5 discusses re- 
lated works. Finally, section 6 gives conclusions and 
discusses future works. 
Throughout this paper, we use morphological 
analysis instead of part-of-speecll tagging since 
Japanese is an agglutinative language. This is the 
standard ternfinology in Japanese literatures. 
2 Pre l iminar ies  
2.1 Stat i s t i ca l  morphological analysis 
The POS tagging problem or the Japanese morpho- 
logical analysis problem must do tokenization and 
find the sequence of POS tags T = t l , .  ?., t:,~ tot the 
word sequence W = wl , . . . ,  w,~ in the int)ut string 
S. Tile target is to find T that maxinfizes tile fol- 
lowing probability: 
Using the Bayes' rule of probability theory, 
P (W,T)  can be decomposed as a sequence of tile 
products of tag probabilities and word probabilities. 
P (TIW) P(T, W) = argmax P(W) 
= argu~}.xP(T,W) 
= .,': uv/xP(WlT)F(T) 
We assumed that tile word probability is con- 
strained only by its trig, and that the tag probability 
is constrained only by its preceding tags, either with 
the t)i-grmn or the tri-gram model: 
P(WIT)  = HP(wi lt i )  
i=1 
P(T) = f l  P(tilti_l) 
i=1 
P(T) = fl P(ti\[ti-2,i-1) ) 
i=1 
The values are estimated from tile frequencies in 
tagged corpora using maximum likelihood estima- 
tion: 
p(w lti) - F (w"L)  r(t ) 
F( t i _ , , td  
P(t lt -l) - 
F(t;i-2,1,i-1, ti) = 
F(ti-2, ti-1) 
Using these parameters, tim most probable tag se- 
quence is determined using the Viterbi algorithm. 
2.2 H ierarch ica l  Tag Set 
We use the IPA POS tag set (RWCP, 2000). This 
tag set consist of three eleinents: tile part -of  speech, 
the type of conjugation and tile form of conjugation 
(the latter two elements are necessary only for words 
that conjugate). 
Tile POS tag set has a hierarchical structure: The 
top POE level consists of 15 categories(e.g., Noun, 
Verb, . . .  ). The second and lower levels are th, e sub- 
division level. For example, Noun is fllrther subdi- 
vided into common nouns(general), llroper nomls, 
numerals, and so on. Proper Noun is sul)divided 
into General, Person, Organization and Place. Per- 
son and Place are subdivided again. The bottom 
level of tile subdivision level is th.c word level, which 
is conceptually regarded as a part of the subdivision 
level. 
In the Japanese language, verbs, adjectives and 
auxiliary verbs have conjugation. These are catego- 
rized into a fixed set of conjugation types(CTYPE),  
each of which has a fxed set of conjugal;ion 
forms(CFORM). It is known that in Japanese that 
the CFORM varies according to the words appear- 
ing in the succeeding position. Thus, at tile condi- 
tional position of the estimated tag probabilities, the 
CFORM plays an important role, while in the case 
of other positions, they need not be distinguished. 
Figure 1 illustrates tile structure of the tag set. 
2.3 P rob lems in s tat is t ica l  mode ls  
On the one hand, most of the i)rol)lems in statistical 
natural language processing stem fi'om the sparse- 
hess of training data. In our case, tile nuinber of the 
most fine-grained tags (disregarding the word level) 
is about 500. Even when we use the bi-gram model, 
we suffer from the data sparseness problem. The 
situation is nmch worse in the case of the tri-grmn 
model. This may be remedied by reducing tile tag 
set by grouping the tags into a smaller tag set. 
On the other hand, there are various kinds of ex- 
ceptions in language phenomena. Some words have 
different contextual features fi'om others in the same 
tag. Such exceptions require a word or some group of 
words to be taken itself as a distinct part-of-speech 
or its statistics to be taken in distinct contexts. In 
our statistical earning tools, those exceptions are 
handled by position-wise grouping, word-level statis- 
tics, smoothing of word-level and POS-level, and se- 
lective tri-gram model, which are described in turn 
ill the next section. These features enable users to 
22 
POS 
.,o 17571 
"rile subdivision level ~ I  I I I 
l he  word level GL~ WI~yO\] NL~ . . . . . . . . . .  
(;TYPE Godul>"K" G?dan"lS" I l l l  No Conjugation No Conjugation 
Type Type 
tbrm form .... No Conjugation No Conjugation 
Figure 1: The examples of the hierarchical tag set 
adjust tile balance between fine and coarse grained 
model settings. 
3 Features  o f  the  too ls  
This section overviews characteristic timtures of tim 
learning tools for coping with the above, mentioned 
prolflems. 
a.:t Pos i t ion -w ise  group ing  of  POS tags 
Since we use a very fine-grained tag set, it is impor- 
tant to classit'y them into some equiva.lence classes 
l;o reduce the, size. of i)rotmbilistic lmramelers. More- 
over, as is discussed in the 1)revious ection, some 
words or P()S bo, lmves ditli;rently according to Ihe 
l)osition they at)pear. In 3at)aneso, tbr instance, 
the CF()I/M play an iml)ortmlt role only to dis- 
mnbiguate the words at their succeeding position. 
In other words, the CFORM should be taken into 
account only when they appear at tim position of 
1,i-1 in either bi-gram or tri-grain model ( t i - i  in 
I'(t~lt~__~ ) and P(t,\]t~_.,,t~_l)). This means that 
when the statistics of verbs are, taken, they should be 
grouped diflbrently according to the positions. Not(', 
that, we named the positions; The current position 
means the position of ti in the hi-gram statistics 
P(tiIti-1) or the tri-grmn statistics P(till, i_.,, ti-~). 
The preceding position means the position of ti-1. 
The second preceding position means the position of 
ti-.2. 
There are quite a few contracted t~rms ill col- 
loquial expressions. For example, auxiliary verb 
"chau" is a contracted tbrms consisting of two words 
"te(particle) + simau(auxiliary verb)" and behaves 
quite differently from other words. One way to learn 
its statistical behavior is to collect various us~ges of 
the word and add the data to the training data after 
correctly mmotating them. In contrast, the idea of 
point-wise grouping provides a nice alternative so- 
hltion to this problem. By simply group this word 
into the same equivalence class of "te" for the cur- 
relfl; 1)osition I,i and grou I) it into the same equiva- 
le, nt class of "simau" for the t)rece(ling position ti-1 
in P ( t i \ ] t i - .  ), it learns the statistical behavior from 
these classes. 
We now describe the point-wise grouping ill a 
nlore precise way. l?or simplicity, we assume, bi- 
gram model. Let 7- = {A,/3,---} be ttw, original 
tag set. \?e introduce two partitions of the tag set, 
one is fin' the current position T ~ = {A (',/)~,..-}, 
mid the other is for the preceding 1)osition T v = 
{AV,13v, .. .}. We define the equivalence mal>l)ing 
of the current position: I ( ' (T -~ T"), and another 
mapping of the t)rece(ling position: U'('\]~ -4 "yv). 
lqgure, 2 shows an exalnple of the lmrtitiollS by 
those, mapl)ings, where the equivalence mappings 
I c = {el --> A c,L? -9 A",C ~ A':,\]) -+ B",E -5 
W, . . .}  
fv = {A --+ AV, \]\] -4 AJ', C -4 B v, D --+ B v, E -~ 
Cl,...} 
Supl)ose we express the equivalence class to which 
the tag t, belongs as It\]" for the current position and 
\[t,\]v for the preceding position, then: 
= F(w , \[td 
\[W) 
1)(ti l l ,  i _ l )  = 
3.2 Word- leve l  s tat is t ics  
Seine words behave ditt'erently flom other words 
even in tile same POS. Especially Japanese particles, 
auxiliary verbs a.nd some affixes are known to have 
different e(mtextual behavior. The tools can define 
23 
03 
g 
ku 
o - -  
b 
(D 
The Preceding Positon Tag Set 
ALB C~Dlc ? FIG\[ H--- 
-~- -  B"  / D" 
- -~ . . . . . . . . . .  ', . . . . . . . . . . . . . . . .  i . . . . . .  i . . . . . . . . . . . .  
. . . . .  ~ . . . . . . . . . . .  ~ . . . . . . . . . . . . . .  i . . . . . .  i . . . . . . . .  
03 
F- 
o = < 
n 
"E 
B 
O 
I- 
The Precedin( Position Tag Set 
Wb 1 
B 
iil..ll,ll.q Wbr, B ,~ 
Figure 2: Position-wise grouping of tags 
some words as distinct POS and their statistics are 
taken individually. 
The tag set T extends to a new tag set T ~xt that 
defines some words as individual POSs (the word 
level). Modification to the probability formulas for 
such word level tags is straightforward. 
Note that the statistics for POS level should be 
modified when some words in the same group are 
individuated. Suppose that the tags A and B are 
defined in tile T and some words l , l~ , . . . ,  l'l~,~ E A 
and l, tZb~ , . . . , I:F~,,~ C 17 arc individuated in 7" ~xt. \Ve 
define tags Ae,~,l, Bext~ C T ~:t as follows: 
Figure 3: the word extended tag set 
We define two smoothing coetIicients: A~ is the 
smoothing ratio for the current position and /~j, is 
the smoothing ratio of the preceding position. Those 
values can be defined for each word. 
Suppose the word wi is individuated and its POS 
is ti. If the current position is smoothed, then the 
tag probability is defined as follows (note that wi 
itself is an individuated tag): 
/5(wilti_l ) = ((1 - A~)P(tilti_\]) + A~P(wilti_l)) 
If the word at the preceding positions is smoothed 
(assume ti-\] is the POS of wi-1): 
. ao~ = ,4  \ 0v ,~, . . . ,~ , ,~ , ,}  
Bcxt = /) \ {wv~,...,wt,,,~} 
To estimate the probability for tile comlection 
A-B, tile frequency F(Aea:t, Bext)  is used rather than 
the total frequency F(A ,  B) .  Figure 3 illustrate the 
tag set extension of this situation. 
These tag set extension is actually a special case of 
position-wise grouping. The equivalence mappings 
are fl'om all word level tags to T ~t.  The mapping 
I ~ maps all the words ill A~xt into A~t  and maps 
each of {W~, . . . ,  14(~., } into itself. In the same way, 
I p maps all the words in B~.~,t into B~:~t and maps 
each of {Wb,, . . . ,  Wb,, } into itself. 
3.3 Smooth ing  of  word and POS level 
stat ist ics 
When a word is individuated while its occurrence 
frequency is not high, x~e have to accumulate in- 
stances to obtain enough statistics. Another solu- 
tion is to smooth the word level statistics with POS 
level statistics. In order to back-off the st)arseness of
the words, we use the statistics of the POS to which 
the words belong. 
P( t i \ ]w i -1 )  = (1 - ~Xp)P(t i l t i - l )  + A~,F(t i lwi-~) 
If the both words of the positions is extend: 
~-  /~p( (1  - -  ~c)\]~(ti\[~Oi_l ) ~- )tcr('ll)i\]'ll)i_ 1 )) 
+(1 - Av)((1 - A~)P(t i l t i_ \ ]  ) + A~P(wi l t i - t ) )  
3.4 Select ive t r i -gram mode l  
Simple tri-gram models are not feasible for a large 
tag set. As a matter of fact, only limited eases re- 
quire as long contexts as tri-grams. We 1)rot)ose to 
take into account Olfly limited tri-gram instances, 
which we call sclcctive tri-flrams. Our model is 
a mixture of such tri-gram statistics with bi-gram 
ones .  
The idea of mixture of different context length 
is not new. Markov Models with varial)le memory 
length are proposed by Ron(Ron et al, 1994), in 
whictl a mixtm'e model of n-grams with various value 
of n is presented as well as its learning algorithms. 
In such a model, the set of contexts (the set of states 
of the automata) should be mutually disjoint for the 
automata to be deterministic and well-defined. 
We give a little different interpretation to tri-grmn 
statistics. We consider a tri-grmn as an exceptional 
24 
,, ......... ,o,,,o. A IA~ 
C The preceding position A I B 
F- 
B hIE 
CA,.....;;....,. A B I( ....... 
Figure 4: Selective tr i-gram 
context. Wheii a bi-grani context and a tri-grani 
context have sonie intersection, the tri-gram context 
is regarded as an exception within the, l)i-graui con- 
text. In this sense, all tim cont, exts are, mutual ly 
disjoint as well in our niodel, and it is possii)h, to 
convert our model into Ron's tormulal;ion. I{owevei, 
we think that oul" %rnmlation is iilore straighforward 
if the longer COlltex(;s ~-/1"(! interln'eted as exc, el)i;ions 
to (;lie shorter (;onte, xts. 
\Ve, assume that th(; grouping at the current 1)o- 
sit;ion ('7 -~) share the same grouping of the t)i-grain 
case. But for the l)re(:eding l)osition and (;lie s(!(:ond 
1)receding 1)osition, we can deline ditl'erent groupings 
of tag sets fl'om those of the bi-gram case. We intro- 
duce the two new tag sets tbr the preceding positions: 
The tag sol; of the preceding position: 
"P/- {W", S/,...} 
The tag set of the s(!(',()ii(l pre(:(!ding l>().~ition: 
.T),, ' _ { A I',' , H~,#. . .}  
We define the equiv~tlence mal)l)ing for the 1)re -
ceding position: I p' (7- 4 "Y p'), and tile nml)ping for 
tile second 1)receding position: I s';/ (7- -4  "Y Jm' ). As- 
stoning that an equivalence classes for 1, detined by 
the mapping I pp' is expressed as \[t\] pj/, the, tri-grani 
t)robal)ility is defined natural ly as fl)llows: 
P(t i l t~- ' , ,  t i - i  ) - -  s ' ( \ [ I ,d" lb ,<- . _ , \ ] ' " " , \ [ l ,~- , \ ] ' " )  
F(\[t.~_.4'"', \[l.~_, \],", It,;\]") 
F(\[ti_~\]m", rl. ,1,.>'~ L ' t - - J  J \] 
Figure 4 shows an image of fl'equency counts for 
tr i-gram model. 
In case some hi-gram COlltext overlaps with a tri- 
grmn context, the bi-graln statistics are taken by 
excluding the tri-gram statistics. 
For (:xmnl)h:, if we inchide (.lie tri-grmn context 
A - C - \]7 in our model, then the slat,)sties of the hi- 
grail) COilteX(; C-  13 is taken as folh>ws (F  stands for 
true, frequency in training corpora while F '  stands 
for estimated frequency to lie used for 1)robat.>ility 
calculation): 
s,"(c, . )  = s~'(c, J3) - F (A ,  C, ix) 
Since selection of t i i -gram contexts is not easy 
task, the tools supports the selection based on ml 
error-driven method. We omit the detail because of 
the sl)a(:e limitation. 
3.5 Est i inat io l i  for unseen  words  in eor i )us 
Since not all the words in (;lie dictionary appear 
in the training corpus, |;lie occurrence probability 
i>f miseen words should 1)e allocated ill $Olil(: way. 
There are a number of method for estimating un- 
se, en events. Our era'rent ool adopts Lidstone's law 
of succession, wlfieh add ;~ fixed (:omit to each obser- 
vat)Oil. 
~'('.,10 = F(,., ,  t) + ~ 
E, ,~ F(,.,  t) + ~: . Itl 
At 1)resent, l;he de, fault frequency (:omit, (t is set (o 
0.5. 
4 Exper iments  and  Eva luat ion  
For evaluating how the 1)rol)osed extension lint)roves 
a normal t)i-glain model, we condllcted several ex- 
periments. We group verl)s according (o the con- 
jugation forms at the preceding i/osition, take word 
level statistics for all l/articles, auxiliary verbs and 
synll)ols, each of which is smoothed with the illlliie,- 
dial:ely higher P()S level. Selective, tri-grani contexts 
are defined for dist:riniinating a few notoriously ;lil/- 
hi~uous particle "no" and auxiliary ve, i'l)s "nai" and 
"aruY This is a very simple extension but suffices 
tbr evaluating the ett'ect of the learning tools. 
We use 5-tbld cross ewfluation over (;he RWCP 
tagged corpus (RXVCP, 2000). The corpus (:late size 
is 37490 sentences(958678 words). The errors of the 
corpus are manually rood)tied. The annotated cor- 
pus is divided into the traiifing data set(29992 sen- 
tences, 80%) and the test data set(7498 se, ntence, s
20%). Experinients were repeated 5 times, and the 
reslllts \v(;r(; averaged. 
The, evaluation is done at the following 3 levels: 
? le, vell: only word segmentation (tokenizati(m) 
is ewduated 
? level2: word segmentation mid (;lie toI) level 
part-of-speech are ewfluated 
? level3: all infornmtion is taken into at, count for 
evaluation 
Using the tools, we create the following six models: 
D: hernial bi-granl model 
D,,, :  D + word level statistics for particles, etc. 
25 
Table 1: Results for test data (F-value %) 
dataset 
D 98.69 98.12 96.91 
Dw 98.75 98.24 97.22 
Dw.~t 98.80 98.26 97.20 
Dws 98.76 98.27 97.23 
Dwqt 98.78 98.35 97.27 
Table 2: Results for learning data (F-value %) 
dataset 
D 98.84 98.36 92.36 
D,o 98.96 98.58 97.81 
Dwq 98.92 98.46 97.6t 
Dw~ 98.96 98.58 97.80 
D,vgt 98.92 98.55 97.70 
Dwo: Dw + groupilLg 
Dw,: D,o + smoothing of word level with POS 
level 
D,~,at: Dwo + selective tri-grmn 
The smoothing rate between the part-of-sl)eech 
and the words is fixed to 0.9 for each word. 
To evahmte the results, we use tile F-value defined 
by the tbllowing formulae: 
number of cor rect  words 
12ccall = 
number of words in  corpus 
number of cor rec t  words  
Precision = number of words by system output 
(/32 + 1) - l~,ecall. Precision 
F~ = /32 ? (Precision + 12ccall) 
For each model, we evaluate the F-value (with ~ = 
1) for tim learlfing data and test data at; each level. 
The results are given in the Tables 1 and 2. 
From the results the tbllowing observation is pos- 
sible: 
Smoothing improve on grouping dataset in test 
data slightly. But in tile other enviromnents the 
accuracy isn't improved. In this experiment, the 
smoothing rate for all words is fixed. We need to 
make the different rate for each word in the future 
work. 
The grouping performs good result for the test 
dataset. It is natural that the grouping is not good 
for learning dataset since all the word level statistics 
are learned in the case of learning dataset. 
Finally, the selective tri-gram (only 25 rules 
added) achieves non-negligible improvement at 
level2 and level3. Compared with the normal bi- 
gram Inodel, it improves about 0.35% on level3 and 
about 0.2% on level2. 
5 Re la ted  work  
Cutting introduced grouping of words into equiv- 
a.lence classes based on the set of possible tags to 
reduce the number of the parameters (Cutting et 
al., 1992) . Schmid used tile equivaleuce classes for 
smoothing. Their classes define not a partition of 
POS tags, but mixtures of some POS tags (Schmid, 
1995). 
Brill proposed a transfbrmation-based method. In 
the selection of tri-gram contexts we will use a sim- 
ilar technique (Brill, 1995) . 
Haruno constructed variable length models based 
on the mistake-driven methods, and mixed these tag 
models. They do not have grouping or smoothing 
facilities (Haruno and Matsumoto, 1997). 
Kitauchi presented a method to determine refine- 
meat of the tag set by a mistake-driven technique. 
Their inethod determines the tag set according to 
the hierarchical definition of tags. Word level dis- 
crimination and grouping beyond the hierarchical 
tag structure are out of scope of their method (Ki- 
tauchi el; al., 1999). 
6 Conc lus ion  and  Future  works  
We proposed several extensions to the statistical 
model for Japanese morphological nalysis. We also 
gave preliminary experiments and showed tile effects 
of the extensions. 
Counting some words individually and smooth- 
ing them with POS level statistics alleviate the data 
sparseness problem. Position-wise grouping enables 
an eflk~ctive refiimment of the probability parameter 
settings. Using selective tri-grain provides an easy 
description of exceptional language phenomena. 
In our future work, we will develol) a method to re- 
fine the models automatically or semi-automatically. 
For example, error-driven methods will be applica- 
ble to the selection of the words to be individuated 
and the useflfl tri-gram contexts. 
For the morphological nalyzer Ch, aSen, we are us- 
ing the mixture modeh Position-wise grouping used 
for conjugation. Smoothing of tile word level and 
the POS level used tbr particles. 
The analyzer and the learning tools are available 
publicly i . 
References  
E. Brill. 1995. Transformation-Based Error-Driven 
Learning and Natural Language Processing: A 
Case Study ill Part-ofSpeecll Tagging. Compu- 
tational Linguistics, 21(4):543 565. 
D. Cutting, J. Kut)iec, J. Pedersen, and P. Sibun. 
1992. A pracl;ical part-of-speech tagger. In Pro- 
cccdings of the Third Conference on Applied Nat- 
ural Language Processing. 
1 http : l / e l .  aist-nara, ac. jpllab/nlt/chasen/ 
26 
M. tlaruno and 3;. Matsumoto. 1997. Mistake- 
\])riven Mixtur(; of Iti(;rarchical '. ?~g; Contcxl; Tre(~s. 
In 35th, Annual Meeting of the Association for 
(7om, puational Linguistics and 8th Co'nfcv('.ncc of 
th, c European Chapter of tit(: Association for Com- 
putational Linguistics, 1)ages 230 237, July. 
F. Jelinek. 1998. Statistical Methods for @cech 
Recognition. MIT Press. 
A. Kitauchi, T. Utsm'o, and Y. Matsmnoto. 1999. 
Probabilistic Model Le~arning tbr JatmlmSC' Mor- 
1)hological Analysis 1)3' lgrror-driven Feat;,rc Se- 
lection (in .lal)mmse). 'J}'(t~t.sa(:l/io'l~, f \]'nfi)rmatio'n 
l'roccssi'ng Sci('ty of ,\]apa'n, 40(5):2325 2337, 5. 
Y. Matsmnoto, A. Kitau('hi, T. Ymmtshita, Y. 1\]i- 
mno, H. M~tsuda, and 54. Asahm'a. 1999. 
Japanese MorphologicM Analyzer ChaSen Users 
Ma.mml version 2.0. Technical l/el)oft NAIST-IS- 
T1~99012, Nma Institute of Science mM ~ibx:lmol- 
ogy ~l~(:lmicM lR,eport. 
l). I~.on, Y. Singer, and N. Tishby. 1994. \]A~A/I'll - 
ing Prolml)ilistic Automal a with Vm'iM)lc Memory 
Length. In COLT-g4, tinges 35 ~16. 
\]/,WCP. 2000. I{\YC Tt~xt l)atabas(~. 
http:/ /www, rwcp. or.  j p /wswg/rwcdb/ text / .  
H. Schmid. 1995. Inproveln(;nts In l)art-of-S1)e(.,(:h 
Tagging With an Applic~tion To (-lermmL In 
IM6'L SIGDA'\]' workshop, tinges ~'17- 50. 
27 
Japanese Unknown Word Identification by Character-based Chunking
Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology, Japan
{masayu-a,matsu}@is.naist.jp
Abstract
We introduce a character-based chunking for un-
known word identification in Japanese text. A major
advantage of our method is an ability to detect low
frequency unknown words of unrestricted character
type patterns. The method is built upon SVM-based
chunking, by use of character n-gram and surround-
ing context of n-best word segmentation candidates
from statistical morphological analysis as features.
It is applied to newspapers and patent texts, achiev-
ing 95% precision and 55-70% recall for newspa-
pers and more than 85% precision for patent texts.
1 Introduction
Japanese and Chinese sentences are written without
spaces between words. A word segmentation pro-
cess is a prerequisite for natural language process-
ing (NLP) of non-segmented language family. Sta-
tistical morphological analyzers are often used for
word segmentation in Japanese NLP, which achieve
over 96% precision. However, unknown word pro-
cessing still remains an issue to be addressed in
those morphological analyzers. Unknown word
processing in non-segmented languages are more
challenging, as it first needs to identify boundaries
of unknown words in texts, prior to assignment of
correspoinding part-of-speech.
Unknown word processing in morphological
analysis of non-segmented language can follow one
of either approaches: modular or embedded. In
the modular approach, a separate off-line module
is used to extract unknown words from text (Mori
1996; Ikeya 2000). They are checked and added to
the lexicon of morphological analyzers. In the em-
bedded approach, an on-line module which statisti-
cally induces the likelihood of a particular string be-
ing a word is embedded in a morphological analyzer
(Nagata, 1999; Uchimoto et al, 2001). A modular
approach is generally preferable in practice, since
it allows developers to maintain a high quality lex-
icon which is crucial for good performance. Previ-
ous work of the modular approach was either un-
able to detect low frequency unknown words (Mori
1996) or limited to predefined character patterns for
low frequency unknown words (Ikeya 2000).
We propose a general-purpose unknown word
identification based on character-based chunking in
order to address these shortcomings. A cascade
model of a morphological analyzer (trained with
Markov Model) and a chunker (trained with Sup-
port Vector Machines) is applied. The morpho-
logical analyzer produces n-best word segmenta-
tion candidates, from which candidate segmentation
boundaries, character n-gram and surrounding con-
texts are extracted as features for each character.
The chunker determines the boundaries of unknown
words based on the features.
The rest of this paper is as follows. We describe
our method in Section 2, and present experimental
results on newspaper articles and patent text in Sec-
tion 3. Related work is provided in Section 4, and a
summary and future directions are given in Section
5.
2 Method
We describe our method for unknown word identifi-
cation. The method is based on the following three
steps:
1. A statistical morphological analyzer is applied
to the input sentence and produces n-best seg-
mentation candidates with their correspoinding
part-of-speech (POS).
2. Features for each character in the sentence are
annotated as the character type and multiple
POS tag information according to the n-best
word candidates.
3. Unknown words are identified by a support
vector machine (SVM)-based chunker based
on annotated features.
Now, we illustrate each of these three steps in
more detail.
2.1 Japanese Morphological Analysis
Japanese morphological analysis is based on
Markov model. The goal is to find the word and
POS tag sequences W and T that maximize the fol-
lowing probability:
T = argmax
W,T
P (T |W ).
Bayes? rule allows P (T |W ) to be decomposed as
the product of tag and word probabilities.
argmax
W,T
P (T |W ) = argmax
W,T
P (W |T )P (T ).
We introduce approximations that the word prob-
ability is conditioned only on the tag of the word,
and the tag probability is determined only by the im-
mediately preceding tag. The probabilities are esti-
mated from the frequencies in tagged corpora using
Maximum Likelihood Estimation. Using these pa-
rameters, the most probable tag and word sequences
are determined by the Viterbi algorithm.
In practice, we use log likelihood as cost. Max-
imizing probabilities means minimizing costs. Re-
dundant analysis outputs in our method mean the
top n-best word candidates within a certain cost
width. The n-best word candidates are picked up
for each character in the ascending order of the ac-
cumulated cost from the beginning of the sentence.
Note that, if the difference between the costs of
the best candidate and n-th best candidate exceeds
a predefined cost width, we abandon the n-th best
candidate. The cost width is defined as the lowest
probability in all events which occur in the train-
ing data. We use ChaSen 1 as the morphological
analyzer. ChaSen induces the n-best segmentation
within a user-defined width.
2.2 Feature for Chunking
There are two general indicators of unknown words
in Japanese texts. First, they have highly ambiguous
boundaries. Thus, a morphological analyzer, which
is trained only with known words, often produces a
confused segmentation and POS assignment for an
unknown word. If we inspect the lattice built during
the analysis, subgraphs around unknown words are
often dense with many equally plausible paths. We
intend to reflect this observation as a feature and do
this by use of n-best candidates from the morpho-
logical analyzer. As shown Figure 1, each charac-
ter (Char.) in an input sentence is annotated with a
1http://chasen.naist.jp/
feature encoded as a pair of segmentation tag and
POS tag. For example, the best POS of the char-
acter ??? is ?GeneralNoun-B?. This renders as the
POS is a common noun (General Noun) and its seg-
mentation makes the character be the first one in a
multi-character token. The POS tagset is based on
IPADIC (Asahara and Matsumoto, 2002) and the
segmentation tag is summarized in Table 1. The 3-
best candidates from the morphological analyzer is
used. The second indicator of Japanese unknown
words is the character type. Unknown words oc-
cur around long Katakana sequences and alphabet-
ical characters. We use character type (Char. Type)
as feature, as shown in Figure 1. Seven charac-
ter types ar defined: Space, Digit, Lowercase al-
phabet, Uppercase alphabet, Hiragana, Katakana,
Other (Kanji). The character type is directly or in-
directly used in most of previous work and appears
an important feature to characterize unknown words
in Japanese texts.
Table 1: Tags for positions in a word
Tag Description
S one-character word
B first character in a multi-character word
E last character in a multi-character word
I intermediate character in a multi-character
word (only for words longer than 2 chars)
2.3 Support Vector Machine-based Chunking
We use the chunker YamCha (Kudo and Matsumoto,
2001), which is based on SVMs (Vapnik, 1998).
Suppose we have a set of training data for a bi-
nary class problem: (x
1
, y
1
), . . . , (xN , yN ), where
xi ? Rn is a feature vector of the i th sample in
the training data and yi ? {+1,?1} is the label of
the sample. The goal is to find a decision function
which accurately predicts y for an unseen x. An
support vector machine classifier gives a decision
function f(x) = sign(g(x)) for an input vector x
where
g(x) =
?
z
i
?SV
?iyiK(x, zi) + b.
K(x, z) is a kernel function which maps vec-
tors into a higher dimensional space. We use a
polynomial kernel of degree 2 given by K(x, z) =
(1 + x ? z)2.
SVMs are binary classifiers. We extend binary
classifiers to an n-class classifier in order to com-
pose chunking rules. Two methods are often used
Char. id Char. Char. Type POS(Best) POS(2nd) POS(3rd) unknown word tag
i? 2 ? Other PrefixNoun-S GeneralNoun-S SuffixNoun-S B
i? 1 ? Other GeneralNoun-B GeneralNoun-S SuffixVerbalNoun-S I
i ? Other GeneralNoun-E SuffixNoun-S GeneralNoun-S I
i + 1 ? Hiragana CaseParticle-S Auxil.Verb-S ConjunctiveParticle-S
i + 2 ? Other VerbalNoun-B * *
Figure 1: An example of features for chunking
for the extension, the ?One vs. Rest method? and
the ?Pairwise method?. In the ?One vs. Rest meth-
ods?, we prepare n binary classifiers between one
class and the remain classes. Whereas in the ?Pair-
wise method?, we prepare nC2 binary classifiers
between all pairs of classes. We use ?Pairwise
method? since it is efficient to train than the ?One
vs. Rest method?.
Chunking is performed by deterministically an-
notating a tag on each character. Table 2 shows the
unknown word tags for chunking, which are known
as the IOB2 model (Ramshaw and Marcus, 1995).
Table 2: Tags for unknown word chunking
Tag Description
B first character in an unknown word
I character in an unknown word (except B)
O character in a known word
We perform chunking either from the beginning
or from the end of the sentence. Figure 1 illustrates
a snapshot of chunking procedure. Two character
contexts on both sides are referred to. Information
of two preceding unknown word tags is also used
since the chunker has already determined them and
they are available. In the example, the chunker uses
the features appearing within the solid box to infer
the unknown word tag (?I?) at the position i.
We perform chunking either from the beginning
of a sentence (forward direction) or from the end of
a sentence (backward direction).
3 Experiments and Evaluation
3.1 Experiments for measuring Recall
Firstly, we evaluate recall of our method. We use
RWCP text corpus (Real World Computing Partner-
ship, 1998) as the gold standard and IPADIC (Ver-
sion 2.6.3) (Asahara and Matsumoto, 2002) as the
base lexicon. We set up two data sets based on the
hit number of a web search engine which is shown
in Appendix A. Table 3 shows the two data sets.
Words with lower hit number than the threshold are
regarded as unknown. We evaluate how many un-
known words in the corpus are identified.
Table 3: Two data for recall evaluation
data threshold # of word in the
lexicon (rate)
# of unknown word
in the corpus (rate)
A 1,000 108,471 (44.2%) 9,814 (1.06%)
B 10,000 52,069 (21.2%) 33,201 (3.60%)
Table 4: Results ? Recall by Newspaper
Token Type
Setting Rec. Prec. Rec. Prec.
A/for 55.9% 75.3% 55.8% 69.5%
A/back 53.5% 73.4% 53.8% 68.0%
B/for 74.5% 82.2% 74.2% 75.8%
B/back 72.0% 80.9% 72.0% 74.3%
We perform five fold cross validation and average
the five results. We carefully separate the data into
the training and test data. The training and test data
do not share any unknown word. We evaluate recall
and precision on both token and type as follows:
Recall = # of words correctly identified
# of words in Gold Std. Data
Precision = # of words correctly identified
# of words identified
The experiment is conducted only for recall,
since it is difficult to make fair judgment of preci-
sion in this setting. The accuracy is estimated by the
word segmentation defined in the corpus. Neverthe-
less, there are ambiguities of word segmentation in
the corpus. For example, while ????? (Kyoto
University)? is defined as one word in a corpus, ??
?/?? (Osaka University)? is defined as two words
in the same corpus. Our analyzer identifies ????
?? as one word based on generalization of ????
??. Then, it will be judged as false in this exper-
iment. We make fairer precision evaluation in the
next section. However, since several related works
make evaluation in this setting, we also present pre-
cision for reference.
Table 4 shows the result of recall evaluation. For
Table 5: Results ? Recall of each POS
POS # of token Recall
GeneralNoun 9,009 67.1
PN (First Name) 3,938 86.8
PN (Organization) 3,800 63.8
PN (Last Name) 3,717 90.4
Verb 3,446 73.4
VerbalNoun 2,895 87.5
PN (Location) 1,911 79.3
PN (Other) 1,864 58.3
AdjectiveNoun 624 83.2
PN (Country) 449 88.4
?PN? stands for ?Proper Noun? Data Set B, forward direction.
Shown POSs are higher than the rank 11th by the token sizes.
example, an experimental setting ?A/for? stands for
the data set A with a forward direction chunking,
while ?A/Back? stands for the data set A with a
backward direction chunking. Since there is no
significant difference between token and type, our
method can detect both high and low frequency
words in the corpus. Table 5 shows the recall of
each POS in the setting data set B and forward di-
rection chunking. While the recall is slightly poor
for the words which include compounds such as or-
ganization names and case particle collocations, it
achieves high scores for the words which include no
compounds such as person names. There are typi-
cal errors of conjugational words such as verbs and
adjectives which are caused by ambiguities between
conjugational suffixes and auxiliary verbs.
3.2 Experiments for measuring Precision
Secondly, we evaluate precision of our method man-
ually. We perform unknown word identification on
newspaper articles and patent texts.
3.2.1 Unknown Word Identification in
Newspapers
Firstly, we examine unknown word identification
experiment in newspaper articles. We use articles
of Mainichi Shinbun in January 1999 (116,863 sen-
tences). Note that, the model is made by RWCP text
corpus, which consists of articles of Mainichi Shin-
bun in 1994 (about 35,000 sentences).
We evaluate the models by the number of iden-
tified words and precisions. The number of iden-
tified words are counted in both token and type.
To estimate the precision, 1,000 samples are se-
lected at random with the surrounding context and
are showed in KWIC (KeyWord in Context) format.
One human judge checks the samples. When the se-
lected string can be used as a word, we regard it as
a correct answer. The precision is the percentage of
correct answers over extracted candidates.
Concerning with compound words, we reject the
words which do not match any constituent of the de-
pendency structure of the largest compound word.
Figure 2 illustrates judgment for compound words.
In this example, we permit ????? (overseas
study)?. However, we reject ????? (short-term
overseas)? since it does not compose any constitu-
tent in the compound word.
??????
Short-term overseas study
??
Short-term
????
overseas study
??
overseas
??
study abroad
OK
??????
????
??
??
??
 NG
????
Figure 2: Judgement for compound words
We make two models: Model A is composed by
data set A in Table 3 and model B is composed by
data set B. We make two settings for the direction
of chunking, forward (from BOS to EOS) and back-
ward (from EOS to BOS).
Table 6 shows the precision for newspaper arti-
cles. It shows that our method achieves around 95%
precision in both models. There is almost no differ-
ence in the several settings of the direction and the
contextual feature.
Table 6: Results ? Precision by Newspaper
# of identified words Precision
Setting Token Type
A/For 58,708 19,880 94.6%
A/Back 59,029 19,658 94.0%
B/For 142,591 41,068 95.3%
B/Back 142,696 41,035 95.5%
3.2.2 Unknown Word Identification from
Patent Texts
We also examine word identification experiment
with patent texts. We use patent texts (25,084 sen-
tences), which are OCR recognized. We evaluate
models by the number of extracted words and pre-
cisions as in the preceding experiment. In this ex-
periments, the extracted tokens may contain errors
of the OCR reader. Thus, we define three categories
for the judgment: Correct, Wrong and OCR Error.
We use the rate of three categories for evaluation.
Note that, our method does not categorize the out-
puts into Correct and OCR Error.
Table 7 shows the precision for patent texts. The
backward direction of chunking gets better score
than the forward one. Since suffixes are critical
clues for the long word identification, the backward
direction is effective for this task.
Table 7: Results ? Precision by Patent Texts
# of identified words Accuracy
Setting Token|Type Correct|Wrong|OCR Error
A/For 56,008|12,263 83.9%|15.4%|0.7%
A/Back 56,004|10,505 89.2%|10.0%|0.8%
B/For 97,296|16,526 85.6%|13.7%|0.7%
B/Back 98,826|15,895 87.0%|11.8%|1.2%
3.3 Word Segmentation Accuracy
Thirdly, we evaluate how our method improves
word segmentation accuracy. In the preceding ex-
periments, we do chunking with tags in Table 2.
We can do word segmentation with unknown word
processing by annotating B and I tags to known
words and rejecting O tag. RWCP text corpus and
IPADIC are used for the experiment. We define sin-
gle occurrence words as unknown words in the cor-
pus. 50% of the corpus (unknown words/all words=
8,274/461,137) is reserved for Markov Model esti-
mation. 40% of the corpus (7,485/368,587) is used
for chunking model estimation. 10% of the corpus
(1,637/92,222) is used for evaluation. As the base-
line model for comparison, we make simple Markov
Model using 50% and 90% of the corpus. The re-
sults of Table 8 show that the unknown word pro-
cessing improves word segmentation accuracy.
Table 8: Results ? Word Segmentation
Rec. Prec. F-Measure
Baseline (50%) 97.7% 96.5% 97.1
Baseline (90%) 97.8% 96.6% 97.2
Our Method 98.5% 98.1% 98.3
4 Related Work
Mori (1996) presents a statistical method based on
n-grammodel for unknownword identification. The
method estimates how likely the input string is to be
a word. The method cannot cover low frequency un-
known words. Their method achieves 87.4% preci-
sion and 73.2% recall by token, 57.1% precision and
69.1% recall by type2 on EDR corpus. Ikeya (2000)
presents a method to find unknown word boundaries
for strings composed by only kanji characters. The
2The evaluation of their method depends on the threshold of
the confidence F
min
in their definition. We refer the precision
and recall at F
min
= 0.25.
method also uses the likelihood based on n-gram
model. Their method achieves 62.8 (F-Measure) for
two kanji character words and 18.2 (F-Measure) for
three kanji character words in newspapers domain.
Nagata (1999) classifies unknown word types
based on the character type combination in an un-
known word. They define likelihood for each com-
bination. The context POS information is also used.
The method achieves 42.0% recall and 66.4% pre-
cision on EDR corpus 3.
Uchimoto (2001) presents Maximum Entropy
based methods. They extract all strings less than
six characters as the word candidates. Then, they
do morphological analysis based on words in lexi-
con and extracted strings. They use Kyoto Univer-
sity text corpus (Version 2) (Kurohashi and Nagao,
1997) as the text and JUMAN dictionary (Version
3.61) (Kurohashi and Nagao, 1999) as the base lex-
icon 4. The recall of Uchimoto?s method is 82.4%
(1,138/1,381) with major POS estimation. We also
perform nearly same experiment 5. The result of
our method is 48.8% precision and 36.2% recall
(293/809) with the same training data (newspaper
articles from Jan. 1 to Jan. 8, 1995) and test data
(articles on Jan. 9, 1995). When we use all of the
corpus excluding the test data, the result is 53.7%
precision and 42.7% recall (345/809).
Uchimoto (2003) also adopts their method for
CSJ Corpus (Maekawa et al 2000) 6. They present
that the recall for short words on the corpus is 55.7%
(928/1,667) (without POS information). We try to
perform the same experiment. However, we cannot
get same version of the corpus. Then, we use CSJ
Corpus ? Monitor Edition (2002). It only contains
short word by the definition of the National Institute
of Japanese Language. 80 % of the corpus is used
for training and the rest 20 % is for test. The result
is 68.4% precision and 61.1% recall (810/1,326) 7.
3They do not assume any base lexicon. Base lexicon size
45,027 words (composed by only the words in the corpus),
training corpus size 100,000 sentences, test corpus size 100,000
sentences. Unknown words are defined by single occurrence
words in the corpus.
4Base lexicon size 180,000 words, training corpus size
7,958 sentences, test corpus size 1,246 sentences OOV (out-of-
vocabulary) rate 17.7%. Unknown words are defined by single
occurrence words in the corpus.
5The difference is the definition of unknown words.
Whereas they define unknown words by the possible word form
frequency, we define ones by the stem form frequency.
6Training corpus size 744,244 tokens, test corpus size
63,037 tokens, OOV rate 1.66%.
7Training corpus size 678,649 tokens, 83,819 utterances,
test corpus size 185,573 tokens, 20,955 utterances OOV rate
0.71%. Single occurence word by the stem form is defined as
the unknown word.
Note, the version of the corpus and the definition
of unknown word are different between Uchimoto?s
one and ours.
The difference of the result may come from the
word unit definition. The word unit in Kyoto Uni-
versity Corpus is longer than the word unit in RWCP
text Corpus and the short word of CSJ Corpus.
Though our method is good at shorter unknown
words, the method is poor at longer words includ-
ing compounds.
For Chinese language, Chen (2002) introduces a
method using statistical methods and human-aided
rules. Their method achieves 89% precision and
68% recall on CKIP lexicon. Zhang (2002) shows
a method with role (position) tagging on charac-
ters in sentences. Their tagging method is based
on Markov model. The role tagging resembles our
method in that it is a character-based tagging. Their
method achieves 69.88% presicion and 91.65% re-
call for the Chinese person names recognition in the
People?s Daily. Goh (2003) also uses a character-
based position tagging method by support vector
machines. Their method achieves 63.8% precision
and 58.4% recall for the Chinese general unknown
words in the People?s Daily. Our method is one vari-
ation of the Goh?s method with redundant outputs of
a morphological analysis.
5 Summary and Future Direction
We introduce a character-based chunking method
for general unknown word identification in Japanese
texts. Our method is based on cascading a mor-
phological analyzer and a chunker. The method
can identify unknown words regardless of their oc-
curence frequencies.
Our research need to include POS guessing for
the identified words. One would argue that, once
the word boundaries are identified, the POS guess-
ing method in European language can be applied
(Brants 2000; Nakagawa 2001). In our preliminary
experiments of POS guessing, both SVM and Maxi-
mum Entropy with contextual information achieves
93% with a coarse-grained POS set evaluation, but
reaches only around 65% with a fine-grained POS
set evaluation.
The poor result may be due to the ?possibility-
based POS tagset?. The tagset is not necessarily
friendly for statistical morphological analyzer de-
velopment, but is widely used in Japanese corpus
annotation. In the scheme, the fine-grained POS
Verbal Noun in Japanese means that the word can
be used both as Verbal Noun with verb and General
Noun without verb. It is difficult to estimate the POS
Verbal Noun, if the word appear in the context with-
out verb. We are currently pursuing the research to
better estimate fine-grained POS for the possibility-
based POS tagset.
????????????
A Unknown Word Definition by Search
Engine Hits
Unknown words mean out-of-vocabulary (hereafter
OOV) words. The definition of the unknown words
depends on the base lexicon. We investigate the
relationship between the base lexicon size and the
number of OOV words. We examine how the reduc-
tion of lexicon size affects the OOV rate in a corpus.
When we reduce the size of lexicon, we reject the
words in increasing order of frequency in a corpus.
Then, we use hits on a web search engine as substi-
tutes for frequencies. We use goo8 as the search en-
gine and IPADIC (Asahara and Matsumoto, 2002)
as the base lexicon. Figure 3 shows the distribution
of the hit numbers. The x-axis is the number of hits
in the search engine. The y-axis is the number of
words which get the number of hits. The curve on
the graph is distorted at 100 at which round-off be-
gins.
Figure 3: The hits of the words in IPADIC
We reduce the size of lexicon according to the
number of hits. The rate of known words in a corpus
is also reduced along the size of lexicon. Figure 4
shows the rate of known words in RWCP text corpus
(Real World Computing Partnership, 1998). The x-
axis is the threshold of the hit number. When the hit
number of a word is less than the threshold, we re-
gard the word as an unknown word. The left y-axis
is the number of known words in the corpus. The
right y-axis is the rate of known words in the cor-
pus. Note, when the hit number of a word is 0, we
do not remove the word from the corpus, because
the word may be a stop word of the web search en-
gine.
When we reject the words less than 1,000 hits
from the lexicon, the lexicon size becomes 1/3 and
8http://www.goo.ne.jp/
Figure 4: The rate of the known words
the OOV rate is 1%. When we reject the words less
than 10,000 hits from the lexicon, the lexicon size
becomes 1/6 and the OOV rate is 3.5%. We use
these two data set, namely the lexicons and the defi-
nition of out-of-vocabulary words, for evaluation in
section 3.1 and 3.2.
References
Masayuki Asahara and Yuji Matsumoto. 2002.
IPADIC User Manual. Nara Institute of Science
and Technology, Japan.
Masayuki Asahara and Yuji Matsumoto. 2003.
Japanese Named Entity Extraction with Redun-
dant Morphological Analysis. In Proc. of HLT-
NAACL 2003, pages 8?15.
Thorsten Brants. 2000. TnT ? A Statistical Part-of-
Speech Tagger In Proc. of ANLP-2000,
Keh-Jiann Chen and Wei-Yun Ma. 2002. Un-
known Word Extraction for Chinese Documents.
In Proc. of COLING-2002, pages 169?175.
Chooi-Ling Goh, Masayuki Asahara and Yuji Mat-
sumoto. 2003. Chinese Unknown Word Identifi-
cation Using Position Tagging and Chunking. In
Proc. of ACL-2003 Interactive Poster/Demo Ses-
sions, Companion volume, pages 197?200.
Masanori Ikeya and Hiroyuki Shinnou. 2000. Ex-
traction of Unknown Words by the Probability
to Accept the Kanji Character Sequence as One
Word (in Japanese). In IPSJ SIG Notes NL-135,
pages 49?54.
Taku Kudo and Yuji Matsumoto. 2001. Chunk-
ing with Support Vector Machines. In Proc. of
NAACL 2001, pages 192?199.
Sadao Kurohashi and Makoto Nagao. 1997. Build-
ing a Japanese Parsed Corpus while Improv-
ing the Parsing System. In Proc. of NLPRS-97,
pages 451?456.
Sadao Kurohashi and Makoto Nagao. 1999.
Japanese Morphological Analysis System JU-
MAN Version 3.61. Department of Informatics,
Kyoto University, Japan.
Kikuo Maekawa, Hanae Koiso, Sasaoki Furui and
Hiroshi Isahara. 2000. Spontaneous Speech Cor-
pus of Japanese. In Proc. of LREC-2000, pages
947?952.
Shinsuke Mori and Makoto Nagao. 1996. Word
Extraction from Corpora and Its Part-of-Speech
Estimation Using Distributional Analysis. In
Proc. of COLING-96, pages 1119?1122.
Masaaki Nagata. 1999. A Part of Speech Estima-
tion Method for Japanese Unknown Words using
a Statistical Model of Morphology and Context.
In Proc. of ACL-99, pages 277?284.
Tetsuji Nakagawa, Taku Kudoh and Yuji Mat-
sumoto. 2001 Unknown Word Guessing and
Part-of-Speech Tagging Using Support Vector
Machines. In Proc. of NLPRS-2001, pages 325?
331.
Lance Ramshaw and Mitchell Marcus. 1995. Text
Chunking using Transformation-based Learning.
In Proc. of the 3rd Workshop on Very Large Cor-
pora, pages 83?94.
Real World Computing Partnership. 1998. RWC
Text Database.
Kiyotaka Uchimoto, Satoshi Sekine and Hitoshi Isa-
hara. 2001. The Unknown Word Problem: a
Morphological Analysis of Japanese Using Max-
imum Entropy Aided by a Dictionary. In Proc. of
EMNLP-2001, pages 91?99.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi
Yamada, Satoshi Sekine and Hiroshi Isahara.
2002. Morphological Analysis of the Sponta-
neous Speech Corpus. In Proc. of COLING-
2002, pages 1298?1302.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Ya-
mada, Satoshi Sekine and Hiroshi Isahara. 2003.
Morphological Analysis of a Large Spontaneous
Speech Corpus in Japanese. In Proc. of ACL-
2003, pages 479?488.
Vladimir Naumovich Vapnik. 1998. Statistical
Learning Theory. A Wiley-Interscience Publica-
tion.
Kevin Zhang, Qun Liu, Hao Zhang and Xue-Qi
Cheng. 2002. Automatic Recognition of Chi-
nese Unknown Words Based on Roles Tagging.
In Proc. of 1st SIGHAN Workshop on Chinese
Language Processing, pages 71?77.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 361?368
Manchester, August 2008
Japanese Dependency Parsing Using a Tournament Model
Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
{masakazu-i, masayu-a, matsu}@is.naist.jp
Abstract
In Japanese dependency parsing, Kudo?s
relative preference-based method (Kudo
and Matsumoto, 2005) outperforms both
deterministic and probabilistic CKY-based
parsing methods. In Kudo?s method, for
each dependent word (or chunk) a log-
linear model estimates relative preference
of all other candidate words (or chunks) for
being as its head. This cannot be consid-
ered in the deterministic parsing methods.
We propose an algorithm based on a tour-
nament model, in which the relative pref-
erences are directly modeled by one-on-
one games in a step-ladder tournament. In
an evaluation experiment with Kyoto Text
Corpus Version 4.0, the proposed method
outperforms previous approaches, includ-
ing the relative preference-based method.
1 Introduction
The shared tasks of multi-lingual dependency pars-
ing took place at CoNLL-2006 (Buchholz and
Marsi, 2006) and CoNLL-2007 (Nivre et al,
2007). Many language-independent parsing al-
gorithms were proposed there. The algorithms
need to adapt to various dependency structure
constraints according to target languages: projec-
tive vs. non-projective, head-initial vs. head-final,
and single-rooted vs. multi-rooted. Eisner (1996)
proposed a CKY-like O(n3) algorithm. Yamada
and Matsumoto (2003) proposed a shift-reduce-
like O(n2) deterministic algorithm. Nivre et al
(2003; 2004) also proposed a shift-reduce-like
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
???hito-da.(man .)????yomanai(doesn?t read)??hon-wo(books)(a)  ?He is a man who doesn?t read books.?
?????yomanai.(doesn?t read .)??hon-wo(books)??kare-wa(He)(b)  ?He doesn?t read books.?
??kare-wa(He)
Figure 1: Examples of Japanese sentences.
O(n) deterministic algorithm for projective lan-
guages. The model is enhanced for non-projective
languages by Nivre and Nilsson (2005). McDon-
ald et al (2005) proposed a method based on
search of maximum spanning trees employing the
Chu-Liu-Edmonds algorithm (hereafter ?CLE al-
gorithm?) (Chu and Liu, 1965; Edmonds, 1967).
Most Japanese dependency parsers are based on
bunsetsu units, which are similar concept to En-
glish base phrases. The constraints in Japanese
dependency structure are stronger than those in
other languages. Japanese dependency structures
have the following constraints: head-final, single-
head, single-rooted, connected, acyclic and projec-
tive. Figure 1 shows examples of Japanese sen-
tences and their dependency structures. Each box
represents a bunsetsu. A dependency relation is
represented by an edge from a dependent to its
head. Though sentence (a) is similar to sentence
(b), the syntactic structures of these two are differ-
ent, especially because ?kare-wa? directly depends
on ?yomanai? in (b) but not in (a).
In dependency parsing of Japanese, determin-
istic algorithms outperform probabilistic CKY
methods. Kudo and Matsumoto (2002) applied the
361
cascaded chunking algorithm (hereafter ?CC al-
gorithm?) to Japanese dependency parsing. Ya-
mada?s method (Yamada and Matsumoto, 2003)
employed a similar algorithm. Sassano (2004)
proposed a linear-order shift-reduce-like algorithm
(hereafter ?SR algorithm?), which is similar to
Nivre?s algorithm (Nivre, 2003). These determin-
istic algorithms are biased to select nearer candi-
date heads since they examine the candidates se-
quentially, and once they find a plausible one they
never consider further candidates.
We experimented the CLE algorithm with
Japanese dependency parsing, and found that the
CLE algorithm is comparable to or in some cases
poorer than the deterministic algorithms in our ex-
periments. Actually, the CLE algorithm is not suit-
able for some of the constraints in Japanese depen-
dency structures: head-final and projective. First,
head-final means that dependency relation always
goes from left to right. Second, since the CLE al-
gorithm may produce non-projective dependency
trees, we need to conduct projectivity check in the
algorithm.
Kudo and Matsumoto (2005) proposed a rela-
tive preference-based method (hereafter ?relative
preference method?). They defined the parsing
algorithm as series of selection steps of the most
likely head for each bunsetsu out of all candidates.
The method has so far achieved the highest ac-
curacy in the experiments with Kyoto Text Cor-
pus Version 3.0 data 1, since other deterministic
methods do not consider relative preference among
candidate heads but solely consider whether the
focused-on pair of bunsetsu?s is in a dependency
relation or not.
We propose a model that takes a bunsetsu and
two candidate heads into consideration and se-
lects the better candidate head out of those two.
This step is repeated in a step ladder tournament
to get the best candidate head (hereafter we call
this model as a ?tournament model?). The tour-
nament model was first introduced by Iida et al
(2003) for coreference resolution. We applied this
model to selecting the most plausible candidate
head for each bunsetsu except for the sentence final
one.
Section 2 describes the tournament model com-
paring with previous research. Section 3 describes
1Note: Sassano?s SR algorithm is the highest by exper-
iment with the smaller data Kyoto Text Corpus Version 2.0
Relative preference method and SR algorithm are not com-
pared directly with the same data.
???hito-da.(man .)??hon-wo(books)??kare-wa(He)Focused-ondependent Its candidate heads
The most likely candidate head
????yomanai(doesn?t read)
Figure 2: Example of a tournament.
how the tournament model is applied to Japanese
dependency parsing. Section 4 shows the results
of evaluation experiments. Section 5 shows our
current and future work, and Section 6 gives con-
clusions of this research.
2 Tournament Model
The tournament model was first introduced by
Iida et al (2003) for coreference resolution. The
model chooses the most likely candidate in a step-
ladder tournament, that is a sequence of one-on-
one games between candidate referents for a given
anaphoric expression. In each game, the winner is
chosen by a binary classifier such as SVMs.
We applied the tournament model to Japanese
dependency parsing taking into consideration
Japanese constraints. The projective constraint is
easily met. When selecting candidate heads for the
focused-on dependent, we only consider those can-
didates that introduce no crossing dependency.
Figure 2 illustrates a tournament. The focused-
on dependent bunsetsu is ?kare-wa?, and the can-
didate heads are the three bunsetsu?s on the right-
hand side: ?hon-wo?, ?yomanai? and ?hito-da?.
The first game is ?hon-wo? vs. ?yomanai?. Then
the next game is the winner of the first game vs.
?hito-da?. The winner of the second game (i.e.,
?hito-da?) is chosen as the most likely candidate
of the dependent, ?kare-wa?.
In the tournament model, the most likely head of
a given bunsetsu is determined by a series of one-
on-one games in a tournament. Below, we present
the advantages of the tournament model by com-
parison with the previous methods.
2.1 Scope of Feature Views
The CC algorithm and SR algorithm consider only
a pair of bunsetsu?s ? a dependent and its candidate
362
head ? in the parsing action determination (here-
after ?2-tuple model?). The same 2-tuple may or
may not have a dependency relation when they ap-
pear in different context. For example, both (a)
and (b) in Figure 1 include the two bunsetsu?s,
?kare-wa? and ?yomanai?; in (b) they have a de-
pendency relation, but not in (a). The 2-tuple mod-
els and relative preference method cannot discrim-
inate between these two patterns without consider-
ing contextual features 2. The tournament model
can be regarded as a ?3-tuple model,? which con-
siders three bunsetsu?s ? a dependent and two can-
didate heads. The discriminative performance of
the 3-tuple model is greater than the 2-tuple mod-
els, since it directly compares two candidate heads
and selects the one that is more plausible than the
other candidate. Consider Figure 1 again. In (a),
?kare-wa? does not depend on ?yomanai? because
there is another bunsetsu ?hito-da? which is a more
plausible head. 2-tuple models may use this infor-
mation as a contextual feature, but the effect is in-
direct. On the other hand, the tournament model
directly compares these candidates and always se-
lects the better one. The situation becomes crucial
when the true head appears outside of the context
window of the current candidate. 2-tuple models
have to select the head without consulting such in-
formation. The advantage of the tournament model
is its capability of deferring the decision by al-
ways keeping the current best candidate head. On
the other hand, a disadvantage of the tournament
model is its space and time complexity. The size of
features is larger since they come from three bun-
setsu?s. The size of training instances is also larger.
2.2 Relative Position in a Sentence
We name the two candidate heads in the 3-tuple
model as ?the nearer candidate head? and ?the far-
ther candidate head.? The dependent, the nearer
candidate head and the farther candidate head ap-
pear in this order in Japanese sentences. The order
defines the relative position of the contextual fea-
tures. The distance between the dependent and a
candidate head is another feature to represent the
relative position. In previous research, the distance
has been represented by feature buckets, such as 1,
2-5, or 6+. While for some dependents and their
heads whether the distance is 1 or not is impor-
tant, absolute distance is not so important since
2Contextual features are features neither in the dependent
nor in the candidate head(s).
Japanese is a free-order language. Relative posi-
tions are more informative since some dependents
tend to appear closer to other dependents, such
as objects that tend to appear closer to predicates
compared with other complements. The tourna-
ment model represents both the distance and rela-
tive position as features.
The deterministic algorithms are biased to select
nearer candidate heads. As most dependent and
head pairs appear within a close window, this ten-
dency does not cause many errors; deterministic
algorithms are weak at finding correct heads that
appear in a long distance as pointed out in Kudo
and Matsumoto (2005).
2.3 Relative Preferences
What the dependency parsers try to learn is rela-
tive preference of bunsetsu dependency, i.e., how
a dependent selects its head among others. The
relative preference method (Kudo and Matsumoto,
2005) learns the relative preferences among the
candidate heads by a discriminative framework.
The relative preferences are learned with the log-
linear model so as to give larger probability to
the correct dependent-head pair over any other
candidates. McDonald?s method (2005) with the
CLE algorithm learns the relative preferences by
a perceptron algorithm ? MIRA (Crammer and
Singer, 2003), so that the correct dependent-head
link receives a higher score. The tournament
model learns which candidate is more likely to be
the head between two candidates in a one-on-one
game in a tournament. Therefore, all of those pars-
ing algorithms try to learn the way to give the high-
est preference to the correct dependent-head pair
among all possibilities though in different settings.
While the relative preference method and Mc-
Donald?s method consider all candidate heads in-
dependently in a discriminative model, the tour-
nament model evaluates which candidate is more
likely to be the head between the latest winner and
the new candidate. The latest winner has already
defeated all of the preceding candidates. If the
new candidate beats the latest winner, it becomes
the new winner, meaning that it is the most pre-
ferred candidate among others so far considered.
Through this way of comparison with the runner-
up candidates, the tournament model uses richer
information in learning relative preferences than
the models in which all candidates are indepen-
dently considered.
363
// N: # of bunsetsu?s in input sentence
// true_head[j]: bunsetsu j?s head at
// training data
// gen(j,i1,i2,LEFT): generate
// an example where bunsetsu j is
// dependent of i1
// gen(j,i1,i2,RIGHT): generate
// an example where bunsetsu j is
// dependent of i2
for j = 1 to N-1 do
h = true_head[j];
for i = j+1 to h-1 do
gen(j,i,h,RIGHT);
for i = h+1 to N do
gen(j,h,i,LEFT);
end-for;
Figure 3: Pseudo code of training example gener-
ation procedure.
3 Proposed Algorithm
3.1 Training Example Generation Algorithm
As shown in Figure 3, for each dependent, we gen-
erate pairs of the correct head and all other candi-
date heads. On the example generation, the proce-
dure does not take into account the projective con-
straint; all bunsetsu?s on the right-hand side of the
focused-on dependent are candidate heads.
Table 1 shows all examples generated from two
sentences shown in Figure 1. 2-tuple models gen-
erate training examples formed as (dependent, can-
didate). So, from the sentences of Figure 1, it gen-
erates opposite classes to the pair (kare-wa, hito-
da). On the other hand, the examples generated by
the tournament model do not contain such incon-
sistency.
3.2 Parsing Algorithm
The tournament model has quite wide freeness in
the parsing steps. We introduce one of the tour-
nament algorithms, in which the dependents are
picked from right to left; and the games of the tour-
nament are performed from left to right. This pars-
ing algorithm takes into account the projective and
head-final constraints.
This algorithm is shown in Figure 4. The over-
all parsing process moves from right to left. On
selecting the head for a dependent all of the bun-
setsu?s to the right of the dependent have already
been decided. In Figure 4, the array ?head?
stores the parsed results and ensures that only non-
crossing candidate heads are taken into considera-
tion.
// N: # of bunsetsu?s in
// input sentence
// head[]: (analyzed-) head of bunsetsu
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
head[] = {2,3,...,N-1,N,EOS};
for j = N-1 downto 1 do
h = j+1;
i = head[h];
while i != EOS do
if classify(j,h,i)==RIGHT
then h = i;
i = head[i];
end-while;
head[j] = h;
end-for;
Figure 4: Pseudo code of parsing algorithm.
Note that the structure of the tournament has lit-
tle effect on the results (< 0.1) in our preliminary
experiments. We tried 2 ? 2 options: the depen-
dents are picked from right to left or from left to
right; and the games of the tournament are per-
formed from right to left or from left to right. We
choose the most natural combination for Japanese
dependency parsing, which is easy to implement.
4 Experiment
4.1 Settings
We implemented the tournament model, the CC al-
gorithm (Kudo and Matsumoto, 2002), SR algo-
rithm (Sassano, 2004) and CLE algorithm (Mc-
Donald et al, 2005) with SVM classifiers. We
evaluated dependency accuracy and sentence accu-
racy using Kyoto Text Corpus Version 4.0, which is
composed by newspaper articles. Dependency ac-
curacy is the percentage of correct dependencies
out of all dependency relations. Sentence accuracy
is the percentage of sentences in which all depen-
dencies are determined correctly. Dependency ac-
curacy is calculated excluding the rightmost bun-
setsu of each sentence. 3 Sentences that consist of
one bunsetsu are not used in our experiments.
We use January 1st to 8th (7,587 sentences) for
the training data. We use January 9th (1,213 sen-
tences), 10th (1,479 sentences) and 15th (1,179
sentences) for the test data. We use TinySVM 4
as a binary classifier. Cubic polynomial kernel is
3Most research such as Kudo?s (2005) uses this criteria.
4http://chasen.org/?taku/software/
TinySVM/
364
Sentence Focused-on dependent Left(Nearer) candidate Right(Farther) candidate Class label
(a) kare-wa hon-wo hito-da. RIGHT
(a) kare-wa yomanai hito-da. RIGHT
(a) hon-wo yomanai hito-da. LEFT
(b) kare-wa hon-wo yomanai. RIGHT
Table 1: Generated examples from sentences in Figure 1.
used for the kernel function. Cost of constraint vi-
olation is 1.0. These SVM settings are the same
as previous research (Kudo and Matsumoto, 2002;
Sassano, 2004). All experiments were performed
on Dual Core Xeon 3GHz x 2 Linux machines.
4.2 Features
Here we describe features used in our experiments.
Note that for the tournament model, features cor-
responding to candidates are created for each of
the nearer and farther candidates. We define the
information of a word as the following features:
lexical forms, coarse-grained POS tags, full POS
tags and inflected forms. We also define the infor-
mation of a bunsetsu as word information for each
of syuji and gokei. Syuji is the head content word
of the bunsetsu, defined as the rightmost content
word. Gokei is the representative function word of
the bunsetsu, defined as the rightmost functional
word.
Existence of punctuations or brackets, whether
the bunsetsu is the first bunsetsu in the sentence,
and whether it is the final bunsetsu in the sentence
are also members of information of a bunsetsu.
Standard features are the following: Informa-
tion of the dependent and the candidate heads, dis-
tance between the dependent and the candidate
heads (1, 2-5 or 6+ bunsetsu?s), all punctuations,
brackets and all particles between the dependent
and the candidate heads.
Additional features are the following: All case
particles in the dependent and the candidate heads,
information of the leftmost word in the candidate
heads, and the lexical form of the neighboring bun-
setsu to the right of the candidate heads.
Case particle features are the following: All
case particles appearing in the candidates? depen-
dent. These features are intended to take into con-
sideration the correlation between the case parti-
cles in the dependent of a head. When the head is
a verb, it has a similar effect of learning case frame
information.
Standard and additional features are introduced
by Sassano (2004). The case particle feature is
newly introduced in this paper. Features corre-
sponding to the already-determined dependency
relation are called dynamic features, and the other
contextual features are called static features. Stan-
dard and additional features are static features,
and case particle features are dynamic features.
Whether a dynamic feature is available for a pars-
ing algorithm depends on the parsing order of the
algorithm.
4.3 Parsing Accuracy
The parsing accuracies of our model and previ-
ous models are summarized in Table 2. Note that,
since the CLE algorithm is non-deterministic and
dynamic features are not available for this algo-
rithm, we use only a standard and additional fea-
ture set instead of an all feature set. By McNemar
test (p < 0.01) on the dependency accuracy, the
tournament model significantly outperforms most
of other methods except for the SR algorithm on
January 10th data with all features (p = 0.083)
and the CC algorithm on January 10th data with
all features (p = 0.099). The difference between
the tournament models with all features and with
the standard feature only is significant except for
on January 9th data (p = 0.25).
The highest dependency accuracy reported for
January 9th of Kyoto Text Corpus Version 2.0 is
89.56% by Sassano(2004)?s SR algorithm. 5
Since we don?t have the outputs of Sassano?s ex-
periments, we cannot do a McNemar test between
the tournament model and Sassano?s results. Our
model outperforms Sassano?s results by the depen-
dency accuracy, but the difference between these
two is not significant by prop test (p = 0.097).
When we add the additional and case particle
features, the improvement of our model is less than
that of other algorithms. This is interpreted that
our model can consider richer contextual informa-
5This accuracy in Sassano (2004) is not for Kyoto Text
Corpus Version 4.0 but Version 2.0 The feature set of Sas-
sano?s experiment is also different from our experiment.
365
Method Features Jan. 9th Jan.10th Jan. 15th
Tournament Standard feature only 89.89/49.63 89.63/48.34 89.40/49.70
All features 90.09/49.71 90.11/49.02 90.35/52.59
SR algorithm Standard feature only 88.18/45.92 88.80/44.76 88.03/47.24
(Sassano, 2004) All features 89.22/47.90 89.79/47.87 89.55/49.79
CC algorithm Standard feature only 88.17/45.92 88.80/44.76 88.00/47.24
(Kudo and Matsumoto, 2002) All features 89.22/47.90 89.80/47.94 89.53/49.79
CLE algorithm Standard feature only 88.64/45.34 88.16/43.14 88.07/45.21
(McDonald et al, 2005) Standard and Additional 89.21/46.83 89.05/45.03 88.90/48.43
Table 2: Dependency and sentence accuracy [%] using 7,587 sentences as training data.
tion within the algorithm itself than other models.
This result also shows that the accuracies of the
SR algorithm and CC algorithm are comparable
when using the same features. However, this does
not mean that their substantial power is compara-
ble because the parsing order limits the available
dynamic features.
4.4 Parsing Speed
Parsing time and the size of the training exam-
ples are shown in Table 3. All features were
used. The column ?# Step? represents the number
of SVM classification steps in parsing all the test
data. Time complexity of the tournament model
and CC algorithm are O(n2) and that of the SR al-
gorithm is O(n). The tournament model needs 1.7
times more SVM classification steps and is 4 times
slower than the SR algorithm. The reason for this
difference in steps (x1.7) and time (x4) is the num-
ber of training examples and features in the SVM
classification.
4.5 Comparison to Relative Preference
Method
We performed another experiment under the same
settings as Kudo?s (2005) to compare the tourna-
ment model and relative preference method. The
corpus is Kyoto Text Corpus Version 3.0 since
Kudo and Matsumoto (2005) used this corpus.
Training data is articles from January 1st to 11th
and editorials from January to August (24,263 sen-
tences). Test data is articles from January 14th
to 17th and editorials from October to December
(9,287 sentences). We did not perform parameter
engineering by development data, although Kudo
and Matsumoto (2005) performed it. The criteria
for dependency accuracy are the same as the exper-
iments above. However, the criteria for sentence
accuracy in this section include all sentences, even
if the length is one as Kudo and Matsumoto (2005)
did.
The results are shown in Table 4. Note that
Kudo and Matsumoto (2005) and our feature sets
are different. Only the CC Algorithm is tested with
both feature sets. Our feature set looks better than
Kudo?s. By McNemar test (p < 0.01) on the de-
pendency accuracy, the tournament model outper-
forms both the SR and CC algorithms significantly.
Since we don?t have the outputs of relative prefer-
ence methods, we cannot do a McNemar test be-
tween the tournament model and the relative pref-
erence methods. By prop test (p < 0.01) on the
dependency accuracy, our model significantly out-
performs the relative preference method of Kudo
and Matsumoto (2005). Though our model outper-
forms the ?combination? model of Kudo and Mat-
sumoto (2005) by the dependency accuracy, the
difference between these two is not significant by
prop test (p = 0.014). 6
Note that, a log-linear model is used in Kudo?s
experiment. The log-linear model has shorter
training time than SVM. The log-linear model re-
quires feature combination engineering by hand,
while SVMs automatically consider the feature
combination by the use of polynomial kernels.
5 Discussion and Future Work
In our error analysis, many errors are observed in
coordination structures. Sassano (2004) reported
that introduction of features of coordinated bun-
6The ?combination? model is the combination of the CC
algorithm and relative preference method. In Kudo?s exper-
iment, whereas the relative preference method outperforms
the CC algorithm for long-distance relations, it is reversed for
short-distance relations. They determined the optimal combi-
nation (the threshold set at bunsetsu length 3) using the devel-
opment set. In our experiment, the tournament model outper-
forms the CC and SR algorithms for relations of all lengths.
Therefore, the tournament model doesn?t need such ad hoc
combination.
366
Method # Step Time[s] # Example # Feature
Tournament 26396 371 374579 56165
SR algorithm (Sassano, 2004) 15641 80 94669 37183
CC algorithm (Kudo and Matsumoto, 2002) 18922 99 112759 37183
Table 3: Parsing time and the size of the training examples.
Method Features Dep. Acc. Sentence Acc.
Tournament All 91.96 57.44
SR algorithm (Sassano, 2004) All 91.48 55.67
CC algorithm (Kudo and Matsumoto, 2002) All 91.47 55.65
Combination ? CC and Relative preference Kudo?s (2005) 91.66 56.30
Relative preference (Kudo and Matsumoto, 2005) Kudo?s (2005) 91.37 56.00
CC algorithm (Kudo and Matsumoto, 2002) Kudo?s (2005) 91.23 55.59
Table 4: Dependency and sentence accuracy [%] using 24,263 sentences as training data with all features:
comparison with Kudo(2005)?s experiments.
setsu improves accuracy. In Kyoto Text Corpus
Version 4.0, coordination and apposition are anno-
tated with different types of dependency relation.
We did not use this information in parsing. A sim-
ple extension is to include those dependency types.
Another extension is to employ a coordination ana-
lyzer as a separate process as proposed by Shimbo
and Hara (2007).
Incorporating co-occurrence information will
also improve the parsing accuracy. One usage of
such information is verb-noun co-occurrence in-
formation that would represent selectional prefer-
ence for case-frame information. Abekawa and
Okumura (2006) proposed a reranking method
of k-best dependency analyzer outputs using co-
occurrence information. We have already devel-
oped a method to output k-best dependency trees.
One of our future works is to test the reranking
method using co-occurrence information on the k-
best dependency trees.
Multilingual parsing is another goal. Japanese
is a strict head-final language. However, most lan-
guages do not have such constraint. A different
parsing algorithm should be employed for other
less constrained languages so as to relax this con-
straint. A simple solution is to introduce a discrim-
ination model according to whether the head is on
the left-hand-side or on the right-hand-side of a de-
pendent. Existence of projective constraint does
not matter for the tournament model. The tourna-
ment model can be extended to relax the projec-
tive constraint. The preliminary results for English
are shown in our CoNLL Shared Task 2008 report
(Watanabe et al, 2008). The unlabeled syntac-
tic dependency accuracy of 90.73% for WSJ data
shows that the model is also effective in other (not
strictly head final, non-projective) languages. In
parsing word sequences, O(n2) time complexity
becomes a serious problem compared to parsing
bunsetsu sequences. Since a bunsetsu is a base
phrase in Japanese, the number of bunsetsu?s is
much less than the number of words. One solution
is to perform base phrase chunking in advance and
to apply dependency parsing on the base phrase se-
quences.
A reviewer pointed out similarities between our
model and RankSVM. RankSVM compares pairs
of elements to find out relative ordering between
elements. Our tournament model is a special case
where two elements are compared, but with a spe-
cific viewpoint of a focused dependent.
6 Conclusions
We proposed a Japanese dependency parsing al-
gorithm using the tournament model. The tour-
nament model is a 3-tuple bunsetsu model and
improves discriminative performance of selecting
correct head compared with the conventional 2-
tuple models. The most likely candidate head is
selected by one-on-one games in the step-ladder
tournament. The proposed model considers the
relative position between the nearer and farther
candidates. The model also considers all candi-
date heads, which are not considered in determin-
istic parsing algorithms. The tournament model
is robust for the free-order language. The accu-
367
racy of our model significantly outperforms that
of the previous methods in most experiment set-
tings. Even though the problem of parsing speed
remains, our research showed that considering
two or more candidate heads simultaneously can
achieve more accurate parsing.
References
Abekawa, Takeshi and Manabu Okumura. 2006.
Japanese Dependency Parsing Using Co-occurrence
Information and a Combination of Case Elements.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL 2006), pages 833?840.
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149?164.
Chu, Yoeng-Jin and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14:1396?1400.
Crammer, Koby and Yoram Singer. 2003. Ultraconser-
vative Online Algorithms for Multiclass Problems.
Journal of Machine Learning Research, 3:951?991.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the Natural Bureau of Standards,
71B:233?240.
Eisner, Jason M. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
COLING-96: Proceedings of the 16th Conference on
Computational Linguistics - Volume 1, pages 340?
345.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ?The Computational Treatment of
Anaphora?.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis Using Cascaded Chunking. In
CoNLL-2002: Proceedings of the Sixth Conference
on Computational Language Learning, pages 1?7.
Kudo, Taku and Yuji Matsumoto. 2005. Japanese De-
pendency Parsing Using Relative Preference of De-
pendency (in Japanese). Information Processing So-
ciety of Japan, Journal, 46(4):1082?1092.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers. In ACL-2005: Proceedings of
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 523?530.
Nivre, Joakim and Jens Nilsson. 2005. Psuedo-
Projective Dependency Parsing. In ACL-2005: Pro-
ceedings of 43rd Annual Meeting of the Association
for Computational Linguistics, pages 99?106.
Nivre, Joakim and Mario Scholz. 2004. Deterministic
Dependency Parsing of English Text. In COLING-
2004: Proceedings of the 20th International Confer-
ence on Computational Linguistics, pages 64?70.
Nivre, Joakim, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915?932.
Nivre, Joakim. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In IWPT-2003: 8th In-
ternational Workshop on Parsing Technology, pages
149?160.
Sassano, Manabu. 2004. Linear-Time Dependency
Analysis for Japanese. In COLING-2004: Proceed-
ings of the 20th International Conference on Com-
putational Linguistics, pages 8?14.
Shimbo, Masashi and Kazuo Hara. 2007. A Discrimi-
native Learning Model for Coordinate Conjunctions.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 610?619.
Watanabe, Yotaro, Masakazu Iwatate, Masayuki Asa-
hara, and Yuji Matsumoto. 2008. A Pipeline Ap-
proach for Syntactic and Semantic Dependency Pars-
ing. In Proceedings of the Twelfth Conference on
Computational Natural Language Learning (To Ap-
pear).
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In IWPT-2003: 8th International Workshop
on Parsing Technology, pages 195?206.
368
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 649?657, Prague, June 2007. c?2007 Association for Computational Linguistics
A Graph-based Approach to Named Entity Categorization in Wikipedia
Using Conditional Random Fields
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{yotaro-w,masayu-a,matsu}@is.naist.jp
Abstract
This paper presents a method for catego-
rizing named entities in Wikipedia. In
Wikipedia, an anchor text is glossed in a
linked HTML text. We formalize named en-
tity categorization as a task of categorizing
anchor texts with linked HTML texts which
glosses a named entity. Using this repre-
sentation, we introduce a graph structure in
which anchor texts are regarded as nodes.
In order to incorporate HTML structure on
the graph, three types of cliques are defined
based on the HTML tree structure. We pro-
pose a method with Conditional Random
Fields (CRFs) to categorize the nodes on
the graph. Since the defined graph may in-
clude cycles, the exact inference of CRFs is
computationally expensive. We introduce an
approximate inference method using Tree-
based Reparameterization (TRP) to reduce
computational cost. In experiments, our pro-
posed model obtained significant improve-
ments compare to baseline models that use
Support Vector Machines.
1 Introduction
Named and Numeric Entities (NEs) refer to proper
nouns (e.g. PERSON, LOCATION and ORGANI-
ZATION), time expressions, date expressions and so
on. Since a large number of NEs exist in the world,
unknown expressions appear frequently in texts, and
they become hindrance to real-world text analysis.
To cope with the problem, one effective ways to add
a large number of NEs to gazetteers.
In recent years, NE extraction has been performed
with machine learning based methods. However,
such methods cannot cover all of NEs in texts.
Therefore, it is necessary to extract NEs from ex-
isting resources and use them to identify more NEs.
There are many useful resources on the Web. We fo-
cus on Wikipedia1 as the resource for acquiring NEs.
Wikipedia is a free multilingual online encyclope-
dia and a rapidly growing resource. In Wikipedia,
a large number of NEs are described in titles of ar-
ticles with useful information such as HTML tree
structures and categories. Each article links to other
related articles. According to these characteristics,
they could be an appropriate resource for extracting
NEs.
Since a specific entity or concept is glossed in a
Wikipedia article, we can regard the NE extraction
problem as a document classification problem of the
Wikipedia article. In traditional approaches for doc-
ument classification, in many cases, documents are
classified independently. However, the Wikipedia
articles are hypertexts and they have a rich structure
that is useful for categorization. For example, hyper-
linked mentions (we call them anchor texts) which
are enumerated in a list tend to refer to the articles
that describe other NEs belonging to the same class.
It is expected that improved NE categorization is ac-
complished by capturing such dependencies.
We structure anchor texts and dependencies be-
tween them into a graph, and train graph-based
CRFs to obtain probabilistic models to estimate cat-
egories for NEs in Wikipedia.
So far, several statistical models that can cap-
1http://wikipedia.org/
649
ture dependencies between examples have been pro-
posed. There are two types of classification meth-
ods that can capture dependencies: iterative classi-
fication methods (Neville and Jensen, 2000; Lu and
Getoor, 2003b) and collective classification methods
(Getoor et al, 2001; Taskar et al, 2002). In this
paper, we use Conditional Random Fields (CRFs)
(Lafferty et al, 2001) for NE categorization in
Wikipedia.
The rest of the paper is structured as follows. Sec-
tion 2 describes the general framework of CRFs.
Section 3 describes a graph-based CRFs for NE cat-
egorization in Wikipedia. In section 4, we show
the experimental results. Section 5 describes related
work. We conclude in section 6.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are undirected graphical models that give a
conditional probability distribution p(y|x) in a form
of exponential model.
CRFs are formalized as follows. Let G = {V,E}
be an undirected graph over random variables y and
x, where V is a set of vertices, and E is a set
of edges in the graph G. When a set of cliques
C = {{yc,xc}} are given, CRFs define the con-
ditional probability of a state assignment given an
observation set.
p(y|x) = 1Z(x)
?
c?C
?(xc,yc) (1)
where ?(xc,yc) is a potential function defined
over cliques, and Z(x) =
?
y
?
c?C ?(xc,yc) is
the partition function.
The potentials are factorized according to the set
of features {fk}.
?(xc,yc) = exp
(
?
k
?kfk(xc,yc)
)
(2)
where F = {f
1
, ..., fK} are feature functions on
the cliques, ? = {?
1
, ..., ?K ? R} are the model
parameters. The parameters ? are estimated itera-
tive scaling or quasi-Newton method from labeled
data.
The original paper (Lafferty et al, 2001) fo-
cused on linear-chain CRFs, and applied them to
part-of-speech tagging problem. McCallum et al
(2003), Sutton et al(2004) proposed Dynamic Con-
ditional Random Fields (DCRFs), the generaliza-
tion of linear-chain CRFs, that have complex graph
structure (include cycles). Since DCRFs model
structure contains cycles, it is necessary to use ap-
proximate inference methods to calculate marginal
probability. Tree-based Reparameterization (TRP)
(Wainwright et al, 2003), a schedule for loopy be-
lief propagation, is used for approximate inference
in these papers.
3 Graph-based CRFs for NE
Categorization in Wikipedia
In this section we describe how to apply CRFs for
NE categorization in Wikipedia.
Each Wikipedia article describes a specific entity
or concept by a heading word, a definition, and one
or more categories. One possible approach is to clas-
sify each NE described in an article into an appropri-
ate category by exploiting the definition of the arti-
cle. This process can be done one by one without
considering the relationship with other articles.
On the other hand, articles in Wikipedia are
semi-structured texts. Especially lists (<UL> or
<OL>) and tables (<TABLE>) have an important
characteristics, that is, occurrence of elements in
them have some sort of dependencies. Structural
characteristics, such as lists (<UL> or <OL>) or
tables (<TABLE>), are useful becase their ele-
ments have some sort of dependencies.
Figure 2 shows an example of an HTML segment
and the corresponding tree structure. The first an-
chor texts in each list tag (<LI>) tend to be in the
same NE category. Such characteristics are useful
feature for the categorization task. In this paper we
focus on lists which appear frequently in Wikipedia.
Furthermore, there are anchor texts in articles.
Anchor texts are glossed entity or concept described
with links to other pages. With this in mind, our NE
categorization problem can be regarded as NE cat-
egory labeling problem for anchor texts in articles.
Exploiting dependencies of anchor texts that are in-
duced by the HTML structure is expected to improve
categorization performance.
We use CRFs for categorization in which anchor
texts correspond to random variables V in G and de-
650
Sibling ES = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) = 1, vTj = ch(pa(vTj , 1), k),
vTi = ch(pa(vTi , 1), max{l|l < k})}
Cousin EC = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) ? 2, vTi = ch(pa(vTi ), k),
vTj = ch(pa(vTj ), k), pa(vTj , d(vTj , ca(vTi , vTj ))? 1) = ch(pa(vTj , d(vTj , ca(vTi , vTj ))), k),
pa(vTi , d(vTi , ca(vTi , vTj ))? 1) = ch(pa(vTi , ca(vTi , vTj )),max{l|l < k})}
Relative ER = {(vTi , vTj )|vTi , vTj ? V T , d(vTi , ca(vTi , vTj )) = 1, d(vTj , ca(vTi , vTj )) = 3,
pa(vTj , 2) = ch(pa(vTj , 3), k), vTi = ch(pa(vTi , 1),max{l|l < k})}
Figure 1: The definitions of sibling, cousin and relative cliques, where ES , EC , ER correspond to sets which
consist of anchor text pairs that have sibling, cousin and relative relations respectively.
pendencies between anchor texts are treated as edges
E in G. In the next section, we describe the concrete
way to construct graphs.
3.1 Constructing a graph from an HTML tree
An HTML document is an ordered tree. We de-
fine a graph G = (V G , EG) on an HTML tree
T HTML = (V T , ET ): the vertices V G are anchor
texts in the HTML text; the edges E are limited to
cliques of Sibling, Cousin, and Relative, which we
will describe later in the section. These cliques are
intended to encode a NE label dependency between
anchor texts where the two NEs tend to be in the
same or related class, or one NE affects the other
NE label.
Let us consider dependent anchor text pairs in
Figure 2. First, ?Dillard & Clark? and ?country
rock? have a sibling relation over the tree structure,
and appearing the same element of the list. The latter
element in this relation tends to be an attribute or a
concept of the other element in the relation. Second,
?Dillard & Clark? and ?Carpenters? have a cousin
relation over the tree structure, and they tend to have
a common attribute such as ?Artist?. The elements in
this relation tend to belong to the same class. Third,
?Carpenters? and ?Karen Carpenter? have a relation
in which ?Karen Carpenter? is a sibling?s grandchild
in relation to ?Carpenters? over the tree structure.
The latter elements in this relation tends to be a con-
stituent part of the other element in the relation. We
can say that the model can capture dependencies by
dealing with anchor texts that depend on each other
as cliques. Based on the observations as above, we
treat a pair of anchor texts as cliques which satisfy
the condtions in Figure 1.
<UL>
<LI>
<A>
<LI> <LI>
<A>
<A>
<UL><A>
Dillard & 
Clark
country 
rock
Carpenters
Karen 
Carpenter
Sibling
Cousin
Relative
 Dillard & Clark ??
?country rock?
 Carpenters
 Karen Carpenter
Figure 2: Correspondence between tree structure
and defined cliques.
Now, we define the three sorts of edges given an
HTML tree. Consider an HTML tree T HTML =
(V T , ET ), where V T and ET are nodes and edges
over the tree. Let d(vTi , vTj ) be the number of edges
between vTi and vTj where vTi , vTj ? V T , pa(vTi , k)
be k-th generation ancestor of vTi , ch(vTi , k) be
vTi ?s k-th child, ca(vTi , vTj ) be a common ances-
tor of vTi , vTj ? V T . Precise definitions of cliques,
namely Sibling, Cousin, and Relative, are given in
Figure 1. A set of cliques used in our graph-based
CRFs are edges defined in Figure 1 and vertices, i.e.
C = ES ? EC ? ER ? V . Note that they are re-
stricted to pairs of the nearest vertices to keep the
graph simple.
3.2 Model
We introduce potential functions for cliques to de-
fine conditional probability distribution over CRFs.
Conditional distribution over label set y given ob-
651
servation set x is defined as:
p(y|x) = 1Z(x)
?
?
?
(vi,vj)?ES?EC?ER
?SCR(yi, yj)
?
?
?
?
?
vi?V
?V (yi,x)
?
? (3)
where ?SCR(yi, yj) is the potential over sibling,
cousin and relative edges, ?V (yi,x) is the potential
over the nodes, and Z(x) is the partition function.
The potentials ?SCR(yi, yj) and ?V (yi,x) factor-
ize according to the features fk and weights ?k as:
?SCR (yi, yj) = exp
(
?
k
?kfk(yi, yj)
)
(4)
?V (yi,x) = exp
(
?
k?
?k?fk?(yi,x)
)
(5)
fk(yi, yj) captures co-occurrences between labels,
where k ? {(yi, yj)|Y ? Y} corresponds to the par-
ticular element of the Cartesian product of the label
set Y . fk?(yi,x) captures co-occurrences between
label yi ? Y and observation features, where k? cor-
responds to the particular element of the label set
and observed features.
The weights of a CRF, ? = {?k, . . . , ?k? , . . .}
are estimated to maximize the conditional log-
likelihood of the graph in a training dataset
D = {?x(1), y(1)?, ?x(2), y(2)?, . . . , ?x(N), y(N)?}
The log-likelihood function can be defined as fol-
lows:
L? =
N
?
d=1
[
?
(vi,vj)?E(d)S ?E
(d)
C ?E
(d)
R
?
k
?kfk(yi, yj)
+
?
vi?V (d)
?
k?
?k?fk?(yi,x(d)) ? logZ(x(d))]
?
?
k
?2k
2?2 ?
?
k?
?2k?
2?2 (6)
where the last two terms are due to the Gaussian
prior (Chen and Rosenfeld, 1999) used to reduce
overfitting. Quasi-Newton methods, such as L-
BFGS (Liu and Nocedal, 1989) can be used for max-
imizing the function.
3.3 Tree-based Reparameterization
Since the proposed model may include loops, it is
necessary to introduce an approximation to calculate
mariginal probabilities. For this, we use Tree-based
Reparameterization (TRP) (Wainwright et al, 2003)
for approximate inference. TRP enumerates a set of
spanning trees from the graph. Then, inference is
performed by applying an exact inference algorithm
such as Belief Propagation to each of the spanning
trees, and updates of marginal probabilities are con-
tinued until they converge.
4 Experiments
4.1 Dataset
Our dataset is a random selection of 2300 articles
from the Japanese version of Wikipedia as of Octo-
ber 2005. All anchor texts appearing under HTML
<LI> tags are hand-annotated with NE class la-
bel. We use the Extended Named Entity Hierar-
chy (Sekine et al, 2002) as the NE class labeling
guideline, but reduce the number of classes to 13
from the original 200+ by ignoring fine-grained cat-
egories and nearby categories in order to avoid data
sparseness. We eliminate examples that consist of
less than two nodes in the SCR model. There are
16136 anchor texts with 14285 NEs. The number
of Sibling, Cousin and Relative edges in the dataset
are |ES | = 4925, |EC | = 13134 and |ER| = 746
respectively.
4.2 Experimental settings
The aims of experiments are the two-fold. Firstly,
we investigate the effect of each cliques. The sev-
eral graphs are composed with the three sorts of
edges. We also compare the graph-based models
with a node-wise method ? just MaxEnt method not
using any edge dependency. Secondly, we com-
pare the proposed method by CRFs with a baseline
method by Support Vector Machines (SVMs) (Vap-
nik, 1998).
The experimental settings of CRFs and SVMs are
as follows.
CRFs In order to investigate which type of clique
boosts classification performance, we perform ex-
periments on several CRFs models that are con-
structed from combinations of defined cliques. Re-
652
SCR SC SR CR
# of loopy examples 318 (36%) 324 (32%) 101 (1%) 42 (2%)
# of linear chain or tree examples 555 (64%) 631 (62%) 2883 (27%) 1464 (54%)
# of one node examples 0 (0%) 60 (6%) 7800 (72%) 1176 (44%)
# of total examples 873 1015 10784 2682
average # of nodes per example 18.5 15.8 1.5 6.0
S C R I
# of loopy examples 0 (0%) 0 (0%) 0 (0%) 0 (0%)
# of linear chain or tree examples 2913 (26%) 1631 (54%) 237 (2%) 0 (0%)
# of one node examples 8298 (74%) 1380 (46%) 15153 (98%) 16136 (100%)
# of total examples 11211 3011 15390 16136
average # of nodes per example 1.4 5.4 1.05 1
Table 1: The dataset details constructed from each model.
sulting models of CRFs evaluated on this experi-
ments are SCR, SC, SR, CR, S, C, R and I (indepen-
dent). Figure 3 shows representative graphs of the
eight models. When the graph are disconnected by
reducing the edges, the classification is performed
on each connected subgraph. We call it an example.
We name the examples according the graph struc-
ture: ?loopy examples? are subgraphs including at
least one cycle; ?linear chain or tree examples? are
subgraphs including not a cycle but at least an edge;
?one node examples? are subgraphs without edges.
Table 1 shows the distribution of the examples of
each model. Since SCR, SC, SR and CR model have
loopy examples, TRP approximate inference is nec-
essary. To perform training and testing with CRFs,
we use GRMM (Sutton, 2006) with TRP. We set the
Gaussian Prior variances for weights as ?2 = 10 in
all models.
SC model
C
C
C C
S S
SSC
SCR model
C
C
C C
S S
SS
R
R
C
SR model
S S
SS
R
R
CR model
C
C
C C
R
R
C
S model
S S
SS
C model
C
C
C C
C
R model
R
R
I model
Figure 3: An example of graphs constructed by
combination of defined cliques. S, C, R in the
model names mean that corresponding model has
Sibling, Cousin, Relative cliques respectively. In
each model, classification is performed on each con-
nected subgraph.
SVMs We introduce two models by SVMs (model
I and model P). In model I, each anchor text is clas-
sified independently. In model P, we ordered the
anchor texts in a linear-chain sequence. Then, we
perform a history-based classification along the se-
quence, in which j ? 1-th classification result is
used in j-th classification. We use TinySVM with
a linear-kernel. One-versus-rest method is used for
multi-class classification. To perform training and
testing with SVMs, we use TinySVM 2 with a linear-
kernel, and one-versus-rest is used for multi-class
classification. We used the cost of constraint vio-
lation C = 1.
Features for CRFs and SVMs The features used
in the classification with CRFs and SVMs are shown
in Table 2. Japanese morphological analyzer MeCab
3 is used to obtain morphemes.
4.3 Evaluation
We evaluate the models by 5 fold cross-validation.
Since the number of examples are different in each
model, the datasets are divided taking the examples
? namely, connected subgraphs ? in SCR model.
The size of divided five sub-data are roughly equal.
We evaluate per-class and total extraction perfor-
mance by F1-value.
4.4 Results and discussion
Table 3 shows the classification accuracy of each
model. The second column ?N? stands for the num-
ber of nodes in the gold data. The second last row
?ALL? stands for the F1-value of all NE classes.
2http://www.chasen.org/?taku/software/
TinySVM/
3http://mecab.sourceforge.net/
653
types feature SVMs CRFs
observation definition (bag-of-words) ? ? (V )
features heading of articles
? ?
(V )
heading of articles (morphemes) ? ? (V )
categories articles
? ?
(V )
categories articles (morphemes) ? ? (V )
anchor texts
? ?
(V )
anchor texts (morphemes) ? ? (V )
parent tags of anchor texts
? ?
(V )
text included in the last header of anchor texts
? ?
(V )
text included in the last header of anchor texts(morphemes) ? ? (V )
label features between-label feature
?
(S,C,R)
previous label
?
Table 2: Features used in experiments. ?
?
? means that the corresponding features are used in classification.
The V , S, C and R in CRFs column corresponds to the node, sibling edges, cousin edges and relative edges
respectively.
CRFs SVMs
NE CLASS N C CR I R S SC SCR SR I P
PERSON 3315 .7419 .7429 .7453 .7458 .7507 .7533 .7981 .7515 .7383 .7386
TIMEX/NUMEX 2749 .9936 .9944 .9940 .9936 .9938 .9931 .9933 .9940 .9933 .9935
FACILITY 2449 .8546 .8541 .8540 .8516 .8500 .8530 .8495 .8495 .8504 .8560
PRODUCT 1664 .7414 .7540 .7164 .7208 .7130 .7371 .7418 .7187 .7154 .7135
LOCATION 1480 .7265 .7239 .6989 .7048 .6974 .7210 .7232 .7033 .7022 .7132
NATURAL OBJECTS 1132 .3333 .3422 .3476 .3513 .3547 .3294 .3304 .3316 .3670 .3326
ORGANIZATION 991 .7122 .7160 .7100 .7073 .7122 .6961 .5580 .7109 .7141 .7180
VOCATION 303 .9088 .9050 .9075 .9059 .9150 .9122 .9100 .9186 .9091 .9069
EVENT 121 .2740 .2345 .2533 .2667 .2800 .2740 .2759 .2667 .3418 .3500
TITLE 42 .1702 .0889 .2800 .2800 .3462 .2083 .1277 .3462 .2593 .2642
NAME OTHER 24 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0690 .0000
UNIT 15 .2353 .1250 .2353 .2353 .2353 .1250 .1250 .2353 .3333 .3158
ALL 14285 .7846 .7862 .7806 .7814 .7817 .7856 .7854 .7823 .7790 .7798
ALL (no articles) 3898 .5476 .5495 .5249 .5274 .5272 .5484 .5465 .5224 .5278 .5386
Table 3: Comparison of F1-values of CRFs and SVMs.
654
The last row ?ALL (no article)? stands for the F1-
value of all NE classes which have no gloss texts in
Wikipedia.
Relational vs. Independent Among the models
constructed by combination of defined cliques, the
best F1-value is achieved by CR model, followed by
SC, SCR, C, SR, S, R and I. We performed McNe-
mar paired test on labeling disagreements between
CR model of CRFs and I model of CRFs. The
difference was significant (p < 0.01). These re-
sults show that considering dependencies work pos-
itively in obtaining better accuracy than classify-
ing independently. The Cousin cliques provide the
highest accuracy improvement among the three de-
fined cliques. The reason may be that the Cousin
cliques appear frequently in comparison with the
other cliques, and also possess strong dependencies
among anchor texts. As for PERSON, better accu-
racy is achieved in SC and SCR models. In fact,
the PERSON-PERSON pairs frequently appear in
Sibling cliques (435 out of 4925) and in Cousin
cliques (2557 out of 13125) in the dataset. Also, as
for PRODUCT and LOCATION, better accuracy is
achieved in the models that contain Cousin cliques
(C, CR, SC and SCR model). 1072 PRODUCT-
PRODUCT pairs and 738 LOCATION-LOCATION
pairs appear in Cousin cliques. ?All (no article)?
row in Table 3 shows the F1-value of nodes which
have no gloss texts. The F1-value difference be-
tween CR and I model of CRF in ?ALL (no article)?
row is larger than the difference in ?All? row. The
fact means that the dependency information helps to
extract NEs without gloss texts in Wikipedia. We
attempted a different parameter tying in which the
SCR potential functions are tied with a particular ob-
servation feature. This parameter tying is introduced
by Ghamrawi and McCallum (2005). However, we
did not get any improved accuracy.
CRFs vs. SVMs The best model of CRFs (CR
model) outperforms the best model of SVMs (P
model). We performed McNemar paired test on la-
beling disagreements between CR model of CRFs
and P model of SVMs. The difference was signifi-
cant (p < 0.01). In the classes having larger num-
ber of examples, models of CRFs achieve better F1-
values than models of SVMs. However, in several
classes having smaller number of examples such as
0.4 0.5 0.6 0.7 0.8
0.
80
0.
85
0.
90
0.
95
Recall
Pr
ec
is
io
n
CR model of CRFs
Figure 4: Precision-Recall curve obtained by vary-
ing the threshold ? of marginal probability from 1.0
to 0.0.
EVENT and UNIT, models of SVMs achieve signif-
icantly better F1-values than models of CRFs.
Filtering NE Candidates using Marginal Prob-
ability The precision-recall curve obtained by
thresholding the marginal probability of the MAP
estimation in the CR models is shown in Figure 4.
The curve reaches a peak at 0.57 in recall, and the
precision value at that point is 0.97. This preci-
sion and recall values mean that 57% of all NEs can
be classified with approximately 97% accuracy on a
particular thresholding of marginal probability. This
results suggest that the extracted NE candidates can
be filtered with fewer cost by exploiting the marginal
probability.
Training Time The total training times of all
CRFs and SVMs models are shown in Table 4. The
training time tends to increase in case models have
complicated graph structure. For instance, model
SCR has complex graph structure compare to model
I, therefore the SCR?s training time is three times
longer than model I. Training the models by SVMs
are faster than training the models by CRFs. The dif-
ference comes from the implementation issues: C++
655
CRFs SVMs
C CR I R S SC SCR SR I P
Training Time (minutes) 207 255 97 90 138 305 316 157 28 29
Table 4: Training Time (minutes)
vs. Java, differences of feature extraction modules,
and so on. So, the comparing these two is not the
important issue in this experiment.
5 Related Work
Wikipedia has become a popular resource for NLP.
Bunescu and Pasca used Wikipedia for detecting and
disambiguating NEs in open domain texts (2006).
Strube and Ponzetto explored the use of Wikipedia
for measuring Semantic Relatedness between two
concepts (2006), and for Coreference Resolution
(2006).
Several CRFs have been explored for informa-
tion extraction from the web. Tang et al pro-
posed Tree-structured Conditional Random Fields
(TCRFs) (2006) that capture hierarchical structure
of web documents. Zhu et al proposed Hierar-
chical Conditional Random Fields (HCRFs) (2006)
for product information extraction from Web docu-
ments. TCRFs and HCRFs are similar to our ap-
proach described in section 4 in that the model struc-
ture is induced by page structure. However, the
model structures of these models are different from
our model.
There are statistical models that capture depen-
dencies between examples. There are two types of
classification approaches: iterative (Lu and Getoor,
2003b; Lu and Getoor, 2003a) or collective (Getoor
et al, 2001; Taskar et al, 2002). Lu et al (2003a;
2003b) proposed link-based classification method
based on logistic regression. This model iterates lo-
cal classification until label assignments converge.
The results vary from the ordering strategy of lo-
cal classification. In contrast to iterative classifica-
tion methods, collective classification methods di-
rectly estimate most likely assignments. Getoor
et al proposed Probabilistic Relational Models
(PRMs) (2001) which are built upon Bayesian Net-
works. Since Bayesian Networks are directed graph-
ical models, PRMs cannot model directly the cases
where instantiated graph contains cycles. Taskar et
al. proposed Relational Markov Networks (RMNs)
(2002). RMNs are the special case of Conditional
Markov Networks (or Conditional Random Fields)
in which graph structure and parameter tying are de-
termined by SQL-like form.
As for the marginal probability to use as a confi-
dence measure shown in Figure 4, Peng et al (2004)
has applied linear-chain CRFs to Chinese word seg-
mentation. It is calculated by constrained forward-
backward algorithm (Culotta and McCallum, 2004),
and confident segments are added to the dictionary
in order to improve segmentation accuracy.
6 Conclusion
In this paper, we proposed a method for categorizing
NEs in Wikipedia. We defined three types of cliques
that are constitute dependent anchor texts in con-
struct CRFs graph structure, and introduced poten-
tial functions for them to reflect classification. The
experimental results show that the effectiveness of
capturing dependencies, and proposed CRFs model
can achieve significant improvements compare to
baseline methods with SVMs. The results also show
that the dependency information from the HTML
tree helps to categorize entities without gloss texts
in Wikipedia. The marginal probability of MAP as-
signments can be used as confidence measure of the
entity categorization. We can control the precision
by filtering the confidence measure as PR curve in
Figure 4. The measure can be also used as a con-
fidence estimator in active learning in CRFs (Kim
et al, 2006), where examples with the most uncer-
tainty are selected for presentation to human anno-
tators.
In future research, we plan to explore NE catego-
rization with more fine-grained label set. For NLP
applications such as QA, NE dictionary with fine-
grained label sets will be a useful resource. How-
ever, generally, classification with statistical meth-
ods becomes difficult in case that the label set is
large, because of the insufficient positive examples.
It is an issue to be resolved in the future.
656
References
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, Carnegie Mellon University.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence estimation for information extraction. In Pro-
ceedings of Human Language Technology Conference
and North American Chapter of the Association for
Computational Linguistics (HLT-NAACL).
Lise Getoor, Eran Segal, Ben Taskar, and Daphne Koller.
2001. Probabilistic models of text and link structure
for hypertext classification. In IJCAI Workshop on
Text Learning: Beyond Supervision, 2001.
Nadia Ghamrawi and Andrew McCallum. 2005. Col-
lective multi-label classification. In Fourteenth Con-
ference on Information and Knowledge Management
(CIKM).
Juanzi Li Jie Tang, Mingcai Hong and Bangyong Liang.
2006. Tree-structured conditional random fields for
semantic annotation. In Proceedings of 5th Interna-
tional Conference of Semantic Web (ISWC-06).
Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-Won
Cha, and Gary Geunbae Lee. 2006. MMR-based ac-
tive machine learning for bio named entity recogni-
tion. In Proceedings of the Human Language Technol-
ogy Conference/North American chapter of the Asso-
ciation for Computational Linguistics annual meeting
(HLT-NAACL06).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Dong C. Liu and Jorge Nocedal. 1989. The limited mem-
ory BFGS methods for large scale optimization. In
Mathematical Programming 45.
Qing Lu and Lise Getoor. 2003a. Link-based classifica-
tion using labeled and unlabeled data. In Proceedings
of the International Conference On Machine Learning,
Washington DC, August.
Qing Lu and Lise Getoor. 2003b. Link-based text clas-
sification. In Proceedings of the International Joint
Conference on Artificial Intelligence.
Andrew McCallum, Khashayar Rohanimanesh, and
Charles Sutton. 2003. Dynamic conditional random
fields for jointly labeling multiple sequences. In NIPS
Workshop on Syntax, Semantics, and Statistics, De-
cember.
J. Neville and D. Jensen. 2000. Iterative classification
in relational data. In Proceedings of AAAI-2000 Work-
shop on Learning Statistical Models from Relational
Data, pages 13?20. AAAI Press.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
The 20th International Conference on Computational
Linguistics (COLING).
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.
2002. Extended named entity hierarchy. In Proceed-
ings of the LREC-2002.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In Proceedings of the 21st National Con-
ference on Artificial Intelligence (AAAI-06).
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of the 21th
International Conference on Machine Learning.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence. Morgan Kaufmann.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley Interscience.
Martin Wainwright, Tommi Jaakkola, and Alan Will-
sky. 2003. Tree-based reparameterization frame-
work for analysis of sum-product and related algo-
rithms. IEEE Transactions on Information Theory,
45(9):1120?1146.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2006. Simultaneous record detection and
attribute labeling in web data extraction. In Proceed-
ings of the 12th ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining.
657
Automatic Extraction of Fixed Multiword
Expressions
Campbell Hore, Masayuki Asahara, and Yu?ji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology,
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{campbe-h, masayu-a, matsu}@is.naist.jp
http://cl.naist.jp
Abstract. Fixed multiword expressions are strings of words which to-
gether behave like a single word. This research establishes a method for
the automatic extraction of such expressions. Our method involves three
stages. In the first, a statistical measure is used to extract candidate bi-
grams. In the second, we use this list to select occurrences of candidate
expressions in a corpus, together with their surrounding contexts. These
examples are used as training data for supervised machine learning, re-
sulting in a classifier which can identify target multiword expressions.
The final stage is the estimation of the part of speech of each extracted
expression based on its context of occurence. Evaluation demonstrated
that collocation measures alone are not effective in identifying target ex-
pressions. However, when trained on one million examples, the classifier
identified target multiword expressions with precision greater than 90%.
Part of speech estimation had precision and recall of over 95%.
1 Introduction
1.1 Multiword Expressions
For natural language processing purposes, a naive definition of a word in English
is ?a sequence of letters delimited by spaces ?. By this definition, the expression
ad hoc, which originally came from Latin, consists of two ?words?, ad and hoc.
However, in isolation hoc is not a meaningful English word. It is always preceded
by ad. This suggests that treating these two words as if they together form
a single ?word with spaces? more closely models their behaviour in text. A
sequence of words which for one reason or another is more sensibly treated as a
single lexical item, rather than as individual words, is known as a multiword
expression (MWE). In other words, an MWE is a sequence of words which
together behave as though they were a single word.
MWEs are not limited to imported foreign phrases such as ad hoc. They cover
a large range of expression types including proper nouns such as New York, verb-
particle constructions such as to call up (i.e. to telephone someone), and light
 Supported by the Japanese government?s MEXT scholarship programme.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 565?575, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
566 C. Hore, M. Asahara, and Y. Matsumoto
verbs such as to make a mistake. The justification for treating such expressions
as MWEs is that their linguistic properties are odd in some way as compared
to ?normal? expressions: either their part of speech or their meaning is unpre-
dictable despite full knowledge about the parts of speech or meanings of their
constituent words.
1.2 Fixed Multiword Expressions
By fixed, we mean that this particular type of MWE consists of a contiguous se-
quence of words. Other MWE types can consist of discontinuous word sequences.
For example, the verb-particle construction to call up takes an indirect object,
the person who receives the telephone call. This person can appear after the
verb-particle construction (e.g. ?I called up Mohammad?) but it can also ap-
pear in the middle of the verb-particle construction, (e.g. ?I called Mohammad
up?). In contrast, fixed MWEs consist of contiguous word sequences. For exam-
ple, by and large cannot be modified by insertion of other words (e.g. *?by and
very large?).
1.3 Multiword Expressions in Parsing
The aim of our research is the development of a method for the automatic extrac-
tion and part of speech estimation of fixed MWEs. The ability to identify this
type of MWE in texts is of potential use in a wide variety of natural language
processing tasks because it should enable an improvement in the precision of sen-
tence parsing. Sentence parsing is frequently the first step in more sophisticated
language processing tasks, so an increase in parsing precision should improve
results in a large number of natural language processing applications.
A parser generally takes a sentence as input, together with the parts of speech
of the tokens1 in the sentence. The parser then attempts to estimate the most
probable syntactic structure for the sentence. Some kinds of MWE have the
potential to disrupt this process because the part of speech of the MWE as a
whole, cannot be predicted on the basis of the parts of speech of its constituent
words. For example, the part of speech sequence for the expression by and large
is Preposition + Conjunction + Adjective, which is a sequence almost certainly
unique to this expression. A parser which is not explicitly informed about by
and large will therefore struggle to cope with this part of speech sequence, and
may incorrectly try to group one or more parts of the expression with words in
the surrounding context.
The solution to this problem of syntactically unpredictable MWEs is to add
them to the dictionary used by the parser. When the parser comes across a
sequence of words that matches an MWE in its dictionary, it can use the MWE?s
part of speech to parse the sentence treating the MWE as a single lexical item.
1 We use the term tokens here rather than words because texts actually contain many
space delimited character strings which are not normally thought of as words, such
as numbers and punctuation. In addition, some purely alphabetic character strings
are not words in their own right, as is the case for hoc, mentioned above.
Automatic Extraction of Fixed Multiword Expressions 567
In reality, some sequences of words are an MWE only in specific contexts.
For example, in the main is an MWE when it means ?overall? or ?mostly? as
seen in the sentence ?In the main, the biggest improvements have been in child
health?. In contrast, in a sentence such as ?Village hotels ought to be in the main
square, not at the outskirts of a village? the word sequence in the main is not an
MWE, and can therefore be treated normally by the parser as separate words.
1.4 Target Problems
We approach the task of extracting fixed MWEs by decomposing it into three
sub-problems, as illustrated in Fig. 1. The first (?3.1, described in Section 3.1) is
simple collocation extraction. We use a standard collocation measure to extract
as many candidate bigram MWEs from the corpus as possible.
Fig. 1. Flowchart of processing
The second problem (?3.2) is refinement of the list of candidate MWEs.
Many of the candidates are not target multiword expressions. Distinguishing
between word sequences that are MWEs, and those that never are, represents
one sub-task. Hereafter, we refer to word sequences which are never MWEs as
non-MWEs. Some word sequences have dual identities. In one context a word
568 C. Hore, M. Asahara, and Y. Matsumoto
sequence may be an MWE, but in another, it may be just a normal, literal word
sequence. For example, a child forced to help wash the family car, and acting
petulantly, might be scolded ?Don?t kick the bucket!?. In this case kick the
bucket is a normal, compositional phrase; its meaning can be understood based
on the literal meaning of its constituent words. When kick the bucket is used as a
euphemism for ?to die? however, its meaning is non-compositional, and thus in
this context it is an MWE. In this paper we refer to occurrences of literal word
sequences which have the appearance of being an MWE as pseudo-MWEs. We
deal with these two sub-tasks simultaneously, using supervised machine learning.
The third problem we tackle (?3.3) is estimation of the part of speech of
MWEs. This problem is also solved using supervised machine learning. In this
research we limit ourselves to MWEs containing only two words (i.e. bigrams).
In the future, we plan to generalize the method so that it works with MWEs of
arbitrary length.
2 Related Work
Collocation extraction has been covered extensively in the literature. One of
the earliest attempts to automatically extract collocations from a corpus was
undertaken by Church and Hanks [1]. The statistical measure they used to iden-
tify collocations was based on mutual information. Smadja [2] developed a tool
called Xtract for the extraction of collocations. His definition of a collocation
differed slightly from that of Church and Hanks because he claimed expressions
such as doctors and nurses are not real collocations, just words related by virtue
of their shared domain or semantics. Thanopoulos, Fakotakis and Kokkinakis
in [3] reviewed the statistical measures most frequently used for collocation ex-
traction, and evaluated them by comparing their performance with that of two
new measures of their own. Their first novel measure, Mutual Dependency (MD)
is pointwise mutual information minus self-information. The second measure at-
tempts to introduce a slight frequency bias by combining the t-score with mutual
dependency. Although frequency alone is not sufficient evidence of collocational
status, they argue that candidate collocations that have a high frequency are
more likely to be valid than those that are very rare.
While collocations have received attention over a number of years, MWEs
have only relatively recently emerged as a research topic within natural lan-
guage processing. In consequence, there are relatively few articles specifically
about MWEs. Sag et al in [4] gave a linguistic categorisation of the differ-
ent types of MWE, and described ways of representing them efficiently within
a computational framework2. Although MWEs as a whole have yet to receive
widespread investigation, attention has been paid to specific types of MWE. For
example, verb-particle constructions have been the subject of several studies (see
for example [5] and [6]).
2 Head-driven Phrase Structure Grammar (HPSG).
Automatic Extraction of Fixed Multiword Expressions 569
3 Method
In order to extract information about fixed MWEs from a corpus, we use a three
stage process. In the first stage we identify a list of candidate MWEs based on the
statistical behaviour of the tokens in the corpus. Two words whose probability of
appearing together is greater than that which would be expected based on their
individual frequencies, are considered to constitute a potential MWE and are
extracted for later processing. In other words, stage one is collocation extraction.
In the second stage, we use this list of candidate MWEs as the basis for
extracting from the corpus examples of candidates together with their contexts.
These examples are then used as training data for supervised machine learning
resulting in a classifier capable of distinguishing between, on the one hand, true
MWEs, and on the other, non-MWEs and pseudo-MWEs.
In the final stage we use supervised machine learning to train a classifier to
perform MWE part of speech assignment. By examining the context surrounding
an MWE, it is possible for the classifier to determine the most likely part of
speech for the MWE in that context.
3.1 Collocation Extraction
Collocation extraction was performed using one of the statistical measures dis-
cussed in [3]. The measures we experimented with were: frequency, ?-square [7],
log-likelihood ratio [8], t-score [7], mutual information (MI) [1], mutual depen-
dency (MD ? mutual information minus self-information) [3], and log-frequency
biased mutual dependency (LFMD ? a combination of the t-score and mutual
dependency) [3]. The equations of these measures are shown in Fig. 2.
We compared the resulting ranked lists of bigrams with a list of target MWEs
extracted from the British National Corpus (BNC)3. The target list was pro-
duced by starting with a list of all MWEs tagged as such in the BNC, and
removing MWEs with a frequency of less than five, and MWEs with a part
of speech of noun, or adjective. This reduction was performed for two reasons.
Firstly, many MWE instances in the BNC can be considered noise in that they
contain spelling variants, and features of spoken language. Secondly, a colloca-
tion consisting entirely of a combination of one or more nouns and adjectives is
almost certainly a noun phrase, or part of a noun phrase. Noun phrases tend to
be easily identifiable as such by parsers, and so are not relevant to our research
aim. By removing the above MWEs we were able to reduce computational costs
at later stages of processing.
3.2 Verification
Verification was performed with the aim of extracting a much higher quality list
of candidate MWEs from the list of candidate MWEs produced in the collocation
stage.
3 http://www.natcorp.ox.ac.uk/
570 C. Hore, M. Asahara, and Y. Matsumoto
t-score
t =
x ? ?
?
s2
N
Where x is the sample mean, s2 is the sample variance, N is the sample size and ? is
the distribution?s mean.
?-square
?2 =
(fw1w2 ? fw1fw2 )2
fw1fw2
+
(fw1w2 ? fw1fw2)2
fw1fw2
+
(fw1w2 ? fw1fw2)2
fw1fw2
+
(fw1w2 ? fw1fw2)2
fw1fw2
Where f is the frequency of an event, w1w2 is the sequence of events (in this case
words) w1 then w2, and w1 is the negation of the event w1.
log-likelihood ratio
?2 log ? = 2 ? log L(H1)
L(H0)
Where L(H) is the likelihood of hypothesis H assuming a binomial distribution.
pointwise mutual information (PMI)
I(w1, w2) = log2
P (w1w2)
P (w1) ? P (w2)
Where P (w) is the probability of a given word.
mutual dependency (MD)
D(w1, w2) = log2
P 2(w1w2)
P (w1) ? P (w2)
Where P (w) is the probability of a given word.
log-frequency biased mutual dependency (LFMD)
DLF (w1, w2) = D(w1, w2) + log2P (w1w2)
Where P (w) is the probability of a given word.
Fig. 2. Collocation measures
Features. In order to train a classifier, a decision must be made about which
features to include in the training data. We decided to use the tokens and their
parts of speech from a context window of three tokens to the left and three tokens
to the right of each candidate MWE. We also used the tokens in the candidate
MWE itself and their parts of speech. The cutoff value of three is somewhat
arbitrary, but most lexical dependencies can be assumed to be relatively local,
so we can assume that it is large enough to capture the most useful information
available from the context.
Automatic Extraction of Fixed Multiword Expressions 571
Contexts were not allowed to cross sentence boundaries. In cases where the
available context surrounding a candidate MWE was shorter than the three token
window, we inserted the appropriate number of dummy tokens and dummy parts
of speech to fill up the deficit: ?BOS? (Beginning Of Sentence) tokens in the left
context, and ?EOS? (End Of Sentence) tokens in the right context.
Part of Speech Tagging. The part of speech information was provided by
tagging the corpus using the part of speech tagger TnT4. The BNC is a part of
speech tagged corpus, but retagging was necessary because although each MWE
in the BNC is tagged with a part of speech, its constituent tokens are not. The
tokens which make up each MWE must therefore be tagged with individual parts
of speech. It might be argued that combining the original BNC tagging with the
TnT tagging of the words in the MWEs would have produced more accurate
training data, but in a real world application, the part of speech information in
the classifier?s input data will be produced entirely using a tagger such as TnT.
By using the same part of speech tagger at both the training, and application
stages, any systematic tagging mistakes will hopefully (at least in part) be learnt
and compensated for by the classifier. The tagger was trained on a subset of BNC
files containing just under 5.5 million tokens. It was then tested on a different
set of files, containing approximately 5.3 million tokens. The tagger?s precision
when tested on this data was 94.7%.
Training. The corpus used for training the classifier was a sub-corpus of the
BNC containing approximately ninety million tokens covering all domains in the
corpus. Examples used for training were the occurrences in the training corpus
of the top 10,000 bigrams identified using the t-score and LFMD collocation
measures. Most of the bigrams in the corpus were negative examples, either
non-MWEs, or pseudo-MWEs.
We used TinySVM5 to create a binary classifier. Training was performed
(using the software?s default settings) on the training corpus. Several training
runs were performed using different amounts of data in order to investigate the
relationship between volume of training data and the resulting model?s perfor-
mance.
Testing. Another sub-corpus of the BNC, independent of that used in training,
was used for testing the classifier. This testing corpus contained approximately
six million tokens and included texts from each domain in the corpus.
3.3 Part of Speech Estimation of Multiword Expressions
We treated the estimation of the part of speech of a given MWE as a classification
task using the same approach as we used for the classification of true and false
MWEs. We trained a separate classifier for each target part of speech. A positive
4 http://www.coli.uni-sb.de/?thorsten/tnt/
5 http://chasen.org/?taku/software/TinySVM/
572 C. Hore, M. Asahara, and Y. Matsumoto
training example was an occurrence of an MWE with the target part of speech.
A negative example was an occurrence of any MWE with a non-target part of
speech. The features used were the token and part of speech of three tokens to
the left and to the right of the target MWE, as well as the tokens and parts of
speech of the words in the MWE itself. The training and testing corpora were
the same as used at the verification stage described above (Section 3.2). This
was acceptable because the two tasks are independent of each other.
We chose the target parts of speech (adverbs, prepositions and conjunctions)
because these relatively closed class, high frequency types are expected to be
most useful in applications like parsing. We also experimented with an open
class type (nouns) for comparison. Verbs could not be tested because there were
insufficient numbers of them in the testing corpus. There are few fixed MWE
verbs, so a scarcity of data was not surprising.
4 Results and Discussion
4.1 Collocation Extraction
Results for collocation extraction (Table 1) show that standard collocation mea-
sures perform poorly in the task of extracting the target MWEs. Even when
Table 1. Precision and recall for top 100, 1,000 and 10,000 candidate multiword ex-
pressions extracted using different collocation measures
Measure Cutoff Precision Recall F-measure
10,000 0.009 0.251 0.017
freq 1,000 0.032 0.091 0.047
100 0.010 0.003 0.004
10,000 0.009 0.257 0.017
t-score 1,000 0.037 0.106 0.055
100 0.030 0.009 0.013
10,000 0.006 0.169 0.011
?2 1,000 0.013 0.037 0.019
100 0.020 0.006 0.009
10,000 0.004 0.117 0.008
log-like 1,000 0.008 0.023 0.012
100 0.000 0.000 0.000
10,000 0.003 0.083 0.006
MI 1,000 0.002 0.006 0.003
100 0.000 0.000 0.000
10,000 0.003 0.091 0.006
MD 1,000 0.003 0.009 0.004
100 0.000 0.000 0.000
10,000 0.008 0.229 0.015
LFMD 1,000 0.017 0.049 0.025
100 0.080 0.023 0.036
Automatic Extraction of Fixed Multiword Expressions 573
calculated based on the top 10,000 candidate collocations, recall is only 26%
(using the t-score).
A limitation in our approach to measuring collocation extraction may be
partly to blame for the poor results in this task. Our target list consisted of all
target MWEs, irrespective of their length. Since the collocations extracted were
limited to bigrams, some of these may in fact be only part of a larger MWE in
our target list.
Nevertheless, it may be that collocation measures are relatively ineffective at
extracting fixed MWEs. Collocation measures are most effective when applied
to expressions such as noun compounds. Many of the target MWEs contain high
frequency function words such as prepositions, and thus are atypical of the types
of expressions for which collocation measures were originally developed.
4.2 Verification
Verification of candidate MWEs produced better results (Table 2). For example,
a classifier trained using one million examples, had precision of 96.56%, and
recall of 89.11% giving an F-measure of 92.69.
Table 2. Performance of verifier using models trained on different quantities of data
Measure Examples Precision (%) Recall (%) F-measure
LFMD 1,000,000 96.56 89.11 92.69
100,000 96.35 79.87 87.34
10,000 93.66 56.04 70.12
1,000 92.83 11.56 20.56
Initial review of classification results suggests a number of sources of error.
Tagging errors seem to cause many of the false negative results. Proper nouns
tend to be tagged as ?unclassified words? which is intelligent in as much as
it bundles all unusual words the tagger is unsure of together, but it results in
incorrect tagging which prevents the classifier identifying true MWEs. Similarly,
capitalisation of words in titles results in incorrect tagging of ordinary words as
proper nouns. One title in particular Sport in Short occurs multiple times in the
corpus, resulting in numerous errors.
False positives seem to be caused by proper nouns (e.g. Kuala Lumpur) and
foreign words (e.g. Vive L?Empereur). Both false positives and false negatives
seem to occur often in the context of punctuation, suggesting that this presents
a particular difficulty for the classifier.
Interestingly, some false positives are in fact substrings of longer MWEs.
Because we focused on bigrams in this paper, MWEs of longer than two tokens
were ignored when assessing whether a candidate MWE was a true or false
MWE. However, some of these candidate MWEs were in fact substrings of a
longer MWE. The classifier may therefore be recognising that a given substring
574 C. Hore, M. Asahara, and Y. Matsumoto
occurs in a context typical of MWEs, and is identifying the MWE substring as
being a MWE in its own right. A fuller implementation which extracts MWEs
longer than two tokens might therefore be expected to eliminate this source
of error.
In spite of the occasional error, applying a classifier to the context surround-
ing a candidate MWE seems to offer an effective means of distinguishing true
MWEs from non-MWEs and pseudo-MWEs.
4.3 Part of Speech Estimation
Evaluation of the part of speech classifiers shows them to be an effective means of
estimating an MWE?s part of speech based on its context of occurrence (Table 3).
As we might expect, the classifier for nouns performed best, with near perfect
recall and high precision. The conjunctions classifier performed least well with
recall in particular being lower than that achieved for other parts of speech.
This may reflect a greater variability in the contexts surrounding conjunctive
MWEs. Conjunctions often play a discursive role in sentences, so evidence of
an expression being a conjunction or not might be found at a higher level of
linguistic analysis than the immediate lexical context used in our experiment.
Table 3. Part of speech estimation results
Part of speech Precision (%) Recall (%) F-measure
Prepositions 98.06 98.40 98.23
Conjunctions 97.10 95.37 96.23
Adverbs 98.73 98.72 98.72
Nouns 98.88 99.25 99.07
5 Future Work
In this work we have focused on bigrams. We hope to generalise our approach,
so that MWEs of length greater than two can be extracted and assigned a part
of speech.
We plan to evaluate the performance of the BNC models described above
on another corpus to determine their flexibility. Specifically, we plan to use a
corpus of North American English such as the Penn Treebank, in the hope of
demonstrating the models? ability to handle American as well as British English.
We also plan to check the effect on parsing accuracy of using the extracted
multiword expressions in the input to a parser such as Collins? [9] or
Yamada?s [10].
6 Conclusion
In this research we aimed to identify a method for the automatic extraction and
part of speech estimation of fixed MWEs. Knowledge about fixed MWEs has
Automatic Extraction of Fixed Multiword Expressions 575
the potential to improve the accuracy of numerous natural language processing
applications. Generating such a list therefore represents an important natural
language processing task.
Our method uses a collocation measure to produce a list of candidate bigrams.
These candidates are then used to select training data for a classifier. The trained
classifier was successfully able to distinguish between contexts containing a true
MWE, from contexts containing a pseudo-MWE or no MWE at all. The classifier
trained on one million example candidates identified using LFMD had precision
of 96.56%, and recall of 89.11%, giving an F-measure of 92.69%. Part of speech
classifiers were then trained and tested. The classifiers were able to identify the
correct part of speech for an MWE with a precision and recall of over 95%.
These results show that the local context surrounding an MWE contains
sufficient information to identify its presence, and estimate its part of speech.
If this information is detailed enough, we may be able to perform additional
processing steps. For example, it may be possible to distinguish between specific
sub-types of fixed MWE. The present method needs to be generalised so it can
deal with MWEs of any length, not just bigrams. We plan to explore these issues
in future research.
References
1. Church, K., Hanks, P.: Word association norms, mutual information, and lexicog-
raphy. Computational Linguistics 16 (1990) 22?29
2. Smadja, F.: Retrieving collocations from text: Xtract. Computational Linguistics
19 (1993) 143?177 *Special Issue on Using Large Corpora: I.
3. Thanopoulos, A., Fakotakis, N., Kokkinakis, G.: Comparative evaluation of col-
location extraction metrics. In: International Conference on Language Resources
and Evaluation (LREC-2002). (2002) 620?625
4. Sag, I., Baldwin, T., Bond, F., Copestake, A., Flickinger, D.: Multiword expres-
sions: A pain in the neck for NLP. In: Proceedings of the Third International Con-
ference on Intelligent Text Processing and Computational Linguistics (CICLING
2002), Mexico City, Mexico, CICLING (2002) 1?15
5. Baldwin, T., Villavicencio, A.: Extracting the unextractable: A case study on verb-
particles. In Roth, D., van den Bosch, A., eds.: Proceedings of the 6th Conference
on Natural Language Learning (CoNLL-2002), Taipei, Taiwan (2002) 98?104
6. Villavicencio, A.: Verb-particle constructions and lexical resources. In Bond, F.,
Korhonen, A., McCarthy, D., Villavicencio, A., eds.: Proceedings of the ACL 2003
Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, ACL
(2003) 57?64
7. Manning, C.D., Schu?tze, H.: Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts (1999)
8. Dunning, T.: Accurate methods for the statistics of surprise and coincidence.
Computational Linguistics 19 (1993) 61?74
9. Collins, M.: Head-Driven Statistical Models for Natural Language Parsing. PhD
thesis, University of Pennsylvania (1999)
10. Yamada, H., Matsumoto, Y.: Statistical dependency analysis with support vector
machines. In: IWPT 2003: 8th International Workshop on Parsing Technologies.
(2003) 195?206
Building a Japanese-Chinese Dictionary Using
Kanji/Hanzi Conversion
Chooi-Ling Goh, Masayuki Asahara, and Yuji Matsumoto
Graduate School of Information Science, Nara Institute of Science and Technology,
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{ling-g, masayu-a, matsu}@is.naist.jp
Abstract. A new bilingual dictionary can be built using two existing
bilingual dictionaries, such as Japanese-English and English-Chinese to
build Japanese-Chinese dictionary. However, Japanese and Chinese are
nearer languages than English, there should be a more direct way of
doing this. Since a lot of Japanese words are composed of kanji, which
are similar to hanzi in Chinese, we attempt to build a dictionary for kanji
words by simple conversion from kanji to hanzi. Our survey shows that
around 2/3 of the nouns and verbal nouns in Japanese are kanji words,
and more than 1/3 of them can be translated into Chinese directly. The
accuracy of conversion is 97%. Besides, we obtain translation candidates
for 24% of the Japanese words using English as a pivot language with
77% accuracy. By adding the kanji/hanzi conversion method, we increase
the candidates by 9%, to 33%, with better quality candidates.
1 Introduction
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 670?681, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Bilingual dictionaries have unlimited usage. In order for one to learn a new lan-
guage, a bilingual dictionary can never be absent. In natural language process-
ing community, bilingual dictionaries are useful in many areas, such as machine
translation and cross language information retrieval.
In this research, we attempt to build a Japanese-Chinese dictionary using
public available resources. There are already some existing Japanese-Chinese
dictionaries, such as Shogakukan?s Ri-Zhong Cidian [1], but they are not publicly
available in electronic form. Our purpose is to build an electronic dictionary from
public resources and make it public available.
The first dictionary that we use is IPADIC [2], a Japanese dictionary used by
ChaSen [3], a Japanese morphological analyzer. We extract only nouns, verbal
nouns and verbs from this dictionary, and try to search for their translation
equivalents in Chinese.
One can build a new bilingual dictionary for a new pair of languages using two
bilingual lexicons [4?8]. Since it is always easier to get bilingual dictionaries that
involve English as one of the languages, using English as a pivot language is pos-
sible. In this case, we first look for the English translations of one language, and
then try to find the possible candidates in the other language through English.
Then we rank the candidates according to the similarities between the two words
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 671
using some linguistic knowledge and statistical information. In our research, we
make use of two public resources, EDICT [9] - a Japanaese-English dictionary
and CEDICT [10] - a Chinese-English dictionary, to create the new language
pair Japanese-Chinese dictionary using English as a pivot language. We obtain
77% accuracy. However, this method extracts only translations for about 24% of
the Japanese words in IPADIC because the EDICT and CEDICT dictionaries
are smaller compared with IPADIC. Therefore, we also look into the possibility
to get the translation words using kanji/hanzi conversion. In Japanese, there are
three types of characters, namely hiragana, katakana and kanji. Kanji characters
are similar to Chinese ideographs. In Chinese, all characters are written in hanzi.
Since most of the kanji characters are originally from China, the usage should
remain unchangeable in certain contexts. The kanji/hanzi conversion method
works only on Japanese words that consist only kanji characters. We obtain a
high accuracy of 97% using this conversion. By combining the two methods, we
increase the number of translation candidates by 9%, from 24% to 33%.
2 Previous Work
Tanaka and Umemura [4] used English as an intermediate language to link
Japanese and French. They are the first who proposed the inverse consultation.
The concept behind is that a translation sometimes may have wider or narrower
meaning than the source word. They first look up the English translations of a
given Japanese word, then the French translations of these English translations.
This step gives a set of French candidates equivalent to the Japanese word. For
each French candidate, its translations in English is collected. The similarity
between the Japanese word and the French word is measured by the number of
matches in their English translation. The more matches show the better candi-
date. This is referred to as ?one time inverse consultation?. The extension can
be furthered by looking up all the Japanese translations of all the English trans-
lation of a given French word and seeing how many times the Japanese word
appears; this is referred to as ?two times inverse consultation?.
Bond at al. [6] applied the ?one time inverse consultation? in constructing
a Japanese-Malay dictionary using a Japanese-English dictionary and a Malay-
English dictionary. They also applied the semantic matching using part of speech
and second-language matching. Matching only compatible parts of speech could
cut down a lot of false matches. The second-language matching score used
Chinese as a second intermediate language. If a word pair could be matched
through two different languages, it is considered a very good match. Their re-
search showed that about 80% of the translations are good if only highest rank
pairs are considered, and 77% for all pairs.
Shirai and Yamamoto [7] used English as an intermediate language to link
Korean and Japanese. They tried on 1,000 Korean words and were able to obtain
the translations for 365 of them. They achieved an accuracy of 72% when the
degree of similarity calculated by one time inverse consultation is higher than a
predefined threshold.
672 C.-L. Goh, M. Asahara, and Y. Matsumoto
Zhang et al [8] used the same approach, that is using English as a pivot
language, for constructing Japanese-Chinese pairs. They used the one time in-
verse consultation method and also the part of speech information for ranking.
Since there is similarity between Japanese kanji and Chinese hanzi, they have
further improved on the method by using the kanji information [11]. First they
searched for the Chinese translations of single character words in Japanese into
using one time inverse consultation. If the Unicode of the two characters are
the same, then the ranking is higher. After getting this list of character pairs,
the similarity between the Japanese word and the Chinese word is calculated
using the edit distance algorithm [12]. Finally, the score obtained from the kanji
information is added to the final score function. Their ranking method was im-
proved and the precision increased from 66.67% to 81.43%. Since only about 50%
of their Japanese words can be translated into Chinese, they also searched for
other approaches to translate the remaining words [13] using web information
and machines translation method.
Our work is quite similar to Zhang et al [11] in the way they constructed
the kanji/hanzi conversion table. The difference is that instead of calculating the
similarity between kanji and hanzi using Unicode and one time inverse consulta-
tion, we make a direct conversion from kanji to hanzi based on the ideographs.
Our method sounds more intuitive and direct because kanji and hanzi are of
the same origin. Later on, they made use of this conversion table to calcutate
the similarity between a Japanese word and a Chinese word from the output of
using English as the pivot language. Their method can make the similar Chinese
words to have higher ranking but cannot generate new translation candidates.
On the other hand, our methods works for both.
3 The Proposed Methods
We propose to combine two methods to find the translations of Japanese entries
in IPADIC version 2.7.0 [2]. IPADIC is a monolingual dictionary and consists of
239,631 entries. We only extract nouns, verbal nouns and verbs (a total of 85,553
entries) in our survey. First, we use English as the pivot languege. Second, we
make direct conversion from kanji to hanzi for kanji word translation. We now
describe in detail the both methods.
3.1 Using Third Language: English
First, we use English as the pivot language to find the translations from Japanese
to English, and then from English to Chinese. Since IPADIC is a monolingual dic-
tionary, we use EDICT as the Japanese-English dictionary. The EDICT version
(V05-001) consists of 110,424 entries. There exist some words that are polyse-
mous with multiple entries. After combining the multiple entry words, we have
106,925 unique entries in the dictionary. For English to Chinese, we use the CE-
DICT dictionary. It consists of 24,665 entries. A word can be polysemous in both
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 673
dictionary, meaning that for each word there is only one entry but with multi-
ple translations. All the English translations of different senses are in the same
record. We assume that a bilingual dictionary should be bi-directional, therefore
we reverse the CEDICT dictionary to obtain an English-Chinese dictionary.
The ranking method is the one time inverse consultation [4, 6?8]. Since a word
can be polysemous in both dictionaries, if a source word shares more English
translations with the target translation word, then they can be considered nearer
in meaning. The score is calculated as in equation (1): Let SE(J,Ci) denotes
the similarity between the Japanese word J and the Chinese translation word
candidate Ci, where E(J) and E(Ci) are the sets of English translations for J
and Ci, respectively:
SE(J,Ci) =
2? (|E(J) ? E(Ci)|)
|E(J)|+ |E(Ci)|
(1)
Currently we do not apply the part of speech information in the scoring be-
cause this method requires linguistic experts to decide on the similarity between
two part of speech tags for different languages1. However, this will become part
of our future work.
Table 1 shows the results of using English as the pivot language and one
time inverse consultation as the scoring function. Using the EDICT and CE-
DICT only, 32,380 Japanese words obtain their Chinese translation candidates.
In total, we obtain 149,841 pairs of translation. We get maximum 90 candidates
for a Japanese word, and 4.6 candidates per word by average. Then we check
the Japanese words in IPADIC to get their part of speech tags. We only inves-
tigate on three categories of part of speech tags from the IPADIC, which are
nouns, verbal nouns and verbs. We randomly selected 200 Japanese words from
each category for evaluation. The results are judged using 4 categories: Correct
means that the first rank word is correct (if there are multiple words in the
first rank, it is considered correct if any one of the words is correct), Not-first
means that the correct word exists but not at the first rank, Acceptable means
that the first rank word is acceptable, and Wrong means that all candidates
are wrong. All the categories are exclusive of each other.
POS Total Translated Correct Not-first Acceptable Wrong
Nouns 58,793 14,275 (24.3%) 152 4 20 24
Verbal nouns 12,041 3,770 (31.3%) 90 12 37 61
Verbs 14,719 2,509 (17.0%) 101 18 27 54
Table 1. Ranking results
There are about 24.3% of nouns, 31.3% of verbal nouns and 17.0% of verbs in
IPADIC that give us some translation candidates in Chinese. For the evaluation
1 There are 120 part of speech tags (13 categories) in IPADIC, and 45 in Peking
University dictionary. Both define some quite specialized part of speech tags which
only exist within the dictionary itself.
674 C.-L. Goh, M. Asahara, and Y. Matsumoto
using 200 randomly selected words, we obtain 88%, 69.5% and 73% accuracy,
respectively. The accuracy is 76%, 45% and 50.5%, respectively, if we considered
only the first rank. The accuracy is a bit lower compared with previous work as
we did not apply other linguistic resources such as parts of speech for scoring.
Although improving scoring function can make the rank of the correct words
higher, it cannot further increase the number of candidates. Since both EDICT
and CEDICT are prepared by different people, the way they translate the words
also varies. Furthermore, there is no standization on the format. For example, to
represent a verb in English, sometimes it is written in base form (e.g. ?discuss?),
and sometimes in infinitive form (e.g. ?to discuss?). In Chinese, and sometimes in
Japanese too, a word shown in the dictionary can be a noun and a verb without
inflection. The part of speech category can only be decided based on the usage in
contexts. Therefore the same word may be translated into a noun in English too
(e.g. ?discussion?). It happened too that we cannot find the matches just because
of singular form or plural form (e.g. ?discussions?) of the English translation.
With these non-standardization of the English translation, we cannot match the
exact words unless we do a morphological analysis in English. Therefore, we
also look for other ways to increase to number of candidates. Since Japanese
and Chinese share some common characters (kanji in Japanese and hanzi in
Chinese), we are looking into the possibility of direct conversion to create the
translations. We discuss this method in the following section.
3.2 Direct Conversion of Kanji/Hanzi
Using English as the pivot language is a good starting point to construct a new
language pair. However, there remain a lot of words for which the translations
cannot be obtained. In Chinese, all the characters are hanzi, but in Japanese,
there are hiragana, katakana and kanji. The kanji characters are originated from
ancient China. This group of characters, used in China, Japan and Korea, are
referred to as Han characters. The Han characters capture some semantic infor-
mation which should be common in those languages. One can create a new word
by combining the existing characters but it is hardly that one can create a new
character. Therefore, these characters are stable in their meaning. Due to the
common sharing on these Han characters, there might be a more straightforward
way to translate a word in Japanese into Chinese if all the characters in the word
are made up from kanji only. We refer to this kind of words as kanji words.
A Chinese word can be a noun or a verb without changes of morphological
forms. There is no inflection to differenciate them. EDICT and CEDICT make
no difference on the parts of speech and therefore the translations in English can
be in any form. For example, the following Japanese words and Chinese words
exist for the translations of ?discussion/discussions/to discuss/discuss?.
Japanese: ??, {, ?{, ?, ??, 	?, ?{, ?, ?{, {, ??, O?,
?{, ?, ?{, H, 0, , , , 1d, 3d
Chinese: &, ??, X, ?, ?X, ??, ??, ??, ?, ?X, ?X, F, FX
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 675
If we were to match each Japanese word to each of the Chinese words (in fact,
we can say that some of them are acceptable translations), then we will get a
redundency of 286 (22?13) pairs. Although these words have similar translation,
but in fact they have slight differences in meaning. For example, ???? means
the conference amongst the ministers, ???? means negotiations. However, ?dis-
cussion? is one of the translations in English as provided by EDICT. Since the
Japanese kanji characters are originated from China, translating Japanese kanji
words directly to Chinese can be more accurate than going through a third lan-
guage like English. If we look from the Japanese side, 12 out of 22 words (??,
{, ?{, ?, ??, ?, ??, O?, ?, ?{, H, ) could get their exact
translations by just simple conversion of kanji/hanzi (??,FX,NF,?X,?
?, ?X, b?, ,?, ?X, ?F, &X, X), in which some of them cannot get
the translations using English. On the other hand, there also exist some words
that are not translated into the semantic meaning ?discuss? in Japanese but in
Chinese, such as ??X? which should be the same as ??? in Japanese2. For
the single character words in Chinese (&, ?, ?, F), they are seldom used in
Japanese but they do exist with the same meaning (H, ?, ?, {).
There exist equivalent characters between Japanese kanji and Chinese hanzi.
Both type of characters (Han characters in general) capture significant seman-
tic information. Although the pronunciation varies across languages, the visual
form of the characters retains certain level of similarity. Furthermore, Chinese
characters can be divided into the characters used by mainland China (referred
to as Simplified Chinese) and Taiwan (including Hong Kong and Macao, referred
to as Traditional Chinese). Although the ideographs may be different, they are
originally the same characters. Most of the Japanese characters are similar to
Traditional characters.
English love garden rice fly kill talk fill up post excellent sun
Japanese ?  ? ? ?  Q
@
,
b
Traditional Chinese ? h 9 ? l ? ?

8
&
Simplified Chinese ? ? , < ? ? V
?
?
?
Table 2. Successful Traditional-Simplified examples
Our original Japanese characters are coded in EUC and Chinese characters
are coded in GB-2312 codes. To convert a kanji to a hanzi is not a trivial task.
Of course most of the characters share the same ideographs. In this case, we can
use the Unicode for the conversion as these characters share the same Unicode.
However, there exist also quite a number of characters in Japanese that are
written in Traditional Chinese ideographs. We have to convert these characters
from Traditional Chinese to Simplified Chinese (see Table 2). Finally, there are
2 The meaning of ???? (a business talk) in Japanese is different from the meaning
of ????? (to discuss verbally) in Chinese.
676 C.-L. Goh, M. Asahara, and Y. Matsumoto
English gas hair deliver check home pass by burn bad money whole
Japanese [ b ` r ? C ? ? ? 
Traditional Chinese ? ? s l  B t ? ? ?
Simplified Chinese ?   5 * ? ?   
Table 3. Unsuccessful Traditional-Simplified examples
English sardine hackberry maple kite inclusive
Japanese ?  ? y -
English crossroad field/patch rice bowl carpentry chimera
Japanese  [ H  '
Table 4. Japanese-GBK examples
also some characters in Japanese having similar ideographs, but they are neither
Traditional Chinese nor Simplified Chinese (see Table 3). We manually convert
these characters by hand. The following shows the steps to convert the characters
from Japanese to Chinese.
1. Convert from EUC to Unicode using iconv.
2. Convert from Unicode to Unicode-simplified using a Chinese encoding con-
verter3. This step converts possible Traditional characters to Simplified char-
acters.
3. Convert from Unicode-simplified to GB-2312.
4. Those failed to be converted are edited manually by hand.
5. Those characters that do not exist in GB-2312 are converted into GBK using
the Chinese encoding converter.
From IPADIC, we extract 36,069 and 8,016 kanji words from noun and verbal
noun categories4, respectively. From these words, we get 4,454 distinct kanji char-
acters. Out of these characters, only 2,547 characters can be directly converted
using Unicode without changes of ideographs. 1,281 characters are converted
from Traditional Chinese to Simplified Chinese using the Chinese encoding con-
verter. Finally 626 characters are manually checked and 339 characters can be
converted to Simplified Chinese. 287 remain in Japanese ideographs but are con-
verted into GBK codes 5. Most of these words are the names of plants, fish, and
things invented by Japanese (see Table 4). While these GBK coded words may
not be used in Chinese, we just leave them in the conversion table for the sake
of completeness.
3 http://www.madarintools.com/zhcode.html
4 These two categories consist of most of the kanji words in Japanese. However, verbs
are normally hiragana only or a mixture of kanji plus hiragana. Therefore, we omit
verbs in this survey.
5 GBK codes consist of all simplified and traditional characters, including their vari-
ants. Therefore, Japanese characters can also be coded in GBK. However, they are
rarely used in Chinese.
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 677
About 61% of nouns and 67% of verbal nouns in Japanese are kanji words
as shown in Table 5. Using the conversion table described above, we convert the
kanji words into Chinese words. Then, we consult these words using a Chinese
dictionary provided by Peking University [14]. There are about 80,000 entries in
this dictionary. About 33% of the nouns and 44% of the verbal nouns are valid
words in Chinese. We randomly select 200 words for evaluation. We evaluate the
results by 3 categories: Correct means that the translation is good, Part-of
means that either the Japanese word or the Chinese word has a wider meaning,
and Wrong means that the meanings are not the same though they have the
same characters. The accuracies obtained are 97% for nouns and 97.5% for verbal
nouns. The pairs that have part-of meaning and different meaning are listed in
Table 6 and Table 7 for references.
POS Total Kanji words Translated Correct Part-of Wrong
Nouns 58,793 36,069 (61%) 11,743 (33%) 189 5 6
Verbal nouns 12,041 8,016 (67%) 3,519 (44%) 190 5 5
Table 5. Kanji/Hanzi conversion results
Japanese Chinese
?? (damage; casualty; victim) ?3 (be murdered; victimization)
? (samurai; warrior; servant) 9 (servant)
?? (a corner; competent) n (a corner; a unit used for money;)
d (charcoal iron; noshi - greeting paper) ?? (charcoal iron)
?? (character formation type) ?? (character formation type; knowing;
understanding)
? (work one?s way) C? (hardship study)
?? (set in place - Buddha statue or
corpse)
?? (set in place - for anything)
{? (artificiality; deliberateness; aggres-
sive action)
*? (action; acomplishment; regard as)
Yu (fall; drop) a (fall; drop; whereabouts; find a place
for; reprove)
?a (attend on the Emperor in his travels;
accompany in the imperial trains)
?a (offer sacrifice to; people gave com-
mend performances in an imperial palace)
Table 6. Part-of translation examples
The advantage of this method is that we can get exact translation for those
borrowed words from Chinese, especially idioms. We all know that it is always
difficult to translate idiomatic phrases from one language to another due to the
different cultural background. If we were to use English as the pivot language
to translate from Japanese to Chinese, it is difficult to have two different bilin-
678 C.-L. Goh, M. Asahara, and Y. Matsumoto
Japanese Chinese
?? (true; really) ?h (ought; should)
?? (nonmember) ?i (ministry councillor; landlord)
?} (deportation; banishment; exile) 5 (flee hither and thither)
s
 (light indigo) ?? (variety; designs and colors)
g (vegetables) h (potherb)
? (picture scroll) ?l (size of a picture)
?o (divide into halves) R? (reduce by half)
?? (self-surrender) ? (private prosecution)
D? (selfish; calculating) K? (plan; intend; calculate)
?? (search for cube root) ? (draw; issue; open)
?? (take a person into custody) ?Z (seduce; tempt)
Table 7. Wrong translation examples
gual dictionaries from two different publishers that translate them in the same
wordings. Since a lot of the idioms in Japanese are originally from China, the
conversion of kanji/hanzi will make the translation process faster and more ac-
curate. Some examples are given below.
???? (same bed different dream - cohabiting but living in different worlds)
O??? (better to be the beak of a rooster than the rump of a bull - better to
be the leader of a small group than a subordinate in a large organization)
#wk? (appearing in unexpected places and at unexpected moments)
The difficulty of this method is the translation of single character words.
Single character words normally have wider meaning (multiple senses) and the
usage is usually based on the context. It is fair enough if we translate the single
character words using the conversion table. However, these characters should
have more translations of other multi-character words. There are 2,049 single
character nouns in Japanese and 1,873 of them exist in Chinese after the con-
version. For verbal nouns, there are 128 Japanese words and 127 words exist in
Chinese (only ? (gossip, rumor) does not exist in Chinese).
3.3 Intergration
We combine both using English as the pivot language and kanji/hanzi conversion
method to get the final list of translation candidates. Table 8 shows the results
in details. We obtain 20,630 for nouns and 5,356 for verbal nouns. In total, we
obtain 28,495 words, in which 7,941 words are new translations. Furthermore,
we add in high quality translation candidates into the new bilingual dictionary.
2,428 of the candidates obtained using kanji/hanzi conversion method already
exist in the translation candidates using English as the pivot language. This can
help to double check on the list of translation candidates and make them rank
higher. 4,893 candidates are served as extra and better quality candidates on
top of the translation candidates obtained using English as the pivot language.
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 679
POS Kanji/ Acc. Est. Using Acc. Est. Total In Extra New
hanzi English
Nouns 11,743 97% 11,391 14,275 88% 12, 562 20,630 2,008 3,380 6,355
Verbal nouns 3,519 97.5% 3,431 3,770 69.5% 2,620 5,356 420 1,513 1,586
Verbs - - - 2,509 73% 1,832 2,509 - - -
Total 15,262 14,822 20,554 17,014 28,495 2,428 4,893 7,941
Table 8. Integration results
As an estimation, we will get about 17,014 Japanese words with correct
translations in Chinese using English as the pivot language. By using kanji/hanzi
conversion method, we could get about 14,822 words with correct translation.
4 Discussion and Future Work
In our survey, only 33% of nouns and 44% of verbal nouns created by kanji/hanzi
conversion method exist in the Peking University dictionary. However, this may
be due to the incompleteness of the Chinese dictionary that we used. We also
found some words after the conversion which are acceptable in Chinese though
they do not exist in the dictionary. Some of the examples are as follows: ?
w(autism), ?(the defense of Constituition or religion), ??(sixth sense),
?(the preceding emperor),??(deep sense),?f(misbelief)?. Therefore, we
can further verify the validity of the Chinese words using other resources such
as the information from the web.
The current work consider only kanji/hanzi conversion for Japanese words
that consists on kanji only. There are a lot of words in Japanese that are mixture
of kanji and hiragana. This happens normally with verbs and adjectives. For
example, ?Rd(eat), ?+d(escape), yXw2(produce), ?0(difficult), K0
(happy), ?$(quite)?. We should be able to get some acceptable translations
of these words after removing the hiragana parts, but most of the cases we
cannot obtain the best or good translations. From the 200 verbs that we used
for the evaluation, 139 words exist in Chinese but only 35 are good and 43 are
acceptable. The single characters used in these words are normally used only in
ancient Chinese but not in contemporary Chinese. For example,Rd =? (eat)
and ?d = ?? (throw away), but  (eat) and ? (throw away) in Chinese
are also possible translation in certain contexts. Furthermore, the contemporary
Chinese uses two character words more often than single character words even
they have the same meaning. This is to reduce the semantic ambiguity as single
character words tend to be polysemous. Therefore, direct kanji/hanzi conversion
is not so appropriate and we need another approach to handle this type of words.
We can apply the kanji/hanzi conversion method directly to most of the
Japanese proper nouns, such as person names, organization names and place
names because these names are normally written in kanji characters. Therefore,
we do not need any effort to translate these words from Japanese to Chinese if
680 C.-L. Goh, M. Asahara, and Y. Matsumoto
we have the character conversion table. This will ease a lot in the processing of
machine translation and cross language information retrieval.
The Unicode Consortium encoded the Han characters in Unicode6. Till date,
all the languages that use Han characters have their own encoding systems.
For example, Japanese is encoded in EUC-JP or JIS, Simplified Chinese is in
GB-2312, Traditional Chinese is in Big 5 etc. The same character that is used
in different languages is assigned with different codes. Therefore it is difficult
to convert from one code to another without a conversion table. The Unicode
Consortium solved the problem by unifying the encoding. The same character
with the same ideograph has only one code no matter in which language it is
used. With this unification, it eased a lot on the CJK research, especially in the
area of cross language information retrieval. Currently, they have increased the
number of Han characters from 27,496 characters (version 3.0) to 70,207 char-
acters (version 4.0). Such a huge increment is done by the addition of a large
amount of unusual characters that only have been used in either person names
or place names. With this new version, it covers almost all possible characters
used in hanzi (Chinese), kanji (Japanese) and hanja (Korean). The Unihan (uni-
code for Han characters) provides a lot of information such as the origin, the
specific language using that character, conversion to other encodings etc. The
most useful information in Unihan to our research is the relationship between
the characters. It embeds the links for the variants of characters which are useful
for the conversion from one encoding to the others (Japanese, Traditional Chi-
nese, Simplified Chinese or Korean). If we can make use of this table, then we
can build a complete conversion table that includes all Han characters.
Zhang et al [11] proposed to use kanji information to find the similarity be-
tween a Japanese word and a Chinese word. They matched on the Unicode and
calculated the similarity using the one time inverse consultation. Since they did
not make any conversion such as traditional characters to simplified characters,
some of the characters have the same meaning but different Unicodes. There-
fore, they could not be matched. If they could use the conversion table that we
proposed, then it would help to increase the score of the kanji words.
To convert from Japanese kanji to Simplified characters is easier than the
reverse. It is because some characters in Traditional characters are simplified into
the same characters in Simplified Chinese. For example,b (hair) and` (deliver)
are simplified to. Therefore, it has to depend on the contexts to decide which
Japanese character to use if we were to convert the Chinese Simplified characters
back to Japanese kanji.
5 Conclusion
As a conclusion, we proposed a method to compile a Japanese-Chinese dictionary
using English as the pivot language as a starting point. We made use of the public
available resources such as EDICT, CEDICT and IPADIC for the construction
6 http://www.unicode.org/chart/unihan.html
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 681
of the new language pair. The accuracy obtained is 77%. Since Japanese and
Chinese share common Han characters which are semantically heavy loaded,
the same characters used should carry the same meaning. Therefore, we also
proposed a kanji/hanzi conversion method to increase the translation candidates.
The accuracy obtained is 97%. The increment of translation candidates is 9%,
from 24% to 33%. The conversion table created can also be used in other fields
like machine translation and cross language information retrieval.
Acknowledgements
This research uses EDICT file which is the property of the Electronic Dictio-
nary Research and Development Group at Monash University. Thanks go to
http://www.mandarintools.com/zhcode.html for their Chinese Encoding Con-
verter.
References
1. Shogakukan and Peking Shomoinshokan, editors: Ri-Zhong Cidian [Japanese-
Chinese Dictionary] (1987)
2. Asahara, M., Matsumoto, Y.: IPADIC version 2.7.0. Users Manual. Nara Institute
of Science and Technology, Nara, Japan. (2003) http://chasen.naist.jp/.
3. Matsumoto, Y., Kitauchi, A., Yamashita, T., Hirano, Y., Matsuda, H., Takaoka, K.,
Asahara, M.: Morphological Analysis System ChaSen version 2.2.9 Manual. Nara
Institute of Science and Technology, Nara, Japan. (2002) http://chasen.naist.jp/.
4. Tanaka, K., Umemura, K.: Construction of a bilingual dictionary intermediated
by a third language. In: Proc. of COLING. (1994) 297?303
5. Lafourcade, M.: Multilingual dictionary construction and services - case study with
the fe* projects. In: Proc. of PACLING. (1997) 289?306
6. Bond, F., Sulong, R.B., Yamazaki, T., Ogura, K.: Design and construction of a
machine-tractable japanese-malay dictionary. In: Proc. of MT Summit VIII. (2001)
53?58
7. Shirai, S., Yamamoto, K.: Linking english words in two bilingual dictionaries to
generate another language pair dictionary. In: Proc. of ICCPOL. (2001) 174?179
8. Zhang, Y., Ma, Q., Isahara, H.: Automatic acquisition of a japanese-chinese bilin-
gual lexicon using english as an intermediary. In: Proc. of NLPKE. (2003) 471?476
9. Jim Breem: EDICT, Japanese-English Dictionary (2005)
http://www.csse.monash.edu.au/?jwb/edict.html.
10. Paul Denisowski: CEDICT, Chinese-English Dictionary (2005)
http://www.mandarintools.com/cedict.html.
11. Zhang, Y., Ma, Q., Isahara, H.: Use of kanji information in constructing a japanese-
chinese bilingual lexicon. In: Proc. of ALR Workshop. (2004) 42?49
12. Levenshtein, V.: Binary codes capable of correcting deletions, insertions and re-
versals. Doklady Akademii Nauk SSSR 163 (1965) 845?848
13. Zhang, Y., Isahara, H.: Acquiring compound word translation both automatically
and dynamically. In: Proc. of PACLIC 18. (2004) 181?185
14. Peking University: (Peking University Dictionary) http://www.icl.pku.edu.cn/.
Chinese Deterministic Dependency Analyzer: Examining Effects of 
Global Features and Root Node Finder 
Yuchang CHENG, Masayuki ASAHARA and Yuji MATSUMOTO 
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan 
{yuchan-c, masayu-a, matsu}@is.naist.jp
Abstract
We present a method for improving 
dependency structure analysis of Chi-
nese. Our bottom-up deterministic ana-
lyzer adopt Nivre?s algorithm (Nivre 
and Scholz, 2004). Support Vector Ma-
chines (SVMs) are utilized to deter-
mine the word dependency relations. 
We find that there are two problems in 
our analyzer and propose two methods 
to solve them. One problem is that 
some operations cannot be solved only 
using local feature. We utilize the 
global features to solve this. The other 
problem is that this bottom-up analyzer 
doesn?t use top-down information. We 
supply the top-down information by 
constructing SVMs based root node 
finder to solve this problem. Experi-
mental evaluation on the Penn Chinese 
Treebank Corpus shows that the pro-
posed extensions improve the parsing 
accuracy significantly. 
1 Introduction 
Many syntactic analyzers for English have been 
implemented and have demonstrated good per-
formance (Charniak, 2000; Collins, 1997; Rat-
naparkhi, 1999). However, implementation of 
Chinese syntactic structure analyzers is still lim-
ited, since the structure of the Chinese language 
is quite different from other languages. There-
fore the experience in processing western lan-
guages cannot be guaranteed that it can apply to 
Chinese language directly (Lee, 1991). Chinese 
language has many special syntactic phenomena 
substantially different from western languages. 
Discussions about such characteristics of Chi-
nese language can be found in the literature 
(Chao 1968; Li and Thompson 1981; Huang 
1982).
About the previous work of Chinese depend-
ency structure analysis, Zhou proposed a rule 
based approach (Zhou, 2000). Lai et al pro-
posed a span-based statistical probability ap-
proach (Lai, 2001). Ma et al proposed a statistic 
dependency parser by using probabilistic model 
(Ma, 2004). Using machine learning-based ap-
proaches for dependency analysis of Chinese is 
still limited. In this paper, we propose a deter-
ministic Chinese syntactic structure analyzer by 
using global features and a root node finder.  
Our analyzer is a dependency structure ana-
lyzer. We utilize a deterministic method for de-
pendency relation construction. First, a 
dependency relation matrix is constructed, in 
which each element corresponds to a pair of to-
kens. A likelihood value is assigned to the de-
pendency relation of each pair of tokens.  
Second, the optimal dependency structure is es-
timated using the likelihood of the whole sen-
tence, provided there is no crossing between 
dependencies. A bottom-up algorithm proposed 
by (Nivre and Scholz, 2004) is use for a deter-
ministic dependency structure analysis. Our de-
pendency relations are composed by machine 
learners. SVMs (Vapnik, 1998) deterministically 
estimate if there is a dependency relation be-
tween a pair of words in the methods. 
However, this method has two problems. First, 
some operations in the algorithm needs long 
distance information. However, the long dis-
tance information cannot be available if we as-
sume a context of a fixed size in all operations. 
17
The second problem is that the top-down infor-
mation isn?t used in the bottom-up approach. 
We use the global features to solve the first 
problem and we construct a SVM-based root 
node finder in our system to supplement the top-
down information. 
Our analyzer is trained on the Penn Chinese 
Treebank 5.0 (Xue et al, 2002), which is a phrase 
structure annotated corpus. The phrase structure 
is converted into a dependency structure accord-
ing to the head rules. We perform experimental 
evaluation in several settings on this corpus. 
In the next section, we describe our determi-
nistic dependency structure analysis algorithm. 
Section 3 shows the global features and the two-
step process. Section 4 describes the use of the 
root node finder. Section 5 describes the ex-
perimental setting and the results. Finally, we 
summarize our findings in the conclusion. 
2 Parsing method 
This chapter presents a basic parsing algorithm 
proposed by (Nivre and Scholz, 2004). The al-
gorithm is the base of our dependency analyzer. 
This algorithm is based on a deterministic ap-
proach, in which the dependency relations are 
constructed by a bottom-up deterministic 
schema. While Nivre?s method uses memory-
based learning, we use SVMs instead. The algo-
rithm consists of two major procedures:  
(i) Extract the surrounding features for the 
focused node (or node pair). 
(ii) Estimate the dependency relation opera-
tion for the focused node by a machine 
learning method. 
Example: ???????????? (The great triumph that Cheng Cheng-Kung recaptured Taiwan.)
Fig. 1. The operations of the Nivre algorithm
??
recaptured
VV
???
(name)
NR
S I
??
recaptured
VV
???
(name)
NR
S I
Right
S I
??
recaptured
VV
???
(name)
NR
S I
Left
S I S I
Reduce
S I S I
Shift
??
recaptured
VV
???
(name)
NR
??
Taiwan
NR
??
Taiwan
NR
??
recaptured
VV
???
(name)
NR
??
Taiwan
NR
??
recaptured
VV
???
(name)
NR
??
Taiwan
NR
?
DE
DEG
?
DE
DEG
??
recaptured
VV
???
(name)
NR
??
Taiwan
NR
??
recaptured
VV
???
(name)
NR
??
Taiwan
NR
??
great
VA
?
DE
DEG
??
great
VA
?
DE
DEG
??
Triumph
NN
??
great
VA
??
great
VA
?
DE
DEG
?
DE
DEG
??
great
VA
??
Taiwan
NR
??
Taiwan
NR
position t-1 position n position n+1position t
t-1 n n+1t
t-1 n n+1t
t-1 n n+1t t-1 n n+1t
t-1 n n+1t
t-1 n n+1t
t-1 n n+1t
A{    } A{??? ->?? }
A{??? ->??} A{??? ->?? ,?? ->?? }
A{??? ->?? ,
?? ->?? }
A{??? ->?? ,
?? ->?? }
A{??? ->?? ,
?? ->?? }
A{??? ->?? ,
?? ->?? }
18
2.1   Algorithm 
We utilize a bottom-up deterministic algorithm 
proposed by (Nivre and Scholz, 2004) in our 
analyzer. In the algorithm, the states of analyzer 
are represented by a triple AIS ,, . S and I are 
stacks, S keeps the words being in consideration, 
and I keeps the words to be processed. A is a list 
of dependency relations decide during the algo-
rithm. Given an input word sequence W, the 
analyzer is initialized by the triple ?,,Wnil .
The analyzer estimates the dependency relation 
between two words (the top elements of stack S
and stack I). The algorithm iterates until the list 
I becomes empty. Then, the analyzer outputs the 
word dependency relations A.
There are four possible operations for the con-
figuration at hand: 
Right: Suppose the current triple is 
AInSt ,|,| (t and n are the top elements, S and 
I are the remaining elements in the stacks), if 
there is a dependency relation that the word t
depends on word n, add the new dependency 
relation ( )nt ?  into A, remove t from S. The 
configuration now becomes ( ){ }ntAInS ?,|, .
Left: In the current triple is AInSt ,|,|  , if 
there is a dependency relation that the word n
depends on the word t, adds the new dependency 
relation ( )tn ?  into A, push n onto the stack S.
The configuration now becomes 
( ){ }tnAIStn ?,,|| .
Suppose the current triple is AInSt ,|,| , if 
there is no dependency relation between n and t, 
check the following conditions. 
Reduce: If there are no more words 'n ( In ?' )
which may depend on t, and t has a parent on its 
left side, the analyzer removes t from the stack S.
The configuration now becomes AInS ,|, .
Shift: If there is no dependency between n and t, 
and the triple doesn?t satisfy the conditions for 
Reduce, then push n onto the stack S. The con-
figuration now becomes AIStn ,,|| .
These operations are depicted in Fig. 1. Given 
an input sentence of length N (words), the ana-
lyzer is guaranteed to terminate after at most 2N
actions. The dependency structure given at the 
termination is well-formed if and only if the re-
lations in A constitute a single connected tree. 
This means that the algorithm produces a well-
formed dependency graph.  
2.2   Machine learning method 
A classification task usually involves with train-
ing and testing data which consist of annotated 
data instances. Each instance in the training set 
contains one ?target value? (class label) and 
several ?attributes? (features). The goal of a 
classifier is to produce a model which predicts 
target value of data instances in the testing set 
which only give the attributes. 
SVMs are binary classifiers based on the 
maximal margin strategy. Suppose we have a set 
of training data for a binary classification prob-
lem: )y)...(y( nn11 ,, ZZ , where nR?iZ  is the fea-
ture vector of the i-th sample in the training data 
and }1,1{ ?+?iy is the class label of the sample. 
The goal is to find a decision function 
))(()( ?
?
+=
SV
ii
i
bKyasignxf
\
i,\Z  for an input vec-
tor Z . The vectors SV?K\  are called support 
vectors, which are representative examples. 
Support vectors and other constants are deter-
mined by solving a quadratic programming 
problem. )( zx,K is a kernel function which maps 
vectors into a higher dimensional space. We use 
the polynomial kernel: dK )1()( zxzx, ?+= . The 
performance of SVMs is better than using other 
machine learning methods, such as memory 
based learning or maximum entropy method, in 
our analyzer. This is because that SVMs can 
adopt combining features automatically (using 
the polynomial kernel), whereas other method 
cannot. To extend binary classifiers to multi-
class classifiers, we use the pair-wise method, 
which utilizes 2Cn  binary classifiers between all 
pairs of the classes (Kreel, 1998). We use 
Libsvm (Lin et al, 2001) in our experiments. 
2.3   Features (Local features) 
 It should be noted that we use a different ma-
chine learner from the original method (Nivre, 
2004). Nivre?s work used memory based learn-
ing in their analyzer, we utilize SVMs in our 
analyzer. Therefore, the features of our analyzer 
are different from the original Nivre?s method.  
In our method, the analyzer considers the de-
pendency of two nodes (n,t) which are in current 
19
triple. The nodes include the word, the POS-tag 
and the information of its children. The context 
features we use are 2 preceding nodes of node t
(and t itself), 2 succeeding nodes of node n (and
n itself), and their child nodes. The distance be-
tween nodes n and t is also used as a feature.  
We call these features as local features.
3 Global features and two-step process 
In the algorithm, the operation Reduce needs 
the condition that the node n should have no 
child in I. However, it is difficult to check this 
condition. In a long sentence, the modifier of the 
focused node n may be far away from n. More-
over, some non-local dependency may cause this 
kind of error. In this section, we will describe 
this problem and a solution to it. 
3.1   Global features 
The analyzer selects features for deciding the 
optimum operation, and then gives these fea-
tures to machine learner. The machine learner 
uses the same information to decide the opti-
mum operation even when these operations es-
sentially disagree. However, the different 
operation consists of different condition. In the 
deterministic bottom-up dependency analysis, 
we can generally consider the process as two 
tasks:
Task 1: Does the focused word depend on a 
neighbor node? 
Task 2: Does the focused word may have a 
child in the remaining token sequence? 
In the Task 1, the problem can be resolved by 
using the information of the neighbor nodes. 
This information is possibly the same as the fea-
tures that we described in section 2.3. However, 
these features may not be able to resolve the 
problem in task 2. For resolving the problem in 
task 2, we need the information of long distance 
dependency. In Fig. 2, for example, the analyzer 
is considering the relation between focused 
words ??? (tell)? and ?? (he)?. The features 
used in this original analysis are the information 
of words ?? (please)?, ??? (tell)?, ??(he)?, 
??? (what time)? and ??? (prepare)?. These 
features are ?local features?. The correct answer 
in this situation is the operation ?Shift?. It is 
because the word ??? (tell)? has a child ???
(start)? which is not yet analyzed and the fo-
cused words don?t depend on each other. How-
ever, the local features do not include the 
information of word ??? (start)?. Therefore, 
the analyzer possibly estimates the answer as the 
operation ?Reduce?. The results make a mistake 
in this situation because of the lack of long dis-
tance information. To resolve this problem, we 
should refer some information of long distance 
dependency in machine learning. The informa-
tion about long distance relations is defined as 
?global features?. In this paper, we select the 
words which remain in stack I but don?t be con-
sider in local features as global features. 
Fig. 2. An example of the ambiguity of deciding the long distance dependency relation and using two-
steps classification dependency relation 
??
prepare
?
please
?
you
??
tell
?
I
??
What time
??
start
?
He
S I
(Please  tell me what time he will prepare to start.)
Classification 
with local 
features
Output :shift
Local features
Global features
Classification 
with  global 
features
Output :
reduce
20
3.2   two-step process 
To use the global features, we cannot use them 
immediately because the global features are not 
effective in all operations. For using global fea-
tures efficiently, we propose a two-step process 
in our analyzer. The analysis processes are di-
vided to two processes. First, the analyzer uses 
only the local features (as described in Section 
2.3) to decide the optimum operation. If the re-
sult is ?Reduce? or ?Shift?, it means that the 
focused words do not have any dependency rela-
tion. The analyzer leaves the decision to another 
machine learner that makes use of global fea-
tures. The analyzer will select global features for 
analyzing the Task 2. Then the analyzer outputs 
the final answer of this analysis process.  
Fig. 2 describes an example of using two-step 
classification for analyzing dependency relation. 
In this example, the focused words are ?? (I)?
and ?? (He)?. The word ?? (I)? depends on 
the word ??? (tell)?. The local features are 
surrounded by dotted line and the global features 
are surrounded by solid line. The analyzer used 
local features to analyze the operation of this 
situation. The result is the operation ?shift?. The 
analyzer then selected the global features to ana-
lyze again and the output is the operation ?re-
duce?. The final result of this situation is the 
operation ?reduce?.
4 The root node finder 
In Isozaki?s work (Isozaki et. al, 2004), they 
adopted a root finder in their system to find the 
root word of the input sentence. Their method 
used the information of the root word as a new 
feature for machine learning. Their experiments 
showed that information of root word was a 
beneficial feature. However, we think the infor-
mation of root word can be used not only as the 
feature of machine learning, but also can be used 
to divide the sentence. Therefore, the complex-
ity of the sentence can be alleviated by dividing 
the input sentence. 
4.1   Root node and dividing sentence by 
using root finder 
In the fundamental definition of dependency 
structure, there is one and only one head word in 
a dependency structure. An element cannot have 
dependents lying on the other side of its own 
governor.  
These peculiarities imply that the head word 
divides the phrase into two independent parts 
and each part does not cross the head word. As 
in Fig. 3, the original input sentence has a root 
word (the head word of phrase) ?? (and)?. 
There are not any dependency relation which 
crosses the root word. Therefore we can divide 
this sentence into two sub-sentence ??? (exo-
dus) / ? (do) / ?? (study) / ? (and)? and ??
(and) / ? (go) / ?? (foreign country) / ?(do)
/ ?? (visit)?. Both these sub-sentences have 
their root word and the root word is ??(and)?.
We can conceive that to analyze the dependency 
structure of the full sentence is to analyze the 
dependency structure of two sub-sentences. 
Combining structures of two sub-sentences, we 
can get the full structure of original sentence. 
Our dependency analyzer is a bottom-up deter-
ministic analyzer. Instinctively, the accuracy of 
analyzing short sentence is significantly better 
than analyzing long sentence. Thus the perform-
ance of the dependency analyzer can be im-
proved by this method. 
4.2   Constructing a root finder 
To use the root node, we should construct the 
root finder. Similarly to Isozaki?s work, we use 
machine learner (SVMs) to construct the root 
finder. We refer to the features which are used 
in Isozaki?s work and investigate other effective 
features. The performance of our root node 
finder is 90.71%. This is better than the root ac-
curacy of our analyzer (86.22%, see Table 2).
Fig. 3. Dividing the phrase as two phrases by the root 
word 
?? ? ?? ? ? ?? ? ??
(To Leave native country to study and to visit other country.)
The root word
?? ? ?? ? ? ? ?? ? ??
The root word The root word
Original input 
sentence:
Divide by the 
root word:
Part 1 Part 2
21
Therefore, using the root finder can give the de-
pendency analyzer more top-down information.  
The tags and features of the root finding are 
shown in Fig. 4. We extract all root words in the 
training data and tagging every word to show 
that it is root word or not. For example, the root 
word in Fig. 4 is ??? (get)?. The root finder 
analyzes each word in the sentence and gives the 
tag ?true? or ?false? to indicate the root word. 
The features for machine learning of root finder 
include the contextual features (the information 
about the focused word, the two preceding 
words, and two succeeding words) and the word 
relation features (the words which are in the out-
side of the window). Other effectual features 
include the Boolean features ?root word is 
found? and ?the focus word is the first/last word 
of sentence?. For example, the contextual fea-
tures of the word ???  (economic)? include 
information of the focused (n) word ??? (eco-
nomic)?, the ?n-1?th word ??? (wide)?, the 
?n-2?th word ?? (DE)?, the ?n+1?th word? ?
?  (environment)? and the ?n+2?th word ??
(will)?. The word relation features include the 
preceding word set {??  (China)}, the suc-
ceeding word set {??, ???, ?, ??} and 
the Boolean features are: 
?root_word_is_found=false?,  
?first_word=false? ,?last_word=false?.  
When we use the root finder to analyze the 
root word of the sentence, we do not know the 
structure of input sentence (either the phrase 
structure or the dependency structure). It may 
look odd that the root finder can analyzes the 
root word without any information of the struc-
ture. However, this analysis is practicable. Natu-
rally, the root word of a sentence is usually a 
verb (about 61% of sentences have a verb as the 
root word in our testing corpus). For example, in 
the example 1 of Fig. 5 ?? / ? / ?? (I go to 
school)?, we know the POS-tags are ?noun, verb, 
noun? thus we can find that the root word is ??
(go)?. However, many sentences include more 
then one verb or the root word is not verb (in NP 
or PP?etc.). We can not only choose the verbs 
as root word directly. To decide the root word of 
complex sentences, there are some special 
word/POS relations that can be used to estimate 
the root node of a sentence. Considering the root 
finder in Fig. 4, the root finder gives the root tag 
to each word of the sentence. 
The processes of analyzing the root word can 
be thought as two tasks:  
Task 1: Does the focus word depend on a 
neighbor word?  
Task 2: Are there any special relation in the sen-
tence? 
 In Fig. 4, the contextual features (two pre-
ceding words and two succeeding words) can be 
used to process the Task 1, and the word rela-
tion features can be used to process the Task 2.
If the focused word possibly depends on  
neighbor words, it is impossible that the focused 
word is the root word. Therefore these words 
will be tagged as ?false?. 
Alternately, considering the example 2 in Fig.
5, the sentence has a verb ??? (recapture)?,
but the special word ?? (DE)? is in the right 
side of the verb ??? (recapture)?. Therefore, 
the verb ??? (recapture)? is possibly in the?
(DE)-phrase and the verb cannot be the root 
word. The special word ?? (DE)? resembles a 
preposition and it is always the last word of DE-
phrase. Therefore, although we do not know the 
structure of sentence, we can identify which 
words can be the root word by the relation and 
position of the features. If the features of the 
focused word include the special word relations 
Fig. 4. The features and tag of root finder 
Word POS Tag
?? NR false
? DEG false
?? JJ false
?? NN false
?? NN false
? AD false
?? VV true
??? JJ false
? DEG false
?? NN false
EOS
Position 0
Position -1
Position -2
Position 1
Position 2
Focus word
Contextual 
feature
Word 
relation Fig. 5. The examples of analyzing the root word 
of sentences 
Root
??? ?? ?? ? ?? ??
NR       VV       NR      DEG   VA        NN
(The great triumph that Cheng Cheng-Kung recaptured 
Taiwan. )
? ? ??
DT      VV      NN
(I go school.) Root
Example 1:
Example 2:
22
(for example, the focused word is in the preposi-
tional phrase), it isn?t the root word. The fea-
tures ?word relations? in Fig. 5 can consider this 
situation.
5 Experiments 
5.1 Corpus and estimation 
We use Penn Chinese Treebank 5.0 (Xue et al, 
2002) in our experiments. This Treebank is rep-
resented by phrase structure and doesn?t include 
the head information of each phrase. The first 
step of using Penn Chinese Treebank is to derive 
the head rules for deciding the head word of 
each phrase. Some examples of head rules are 
shown in Table 1. We convert the Treebank by 
using these head rules. The training corpus in-
cludes about 377,408 words for learning and 
63,886 words for testing. It should be noted that 
the punctuation mark ??? marks the end of a 
sentence in the Treebank. However, the punc-
tuation mark ??? also can be the end of a sen-
tence. It is hard to determine the dependency 
rule of the clauses on the both side of comma. 
Therefore, to decide the dependency relation 
which crosses a punctuation mark ??? is difficult. 
We do not deal with the ambiguity of commas 
and divide the sentence by the punctuation mark 
???.
Phrase The order of deciding the head 
of phrase (from left) 
ADJP CC PZ ADJP JJ 
ADVP CC PZ AD 
CLP PZ CLP M LC 
DP DP CLP QP DT 
DVP DEV DEC DEG 
VCP VC VV 
Table 1. Some examples of head rules 
The performance of our dependency structure 
analyzer is evaluated by the following three 
measures:  
Dependency Accuracy: 
relationsdependencyofnumber
relationsdependencyanalyzedcorrectlyofnumber
=
Root Accuracy:  
clausesofnumber
nodesrootanalyzedcorrectlyofnumber
=
Sentence Accuracy: 
clausesofnumber
clauseanalyzedcorrectlyfullyofnumber
=
5.2 Results and discussion 
Our experimental results are shown in Table. 2.
First row in the table is the result of our basic 
analyzer (Nivre algorithm with SVMs), second 
and third row show the effects of the proposed 
extensions. The last row is the result of combin-
ing the two extensions. We had used McNemar 
test to confirm the significance of the methods. 
The McNemar test proves that using the pro-
posed methods improve the analyzers signifi-
cantly. Comparing the results of our basic 
analyzer to related works, our analyzer (dep. 
Accuracy: 87.64) is better than (Ma et al, 2004, 
dep. Accuracy: 80.38) and (Zhou, 2000, dep. 
Accuracy of newspaper: 67.7). However, these 
researches used different corpus. We cannot 
compare the performances directly.  
According to the second row of Table. 2, di-
viding the process of classification as two steps 
can improve the performance of dependency 
analyzer. However, the improvement of using 
this method is limited. This is because that long 
distance relations are not many in the corpus. 
The absence of global information does not oc-
cur in the sentences without long distance rela-
tions. Another reason is the distribution of 
operations. The instances of operations in our 
experimental corpus are not balanced. The op-
eration ?reduce? is the least (7.8%) and it is far 
less than other operations. Therefore the in-
stances for creating the model of operation ?re-
duce? are not satisfactory. These facts result in   
that our experiment of using two step classifica-
tion cannot improve the analyzer remarkably. 
About the experiment of utilizing root finder 
in our analyzer, we tried to adopt the root infor-
mation to the analyzer (using the information as 
features for machine learning). However, the 
performance is worse than the baseline (the fun-
damental analyzer ?Nivre+SVMs?). Therefore, 
we use our method to improve the analyzer by 
using root information (dividing the sentence 
according to root node). 
According to the third row of Table. 2, divid-
ing the sentence into two sub-sentences can im-
prove the performance of dependency analyzer. 
However, the sentence accuracy cannot increase 
reliably. This result shows that using root finder 
and dividing sentence can reconstruct some mis-
takes in sentences. Certainly, the performance of 
the root finder influences the analyzer strongly. 
If we use a perfect root node finder into our ana-
lyzer, the performance will improve signifi-
cantly. 
23
The last row of Table. 2 shows the results of 
combining the two proposed methods (using 
global features and root node finder) to improve 
our analyzer. Combining two methods can in-
crease the dependency accuracy better than us-
ing either one of the methods. It means that 
some analysis errors of fundamental analyzer 
can be resolved by using both improvement 
methods. Therefore using combined method 
cannot supply higher improvement. 
 Dep. 
Acc.
Root
Acc.
Sent.
Acc.
Baseline
(Nivre with 
SVMs)
85.25 86.18 59.98 
Baseline with 
two-step
process
85.44 86.22 60.1 
Baseline with 
root node 
finder
86.13 90.94 61.33 
Baseline with 
two-step
process and 
root node 
finder
86.18 90.94 61.33 
Table 2. The experimental results 
6 Conclusion and future work 
In this paper, we present two methods to im-
prove a deterministic dependency structure ana-
lyzer for Chinese. This basic analyzer 
implements a bottom-up deterministic algorithm 
with SVMs. We convert a phrase structure anno-
tated corpus (Penn Chinese Treebank) to de-
pendency tagged corpus by using head rules. 
According to the properties of Chinese language 
and dependency structure, we try to add a root 
finder in our dependency analyzer to improve 
the analyzer. Moreover, considering the machine 
learning process of our analyzer, we divide the 
process into two processes to improve the per-
formance of analyzer. The improving methods 
(using root finder and dividing machine learning 
process) showed to improve the analyzer. 
Future work includes three points. First, we 
should improve the performance of the root 
finder. Second, we should construct a useful 
prepositional phrase chunker, because the 
prepositional phrase is a major error source of 
our basic analyzer. The original analyzer tends 
to let the preposition governing a partial subtree 
of the full phrase. According to the properties of 
Chinese language, the prepositional phrases in 
Chinese are head-initial. Intuitively, if we can 
extract the prepositional phrases from sentence, 
the complexity of the sentence will decrease. 
Thus an important task is how to chunk the 
prepositional phrase in the sentence.  
Finally, we should deal with the ambiguity of 
the meaning of punctuation mark ?,?.  The defi-
nition of ?sentence? is ambiguous in Chinese. In 
Chinese articles, the normal ending mark of a 
sentence is the punctuation mark ???. However, 
the mark ??? is often used at the end of a sen-
tence. To distinguish the meaning of the punc-
tuation mark ??? is difficult. Therefore, we 
should adopt semantic analysis in our analyzer. 
References
1. Eugene Charniak, 2001. Immediate-Head Parsing 
for Language Models. pages 124-131, NAACL-
2001. 
2. Yuen Ren Chao, 1968. A Grammar of Spoken 
Chinese. Berkeley, CA: University of California 
Press.  
3. Michael Collins, Brian Roark, 2004, Incremental 
parsing with the Perceptron algorithm. Pages 112-
119, ACL-2004. 
4. J. Huang, 1982. Logical relations in Chinese and 
the theory of grammar Doctoral dissertation, Mas-
sachusetts Institute of Technology, Cambridge. 
5. Ulrich. H.-G. Kre?el, 1998. Pairwise classification 
and support vector machines. In Advances in 
Kernel Methods, pages 255?268. The MIT Press. 
6. Chih Jen Lin, 2001. A practical guide to support 
vector classification, http://www.csie.ntu.edu.tw/
~cjlin/libsvm/. 
7. Lai, Bong Yeung Tom, Huang, Changning, 1994. 
Dependency Grammar and the Parsing of Chinese 
Sentences.  PACLIC 1994 
8. Hideki Isozaki, Hideto Kazawa, Tsutomu Hirao, 
2004. A Deterministic Word Dependency Ana-
lyzer Enhanced With Preference Learning, pages 
275-281, COLING-2004 
9. Charles Li, and Thompson Sandra A., 1981. Man-
darin Chinese. University of California Press.  
10. Lin-Shan Lee, Long-Ji Lin, Keh-Jiann Chen, and 
James Huang, 1991. An Efficient Natural Lan-
guage Processing System Specially Designed for 
the Chinese Language. ComputationaI Linguistics, 
Volume 17, Number 4. 
11. Ma Jinshan, Zhang yu, Liu ting, and Li sheng, 
2004. A Statistical Dependency Parser of Chinese-
under Small Training Data. IJCNLP 2004 Work-
shop: Beyond shallow analyses, Formalisms and 
statistical modeling for deep analyses. 
12. Joakim Nivre and Mario Scholz, 2004. Determi-
nistic Dependency Parsing of English Text. Pages 
64-70, COLING-2004. 
13. Adwait Ratnaparkhi, 1999. Learning to parse 
natural language with maximum entropy models. 
Machine Learning, 34(1-3) pages151?175. 
14. Vladimir N. Vapnik, 1998. Statistical Learning 
Theory.  A Wiley-Interscience Publication. 
15. Nianwen Xue, Fu-Dong Chiou, Martha Stone 
Palmer, 2002. Building a Large-Scale Annotated 
Chinese Corpus. COLING 2002 
16. Ming Zhou, 2000. A block-based robust depend-
ency parser for unrestricted Chinese text. The sec-
ond Chinese Language Processing Workshop 
attached to ACL-2000. 
24
Combination of Machine Learning Methods
for Optimum Chinese Word Segmentation
Masayuki Asahara
Chooi-Ling Goh
Kenta Fukuoka
Yotaro Watanabe
Nara Institute of Science and Technology, Japan
E-mail: cje@is.naist.jp
Ai Azuma
Yuji Matsumoto
Takashi Tsuzuki
Matsushita Electric
Industrial Co., Ltd.
Abstract
This article presents our recent work for par-
ticipation in the Second International Chi-
nese Word Segmentation Bakeoff. Our
system performs two procedures: Out-of-
vocabulary extraction and word segmenta-
tion. We compose three out-of-vocabulary
extraction modules: Character-based tag-
ging with different classifiers ? maximum
entropy, support vector machines, and con-
ditional random fields. We also com-
pose three word segmentation modules ?
character-based tagging by maximum en-
tropy classifier, maximum entropy markov
model, and conditional random fields. All
modules are based on previously proposed
methods. We submitted three systems which
are different combination of the modules.
1 Overview
We compose three systems: Models a, b and c for the
closed test tracks on all four data sets.
For Models a and c, three out-of-vocabulary (OOV)
word extraction modules are composed: 1. Maximum
Entropy (MaxEnt) classifier-based tagging; 2. Max-
imum Entropy Markov Model (MEMM)-based word
segmenter with Conditional Random Fields (CRF)-
based chunking; 3. MEMM-based word segmenter
with Support Vector Machines (SVM)-based chunk-
ing. Two lists of OOV word candidates are constructed
either by voting or merging the three OOV word ex-
traction modules. Finally, a CRFs-based word seg-
menter produces the final results using either of the
voted list (Model a) or the merged list (Model c).
Most of the classifiers use surrounding words and
characters as the contextual features. Since word and
character features may cause data sparse problem, we
utilize a hard clustering algorithm (K-means) to define
word classes and character classes in order to over-
come the data sparse problem. The word classes are
used as the hidden states in MEMM and CRF-based
word segmenters. The character classes are used as the
features in character-based tagging, character-based
chunking and word segmentation.
Model b is our previous method proposed in (Goh
et al, 2004b): First, a MaxEnt classifier is used to per-
form character-based tagging to identify OOV words
in the test data. In-vocabulary (IV) word list together
with the extracted OOV word candidates is used in
Maximum Matching algorithm. Overlapping ambi-
guity is denoted by the different outputs from For-
ward and Backward Maximum Matching algorithm.
Finally, character-based tagging by MaxEnt classifier
resolves the ambiguity.
Section 2 describes Models a and c. Section 3 de-
scribes Model b. Section 4 discusses the differences
among the three models.
2 Models a and c
Models a and c use several modules. First, a hard
clustering algorithm is used to define word classes and
character classes. Second, three OOV extraction mod-
ules are trained with the training data. These modules,
then, extract the OOV words in the test data. Third,
the OOV word candidates produced by the three OOV
extraction modules are refined by voting (Model a) or
merging (Model c) them. The final word list is com-
posed by appending the OOV word candidates to the
IV word list. Finally, a CRF-based word segmenter
analyzes the sentence based on the new word list.
2.1 Clustering for word/character classes
We perform hard clustering for all words
and characters in the training data. K-
means algorithm is utilized. We use R 2.2.1
(http://www.r-project.org/) to perform
k-means clustering.
134
Since the word types are too large, we cannot run k-
means clustering on the whole data. Therefore, we di-
vide the word types into 4 groups randomly. K-means
clustering is performed for each group. Words in each
group are divided into 5 disjoint classes, producing 20
classes in total. Preceding and succeeding words in the
top 2000 rank are used as the features for the cluster-
ing. We define the set of the OOV words as the 21st
class. We also define two other classes for the begin-
of-sentence (BOS) and end-of-sentence (EOS). So, we
define 23 classes in total.
20 classes are defined for characters. K-means clus-
tering is performed for all characters in the training
data. Preceding and succeeding characters and BIES
position tags are used as features for the clustering:
?B? stands for ?the first character of a word?; ?I? stands
for ?an intermediate character of a word?; ?E? stands
for ?the last character of a word?; ?S? stands for ?the
single character word?. Characters only in the test data
are not assigned with any character class.
2.2 Three OOV extraction modules
In Models a and c, we use three OOV extraction mod-
ules.
First and second OOV extraction modules use
the output of a Maximam Entropy Markov Model
(MEMM)-based word segmenter (McCallum et al,
2000) (Uchimoto et al, 2001). Word list is composed
by the words appeared in 80% of the training data.
The words occured only in the remaining 20% of the
training data are regarded as OOV words. All word
candidates in a sentence are extracted to form a trel-
lis. Each word is assigned with a word class. The
word classes are used as the hidden states in the trellis.
In encoding, MaxEnt estimates state transition proba-
bilities based on the preceding word class (state) and
observed features such as the first character, last char-
acter, first character class, last character class of the
current word. In decoding, a simple Viterbi algorithm
is used.
The output of the MEMM-based word segmenter is
splitted character by character. Next, character-based
chunking is performed to extract OOV words. We use
two chunkers: based on SVM (Kudo and Matsumoto,
2001) and CRF (Lafferty et al, 2001). The chunker
annotates BIO position tags: ?B? stands for ?the first
character of an OOV word?; ?I? stands for ?other char-
acters in an OOV word?; ?O? stands for ?a character
outside an OOV word?.
The features used in the two chunkers are the char-
acters, the character classes and the information of
other characters in five-character window size. The
word sequence output by the MEMM-based word seg-
menter is converted into character sequence with BIES
position tags and the word classes. The position tags
with the word classes are also introduced as the fea-
tures.
The third one is a variation of the OOV module in
section 3 which is character-based tagging by MaxEnt
classifier. The difference is that we newly introduce
character classes in section 2.1 as the features.
In summary, we introduce three OOV word extrac-
tion modules: ?MEMM+SVM?, ?MEMM+CRF? and
?MaxEnt classifier?.
2.3 Voting/Merging the OOV words
The word list for the final word segmenter are com-
posed by voting or merging. Voting means the OOV
words which are extracted by two or more OOV word
extraction modules. Merging means the OOV words
which are extracted by any of the OOV word extrac-
tion modules. The model with the former (voting)
OOV word list is used in Model a, and the model with
the latter (merging) OOV word list is used in Model c.
2.4 CRF-based word segmenter
Final word segmentation is carried out by a CRF-based
word segmenter (Kudo and Matsumoto, 2004) (Peng
and McCallum, 2004). The word trellis is composed
by the similar method with MEMM-based word seg-
menter. Though state transition probabilities are esti-
mated in the case of MaxEnt framework, the proba-
bilities are normalized in the whole sentence in CRF-
based method. CRF-based word segmenter is robust to
length-bias problem (Kudo and Matsumoto, 2004) by
the global normalization. We will discuss the length-
bias problem in section 4.
2.5 Note on MSR data
Unfortunately, we could not complete Models a and
c for the MSR data due to time constraints. There-
fore, we submitted the following 2 fragmented mod-
els: Model a for MSR data is MEMM-based word
segmenter with OOV word list by voting; Model c for
MSR data is CRF-based word segmenter with no OOV
word candidate.
3 Model b
Model b uses a different approach. First, we extract the
OOV words using a MaxEnt classifier with only the
character as the features. We did not use the character
classes as the features. Each character is assigned with
BIES position tags. Word segmentation by character-
based tagging is firstly introduced by (Xue and Con-
verse, 2002). In encoding, we extract characters within
five-character window size for each character position
in the training data as the features for the classifier.
In decoding, the BIES position tag is deterministically
annotated character by character in the test data. The
135
words that appear only in the test data are treated as
OOV word candidates.
We can obtain quite high unknown word recall with
this model but the precision is a bit low. However,
the following segmentation model will try to elimi-
nate some false unknown words. In the next step, we
append OOV word candidates into the IV word list
extracted from the training data. The segmentation
model is similar to the OOV extraction method, except
that the features include the output from the Maximum
Matching (MaxMatch) algorithm. The algorithm runs
in both forward (FMaxMatch) and backward (BMax-
Match) directions using the final word list as the ref-
erences. The outputs of FMaxMatch and BMaxMatch
are also assigned with BIES tags. The differences be-
tween the FMaxMatch and BMaxMatch outputs indi-
cate the positions where the overlapping ambiguities
occur. The final word segmentation is carried out by
MaxEnt classifier again.
Note, both procedures in Model b use whole train-
ing data in the training phase. The dictionary used in
the MaxMatch algorithm is extracted from the training
data only during the training phase. So, the training of
segmentation model does not explicitly consider OOV
words. We did not use the word and character classes
as features in Model b unlike in the case of Models a
and c. The details of the model can be found in (Goh
et al, 2004b). The difference is that we do not pro-
vide character types here because it is forbidden in
this round. Besides, we also did not prune the OOV
words because this step involve the intervention of hu-
man knowledge.
4 Discussions and Conclusions
Table 1 summarizes the results of the three models.
The proposed systems employ purely corpus-based
statistical/machine learning method. Now, we discuss
what we observe in the three models. We remark two
problems in word segmentation: OOV word problem
and length-bias problem.
OOV word problem is that simple word-based
Markov Model family cannot analyze the words not
included in the word list. One of the solutions is
character-based tagging (Xue and Converse, 2002)
(Goh et al, 2004a). The simple character-based tag-
ging (Model b) achieved high ROOV but the precision
is low. We tried to refine OOV extraction by voting
and merging (Model a and c). However, the ROOV
of Models a and c are not as good as that of Model
b. Figure 1 shows type-precision and type-recall of
each OOV extraction modules. While voting helps to
make the precision higher, voting deteriorates the re-
call. Defining some hand written rules to prune false
OOV words will help to improve the IV word segmen-
tation (Goh et al, 2004b), because the precision of
OOV word extraction becomes higher. Other types of
OOV word extraction methods should be introduced.
For example, (Uchimoto et al, 2001) embeded OOV
models in MEMM-based word segmenter (with POS
tagging). Less than six-character substrings are ex-
tracted as the OOV word candidates in the word trel-
lis. (Peng and McCallum, 2004) proposed OOV word
extraction methods based on CRF-based word seg-
menter. Their CRF-based word segmenter can com-
pute a confidence in each segment. The high confi-
dent segments that are not in the IV word list are re-
garded as OOV word candidates. (Nakagawa, 2004)
proposed integration of word and OOV word position
tag in a trellis. These three OOV extraction method are
different from our methods ? character-based tagging.
Future work will include implementation of these dif-
ferent sorts of OOV word extraction modules.
Length bias problem means the tendency that the lo-
cally normalized Markov Model family prefers longer
words. Since choosing the longer words reduces the
number of words in a sentence, the state-transitions are
reduced. The less the state-transitions, the larger the
likelihood of the whole sentence. Actually, the length-
bias reflects the real distribution in the corpus. Still,
the length-bias problem is nonnegligible to achieve
high accuracy due to small exceptional cases. We used
CRF-based word segmenter which relaxes the prob-
lem (Kudo and Matsumoto, 2004). Actually, the CRF-
based word segmenter achieved high RIV .
We could not complete Model a and c for MSR.
After the deadline, we managed to complete Model
a (CRF + Voted Unk.) and c (CRF + Merged Unk.)
The result of Model a was precesion 0.976, recall
0.966, F-measure 0.971, OOV recall 0.570 and IV re-
call 0.988. The result of Model c was precesion 0.969,
recall 0.963, F-measure 0.966, OOV recall 0.571 and
IV recall 0.974. While the results are quite good, un-
fortunately, we could not submit the outputs in time.
While our results for the three data sets (AS,
CITYU, MSR) are fairly good, the result for the PKU
data is not as good. There is no correlation between
scores and OOV word rates. We investigate unseen
character distributions in the data set. There is no cor-
relation between scores and unseen character distribu-
tions.
We expected Model c (merging) to achieve higher
recall for OOV words than Model a (voting). How-
ever, the result was opposite. The noises in OOV
word candidates should have deteriorated the F-value
of overall word segmentation. One reason might be
that our CRF-based segmenter could not encode the
occurence of OOV words. We defined the 21st word
class for OOV words. However, the training data for
CRF-based segmenter did not contain the 21st class.
We should include the 21st class in the training data
136
Table 1: Our Three Models and Results: F-value/ROOV /RIV (Rank of F-value)
AS CITYU MSR PKU
Model a CRF + Voted Unk. CRF + Voted Unk. MEMM + Voted Unk. CRF + Voted Unk.
0.947/0.606/0.971 0.942/0.629/0.967 0.949/0.378/0.971 0.934/0.521/0.955
(2/11) (2/15) (16/29) (10/23)
Model b Char.-based tagging Char.-based tagging Char.-based tagging Char.-based tagging
0.952/0.696/0.963 0.941/0.736/0.953 0.958/0.718/0.958 0.941/0.760/0.941
(1/11) (3/15) (6/29) (7/23)
Model c CRF + Merged Unk. CRF + Merged Unk. CRF + No Unk. CRF + Merged Unk.
0.939/0.445/0.967 0.928/0.598/0.940 0.943/0.025/0.990 0.917/0.325/0.940
(7/11) (8/15) (21/29) (14/23)
150/764
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1727/2504=0.689Voted Recall = 1727/3226=0.535Merged Precision = 2532/6003=0.421Merged Recall = 2532/3226=0.784
69/599 586/2136
165/480 420/579
184/304
958/1141
AS
51/406
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1068/1714=0.623Voted Recall = 1068/1670=0.639Merged Precision = 1367/3531=0.387Merged Recall = 1367/1670=0.818
42/352 206/1059
87/439 114/188
109/196
758/891
CITYU
correctly extracted types(left side)
extracted types(right side)
57/555
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1196/1659=0.720Voted Recall = 1196/1991=0.600Merged Precision = 1628/4454=0.365Merged Recall = 1628/1991=0.817
40/330 335/1910
93/293 149/243
245/333
709/790
MSR
67/882
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1528/2827=0.540Voted Recall = 1528/2863=0.533Merged Precision = 2184/7064=0.309Merged Recall = 2184/2863=0.762
87/720 502/2635
181/727 217/424
201/407
929/1269
PKU
Figure 1: OOV Extraction Precision and Recall by Type
by regarding some words as pseudo OOV words.
We also found a bug in the CRF-based OOV word
extration module. The accuracy of the module might
be slightly better than the reported results. However,
the effect of the bug on overall F-value might be lim-
ited, since the module was only part of the OOV ex-
traction module combination ? voting and merging.
Acknowledgement
We would like to express our appreciation to Dr. Taku
Kudo who developed SVM-based chunker and gave us
several fruitful comments.
References
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004a. Chinese Word Segmentation by
Classification of Characters. In Proc. of Third
SIGHAN Workshop, pages 57?64.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004b. Pruning False Unknown Words to
Improve Chinese Word Segmentation. In Proc. of
PACLIC-18, pages 139?149.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proc. of NAACL-
2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2004. Applying
Conditional Random Fields to Japanese Morpho-
logical Analysis. In Proc. of EMNLP-2004, pages
230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proc. of ICML-2001, pages 282?
289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation. In
Proc. of ICML-2000, pages 591?598.
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-
Level Information. In Proc. of COLING-2004,
pages 466?472.
Fuchun Peng and Andrew McCallum. 2004. Chinese
Segmentation and New Word Detection using Con-
ditional Random Fields. In Proc. of COLING-2004,
pages 562?568.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The Unknown Word Problem: a
Morphological Analysis of Japanese Using Maxi-
mum Entropy Aided by a Dictionary. In Proc. of
EMNLP-2001, pages 91?99.
Nianwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In
Proc. of First SIGHAN Workshop, pages 63?70.
137
Japanese-Spanish Thesaurus Construction 
 Using English as a Pivot 
Jessica  Ram?rez, Masayuki Asahara, Yuji Matsumoto 
Graduate  School of Information Science 
Nara Institute of Science and Technology  
Ikoma, Nara, 630-0192 Japan 
{jessic-r,masayu-a,matsu}@is.naist.jp 
 
 
Abstract 
We present the results of research with the 
goal of automatically creating a multilin-
gual thesaurus based on the freely available 
resources of Wikipedia and WordNet. Our 
goal is to increase resources for natural 
language processing tasks such as machine 
translation targeting the Japanese-Spanish 
language pair. Given the scarcity of re-
sources, we use existing English resources 
as a pivot for creating a trilingual Japanese-
Spanish-English thesaurus. Our approach 
consists of extracting the translation tuples 
from Wikipedia, disambiguating them by 
mapping them to WordNet word senses. 
We present results comparing two methods 
of disambiguation, the first using VSM on 
Wikipedia article texts and WordNet defi-
nitions, and the second using categorical 
information extracted from Wikipedia, We 
find that mixing the two methods produces 
favorable results. Using the proposed 
method, we have constructed a multilingual 
Spanish-Japanese-English thesaurus con-
sisting of 25,375 entries. The same method 
can be applied to any pair of languages that 
are linked to English in Wikipedia. 
1 Introduction 
 
Aligned data resources are indispensable compo-
nents of many Natural Language Processing (NLP) 
applications; however lack of annotated data is the 
main obstacle for achieving high performance NLP 
systems. Current success has been moderate. This 
is because for some languages there are few re-
sources that are usable for NLP. 
Manual construction of resources is expensive 
and time consuming. For this reason NLP re-
searchers have proposed semi-automatic or auto-
matic methods for constructing resources such as 
dictionaries, thesauri, and ontologies, in order to 
facilitate NLP tasks such as word sense disam-
biguation, machine translation and other tasks. 
Hoglan Jin and Kam-Fai Wong (2002) automati-
cally construct a Chinese dictionary from different 
Chinese corpora, and Ahmad Khurshid et al 
(2004) automatically develop a thesaurus for a 
specific domain by using text that is related to an 
image collection to aid in image retrieval. 
With the proliferation of the Internet and the 
immense amount of data available on it, a number 
of researchers have proposed using the World 
Wide Web as a large-scale corpus (Rigau et al, 
2002). However due to the amount of redundant 
and ambiguous information on the web, we must 
find methods of extracting only the information 
that is useful for a given task. 
1.1 Goals 
This research deals with the problem of developing 
a multilingual Japanese-English-Spanish thesaurus 
that will be useful to future Japanese-Spanish NLP 
research projects. 
A thesaurus generally means a list of words 
grouped by concepts; the resource that we create is 
similar because we group the words according to 
semantic relations. However, our resource is also 
473
composed of three languages ? Spanish, English, 
and Japanese. Thus we call the resource we created 
a multilingual thesaurus. 
Our long term goal is the construction of a Japa-
nese-Spanish MT system. This thesaurus will be 
used for word alignments and building comparable 
corpus. 
We construct our multilingual thesaurus by fol-
lowing these steps:   
? Extract the translation tuples from Wikipedia  
article titles 
? Align the word senses of these tuples with 
those of English WordNet (disambigua-
tion) 
? Construct a parallel thesaurus of Spanish-
English-Japanese from these tuples 
1.2 Method summary 
We extract the translation tuples using Wikipedia?s 
hyperlinks to articles in different languages and 
align these tuples to WordNet by measuring cosine 
vector similarity measures between Wikipedia arti-
cle texts and WordNet glosses. We also use heuris-
tics comparing the Wikipedia categories of a word 
with its hypernyms in WordNet. 
  A fundamental step in the construction of a the-
saurus is part of speech (POS) identification of 
words and word sense disambiguation (WSD) of 
polysemous entries. 
For POS identification, we cannot use Wikipedia, 
because it does not contain POS information. So 
we use another well-structured resource, WordNet, 
to provide us with the correct POS for a word.  
These two resources, Wikipedia and WordNet, 
contain polysemous entries. We also introduce 
WSD method to align these entries.  
We focus on the multilingual application of 
Wikipedia to help transfer information across lan-
guages. This paper is restricted mainly to nouns, 
noun phrases, and to a lesser degree, named enti-
ties, because we only use Wikipedia article titles. 
2 Resources 
2.1 Wikipedia 
Wikipedia is an online multilingual encyclopedia 
with articles on a wide range of topics, in which 
the texts are aligned across different languages.  
Wikipedia has some features that make it suitable 
for research such as: 
Each article has a title, with a unique ID. ?Redi-
rect pages? handle synonyms, and ?disambiguation 
pages? are used when a word has several senses. 
?Category pages? contain a list of words that share 
the same semantic category. For example the cate-
gory page for ?Birds? contains links to articles like 
?parrot?, ?penguin?, etc. Categories are assigned 
manually by users and therefore not all pages have 
a category label. 
Some articles belong to multiple categories. For 
example, the article ?Dominican Republic? be-
longs to three categories: ?Dominican Republic?, 
?Island countries? and ?Spanish-speaking coun-
tries?. Thus, the article Dominican Republic ap-
pears in three different category pages. 
The information in redirect pages, disambigua-
tion pages and Category pages combines to form a 
kind of Wikipedia taxonomy, where entries are 
identified by semantic category and word sense. 
2.2 WordNet 
 
WordNet (C. Fellbaum, 1998) ?is considered to be 
one of the most important resources in computa-
tional linguistics and is a lexical database, in 
which concepts have been grouped into sets of 
synonyms (words with different spellings, but the 
same meaning), called synsets, recording different 
semantic relations between words?. 
WordNet can be considered to be a kind of ma-
chine-readable dictionary. The main difference 
between WordNet and conventional dictionaries is 
that WordNet groups the concepts into synsets, and 
each concept has a small definition sentence call a 
?gloss? with one or more sample sentences for 
each synset. 
When we look for a word in WordNet it presents 
a finite number of synsets, each one representing a 
concept or idea.  
The entries in WordNet have been classified ac-
cording to the syntactic category such as: nouns, 
verbs, adjectives and adverbs, etc. These syntactic 
categories are known as part of speech (POS). 
3 Related Work 
Compared to well-established resources such as 
WordNet, there are currently comparatively fewer 
researchers using Wikipedia as a data resource in 
474
NLP. There are, however, works showing promis-
ing results. 
The work most closely related to this paper is 
(M. Ruiz et al, 2005), which attempts to create an 
ontology by associating the English Wikipedia 
links with English WordNet. They use the ?Sim-
ple English Wikipedia? and WordNet version 1.7 
to measure similarity between concepts. They 
compared the WordNet glosses and Wikipedia by 
using the Vector Space Model, and presented re-
sults using the cosine similarity. 
 Our approach differs in that we disambiguate 
the Wikipedia category tree using WordNet hyper-
/hyponym tree. We compare our approach to M. 
Ruiz et al, (2005) using it as the baseline in sec-
tion 7. 
Oi Yee Kwong (1998) integrates different re-
sources to construct a thesaurus by using WordNet 
as a pivot to fill gaps between thesaurus and a dic-
tionary.  
Strube and Ponzetto (2006) present some ex-
periments using Wikipedia for the computing se-
mantic relatedness of words (a measure of degree 
to which two concepts are related in a taxonomy 
measured using all semantic relations), and com-
pare the results with WordNet. They also integrate 
Google hits, in addition to Wikipedia and WordNet 
based measures. 
4 General Description 
First we extract from Wikipedia all the aligned 
links i.e. Wikipedia article titles.  We map these on 
to WordNet to determine if a word has more than 
one sense (polysemous) and extract the ambiguous 
articles. We use two methods to disambiguate by 
assigning the WordNet sense to the polysemous 
words, we use two methods: 
 Measure the cosine similarity between each 
Wikipedia article?s content and the WordNet 
glosses.  
 Compare the Wikipedia category to which the 
article belongs with the corresponding word in 
WordNet?s ontology  
Finally, we substitute the target word into Japa-
nese and Spanish. 
 
5 Extracting links from Wikipedia 
 
The goal is the acquisition of Japanese-Spanish-
English tuples of Wikipedia?s article titles. Each 
Wikipedia article provides links to corresponding 
articles in different languages. 
Every article page in Wikipedia has on the left 
hand side some boxes labeled: ?navigation?, 
?search?, ?toolbox? and finally ?in other languages?. 
This has a list of all the languages available for that 
article, although the articles in each language do 
not all have exactly the same contents. In most 
cases English articles are longer or have more in-
formation than their counterparts in other lan-
guages, because the majority of Wikipedia collabo-
rators are native English speakers. 
 
Pre-processing procedure: 
 
Before starting with the above phases, we first 
eliminate the irrelevant information from Wikipe-
dia articles, to make processing easy and faster.The 
steps applied are as follows: 
1. Extract the Wikipedia web articles 
2. Remove from the pages all irrelevant informa-
tion, such as images, menus, and special 
markup such as: ?()?, ?&quot;?, ?*?, etc... 
3. Verify if a link is a redirected article and ex-
tract the original article 
4. Remove all stopwords and function words that 
do not give information about a specific topic 
such as ?the?, ?between?, ?on?, etc. 
 
Methodology 
 
 
 
Figure 1.  The article ?bird? in English, Spanish and 
Japanese 
 
Take all articles titles that are nouns or named enti-
ties and look in the articles? contents for the box 
475
called ?In other languages?. Verify that it has at 
least one link. If the box exists, it links to the same 
article in other languages. Extract the titles in these 
other languages and align them with the original 
article title. 
For instance, Figure 1. shows the English article 
titled ?bird? translated into Spanish as ?ave?, and 
into Japanese as ?chourui? (??). When we click 
Spanish or Japanese ?in other languages? box, we 
obtain an article about the same topic in the other 
language. This gives us the translation as its title, 
and we proceed to extract it. 
 
6 Aligning Wikipedia entries to WordNet 
senses 
 
The goal of aligning English Wikipedia entries to 
WordNet 2.1 senses is to disambiguate the 
polysemous words in Wikipedia by means of com-
parison with each sense of a given word existing in 
WordNet. 
A gloss in WordNet contains both an association 
of POS and word sense. For example, the entry 
?bark#n#1? is different than ?bark#v#1? because 
their POSes are different. In this example, ?n? de-
notes noun and ?v? denotes verb. So when we 
align a Wikipedia article to a WordNet gloss, we 
obtain both POS and word sense information. 
 
Methodology 
 
We assign WordNet senses to Wikipedia?s polyse-
mous articles. Firstly, after extracting all links and 
their corresponding translations in Spanish and 
Japanese, we look up the English words in Word-
Net and count the number of senses that each word 
has. If the word has more than one sense, the word 
is polysemous. 
We use two methods to disambiguate the am-
biguous articles, the first uses cosine similarity and 
the second uses Wikipedia?s category tree and 
WordNet?s ontology tree. 
6.1 Disambiguation using Vector Space Model 
We use a Vector Space Model (VSM) on Wikipe-
dia and WordNet to disambiguate the POS and-
word sense of Wikipedia article titles. This gives 
us a correspondence to a WordNet gloss. 
 
 
Where V1 represents the Wikipedia article?s word 
vector and V2  represents the WordNet gloss? word 
vector. 
In order to transfer the POS and word sense in-
formation, we have to measure similarity metric 
between a Wikipedia article and a WordNet gloss.  
 
Background 
 
VSM is an algebraic model, in which we convert a 
Wikipedia article into a vector and compares it to a 
WordNet gloss (that has also been converted into a 
vector) using the cosine similarity measure. It takes 
the set of words in some Wikipedia article and 
compares them with the set of words of WordNet 
gloss. Wikipedia articles which have more words 
in common are considered similar documents. 
  In Figure 2 shows the vector of the word ?bank?, 
we want to compare the similitude between the 
Wikipedia article ?bank-1? with the English 
WordNet ?bank-1? and ?bank-2?.  
 
 
   
 
Figure 2. Vector Space Model with the word ?bank? 
 
VSM Algorithm: 
 
1. Encode the Wikipedia article as a vector, 
where each dimension represents a word 
in the text of the article 
2. Encode the WordNet gloss of each sense 
as a vector in the same manner 
bank -1(a) 
bank -1(b) 
bank -2(a) 
bank -2(b) 
(a) Wikipedia 
(b) WordNet 
?
||.||cos 21
21
VV
VV ?
=?
476
3. Compute the similarity between the 
Wikipedia vector and WordNet senses? 
vectors for a given word using the cosine 
measure 
4. Link the Wikipedia article to the Word-
Net gloss with the highest similarity 
 
6.2   Disambiguation by mapping the WordNet 
ontological tree to Wikipedia categories 
 
This method consists of mapping the Wikipedia 
Category tree to the WordNet ontological tree, by 
comparing hypernyms and hyponyms. The main 
assumption is that there should be overlap between 
the hypernyms and hyponyms of Wikipedia arti-
cles and their correct WordNet senses. We will 
refer to this method as MCAT (?Map CATego-
ries?) throughout the rest of this paper. 
Wikipedia has in the bottom of each page a box 
containing the category or categories to which the 
page belongs, as we can see in Figure 3. Each cate-
gory links to the corresponding category page to 
which the title is affiliated. This means that the 
?category page? contains a list of all articles that 
share a common category. 
 
 
 
Figure 3. Relation between WordNet ontological tree 
and Wikipedia categories  
 
Methodology 
 
1. We extract ambiguous Wikipedia article 
titles (links) and the corresponding cate-
gory pages 
2. Extract the category pages, containing all 
pages which belong to that category, its 
subcategories, and other category pages 
that have a branch in the tree and categories 
to which it belongs. 
3. If the page has a category: 
3.1 Construct an n-dimensional vector con-
taining the links and their categories 
3.2 Construct an n-dimensional vector of the 
category pages, where every dimension 
represents a link which belongs to that 
category 
4. For each category that an article belongs 
to: 
4.1 
 Map the categoryto the WordNet hy-
pernym-/hyponym tree by looking in 
each place that the given word ap-
pears and verify if any of its branches 
exist in the category page vector. 
4.2 
 If a relation cannot be found then con-
tinue with other categories 
4.3 
 If there is no correspondence at all 
then take the category pages vector 
and look to see if any of the links has 
relation with the WordNet tree 
5. If there is at least one correspondence then 
assign this sense 
    
 
6.3 Constructing the multilingual thesaurus 
 
After we have obtained the English words with its 
corresponding English WordNet sense aligned in 
the three languages, we construct a thesaurus from 
these alignments. 
The thesaurus contains a unique ID for every tu-
ple of word and POS that it will have information 
about the syntactic category. 
 It also contains the sense of the word (obtain in 
the disambiguation process) and finally a small 
definition, which have the meaning of the word in 
the three languages. 
 We assign a unique ID to every tuple of words 
 For Spanish and Japanese we assign for de-
fault sense 1 to the first occurrence of the 
word if there exists more than 1 occurrence 
we continue incrementing 
 Extract a small definition from the corre-
sponding Wikipedia articles 
animal 
  bird 
life form 
WordNet tree                Wikipedia article 
Categories: Birds  
477
The definition of title word in Wikipedia tends to 
be in the first sentence of the article.  
Wikipedia articles often include sentences defin-
ing the meaning of the article?s title. We mine 
Wikipedia for these sentences include them in our 
thesaurus. There is a large body of research dedi-
cate to identifying definition sentences (Wilks et 
al., 1997), However, we currently rely on very 
simple patterns to this (e.g. ?X is a/are Y?,  ?X es 
un/a Y?, ?X ?/? Y ?????). Incorporating 
more sophisticated methods remains an area of 
future work.  
7 Experiments 
7.1 Extracting links from Wikipedia 
 
We use the articles titles from Wikipedia which are 
mostly nouns (including named entities) in Spanish, 
English and Japanese; (es.wikipedia.org, 
en.wikipedia.org, and ja.wikipedia.org), specifi-
cally ?the latest all titles? and ?the latest pages ar-
ticles? files retrieved in April of 2006, and English 
WordNet version 2.1. 
Our Wikipedia data contains a total of 377,621 
articles in Japanese; 2,749,310 in English; and 
194,708 in Spanish. We got a total of 25,379 words 
aligned in the three languages. 
 
7.2 Aligning Wikipedia  entries to WordNet 
senses 
In WordNet there are 117,097 words and 141,274 
senses. In Wikipedia (English) there are 2,749,310 
article titles. 78,247 word types exist in WordNet. 
There are 14,614 polysemous word types that will 
align with one of the 141,274 senses in WordNet. 
We conduct our experiments using 12,906 am-
biguous articles from Wikipedia. 
Table 1 shows the results obtained for WSD. 
The first column is the baseline (M. Ruiz et al, 
2005) using the whole article; the second column is 
the baseline using only the first part of the article.  
   The third column (MCAT) shows the results of 
the second disambiguation method (disambigua-
tion by mapping the WordNet ontological tree to 
Wikipedia categories). Finally the last column 
shows the results of combined method of taking 
the MCAT results when available and falling back 
to MCAT otherwise. The first row shows the sense 
assignments, the second row shows the incorrect 
sense assignment, and the last row shows the num-
ber of word used for testing. 
 
7.2.1 Disambiguation using VSM 
 
In the experiment using VSM, we used human 
evaluation over a sample of 507 words to verify if 
a given Wikipedia article corresponds to a given 
WordNet gloss. We took a the stratified sample of 
our data selecting the first 5 out of every 66 entries 
as ordered alphabetically for a total of 507 entries.  
   We evaluate the effectiveness of using whole 
articles in Wikipedia versus only a part (the first 
part up to the first subtitle), we found that the best 
score was obtained when using the whole articles 
81.5% (410 words) of them are correctly assigned 
and 18.5% (97 words) incorrect.  
 
Discussion 
 
In this experiment because we used VSM the result 
was strongly affected by the length of the glosses 
in WordNet, especially in the case of related defi-
nitions because the longer the gloss the greater the 
probability of it having more words in common.  
An example of related definitions in English 
WordNet is the word ?apple?. It has two senses as 
follows: 
 apple#n#1: fruit with red or yellow or green 
skin and sweet to tart crisp whitish flesh. 
 apple#n#2: native Eurasian tree widely culti-
vated in many varieties for its firm rounded 
edible fruits. 
The Wikipedia article ?apple? refers to both 
senses, and so selection of either WordNet sense is 
correct. It is very difficult for the algorithm to dis-
tinguish between them. 
 
7.2.2 Disambiguation by mapping the WordNet 
ontological tree to Wikipedia categories 
 
Our 12,906 articles taken from Wikipedia belong 
to a total of 18,810 associated categories. Thus, 
clearly some articles have more than one category; 
however some articles also do not have any cate-
gory. 
In WordNet there are 107,943 hypernym relations. 
 
 
478
 Baseline Our methods  
VSM VSM (using first 
part of the article) 
MCAT VSM+ MCAT 
Correct sense identification 410  
(81.5%) 
403  
(79.48%) 
380 
(95%) 
426 
 (84.02%) 
Incorrect sense identification 97  
(18.5%) 
104 
 (20.52%) 
20  
(5%) 
81  
(15.98%) 
Total ambiguous words 507 
 (100%) 
400 
(100%) 
507  
(100%) 
 
Table 1. Results of disambiguation 
 
 
Results: 
 
We successfully aligned 2,239 Wikipedia article 
titles with a WordNet sense. 400 of the 507 arti-
cles in our test data have Wikipedia category 
pages allowing us apply MCAT. Our human 
evaluation found that 95% (380 words) were cor-
rectly disambiguated. This outperformed disam-
biguation using VSM, demonstrating the utility of 
the taxonomic information in Wikipedia and 
WordNet. However, because not all words in 
Wikipedia have categories, and there are very few 
named entities in WordNet, the number of disam-
biguated words that can be obtained with MCAT 
(2,239) is less than when using VSM, (12,906). 
Using only MCAT reduces the size of the Japa-
nese-Spanish thesaurus. We had the intuition that 
by combining both disambiguation methods we 
can achieve a better balance between coverage 
and accuracy. VSM+MCAT use the MCAT WSD 
results when available falling back to VSM results 
otherwise. 
 We got an accuracy of 84.02% (426 of 507 to-
tal words) with VSM+MCAT, outperforming the 
baselines. 
 
Evaluating the coverage over Comparable cor-
pus 
 
 Corpus construction 
 
We construct comparable corpus by extracting 
from Wikipedia articles content information as 
follows: 
Choose the articles whose content belongs to the 
thesaurus. We only took the first part of the article 
until a subtitle and split into sentences. 
 
 Evaluation of  coverage 
 
We evaluate the coverage of the thesaurus over an 
automated comparable corpus automatically ex-
tracted from Wikipedia. The comparable corpus 
consists of a total of 6,165 sentences collected 
from 12,900 articles of Wikipedia. 
 We obtained 34,525 types of words; we map 
them with 15,764 from the Japanese-English-
Spanish thesaurus. We found 10,798 types of 
words that have a coincidence that it is equivalent 
to 31.27%. 
We found this result acceptable for find informa-
tion inside Wikipedia. 
 
 
8 Conclusion and future work 
 
This paper focused on the creation of a Japanese-
Spanish-English thesaurus and ontological rela-
tions. We demonstrated the feasibility of using 
Wikipedia?s features for aligning several lan-
guages.We present the results of three sub-tasks: 
The first sub-task used pattern matching to 
align the links between Spanish, Japanese, and 
English articles? titles. 
The second sub-task used two methods to dis-
ambiguate the English article titles by assigning 
the WordNet senses to each English word; the 
first method compares the disambiguation using 
cosine similarity. The second method uses 
Wikipedia categories. We established that using 
Wikipedia categories and the WordNet ontology 
gives promising results, however the number of 
words that can be disambiguated with this method 
479
is small compared to the VSM method. However, 
we showed that combining the two methods 
achieved a favorable balance of coverage and ac-
curacy. 
Finally, the third sub-task involved translating 
English thesaurus entries into Spanish and Japa-
nese to construct a multilingual aligned thesaurus. 
So far most of research on Wikipedia focuses 
on using only a single language. The main contri-
bution of this paper is that by using a huge multi-
lingual data resource (in our case Wikipedia) 
combined with a structured monolingual resource 
such as WordNet, we have shown that it is possi-
ble to extend a monolingual resource to other lan-
guages. Our results show that the method is quite 
consistent and effective for this task. 
The same experiment can be repeated using 
Wikipedia and WordNet on languages others than 
Japanese and Spanish offering useful results espe-
cially for minority languages. 
In addition, the use of Wikipedia and WordNet 
in combination achieves better results than those 
that could be achieved using either resource inde-
pendently. 
We plan to extent the coverage of the thesaurus 
to other syntactic categories such as verbs, adverb, 
and adjectives. We also evaluate our thesaurus in 
real world tasks such as the construction of com-
parable corpora for use in MT.  
Acknowledgments 
We would like to thanks to Eric Nichols for his 
helpful comments.  
References 
K. Ahmad, M. Tariq, B. Vrusias and C. Handy. 2003. 
Corpus-Based Thesaurus Construction for Image 
Retrieval in Specialist Domains. In Proceedings of 
ECIR 2003. pp. 502-510. 
R. Bunescu and M. Pa?ca. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In Pro-
ceedings of EACL-06, pp. 9-16. 
 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, Mass.: MIT press. pp. 25-43. 
 
J. Honglan and Kam-Fai-Won. 2002. A Chinese dic-
tionary construction algorithm for information re-
trieval. ACM Transactions on Asian Language In-
formation Processing. pp. 281-296. 
 
C. Manning and H. Sch?tze. 2000. Foundations of 
Statistical Natural Language Processing. Cam-
bridge, Mass.: MIT press. pp. 230-259. 
 
K. Oi Yee, 1998. Bridging the Gap between Dictionary 
and Thesaurus. COLING-ACL.  pp. 1487-1489. 
 
R. Rada, H. Mili, E. Bicknell and M. Blettner. 1989. 
Development and application of a metric semantic 
nets. IEEE Transactions on Systems, Man and Cy-
bernetics, 19(1):17-30. 
 
M. Ruiz, E. Alfonseca and P. Castells. 2005. Auto-
matic assignment of Wikipedia encyclopedic entries 
to WordNet synsets. In Proceedings of AWIC-05. 
Lecture Notes in Computer Science 3528. pp. 380-
386, Springer, 2005. 
 
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia. 21st  
National Conference on Artificial Intelligence. 
 
L. Urdang. 1991. The Oxford Thesaurus. Clarendon 
press. Oxford. 
 
480
Use of Event Types for Temporal Relation Identification in Chinese 
Text 
Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto 
Graduate School of Information Science 
Nara Institute of Science and Technology 
8916-5 Takayama, Ikoma, Nara 630-0192, Japan 
{yuchan-c,masayu-a,matsu}@is.naist.jp 
Abstract 
This paper investigates a machine learning 
approach for identification of temporal re-
lation between events in Chinese text. We 
proposed a temporal relation annotation 
guideline (Cheng, 2007) and constructed 
temporal information annotated corpora. 
However, our previous criteria did not deal 
with various uses of Chinese verbs. For 
supplementing the previous version of our 
criteria, we introduce attributes of verbs 
that describe event types. We illustrate the 
attributes by the different examples of verb 
usages. We perform an experiment to 
evaluate the effect of our event type attrib-
utes in the temporal relation identification. 
As far as we know, this is the first work of 
temporal relation identification between 
verbs in Chinese texts. The result shows 
that the use of the attributes of verbs can 
improve the annotation accuracy. 
1 Introduction 
Extracting temporal information in documents is a 
useful technique for many NLP applications such 
as question answering, text summarization, ma-
chine translation, and so on. The temporal informa-
tion is coded in three types of expressions: 1. tem-
poral expressions, which describe time or period in 
the actual or hypothetical world; 2. event or situa-
tion expressions that occur at a time point or that 
last for a period of time; 3. temporal relations, 
which describe the ordering relation between an 
event expression and a temporal expression, or be-
tween two event expressions.  
There are many researches dealing with the 
temporal expressions and event expressions. Ex-
tracting temporal expressions is a subtask of 
Named Entity Recognition (IREX committee, 1999) 
and is widely studied in many languages. Normal-
izing temporal expressions is investigated in 
evaluation workshops (Chinchor, 1997). Event se-
mantics is investigated in linguistics and AI fields 
(Bach, 1986). However, researches at temporal 
relation extraction are still limited. Temporal rela-
tion extraction includes the following issues: iden-
tifying events, anchoring events on the timeline, 
ordering events, and reasoning with contextually 
underspecified temporal expressions. To extract 
temporal relations, several knowledge resources 
are necessary, such as tense and aspect of verbs, 
temporal adverbs, and world knowledge (Mani, et 
al., 2006).  
In English, TimeBank (Pustejovsky, et al, 2006), 
a temporal information annotated corpus, is avail-
able to machine learning approaches for automati-
cally extracting temporal relation. In Chinese, Li 
(2004) proposed a machine learning based method 
for temporal relation identification, but they con-
sidered the relation between adjacent verbs in a 
small scale corpus. There is no publicly available 
Chinese resource for temporal information proc-
essing. We proposed (Cheng, 2007) a dependency 
structure based method to annotate temporal rela-
tions manually on a limited set of event pairs and 
extend the relations using inference rules. In our 
previous research, the dependency structure helps 
to detect subordinate and coordinate structures in 
sentences. Our proposed criteria can reduce the 
manual effort for annotating the temporal relation 
tagged corpus. 
Our research focuses on the relations between 
events where they are assumed to be described by 
verbs. Verbs in an article can represent events in 
actual world (which describe actual situations or 
actions) and events in hypothetical world (which 
describe possible situations, imagination or back-
ground knowledge). However, our previous re-
search does not define the class of event types. Our 
31
Sixth SIGHAN Workshop on Chinese Language Processing
previous annotation guideline requires annotators 
to decide the attributes of temporal relations of a 
verb by annotators? own judgment but does not 
describe the difference between events (verbs) in 
actual and hypothetical world. 
In this paper, we attempt to give the definition 
of actual / hypothetical world events (verbs). We 
collect usages of verbs in Penn Chinese treebank 
and classify them to actual / hypothetical worlds. 
We add another attribute to our previous criteria. 
Then we train the temporal relation annotated cor-
pus to investigate the effect of using the event 
types for automatic annotation.  
In the next section, we describe the criteria of 
temporal relations between events that are pro-
posed in our previous research (Cheng, 2007). In 
section 3, we discuss the event types of verbs and 
define the actual / hypothetical world events. In 
section 4, we perform an experiment of a machine 
learning based temporal relation identifier with and 
without the event type information. Finally, we 
discuss the results of experiments and our future 
direction. 
2 Temporal relations between events 
We propose an annotation guideline for developing 
a Chinese temporal relation annotated corpus. The 
guideline is based on TimeML (Saur?, 2005) and 
focuses on the temporal relations between events. 
To reduce manual effort, we introduce several con-
straints on the original TimeML. First, we restrict 
the definition of events to verbs. Second, we focus 
on three types of event pairs according to syntactic 
dependency structure. 
2.1 The definition of the events 
According to the TimeML guideline for English, 
verbs, nominalized verbs, adjectives, predicative 
and prepositional phrases can represent events. 
However, to recognize an instance of nominalized 
verb represents whether an event or not is difficult 
in Chinese articles. Chunking phrases and clauses 
is another difficult process in Chinese. To simplify 
the process of recognizing events, the criteria only 
regard verbs as events.  
2.2 Three types of event pairs 
The criteria of temporal relation between events 
include three types of event pairs in the complete 
graph as follows:  
 RLP (Relation to Linear Preceding event): 
Relation between the focus event and the ad-
jacent event at the immediately proceeding 
position. (Relation of adjacent event pair). 
 RTA (Relation to Tree Ancestor event): 
Relation between the focus event and the 
ancestor event in a dependency structure 
(Relation of Head-modifier event pair). 
 RTP (Relation to Tree Preceding event): 
Relation between the focus event and its sib-
ling event in a dependency structure (Rela-
tion of Sibling event pair). 
The first type stands for the adjacent event pairs. 
The second and third types are the head-modifier 
event pairs and the sibling event pairs in depend-
ency tree representation of a sentence. Figure 1 
describes the relation of three types of event pairs 
in an article. There are two sentences with twelve 
events (from e1 to e12) in the figure and the poly-
gons with dashed-lines show the boundary of sen-
tences. The angle-line links show adjacent event 
pairs (from Ll-1 to Ll-11). The dotted-line links 
show head-modifier event pairs (from Hl-1 to Hl-
10) and the curve links show sibling event pairs 
(from Sl-1 to Sl-6). The first type (adjacent event 
Figure 1: The example of annotating the temporal relations between events. 
e5
e2 e4 e6
e1 e3 e7
e11
e9 e10 e12
e8
Legend:
Sl-1
Sl-3
Sl-4
Sl-6
Hl-1 Hl-2
Hl-3 Hl-4 Hl-5
Hl-6 Hl-7
Hl-8 Hl-9 Hl-10
Ll-1 Ll-2 Ll-3
Ll-4 Ll-5
Ll-6 Ll-7 Ll-8
Ll-9 Ll-10 Ll-11
Adjacent event pair: Ll-X
Head-modifier event pair: Hl-X
Sibling event pair: Sl-X
Sentence 1 Sentence 2
Sl-5
Sl-2
32
Sixth SIGHAN Workshop on Chinese Language Processing
pairs) and the other two types (head-modifier or 
sibling event pairs) are not exclusive. An event 
pair can be a head-modifier event pairs and can be 
a head-modifier event at the same time. 
The adjacent event pair links and the sibling 
event pair links can be used to connect the tempo-
ral relations between sentences. The links Sl-4 and 
Ll-7 span two sentences in the example.  
Subordinate event pairs are head-modifier rela-
tions and coordinate event pairs are sibling rela-
tions. Using dependency structure can help to ex-
tract subordinate relations and coordinate relations 
in a sentence. 
2.3 Deficiency of our previous criteria 
Our criteria can reduce manual effort of temporal 
relation annotation. However, our previous guide-
line does not distinguish actual world and hypo-
thetical world events. Because all verbs in the pre-
vious guideline are regarded as events, verbs of 
hypothetical world events are also included in the 
events. For example: (the italicized words in our 
examples indicate verbs) 
 (a) ???/??/?/??/??/?? (after 
the industrial estate was established, it at-
tracted a great deal of foreign capital) 
 (b) ???/??/?/??/??/??/??
(after the industrial estate is established, it 
can attract a great deal of foreign capital) 
The difference between examples (a) and (b) is 
only with or without the word ??? (can)?, which 
governs a verb phrase and explains a possible 
situation. It should be noted that verbs in Chinese 
do not have morphological change. The complete 
meaning of verbs in the examples should consider 
the global context in the article. The example (a) 
explains an actual world event that the industrial 
estate attracted a great deal of foreign capital. 
However, in example (b), the word ??? (can)? 
changes  the phrase ???/??/?? (to attract a 
great deal of foreign capital)? into a hypothetical 
world event. This clause presents a possibility and 
does not indicate an event in the actual world.  
Considering the temporal relation between the 
verbs ???(establish)? and ???(attract)?,  the 
temporal relation in the example (a) means that  
the event ??(establish) occurs before the event 
??(attract). On the other hand, in the example 
(b), the verb ???(attract)? indicates a possibility. 
We cannot make sure if it could really happen. We 
regard that the temporal relation in the example (b) 
is unidentifiable. In the previous guideline, we re-
quest annotators to decide the temporal relation 
between them. However we do not classify the dif-
ference between actual and hypothetical worlds. 
The annotators annotate even some incomprehen-
sible temporal relations (such as the relation in ex-
ample (b)) with the tag ?unknown?. We clarify the 
issue by introducing event types to verbs.  
Aside form the problem of actual and hypotheti-
cal world events, verbs in our temporal relation 
annotated corpus still include some incomprehen-
sible events (We consider these in the next section). 
For solving these problems, we investigated differ-
ent types of events (verbs) in the Penn Chinese 
Treebank (Palmer, 2005) then give a clear classifi-
cation of event types. We use this classification of 
events to annotate events in the temporal relation 
tagged corpus. 
3 Event types of verbs 
Our criteria restrict events to verbs according to the 
POS-tag of Penn Chinese Treebank. Therefore, all 
the words tagged with the POS-tags (Xia, 2000), 
?VA?, ?VE?, ?VC?, and ?VV? are the ?event can-
didates?. However, these POS-tags include not 
only actual world events but also hypothetical 
world events, modifiers of nouns, and sub-
segments of named entities. We will exemplify 
these situations in this section. 
3.1 Verbs of actual world events 
The ?event? that we want to annotate is an action 
or situation that has happened or will definitely 
happen in the actual world. We define these events 
as actual world events. For example: 
 (c) ??/??/?? (A fire occurred in the 
market.) 
 (d) ???/??/??/??/?? (The 
construction work of the city hall will finish 
at the end of the year.) 
 (e) ??/??/??/?? (The function of 
financial market is smooth.)
The verbs in these examples represent actual 
world events. We want to distinguish between 
these events and hypothetical world events. 
The example (c) is a general instance of an ac-
tual world event. The verb ??? (happen)? in the 
33
Sixth SIGHAN Workshop on Chinese Language Processing
sentence indicates an occurrence of an event. The 
verb ??? (finish)? in example (d) is a confirma-
tive result that definitely happens. The word ???
(will)? indicates that the sentence describes a fu-
ture statement. If there is no other statement that 
describes an accident event in the context, we can 
trust the event in the example (d) is an actual world 
event. 
In Chinese, an adjective can be a predicate with-
out a copula (corresponding to the verb ?be?). The 
example (e) contains no copula. Still, the adjective 
??? (smooth)? is a predicate and represents an 
actual world situation. This kind of adjective is the 
POS-tag ?VA? in Penn Chinese Treebank and also 
can represent an actual world event.  
3.2 Verbs of hypothetical world events 
Sometime verbs indicate hypothetical world events. 
In such situations, verbs describe a possibility, a 
statement of ability, anticipation, a request or an 
inconclusive future. For example: 
 (b) ???/??/?/??/??/??/??
(after the industrial estate is established, it 
can attract a great deal of foreign capital) 
 (g) ???/?/??/???? (A big oil 
tanker can berth at the new port) 
 (h) ?? /?? /?? /?? /?? /??
(They wish the government to legislate 
against affiliated bill) 
 (i) ??/??/??/??/?? (The gov-
ernment requires the factory to amend their 
equipments) 
 (j) ?/??/???/??/??/?? (this 
technology can help to develop a new kind 
of medicine) 
The verb ??? (attract)? in example (b) ex-
plains a possibility that ?may? occur after a con-
firmative result ??? (establish)? in future. We 
cannot decide the temporal relation between the 
actual world event ??? (establish)? and the pos-
sible event ??? (attract)? in the example (b), be-
cause we do not know if the event ??? (attract)? 
will realize. 
The verb ??? (berth)? in example (g) explains 
the capacity of the new port. The verb ???
(berth)? does not indicate truth or a confirmative 
result. We cannot confirm when an oil tanker will 
berth at the new port. This verb represents a hypo-
thetical world event. The verb ??? (legislate)? in 
the example (h) and the verb ???(amend)? in the 
example (i) explain a wish and a request. Even the 
sentences describe that the government (in the ex-
ample (h)) or the factory (in the example (i)) was 
required to do something; the descriptions do not 
show any evidence that the request will be exe-
cuted. Although the wish and request will be real-
ized in future, we cannot identify the time point of 
the realization of these events. Therefore we 
should consider that these verbs represent hypo-
thetical world events. 
The verb ??? (develop)? in the example (j) 
explains an inconclusive plan in future. The devel-
oped technology can be used for a new develop-
ment plan. However, we also cannot make sure if 
the development plan will be realized or not. We 
cannot identify the verb ??? (develop)? on a 
timeline. Since the verb represents a hypothetical 
world event. 
These examples (from the examples (b), (g) to 
(j)) indicate hypothetical world events. However, 
as we introduced in section 2.3 (the examples (a) 
and (b)), the instances with different types of 
events have the same context in local structure (the 
phrase ???/??/?? (to attract a great deal of 
foreign capital)?). The difference between the ex-
ample (a) and the example (b) is that the word ??
? (can)? exists or not. To distinguish an actual 
world event and a hypothetical world event with 
similar local context, the dependency structure 
analysis is quite helpful. 
3.3 Copula verbs  
There are two special POS-tags of verbs in Penn 
Chinese Treebank, VC and VE. These verbs are 
copulas in Chinese. The copula verb (such as the 
verb ?? (be)?) indicates existence and corre-
sponds to ?be? in English. In TimeML, these copu-
las are not considered as an independent verb. It is 
included in another verb phrase or in a nominal 
phrase that represents an event. However, the cop-
ula verb ?? (be)? is an independent verb in Penn 
Chinese Treebank. We should investigate how to 
deal with this copula verb. For example: 
 (k) ?/??/?/???/??/? (The older 
version of bill was legislated at three years 
ago.) 
34
Sixth SIGHAN Workshop on Chinese Language Processing
 (l) ?/??/?/???/??/?/????
(The company is the largest electric power 
company in the world.) 
Considering the use of copula in Penn Chinese 
Treebank, sentences that include copula verbs can 
be distinguished to two types. The copula verbs 
describe existence. The existence could be a verb 
phrase (the example (k)) or a nominal phrase (the 
example (l)). In the example (k), the verb phrase 
????/?? (was legislated at three years ago)? 
represents an event that the copula verb accentu-
ates the existence of  the verb phrase. Although 
there are two verbs in the example (k), the sentence 
only includes an event which is the verb phrase 
????/?? (was legislated at three years ago)?.  
According to the dependency structure of sen-
tence, copula verbs represent the root of the de-
pendency structure and the head of a verb phrase 
that modifies a copula verb. We define a pair of a 
copula and a verb that modifies the copula as a 
?copula phrase?. Therefore we regard the copula 
verb in the example (k) as the main verb of the 
verb phrase ??? (legislate)? and it represents an 
actual world event1.
The copula verb ?? (be)? in the example (l) ac-
centuates the truth of the nominal phrase ????/
??/?/???? (the largest electric power com-
pany in the world)?. According to the discussion in 
the previous paragraph, the meaning of this copula 
comes from the nominal phrase. We can recognize 
the nominal phrase as a truth at the time point 
?NOW? (the company is largest in the world now). 
However, this phrase does not indicate any specific 
period of time that the fact holds. We can regard it 
as the background knowledge and it does not in-
clude an event. To identify the temporal relation 
between this noun phrase and other actual world 
event is impossible2. We also regard this copula 
verb as a hypothetical world event. 
3.4 Non-event verbs 
 
1 Whether the copula verbs are actual world events or hypo-
thetical world events depend on the modifier verb phrases. 
2 We cannot know when the company became the largest one 
on the world. And other events in the context distribute in a 
shorter period on a timeline. Therefore to compare the exis-
tence period of the truth and other events is impossible. How-
ever, if a temporal expression with a passed time period in the 
context, the truth could have a boundary of occurrence time. 
Then the copula can be recognized as an actual world event. 
There are several types of words that have a verbal 
POS-tag but do not represent events. These words 
include non-event predicative adjectives and 
named entities. 
In Chinese, adjectives can be predicates of a 
sentence without verbs. This kind of adjectives are 
predicative adjective and have a POS-tag ?VA? in 
Penn Chinese Treebank. These predicative adjec-
tives indicate situations. However, some instances 
in the Treebank are close to normal adjectives. We 
should distinguish the difference between the 
predicative adjectives that describe situations and 
predicative adjectives that are normal adjectives. 
For example: 
 (e) ??/??/??/?? (The function of 
financial market is smooth.)
 (n)?? /? /?/?? (To provide a new 
kind of power) 
The adjective ??? (smooth)? in the example 
(e) indicates a situation. We regard this adjective 
as an actual world event. However, the adjective 
?? (new)? in the example (n) is a modifier of the 
noun ??? (power)?. This adjective do not indi-
cate a situation, therefore it dose not represent an 
event. 
Another situation of non-event verbs is a verb in 
a named entity. Because of the strategy of the 
POS-tagging of Penn Chinese Treebank, a named 
entity is separated to several words and these 
words are tagged independently. For example: 
 (o) ???/??/??/?? (Alliance of 
Democratic Forces for Liberating Congo-
Zaire)? 
The example (o) shows a named entity that in-
cludes a word ??? (liberate)? has the POS-tag 
?VV?. However, this verb does not represent an 
actual event or a hypothetical event. It is a sub-
string of the named entity. We define this kind of 
verbs as non-event verbs. 
3.5 Attribute of event types 
Figure 2 summarizes the event types of verbs in 
section 3.1-3.4. We divide the verbs roughly into 
two types ?actual world? and ?hypothetical world?. 
Each type includes several sub-types. We annotate 
these two event types of verbs to our previous 
temporal relation annotated corpus. The definition 
of these event types in previous sections is a guide-
line for our annotators. This new attribute has two 
35
Sixth SIGHAN Workshop on Chinese Language Processing
values ?actual world? and ?hypothetical world?. 
Although the types of values are coarse-grained, 
this attribute can describe whether a verb can be 
recognized as an event with understandable tempo-
ral relation on the timeline or not. 
However, the value ?hypothetical world? of the 
event types means not only that the verbs with this 
value are temporal relation un-recognizable events, 
but also that the verbs with this value are ?locally 
recognizable? events. For example: 
 (p) ??/??/??/??/??/?/??/
?? (They wish the government to in-
crease budget to repair the bank) 
The verb ??? (wish)? governs the verb phrase 
???/??/??/?/??/?? (the government 
increases budget to repair the bank)?. Therefore the 
verb phrase represents a hypothetical world event 
(because we do not know if the government will do 
it or not). However, considering the local context 
of the verb phrase, it includes two verbs that have a 
causal relation between them. The event ???
(increase)? should occur before the event ???
(repair)?3. The temporal relation between the two 
verbs exists in the local context. We do not ignore 
this kind of temporal relations and annotate them. 
The temporal relation between the verb ??? (in-
crease)? and the verb ??? (repair)? is not un-
known but the temporal relation between the verbs 
??? (increase)? and the verb ??? (wish)? is 
unknown. 
Therefore, we regard the attribute of event type 
as a ?bridge? between an actual world and a hypo-
thetical world. The event in the actual world means 
that we can identify the temporal relation between 
 
3 The government must increase the budget and pass the delib-
eration in the congress, and then the budget can be used to 
repair the bank. 
an event and the other occurred events in an actual 
world. The temporal relations between a hypo-
thetical world event and an actual world event can 
only be identified in a hypothetical world. Figure 3 
describes this concept. The index on each event 
indicates the linear ordering of the event mention 
in the article. The two events with rectangles rep-
resent the actual world and the four events with 
diamond shapes represent the hypothetical world. 
There is no understandable temporal relation be-
tween actual and hypothetical worlds (for example 
the relation between the event 1 and event 2). The 
events in hypothetical world have their temporal 
relation with other events in the same hypothetical 
world. However, a hypothetical world is independ-
ent to other hypothetical worlds. Therefore, the 
temporal relation between event 2 and event 3 un-
derstandable but the relation between event 3 and 
event 4 are unknown. We ask our annotators to 
annotate the understandable temporal relations in 
each hypothetical world because the instances of 
the local context are useful in analyzing the tempo-
ral relation between events in actual world by ma-
chine learning. 
4 Evaluation Experiments 
Figure 3: The actual world and hypothetical 
worlds 
HYPOTHETICAL WORLD 2
ACTUAL WORLD
HYPOTHETICAL WORLD 1
Event 1 Event 5
Event 
2
Event 
3
Event 
4
Event 
6
UNKNOWN
UNKNOWN
UNKNOWN UNKNOWN
UNKNOWNRELATION RELATION
RELATION
Figure 2: The classifications of event types 
EVENT
hypothetical worldactual world
happened 
truth
uncertain 
future (b), (j)
certain 
future (d)
wish 
(h)requisition (i),(p)
ability (g)non-event
happened truth as a 
modifier of copula 
(k)
normal occurrence 
(a),(c)
modifier without 
event (n)
back ground 
knowledge (l)
uncertain 
desire
name entity 
(o)
Note: the characters in the brackets refer to the examples of each event type
statement 
(e)
36
Sixth SIGHAN Workshop on Chinese Language Processing
After we manually annotate the event type of verbs 
on our temporal relation tagged corpus, we use 
support vector machines as machine learner to 
compose a temporal relation identifier. We per-
form an experiment to investigate the effect of the 
event type information.  
4.1 The data set 
We annotated a part of Penn Chinese Treebank 
with our previous criteria. The temporal relation 
tagged corpus includes 7520 verbs. Each verb has 
three types of temporal relation that we introduce 
in section 2.3. We annotate the event type informa-
tion manually and refine some ambiguous in-
stances. For efficiency, we introduce grouping on 
the temporal relation classes. Our criteria defined 
ten classes of temporal relation values. We com-
pose three types of temporal relation identifiers 
(RLP, RTA and RTP) and an event type classifier.  
To discriminate the event types of verbs, we add 
two possible values of temporal relations, the value 
?hypothetical? and ?copula-existence?. The value 
?hypothetical? is introduced in the temporal rela-
tion type ?RTA?. If the verb represents a hypo-
thetical world event or non-event, the verb is en-
closed into the hypothetical world. The verb in hy-
pothetical world cannot have a RLP relation (Rela-
tion of adjacent event pair) between hypothetical 
and actual worlds. However, for recognizing the 
verb that is the root event of the hypothetical world, 
we annotate the RTA relation (Relation of adjacent 
event pair) of the root event in hypothetical world 
as the value ?hypothetical?. The value ?copula-
existence? is introduced to annotate the event em-
phasized by the copula verb. If the copula verb 
governs a verb phrase with several verbs, the root 
event of the verb phrase has the value ?copula-
existence?. 
The possible values of three types of temporal 
relations and event types in our experiment are 
summarized as follows: 
 Event types: actual world and hypothetical 
world 
 RLP: after (includes the values ?after? and 
?begun-by? in our criteria), before (includes 
the values ?before? and ?end-by? in our cri-
teria), simultaneous, overlap (includes the 
values ?overlap?, ?overlapped-by?, ?in-
clude?, ?during? our criteria) 
 RTA: after, before, simultaneous, overlap, 
unknown, copula-existence, hypothetical 
 RTP: after, before, simultaneous, overlap 
The training data for SVMs includes 151 articles 
with 49620 words and 7520 verbs and the testing 
data is collected from articles in Penn Chinese 
Treebank other than training data (testing data in-
cludes 50 short articles with 5010 words and 732 
verbs). The basic information of our corpus and the 
distribution of the value of attributes in our training 
and testing data are shown in Table 1. It should be 
noted that the number of the attributes of the data 
ignore some negligible instances. Such as, if a verb 
does not have sibling verbs in the dependency 
structure, to consider the attribute ?RTP (Relation 
between focus event and its sibling event)? is un-
necessary. Therefore the total numbers of the at-
tribute ?RTA? and the attribute ?RTP? are less 
than the number of all verbs. 
4.2 Experiment 
We train each classifier (event types, RLP, RTA 
and RTP) by an independent model. The features 
for machine learning are also tuned independently. 
We evaluate the accuracy of automatic annotation 
of event types and temporal relations with and 
without our event types. We use our event type tag 
as a feature of the three temporal relations. Other 
features for SVM analyzer to annotate the three 
types of temporal relations include the morpho-
logical information of the focus event pair and the 
dependency structure of the sentence. These fea-
tures can be extracted from the dependency struc-
tures automatically. 
The results are shown in Table 2. The abbrevia-
tions ?R?, ?P? and ?F? mean ?Recall?, ?Precision? 
and ?F-measure?. The row ?Accuracy w/o event 
type? means the results of the temporal relations 
annotating without using the event type as a feature. 
Other rows use the event type which is annotated 
Table 1: The distribution of our data set 
actual world: 453
hypothetical world: 279
actual world: 4584
hypothetical world: 
2936
Test Test TestTrainTrainTrain
Test (732 verbs)
234
75
2
20
36
101
552
39
163
15
63
191
81
732
216
24
75
139
278
261155537520Total
Train (7520 verbs)Event Types
7042157Unknown
417
1587
96
1011
1864
580
RTA
copula-
existence
85212overlap
3721273before
525
925
RTP
1391
2487
hypothetical
after
simultaneous
RLP
37
Sixth SIGHAN Workshop on Chinese Language Processing
by a machine learning-based analyzer as a feature. 
Because there is no similar related research that 
analyzes temporal relation between Chinese verbs 
based on machine learning, we cannot make any 
comparison. We discuss the accuracy of temporal 
relation annotating with and without using our 
event type according to the result of our experi-
ment.  
4.2 Discussions 
Table 2 shows that the model with the result of the 
event type classifier is better than that without us-
ing the result of the event type classifier. However, 
the improvement of using event types is limited. 
The reason might be the accuracy of event type is 
as low as 83%. To improve the performance of 
event type annotation helps to improve the relation 
annotation. 
There is no research based on the same data set 
and corpus guideline, therefore we can not com-
pare the result to other research. However, in the 
shared task: ?TempEval4 Temporal Relation Identi-
fication? (Verhagen, 2007), the task ?temporal re-
lations between matrix verbs? resembles the goal 
of our corpus. The F-measure in TempEval shared 
task distribute between 40%~50%. The result of 
the shared task also shows the difficulty of auto-
matic temporal relation analysis. 
5 Conclusions  and future directions 
We propose a machine learning-based temporal 
relation identification method. This is the first 
work of the temporal relation identification be-
tween verbs in Chinese texts. To deal with the de-
ficiency in our previous temporal relation annotat-
 
4 This shared task deals with English news articles.(TimeBank 
1.2)  
ing criteria, we newly introduce the event types of 
Chinese verb. The result of evaluation experiments 
shows that the event type information helps to im-
prove the accuracy of the identifier.  
A deficient of our experiment is that we do not 
use semantic information as features for machine 
learner. Semantic information of temporal and 
event expressions is important for recognizing 
temporal relations between events. As a future re-
search, we would like to introduce causal relation 
knowledge of verbs (this is similar to VerbOcean 
(Chklovski, 2004)). We are collecting this kind of 
verb pairs and expect that this causal relation helps 
to improve the performance of automatic annota-
tion. 
References 
Emmon Bach. 1986. the algebra of events. Linguistics 
and Philosophy 9. 
Yuchang Cheng, et al 2007. Constructing a Temporal 
Relation Tagged Corpus of Chinese based on De-
pendency Structure Analysis. TIME 2007. 
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic 
Verb Relations. EMNLP 2004. 
Nancy Chinchor. 1997. MUC-7 named entity task defi-
nition.
http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/proceedings/muc_7_proceedings/overview.html. 
Wenjie Li, et al 2004. Applying Machine Learning to 
Chinese Temporal Relation Resolution. ACL 2004. 
Inderjeet Mani, et al 2006. Machine Learning of Tem-
poral Relations. COLING/ACL 2006. 
IREX Committee. 1999. Named entity extraction task 
definition. http://nlp.cs.nyu.edu/irex/NE/df990214.txt, 
1999. 
Martha Palmer, et al 2005. Chinese Treebank 5.1.
http://www.ldc.upenn.edu/.  LDC. 
James Pustejovsky, et al 2006. TimeBank 1.2.
http://www.ldc.upenn.edu/. LDC. 
Roser Saur?, et al 2005. TimeML Annotation Guidelines.
http://www.timeml.org/. 
Marc Verhagen, et al 2007. SemEval-2007 Task 15: 
TempEval Temporal Relation Identification. ACL 
2007 Workshop: SemEval-2007. 
Fei Xia. 2000. The Part-Of-Speech Tagging Guidelines 
for the Penn Chinese Treebank.
http://www.cis.upenn.edu/~chinese/ctb.html. 
Table 2: The results of our experiment 
0.610.600.61Accuracy w/o 
Event type
0.70
0.45
0.33
0.46
0.67
F
0.72
0.62
0.69
0.32
0.73
0.52
F
0.71
0.51
0.33
0.50
0.65
FP P PRRR
0.68
1
0.35
0.52
0.63
0.67
0.68
0.81
0.32
0.68
0.64
0.69
1
0.35
0.51
0.62
0.610.630.62Accuracy
0.740.73unknown
0.80
0.57
0.6
0.31
0.81
0.45
RTA
copula-existence
0.290.45overlap
0.420.45before
0.32
0.71
RTP
0.32
0.70
hypothetical
after
simultaneous
RLP
0.83Accuracy
0.770.760.780.920.910.93R /P /F
hypothetical worldactual worldEvent Types
38
Sixth SIGHAN Workshop on Chinese Language Processing
Analyzing Chinese Synthetic Words with Tree-based Information
and a Survey on Chinese Morphologically Derived Words
Jia Lu
Nara Institute of
Science and Technology
jia-l@is.naist.jp
Masayuki Asahara
Nara Institute of
Science and Technology
masayu-a@is.naist.jp
Yuji Matsumoto
Nara Institute of
Science and Technology
matsu@is.naist.jp
Abstract
The lack of internal information of Chinese
synthetic words has become a crucial prob-
lem for Chinese morphological analysis sys-
tems which will face various needs of seg-
mentation standards for upper NLP applica-
tions in the future. In this paper, we first
categorize Chinese synthetic words into sev-
eral types according to their inside semantic
and syntactic structure, and then propose a
method to represent these inside information
of word by applying a tree-based structure.
Then we try to automatically identify the in-
ner morphological structure of 3-character
synthetic words by using a large corpus and
try to add syntactic tags to their internal
structure. We believe that this tree-based
word internal information could be useful
in specifying a Chinese synthetic word seg-
mentation standard.
1 Introduction
Chinese word segmentation has always been a diffi-
cult and challenging task in Chinese language pro-
cessing. Several Chinese morphological analysis
systems have been developed by different research
groups and they all have quite good performance
when doing segmentation of written Chinese. But
there still remain some problems. The biggest one is
that each research group has its own segmentation
standard for their system, which means that there
is no single segmentation standard for all tagged
corpora which can be agreeable across different re-
search groups. And we believe that this situation
slows down the progress of Chinese NLP research.
Among all the differences of segmentation stan-
dards, the segmentation method for Chinese syn-
thetic words is the most controversial part because
Chinese synthetic words have a quite complex struc-
ture and should be represented by several segmen-
tion levels according to the needs of upper applica-
tions such as MT, IR and IME.
For instance, a long(upper level) segmentation
unit may simplify syntactic analysis and IME ap-
plication but a small(lower level) segmentation unit
might be better for information retrieval or word-
based statistical summarization. But for now, no
Chinese morphological analysis system can do all
kinds of these workwith only one segmentation stan-
dard.
Furthermore, although every segmentation system
has good performance, in the analysis of real world
text, there are still many out-of-vocabulary words
which could not be easily recognized because of the
flexibility of Chinese synthetic word construction,
especially proper names that could always appear as
synthetic words.
In order to make our Chinese morphological anal-
ysis system to recognize more out-of-vocabulary
words and to fit different kinds of NLP applications,
we try to analyze the structure of the internal infor-
mation of Chinese synthetic words, categorize them
into semantic and syntactic types and store these in-
formation into a synthetic word dictionary by repre-
senting themwith a kind of tree structure built on our
system dictionary.
In this paper, we first make the definition of Chi-
53
Sixth SIGHAN Workshop on Chinese Language Processing
nese synthetic words and classify them into several
categories in Section 2. In Section 3, two previous
researches on Chinese synthetic words will be intro-
duced. Then we propose a tree-based method for
analyzing Chinese synthetic words and make a sur-
vey focused on 3-character morphological derived
words to get the features for future machine learning
process. In Section 4, we do an experiment by us-
ing SVM classifier to annotate 3-character morpho-
logically derived words. Finally, Section 5 shows
how this method could benefit Chinese morphologi-
cal analysis and our future work.
2 Detailed study of Chinese synthetic
words
2.1 Definition of Chinese words
There has always been a common belief that Chi-
nese 'doesn't have words', but instead has 'charac-
ters', or that Chinese 'has no morphology' and so is
'morphologically impoverished', because in Chinese
a 'word' is by no means a clear and intuitive notion.
But actually for native Chinese speakers, they know
that words are those lexical entries which represent
a complete concept and occur innately in the form of
specific language rules based on the speaker's mental
lexicon.
Though there are a lot of ways to classify Chinese
words, we believe that Chinese words should be first
divided into single-morpheme words and synthetic
words according to the way of construction of their
internal parts.
Single-morpheme words are those that could not
be divided into smaller parts when representing as
the whole concept. In other words, if we divide sin-
gle morpheme words into characters or parts, the
meaning of individual parts become independent and
does not indicate any connectionwith themeaning of
the original word. Following are the three different
types of single-morpheme words:
?one-character words:
?[human],?[horse],?[vehicle]
?one-morpheme words:
??[quail],??[jadeite]
??[mandarin duck]
?transliteration words:
??[pizza],???[Kentucky]
????[aspirin]
The first kind is obvious single words in that an
ordinary character in Chinese stands for an indepen-
dent morpheme with one or several senses. The sec-
ond kind shows those words which are composed of
several characters and always used as a whole. For
the last kind, as can be seen from the above exam-
ples, if we divide ???[Kentucky] into '?[can]',
'?[moral]' and '?[base]', it definitely can not indi-
cate the meaning of the well-known fried chicken
restaurant chain from those three characters. So
these three kinds of single-morpheme words should
be segmented as one word in any morphological
analysis systems.
However, it becomes much more complicated
when dealing with synthetic words. Generally, syn-
thetic words are the type of words which are com-
posed of single-morpheme words and represent a
new entity or meaning which can be indicated from
the internal constituents. According to this defini-
tion, if we divide synthetic words into smaller parts,
we could still somehow guess the original meaning
from the meaning of internal parts despite the fact
that it may not be a very precise one. For example the
word??[driver]. If we don't know the meaning of
'??', but we do know themeaning of '?' is 'control'
and the meaning of '?' is 'machine'. Then we can
guess the meaning of '??' may be connected with
'control' and 'machine', and actually the real meaning
is the person who drives(controls) a car(machine).
In Chinese language, according to the encoding
standard of GB2312, there are about 6,763 com-
monly used characters. And in our own system dic-
tionary which has about 129,440 word entries, the
number of one-character words is only 6,188 (about
4.78%). From these figures, we know that most Chi-
nese words belong to synthetic words and a deep
analysis for synthetic words is necessary for Chinese
language processing.
2.2 Classification of Chinese synthetic words
The Synthetic words may be understood as the re-
sult or 'output' of a word-formation rule in Chi-
nese language. Classification of these Chinese syn-
thetic words is a difficult task because the 'formation
rule' is not so obvious and sometimes even a native
speaker can not determine which category a word
54
Sixth SIGHAN Workshop on Chinese Language Processing
Figure 1: Types of Chinese synthetic words
should belong to. However, since it is quite impor-
tant to understand the structure of Chinese words,
there have been a lot of research on classification
of Chinese synthetic words from both linguistic and
computational points of view until now. Each of
them has divided synthetic words into different cat-
egories according to their own criteria. In our re-
search, based on our experience on Chinese morpho-
logical analysis and unknown word detection, we di-
vide Chinese synthetic words into the categories as
shown in Figure 1 to help us understand the inner
constituents of them.
2.3 Compound words
Compound words, whose internal constituents have
some syntactic relations with each other, can be
divided into the following six kinds according to
(Yuanjian He, 2004).
? Subject-predicate[???]
words that only have subject and predicate
parts. This type is subdivided into two types:
SV and VS.
VS:??/?[porter],??/?[referee]
SV:?/??[gastroptosia],?/?[earthquake]
? Verb-object[???]
words that have verb and object parts, which
contains two types: VO and OV.
OV:?/??[representative of party]
VO:?/?[haircut],?/??[anti-government]
? Verb-modification[?????]
words that have a verb part and an adjunct part
which are neither subject nor object of the verb.
The adjunct part always shows the property of
the verb part or be the media of the verb's ac-
tion.This type contains VX and XV.
VX:??/?[amplifier],??/?[print shop]
XV:??/??[automatic control]
?/?[wholesale]
? Predicate-complement[???]
words that have a verb part and a complement
part, which shows the result, direction or aspect
of the action. This type also have two kinds:
VV and VA.
VV:?/??[running out of]
??/?[get rid of]
VA:?/?[dyeing red]
? Parallel-combination[???]
words that have a coordinate structure where
the meanings of constituents are same, similar,
related or opposite.
Example: ?/?[switch],?/?[learning]
?/?[nation],?/?[brother]
?/?/?[China, Japan and Korea]
? Noun-modification[?????]
words that have a noun part which is the root of
the word, and a modification part which shows
the property of the noun part.
Example: ?/?[computer],?/?[book shelf]
??/?[bus stop]
Here we have made some compromises with these
categories mentioned above based on the simplic-
ity of machine learning process and our experience
on tagging compund words. For example, we only
55
Sixth SIGHAN Workshop on Chinese Language Processing
define SV and VS as the subject-predicate type be-
cause it will make machine learning process much
easier when it comes to those words with a struc-
ture like SVO or SVX. Futhermore, in the case that
a word with an internal constituent which has both
NN and VV parts of speech, even human annota-
tors can not easily tell which type this word should
be categarized into between noun-modification and
verb-modification. So we tagged all these words to
verb-modification when they have an internal part
with both NN and VV parts of speech.
2.4 Morphologically derived words
Morphologically derived words are those which
have specific word formation. It can be categoried
into the following types:
? Merging
words that are composed of two adjacent and
semantically related words, which have some
characters in common. It could be seen as a
kind of abbreviation.
Example: ??+??????
[middle and preliminary school]
??+??????[context]
???+???????
[mayer of Beijing city]
? Reduplication
words that contain some reduplicated charac-
ters. There are eight main patterns of redu-
plication: AA, ABAB, AABB, AXA, AXAY,
XAYA, AAB and ABB.
Example: ?/?[listen],?/??[valiantly]
??/??[research]
? Affixation
words that are composed of a word and an af-
fix(either a prefix, a suffix or an infix).
Example: ???[vice president]
????[Executive Engineer]
???[can't see]
???[can hear]
???[bureau of investigation]
???[security agency]
2.5 Exceptions
Apart from compound words and morphologically
derived words, there still exist some types of words
which need discussion about whether they belong
to synthetic words or not. However, we can use
some other methods like time expression extraction
or named entity recognition to deal with these kinds
of words.
? Abbreviations
expressions that have a short appearance, but
stand for a long term.
Example: ????????
[Communist Party of China]
? Factoids
expressions that indicate date, time, number,
money, score or range. This kind of expressions
have a large variation in their appearance.
Example: 2007.1.30
???[five thirty]
?????[3.56 yuan]
? Idioms, proverbs, sayings and poems
expressions that usually consist of more than
three characters and always have a special
meaning.
Example: ????[sparsely visited]
???????
[be the First to bear hardships]
3 Previous research and tree-based
method
3.1 Previous research
Until now, there is little specific research on Chi-
nese synthetic words. However, every institution has
its own way of dealing with synthetic words in their
segmentation standard when doing Chinese morpho-
logical analysis. There are two main previous re-
searches on the analysis of Chinese synthetic words.
The first one is done by Microsoft (Andi Wu,
2003) by creating a customizable segmentation sys-
tem of Chinesemorphologically derivedwords. This
system uses a parameter driven method which can
divide synthetic words into different levels of word
56
Sixth SIGHAN Workshop on Chinese Language Processing
components based on some pre-defined rules, ac-
cording to the needs of different NLP application.
For instance, in machine translation, we will trans-
late '????' into 'toaster' if our system dictio-
nary has this kind of information. But if we do
not have this entry in our dictionary, we have to
split '????[toaster]' into lower level such as
'?[bake] / ??[bread] / ?[machine]', the transla-
tion of whichwill probably give us some information
about the original meaning of the whole word. Al-
though this system achieves higher score than other
systems that do not have synthetic analysis, it only
takes morphologically derived words into account,
which means it does not contain information about
internal syntactic relations.
The second one (C. Huang, 1997) is actually a
proposal of segmentation standard rather than a de-
tailed synthetic word analysis research. It is first
used by Sinica when doing the tagging task of Chi-
nese word segmentation. If the tagging object is
a synthetic word, one tag among w0, w1 and w2,
which stand for 'faithful', 'truthful' and 'graceful', will
be selected for it. For example, if we have a syn-
thetic word '??????[security agency of Bei-
jing city]', this tagging method will divide the word
as follows:
<w2>
<w1><w0>??</w0><w0>?</w0></w1>
<w1><w0>??</w0><w0>?</w0></w1>
</w2>
Again, this kind of method does not take word in-
ternal syntactic relations into account either. Fur-
thermore, it even does not have the POS information
of different levels of word, thus can not be used to
construct a customizable system.
3.2 Synthetic word analysis with tree-based
structure information
For specifying consistent Chinese segmentation
standard for our morphological analysis system and
fertilizing the information of our dictionary, we pro-
pose a synthetic word analysis method with tree-
based structure information.
We assume that words which are already in our
current system dictionary could be word components
of other out-of-vocabulary synthetic words. So the
first thing to do is to classify all synthetic words
in our current dictionary into the categories defined
in section 2.2. Because intuitively most 2-character
words, though they could have internal syntactic re-
lations, are often used as single words by native
speakers and have already been registered as lexical
entries in our Chinese dictionary, we can first clas-
sify all 3-character words into those categories and
link their internal components to 1-character words
and 2-character words which are already in our dic-
tionary.
After finishing the internal structure annotation
for 3-character words, we can easily construct 4-
character or 5-character words' structure by using
3-character and 2-character words' information and
store these structure information into synthetic word
dictionary.
Finally, when we get a long synthetic word, we
can build a tree structure recursively like in Figure
2 by using the constituent words' internal structures,
which have already been stored in our synthetic word
dictionary.
Figure 2: Synthetic word tree of '???????
[vice spokesman of State Department]'
When constructing this kind of tree, we can use
some rules which have the following form:
A + B? Category
or A + B + C? Category
where A, B and C are parts of speech, affixation
or other properties of word components.
3.3 Annotation of morphologically derived
words in system dictionary
Usually, in Chinese, 2-character words are thought
and used as single words by native speakers. And
words which have more than two characters are of-
ten synthetic words which can be categorized into
57
Sixth SIGHAN Workshop on Chinese Language Processing
compound words or morphologically derived words.
So during our work of analyzing Chinese synthetic
words, we first choose 3-character words as ourmain
target in that starting from 3-character words will
give us a good chain effect when analyzing words
which have more than three characters.
At first, there is no other resource at hand ex-
cept for the morphological analysis system dictio-
nary with 129440 entries. Because a standard set
with all category information of Chinese synthetic
words is needed in further research, we first ex-
tracted 1000 3-character words from our dictionary
and annotated them by hand according to the cate-
gories introduced in section 2. As the result of hu-
subject-predicate 4.8%
verb-object 2.0%
verb-modification 21.3%
predicate-complement 3.2%
parallel-combination 0.2%
noun-modification 62.9%
single-morpheme word 5.4%
Table 1: Compound words in 1000 words
man annotation, Table 1 and 2 show the distribution
of compound words and morphologically derived
words. In Table 1, although about 6% of the 1000
words are single-morpheme words, we still can see
that noun-modification words occupy the largest part
(62.9%) in synthetic words from a syntactic point of
view. Table 2 gives us the information that most syn-
prefix infix suffix merging reduplication
9.0% 0.5% 83.0% 1.5% 0.2%
Table 2: Morphologically derived words in 1000
words
thetic words (83%) have an internal structure with a
suffix.
Since most 3-character Chinese words have the
structure such as 'two+one' or 'one+two' character
formation, it is obvious that we should first look at
noun-modification words with frequently used suf-
fixes as the beginning of our analysis. We could get
a list of characters of possible affixation from this
process too. Furthermore, we also find that parallel-
combination words and reduplication words tend to
have some fixed structures which makes them easy
to recognize.
4 Experiment on Morphologically derived
words
In order to apply our proposed tree-based analysis
method, we first have to annotate all 3-character
words in our dictionary with their internal parts
linked to 2-character and 1-character words. Be-
cause most Chinese 3-character words have prefix or
suffix structure, we assume that it will be much effi-
cient for us to annotate 3-character words if we can
classify them from the aspect of morphologically de-
rived words.
Because we don't have any other useful resources
except ChineseGigaword(CGW),We first computed
mutual information for all 3-character words in two
ways by referencing the CGW. For example, if we
have a word ABC and we assume that A, C, AB and
BC are all independent entries in our dictionary, we
compute the mutual information Mi-pre for A and
BC, and the mutual information Mi-suf for AB and
C.
Since A, C, AB and BC are all independent words,
if the result showsMi-pre<Mi-suf, we could say that
the relation between A and BC is more independent
than the relation between AB and C, which means it
is more possible that A and some other 2-character
word XY cound form the word AXY. So the possi-
bility of A being a prefix is greater than the possibil-
ity of C being a suffix. Then we could conclude that
the word ABC has a prefix internal structure. Other-
wise, we could say it has suffix internal structure.
After this process, we got a Mi(Mi-pre or Mi-suf)
and a possible internal structure(prefix or suffix) for
every 3-character word in system dictionary. By
comparing these results to the 1000 extracted words
whose internal structure has been already known, we
can easily get the correct ones whose possible inter-
nal structure is the same as the ones annotated by
hand.
Except for the ones which have infix, merging
or redplication structure, there are 920 words in the
1000 extracted words which are tagged as prefix or
suffix structure by hand. After comparing, we got
58
Sixth SIGHAN Workshop on Chinese Language Processing
676 out of 920 words(73.48%), which was divided
correctly by only looking at the internal mutual in-
formation in a large corpus(Chinese Gigaword).
The above result shows that overall accuracy is
quite low by only taking account the internal mutual
information when classifying prefix and suffix struc-
ture. Some examples of wrongly classifiedwords are
shown in Table 3.
words Mi-pre Mi-suf result
??? 7.024e-07 9.981e-07 ? /??
??? 1.084e-06 1.171e-05 ? /??
??? 1.384e-05 1.978e-05 ? /??
??? 1.993e-06 2.440e-06 ? /??
??? 8.971e-08 6.762e-08 ?? /?
Table 3: Examples of wrongly dividedwords by only
using mutual information
As shown in Table 3, most uncorrect ones are
words having suffix internal structure but wrongly
classified to have prefix internal structure. This is
because we only counted the frequencies of internal
parts of words without considerring their properties
such as parts of speech and the frequencies that the
internal parts show out at a particular position, etc.
In order to improve the whole accuracy when rec-
ognizing prefix or suffix internal structure automati-
cally, we used an SVM classifier with the following
features(in the case of 3-character string ABC):
1.internal part: A, C, BC, AB, ABC
2.pos of each internal part:
pos(A), pos(C), pos(BC), pos(AB), pos(ABC)
3.frequency of each part in Chinese Gigaword:
fre(A), fre(C), fre(BC), fre(AB), fre(ABC)
4.mutual information of internal part:
Mi-pre(A-BC), Mi-suf(AB-C)
(In the actual classification process, we set the fre-
quency range by 2000 and the mutual information
range by log10)
After dividing 80% of 920 words into traning set
and 20% into testing set, the accuracy of SVM clas-
sifier is 94.02%. The precision and recall are shown
in the first row of Table 4. Because the above ex-
periment(A) did not take the existence of 2-character
words in system dictionary into account, we then add
these features and run the SVM classifier again. Fi-
Exp Acc. F Prefix(%) Suffix(%)
(%) Rec. Pre. Rec. Pre.
A 94.02 0.56 38.89 100.0 100.0 93.79
B 94.57 0.67 55.56 83.33 98.80 95.35
Table 4: Results of Recall and Precision for words
which have prefix or suffix structure
nally we get the result of experiment(B) shown in the
second row of Table 4.
This result is quite unbalanced because there
are only a few intances of prefixes both in
training(72/736=9.78%) and testing(18/184=9.78%)
sets. This is the reason of low recall in classifying
instances of prefixes. The following words are the
ones which were wrongly classified.
???????????????????
???????????????????
It turns out that these words contains mainly two
types: the first type contains word like '???', a
prefix structure word whose last character has a quite
high probability to be a suffix. This makes it difficult
for SVM to determine to which class the whole word
should be classified; an example of the second type
is suffix structure word like '???', whose front
part '??' does not appear in the Chinese Gigaword
independently, which in the end make it unsure for
SVM to classify it into suffix structure word.
Though there are some words that were wrongly
categorized, we still got a overall accuracy of
94.57% which would be much higher if we recur-
sively use SVMs for classification. We believe that
this method could classify morphologically derived
words quite efficiently if we add somemore rules for
recognizing merging and reduplication words. And
by applying the tree-based method, we could use this
method on words with more than 3 characters in fu-
ture.
5 Conclusion and future work
This paper proposed a tree-based method for analyz-
ing Chinese synthetic words by constructing a Chi-
nese synthetic word dictionary. This method is based
on the classification of Chinese synthetic words both
from syntactic and morphological ways. After anno-
tating and investigating the distribution of one thou-
sand 3-character words, we used frequencies and
59
Sixth SIGHAN Workshop on Chinese Language Processing
mutual information as features from Chinese Giga-
word for machine learning. With these features, we
tried to classify morphologically derived words into
prefix or suffix internal structure by using SVM clas-
sifier.
For future work, we have to take other morpholog-
ical internal structures into account and try to clas-
sify all synthetic words intomorphologically derived
word categories. Then, we should also find some
thesaurus that contain syntactic information ofwords
or characters to help us analyze the compoundwords'
internal structure. Finally, after gathering the infor-
mation of Chinese synthetic words from both syn-
tactic and morphological aspect, we will build a Chi-
nese synthetic word dictionary and try to use it to im-
prove the performance of ourmorphological analysis
system and unknown word extraction.
References
[Andi Wu2003] Andi Wu. 2003. Customizable Segmen-
tation of Morphologically Derived Words in Chinese.
Vol.8, No.1, February 2003, pp. 1-28 Computational
Linguistics and Chinese Language Processing
[C. Huang1997] C. Huang, K. Chen and L. Chang 1997.
Segmentation standard for Chinese natural language
processing. International Journal of Computational
Linguistics and Chinese Language Processing
[Chooi-Ling Goh2006] Chooi-Ling Goh, Jia Lu, Yuchang
Cheng, Masayuki Asahara and Yuji Matsumoto 2006.
The Construction of a Dictionary for a Two-layer Chi-
nese Morphological Analyzer. PACLIC 2006
[Hiroshi Nakagawa, Hiroyuki Kojima, Akira Maeda2004]
Hiroshi Nakagawa, Hiroyuki Kojima, Akira Maeda
2004. Chinese Term Extraction fromWeb Pages Based
on Compound word Productivity. Third SIGHAN
Workshop on Chinese Language Processing, ACL
2004
[Huihsin Tseng and Keh-Jiann Chen2002] Huihsin Tseng
and Keh-Jiann Chen 2002. Design of Chinese Mor-
phological Analyzer. First SIGHAN Workshop 2002
[Jerome L. Packard2000] Jerome L. Packard 2000. The
Morphology of Chinese-A Linguistic and Cognitive
Approach.
[Keh-Jiann Chen, Chao-jan Chen2000] Keh-Jiann Chen,
Chao-jan Chen 2000. Automatic Semantic Classifi-
cation for Chinese Unknown Compound Nouns. ACL
2000
[Shengfen Luo and Maosong Sun2003] Shengfen Luo
and Maosong Sun 2003. Two-character Chinese
Word Extraction Based on Hybrid of Internal and
Contextual Measures. ACL 2003
[Yuanjian He2004] Yuanjian He 2004. ???????
-??????????????????????.
The Chinese University of Hong Kong
60
Sixth SIGHAN Workshop on Chinese Language Processing
Japanese Named Entity Extraction
with Redundant Morphological Analysis
Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology, Japan
 
masayu-a,matsu  @is.aist-nara.ac.jp
Abstract
Named Entity (NE) extraction is an important
subtask of document processing such as in-
formation extraction and question answering.
A typical method used for NE extraction of
Japanese texts is a cascade of morphological
analysis, POS tagging and chunking. However,
there are some cases where segmentation gran-
ularity contradicts the results of morphologi-
cal analysis and the building units of NEs, so
that extraction of some NEs are inherently im-
possible in this setting. To cope with the unit
problem, we propose a character-based chunk-
ing method. Firstly, the input sentence is an-
alyzed redundantly by a statistical morpholog-
ical analyzer to produce multiple (n-best) an-
swers. Then, each character is annotated with
its character types and its possible POS tags of
the top n-best answers. Finally, a support vec-
tor machine-based chunker picks up some por-
tions of the input sentence as NEs. This method
introduces richer information to the chunker
than previous methods that base on a single
morphological analysis result. We apply our
method to IREX NE extraction task. The cross
validation result of the F-measure being 87.2
shows the superiority and effectiveness of the
method.
1 Introduction
Named Entity (NE) extraction aims at identifying proper
nouns and numerical expressions in a text, such as per-
sons, locations, organizations, dates, and so on. This is
an important subtask of document processing like infor-
mation extraction and question answering.
A common standard data set for Japanese NE extrac-
tion is provided by IREX workshop (IREX Committee,
editor, 1999). Generally, Japanese NE extraction is done
in the following steps: Firstly, a Japanese text is seg-
mented into words and is annotated with POS tags by a
morphological analyzer. Then, a chunker brings together
the words into NE chunks based on contextual informa-
tion. However, such a straightforward method cannot ex-
tract NEs whose segmentation boundary contradicts that
of morphological analysis outputs. For example, a sen-
tence ? 
	 ? is segmented as ?
 / 	 /  /  / Chinese Unknown Word Identification Using Character-based Tagging and
Chunking
GOH Chooi Ling, Masayuki ASAHARA, Yuji MATSUMOTO
Graduate School of Information Science
Nara Institute of Science and Technology
 
ling-g,masayu-a,matsu  @is.aist-nara.ac.jp
Abstract
Since written Chinese has no space to de-
limit words, segmenting Chinese texts be-
comes an essential task. During this task,
the problem of unknown word occurs. It is
impossible to register all words in a dictio-
nary as new words can always be created
by combining characters. We propose a
unified solution to detect unknown words
in Chinese texts. First, a morphological
analysis is done to obtain initial segmen-
tation and POS tags and then a chunker is
used to detect unknown words.
1 Introduction
Like many other Asian languages (Thai, Japanese,
etc), written Chinese does not delimit words by
spaces and there is no clue to tell where the word
boundaries are. Therefore, it is usually required to
segment Chinese texts prior to further processing.
Previous research has been done for segmentation,
however, the results obtained are not quite satisfac-
tory when unknown words occur in the texts. An
unknown word is defined as a word that is not found
in the dictionary. As for any other language, all pos-
sibilities of derivational morphology cannot be fore-
seen in the form of a dictionary with a fixed number
of entries. Therefore, proper solutions are necessary
for the detection of unknown words.
Along traditional methods, unknown word detec-
tion has been done using rules for guessing their
location. This can ensure a high precision for the
detection of unknown words, but unfortunately the
recall is not quite satisfactory. It is mainly due to
the Chinese language, as new patterns can always
be created, that one can hardly efficiently maintain
the rules by hand. Since the introduction of statis-
tical techniques in NLP, research has been done on
Chinese unknown word detection using such tech-
niques, and the results showed that statistical based
model could be a better solution. The only resource
needed is a large corpus. Fortunately, to date, more
and more Chinese tagged corpora have been created
for research purpose.
We propose an ?all-purpose? unknown word de-
tection method which will extract person names, or-
ganization names and low frequency words in the
corpus. We will treat low frequency words as gen-
eral unknown words in our experiments. First, we
segment and assign POS tags to words in the text
using a morphological analyzer. Second, we break
segmented words into characters, and assign each
character its features. At last, we use a SVM-based
chunker to extract the unknown words.
2 Proposed Method
We shall now describe the 3 steps successively.
2.1 Morphological Analysis
ChaSen is a widely used morphological analyzer for
Japanese texts (Matsumoto et al, 2002). It achieves
over 97% precision for newspaper articles. We as-
sume that Chinese language has similar characteris-
tics with Japanese language to a certain extent, as
both languages share semantically heavily loaded
characters, i.e. kanji for Japanese, hanzi for Chinese.
Based on this assumption, a model for Japanese may
do well enough on Chinese. This morphological an-
alyzer is based on Hidden Markov Models. The tar-
get is to find the word and POS sequence that max-
imize the probability. The details can be found in
(Matsumoto et al, 2002).
2.2 Character Based Features
Character based features allow the chunker to detect
unknown words more efficiently. It is especially the
case when unknown words overlap known words.
For example, ChaSen will segment the phrase ? 

. . . ? (Deng Yingchao before death) into
?  /  / 	 /  /. . . ? (Deng Ying before next life). If
we use word based features, it is impossible to detect
the unknown person name ?  
 ? because it will
not break up the word ?  ? (next life). Breaking
words into characters enables the chunker to look at
characters individually and to identify the unknown
person name above.
The POS tag from the output of morphological
analysis is subcategorized to include the position of
the character in the word. The list of positions is
shown in Table 1. For example, if a word contains
three characters, then the first character is  POS  -B,
the second is  POS  -I and the third is  POS  -E. A
single character word is tagged as  POS  -S.
Table 1: Position tags in a word
Tag Description
S one-character word
B first character in a multi-character word
I intermediate character in a multi-
character word (for words longer than
two characters)
E last character in a multi-character word
Character types can also be used as features for
chunking. However, the only information at our dis-
posal is the possibility for a character to be a fam-
ily name. The set of characters used for translitera-
tion may also be useful for retrieving transliterated
names.
2.3 Chunking with Support Vector Machine
We use a Support Vector Machines-based chunker,
YamCha (Kudo and Matsumoto, 2001), to extract
unknown words from the output of the morphologi-
cal analysis. The chunker uses a polynomial kernel
of degree 2. Please refer to the paper cited for de-
tails.
Basically we would like to classify the characters
into 3 categories, B (beginning of a chunk), I (inside
a chunk) and O (outside a chunk). A chunk is con-
sidered as an unknown word in this case. We can
either parse a sentence forwardly, from the begin-
ning of a sentence, or backwardly, from the end of
a sentence. There are always some relationships be-
tween the unknown words and the their contexts in
the sentence. We will use two characters on each left
and right side as the context window for chunking.
Figure 1 illustrates a snapshot of the chunking
process. During forward parsing, to infer the un-
known word tag ?I? at position i, the chunker uses
the features appearing in the solid box. Reverse is
done in backward parsing.
3 Experiments
We conducted an open test experiment. A one-
month news of year 1998 from the People?s Daily
was used as the corpus. It contains about 300,000
words (about 1,000,000 characters) with 39 POS
tags. The corpus was divided into 2 parts randomly
with a size ratio for training/testing of 4/1.
All person names and organization names were
deleted from the dictionary for extraction. There
were 4,690 person names and 2,871 organization
names in the corpus. For general unknown word,
all words that occurred only once in the corpus were
deleted from the dictionary, and were treated as un-
known words. 12,730 unknown words were created
under this condition.
4 Results
We now present the results of our experiments in re-
call, precision and F-measure, as usual in such ex-
periments.
4.1 Person Name Extraction
Table 2 shows the results of person name extraction.
The accuracy for retrieving person names was quite
satisfiable. We could also extract names overlap-
ping with the next known word. For example, for
the sequence ?  /Ng  /Ag  /v  /f  /v  /v
Position Char. POS(best) Family Name Chunk
i - 2  n-S Y B
i - 1  Ag-S N I
i  Ng-S N I
i + 1  n-B N O
i + 2  n-E Y O
Figure 1: An illustration of chunking process ?President Jiang Zemin?
 /u  /n? (The things that Deng Yingchao used
before death), the system was able to correctly re-
trieve the name ?  Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 405?413,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Jointly Identifying Temporal Relations with Markov Logic
Katsumasa Yoshikawa
NAIST, Japan
katsumasa-y@is.naist.jp
Sebastian Riedel
University of Tokyo, Japan
sebastian.riedel@gmail.com
Masayuki Asahara
NAIST, Japan
masayu-a@is.naist.jp
Yuji Matsumoto
NAIST, Japan
matsu@is.naist.jp
Abstract
Recent work on temporal relation iden-
tification has focused on three types of
relations between events: temporal rela-
tions between an event and a time expres-
sion, between a pair of events and between
an event and the document creation time.
These types of relations have mostly been
identified in isolation by event pairwise
comparison. However, this approach ne-
glects logical constraints between tempo-
ral relations of different types that we be-
lieve to be helpful. We therefore propose a
Markov Logic model that jointly identifies
relations of all three relation types simul-
taneously. By evaluating our model on the
TempEval data we show that this approach
leads to about 2% higher accuracy for all
three types of relations ?and to the best
results for the task when compared to those
of other machine learning based systems.
1 Introduction
Temporal relation identification (or temporal or-
dering) involves the prediction of temporal order
between events and/or time expressions mentioned
in text, as well as the relation between events in a
document and the time at which the document was
created.
With the introduction of the TimeBank corpus
(Pustejovsky et al, 2003), a set of documents an-
notated with temporal information, it became pos-
sible to apply machine learning to temporal order-
ing (Boguraev and Ando, 2005; Mani et al, 2006).
These tasks have been regarded as essential for
complete document understanding and are useful
for a wide range of NLP applications such as ques-
tion answering and machine translation.
Most of these approaches follow a simple
schema: they learn classifiers that predict the tem-
poral order of a given event pair based on a set of
the pair?s of features. This approach is local in the
sense that only a single temporal relation is consid-
ered at a time.
Learning to predict temporal relations in this iso-
lated manner has at least two advantages over any
approach that considers several temporal relations
jointly. First, it allows us to use off-the-shelf ma-
chine learning software that, up until now, has been
mostly focused on the case of local classifiers. Sec-
ond, it is computationally very efficient both in
terms of training and testing.
However, the local approach has a inherent
drawback: it can lead to solutions that violate logi-
cal constraints we know to hold for any sets of tem-
poral relations. For example, by classifying tempo-
ral relations in isolation we may predict that event
A happened before, and event B after, the time
of document creation, but also that event A hap-
pened after event B?a clear contradiction in terms
of temporal logic.
In order to repair the contradictions that the local
classifier predicts, Chambers and Jurafsky (2008)
proposed a global framework based on Integer Lin-
ear Programming (ILP). They showed that large
improvements can be achieved by explicitly incor-
porating temporal constraints.
The approach we propose in this paper is similar
in spirit to that of Chambers and Jurafsky: we seek
to improve the accuracy of temporal relation iden-
tification by predicting relations in a more global
manner. However, while they focused only on the
temporal relations between events mentioned in a
document, we also jointly predict the temporal or-
der between events and time expressions, and be-
tween events and the document creation time.
Our work also differs in another important as-
pect from the approach of Chambers and Jurafsky.
Instead of combining the output of a set of local
classifiers using ILP, we approach the problem of
joint temporal relation identification using Markov
Logic (Richardson and Domingos, 2006). In this
405
framework global correlations can be readily cap-
tured through the addition of weighted first order
logic formulae.
Using Markov Logic instead of an ILP-based ap-
proach has at least two advantages. First, it allows
us to easily capture non-deterministic (soft) rules
that tend to hold between temporal relations but do
not have to. 1 For example, if event A happens be-
fore B, and B overlaps with C, then there is a good
chance that A also happens before C, but this is not
guaranteed.
Second, the amount of engineering required to
build our system is similar to the efforts required
for using an off-the-shelf classifier: we only need
to define features (in terms of formulae) and pro-
vide input data in the correct format. 2 In particu-
lar, we do not need to manually construct ILPs for
each document we encounter. Moreover, we can
exploit and compare advanced methods of global
inference and learning, as long as they are imple-
mented in our Markov Logic interpreter of choice.
Hence, in our future work we can focus entirely
on temporal relations, as opposed to inference or
learning techniques for machine learning.
We evaluate our approach using the data of the
?TempEval? challenge held at the SemEval 2007
Workshop (Verhagen et al, 2007). This challenge
involved three tasks corresponding to three types
of temporal relations: between events and time ex-
pressions in a sentence (Task A), between events of
a document and the document creation time (Task
B), and between events in two consecutive sen-
tences (Task C).
Our findings show that by incorporating global
constraints that hold between temporal relations
predicted in Tasks A, B and C, the accuracy for
all three tasks can be improved significantly. In
comparison to other participants of the ?TempE-
val? challenge our approach is very competitive:
for two out of the three tasks we achieve the best
results reported so far, by a margin of at least 2%. 3
Only for Task Bwe were unable to reach the perfor-
mance of a rule-based entry to the challenge. How-
ever, we do perform better than all pure machine
1It is clearly possible to incorporate weighted constraints
into ILPs, but how to learn the corresponding weights is not
obvious.
2This is not to say that picking the right formulae in
Markov Logic, or features for local classification, is always
easy.
3To be slightly more precise: for Task C we achieve this
margin only for ?strict? scoring?see sections 5 and 6 for more
details.
learning-based entries.
The remainder of this paper is organized as fol-
lows: Section 2 describes temporal relation identi-
fication including TempEval; Section 3 introduces
Markov Logic; Section 4 explains our proposed
Markov Logic Network; Section 5 presents the set-
up of our experiments; Section 6 shows and dis-
cusses the results of our experiments; and in Sec-
tion 7 we conclude and present ideas for future re-
search.
2 Temporal Relation Identification
Temporal relation identification aims to predict
the temporal order of events and/or time expres-
sions in documents, as well as their relations to the
document creation time (DCT). For example, con-
sider the following (slightly simplified) sentence of
Section 1 in this paper.
With the introduction of the TimeBank cor-
pus (Pustejovsky et al, 2003), machine
learning approaches to temporal ordering
became possible.
Here we have to predict that the ?Machine learn-
ing becoming possible? event happened AFTER
the ?introduction of the TimeBank corpus? event,
and that it has a temporal OVERLAP with the year
2003. Moreover, we need to determine that both
events happened BEFORE the time this paper was
created.
Most previous work on temporal relation iden-
tification (Boguraev and Ando, 2005; Mani et al,
2006; Chambers and Jurafsky, 2008) is based on
the TimeBank corpus. The temporal relations in
the Timebank corpus are divided into 11 classes;
10 of them are defined by the following 5 relations
and their inverse: BEFORE, IBEFORE (immedi-
ately before), BEGINS, ENDS, INCLUDES; the re-
maining one is SIMULTANEOUS.
In order to drive forward research on temporal
relation identification, the SemEval 2007 shared
task (Verhagen et al, 2007) (TempEval) included
the following three tasks.
TASK A Temporal relations between events and
time expressions that occur within the same
sentence.
TASK B Temporal relations between the Docu-
ment Creation Time (DCT) and events.
TASK C Temporal relations between the main
events of adjacent sentences.4
4The main event of a sentence is expressed by its syntacti-
cally dominant verb.
406
To simplify matters, in the TempEval data, the
classes of temporal relations were reduced from
the original 11 to 6: BEFORE, OVERLAP, AFTER,
BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER,
and VAGUE.
In this work we are focusing on the three tasks of
TempEval, and our running hypothesis is that they
should be tackled jointly. That is, instead of learn-
ing separate probabilistic models for each task, we
want to learn a single one for all three tasks. This
allows us to incorporate rules of temporal consis-
tency that should hold across tasks. For example, if
an event X happens before DCT, and another event
Y after DCT, then surely X should have happened
before Y. We illustrate this type of transition rule in
Figure 1.
Note that the correct temporal ordering of events
and time expressions can be controversial. For in-
stance, consider the example sentence again. Here
one could argue that ?the introduction of the Time-
Bank? may OVERLAP with ?Machine learning be-
coming possible? because ?introduction? can be
understood as a process that is not finished with
the release of the data but also includes later adver-
tisements and announcements. This is reflected by
the low inter-annotator agreement score of 72% on
Tasks A and B, and 68% on Task C.
3 Markov Logic
It has long been clear that local classification
alone cannot adequately solve all prediction prob-
lems we encounter in practice.5 This observa-
tion motivated a field within machine learning,
often referred to as Statistical Relational Learn-
ing (SRL), which focuses on the incorporation
of global correlations that hold between statistical
variables (Getoor and Taskar, 2007).
One particular SRL framework that has recently
gained momentum as a platform for global learn-
ing and inference in AI is Markov Logic (Richard-
son and Domingos, 2006), a combination of first-
order logic and Markov Networks. It can be under-
stood as a formalism that extends first-order logic
to allow formulae that can be violated with some
penalty. From an alternative point of view, it is an
expressive template language that uses first order
logic formulae to instantiate Markov Networks of
repetitive structure.
From a wide range of SRL languages we chose
Markov Logic because it supports discriminative
5It can, however, solve a large number of problems surpris-
ingly well.
Figure 1: Example of Transition Rule 1
training (as opposed to generative SRL languages
such as PRM (Koller, 1999)). Moreover, sev-
eral Markov Logic software libraries exist and are
freely available (as opposed to other discrimina-
tive frameworks such as Relational Markov Net-
works (Taskar et al, 2002)).
In the following we will explain Markov Logic
by example. One usually starts out with a set
of predicates that model the decisions we need to
make. For simplicity, let us assume that we only
predict two types of decisions: whether an event e
happens before the document creation time (DCT),
and whether, for a pair of events e1 and e2, e1
happens before e2. Here the first type of deci-
sion can be modeled through a unary predicate
beforeDCT(e), while the latter type can be repre-
sented by a binary predicate before(e1, e2). Both
predicates will be referred to as hidden because we
do not know their extensions at test time. We also
introduce a set of observed predicates, representing
information that is available at test time. For ex-
ample, in our case we could introduce a predicate
futureTense(e) which indicates that e is an event
described in the future tense.
With our predicates defined, we can now go on
to incorporate our intuition about the task using
weighted first-order logic formulae. For example,
it seems reasonable to assume that
futureTense (e) ? ?beforeDCT (e) (1)
often, but not always, holds. Our remaining un-
certainty with regard to this formula is captured
by a weight w we associate with it. Generally
we can say that the larger this weight is, the more
likely/often the formula holds in the solutions de-
scribed by our model. Note, however, that we do
not need to manually pick these weights; instead
they are learned from the given training corpus.
The intuition behind the previous formula can
also be captured using a local classifier.6 However,
6Consider a log-linear binary classifier with a ?past-tense?
407
Markov Logic also allows us to say more:
beforeDCT (e1) ? ?beforeDCT (e2)
? before (e1, e2) (2)
In this case, we made a statement about more
global properties of a temporal ordering that can-
not be captured with local classifiers. This formula
is also an example of the transition rules as seen in
Figure 2. This type of rule forms the core idea of
our joint approach.
A Markov Logic Network (MLN) M is a set of
pairs (?,w) where ? is a first order formula and w
is a real number (the formula?s weight). It defines a
probability distribution over sets of ground atoms,
or so-called possible worlds, as follows:
p (y) = 1
Z
exp
?
?
?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (3)
Here each c is a binding of free variables in ? to
constants in our domain. Each f?c is a binary fea-
ture function that returns 1 if in the possible world
y the ground formula we get by replacing the free
variables in ? with the constants in c is true, and
0 otherwise. C? is the set of all bindings for the
free variables in ?. Z is a normalisation constant.
Note that this distribution corresponds to a Markov
Network (the so-called Ground Markov Network)
where nodes represent ground atoms and factors
represent ground formulae.
Designing formulae is only one part of the game.
In practice, we also need to choose a training
regime (in order to learn the weights of the formu-
lae we added to the MLN) and a search/inference
method that picks the most likely set of ground
atoms (temporal relations in our case) given our
trained MLN and a set of observations. How-
ever, implementations of these methods are often
already provided in existing Markov Logic inter-
preters such as Alchemy 7 and Markov thebeast. 8
4 Proposed Markov Logic Network
As stated before, our aim is to jointly tackle
Tasks A, B and C of the TempEval challenge. In
this section we introduce the Markov Logic Net-
work we designed for this goal.
We have three hidden predicates, corresponding
to Tasks A, B, and C: relE2T(e, t, r) represents the
temporal relation of class r between an event e
feature: here for every event e the decision ?e happens be-
fore DCT? becomes more likely with a higher weight for this
feature.
7http://alchemy.cs.washington.edu/
8http://code.google.com/p/thebeast/
Figure 2: Example of Transition Rule 2
and a time expression t; relDCT(e, r) denotes the
temporal relation r between an event e and DCT;
relE2E(e1, e2, r) represents the relation r between
two events of the adjacent sentences, e1 and e2.
Our observed predicates reflect information we
were given (such as the words of a sentence), and
additional information we extracted from the cor-
pus (such as POS tags and parse trees). Note that
the TempEval data also contained temporal rela-
tions that were not supposed to be predicted. These
relations are represented using two observed pred-
icates: relT2T(t1, t2, r) for the relation r between
two time expressions t1 and t2; dctOrder(t, r) for
the relation r between a time expression t and a
fixed DCT.
An illustration of all ?temporal? predicates, both
hidden and observed, can be seen in Figure 3.
4.1 Local Formula
Our MLN is composed of several weighted for-
mulae that we divide into two classes. The first
class contains local formulae for the Tasks A, B
and C. We say that a formula is local if it only
considers the hidden temporal relation of a single
event-event, event-time or event-DCT pair. The
formulae in the second class are global: they in-
volve two or more temporal relations at the same
time, and consider Tasks A, B and C simultane-
ously.
The local formulae are based on features em-
ployed in previous work (Cheng et al, 2007;
Bethard andMartin, 2007) and are listed in Table 1.
What follows is a simple example in order to illus-
trate how we implement each feature as a formula
(or set of formulae).
Consider the tense-feature for Task C. For this
feature we first introduce a predicate tense(e, t)
that denotes the tense t for an event e. Then we
408
Figure 3: Predicates for Joint Formulae; observed
predicates are indicated with dashed lines.
Table 1: Local Features
Feature A B C
EVENT-word X X
EVENT-POS X X
EVENT-stem X X
EVENT-aspect X X X
EVENT-tense X X X
EVENT-class X X X
EVENT-polarity X X
TIMEX3-word X
TIMEX3-POS X
TIMEX3-value X
TIMEX3-type X
TIMEX3-DCT order X X
positional order X
in/outside X
unigram(word) X X
unigram(POS) X X
bigram(POS) X
trigram(POS) X X
Dependency-Word X X X
Dependency-POS X X
add a set of formulae such as
tense(e1, past) ? tense(e2, future)
? relE2E(e1, e2, before) (4)
for all possible combinations of tenses and tempo-
ral relations.9
4.2 Global Formula
Our global formulae are designed to enforce con-
sistency between the three hidden predicates (and
the two observed temporal predicates we men-
tioned earlier). They are based on the transition
9This type of ?template-based? formulae generation can be
performed automatically by the Markov Logic Engine.
rules we mentioned in Section 3.
Table 2 shows the set of formula templates we
use to generate the global formulae. Here each
template produces several instantiations, one for
each assignment of temporal relation classes to the
variables R1, R2, etc. One example of a template
instantiation is the following formula.
dctOrder(t1, before) ? relDCT(e1, after)
? relE2T(e1, t1, after) (5a)
This formula is an expansion of the formula tem-
plate in the second row of Table 2. Note that it
utilizes the results of Task B to solve Task A.
Formula 5a should always hold,10 and hence we
could easily implement it as a hard constraint in
an ILP-based framework. However, some transi-
tion rules are less determinstic and should rather
be taken as ?rules of thumb?. For example, for-
mula 5b is a rule which we expect to hold often,
but not always.
dctOrder(t1, before) ? relDCT(e1, overlap)
? relE2T(e1, t1, after) (5b)
Fortunately, this type of soft rule poses no prob-
lem for Markov Logic: after training, Formula 5b
will simply have a lower weight than Formula 5a.
By contrast, in a ?Local Classifier + ILP?-based
approach as followed by Chambers and Jurafsky
(2008) it is less clear how to proceed in the case
of soft rules. Surely it is possible to incorporate
weighted constraints into ILPs, but how to learn the
corresponding weights is not obvious.
5 Experimental Setup
With our experiments we want to answer two
questions: (1) does jointly tackling Tasks A, B,
and C help to increase overall accuracy of tempo-
ral relation identification? (2) How does our ap-
proach compare to state-of-the-art results? In the
following we will present the experimental set-up
we chose to answer these questions.
In our experiments we use the test and training
sets provided by the TempEval shared task. We
further split the original training data into a training
and a development set, used for optimizing param-
eters and formulae. For brevity we will refer to the
training, development and test set as TRAIN, DEV
and TEST, respectively. The numbers of temporal
relations in TRAIN, DEV, and TEST are summa-
rized in Table 3.
10However, due to inconsistent annotations one will find vi-
olations of this rule in the TempEval data.
409
Table 2: Joint Formulae for Global Model
Task Formula
A? B dctOrder(t, R1) ? relE2T(e, t, R2) ? relDCT(e,R3)
B ? A dctOrder(t, R1) ? relDCT(e,R2) ? relE2T(e, t, R3)
B ? C relDCT(e1, R1) ? relDCT(e2, R2) ? relE2E(e1, e2, R3)
C ? B relDCT(e1, R1) ? relE2E(e1, e2, R2) ? relDCT(e2, R3)
A? C relE2T(e1, t1, R1) ? relT2T(t1, t2, R2) ? relE2T(e2, t2, R3) ? relE2E(e1, e2, R4)
C ? A relE2T(e2, t2, R1) ? relT2T(t1, t2, R2) ? relE2E(e1, e2, R3) ? relE2T(e1, t1, R4)
Table 3: Numbers of Labeled Relations for All
Tasks
TRAIN DEV TEST TOTAL
Task A 1359 131 169 1659
Task B 2330 227 331 2888
Task C 1597 147 258 2002
For feature generation we use the following
tools. 11 POS tagging is performed with TnT
ver2.2;12 for our dependency-based features we use
MaltParser 1.0.0.13 For inference in our models
we use Cutting Plane Inference (Riedel, 2008) with
ILP as a base solver. This type of inference is ex-
act and often very fast because it avoids instantia-
tion of the complete Markov Network. For learning
we apply one-best MIRA (Crammer and Singer,
2003) with Cutting Plane Inference to find the cur-
rent model guess. Both training and inference algo-
rithms are provided by Markov thebeast, a Markov
Logic interpreter tailored for NLP applications.
Note that there are several ways to manually op-
timize the set of formulae to use. One way is to
pick a task and then choose formulae that increase
the accuracy for this task on DEV. However, our
primary goal is to improve the performance of all
the tasks together. Hence we choose formulae with
respect to the total score over all three tasks. We
will refer to this type of optimization as ?averaged
optimization?. The total scores of the all three tasks
are defined as follows:
Ca + Cb + Cc
Ga +Gb +Gc
where Ca, Cb, and Cc are the number of the cor-
rectly identified labels in each task, and Ga, Gb,
and Gc are the numbers of gold labels of each task.
Our system necessarily outputs one label to one re-
lational link to identify. Therefore, for all our re-
11Since the TempEval trial has no restriction on pre-
processing such as syntactic parsing, most participants used
some sort of parsers.
12http://www.coli.uni-saarland.de/
?thorsten/tnt/
13http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
sults, precision, recall, and F-measure are the exact
same value.
For evaluation, TempEval proposed the two scor-
ing schemes: ?strict? and ?relaxed?. For strict scor-
ing we give full credit if the relations match, and no
credit if they do not match. On the other hand, re-
laxed scoring gives credit for a relation according
to Table 4. For example, if a system picks the re-
lation ?AFTER? that should have been ?BEFORE?
according to the gold label, it gets neither ?strict?
nor ?relaxed? credit. But if the system assigns
?B-O (BEFORE-OR-OVERLAP)? to the relation,
it gets a 0.5 ?relaxed? score (and still no ?strict?
score).
6 Results
In the following we will first present our com-
parison of the local and global model. We will then
go on to put our results into context and compare
them to the state-of-the-art.
6.1 Impact of Global Formulae
First, let us show the results on TEST in Ta-
ble 5. You will find two columns, ?Global? and
?Local?, showing scores achieved with and with-
out joint formulae, respectively. Clearly, the global
models scores are higher than the local scores for
all three tasks. This is also reflected by the last row
of Table 5. Here we see that we have improved
the averaged performance across the three tasks by
approximately 2.5% (? < 0.01, McNemar?s test 2-
tailed). Note that with 3.5% the improvements are
particularly large for Task C.
The TempEval test set is relatively small (see Ta-
ble 3). Hence it is not clear how well our results
would generalize in practice. To overcome this is-
sue, we also evaluated the local and global model
using 10-fold cross validation on the training data
(TRAIN + DEV). The corresponding results can be
seen in Table 6. Note that the general picture re-
mains: performance for all tasks is improved, and
the averaged score is improved only slightly less
than for the TEST results. However, this time the
score increase for Task B is lower than before. We
410
Table 4: Evaluation Weights for Relaxed Scoring
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
B: BEFORE O: OVERLAP
A: AFTER B-O: BEFORE-OR-OVERLAP
O-A: OVERLAP-OR-AFTER V: VAGUE
Table 5: Results on TEST Set
Local Global
task strict relaxed strict relaxed
Task A 0.621 0.669 0.645 0.687
Task B 0.737 0.753 0.758 0.777
Task C 0.531 0.599 0.566 0.632
All 0.641 0.682 0.668 0.708
Table 6: Results with 10-fold Cross Validation
Local Global
task strict relaxed strict relaxed
Task A 0.613 0.645 0.662 0.691
Task B 0.789 0.810 0.799 0.819
Task C 0.533 0.608 0.552 0.623
All 0.667 0.707 0.689 0.727
see that this is compensated by much higher scores
for Task A and C. Again, the improvements for all
three tasks are statistically significant (? < 10?8,
McNemar?s test, 2-tailed).
To summarize, we have shown that by tightly
connecting tasks A, B and C, we can improve tem-
poral relation identification significantly. But are
we just improving a weak baseline, or can joint
modelling help to reach or improve the state-of-the-
art results? We will try to answer this question in
the next section.
6.2 Comparison to the State-of-the-art
In order to put our results into context, Table 7
shows them along those of other TempEval par-
ticipants. In the first row, TempEval Best gives
the best scores of TempEval for each task. Note
that all but the strict scores of Task C are achieved
by WVALI (Puscasu, 2007), a hybrid system that
combines machine learning and hand-coded rules.
In the second row we see the TempEval average
scores of all six participants in TempEval. The
third row shows the results of CU-TMP (Bethard
and Martin, 2007), an SVM-based system that
achieved the second highest scores in TempEval for
all three tasks. CU-TMP is of interest because it is
the best pure Machine-Learning-based approach so
far.
The scores of our local and global model come
in the fourth and fifth row, respectively. The last
row in the table shows task-adjusted scores. Here
we essentially designed and applied three global
MLNs, each one tailored and optimized for a dif-
ferent task. Note that the task-adjusted scores are
always about 1% higher than those of the single
global model.
Let us discuss the results of Table 7 in detail. We
see that for task A, our global model improves an
already strong local model to reach the best results
both for strict scores (with a 3% points margin) and
relaxed scores (with a 5% points margin).
For Task C we see a similar picture: here adding
global constraints helped to reach the best strict
scores, again by a wide margin. We also achieve
competitive relaxed scores which are in close range
to the TempEval best results.
Only for task B our results cannot reach the best
TempEval scores. While we perform slightly better
than the second-best system (CU-TMP), and hence
report the best scores among all pure Machine-
Learning based approaches, we cannot quite com-
pete with WVALI.
6.3 Discussion
Let us discuss some further characteristics and
advantages of our approach. First, notice that
global formulae not only improve strict but also re-
laxed scores for all tasks. This suggests that we
produce more ambiguous labels (such as BEFORE-
OR-OVERLAP) in cases where the local model has
been overconfident (and wrongly chose BEFORE
or OVERLAP), and hence make less ?fatal errors?.
Intuitively this makes sense: global consistency is
easier to achieve if our labels remain ambiguous.
For example, a solution that labels every relation
as VAGUE is globally consistent (but not very in-
formative).
Secondly, one could argue that our solution to
joint temporal relation identification is too com-
plicated. Instead of performing global inference,
one could simply arrange local classifiers for the
tasks into a pipeline. In fact, this has been done by
Bethard and Martin (2007): they first solve task B
and then use this information as features for Tasks
A and C.While they do report improvements (0.7%
411
Table 7: Comparison with Other Systems
Task A Task B Task C
strict relaxed strict relaxed strict relaxed
TempEval Best 0.62 0.64 0.80 0.81 0.55 0.64
TempEval Average 0.56 0.59 0.74 0.75 0.51 0.58
CU-TMP 0.61 0.63 0.75 0.76 0.54 0.58
Local Model 0.62 0.67 0.74 0.75 0.53 0.60
Global Model 0.65 0.69 0.76 0.78 0.57 0.63
Global Model (Task-Adjusted) (0.66) (0.70) (0.76) (0.79) (0.58) (0.64)
on Task A, and about 0.5% on Task C), generally
these improvements do not seem as significant as
ours. What is more, by design their approach can
not improve the first stage (Task B) of the pipeline.
On the same note, we also argue that our ap-
proach does not require more implementation ef-
forts than a pipeline. Essentially we only have to
provide features (in the form of formulae) to the
Markov Logic Engine, just as we have to provide
for a SVM or MaxEnt classifier.
Finally, it became more clear to us that there are
problems inherent to this task and dataset that we
cannot (or only partially) solve using global meth-
ods. First, there are inconsistencies in the training
data (as reflected by the low inter-annotator agree-
ment) that often mislead the learner?this prob-
lem applies to learning of local and global formu-
lae/features alike. Second, the training data is rela-
tively small. Obviously, this makes learning of re-
liable parameters more difficult, particularly when
data is as noisy as in our case. Third, the tempo-
ral relations in the TempEval dataset only directly
connect a small subset of events. This makes global
formulae less effective.14
7 Conclusion
In this paper we presented a novel approach to
temporal relation identification. Instead of using
local classifiers to predict temporal order in a pair-
wise fashion, our approach uses Markov Logic to
incorporate both local features and global transi-
tion rules between temporal relations.
We have focused on transition rules between
temporal relations of the three TempEval subtasks:
temporal ordering of events, of events and time ex-
pressions, and of events and the document creation
time. Our results have shown that global transition
rules lead to significantly higher accuracy for all
three tasks. Moreover, our global Markov Logic
14See (Chambers and Jurafsky, 2008) for a detailed discus-
sion of this problem, and a possible solution for it.
model achieves the highest scores reported so far
for two of three tasks, and very competitive results
for the remaining one.
While temporal transition rules can also be cap-
tured with an Integer Linear Programming ap-
proach (Chambers and Jurafsky, 2008), Markov
Logic has at least two advantages. First, handling
of ?rules of thumb? between less specific tempo-
ral relations (such as OVERLAP or VAGUE) is
straightforward?we simply let the Markov Logic
Engine learn weights for these rules. Second, there
is less engineering overhead for us to perform, be-
cause we do not need to generate ILPs for each doc-
ument.
However, potential for further improvements
through global approaches seems to be limited by
the sparseness and inconsistency of the data. To
overcome this problem, we are planning to use ex-
ternal or untagged data along with methods for un-
supervised learning in Markov Logic (Poon and
Domingos, 2008).
Furthermore, TempEval-2 15 is planned for 2010
and it has challenging temporal ordering tasks in
five languages. So, we would like to investigate the
utility of global formulae for multilingual tempo-
ral ordering. Here we expect that while lexical and
syntax-based features may be quite language de-
pendent, global transition rules should hold across
languages.
Acknowledgements
This work is partly supported by the Integrated
Database Project, Ministry of Education, Culture,
Sports, Science and Technology of Japan.
References
Steven Bethard and James H. Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Interna-
tional Workshop on SemEval-2007., pages 129?132.
15http://www.timeml.org/tempeval2/
412
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal reason-
ing. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence, pages 997?
1003.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves tem-
poral ordering. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: Temporal relation iden-
tification using dependency parsed tree. In Proceed-
ings of the 4th International Workshop on SemEval-
2007., pages 245?248.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press.
Daphne Koller, 1999. Probabilistic Relational Models,
pages 3?13. Springer, Berlin/Heidelberg, Germany.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 753?760, Morristown, NJ, USA.
Association for Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
650?659, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Georgiana Puscasu. 2007. Wvali: Temporal relation
identification by syntactico-semantic analysis. In
Proceedings of the 4th International Workshop on
SemEval-2007., pages 484?487.
James Pustejovsky, Jose Castano, Robert Ingria, Reser
Sauri, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. The timebank corpus. In Proceed-
ings of Corpus Linguistics 2003, pages 647?656.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. In Machine Learning.
Sebastian Riedel. 2008. Improving the accuracy and
efficiency of map inference for markov logic. In Pro-
ceedings of UAI 2008.
Ben Taskar, Abbeel Pieter, and Daphne Koller. 2002.
Discriminative probabilistic models for relational
data. In Proceedings of the 18th Annual Conference
on Uncertainty in Artificial Intelligence (UAI-02),
pages 485?492, San Francisco, CA. Morgan Kauf-
mann.
Marc Verhagen, Robert Gaizaukas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on SemEval-2007., pages 75?80.
413
Combining Segmenter and Chunker for Chinese Word Segmentation
Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology, Japan
{masayu-a,ling-g,xiaoji-w,matsu}@is.aist-nara.ac.jp
Abstract
Our proposed method is to use a Hidden
Markov Model-based word segmenter and a
Support Vector Machine-based chunker for
Chinese word segmentation. Firstly, input sen-
tences are analyzed by the Hidden Markov
Model-based word segmenter. The word seg-
menter produces n-best word candidates to-
gether with some class information and confi-
dence measures. Secondly, the extracted words
are broken into character units and each char-
acter is annotated with the possible word class
and the position in the word, which are then
used as the features for the chunker. Finally, the
Support Vector Machine-based chunker brings
character units together into words so as to de-
termine the word boundaries.
1 Methods
We participate in the closed test for all four sets of data
in Chinese Word Segmentation Bakeoff. Our method is
based on the following two steps:
1. The input sentence is segmented into a word se-
quence by Hidden Markov Model-based word seg-
menter. The segmenter assigns a word class with
a confidence measure for each word at the hidden
states. The model is trained by Baum-Welch algo-
rithm.
2. Each character in the sentence is annotated with the
word class tag and the position in the word. The
n-best word candidates derived from the word seg-
menter are also extracted as the features. A sup-
port vector machine-based chunker corrects the er-
rors made by the segmenter using the extracted fea-
tures.
We will describe each of these steps in more details.
1.1 Hidden Markov Model-based Word Segmenter
Our word segmenter is based on Hidden Markov Model
(HMM). We first decide the number of hidden states
(classes) and assume that the each word can belong to
all the classes with some probability. The problem is de-
fined as a search for the sequence of word classes C =
c1, . . . , cn given a word sequence W = w1, . . . , wn. The
target is to find W and C for a given input S that maxi-
mizes the following probability:
argmax
W,C
P (W |C)P (C)
We assume that the word probability P (W |C) is con-
strained only by its word class, and that the class prob-
ability P (C) is constrained only by the class of the pre-
ceding word. These probabilities are estimated by the
Baum-Welch algorithm using the training material (See
(Manning and Schu?tze., 1999)). The learning process is
based on the Baum-Welch algorithm and is the same as
the well-known use of HMM for part-of-speech tagging
problem, except that the number of states are arbitrarily
determined and the initial probabilities are randomly as-
signed in our model.
1.2 Correction by Support Vector Machine-based
Chunker
While the HMM-based word segmenter achieves good
accuracy for known words, it cannot identify compound
words and out-of-vocabulary words. Therefore, we in-
troduce a Support Vector Machine(below SVM)-based
chunker (Kudo and Matsumoto, 2001) to cover the er-
rors made by the segmenter. The SVM-based chunker
re-assigns new word boundaries to the output of the seg-
menter.
An SVM (Vapnik, 1998) is a binary classifier. Sup-
pose we have a set of training data for a binary class
problem: (x1, y1), . . . , (xN , yN ), where xi ? Rn is a
feature vector of the i th sample in the training data and
yi ? {+1,?1} is the label of the sample. The goal is to
find a decision function which accurately predicts y for
an unseen x. An SVM classifier gives a decision function
f(x) for an input vector x where
f(x) = sign(
?
zi?SV
?iyiK(x, zi) + b).
f(x) = +1 means that x is a positive member, and
f(x) = ?1 means that x is a negative member. The vec-
tors zi are called support vectors, which receive a non-
zero weight ?i. Support vectors and the parameters are
determined by solving a quadratic programming prob-
lem. K(x, z) is a kernel function which maps vectors
into a higher dimensional space. We use a polynomial
kernel of degree 2 given by K(x, z) = (1 + x ? z)2.
The SVM classifier determines the position tag for
each character. We introduce the word class tag as the
feature, which is generated by the word segmenter. Since
we perform chunking by character units, the feature used
by the classifier will be the information for the character
unit.
The training data for our SVM-based chunker is con-
structed from the output of the HMM-based word seg-
menter defined in the previous section. In the current
setting, the HMM produces all the possible tags (class
labels) for each of the word within a predefined probabil-
ity bound. All the words in the output are then segmented
into characters, and each of the characters is tagged with
pairs of a word class and a position tag. For example,
in the paired tag ?0-B?, ?0? is a class label of the word
which the character belongs to and ?B? indicates the char-
acter?s position in the word. The number of classes is
determined in advance of the HMM learning. The po-
sition tag consists of the following four tags (S/B/E/I):
S means a single-character word; B is the first charac-
ter in a multi-character word; E is the last character in a
multi-character word; I is the intermediate character in a
multi-character word longer than 2 characters. As shown
in Figure 1, we set the HMM-based word segmenter to
produce the classes of n-best word candidates to take into
account multiple possibility of word boundaries.
The correct word boundary can be defined by assigning
either of two kinds of tags to each of the characters. Look
at the rightmost column of Figure 1 named as ?Chunker
Outputs.? The label ?B? in this column shows that the
character is the first character of a correct word, and ?I?
shows that the character is the other part of a word. This
means that the preceding positions of ?B? tags are the
word boundaries.
Those two tags correspond to the two classes of the
SVM chunker: In the training (and test) phrase, we use
window size of two characters to the left and right direc-
tion to learn (and estimate) the class for a character. For
example, the shadowed parts in Figure 1 are used as the
Figure 1: The Extracted Features for the Chunker
features to learn (or estimate) the word boundary tag ?I?
for the character ? ?.
2 Model Validation
To find out the best setting of learning, we would like to
determine ?the number of word classes? and ?the depth of
n-best word candidates? by using some sort of confidence
measure. We perform validation experiments for these
two types of parameters by using the training material
provided.
2.1 Validation Tests for HMM-based Word
Segmenter
The first validation experiment is to determine ?the num-
ber of word classes? of the HMM. 80% of the material is
used for the HMM training, and the other 20% is used as
the validation set. We test two settings for the number of
classes ? 5 & 10. The results are shown in Table 1.
Table 1: Validation Results for HMM
Data # of classes Rec. Prec. F
AS 5 0.845 0.768 0.804
AS 10 0.900 0.857 0.878
CTB 5 0.909 0.844 0.875
CTB 10 0.912 0.848 0.879
HK 5 0.867 0.742 0.799
HK 10 0.867 0.741 0.799
PK 5 0.942 0.902 0.921
PK 10 0.944 0.905 0.924
In most cases, models perform slightly better with the
increasing of the number of classes. When the corpus
size is large like the Academia Sinica data, this tendency
becomes more significant.
Whereas it is known that the Baum Welch algorithm is
very sensitive to the initialization of the classes, we ran-
domly assigned the initial classes without making much
effort. There are two reasons: (1) Since the word seg-
menter outputs are used as the clues to the chunker in our
method, we only need some consistent class annotations.
(2) The initial classes did not affect on the word segmen-
tation accuracy in our pilot experiments.
2.2 Validation Tests for SVM-based Chunker
The second validation test is for the chunking model to
determine both ?the number of word classes? and ?the
depth of the n-best candidates?. 80% of the material used
for the HMM training, another 10% is used for the chunk-
ing model training and the last 10% is used for the val-
idation test. The results are shown in Table 2, 3 and 4.
Since the training of this model is time- and resource-
consuming, the Academia Sinica data being very large
could not get enough time to finish the validation test.
Table 2: Validation Results (CTB) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.957 0.930 0.943
5 2 0.957 0.931 0.944
5 3 0.957 0.930 0.943
5 4 0.957 0.930 0.943
10 1 0.956 0.929 0.943
10 2 0.957 0.928 0.942
10 3 0.956 0.929 0.942
10 4 0.955 0.928 0.941
Table 3: Validation Results (HK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.853 0.793 0.822
5 2 0.859 0.799 0.828
5 3 0.859 0.799 0.828
5 4 0.859 0.800 0.828
10 1 0.856 0.793 0.823
10 2 0.858 0.797 0.826
10 3 0.857 0.796 0.826
10 4 0.858 0.797 0.826
The results show that the chunker actually improves
the word segmentation accuracy compared with the out-
put of the HMM word segmenter for these three data sets.
The segmentation errors made by the word segmenter for
compound words and unknown words are corrected. The
Table 4: Validation Results (PK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.960 0.934 0.947
5 2 0.961 0.935 0.948
5 3 0.962 0.936 0.949
5 4 0.962 0.935 0.948
10 1 0.961 0.932 0.946
10 2 0.962 0.935 0.948
10 3 0.961 0.934 0.947
10 4 0.961 0.934 0.947
improvement in Chinese Treebank (CTB) data set is sig-
nificant, because the data set contains many compound
words.
There is no significant difference in the results between
the different depths of n-best answers. Still, we choose
the best model for the test materials among them. If we
need to have a faster analyzer, we should employ only the
best answer of the word segmentation.
For the HMM, the larger number of classes tends to
get better accuracy than smaller ones. However, for the
chunking model, the result is the other way round. The
model with the smaller number of classes gets slightly
better accuracy. So, there should be trade-off between
smaller and larger number of classes.
3 Final Models for Test Material
For the final models, 80% of the training material is used
for HMM training and 100% of the material is used for
the chunking model training. The parameters, namely
?the number of word classes? and ?the depth of n-best
word candidates?, are determined by the validation tests
described in Section 2. While there is no significant dif-
ference between the depths of n-best answers, we choose
the best model among them for the testing. The parame-
ters are shown in Table 7.
We cannot create the model using all the original
Academia Sinica data because of its large size. Therefore,
we use 80% of the data for HMM training (5 classes) and
only 10% for chunking model training (with only the best
candidates).
Table 7: The Models for the Test Material
? with respect to F-Measure in Our Validation Test
Data # of classes n-best F
AS 5 1 N/A
CTB 5 2 0.943
HK 5 4 0.828
PK 5 3 0.948
Table 5: Throughput Speeds (characters per second)
Data Word Seg. (# of words) Fea. Ext. (n-best) Chunker (# of SV) Total Speed
AS 57000 (462750) 7640 (Only Best) 279 (96452) 241
CTB 54400 (77324) 4040 (to 2nd Best) 894 (16736) 671
HK 38900 (93231) 3870 (to 4th Best) 649 (14904) 524
PK 57400 (215865) 6209 (to 3rd Best) 254 (49736) 200
Table 6: Results for the Test Materials
Data T. Rec. T. Prec. F OOV Rec. IV Rec. Ranking
AS 0.944 0.945 0.945 0.574 0.952 3rd/6
CTB 0.852 0.807 0.829 0.412 0.949 8th/10
HK 0.940 0.908 0.924 0.415 0.980 5th/6
PK 0.933 0.916 0.924 0.357 0.975 2nd/4
4 Throughput Speeds
As described, our system is based on three modules:
HMM-based word segmenter, Feature extractor and
SVM-based chunker. The word segmenter is composed
by ChaSen (written in C/C++) (Matsumoto et. al., 2003)
which is adopted for GB/Big5 encoding. The feature
extractor is written in Perl. The SVM-based chunker is
composed by YamCha (written in C++) (Kudo and Mat-
sumoto, 2001).
Table 5 shows the speeds 1 of the three modules indi-
vidually and of the total system. ?# of words? means the
size of the word segmenter lexicon. Note that, if a word
belongs to more than one class, we regard them as differ-
ent words in our definition. ?# of SV? means the number
of support vectors in the chunker. The total system speed
depends highly on that of the chunker. It is known that
the speed of SVM classifiers depends on the number of
support vectors and the number of features.
5 Conclusion
We presented our method for Chinese Word Segmenta-
tion Bakeoff in 2nd SIGHAN Workshop. The results for
the test materials are shown in Table 6. The proposed
method is purely corpus-based statistical/machine learn-
ing method. Although we did not incorporate any heuris-
tic rules (e.g. part-of-speeches, functional words and
concatenation for numbers) into the model, the method
achieved considerable accuracy for the word segmenta-
tion task.
Acknowledgments
We thank Mr. Taku Kudo of NAIST for his development
of the SVM-based chunker YamCha.
1The throughput speeds are measured on a machine: In-
tel(R) Xeon(TM) CPU 2.80GHz ? 2, Memory 4GB, RedHat
Linux 9.
References
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of NAACL 2001, pages
192?199.
C. D. Manning and H. Schu?tze. 1999. Foundation of
Statistical Natural Language Processing. Chapter 9.
Markov Models, pages 317?340.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, K.
Takaoka and M. Asahara 2003. Morphological Ana-
lyzer ChaSen-2.3.0 Users Manual Tech. Report. Nara
Institute of Science and Technology, Japan.
L. A. Ramshaw and M. P. Marcus. 1995 Text chunking
using transformation-bases learning In Proc. of the 3rd
Workshop on Very Large Corpora, pages 83?94.
V. N. Vapnik. 1998. Statistical Learning Theory. A
Wiley-Interscience Publication.
 
	 
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 191?195, New York City, June 2006. c?2006 Association for Computational Linguistics
Multi-lingual Dependency Parsing at NAIST 
Yuchang CHENG, Masayuki ASAHARA and Yuji MATSUMOTO 
Nara Institute of Science and Technology  
8916-5 Takayama, Ikoma, Nara 630-0192, Japan  
{yuchan-c, masayu-a, matsu}@is.naist.jp 
 
Abstract 
In this paper, we present a framework for 
multi-lingual dependency parsing. Our 
bottom-up deterministic parser adopts 
Nivre?s algorithm (Nivre, 2004) with a 
preprocessor. Support Vector Machines 
(SVMs) are utilized to determine the word 
dependency attachments. Then, a maxi-
mum entropy method (MaxEnt) is used 
for determining the label of the depend-
ency relation. To improve the perform-
ance of the parser, we construct a tagger 
based on SVMs to find neighboring at-
tachment as a preprocessor. Experimental 
evaluation shows that the proposed exten-
sion improves the parsing accuracy of our 
base parser in 9 languages. (Haji? et al, 
2004; Simov et al, 2005; Simov and 
Osenova, 2003; Chen   et al, 2003; B?h-
mov? et al, 2003; Kromann, 2003;    van 
der Beek et al, 2002; Brants et al, 
2002;   Kawata and Bartels, 2000; Afonso 
et al, 2002;   D?eroski et al, 2006; Civit 
and Mart?, 2002; Nilsson   et al, 2005; 
Oflazer et al, 2003; Atalay et al, 2003). 
1 Introduction 
The presented dependency parser is based on our 
preceding work (Cheng, 2005a) for Chinese. The 
parser is a bottom-up deterministic dependency 
parser based on the algorithm proposed by (Nivre, 
2004). A dependency attachment matrix is con-
structed, in which each element corresponds to a 
pair of tokens. Each dependency attachment is in-
crementally constructed, with no crossing con-
straint. In the parser, SVMs (Vapnik, 1998) 
deterministically estimate whether a pair of words 
has either of four relations: right, left, shift and 
reduce. While dependency attachment is estimated 
by SVMs, we use a MaxEnt (Ratnaparkhi, 1999) 
based tagger with the output of the parser to esti-
mate the label of dependency relations. This tagger 
uses the same features as for the word dependency 
analysis. 
In our preceding work (Cheng, 2005a), we not 
only adopted the Nivre algorithm with SVMs, but 
also tried some preprocessing methods. We inves-
tigated several preprocessing methods on a Chi-
nese Treebank. In this shared task (Buchholz et. al, 
2006), we also investigate which preprocessing 
method is effective on other languages. We found 
that only the method that uses a tagger to extract 
the word dependency attachment between two 
neighboring words works effectively in most of the 
languages. 
2 System Description 
The main part of our dependency parser is based 
on Nivre?s algorithm (Nivre, 2004), in which the 
dependency relations are constructed by a bottom-
up deterministic schema. While Nivre?s method 
uses memory-based learning to estimate the de-
pendency attachment and the label, we use SVMs 
to estimate the attachment and MaxEnt to estimate 
Fig. 1 The architecture of our parser 
(i)Preprocessor (neighboring 
relation tagger)
(ii)Get contextual features
(iii)Estimate dependency
attachment by SVM
(iv)Tag label by MaxEnt
Construct Subtree
No more construction
Dependency tree
False
True
Left or Right attachment
None
Input sentence (word tokens)
191
Fig. 2. The features for dependency analysis 
BOS
-
BOS
BOS
-
??
-
VC
V
-
??
-
Nb
N
-
?
-
DE
DE
-
??
-
VH
V
-
??
-
Nac
N
-
???
-
Na
N
-
S I
position t-1position t-2
The child of the position t-1
position n position n+1position n+2position t
A feature: the distance between the position t and n
FORM 
LEMMA 
CPOSTAG 
POSTAG 
FEATS 
Key: The features for machine 
learning of each token
the label. The architecture of the parser consists of 
four major procedures and as in Fig.1:  
(i) Decide the neighboring dependency at-
tachment between all adjacent words in the 
input sentence by SVM-based tagger (as a 
preprocessing) 
(ii) Extract the surrounding features for the 
focused pair of nodes. 
(iii) Estimate the dependency attachment op-
eration of the focused pair of nodes by 
SVMs. 
(iv) If there is a left or right attachment, esti-
mate the label of dependency relation by 
MaxEnt. 
We will explain the main procedures (steps (ii)-
(iv)) in sections 2.1 and 2.2, and the preprocessing 
in section 2.3. 
2.1   Word dependency analysis 
In the algorithm, the state of the parser is repre-
sented by a triple AIS ,, . S and I are stacks, S 
keeps the words being in consideration, and I 
keeps the words to be processed. A is a list of de-
pendency attachments decided in the algorithm. 
Given an input word sequence W, the parser is ini-
tialized by the triple ?,,Wnil . The parser esti-
mates the dependency attachment between two 
words (the top elements of stacks S and I). The 
algorithm iterates until the list I becomes empty. 
There are four possible operations (Right, Left, 
Shift and Reduce) for the configuration at hand.  
Right or Left: If there is a dependency relation 
that the word t or n attaches to word n or t, add the 
new dependency relation ( )nt ?  or ( )tn ? into A, 
remove t or n from S or I. 
If there is no dependency relation between n and 
t, check the following conditions. 
Reduce: If there is no word 'n  ( In ?' ) which may 
depend on t, and t has a parent on its left side, the 
parser removes t from the stack S. 
Shift: If there is no dependency between n and t, 
and the triple does not satisfy the conditions for 
Reduce, then push n onto the stack S. 
In this work, we adopt SVMs for estimating the 
word dependency attachments. SVMs are binary 
classifiers based on the maximal margin strategy.  
We use the polynomial kernel: dK )1()( zxzx, ?+=  
with d =2. The performance of SVMs is better than 
that of the maximum entropy method in our pre-
ceding work for Chinese dependency analysis 
(Cheng, 2005b). This is because that SVMs can 
combine features automatically (using the polyno-
mial kernel), whereas the maximum entropy 
method cannot. To extend binary classifiers to 
multi-class classifiers, we use the pair-wise method, 
in which we make 2Cn
1  binary classifiers between 
all pairs of the classes (Kre?el, 1998). We use 
Libsvm (Lin et al, 2001) in our experiments. 
In our method, the parser considers the depend-
ency attachment of two nodes (n,t). The features of 
a node are the word itself, the POS-tag and the in-
formation of its child node(s). The context features 
are 2 preceding nodes of node t (and t itself), 2 suc-
ceeding nodes of node n (and n itself), and their 
child nodes. The distance between nodes n and t is 
also used as a feature. The features are shown in 
Fig.2. 
2.2   Label tagging 
We adopt MaxEnt to estimate the label of depend-
ency relations. We have tried to use linear-chain 
conditional random fields (CRFs) for estimating 
the labels after the dependency relation analysis. 
This means that the parser first analyzes the word 
dependency (head-modifier relation) of the input 
sentence, then the CRFs model analyzes the most 
suitable label set with the basic information of in-
put sentence (FORM, LEMMA, POSTAG??etc) 
and the head information (FORM and POSTAG) 
of each word. However, as the number of possible 
labels in some languages is large, training a CRF 
model with these corpora (we use CRF++ (Kudo, 
2005)) cost huge memory and time. 
Instead, we combine the maximum entropy 
method in the word dependency analysis to tag the 
label of dependency relation. As shown in Fig. 1, 
the parser first gets the contextual features to esti-
mate the word dependency. If the parsing operation 
                                                          
1  To estimate the current operation (Left, Right, Shift and 
Reduce) by SVMs, we need to build 6 classifiers(Left-Right, 
Left-Shift, Left-Reduce, Right-Shift, Right-Reduce and Shift-
Reduce).  
192
is ?Left? or ?Right?, the parser then use MaxEnt 
with the same features to tag the label of relation. 
This strategy can tag the label according to the cur-
rent states of the focused word pair. We divide the 
training instances according to the CPOSTAG of 
the focused word n, so that a classifier is con-
structed for each of distinct POS-tag of the word n. 
2.3 Preprocessing 
2.3.1   Preceding work 
In our preceding work (Cheng, 2005a), we dis-
cussed three problems of our basic methods (adopt 
Nivre?s algorithm with SVMs) and proposed three 
preprocessing methods to resolve these problems. 
The methods include: (1) using global features and 
a two-steps process to resolve the ambiguity be-
tween the parsing operations ?Shift? and ?Reduce?. 
(2) using a root node finder and dividing the sen-
tence at the root node to make use of the top-down 
information. (3) extracting the prepositional phrase 
(PP) to resolve the problem of identifying the 
boundary of PP. 
We incorporated Nivre?s method with these 
preprocessing methods for Chinese dependency 
analysis with Penn Chinese Treebank and Sinica 
Treebank (Chen   et al, 2003). This was effective 
because of the properties of Chinese: First, there is 
no multi-root in Chinese Treebank. Second, the 
boundary of prepositional phrases is ambiguous. 
We found that these methods do not always im-
prove the accuracy of all the languages in the 
shared task.  
We have tried the method (1) in some lan-
guages to see if there is any improvement in the 
parser. We attempted to use global features and 
two-step analysis to resolve the ambiguity of the 
operations. In Chinese (Chen   et al, 2003) and 
Danish (Kromann, 2003), this method can improve 
the parser performance. However, in other lan-
guages, such as Arabic (Haji? et al, 2004), this 
method decreased the performance. The reason is 
that the sentence in some languages is too long to 
use global features. In our preceding work, the 
global features include the information of all the 
un-analyzed words. However, for analyzing long 
sentences, the global features usually include some 
useless information and will confuse the two-step 
process. Therefore, we do not use this method in 
this shared task. 
In the method (2), we construct an SVM-based 
root node finder to identify the root node and di-
vided the sentence at the root node in the Chinese 
Treebank. This method is based on the properties 
of dependency structures ?One and only one ele-
ment is independent? and ?An element cannot have 
modifiers lying on the other side of its own head?. 
However, there are some languages that include 
multi-root sentences, such as Arabic, Czech, and 
Spanish (Civit and Mart?, 2002), and it is difficult 
to divide the sentence at the roots. In multi-root 
sentences, deciding the head of the words between 
roots is difficult. Therefore, we do not use the 
method (2) in the share task.  
The method (3) ?namely PP chunker? can iden-
tify the boundary of PP in Chinese and resolve the 
ambiguity of PP boundary, but we cannot guaran-
tee that to identify the boundary of PP can improve 
the parser in other languages. Even we do not un-
derstand construction of PP in all languages. 
Therefore, for the robustness in analyzing different 
languages, we do not use this method. 
2.3.2   Neighboring dependency attachment 
tagger 
In the bottom-up dependency parsing approach, the 
features and the strategies for parsing in early stage 
(the dependency between adjacent2 words) is dif-
ferent from parsing in upper stage (the dependency 
between phrases). Parsing in upper stage needs the 
information at the phrases not at the words alone. 
The features and the strategies for parsing in early 
and upper stages should be separated into distinct. 
Therefore, we divide the neighboring dependency 
attachment (for early stage) and normal depend-
ency attachment (for upper stage), and set the 
neighboring dependency attachment tagger as a  
preprocessor. 
When the parser analyzes an input sentence, it 
extracts the neighboring dependency attachments 
first, then analyzes the sentence as described be-
fore. The results show that tagging the neighboring 
dependency word-pairs can improve 9 languages 
out of 12 scoring languages, although in some lan-
guages it degrades the performance a little. Poten-
tially, there may be a number of ways for 
decomposing the parsing process, and the current 
method is just the simplest decomposition of the 
process. The best method of decomposition or dy-
namic changing of parsing models should be inves-
tigated as the future research. 
                                                          
2 We extract all words that depend on the adjacent word (right 
or left). 
193
3 Experiment 
3.1 Experimental setting 
Our system consists of three parts; first, the SVM-
based tagger extracts the neighboring attachment 
relations of the input sentence. Second, the parser 
analyzes further dependency attachments. If a new 
dependency attachment is generated, the MaxEnt 
based tagger estimates the label of the relation. The 
three parts of our parser are trained on the avail-
able data of the languages. 
In our experiment, we used the full information 
of each token (FORM, LEMMA, CPOSTAG, 
POSTAG, FEATS) when we train and test the 
model. Fig. 2 describes the features of each token. 
Some languages do not include all columns; such 
that the Chinese data does not include LEMMA 
and FEATURES, these empty columns are shown 
by the symbol ?-? in Fig. 2. The features for the 
neighboring dependency tagging are the informa-
tion of the focused word, two preceding words and 
two succeeding words. Fig. 2 shows the window 
size of our features for estimating the word de-
pendency in the main procedures. These features 
include the focused words (n, t), two preceding 
words and two succeeding words and their children. 
The features for estimating the relation label are 
the same as the features used for word dependency 
analysis. For example, if the machine learner esti-
mates the operation of this situation as ?Left? or 
?Right? by using the features in Fig. 2, the parser 
uses the same features in Fig. 2 and the depend-
ency relation to estimate the label of this relation.  
For training the models efficiently, we divided 
the training instances of all languages at the 
CPOSTAG of the focused word n in Fig .2. In our 
preceding work, we found this procedure can get 
better performance than training with all the in-
stances at once. However, only the instances in 
Czech are divided at the CPOSTAG of the focused 
word-pair t-n3. The performance of this procedure 
is worse than using the CPOSTAG of the focused 
word n, because the training instances of each 
CPOSTAG-pair will become scarce. However, the 
data size of Czech is much larger than other lan-
guages; we couldn?t finish the training of Czech 
using the CPOSTAG of the focused word n, before 
the deadline for submitting. Therefore we used this 
procedure only for the experiment of Czech. 
                                                          
3 For example, we have 15 SVM-models for Arabic according 
to the CPOSTAG of Arabic (A, C, D, F, G?etc.). However, 
we have 139 SVM-models for Czech according the 
CPOSTAG pair of focused words (A-A, A-C, A-D?etc.) 
All our experiments were run on a Linux ma-
chine with XEON 2.4GHz and 4.0GB memory. 
The program is implemented in JAVA. 
3.2   Results 
Table 1 shows the results of our parser. We do not 
take into consideration the problem of cross rela-
tion. Although these cross relations are few in 
training data, they would make our performance 
worse in some languages. We expect that this is 
one reason that the result of Dutch is not good. The 
average length of sentences and the size of training 
data may have affected the performance of our 
parser. Sentences of Arabic are longer and training 
data size of Arabic is smaller than other languages; 
therefore our parser is worse in Arabic. Similarly, 
our result in Turkish is also not good because the 
data size is small. 
     We compare the result of Chinese with our pre-
ceding work. The score of this shared task is better 
than our preceding work. It is expected that we 
selected the FORM and CPOSTAG of each nodes 
as features in the preceding work. However, the 
POSTAG is also a useful feature for Chinese, and 
we grouped the original POS tags of Sinica Tree-
bank from 303 to 54 in our preceding work. The 
number of CPOSTAG(54) in our preceding work 
is more than the number of CPOSTAG(22) in this 
shared task, the training data of each CPOSTAG in 
our preceding work is smaller than in this work.  
Therefore the performance of our preceding work 
in Sinica Treebank is worse than this task. 
     The last column of the Table 1 shows the unla-
beled scores of our parser without the preprocess-
ing. Because our parser estimates the label after the 
dependency relation is generated. We only con-
sider whether the preprocessing can improve the 
unlabeled scores. Although the preprocessing can 
not improve some languages (such as Chinese, 
Spanish and Swedish), the average score shows 
that using preprocessing is better than parsing 
without preprocessing. 
     Comparing the gold standard data and the sys-
tem output of Chinese, we find the CPOSTAG 
with lowest accuracy is ?P (preposition)?, the accu-
racy that both dependency and head are correct is 
71%. As we described in our preceding work and 
Section 2.3, we found that boundaries of preposi-
tional phrases are ambiguous for Chinese. The bot-
tom-up algorithm usually wrongly parses the 
prepositional phrase short. The parser does not  
capture the correct information of the children of 
the preposition. According to the results, this prob-
lem does not cause the accuracy of head of  
194
CPOSTAG ?P? decrease. Actually, the head accu-
racy of ?P? is better than the CPOSTAG ?C? or 
?V?. However, the dep. accuracy of ?P? is worse. 
We should consider the properties of prepositions 
in Chinese to resolve this question. In Chinese, 
prepositions are derived from verbs; therefore 
some prepositions can be used as a verb. Naturally, 
the dependency relation of a preposition is differ-
ent from that of a verb. Important information for 
distinguishing whether the preposition is a verb or 
a preposition is the information of the children of 
the preposition. The real POS tag of a preposition 
which includes few children is usually a verb; on 
the other hand, the real POS tag of a preposition is 
usually a preposition.  
If our parser considers the preposition which 
leads a short phrase, the parser will estimate the 
relation of the preposition as a verb. At the same 
time, if the boundary of prepositional phrase is 
analyzed incorrectly, other succeeding words will 
be wrongly analyzed, too.  
Error analysis of Japanese data (Kawata and  
Bartels, 2000) shows that CNJ (Conjunction) is a 
difficult POS tag. The parser does not have any 
module to detect coordinate structures. (Kurohashi, 
1995) proposed a method in which coordinate 
structure with punctuation is detected by a coeffi-
cient of similarity. Similar framework is necessary 
for solving the problem. 
 Another characteristic error in Japanese is seen 
at adnominal dependency attachment for a com-
pound noun. In such dependency relations, adjec-
tives and nouns with "no" (genitive marker) can be 
a dependent and compound nouns which consist of 
more than one consecutive nouns can be a head. 
The constituent of compound nouns have same 
POSTAG, CPOSTAG and FEATS. So, the ma-
chine learner has to disambiguate the dependency 
attachment with sparce feature LEMMA and 
FORM. Compound noun analysis by semantic fea-
ture is necessary for addressing the issue. 
4 Conclusion 
This paper reported on multi-lingual dependency 
parsing on combining SVMs and MaxEnt. The 
system uses SVMs for word dependency attach-
ment analysis and MaxEnt for the label tagging 
when the new dependency attachment is generated. 
We discussed some preprocessing methods that are 
useful in our preceding work for Chinese depend-
ency analysis, but these methods, except one, can-
not be used in multi-lingual dependency parsing. 
Only using the SVM-based tagger to extract the 
neighbor relation could improve many languages 
in our experiment, therefore we use the tagger in 
the parser as its preprocessing. 
References 
S. Buchholz, E. Marsi, A. Dubey and Y. Krymolowski. 2006. 
CoNLL-X: Shared Task on Multilingual Dependency Pars-
ing, CoNLL 2006. 
Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto. 
2005a. Chinese Deterministic Dependency Parser: Exam-
ining Effects of Global Features and Root Node Finder, 
Fourth SIGHAN Workshop, pp.17-24. 
Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto. 
2005b. Machine Learning-based Dependency Parser for 
Chinese, the International Conference on Chinese Comput-
ing, pp.66-73. 
Ulrich. H.-G. Kre?el, 1998. Pairwise classification and sup-
port vector machines. In Advances in Kernel Methods, pp. 
255-268. The MIT Press. 
Taku Kudo. CRF++: Yet Another CRF toolkit, 
http://www.chasen.org/~taku/software/CRF++/. 
Sadao Kurohashi. 1995. Analyzing Coordinate Structures 
Including Punctuation in English, In IWPT-95, pp. 136-147. 
Chih Jen Lin, 2001. A practical guide to support vector classi-
fication, http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 
Joakim Nivre, 2004. Incrementality in Deterministic Depend-
ency Parsing, In Incremental Parsing: Bringing Engineer-
ing and Cognition Together. Workshop at ACL-2004, pp. 
50-57. 
Adwait Ratnaparkhi, 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine Learning, 
34(1-3):151-175. 
Vladimir N. Vapnik, 1998. Statistical Learning Theory.  A 
Wiley-Interscience Publication. 
Language: LAS: UAS: LAcc. UAS with out preprocessing:
Arabic 65.19 77.74 79.02 76.74 
Chinese 84.27 89.46 86.42 90.03 
Czech 76.24 83.4 83.52 82.88 
Danish 81.72 88.64 86.11 88.45 
Dutch 71.77 75.49 75.83 74.97 
German 84.11 87.66 90.67 87.53 
Japanese 89.91 93.12 92.40 92.99 
Portugese 85.07 90.3 88.00 90.21 
Slovene 71.42 81.14 80.96 80.43 
Spanish 80.46 85.15 88.90 85.19 
Swedish 81.08 88.57 83.99 88.83 
Turkish 61.22 74.49 73.91 74.3 
AV: 77.7 84.6 84.1 84.38 
SD: 8.67 6.15 5.78 6.42 
Bulgarian 86.34 91.3 89.27 91.44 
Table 1: Results 
195
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 245?248,
Prague, June 2007. c?2007 Association for Computational Linguistics
NAIST.Japan: Temporal Relation Identification Using Dependency Parsed
Tree
Yuchang Cheng, Masayuki Asahara and Yuji Matsumoto
Graduate School of Informatino Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
 yuchan-c, masayu-a, matsu@is.naist.jp
Abstract
In this paper, we attempt to use a sequence
labeling model with features from depen-
dency parsed tree for temporal relation iden-
tification. In the sequence labeling model,
the relations of contextual pairs can be used
as features for relation identification of the
current pair. Head-modifier relations be-
tween pairs of words within one sentence
can be also used as the features. In our pre-
liminary experiments, these features are ef-
fective for the temporal relation identifica-
tion tasks.
1 Overview of our system
This paper presents a temporal relation identifier by
the team NAIST.Japan. Our identifier has two char-
actaristics: sequence labeling model and use of de-
pendency parsed tree.
Firstly, we treated each problem a sequence la-
beling problem, such that event/time pairs were or-
dered by the position of the events and times in the
document. This idea is for task B and C. In task
B, the neighbouring relations between an EVENT
and DCT-TIMEX3 tend to interact. In task C, when
EVENT-a, EVENT-b, and EVENT-c are linearly or-
dered, the relation between EVENT-a and EVENT-
b tends to affect the one between EVENT-b and
EVENT-c.
Secondly, we introduced dependency features
where each word was annotated with a label indi-
cating its tree position to the event and the time, e.g.
?descendant? of the event and ?ancestor? of the time.
The dependency features are introduced for our ma-
chine learning-based relation identifier. In task A,
we need to label several different event-time pairs
within the same sentence. We can use information
from TIMEX3, which is a descendent of the target
EVENT in the dependency tree.
Section 2 shows how to use a sequence labeling
model for the task. Section 3 shows how to use
the dependency parsed tree for the model. Section
4 presents the results and discussions.
2 Temporal Relation Identification by
Sequence Labeling
Our approach to identify temporal relation is based
on a sequence labeling model. The target pairs are
linearly ordered in the texts.
Sequence labeling model can be defined as a
method to estimate an optimal label sequence    
  
 
  

      
 
 over an observed sequence   
 
 
 

     
 
. We consider, -parameterized
function
   	
 

      	
 
   
Here,  denotes all possible label combinations over
 ;    denotes a feature expression over   .
Introducing a kernel function:
   	 	       	 	 
we have a dual representation:

     

 
 





 
 

   
245
given a training data set


 
 
 
 
     


 
 

. We use
HMM SVM (Altun et al, 2003) as the sequence
labeling model, in which the training is performed
to maximize a margin


  
 


 
 

  
  
  

 


  
The sequence labeling approach is natural for task
B and C. In task B, if a document is about affairs in
the past, the relations between events and a docu-
ment creation time tend to be ?BEFORE?. All rela-
tions in task B depend on each other. In task C, if a
relation between the preceding event and the current
one is ?AFTER?, the current one is in the past. The
information helps to determine the relation between
the current and succeeding one. Whereas we have
reasonable explanation to introduce sequence label-
ing for task B and C, we cannot for task A. However,
in our preliminary experiments with trial data, the
sequence labeling model outperformed point-wise
models for task A. Thus, we introduce the sequence
labeling model for task A.
Now, we present the sequence labeling approach
for each task in detail by figure 1, 2 and 3. The
left parts of figures are the graphical models of the
sequence labeling. The right parts are the tagged
corpus:  S and  S are sentence boundaries; a
EVENT-nn denotes an EVENT; a TIME-nn de-
notes a TIMEX3; a TIME-DCT in figure 2 de-
notes a TIMEX3 with document creation time; a
boxed EVENT-nn in figure 3 denotes a matrix verb
EVENT.
For task A (figure 1),  is a sequence of pairs be-
tween an EVENT and a TIMEX3 within the same
sentence.   is a sequence of corresponding relations.
Event-time pairs are ordered first by sentence posi-
tion, then by event position and finally by time posi-
tion. For task B (figure 2),  is a sequence of pairs
between an EVENT and a DCT-TIMEX3.   is a se-
quence of corresponding relations. All pairs in the
same text are linearly ordered and connected. For
task C (figure 3),  is a sequence of pairs between
two matrix verb EVENTs in the neighboring sen-
tences.   is a sequence of corresponding relations.
All pairs in the same text are linearly ordered and
connected, even if the two relations are not in the
adjacent sentences.
xy
EVENT_01?TIME_01 ......................
..............TIME_02..........................
.................EVENT_02????..
......TIME_03 .........EVENT_03.......
EVENT_01?TIME_01
<s>
.
.
.
<s>
</s>
</s>
.
.
.
Before
Before
After
Overlap
Overlap
EVENT_01?TIME_02
EVENT_02?TIME_01
EVENT_02?TIME_02
EVENT_03?TIME_03
Figure 1: Sequence Labeling Model for Task A
xy
EVENT_01..................................
.................EVENT_02
.........EVENT_03...........
EVENT_01?TIME_DCT
EVENT_02?TIME_DCT
EVENT_03?TIME_DCT
EVENT_04?TIME_DCT
EVENT_05?TIME_DCT
<s>
.
.
.
<s>
</s>
</s>
Before
Before
Overlap
Before
Before
TIME_DCT
.................EVENT_04.................
.................EVENT_05
<s>
</s>
Figure 2: Sequence Labeling Model for Task B
xy EVENT_01 ..................................
................. EVENT_02
......... EVENT_03 ...........
EVENT_01?EVENT_03
EVENT_03?EVENT_04
EVENT_04?EVENT_06
<s>
<s>
</s>
</s>Before
After
Overlap
................. EVENT_04 ............... EVENT_05
<s>
</s>
......... EVENT_06 ...........
<s>
</s>
Figure 3: Sequence Labeling Model for Task C
3 Features from Dependency Parsed Tree
A dependency relation is a head-modifier relation on
a syntactic tree. Figure 4 shows an example de-
pendency parsed tree of the following sentence ?
?The warrants may be exercised until 90 days after
their issue date?. We parsed the TimeEval data us-
ing MSTParser v0.2 (McDonald and Pereira, 2006),
which is trained with all Penn Treebank (Marcus et
al., 1993) without dependency label.
We introduce tree position labels between an tar-
get node and another node on the dependency parsed
tree: ANC (ancestor), DES (descendant), SIB (sib-
ling), and TARGET (target word). Figure 5 shows
the labels, in which the box with double lines is the
target node. The tree position between the target
EVENT and a word in the target TIMEX3 is used
as a feature for our machine learning-based relation
identifier.
We also use the words in the sentence including
the target entities as features. Each word is anno-
246
The
warrants
may
be
exercised
until
90
days
after
their
issue
date
Figure 4: An example of dependency parsed tree
ANC
ANC
TARGET
DES
ANC
SIB
DESDES
SIB
ANC
Figure 5: Tree position labels
The
warrants
may
be
exercised
until
90
days
after
their
issue
date
ANC
ANC
ANC
ANC
DES
DES
DES
DES
DES
DES
DES
TARGET The
warrants
may
be
exercised
until
90
days
after
their
issue
date
ANC
ANC
ANC
ANC
ANC
ANC
TARGET
TARGET
SIB
SIB
SIB
ANC The
warrants
may
be
exercised
until
90
days
after
their
issue
date
ANC/ANC
ANC/ANC
ANC/ANC
ANC/ANC
DES/ANC
DES/ANC
DES/TARGET
DES/TARGET
DES/SIB
DES/SIB
DES/SIB
TARGET/ANC
TARGET node: ?exercised? TARGET nodes: ?90? and ?days? TARGET-A node: ?exercised?
TARGET-B nodes: ?90? and ?days?
(1) EVENT-based (2) TIMEX3-based (3) JOINT
Figure 6: Tree position labels on the example dependency parsed tree
tated with (1) its tree position to the EVENT, (2)
its tree position to the TIMEX3, and (3) the com-
bination of the labels from (1) and (2). Fig. 6
shows the labels of tree positions. The left picture
shows (1) EVENT-based labels of the tree position
with the target EVENT ?exercised?. The center pic-
ture shows (2) TIMEX3-based ones with the target
TIMEX3 ?90 days?. The right picture shows (3)
JOINT ones which are combinations of the relation
label with the EVENT and with the TIMEX3. We
perform feature selection on the words in the cur-
rent sentence according to the tree position labels.
Note that, when MSTparser outputs more than one
trees for a sentence, we introduce a meta-root node
to bundle the ones in a tree.
4 Results and Discussions
We use HMM SVM 1as a sequence labeling model
with features in Table 1, 2 and 3 for task A, B and
C, respectively. The attributes value in TIMEX3
1http://svmlight.joachims.org/svm_
struct.html
is encoded as the relation with DCT-TIMEX3:
BEFORE, OVERLAP, AFTER, VAGUE. In
task A, only words in the current sentence with
JOINT relation labels ?TARGET/? or ?ANC/? or
?*/DES?2 were used. In task C, attributes in the
TIMEX3 are annotated with the flag whether the
TIMEX3 entity is the highest (namely the nearest
to the root node) in the tree. Some adverbs and con-
junctions in the succeeding sentence help to deter-
mine the adjacent two relations. Thus, we introduce
all words in the succeeding sentence for Task A and
B. These features are determined by our preliminary
experiments with the trial data .
Table 4 is our results on the test data. Whereas,
our system is average rank in task A and B, it is
worst mark in task C. The features from dependency
parsed trees are effective for task A and B. However,
these are not for task C.
Now, we focus on what went wrong instead of
what went right in our preliminary experiments in
trial data. We tried point-wise methods with other
2
? ? stands for wild cards.
247
Table 1: Features for Task A
all attributes in the target EVENT
all attributes in the target TIMEX3
the attributes value is encoded as the relation with
DCT-TIMEX3
all words in the current sentence with TIMEX3-based
label (2) of tree position
words in the current sentence with JOINT label (3) of
tree position
 only relation label with ?TARGET/ ? or ?ANC/ ? or
?*/DES? (  stands for wild cards)
label (1) of tree position from the EVENT to the
TIMEX3
all words in the succeeding sentence
Table 2: Features for Task B
all attributes in the target EVENT
all attributes in the target TIMEX3 of in the current sen-
tence with EVENT-based label (1) of tree position
all attributes in the target TIMEX3 of in the preceding
and succeeding sentence
all words in the current sentence with EVENT-based la-
bel (1) of tree position
all words in the succeeding sentence
Table 3: Features for Task C
all attributes in the target two EVENTs (EVENT-1 and
EVENT-2)
all attributes in the TIMEX3 in the sentence including
EVENT-1 with the label (1) of tree position to EVENT-
1
all attributes in the TIMEX3 in the sentence including
EVENT-2 with the label (1) of tree position to EVENT-
2
all words in the sentence including EVENT-1 with the
label (1) of tree position to EVENT-1
all words in the sentence including EVENT-2 with the
label (1) of tree position to EVENT-2
machine learners such as maximum entropy and
multi-class support vector machines. However, se-
quence labeling method with HMM SVM outper-
formed other point-wise methods in the trial data.
We have dependency parsed trees of the sen-
tences. Naturally, it would be effective to intro-
duce point-wise tree-based classifiers such as Tree
Kernels in SVM (Collins and Duffy, 2002; Vish-
wanathan and Smola, 2002) and boosting for clas-
sification of trees (Kudo and Matsumoto, 2004). We
tried a boosting learner 3which enables us to perform
subtree feature selection for the tasks. However, the
boosting learner selected only one-node subtrees as
useful features. Thus, we perform simple vector-
based feature engineering on HMM SVM.
3http://chasen.org/?taku/software/bact/
Table 4: Results
Task P R F Rank
Task A (strict) 0.61 0.61 0.61 2/6
Task A (relaxed) 0.63 0.63 0.63 2/6
Task B (strict) 0.75 0.75 0.75 2/6
Task B (relaxed) 0.76 0.76 0.76 2/6
Task C (strict) 0.49 0.49 0.49 5/6
Task C (relaxed) 0.56 0.56 0.56 6/6
We believe that it is necessary for solving task C
to incorporate knowledge of verb-verb relation. We
also tried to use features in verb ontology such as
VERBOCEAN (Chklovsky and Pantel, 2004) which
is used in (Mani et al, 2006). It did not improved
performance in our preliminary experiments with
trial data.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den markov support vector machines. In Proc. of
ICML-2003.
T. Chklovsky and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantiv verb relations. In
Proc. of EMNLP-2004.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. of ACL-2002.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proc. of
EMNLP-2004.
I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and
J. Pustejovsky. 2006. Machine learning of temporal
relations. In Proc. of ACL-2006.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. 19(2):313?330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL-2006.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
and J. Pustejovsky. 2007. Semeval-2007 task 15:
Tempeval temporal relation identification. In Proc. of
SemEval-2007.
S. V. N. Vishwanathan and A. J. Smola. 2002. Fast ker-
nels on strings and trees. In Proc. of NIPS-2002.
248
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114?119,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage
Approximate Max-Margin Linear Models
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes a system for syntactic-
semantic dependency parsing for multiple lan-
guages. The system consists of three parts: a
state-of-the-art higher-order projective depen-
dency parser for syntactic dependency pars-
ing, a predicate classifier, and an argument
classifier for semantic dependency parsing.
For semantic dependency parsing, we ex-
plore use of global features. All components
are trained with an approximate max-margin
learning algorithm.
In the closed challenge of the CoNLL-2009
Shared Task (Hajic? et al, 2009), our system
achieved the 3rd best performances for En-
glish and Czech, and the 4th best performance
for Japanese.
1 Introduction
In recent years, joint inference of syntactic and se-
mantic dependencies has attracted attention in NLP
communities. Ideally, we would like to choose the
most plausible syntactic-semantic structure among
all possible structures in that syntactic dependencies
and semantic dependencies are correlated. How-
ever, solving this problem is too difficult because
the search space of the problem is extremely large.
Therefore we focus on improving performance for
each subproblem: dependency parsing and semantic
role labeling.
In the past few years, research investigating
higher-order dependency parsing algorithms has
found its superiority to first-order parsing algo-
rithms. To reap the benefits of these advances, we
use a higher-order projective dependency parsing al-
gorithm (Carreras, 2007) which is an extension of
the span-based parsing algorithm (Eisner, 1996), for
syntactic dependency parsing.
In terms of semantic role labeling, we would
like to capture global information about predicate-
argument structures in order to accurately predict the
correct predicate-argument structure. Previous re-
search dealt with such information using re-ranking
(Toutanova et al, 2005; Johansson and Nugues,
2008). We explore a different approach to deal
with such information using global features. Use
of global features for structured prediction problem
has been explored by several NLP applications such
as sequential labeling (Finkel et al, 2005; Krishnan
and Manning, 2006; Kazama and Torisawa, 2007)
and dependency parsing (Nakagawa, 2007) with a
great deal of success. We attempt to use global fea-
tures for argument classification in which the most
plausible semantic role assignment is selected using
both local and global information. We present an
approximate max-margin learning algorithm for ar-
gument classifiers with global features.
2 Dependency Parsing
As in previous work, we use a linear model for de-
pendency parsing. The score function used in our
dependency parser is defined as follows.
s(y) = ?
(h,m)?y
F (h,m,x) (1)
where h and m denote the head and the dependent
of the dependency edge in y, and F (h,m,x) is a
Factor that specifies dependency edge scores.
114
We used a second-order factorization as in (Car-
reras, 2007). The second-order factor F is defined
as follows.
F (h,m,x) = w ??(h,m,x)+w ??(h,m, ch,x)
+w ? ?(h,m, cmi,x) +w ? ?(h,m, cmo,x) (2)
where w is a parameter vector, ? is a feature vector,
ch is the child of h in the span [h...m] that is closest
to m, cmi is the child of m in the span [h...m] that is
farthest fromm and cmo is the child of m outside the
span [h...m] that is farthest fromm. For more details
of the second-order parsing algorithm, see (Carreras,
2007).
For parser training, we use the Passive Aggres-
sive Algorithm (Crammer et al, 2006), which is an
approximate max-margin variant of the perceptron
algorithm. Also, we apply an efficient parameter av-
eraging technique (Daume? III, 2006). The resulting
learning algorithm is shown in Algorithm 1.
Algorithm 1 A Passive Aggressive Algorithm with
parameter averaging
input Training set T = {xt,yt}Tt=1, Number of iterations
N and Parameter C
w ? 0, v ? 0, c ? 1
for i ? 0 to N do
for (xt,yt) ? T do
y? = argmaxy w ? ?(xt,y) + ?(yt, y?)
?t = min
?
C, w??(xt,y?)?w??(xt,yt)+?(yt,y?)||?(xt,yt)??(xt,y?)||2
?
w ? w + ?t(?(xt,yt)? ?(xt, y?))
v ? v + c?t(?(xt,yt)? ?(xt, y?))
c ? c + 1
end for
end for
return w ? v/c
We set ?(yt, y?) as the number of incorrect head
predictions in the y?, and C as 1.0.
Among the 7 languages of the task, 4 languages
(Czech, English, German and Japanese) contain
non-projective edges (13.94 %, 3.74 %, 25.79 %
and 0.91 % respectively), therefore we need to deal
with non-projectivity. In order to avoid losing the
benefits of higher-order parsing, we considered ap-
plying pseudo-projective transformation (Nivre and
Nilsson, 2005). However, growth of the number of
dependency labels by pseudo-projective transforma-
tion increases the dependency parser training time,
so we did not adopt transformations. Therefore, the
parser ignores the presence of non-projective edges
in the training and the testing phases.
The features used for our dependency parser are
based on those listed in (Johansson, 2008). In addi-
tion, distance features are used. We use shorthand
notations in order to simplify the feature represen-
tations: ?h?, ?d?, ?c?, ?l?, ?p?, ??1? and ?+1? cor-
respond to head, dependent, head?s or dependent?s
child, lemma , POS, left position and right position
respectively.
First-order Features
Token features: hl, hp, hl+hp, dl, dp and dl+dp.
Head-Dependent features: hp+dp, hl+dl, hl+dl,
hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and
hl+hp+dl+dp.
Context features: hp+hp+1+dp?1+dp,
hp?1+hp+dp?1+dp, hp+hp+1+dp+dp+1 and
hp?1+hp+dp+dp+1.
Distance features: The number of tokens between the
head and the dependent.
Second-order Features
Head-Dependent-Head?s or Dependent?s Child:
hl+cl, hl+cl+cp, hp+cl, hp+cp, hp+dp+cp, dp+cp,
dp+cl+cp, dl+cp, dl+cp+cl
3 Semantic Role Labeling
Our SRL module consists of two parts: a predicate
classifier and an argument classifier. First, our sys-
tem determines the word sense for each predicate
with the predicate classifier, and then it detects the
highest scored argument assignment using the argu-
ment classifier with global features.
3.1 Predicate Classification
The first phase of SRL in our system is to detect
the word sense for each predicate. WSD can be for-
malized as a multi-class classification problem given
lemmas. We created a linear model for each lemma
and used the Passive Aggressive Algorithm with pa-
rameter averaging to train the models.
3.1.1 Features for Predicate Classification
Word features: Predicted lemma and the predicted POS
of the predicate, predicate?s head, and its conjunc-
tions.
Dependency label: The dependency label between the
predicate and the predicate?s head.
115
Dependency label sequence: The concatenation of the
dependency labels of the predicate dependents.
Since effective features for predicate classifica-
tion are different for each language, we performed
greedy forward feature selection.
3.2 Argument Classification
In order to capture global clues of predicate-
argument structures, we consider introducing global
features for linear models. Let A(p) be a joint
assignment of role labels for argument candidates
given the predicate p. Then we define a score func-
tion s(A(p)) for argument label assignments A(p).
s(A(p)) =?
k
Fk(x,A(p)) (3)
We introduce two factors: Local Factor FL and
Global Factor FG defined as follows.
FL(x, a(p)) = w ? ?L(x, a(p)) (4)
FG(x,A(p)) = w ? ?G(x,A(p)) (5)
where ?L, ?G denote feature vectors for the local
factor and the global factor respectively. FL scores a
particular role assignment for each argument candi-
date individually, and FG treats global features that
capture what structure the assignment A has. Re-
sulting scoring function for the assignment A(p) is
as follows.
s(A(p)) = ?
a(p)?A(p)
w??L(x, a(p))+w??G(x,A(p))
(6)
Use of global features is problematic, because it
becomes difficult to find the highest assignment ef-
ficiently. In order to deal with the problem, we use
a simple approach, n-best relaxation as in (Kazama
and Torisawa, 2007). At first we generate n-best as-
signments using only the local factor, and then add
the global factor score for each n-best assignment, fi-
nally select the best scoring assignment from them.
In order to generate n-best assignments, we used a
beam-search algorithm.
3.2.1 Learning the Model
As in dependency parser and predicate classifier,
we train the model using the PA algorithm with pa-
rameter averaging. The learning algorithm is shown
in Algorithm 2. In this algorithm, the weights cor-
respond to local factor features ?L and global factor
features ?G are updated simultaneously.
Algorithm 2 Learning with Global Features for Ar-
gument Classification
input Training set T = {xt,At}Tt=1, Number of iterations
N and Parameter C
w ? 0, v ? 0, c ? 1
for i ? 0 to N do
for (xt,At) ? T do
let ?(xt,A) = Pa?A ?L(xt, a) + ?G(xt,A)generate n-best assignments {An} using FL
A? = argmaxA?{An} w ? ?(xt,A) + ?(At,A)
?t = min
?
C, w??(xt,A?)?w??(xt,At)+?(At,A?)||?(xt,At)??(xt,A?)||2
?
w ? w + ?t(?(xt,At)? ?(xt, A?))
v ? v + c?t(?(xt,At)? ?(xt, A?))
c ? c + 1
end for
end for
return w ? v/c
We set the margin value ?(A, A?) as the number
of incorrect assignments plus ?(A, A?), and C as 1.0.
The delta function returns 1 if at least one assign-
ment is different from the correct assignment and 0
otherwise.
The model is similar to re-ranking (Toutanova et
al., 2005; Johansson and Nugues, 2008). However
in contrast to re-ranking, we only have to prepare
one model. The re-ranking approach requires other
training datasets that are different from the data used
in local model training.
3.2.2 Features for Argument Classification
The local features used in our system are the same
as our previous work (Watanabe et al, 2008) except
for language dependent features. The global features
that used in our system are based on (Johansson and
Nugues, 2008) that used for re-ranking.
Local Features
Word features: Predicted lemma and predicted POS of
the predicate, predicate?s head, argument candidate,
argument candidate?s head, leftmost/rightmost de-
pendent and leftmost/rightmost sibling.
Dependency label: The dependency label of predicate,
argument candidate and argument candidate?s de-
pendent.
Family: The position of the argument candicate with re-
spect to the predicate position in the dependency
tree (e.g. child, sibling).
116
Average Catalan Chinese Czech English German Japanese Spanish
Macro F1 Score 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
(78.00*) (74.83*) (73.43*) (81.38*) (86.40*) (68.39*) (84.84*) (76.74*)
Semantic Labeled F1 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
(75.17*) (71.05*) (74.17*) (84.66*) (84.26*) (61.94*) (77.91*) (72.25*)
Labeled Syntactic Accuracy 81.16 79.48 72.66 78.17 88.54 75.85 91.69 81.74
(80.77*) (78.62*) (72.66*) (78.10*) (88.54*) (74.60*) (91.66*) (81.23*)
Macro F1 Score 84.30 84.79 81.63 83.08 87.93 83.25 85.54 83.94
Semantic Labeled F1 81.58 80.99 79.99 86.67 85.09 79.46 79.03 79.85
Labeled Syntactic Accuracy 87.02 88.59 83.27 79.48 90.77 87.03 91.96 88.04
Table 1: Scores of our system.
Position: The position of the head of the dependency re-
lation with respect to the predicate position in the
sentence.
Pattern: The left-to-right chain of the predicted
POS/dependency labels of the predicate?s children.
Path features: Predicted lemma, predicted POS and de-
pendency label paths between the predicate and the
argument candidate.
Distance: The number of dependency edges between the
predicate and the argument candidate.
Global Features
Predicate-argument label sequence: The sequence of
the predicate sense and argument labels in the
predicate-argument strucuture.
Presence of labels defined in frame files: Whether the
semantic roles defined in the frame present in the
predicate-argument structure (e.g. MISSING:A1 or
CONTAINS:A1.)
3.2.3 Argument Pruning
We observe that most arguments tend to be not far
from its predicate, so we can prune argument candi-
dates to reduce search space. Since the characteris-
tics of the languages are slightly different, we apply
two types of pruning algorithms.
Pruning Algorithm 1: Let S be an argument candi-
date set. Initially set S ? ? and start at predicate node.
Add dependents of the node to S, and move current node
to its parent. Repeat until current node reaches to ROOT.
Pruning Algorithm 2: Same as the Algorithm 1 ex-
cept that added nodes are its grandchildren as well as its
dependents.
The pruning results are shown in Table 2. Since
we could not prune arguments in Japanese accu-
rately using the two algorithms, we pruned argument
candidates simply by POS.
algorithm coverage (%) reduction (%)
Catalan 1 100 69.1
Chinese 1 98.9 69.1
Czech 2 98.5 49.1
English 1 97.3 63.1
German 1 98.3 64.3
Japanese POS 99.9 41.0
Spanish 1 100 69.7
Table 2: Pruning results.
4 Results
The submitted results on the test data are shown in
the upper part of Table 1. Due to a bug, we mistak-
enly used the gold lemmas in the dependency parser.
Corrected results are shown in the part marked with
*. The lower part shows the post evaluation results
with the gold lemmas and POSs.
For some of the 7 languages, since the global
model described in Section 3.2 degraded perfor-
mance compare to a model trained with only FL,
we did NOT use the model for all languages. We
used the global model for only three languages: Chi-
nese, English and Japanese. The remaining lan-
guages (Catalan, Czech, German and Spanish) used
a model trained with only FL.
4.1 Dependency Parsing Results
The parser achieved relatively high accuracies for
Czech, English and Japanese, and for each language,
the difference between the performance with correct
POS and predicted POS is not so large. However, in
Catalan, Chinese German and Spanish, the parsing
accuracies was seriously degraded by replacing cor-
rect POSs with predicted POSs (6.3 - 11.2 %). This
is likely because these languages have relatively low
predicted POS accuracies (92.3 - 95.5 %) ; Chinese
117
FL FL+FG (?P, ?R)
Catalan 85.80 85.68 (+0.01, -0.26)
Chinese 86.58 87.39 (+0.24, +1.36)
Czech 89.63 89.05 (-0.87, -0.28)
English 85.66 85.74 (-0.87, +0.98)
German 80.82 77.30 (-7.27, +0.40)
Japanese 79.87 81.01 (+0.17, +1.88)
Spanish 84.38 83.89 (-0.42, -0.57)
Table 3: Effect of global features (semantic labeled F1).
?P and ?R denote the differentials of labeled precision
and labeled recall between FL and FL+FG respectively.
has especially low accuracy (92.3%). The POS ac-
curacy may affect the parsing performances.
4.2 SRL Results
In order to highlight the effect of the global fea-
tures, we compared two models. The first model
is trained with only the local factor FL. The sec-
ond model is trained with both the local factor FL
and the global factor FG. The results are shown in
Table 3. In the experiments, we used the develop-
ment data with gold parse trees. For Chinese and
Japanese, significant improvements are obtained us-
ing the global features (over +1.0% in labeled re-
call and the slightly better labeled precision). How-
ever, for Catalan, Czech, German and Spanish, the
global features degraded the performance in labeled
F1. Especially, in German, the precision is substan-
tially degraded (-7.27% in labeled F1). These results
indicate that it is necessary to introduce language de-
pendent features.
4.3 Training, Evaluation Time and Memory
Requirements
Table 4 and 5 shows the training/evaluation times
and the memory consumption of the second-order
dependency parsers and the global argument classi-
fiers respectively. The training times of the predi-
cate classifier were less than one day, and the testing
times were mere seconds.
As reported in (Carreras, 2007; Johansson and
Nugues, 2008), training and inference of the second-
order parser are very expensive. For Chinese, we
could only complete 2 iterations.
In terms of the argument classifier, since N-best
generation time account for a substantial proportion
of the training time (in this work N = 100), chang-
iter hrs./iter sent./min. mem.
Catalan 9 14.6 9.0 9.6 GB
Chinese 2 56.5 3.7 16.2 GB
Czech 8 14.6 20.5 12.6 GB
English 7 22.0 13.4 15.1 GB
German 4 12.3 59.1 13.1 GB
Japanese 7 11.2 21.8 13.0 GB
Spanish 7 19.5 7.3 17.9 GB
Table 4: Training, evaluation time and memory require-
ments of the second-order dependency parsers. The ?iter?
column denote the number of iterations of the model
used for the evaluations. Catalan, Czech and English
are trained on Xeon 3.0GHz, Chinese and Japanese are
trained on Xeon 2.66GHz, German and Spanish are
trained on Opteron 2.3GHz machines.
train (hrs.) sent./min. mem.
Chinese 6.5 453.7 2.0 GB
English 13.5 449.8 3.2 GB
Japanese 3.5 137.6 1.1 GB
Table 5: Training, evaluation time and memory require-
ments of the global argument classifiers. The classifiers
are all trained on Opteron 2.3GHz machines.
ing N affects the training and evaluation times sig-
nificantly.
All modules of our system are implemented in
Java. The required memory spaces shown in Table
4 and 5 are calculated by subtracting free memory
size from the total memory size of the Java VM.
Note that we observed that the value fluctuated dras-
tically while measuring memory usage, so the value
may not indicate precise memory requirements of
our system.
5 Conclusion
In this paper, we have described our system for syn-
tactic and semantic dependency analysis in multilin-
gual. Although our system is not a joint approach
but a pipeline approach, the system is comparable to
the top system for some of the 7 languages.
A further research direction we are investigating
is the application of various types of global features.
We believe that there is still room for improvements
since we used only two types of global features for
the argument classifier.
Another research direction is investigating joint
approaches. To the best of our knowledge, three
118
types of joint approaches have been proposed:
N-best based approach (Johansson and Nugues,
2008), synchronous joint approach (Henderson et
al., 2008), and a joint approach where parsing
and SRL are performed simultaneously (Llu??s and
Ma`rquez, 2008). We attempted to perform N-
best based joint approach, however, the expen-
sive computational cost of the 2nd-order projective
parser discouraged it. We would like to investigate
syntactic-semantic joint approaches with reasonable
time complexities.
Acknowledgments
We would like to thank Richard Johansson for his
advice on parser implementation, and the CoNLL-
2009 organizers (Hajic? et al, 2009; Taule? et al,
2008; Palmer and Xue, 2009; Hajic? et al, 2006; Sur-
deanu et al, 2008; Burchardt et al, 2006; Kawahara
et al, 2002; Taule? et al, 2008).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proc. of LREC-2006, Genoa,
Italy.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL 2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA, August.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing. In Proc. of ICCL 1996.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proc. of ACL 2005.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proc. of CoNLL-2009,
Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proc. of CoNLL 2008.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proc. of CoNLL
2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proc. of LREC-2002, pages 2008?2013,
Las Palmas, Canary Islands.
Jun?Ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proc. of EMNLP-CoNLL 2007.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In Proc. of
ACL-COLING 2006.
Xavier Llu??s and Llu??s Ma`rquez. 2008. A joint model for
parsing syntactic and semantic dependencies. In Proc.
of CoNLL 2008.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL 2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proc. of LREC-2008, Mar-
rakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proc. of ACL 2005.
Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara,
and Yuji Matsumoto. 2008. A pipeline approach for
syntactic and semantic dependency parsing. In Proc.
of CoNLL 2008.
119
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479?1488,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Third-order Variational Reranking on Packed-Shared Dependency Forests
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Insutitute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
{katsuhiko-h,masayu-a,matsu}@is.naist.jp
taro.watanabe@nict.go.jp
Abstract
We propose a novel forest reranking algorithm
for discriminative dependency parsing based
on a variant of Eisner?s generative model. In
our framework, we define two kinds of gener-
ative model for reranking. One is learned from
training data offline and the other from a for-
est generated by a baseline parser on the fly.
The final prediction in the reranking stage is
performed using linear interpolation of these
models and discriminative model. In order to
efficiently train the model from and decode
on a hypergraph data structure representing a
forest, we apply extended inside/outside and
Viterbi algorithms. Experimental results show
that our proposed forest reranking algorithm
achieves significant improvement when com-
pared with conventional approaches.
1 Introduction
Recently, much of research on statistical parsing
has been focused on k-best (or forest) reranking
(Collins, 2000; Charniak and Johnson, 2005; Huang,
2008). Typically, reranking methods first generate
a list of top-k candidates (or a forest) from a base-
line system, then rerank the candidates with arbi-
trary features that are intractable within the baseline
system. In the reranking framework, the baseline
system is usually modeled with a generative model,
and a discriminative model is used for reranking.
Sangati et al (2009) reversed the usual order of the
two models for dependency parsing by employing
a generative model to rescore the k-best candidates
provided by a discriminative model. They use a vari-
ant of Eisner?s generative model C (Eisner, 1996b;
Eisner, 1996a) for reranking and extend it to capture
higher-order information than Eisner?s second-order
generative model. Their reranking model showed
large improvements in dependency parsing accu-
racy. They reported that the discriminative model is
very effective at filtering out bad candidates, while
the generative model is able to further refine the se-
lection among the few best candidates.
In this paper, we propose a forest generative
reranking algorithm, opposed to Sangati et al
(2009)?s approach which reranks only k-best candi-
dates. Forests usually encode better candidates more
compactly than k-best lists (Huang, 2008). More-
over, our reranking uses not only a generative model
obtained from training data, but also a sentence spe-
cific generative model learned from a forest. In the
reranking stage, we use linearly combined model
of these models. We call this variational rerank-
ing model. The model proposed in this paper is
factored in the third-order structure, therefore, its
non-locality makes it difficult to perform the rerank-
ing with an usual 1-best Viterbi search. To solve
this problem, we also propose a new search algo-
rithm, which is inspired by the third-order dynamic
programming parsing algorithm (Koo and Collins,
2010). This algorithm enables us an exact 1-best
reranking without any approximation. We summa-
rize our contributions in this paper as follows.
? To extend k-best to forest generative reranking.
? We introduce variational reranking which is a
combination approach of generative reranking
and variational decoding (Li et al, 2009).
? To obtain 1-best tree in the reranking stage, we
1479
propose an exact 1-best search algorithm with
the third-order model.
In experiments on English Penn Treebank data,
we show that our proposed methods bring signif-
icant improvement to dependency parsing. More-
over, our variational reranking framework achieves
consistent improvement, compared to conventional
approaches, such as simple k-best and forest-based
generative reranking algorithms.
2 Dependency Parsing
Given an input sentence x ? X , the task of statis-
tical dependency parsing is to predict output depen-
dencies y? for x. The task is usually modeled within a
discriminative framework, defined by the following
equation:
y? = argmax
y?Y
s(x, y)
= argmax
y?Y
?? ? F(y, x) (1)
where Y is the output space, ? is a parameter vector,
and F() is a set of feature functions.
We denote a set of candidates as G(x). By using
G(x), the conditional probability p(y|x) is typically
derived as follows:
p(y|x) = e
??s(x,y)
Z(x) =
e??s(x,y)?
y?G(x) e??s(x,y)
(2)
where s(x, y) is the score function shown in Eq.1
and ? is a scaling factor to adjust the sharpness of
the distribution and Z(x) is a normarization factor.
2.1 Hypergraph Representation
We propose to encode many hypotheses in a com-
pact representation called dependency forest. While
there may be exponentially many dependency trees,
the forest represents them in polynomial space. A
dependency forest (or tree) can be defined as a hy-
pergraph data strucureHG (Tu et al, 2010).
Figure 1 shows an example of a hypergraph for a
dependency tree. A shaded hyperedge e is defined
as the following form:
e : ?(I1,2, girl3,5,with5,8), saw1,8?.
.
.
.top0,8
..
.
..saw1,8:V
.
.
. .I1,2:N .. . . . .
.
..girl3,5:N
..a3,4:D ..
. .
.
..with5,8:P
..
.
..telescope6,8:N
. ..a6,7:D ..
. .
. .
.e . . . . .
. .
Figure 1: An example of dependency tree for a sentence
?I saw a girl with a telescope?.
The node saw1,8 is a head node of e. The nodes, I1,2,
girl3,5 and with5,8, are tail nodes of e. The hyper-
edge e is an incoming edge for saw1,8 and outgoing
edge for each of I1,2, girl3,5 and with5,81.
More formally, HG(x) of a forest is a pair
?V,E?, where V is a set of nodes and E is a set
of hyperedges. Given a length m sentence x =
(w1 . . . wm), each node v ? V is in the form of
wi,j (= (wi . . . wj+1)) which denotes that a word
w dominates the substring from positions i to j. In
our implementation, each word is paired with POS-
tag tag(w). We denote the root node of dependency
tree y as top. Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the head
and tails(e) ? V + are its dependants. For nota-
tional brevity of algorithmic description, we do not
distinguish left and right tails in the definition, but,
our implementation implicitly distinguishes left tails
tailsL(e) and right tails tailsR(e). We define the set
of incoming edges of a node v as IE(v) and the set
of outgoing edges of a node v as OE(v).
3 Forest Reranking
3.1 Generative Model for Reranking
Given a node v in a dependency tree y, the left and
right children are generated as two separate Markov
sequences, each conditioned on ancestral and sibling
information (context). Like a variation of Eisner?s
generative model C (Eisner, 1996b; Eisner, 1996a),
1In Figure 1, according to custom of dependency tree
description, the direction of hyperedge is written as from
head to tail nodes. However, in this paper, ?incoming? and
?outgoing? have the same meanings as those in (Huang, 2006).
1480
Table 1: An event list of tri-sibling model whose event
space is v|h, sib, tsib, dir, extracted from hyperedge e in
Figure 1. EOC is an end symbol of sequence.
event space
I | saw NONE NONE L
EOC | saw I NONE L
girl | saw NONE NONE R
with | saw girl NONE R
EOC | saw with girl R
the probability of our model q is defined as follows:
q(v) =
|tailsL(e)|?
l=1
q(vl|C(vl)) ? q(vl)
?
|tailsR(e)|?
r=1
q(vr|C(vr)) ? q(vr) (3)
where |tailsL(e)| and |tailsR(e)| are the number of
left and right children of v, vl and vr are the left and
right child of position l and r in each side. C(v) is
a context event space of v. We explain the context
event space later in more detail. The probability of
the entire dependency tree y is recursively computed
by q(y(top)) where y(top) denotes a top node of y.
The probability q(v|C(v)) is dependent on a con-
text space C(v) for a node v. We define two kinds of
context spaces. First, we define a tri-sibling model
whose context space consists of the head node, sib-
ling node, tri-sibling node and direction of a node
v:
q1(v|C(v)) = q1(v|h, sib, tsib, dir) (4)
where h, sib and tsib are head, sibling and tri-sibling
node of v, and dir is a direction of v from h. Table
1 shows an example of an event list of the tri-sibling
model, which is extracted from hyperedge e in Fig-
ure 1. EOC indicates the end of the left or right child
sequence. This is factored in a tri-sibling structure
shown in the left side of Figure 2.
Eq.4 is further decomposed into a product of the
form consisting of three terms:
q1(v|h, sib, tsib, dir) (5)
= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)
= q1(tag(v)|h, sib, tsib, dir)
?q1(wrd(v)|tag(v), h, sib, tsib, dir)
?q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)
where tag(v) and wrd(v) are the POS-tag and word
of v and dist(v, h) is the distance between positions
of v and h. The values of dist(v, h) are partitioned
into 4 categories: 1, 2, 3 ? 6, 7 ??.
Second, following Sangati et al (2009), we define
a grandsibling model whose context space consists
of the head node, sibling node, grandparent node and
direction of a node v.
q2(v|C(v)) = q2(v|h, sib, g, dir) (6)
where g is a grandparent node of v. Analogous to
Eq.5, Eq.6 is decomposed into three terms:
q2(v|h, sib, g, dir) (7)
= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)
= q2(tag(v)|h, sib, g, dir)
?q2(wrd(v)|tag(v), h, sib, g, dir)
?q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)
where notations are the same as those in Eq.5 with
the exception of tri-sibling tsib and grandparent g.
This model is factored in a grandsibling structure
shown in the right side of Figure 2.
The direct estimation of tri-sibling and grandsib-
ling models from a corpus suffers from serious data
sparseness issues. To overcome this, Eisner (1996a)
proposed a back-off strategy which reduces the con-
ditioning of a model. We show the reductions list
for each term of two models in Table 2. The usage
of reductions list is identical to Eisner (1996a) and
readers may refer to it for further details.
The final prediction is performed using a log-
linear interpolated model. It interpolates the base-
line discriminative model and two (tri-sibling and
grandsibling) generative models.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+ log p(y|x)?base (8)
where ? are parameters to adjust the weight of each
term in prediction. These parameters are tuned using
MERT algorithm (Och, 2003) on development data
using a criterion of accuracy maximization. The rea-
son why we chose MERT is that it effectively tunes
dense parameters with a line search algorithm.
1481
Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,
POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition;
later reductions throw away more and more of this information.
tri-sibling grandsibling
1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd term
wt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),d
wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d
t(h),wt(sib),t(tsib),d ? ? t(h),wt(sib),t(g),d ? ?wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),d
t(h),t(sib),t(tsib),d ? ? t(h),t(sib),t(g),d ? ?
..h .tsib .sib .v ..g .h .sib .v
Figure 2: The left side denotes tri-sibling structure and
the right side denotes grandsibling structure.
Table 3: A summarization of the model factorization and
order
first-order McDonald et al (2005)
second-order Eisner (1996a)
(sibling) McDonald et al (2005)
third-order tri-sibling model
(tri-sibling) Model 2 (Koo and Collins, 2010)
third-order grandsibling model (Sangati et al, 2009)
(grandsibling) Model 1 (Koo and Collins, 2010)
3.2 Exact Search Algorithm
Our baseline discriminative model uses first- and
second-order features provided in (McDonald et al,
2005; McDonald and Pereira, 2006). Therefore,
both our tri-sibling model and baseline discrimina-
tive model integrate local features that are factored
in one hyperedge. On the other hand, the grandsib-
ling model has non-local features because the grand-
parent is not factored in one hyperedge. We sum-
marize the order of each model in Table 3. Our
reranking models are generative versions of Koo and
Collins (2010)?s third-order factorization model.
Non-locality of weight function makes it difficult
to perform the search of Eq.8 with an usual exact
Viterbi 1-best algorithm. One solution to resolve
the intractability is an approximate k-best Viterbi
search. For a constituent parser, Huang (2008) ap-
plied cube pruning techniques to forest reranking
with non-local features. Cube pruning is originally
proposed for the decoding of statistical machine
translation (SMT) with an integrated n-gram lan-
guage model (Chiang, 2007). It is an approximate
k-best Viterbi search algorithm using beam search
and lazy computation (Huang and Chiang, 2005).
In the case of a dependency parser, Koo and
Collins (2010) proposed dynamic-programming-
based third-order parsing algorithm, which enumer-
ates all grandparents with an additional loop. Our
hypergraph based search algorithm for Eq.8 share
the same spirit to their third-order parsing algo-
rithm since the grandsibling model is similar to their
model 1 in that it is factored in grandsibling struc-
ture. Algorithm 1 shows the search algorithm. This
is almost the same bottom-up 1-best Viterbi algo-
rithm except an additional loop in line 4. Line 4 ref-
erences outgoing edge e? of node h from a set of out-
going edges OE(h). tails(e) contains a node v, the
sibling node sib and tri-sibling node tsib of v, more-
over, the head of e? (head(e?)) is the grandparent for
v and sib. Thus, in line 5, we can capture tri-sibling
and grandsibling information and compute the cur-
rent inside estimate of Eq.8.
In our actual implementation, each score of com-
ponents in Eq.8 is represented as a cost. This is writ-
ten as a shortest path search algorithm with a tropi-
cal (real) semiring framework (Mohri, 2002; Huang,
2006). Therefore,? denotes the min operater and?
denotes the + operater. The function f is defined as
follows:
f(d(v1, e), . . . , d(v|e|, e))) =
|e|?
i=1
d(vi, e) (9)
where d(vi, e) denotes the current estimate of the
best cost for a pair of node vi and a hyperedge e.? sums the best cost of a pair of a sub span node
and hyperedge e. Each ctsib and cgsib in line 5 and
7 indicates the cost of tri-sibling and grandsibling
1482
Algorithm 1 Exact DP-Search Algorithm(HG(x))
1: for h ? V in bottom-up topological order do
2: for e ? IE(h) do
3: // tails(e) is {v1, . . . , v|e|
}.
4: for e? ? OE(h) do
5: d(h, e?) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e)) ? cgsib(head(e?), h, tails(e))
6: if h == top then
7: d(h) = ?f(d(v1, e), . . . , d(v|e|, e)) ? we ? ctsib(h, tails(e))
model. we indicates the cost of hyperedge e com-
puted from a baseline discriminative model. Lines
6-7 denote the calculation of the best cost for a top
node. We do not compute the cost of the grandsib-
ling model when h is top node because top node has
no outgoing edges.
Our baseline k-best second-order parser is imple-
mented using Huang and Chiang (2005)?s algorithm
2 whose time complexity is O(m3+mk log k). Koo
and Collins (2010)?s third-order parser has O(m4)
time complexity and is theoretically slower than our
baseline k-best parser for a long sentence. Our
search algorithm is based on the third-order parsing
algorithm, but, the search space is previously shrank
by a baseline parser?s k-best approximation and a
forest pruning algorithm presented in the next sec-
tion. Therefore, the time efficiency of our reranking
is unimpaired.
3.3 Forest Pruning
Charniak and Johnson (2005) and Huang (2008)
proposed forest pruning algorithms to reduce the
size of a forest. Huang (2008)?s pruning algo-
rithm uses a 1-best Viterbi inside/outside algorithm
to compute an inside probability ?(v) and an out-
side probability ?(v), while Charniak and Johnson
(2005) use the usual inside/outside algorithm.
In our experiments, we use Charniak and Johnson
(2005)?s forest pruning criterion because the varia-
tional model needs traditional inside/outside proba-
bilities for its ML estimation. We prune away all
hyperedges that have score < ? for a threshold ?.
score = ??(e)?(top) . (10)
Following Huang (2008), we also prune away nodes
with all incoming and outgoing hyperedges pruned.
4 Variational Reranking Model
In place of a maximum a posteriori (MAP) decision
based on Eq.2, the minimum Bayes risk (MBR) deci-
sion rule (Titov and Henderson, 2006) is commonly
used and defined as following equation:
y? = argmin
y?G(x)
?
y??G(x)
loss(y, y?)p(y?|x) (11)
where loss(y, y?) represents a loss function2. As an
alternative to the MBR decision rule, Li et al (2009)
proposed a variational decision rule that rescores
candidates with an approximate distribution q? ? Q.
y? = argmax
y?G(x)
q?(y) (12)
where q? minimizes the KL divergence KL(p||q)
q? = argmin
q?Q
KL(p||q)
= argmax
q?Q
?
y?G(x)
p log q (13)
where each p and q represents p(y|x) and q(y). For
SMT systems, q? is modeled by n-gram language
model over output strings. While the decoding based
on q? is an approximation of intractable MAP de-
coding3, it works as a rescoring function for candi-
dates generated from a baseline model. Here, we
propose to apply the variational decision rule to de-
pendency parsing. For dependency parsing, we can
choose to model q? as the tri-sibling and grandsib-
ling generative models in section 3.
2In case of dependency parsing, Titov and Henderson (2006)
proposed that a loss function is simply defined using a depen-
dency attachment score.
3In SMT, a marginalization of all derivations which yield
a paticular translation needs to be carried out for each trans-
lation. This makes the MAP decoding NP-hard in SMT. This
variational approximate framework can be applied to other tasks
collapsing spurious ambiguity, such as latent-variable parsing
(Matsuzaki et al, 2005).
1483
Algorithm 2 DP-ML Estimation(HG(x))
1: run inside and outside algorithm onHG(x)
2: for v ? V do
3: for e ? IE(v) do
4: ctsib = pe ? ?(v)/?(top)
5: for u ? tails(e) do
6: ctsib = ctsib ? ?(u)
7: for e? ? IE(u) do
8: cgsib = pe ? pe? ? ?(v)/?(top)
9: for u? ? tails(e) \ u do
10: cgsib = cgsib ? ?(u?)
11: for u?? ? tails(e?) do
12: cgsib = cgsib ? ?(u??)
13: for u?? ? tails(e?) do
14: c2(u??|C(u??))+ = cgsib
15: c2(C(u??))+ = cgsib
16: for u ? tails(e) do
17: c1(u|C(u))+ = ctsib
18: c1(C(u))+ = ctsib
19: MLE estimate q?1 , q?2 using formula Eq.14
4.1 ML Estimation from a Forest
q?(v|C(v)) is estimated from a forest using a max-
imum likelihood estimation (MLE). The count of
events is no longer an integer count, but an expected
count under p, which is formulated as follows:
q?(v|C(v)) = c(v|C(v))c(C(v))
=
?
y p(y|x)cv|C(v)(y)?
y p(y|x)cC(v)(y)
(14)
where ce(y) is the number of event e in y. The es-
timation of Eq.14 can be efficiently performed on a
hypergraph data structureHG(x) of a forest.
Algorithm 2 shows the estimation algorithm.
First, it runs the inside/outside algorithm onHG(x).
We denote inside weight for a node v as ?(v) and
outside weight as ?(v). For each hyperedge e, we
denote ctsib as the posterior weight for computing
expected count c1 of events in the tri-sibling model
q?1 . Lines 16-18 compute c1 for all events occuring
in a hyperedge e.
The expected count c2 needed for the estimation
of grandsibling model q?2 is extracted in lines 7-15.
c2 for a grandsibling model must be extracted over
two hyperedges e and e? because it needs grandpar-
ent information. Lines 8-12 show the algorithm to
compute the posterior weight cgsib of e and e?, which
 92
 93
 94
 95
 96
 97
 98
 99
 100
 0  200  400  600  800  1000  1200  1400
Un
la
be
le
d 
Ac
cu
ra
cy
the number of hyperedges per sentence
p=0.001
k=20
k=100
"kbest"
"forest"
Figure 3: The relationship between tha data size (the
number of hyperedges) and oracle scores on develop-
ment data: Forests encode candidates with high accuracy
scores more compactly than k-best lists.
is similar to that to compute the posterior weight
of rules of tree substitution grammars used in tree-
based MT systems (Mi and Huang, 2008). Lines
13-15 compute expected counts c2 of events occur-
ing over two hyperedges e and e?. Finally, line 19
estimates q?1 and q?2 using the form in Eq.14.
Li et al (2009) assumes n-gram locality of the
forest to efficiently train the model, namely, the
baseline n-gram model has larger n than that of vari-
ational n-gram model. In our case, grandsibling lo-
cality is not embedded in the forest generated from
the baseline parser. Therefore, we need to reference
incoming hyperedges of tail nodes in line 7.
y? of Eq.12 may be locally appropriate but glob-
ally inadequate because q? only approximates p.
Therefore, we log-linearly combine q? with a global
generative model estimated from the training data
and the baseline discriminative model.
y? = argmax
y?G(x)
2?
n=1
log qn(top(y))?n
+
2?
n=1
log q?n(top(y))?
?
n
+ log p(y|x)?base (15)
Algorithm1 is also applicable to the decoding of
Eq.15. Note that this framework is a combination of
variational decoding and generative reranking. We
call this framework variational reranking.
1484
Table 4: The statistics of forests and 20-best lists on de-
velopment data: this shows the average number of hyper-
edges and nodes per sentence and oracle scores.
forest 20-best
pruning threshold ? = 10?3 ?
ave. num of hyperedges 180.67 255.04
ave. num of nodes 135.74 491.42
oracle scores 98.76 96.78
5 Experiments
Experiments are performed on English Penn Tree-
bank data. We split WSJ part of the Treebank into
sections 02-21 for training, sections 22 for develop-
ment, sections 23 for testing. We use Yamada and
Matsumoto (2003)?s head rules to convert phrase
structure to dependency structure. We obtain k-best
lists and forests generated from the baseline discrim-
inative model which has the same feature set as pro-
vided in (McDonald et al, 2005), using the second-
order Eisner algorithms. We use MIRA for training
as it is one of the learning algorithms that achieves
the best performance in dependency parsing. We set
the scaling factor ? = 1.0.
We also train a generative reranking model from
the training data. To reduce the data sparseness
problem, we use the back-off strategy proposed in
(Eisner, 1996a). Parameters ? are trained using
MERT (Och, 2003) and for each sentence in the de-
velopment data, 300-best dependency trees are ex-
tracted from its forest. Our variational reranking
does not need much time to train the model be-
cause the training is performed over not the train-
ing data (39832 sentences) but the development data
(1700 sentences)4. After MERT was performed un-
til the convergence, the variational reranking finally
achieved a 94.5 accuracy score on development data.
5.1 k-best Lists vs. Forests
Figure 3 shows the relationship between the size of
data structure (the number of hyperedges) and accu-
racy scores on development data. Obviously, forests
can encode a large number of potential candidates
more compactly than k-best lists. This means that
4To generate forests, sentences are parsed only once before
the training. MERT is performed over the forests. We can also
apply a more efficient hypergraph MERT algorithm (Kumar et
al., 2009) to the training than a simple MERT algorithm.
for reranking, there is more possibility of selecting
good candidates in forests than k-best lists.
Table 4 shows the statistics of forests and 20-
best lists on development data. This setting, thresh-
old ? = 10?3 for pruning, is also used for testing.
Forests, which have an average of 180.67 hyper-
edges per sentence, achieve oracle score of 98.76,
which is about 1.0% higher than the 96.78 oracle
score of 20-best lists with 255.04 hyperedges per
sentence. Though the size of forests is smaller than
that of k-best lists, the oracle scores of forests are
much higher than those of k-best lists.
5.2 The Performance of Reranking
First, we compare the performance of variational de-
coding with that of MBR decoding. The results are
shown in Table 5. Variational decoding outperforms
MBR decodings. However, compared with base-
line, the gains of variational and MBR decoding are
small. Second, we also compare the performance of
variational reranking with k-best and forest gener-
ative reranking algorithms. Table 6 shows that our
variational reranking framework achieves the high-
est accuracy scores.
Being different from the decoding framework,
reranking achieves significant improvements. This
result is intuitively reasonable because the rerank-
ing model obtained from training data has the ability
to select a globally consistent candidate, while the
variational approximate model obtained from a for-
est only supports selecting a locally consistent can-
didate. On the other hand, the fact that variational
reranking achieves the best results clearly indicates
that the combination of sentence specific generative
model and that obtained from training data is suc-
cessful in selecting both locally and globally appro-
priate candidate from a forest.
Table 7 shows the parsing time (on 2.66GHz
Quad-Core Xeon) of the baseline k-best, generative
reranking and variational reranking parsers (java im-
plemented). The variational reranking parser con-
tains the following procedures.
1. k-best forest creation (baseline)
2. Estimation of variational model
3. Forest pruning
4. Search with the third-order model
Our reranking parser incurred little overhead to the
1485
Table 5: The comparison of the decoding frameworks:
MBR decoding seeks a candidate which has the high-
est accuracy scores over a forest (Kumar et al, 2009).
Variational decoding is performed based on Eq.8.XXXXXXXXXXDecoding
Eval Unlabeled
baseline 91.9
MBR (8-best forest) 91.99
Variational (8-best forest) 92.17
Table 6: The comparison of the reranking frameworks:
Generative means k-best or forest reranking algorithm
based on a generative model estimated from a corpus.
Variational reranking is performed based on Eq.15.XXXXXXXXXXReranking
Eval Unlabeled
Generative (8-best) 92.66
Generative (8-best forest) 92.72
Variational (8-best forest) 92.87
Table 7: The parsing time (CPU second per sentence) and
accuracy score of the baseline k-best, generative rerank-
ing and variational reranking parsers
k baseline generative variational
2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)
4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)
8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)
16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)
32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)
64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)
Table 8: The comparison of tri-sibling and grandsibling
models: the performance of the grandsibling model out-
performs that of the tri-sibling model.PPPPPPPModel
Eval Unlabeled
tri-sibling 92.63
grandsibling 92.74
baseline parser in terms of runtime. This means that
our reranking parser can parse sentences at reason-
able times.
5.3 The Effects of Third-order Factors and
Error Analysis
From results in section 5.2, our variational rerank-
ing model achieves higher accuracy scores than the
others. To analyze the factors that improve accu-
racy scores, we further investigate whether varia-
tional reranking is performed better with the tri-
sibling or grandsibling model. Table 8 indicates that
grandsibling model achieves a larger gain than that
of tri-sibling model. Table 9 shows the examples
whose accuracy scores improved by the grandsib-
ling model. For example, the dependency relation-
ship from Verb to Noun phrase was corrected by our
proposed model.
On the other hand, many errors remain still in
Table 10: Comparison of our best result (using 16-best
forests) with other best-performing Systems on the whole
section 23
Parser English
McDonald et al (2005) 90.9
McDonald and Pereira (2006) 91.5
Koo et al (2008) standard 92.02
Huang and Sagae (2010) 92.1
Koo and Collins (2010) model1 93.04
Koo and Collins (2010) model2 92.93
this work 92.89
Koo et al (2008) semi-sup 93.16
Suzuki et al (2009) 93.79
our results. In our experiments, 48% of sentences
which contain errors have Prepositionalword errors.
In fact, well-known PP-Attachment is a problem to
be solved for natural language parsers. Other re-
maining errors are caused by symbols such as .,:??().
45% sentences contain such a dependency mistake.
Adding features to solve these problems may poten-
tially improve our parser more.
5.4 Comparison with Other Systems
Table 10 shows the comparison of the performance
of variational reranking (16-best forests) with that of
other systems. Our method outperforms supervised
parsers with second-order features, and achieves
comparable results compared to a parser with third-
order features (Koo and Collins, 2010). We can not
directly compare our method with semi-supervised
parsers such as Koo et al (2008)?s semi-sup and
Suzuki et al (2009), because ours does not use addi-
tional unlabeled data for training. The model trained
from unlabeled data can be easily incorporated into
our reranking framework. We plan to investigate
semi-supervised learning in future work.
1486
Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational
reranking parsers. The underlined portions show the effect of the grandsibling model.
sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .
correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4
baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4
proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4
sent (No.283) Many called it simply a contrast in styles .
correct 2 0 2 6 6 2 6 7 2
baseline 2 0 2 2 6 2 6 7 2
proposed 2 0 2 6 6 2 6 7 2
6 Related Work
Collins (2000) and Charniak and Johnson (2005)
proposed a reranking algorithm for constituent
parsers. Huang (2008) extended it to a forest rerank-
ing algorithm with non-local features. Our frame-
work is for a dependency parser and the decoding in
the reranking stage is done with an exact 1-best dy-
namic programming algorithm. Sangati et al (2009)
proposed a k-best generative reranking algorithm for
dependency parsing. In this paper, we use a similar
generative model, but combined with a variational
model learned on the fly. Moreover, our framework
is applicable to forests, not k-best lists.
Koo and Collins (2010) presented third-order de-
pendency parsing algorithm. Their model 1 is de-
fined by an enclosing grandsibling for each sibling
or grandchild part used in Carreras (2007). Our
grandsibling model is similar to the model 1, but
ours is defined by a generative model. The decod-
ing in the reranking stage is also similar to the pars-
ing algorithm of their model 1. In order to capture
grandsibling factors, our decoding calculates inside
probablities for not the current head node but each
pair of the node and its outgoing edges.
Titov and Henderson (2006) reported that the
MBR approach could be applied to a projective de-
pendency parser. In the field of SMT, for an approx-
imation of MAP decoding, Li et al (2009) proposed
variational decoding and Kumar et al (2009) pre-
sented hypergraph MBR decoding. Our variational
model is inspired by the study of Li et al (2009) and
we apply it to a dependency parser in order to select
better candidates with third-order information. We
also propose an efficient algorithm to estimate the
non-local third-order model structure.
7 Conclusions
In this paper, we propose a novel forest reranking
algorithm for dependency parsing. Our reranking
algorithm is a combination approach of generative
reranking and variational decoding. The search al-
gorithm in the reranking stage can be performed
using dynamic programming algorithm. Our vari-
ational reranking is aimed at selecting a candidate
from a forest, which is correct both in local and
global. Our experimental results show more signif-
icant improvements than conventional approaches,
such as k-best and forest generative reranking.
In the future, we plan to investigate more ap-
propriate generative models for reranking. PP-
Attachment is one of the most difficult problems
for a natural language parser. We plan to exam-
ine to model such a complex structure (granduncle)
(Goldberg and Elhadad, 2010) or higher-order struc-
ture than third-order for reranking which is compu-
tationally expensive for a baseline parser. As we
mentioned in Section 5.4, we also plan to incorpo-
rate semi-supervised learning into our framework,
which may potentially improve our reranking per-
formance.
Acknowledgments
Wewould like to thank GrahamNeubig andMasashi
Shimbo for their helpful comments and to the anony-
mous reviewers for their effort of reviewing our pa-
per and giving valuable comments. This work was
supported in part by Grant-in-Aid for Japan Society
for the Promotion of Science (JSPS) Research Fel-
lowship for Young Scientists.
1487
References
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. the CoNLL-
EMNLP, pages 957?961.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. the 43rd ACL, pages 173?180.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33:201?228.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. the ICML.
J. M. Eisner. 1996a. An empirical comparison of prob-
ability models for dependency grammar. In Technical
Report, pages 1?18.
J. M. Eisner. 1996b. Three new probabilistic models for
dependency parsing: An exploration. In Proc. the 16th
COLING, pages 340?345.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. the IWPT, pages 53?64.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the ACL,
pages 1077?1086.
L. Huang. 2006. Dynamic programming al-
gorithms in semiring and hypergraph frame-
works. Qualification Exam Report, pages 1?19.
http://www.cis.upenn.edu/ lhuang3/wpe2/.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. the 46th ACL,
pages 586?594.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. the ACL,
pages 595?603.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. the 47th ACL, pages 163?171.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
the 47th ACL, pages 593?601.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic cfg with latent annotations. In Proc. the ACL, pages
75?82.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
the 43rd ACL, pages 91?98.
H. Mi and L. Huang. 2008. Forest-based translation rule
extraction. In Proceedings of EMNLP, pages 206?
214.
M. Mohri. 2002. Semiring framework and algorithms
for shortest-distance problems. Automata, Languages
and Combinatorics, 7:321?350.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. the 41st ACL, pages
160?167.
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
re-ranking model for dependency parsing. In Proc. the
11th IWPT, pages 238?241.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proc. the
EMNLP, pages 551?560.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. In Technical Report,
pages 1?9.
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
pendency forest for statistical machine translation. In
Proc. the 23rd COLING, pages 1092?1100.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
1488
Proceedings of the ACL 2010 Conference Short Papers, pages 98?102,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Structured Model for Joint Learning of
Argument Roles and Predicate Senses
Yotaro Watanabe
Graduate School of Information Sciences
Tohoku University
6-6-05, Aramaki Aza Aoba, Aoba-ku,
Sendai 980-8579, Japan
yotaro-w@ecei.tohoku.ac.jp
Masayuki Asahara Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma,
Nara, 630-0192, Japan
{masayu-a, matsu}@is.naist.jp
Abstract
In predicate-argument structure analysis,
it is important to capture non-local de-
pendencies among arguments and inter-
dependencies between the sense of a pred-
icate and the semantic roles of its argu-
ments. However, no existing approach ex-
plicitly handles both non-local dependen-
cies and semantic dependencies between
predicates and arguments. In this pa-
per we propose a structured model that
overcomes the limitation of existing ap-
proaches; the model captures both types of
dependencies simultaneously by introduc-
ing four types of factors including a global
factor type capturing non-local dependen-
cies among arguments and a pairwise fac-
tor type capturing local dependencies be-
tween a predicate and an argument. In
experiments the proposed model achieved
competitive results compared to the state-
of-the-art systems without applying any
feature selection procedure.
1 Introduction
Predicate-argument structure analysis is a process
of assigning who does what to whom, where,
when, etc. for each predicate. Arguments of a
predicate are assigned particular semantic roles,
such as Agent, Theme, Patient, etc. Lately,
predicate-argument structure analysis has been re-
garded as a task of assigning semantic roles of
arguments as well as word senses of a predicate
(Surdeanu et al, 2008; Hajic? et al, 2009).
Several researchers have paid much attention to
predicate-argument structure analysis, and the fol-
lowing two important factors have been shown.
Toutanova et al (2008), Johansson and Nugues
(2008), and Bjo?rkelund et al (2009) presented
importance of capturing non-local dependencies
of core arguments in predicate-argument structure
analysis. They used argument sequences tied with
a predicate sense (e.g. AGENT-buy.01/Active-
PATIENT) as a feature for the re-ranker of the
system where predicate sense and argument role
candidates are generated by their pipelined archi-
tecture. They reported that incorporating this type
of features provides substantial gain of the system
performance.
The other factor is inter-dependencies between
a predicate sense and argument roles, which re-
late to selectional preference, and motivated us
to jointly identify a predicate sense and its argu-
ment roles. This type of dependencies has been
explored by Riedel and Meza-Ruiz (2008; 2009b;
2009a), all of which use Markov Logic Networks
(MLN). The work uses the global formulae that
have atoms in terms of both a predicate sense and
each of its argument roles, and the system identi-
fies predicate senses and argument roles simulta-
neously.
Ideally, we want to capture both types of depen-
dencies simultaneously. The former approaches
can not explicitly include features that capture
inter-dependencies between a predicate sense and
its argument roles. Though these are implicitly in-
corporated by re-ranking where the most plausi-
ble assignment is selected from a small subset of
predicate and argument candidates, which are gen-
erated independently. On the other hand, it is dif-
ficult to deal with core argument features in MLN.
Because the number of core arguments varies with
the role assignments, this type of features cannot
be expressed by a single formula.
Thompson et al (2010) proposed a gener-
ative model that captures both predicate senses
and its argument roles. However, the first-order
markov assumption of the model eliminates abil-
ity to capture non-local dependencies among ar-
guments. Also, generative models are in general
inferior to discriminatively trained linear or log-
98
!!!
"#!
"$%!
"%!
"$!
&'!&(! &)!&*!&+!
,!
Figure 1: Undirected graphical model representa-
tion of the structured model
linear models.
In this paper we propose a structured model
that overcomes limitations of the previous ap-
proaches. For the model, we introduce several
types of features including those that capture both
non-local dependencies of core arguments, and
inter-dependencies between a predicate sense and
its argument roles. By doing this, both tasks are
mutually influenced, and the model determines
the most plausible set of assignments of a predi-
cate sense and its argument roles simultaneously.
We present an exact inference algorithm for the
model, and a large-margin learning algorithm that
can handle both local and global features.
2 Model
Figure 1 shows the graphical representation of our
proposed model. The node p corresponds to a
predicate, and the nodes a1, ..., aN to arguments
of the predicate. Each node is assigned a particu-
lar predicate sense or an argument role label. The
black squares are factors which provide scores of
label assignments. In the model, the nodes for ar-
guments depend on the predicate sense, and by in-
fluencing labels of a predicate sense and its argu-
ment roles, the most plausible label assignment of
the nodes is determined considering all factors.
In this work, we use linear models. Let x be
words in a sentence, p be a sense of a predicate in
x, and A = {an}N1 be a set of possible role label
assignments for x. A predicate-argument structure
is represented by a pair of p and A. We define
the score function for predicate-argument struc-
tures as s(p,A) =
?
Fk?F Fk(x, p,A). F is a
set of all the factors, Fk(x, p,A) corresponds to a
particular factor in Figure 1, and gives a score to a
predicate or argument label assignments. Since we
use linear models, Fk(x, p,A) = w ??k(x, p,A).
2.1 Factors of the Model
We define four types of factors for the model.
Predicate Factor FP scores a sense of p, and
does not depend on any arguments. The score
function is defined byFP (x, p,A) = w??P (x, p).
Argument Factor FA scores a label assignment
of a particular argument a ? A. The score is deter-
mined independently from a predicate sense, and
is given by FA(x, p, a) = w ? ?A(x, a).
Predicate-Argument Pairwise Factor
FPA captures inter-dependencies between
a predicate sense and one of its argument
roles. The score function is defined as
FPA(x, p, a) = w ? ?PA(x, p, a). The dif-
ference from FA is that FPA influences both
the predicate sense and the argument role. By
introducing this factor, the role label can be
influenced by the predicate sense, and vise versa.
Global Factor FG is introduced to capture plau-
sibility of the whole predicate-argument structure.
Like the other factors, the score function is de-
fined as FG(x, p,A) = w ? ?G(x, p,A). A pos-
sible feature that can be considered by this fac-
tor is the mutual dependencies among core argu-
ments. For instance, if a predicate-argument struc-
ture has an agent (A0) followed by the predicate
and a patient (A1), we encode the structure as a
string A0-PRED-A1 and use it as a feature. This
type of features provide plausibility of predicate-
argument structures. Even if the highest scoring
predicate-argument structure with the other factors
misses some core arguments, the global feature
demands the model to fill the missing arguments.
The numbers of factors for each factor type are:
FP and FG are 1, FA and FPA are |A|. By inte-
grating the all factors, the score function becomes
s(p,A) = w ? ?P (x, p) +w ? ?G(x, p,A) +w ?
?
a?A{?A(x, a) + ?PA(x, p, a)}.
2.2 Inference
The crucial point of the model is how to deal
with the global factor FG, because enumerating
possible assignments is too costly. A number of
methods have been proposed for the use of global
features for linear models such as (Daume? III
and Marcu, 2005; Kazama and Torisawa, 2007).
In this work, we use the approach proposed in
(Kazama and Torisawa, 2007). Although the ap-
proach is proposed for sequence labeling tasks, it
99
can be easily extended to our structured model.
That is, for each possible predicate sense p of the
predicate, we provide N-best argument role as-
signments using three local factors FP , FA and
FPA, and then add scores of the global factor FG,
finally select the argmax from them. In this case,
the argmax is selected from |Pl|N candidates.
2.3 Learning the Model
For learning of the model, we borrow a funda-
mental idea of Kazama and Torisawa?s perceptron
learning algorithm. However, we use a more so-
phisticated online-learning algorithm based on the
Passive-Aggressive Algorithm (PA) (Crammer et
al., 2006).
For the sake of simplicity, we introduce some
notations. We denote a predicate-argument struc-
ture y = ?p,A?, a local feature vector as
?L(x,y) = ?P (x, p) +
?
a?A{?A(x, a) +
?PA(x, p, a)}?a feature vector coupling both
local and global features as ?L+G(x,y) =
?L(x,y) + ?G(x, p,A), the argmax using ?L+G
as y?L+G, the argmax using ?L as y?L. Also, we
use a loss function ?(y,y?), which is a cost func-
tion associated with y and y?.
The margin perceptron learning proposed by
Kazama and Torisawa can be seen as an optimiza-
tion with the following two constrains.
(A) w??L+G(x,y)?w??L+G(x, y?L+G) ? ?(y, y?L+G)
(B) w ? ?L(x,y) ?w ? ?L(x, y?L) ? ?(y, y?L)
(A) is the constraint that ensures a sufficient
margin ?(y, y?L+G) between y and y?L+G. (B)
is the constraint that ensures a sufficient margin
?(y, y?L) between y and y?L. The necessity of
this constraint is that if we apply only (A), the al-
gorithm does not guarantee a sufficient margin in
terms of local features, and it leads to poor quality
in the N-best assignments. The Kazama and Tori-
sawa?s perceptron algorithm uses constant values
for the cost function ?(y, y?L+G) and ?(y, y?L).
The proposed model is trained using the follow-
ing optimization problem.
wnew = arg min
w??<n
1
2 ||w
? ?w||2 + C?
(
s.t. lL+G ? ?, ? ? 0 if y?L+G 6= y
s.t. lL ? ?, ? ? 0 if y?L+G = y 6= y?L
(1)
lL+G = w ? ?L+G(x, y?L+G)
?w ? ?L+G(x,y) + ?(y, y?L+G) (2)
lL = w ? ?L(x, y?L) ?w ? ?L(x,y) + ?(y, y?L) (3)
lL+G is the loss function for the case of using
both local and global features, corresponding to
the constraint (A), and lL is the loss function for
the case of using only local features, correspond-
ing to the constraints (B) provided that (A) is sat-
isfied.
2.4 The Role-less Argument Bias Problem
The fact that an argument candidate is not as-
signed any role (namely it is assigned the la-
bel ?NONE?) is unlikely to contribute pred-
icate sense disambiguation. However, it re-
mains possible that ?NONE? arguments is bi-
ased toward a particular predicate sense by FPA
(i.e. w ? ?PA(x, sensei, ak= ?NONE??) > w ?
?PA(x, sensej , ak= ?NONE??).
In order to avoid this bias, we define a spe-
cial sense label, senseany, that is used to cal-
culate the score for a predicate and a roll-less
argument, regardless of the predicate?s sense.
We use the feature vector ?PA(x, senseany, ak)
if ak= ?NONE?? and ?PA(x, sensei, ak) other-
wise.
3 Experiment
3.1 Experimental Settings
We use the CoNLL-2009 Shared Task dataset
(Hajic? et al, 2009) for experiments. It is a
dataset for multi-lingual syntactic and semantic
dependency parsing 1. In the SRL-only challenge
of the task, participants are required to identify
predicate-argument structures of only the specified
predicates. Therefore the problems to be solved
are predicate sense disambiguation and argument
role labeling. We use Semantic Labeled F1 for
evaluation.
For generating N-bests, we used the beam-
search algorithm, and the number of N-bests was
set to N = 64. For learning of the joint model, the
loss function ?(yt,y?) of the Passive-Aggressive
Algorithm was set to the number of incorrect as-
signments of a predicate sense and its argument
roles. Also, the number of iterations of the model
used for testing was selected based on the perfor-
mance on the development data.
Table 1 shows the features used for the struc-
tured model. The global features used for FG are
based on those used in (Toutanova et al, 2008;
Johansson and Nugues, 2008), and the features
1The dataset consists of seven languages: Catalan, Chi-
nese, Czech, English, German, Japanese and Spanish.
100
FP Plemma of the predicate and predicate?s head, and ppos of the predicate
Dependency label between the predicate and predicate?s head
The concatenation of the dependency labels of the predicate?s dependents
FA Plemma and ppos of the predicate, the predicate?s head, the argument candidate, and the argument?s head
Plemma and ppos of the leftmost/rightmost dependent and leftmost/rightmost sibling
The dependency label of predicate, argument candidate and argument candidate?s dependent
The position of the argument candidate with respect to the predicate position in the dep. tree (e.g. CHILD)
The position of the head of the dependency relation with respect to the predicate position in the sentence
The left-to-right chain of the deplabels of the predicate?s dependents
Plemma, ppos and dependency label paths between the predicate and the argument candidates
The number of dependency edges between the predicate and the argument candidate
FPA Plemma and plemma&ppos of the argument candidate
Dependency label path between the predicate and the argument candidates
FG The sequence of the predicate and the argument labels in the predicate-argument structure (e.g. A0-PRED-A1?
Whether the semantic roles defined in frames exist in the structure, (e.g. CONTAINS:A1)
The conjunction of the predicate sense and the frame information (e.g. wear.01&CONTAINS:A1)
Table 1: Features for the Structured Model
Avg. Ca Ch Cz En Ge Jp Sp
FP+FA 79.17 78.00 76.02 85.24 83.09 76.76 77.27 77.83
FP+FA+FPA 79.58 78.38 76.23 85.14 83.36 78.31 77.72 77.92
FP+FA+FG 80.42 79.50 76.96 85.88 84.49 78.64 78.32 79.21
ALL 80.75 79.55 77.20 85.94 84.97 79.62 78.69 79.29
Bjo?rkelund 80.80 80.01 78.60 85.41 85.63 79.71 76.30 79.91
Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
Table 2: Results on the CoNLL-2009 Shared Task dataset (Semantic Labeled F1).
SENSE ARG
FP+FA 89.65 72.20
FP+FA+FPA 89.78 72.74
FP+FA+FG 89.83 74.11
ALL 90.15 74.46
Table 3: Predicate sense disambiguation and argu-
ment role labeling results (average).
used for FPA are inspired by formulae used in
the MLN-based SRL systems, such as (Meza-Ruiz
and Riedel, 2009b). We used the same feature
templates for all languages.
3.2 Results
Table 2 shows the results of the experiments, and
also shows the results of the top 3 systems in the
CoNLL-2009 Shared Task participants of the SRL-
only system.
By incorporating FPA, we achieved perfor-
mance improvement for all languages. This results
suggest that it is effective to capture local inter-
dependencies between a predicate sense and one
of its argument roles. Comparing the results with
FP+FA and FP+FA+FG, incorporating FG also
contributed performance improvements for all lan-
guages, especially the substantial F1 improvement
of +1.88 is obtained in German.
Next, we compare our system with top 3 sys-
tems in the CoNLL-2009 Shared Task. By in-
corporating both FPA and FG, our joint model
achieved competitive results compared to the top 2
systems (Bjo?rkelund and Zhao), and achieved the
better results than the Meza-Ruiz?s system 2. The
systems by Bjo?rkelund and Zhao applied feature
selection algorithms in order to select the best set
of feature templates for each language, requiring
about 1 to 2 months to obtain the best feature set.
On the other hand, our system achieved the com-
petitive results with the top two systems, despite
the fact that we used the same feature templates
for all languages without applying any feature en-
gineering procedure.
Table 3 shows the performances of predicate
sense disambiguation and argument role labeling
separately. In terms of sense disambiguation re-
sults, incorporating FPA and FG worked well. Al-
though incorporating either of FPA and FG pro-
vided improvements of +0.13 and +0.18 on av-
erage, adding both factors provided improvements
of +0.50. We compared the predicate sense dis-
2The result of Meza-Ruiz for Czech is substantially worse
than the other systems because of inappropriate preprocess-
ing for predicate sense disambiguation. Excepting Czech, the
average F1 value of the Meza-Ruiz is 77.75, where as our
system is 79.89.
101
ambiguation results of FP +FA and ALL with the
McNemar test, and the difference was statistically
significant (p < 0.01). This result suggests that
combination of these factors is effective for sense
disambiguation.
As for argument role labeling results, incorpo-
rating FPA and FG contributed positively for all
languages. Especially, we obtained a substan-
tial gain (+4.18) in German. By incorporating
FPA, the system achieved the F1 improvements
of +0.54 on average. This result shows that cap-
turing inter-dependencies between a predicate and
its arguments contributes to argument role label-
ing. By incorporating FG, the system achieved the
substantial improvement of F1 (+1.91).
Since both tasks improved by using all factors,
we can say that the proposed joint model suc-
ceeded in joint learning of predicate senses and
its argument roles.
4 Conclusion
In this paper, we proposed a structured model that
captures both non-local dependencies between ar-
guments, and inter-dependencies between a pred-
icate sense and its argument roles. We designed
a linear model-based structured model, and de-
fined four types of factors: predicate factor, ar-
gument factor, predicate-argument pairwise fac-
tor and global factor for the model. In the ex-
periments, the proposed model achieved compet-
itive results compared to the state-of-the-art sys-
tems without any feature engineering.
A further research direction we are investi-
gating is exploitation of unlabeled texts. Semi-
supervised semantic role labeling methods have
been explored by (Collobert and Weston, 2008;
Deschacht and Moens, 2009; Fu?rstenau and La-
pata, 2009), and they have achieved successful
outcomes. However, we believe that there is still
room for further improvement.
References
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In
CoNLL-2009.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML
2008.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JMLR, 7:551?585.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In ICML-2005.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In EMNLP-2009.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In EMNLP-2009.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009, Boul-
der, Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL-2008.
Jun?Ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with
non-local features. In EMNLP-CoNLL 2007.
Ivan Meza-Ruiz and Sebastian Riedel. 2009a. Jointly
identifying predicates, arguments and senses using
markov logic. In HLT/NAACL-2009.
Ivan Meza-Ruiz and Sebastian Riedel. 2009b. Multi-
lingual semantic role labelling with markov logic.
In CoNLL-2009.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with markov logic. In
CoNLL-2008.
Mihai Surdeanu, Richard Johansson, Adam Mey-
ers, Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In CoNLL-2008.
Synthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2010. A generative model for semantic
role labeling. In Proceedings of the 48th Annual
Meeting of the Association of Computational Lin-
guistics (to appear).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2).
102
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 657?665,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-driven Transition-based Parsing with Top-down Prediction
Katsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Institute of Science and Technology
Ikoma, Nara, 630-0192, Japan
?National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, Japan
?National Institute for Japanese Language and Linguistics
Tachikawa, Tokyo, 190-8561, Japan
katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp
masayu-a@ninjal.ac.jp, matsu@is.naist.jp
Abstract
This paper presents a novel top-down head-
driven parsing algorithm for data-driven pro-
jective dependency analysis. This algorithm
handles global structures, such as clause and
coordination, better than shift-reduce or other
bottom-up algorithms. Experiments on the
English Penn Treebank data and the Chinese
CoNLL-06 data show that the proposed algo-
rithm achieves comparable results with other
data-driven dependency parsing algorithms.
1 Introduction
Transition-based parsing algorithms, such as shift-
reduce algorithms (Nivre, 2004; Zhang and Clark,
2008), are widely used for dependency analysis be-
cause of the efficiency and comparatively good per-
formance. However, these parsers have one major
problem that they can handle only local information.
Isozaki et al (2004) pointed out that the drawbacks
of shift-reduce parser could be resolved by incorpo-
rating top-down information such as root finding.
This work presents an O(n2) top-down head-
driven transition-based parsing algorithm which can
parse complex structures that are not trivial for shift-
reduce parsers. The deductive system is very similar
to Earley parsing (Earley, 1970). The Earley predic-
tion is tied to a particular grammar rule, but the pro-
posed algorithm is data-driven, following the current
trends of dependency parsing (Nivre, 2006; McDon-
ald and Pereira, 2006; Koo et al, 2010). To do the
prediction without any grammar rules, we introduce
a weighted prediction that is to predict lower nodes
from higher nodes with a statistical model.
To improve parsing flexibility in deterministic
parsing, our top-down parser uses beam search al-
gorithm with dynamic programming (Huang and
Sagae, 2010). The complexity becomes O(n2 ? b)
where b is the beam size. To reduce prediction er-
rors, we propose a lookahead technique based on a
FIRST function, inspired by the LL(1) parser (Aho
and Ullman, 1972). Experimental results show that
the proposed top-down parser achieves competitive
results with other data-driven parsing algorithms.
2 Definition of Dependency Graph
A dependency graph is defined as follows.
Definition 2.1 (Dependency Graph) Given an in-
put sentence W = n0 . . .nn where n0 is a spe-
cial root node $, a directed graph is defined as
GW = (VW , AW ) where VW = {0, 1, . . . , n} is a
set of (indices of) nodes and AW ? VW ? VW is a
set of directed arcs. The set of arcs is a set of pairs
(x, y) where x is a head and y is a dependent of x.
x ?? l denotes a path from x to l. A directed graph
GW = (VW , AW ) is well-formed if and only if:
? There is no node x such that (x, 0) ? AW .
? If (x, y) ? AW then there is no node x? such
that (x?, y) ? AW and x? ?= x.
? There is no subset of arcs {(x0, x1), (x1, x2),
. . . , (xl?1, xl)} ? AW such that x0 = xl.
These conditions are refered to ROOT, SINGLE-
HEAD, and ACYCLICITY, and we call an well-
formed directed graph as a dependency graph.
Definition 2.2 (PROJECTIVITY) A dependency
graph GW = (VW , AW ) is projective if and only if,
657
input: W = n0 . . .nn
axiom(p0): 0 : ?1, 0, n + 1,n0? : ?
predx:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
?k : i ? k < h
predy:
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, j, sd?1|...|s0|nk? : {p}
?k : i ? k < j ? h < i
scan:
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i + 1, h, j, sd|...|s0? : pi
i = h
comp:
state q
? ?? ?
: ? , h?, j?, s?d|...|s?0? : pi?
state p
? ?? ?
? : ?i, h, j, sd|...|s0? : pi
? + 1 : ?i, h?, j?, s?d|...|s?1|s?0ys0? : pi?
q ? pi, h < i
goal: 3n : ?n + 1, 0, n + 1, s0? : ?
Figure 1: The non-weighted deductive system of top-down dependency parsing algorithm: means ?take anything?.
for every arc (x, y) ? AW and node l in x < l < y
or y < l < x, there is a path x ?? l or y ?? l.
The proposed algorithm in this paper is for projec-
tive dependency graphs. If a projective dependency
graph is connected, we call it a dependency tree,
and if not, a dependency forest.
3 Top-down Parsing Algorithm
Our proposed algorithm is a transition-based algo-
rithm, which uses stack and queue data structures.
This algorithm formally uses the following state:
? : ?i, h, j, S? : pi
where ? is a step size, S is a stack of trees sd|...|s0
where s0 is a top tree and d is a window size for
feature extraction, i is an index of node on the top
of the input node queue, h is an index of root node
of s0, j is an index to indicate the right limit (j ?
1 inclusive) of predy, and pi is a set of pointers to
predictor states, which are states just before putting
the node in h onto stack S. In the deterministic case,
pi is a singleton set except for the initial state.
This algorithm has four actions, predictx(predx),
predicty(predy), scan and complete(comp). The
deductive system of the top-down algorithm is
shown in Figure 1. The initial state p0 is a state ini-
tialized by an artificial root node n0. This algorithm
applies one action to each state selected from appli-
cable actions in each step. Each of three kinds of
actions, pred, scan, and comp, occurs n times, and
this system takes 3n steps for a complete analysis.
Action predx puts nk onto stack S selected from
the input queue in the range, i ? k < h, which is
to the left of the root nh in the stack top. Similarly,
action predy puts a node nk onto stack S selected
from the input queue in the range, h < i ? k < j,
which is to the right of the root nh in the stack top.
The node ni on the top of the queue is scanned if it
is equal to the root node nh in the stack top. Action
comp creates a directed arc (h?, h) from the root h?
of s?0 on a predictor state q to the root h of s0 on a
current state p if h < i 1.
The precondition i < h of action predx means
that the input nodes in i ? k < h have not been
predicted yet. Predx, scan and predy do not con-
flict with each other since their preconditions i < h,
i = h and h < i do not hold at the same time.
However, this algorithm faces a predy-comp con-
flict because both actions share the same precondi-
tion h < i, which means that the input nodes in
1 ? k ? h have been predicted and scanned. This
1In a single root tree, the special root symbol $0 has exactly
one child node. Therefore, we do not apply comp action to a
state if its condition satisfies s1.h = n0 ? ? ?= 3n? 1.
658
step state stack queue action state information
0 p0 $0 I1 saw2 a3 girl4 ? ?1, 0, 5? : ?
1 p1 $0|saw2 I1 saw2 a3 girl4 predy ?1, 2, 5? : {p0}
2 p2 saw2|I1 I1 saw2 a3 girl4 predx ?1, 1, 2? : {p1}
3 p3 saw2|I1 saw2 a3 girl4 scan ?2, 1, 2? : {p1}
4 p4 $0|I1xsaw2 saw2 a3 girl4 comp ?2, 2, 5? : {p0}
5 p5 $0|I1xsaw2 a3 girl4 scan ?3, 2, 5? : {p0}
6 p6 I1xsaw2|girl4 a3 girl4 predy ?3, 4, 5? : {p5}
7 p7 girl4|a3 a3 girl4 predx ?3, 3, 4? : {p6}
8 p8 girl4|a3 girl4 scan ?4, 3, 4? : {p6}
9 p9 I1xsaw2|a3xgirl4 girl4 comp ?4, 4, 5? : {p5}
10 p10 I1xsaw2|a3xgirl4 scan ?5, 4, 5? : {p5}
11 p11 $0|I1xsaw2ygirl4 comp ?5, 2, 5? : {p0}
12 p12 $0ysaw2 comp ?5, 0, 5? : ?
Figure 2: Stages of the top-down deterministic parsing process for a sentence ?I saw a girl?. We follow a convention
and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example,
we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1.
parser constructs left and right children of a head
node in a left-to-right direction by scanning the head
node prior to its right children. Figure 2 shows an
example for parsing a sentence ?I saw a girl?.
4 Correctness
To prove the correctness of the system in Figure
1 for the projective dependency graph, we use the
proof strategy of (Nivre, 2008a). The correct deduc-
tive system is both sound and complete.
Theorem 4.1 The deductive system in Figure 1 is
correct for the class of dependency forest.
Proof 4.1 To show soundness, we show that Gp0 =
(VW , ?), which is a directed graph defined by the
axiom, is well-formed and projective, and that every
transition preserves this property.
? ROOT: The node 0 is a root in Gp0 , and the
node 0 is on the top of stack of p0. The two pred
actions put a word onto the top of stack, and
predict an arc from root or its descendant to
the child. The comp actions add the predicted
arcs which include no arc of (x, 0).
? SINGLE-HEAD: Gp0 is single-head. A node
y is no longer in stack and queue after a comp
action creates an arc (x, y). The node y cannot
make any arc (x?, y) after the removal.
? ACYCLICITY: Gp0 is acyclic. A cycle is cre-
ated only if an arc (x, y) is added when there
is a directed path y ?? x. The node x is no
longer in stack and queue when the directed
path y ?? x was made by adding an arc (l, x).
There is no chance to add the arc (x, y) on the
directed path y ?? x.
? PROJECTIVITY: Gp0 is projective. Projec-
tivity is violated by adding an arc (x, y) when
there is a node l in x < l < y or y < l < x
with the path to or from the outside of the span
x and y. When predy creates an arc relation
from x to y, the node y cannot be scanned be-
fore all nodes l in x < l < y are scanned and
completed. When predx creates an arc rela-
tion from x to y, the node y cannot be scanned
before all nodes k in k < y are scanned and
completed, and the node x cannot be scanned
before all nodes l in y < l < x are scanned
and completed. In those processes, the node l
in x < l < y or y < l < x does not make a
path to or from the outside of the span x and y,
and a path x ?? l or y ?? l is created. 2
To show completeness, we show that for any sen-
tence W , and dependency forest GW = (VW , AW ),
there is a transition sequence C0,m such that Gpm =
GW by an inductive method.
? If |W | = 1, the projective dependency graph
for W is GW = ({0}, ?) and Gp0 = GW .
? Assume that the claim holds for sentences with
length less or equal to t, and assume that
|W | = t + 1 and GW = (VW , AW ). The sub-
graph GW ? is defined as (VW ? t, A?t) where
659
..
.s2 h
.
.. . .
. .
.
.
.s1 h
.. . .
.. . . .
..s1.l
.. . . .
.. . .
.. . . .
..s1.r
. .. . .
.
.
.s0 h
.. . .
.. . . .
..s0.l
.. . . .
.. . .
.. . . .
..s0.r
. .. . .
Figure 3: Feature window of trees on stack S: The win-
dow size d is set to 2. Each x.h, x.l and x.r denotes root,
left and right child nodes of a stack element x.
A?t = AW ?{(x, y)|x = t? y = t}. If GW is
a dependency forest, then GW ? is also a depen-
dency forest. It is obvious that there is a transi-
tion sequence for constructing GW except arcs
which have a node t as a head or a dependent2.
There is a state pq = q : ?i, x, t + 1? :
for i and x (0 ? x < i < t + 1). When
x is the head of t, predy to t creates a state
pq+1 = q + 1 : ?i, t, t + 1? : {pq}. At least one
node y in i ? y < t becomes the dependent of
t by predx and there is a transition sequence
for constructing a tree rooted by y. After con-
structing a subtree rooted by t and spaned from
i to t, t is scaned, and then comp creates an
arc from x to t. It is obvious that the remaining
transition sequence exists. Therefore, we can
construct a transition sequence C0,m such that
Gpm = GW . 2
The deductive sysmtem in Figure 1 is both sound and
complete. Therefore, it is correct. 2
5 Weighted Parsing Model
5.1 Stack-based Model
The proposed algorithm employs a stack-based
model for scoring hypothesis. The cost of the model
is defined as follows:
cs(i, h, j, S) = ?s ? fs,act(i, h, j, S) (1)
where ?s is a weight vector, fs is a feature function,
and act is one of the applicable actions to a state ? :
?i, h, j, S? : pi. We use a set of feature templates of
(Huang and Sagae, 2010) for the model. As shown
in Figure 3, left children s0.l and s1.l of trees on
2This transition sequence is defined for GW ? , but it is pos-
sible to be regarded as the definition for GW as long as the
transition sequence is indifferent from the node t.
Algorithm 1 Top-down Parsing with Beam Search
1: input W = n0, . . . ,nn
2: start? ?1, 0, n + 1,n0?
3: buf [0]? {start}
4: for ?? 1 . . . 3n do
5: hypo? {}
6: for each state in buf [?? 1] do
7: for act?applicableAct(state) do
8: newstates?actor(act, state)
9: addAll newstates to hypo
10: add top b states to buf [?] from hypo
11: return best candidate from buf [3n]
stack for extracting features are different from those
of Huang and Sagae (2010) because in our parser the
left children are generated from left to right.
As mentioned in Section 1, we apply beam search
and Huang and Sagae (2010)?s DP techniques to
our top-down parser. Algorithm 1 shows the our
beam search algorithm in which top most b states
are preserved in a buffer buf [?] in each step. In
line 10 of Algorithm 1, equivalent states in the step
? are merged following the idea of DP. Two states
?i, h, j, S? and ?i?, h?, j?, S?? in the step ? are equiv-
alent, notated ?i, h, j, S? ? ?i?, h?, j?, S??, iff
fs,act(i, h, j, S) = fs,act(i?, h?, j?, S?). (2)
When two equivalent predicted states are merged,
their predictor states in pi get combined. For fur-
ther details about this technique, readers may refer
to (Huang and Sagae, 2010).
5.2 Weighted Prediction
The step 0 in Figure 2 shows an example of predic-
tion for a head node ?$0?, where the node ?saw2? is
selected as its child node. To select a probable child
node, we define a statistical model for the prediction.
In this paper, we integrate the cost from a graph-
based model (McDonald and Pereira, 2006) which
directly models dependency links. The cost of the
1st-order model is defined as the relation between a
child node c and a head node h:
cp(h, c) = ?p ? fp(h, c) (3)
where ?p is a weight vector and fp is a features func-
tion. Using the cost cp, the top-down parser selects
a probable child node in each prediction step.
When we apply beam search to the top-down
parser, then we no longer use ? but ? on predx and
660
..
.h
..l1 . .. . . . ..ll . ..r1 . .. . . . ..rm
Figure 4: An example of tree structure: Each h, l and r
denotes head, left and right child nodes.
predy in Figure 1. Therefore, the parser may predict
many nodes as an appropriate child from a single
state, causing many predicted states. This may cause
the beam buffer to be filled only with the states, and
these may exclude other states, such as scanned or
completed states. Thus, we limit the number of pre-
dicted states from a single state by prediction size
implicitly in line 10 of Algorithm 1.
To improve the prediction accuracy, we introduce
a more sophisticated model. The cost of the sibling
2nd-order model is defined as the relationship be-
tween c, h and a sibling node sib:
cp(h, sib, c) = ?p ? fp(h, sib, c). (4)
The 1st- and sibling 2nd-order models are the same
as McDonald and Pereira (2006)?s definitions, ex-
cept the cost factors of the sibling 2nd-order model.
The cost factors for a tree structure in Figure 4 are
defined as follows:
cp(h,?, l1) +
l?1
?
y=1
cp(h, ly, ly+1)
+cp(h,?, r1) +
m?1
?
y=1
cp(h, ry, ry+1).
This is different from McDonald and Pereira (2006)
in that the cost factors for left children are calcu-
lated from left to right, while those in McDonald and
Pereira (2006)?s definition are calculated from right
to left. This is because our top-down parser gener-
ates left children from left to right. Note that the
cost of weighted prediction model in this section is
incrementally calculated by using only the informa-
tion on the current state, thus the condition of state
merge in Equation 2 remains unchanged.
5.3 Weighted Deductive System
We extend deductive system to a weighted one, and
introduce forward cost and inside cost (Stolcke,
1995; Huang and Sagae, 2010). The forward cost is
the total cost of a sequence from an initial state to the
end state. The inside cost is the cost of a top tree s0
in stack S. We define these costs using a combina-
tion of stack-based model and weighted prediction
model. The forward and inside costs of the combi-
nation model are as follows:
{
cfw = cfws + cfwp
cin = cins + cinp
(5)
where cfws and cins are a forward cost and an inside
cost for stack-based model, and cfwp and cinp are a for-
ward cost and an inside cost for weighted prediction
model. We add the following tuple of costs to a state:
(cfws , cins , cfwp , cinp ).
For each action, we define how to efficiently cal-
culate the forward and inside costs3 , following Stol-
cke (1995) and Huang and Sagae (2010)?s works. In
either case of predx or predy,
(cfws , , cfwp , )
(cfws + ?, 0, cfwp + cp(s0.h,nk), 0)
where
? =
{
?s ? fs,predx(i, h, j, S) if predx
?s ? fs,predy(i, h, j, S) if predy
(6)
In the case of scan,
(cfws , cins , cfwp , cinp )
(cfws + ?, cins + ?, cfwp , cinp )
where
? = ?s ? fs,scan(i, h, j, S). (7)
In the case of comp,
(c?fws , c?
in
s , c?
fw
p , c?
in
p ) (cfws , cins , cfwp , cinp )
(c?fws + cins + ?, c?ins + cins + ?,
c?fwp + cinp + cp(s?0.h, s0.h),
c?inp + cinp + cp(s?0.h, s0.h))
where
? = ?s ? fs,comp(i, h, j, S) + ?s ? fs,pred ( , h?, j?, S?).
(8)
3For brevity, we present the formula not by 2nd-order model
as equation 4 but a 1st-order one for weighted prediction.
661
Pred takes either predx or predy. Beam search is
performed based on the following linear order for
the two states p and p? at the same step, which have
(cfw, cin) and (c?fw, c?in) respectively:
p ? p? iff cfw < c?fw or cfw = c?fw ? cin < c?in. (9)
We prioritize the forward cost over the inside cost
since forward cost pertains to longer action sequence
and is better suited to evaluate hypothesis states than
inside cost (Nederhof, 2003).
5.4 FIRST Function for Lookahead
Top-down backtrack parser usually reduces back-
tracking by precomputing the set FIRST(?) (Aho and
Ullman, 1972). We define the set FIRST(?) for our
top-down dependency parser:
FIRST(t?) = {ld.t|ld ? lmdescendant(Tree, t?)
Tree ? Corpus} (10)
where t? is a POS-tag, Tree is a correct depen-
dency tree which exists in Corpus, a function
lmdescendant(Tree, t?) returns the set of the leftmost
descendant node ld of each nodes in Tree whose
POS-tag is t?, and ld.t denotes a POS-tag of ld.
Though our parser does not backtrack, it looks ahead
when selecting possible child nodes at the prediction
step by using the function FIRST. In case of predx:
?k : i ? k < h ? ni.t ? FIRST(nk.t)
state p
? ?? ?
? : ?i, h, j, sd|...|s0? :
? + 1 : ?i, k, h, sd?1|...|s0|nk? : {p}
where ni.t is a POS-tag of the node ni on the top of
the queue, and nk.t is a POS-tag in kth position of
an input nodes. The case for predy is the same. If
there are no nodes which satisfy the condition, our
top-down parser creates new states for all nodes, and
pushes them into hypo in line 9 of Algorithm 1.
6 Time Complexity
Our proposed top-down algorithm has three kinds
of actions which are scan, comp and predict. Each
scan and comp actions occurs n times when parsing
a sentence with the length n. Predict action also oc-
curs n times in which a child node is selected from
a node sequence in the input queue. Thus, the algo-
rithm takes the following times for prediction:
n + (n? 1) + ? ? ? + 1 =
n
?
i
i = n(n + 1)2 . (11)
As n2 for prediction is the most dominant factor, the
time complexity of the algorithm is O(n2) and that
of the algorithm with beam search is O(n2 ? b).
7 Related Work
Alshawi (1996) proposed head automaton which
recognizes an input sentence top-down. Eisner
and Satta (1999) showed that there is a cubic-time
parsing algorithm on the formalism of the head
automaton grammars, which are equivalently con-
verted into split-head bilexical context-free gram-
mars (SBCFGs) (McAllester, 1999; Johnson, 2007).
Although our proposed algorithm does not employ
the formalism of SBCFGs, it creates left children
before right children, implying that it does not have
spurious ambiguities as well as parsing algorithms
on the SBCFGs. Head-corner parsing algorithm
(Kay, 1989) creates dependency tree top-down, and
in this our algorithm has similar spirit to it.
Yamada and Matsumoto (2003) applied a shift-
reduce algorithm to dependency analysis, which is
known as arc-standard transition-based algorithm
(Nivre, 2004). Nivre (2003) proposed another
transition-based algorithm, known as arc-eager al-
gorithm. The arc-eager algorithm processes right-
dependent top-down, but this does not involve the
prediction of lower nodes from higher nodes. There-
fore, the arc-eager algorithm is a totally bottom-up
algorithm. Zhang and Clark (2008) proposed a com-
bination approach of the transition-based algorithm
with graph-based algorithm (McDonald and Pereira,
2006), which is the same as our combination model
of stack-based and prediction models.
8 Experiments
Experiments were performed on the English Penn
Treebank data and the Chinese CoNLL-06 data. For
the English data, we split WSJ part of it into sections
02-21 for training, section 22 for development and
section 23 for testing. We used Yamada and Mat-
sumoto (2003)?s head rules to convert phrase struc-
ture to dependency structure. For the Chinese data,
662
time accuracy complete root
McDonald05,06 (2nd) 0.15 90.9, 91.5 37.5, 42.1 ?
Koo10 (Koo and Collins, 2010) ? 93.04 ? ?
Hayashi11 (Hayashi et al, 2011) 0.3 92.89 ? ?
2nd-MST? 0.13 92.3 43.7 96.0
Goldberg10 (Goldberg and Elhadad, 2010) ? 89.7 37.5 91.5
Kitagawa10 (Kitagawa and Tanaka-Ishii, 2010) ? 91.3 41.7 ?
Zhang08 (Sh beam 64) ? 91.4 41.8 ?
Zhang08 (Sh+Graph beam 64) ? 92.1 45.4 ?
Huang10 (beam+DP) 0.04 92.1 ? ?
Huang10? (beam 8, 16, 32+DP) 0.03, 0.06, 0.10 92.3, 92.27, 92.26 43.5, 43.7, 43.8 96.0, 96.0, 96.1
Zhang11 (beam 64) (Zhang and Nivre, 2011) ? 93.07 49.59 ?
top-down? (beam 8, 16, 32+pred 5+DP) 0.07, 0.12, 0.22 91.7, 92.3, 92.5 45.0, 45.7, 45.9 94.5, 95.7, 96.2
top-down? (beam 8, 16, 32+pred 5+DP+FIRST) 0.07, 0.12, 0.22 91.9, 92.4, 92.6 45.0, 45.3, 45.5 95.1, 96.2, 96.6
Table 1: Results for test data: Time measures the parsing time per sentence in seconds. Accuracy is an unlabeled
attachment score, complete is a sentence complete rate, and root is a correct root rate. ? indicates our experiments.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70
pa
rs
in
g 
tim
e 
(cp
u s
ec
)
length of input sentence
"shift-reduce"
"2nd-mst"
"top-down"
Figure 5: Scatter plot of parsing time against sentence
length, comparing with top-down, 2nd-MST and shift-
reduce parsers (beam size: 8, pred size: 5)
we used the information of words and fine-grained
POS-tags for features. We also implemented and ex-
perimented Huang and Sagae (2010)?s arc-standard
shift-reduce parser. For the 2nd-order Eisner-Satta
algorithm, we used MSTParser (McDonald, 2012).
We used an early update version of averaged per-
ceptron algorithm (Collins and Roark, 2004) for
training of shift-reduce and top-down parsers. A
set of feature templates in (Huang and Sagae, 2010)
were used for the stack-based model, and a set of
feature templates in (McDonald and Pereira, 2006)
were used for the 2nd-order prediction model. The
weighted prediction and stack-based models of top-
down parser were jointly trained.
8.1 Results for English Data
During training, we fixed the prediction size and
beam size to 5 and 16, respectively, judged by pre-
accuracy complete root
oracle (sh+mst) 94.3 52.3 97.7
oracle (top+sh) 94.2 51.7 97.6
oracle (top+mst) 93.8 50.7 97.1
oracle (top+sh+mst) 94.9 55.3 98.1
Table 2: Oracle score, choosing the highest accuracy
parse for each sentence on test data from results of top-
down (beam 8, pred 5) and shift-reduce (beam 8) and
MST(2nd) parsers in Table 1.
accuracy complete root
top-down (beam:8, pred:5) 90.9 80.4 93.0
shift-reduce (beam:8) 90.8 77.6 93.5
2nd-MST 91.4 79.3 94.2
oracle (sh+mst) 94.0 85.1 95.9
oracle (top+sh) 93.8 84.0 95.6
oracle (top+mst) 93.6 84.2 95.3
oracle (top+sh+mst) 94.7 86.5 96.3
Table 3: Results for Chinese Data (CoNLL-06)
liminary experiments on development data. After
25 iterations of perceptron training, we achieved
92.94 unlabeled accuracy for top-down parser with
the FIRST function and 93.01 unlabeled accuracy
for shift-reduce parser on development data by set-
ting the beam size to 8 for both parsers and the pre-
diction size to 5 in top-down parser. These trained
models were used for the following testing.
We compared top-down parsing algorithm with
other data-driven parsing algorithms in Table 1.
Top-down parser achieved comparable unlabeled ac-
curacy with others, and outperformed them on the
sentence complete rate. On the other hand, top-
down parser was less accurate than shift-reduce
663
No.717 Little Lily , as Ms. Cunningham calls7 herself in the book , really was14 n?t ordinary .
shift-reduce 2 7 2 2 6 4 14 7 7 11 9 7 14 0 14 14 14
2nd-MST 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
top-down 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
correct 2 14 2 2 6 7 4 7 7 11 9 2 14 0 14 14 14
No.127 resin , used to make garbage bags , milk jugs , housewares , toys and meat packaging25 , among other items .
shift-reduce 25 9 9 13 11 15 13 25 18 25 25 25 25 25 25 25 7 25 25 29 27 4
2nd-MST 29 9 9 13 11 15 13 29 18 29 29 29 29 25 25 25 29 25 25 29 7 4
top-down 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
correct 7 9 9 13 11 15 25 25 18 25 25 25 25 25 25 25 13 25 25 29 27 4
Table 4: Two examples on which top-down parser is superior to two bottom-up parsers: In correct analysis, the boxed
portion is the head of the underlined portion. Bottom-up parsers often mistake to capture the relation.
parser on the correct root measure. In step 0, top-
down parser predicts a child node, a root node of
a complete tree, using little syntactic information,
which may lead to errors in the root node selection.
Therefore, we think that it is important to seek more
suitable features for the prediction in future work.
Figure 5 presents the parsing time against sen-
tence length. Our proposed top-down parser is the-
oretically slower than shift-reduce parser and Fig-
ure 5 empirically indicates the trends. The domi-
nant factor comes from the score calculation, and
we will leave it for future work. Table 2 shows
the oracle score for test data, which is the score
of the highest accuracy parse selected for each sen-
tence from results of several parsers. This indicates
that the parses produced by each parser are differ-
ent from each other. However, the gains obtained by
the combination of top-down and 2nd-MST parsers
are smaller than other combinations. This is because
top-down parser uses the same features as 2nd-MST
parser, and these are more effective than those of
stack-based model. It is worth noting that as shown
in Figure 5, our O(n2?b) (b = 8) top-down parser is
much faster than O(n3) Eisner-Satta CKY parsing.
8.2 Results for Chinese Data (CoNLL-06)
We also experimented on the Chinese data. Fol-
lowing English experiments, shift-reduce parser was
trained by setting beam size to 16, and top-down
parser was trained with the beam size and the predic-
tion size to 16 and 5, respectively. Table 3 shows the
results on the Chinese test data when setting beam
size to 8 for both parsers and prediction size to 5 in
top-down parser. The trends of the results are almost
the same as those of the English results.
8.3 Analysis of Results
Table 4 shows two interesting results, on which top-
down parser is superior to either shift-reduce parser
or 2nd-MST parser. The sentence No.717 contains
an adverbial clause structure between the subject
and the main verb. Top-down parser is able to han-
dle the long-distance dependency while shift-reudce
parser cannot correctly analyze it. The effectiveness
on the clause structures implies that our head-driven
parser may handle non-projective structures well,
which are introduced by Johansonn?s head rule (Jo-
hansson and Nugues, 2007). The sentence No.127
contains a coordination structure, which it is diffi-
cult for bottom-up parsers to handle, but, top-down
parser handles it well because its top-down predic-
tion globally captures the coordination.
9 Conclusion
This paper presents a novel head-driven parsing al-
gorithm and empirically shows that it is as practi-
cal as other dependency parsing algorithms. Our
head-driven parser has potential for handling non-
projective structures better than other non-projective
dependency algorithms (McDonald et al, 2005; At-
tardi, 2006; Nivre, 2008b; Koo et al, 2010). We are
in the process of extending our head-driven parser
for non-projective structures as our future work.
Acknowledgments
We would like to thank Kevin Duh for his helpful
comments and to the anonymous reviewers for giv-
ing valuable comments.
664
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation and Compiling, volume 1: Parsing.
Prentice-Hall.
H. Alshawi. 1996. Head automata for speech translation.
In Proc. the ICSLP.
G. Attardi. 2006. Experiments with a multilanguage
non-projective dependency parser. In Proc. the 10th
CoNLL, pages 166?170.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. the 42nd ACL.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94?102.
J. M. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. the 37th ACL, pages 457?464.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proc. the HLT-NAACL, pages 742?750.
K. Hayashi, T. Watanabe, M. Asahara, and Y. Mat-
sumoto. 2011. The third-order variational rerank-
ing on packed-shared dependency forests. In Proc.
EMNLP, pages 1479?1488.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. the 48th
ACL, pages 1077?1086.
H. Isozaki, H. Kazawa, and T. Hirao. 2004. A determin-
istic word dependency analyzer enhanced with prefer-
ence learning. In Proc. the 21st COLING, pages 275?
281.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. NODALIDA.
M. Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proc. the 45th ACL, pages 168?
175.
M. Kay. 1989. Head driven parsing. In Proc. the IWPT.
K. Kitagawa and K. Tanaka-Ishii. 2010. Tree-based de-
terministic dependency parsing ? an application to
nivre?s method ?. In Proc. the 48th ACL 2010 Short
Papers, pages 189?193, July.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. the 48th ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. EMNLP, pages
1288?1298.
D. McAllester. 1999. A reformulation of eisner and
satta?s cubic time parser for split head automata gram-
mars. http://ttic.uchicago.edu/ dmcallester/.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP, pages 523?
530.
R. McDonald. 2012. Minimum spanning tree parser.
http://www.seas.upenn.edu/ strctlrn/MSTParser.
M.-J. Nederhof. 2003. Weighted deductive parsing
and knuth?s algorithm. Computational Linguistics,
29:135?143.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proc. the IWPT, pages 149?160.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proc. the ACL Workshop Incremen-
tal Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
J. Nivre. 2008a. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34:513?553.
J. Nivre. 2008b. Sorting out dependency parsing. In
Proc. the CoTAL, pages 16?27.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):165?201.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
the IWPT, pages 195?206.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In
Proc. EMNLP, pages 562?571.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
the 49th ACL, pages 188?193.
665
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228?232
Manchester, August 2008
A Pipeline Approach for Syntactic and Semantic Dependency Parsing
Yotaro Watanabe and Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masakazu-i, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes our system for syn-
tactic and semantic dependency parsing
to participate the shared task of CoNLL-
2008. We use a pipeline approach, in
which syntactic dependency parsing, word
sense disambiguation, and semantic role
labeling are performed separately: Syn-
tactic dependency parsing is performed
by a tournament model with a support
vector machine; word sense disambigua-
tion is performed by a nearest neighbour
method in a compressed feature space by
probabilistic latent semantic indexing; and
semantic role labeling is performed by
a an online passive-aggressive algorithm.
The submitted result was 79.10 macro-
average F1 for the joint task, 87.18% syn-
tactic dependencies LAS, and 70.84 se-
mantic dependencies F1. After the dead-
line, we constructed the other configura-
tion, which achieved 80.89 F1 for the joint
task, and 74.53 semantic dependencies F1.
The result shows that the configuration of
pipeline is a crucial issue in the task.
1 Introduction
This paper presents the description of our system
in CoNLL-2008 shared task. We split the shared
task into five sub-problems ? syntactic dependency
parsing, syntactic dependency label classification,
predicate identification, word sense disambigua-
tion, and semantic role labeling. The overview
of our system is illustrated in Figure 1. Our de-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Figure 1: Overview of the System
pendency parsing module is based on a tourna-
ment model (Iida et al, 2003), in which a depen-
dency attachment is estimated in step-ladder tour-
nament matches. The relative preference of the at-
tachment is modeled by one-on-one match in the
tournament. Iwatate et al (Iwatate et al, 2008)
initially proposed the method for Japanese depen-
dency parsing, and we applied it to other languages
by relaxing some constraints (Section 2.1). Depen-
dency label classification is performed by a linear-
chain sequential labeling on the dependency sib-
lings like McDonald?s schemata (McDonald et al,
2006). We use an online passive-aggressive al-
gorithm (Crammer et al, 2006) for linear-chain
sequential labeling (Section 2.2). We also use
the other linear-chain sequential labeling method
to annotate whether each word is a predicate or
not (Section 2.3). If an identified predicate has
more than one sense, a nearest neighbour classifier
disambiguates the word sense candidates (Section
2.4). We use an online passive-aggressive algo-
rithm again for the semantic role labeling (Section
2.5). The machine learning algorithms used in sep-
arated modules are diverse due to role sharing.
1
1
Unlabeled dependency parsing was done by Iwatate, de-
pendency label classification and semantic role labeling was
done by Watanabe, predicate identification and word sense
228
We attempt to construct a framework in which
each module passes k-best solutions and the last
semantic role labeling module performs rerank-
ing of the k-best solutions using the overall infor-
mation. Unfortunately, we couldn?t complete the
framework before the deadline of the test run. Our
method is not a ?joint learning? approach but a
pipeline approach.
2 Methods
2.1 Unlabeled Dependency Parsing
The detailed description of the tournament model-
based Japanese dependency parsing is found in
(Iwatate et al, 2008). The original Iwatate?s pars-
ing algorithm was for Japanese, which is for a
strictly head-final language. We adapt the algo-
rithm to English in this shared task. The tour-
nament model chooses the most likely candidate
head of each of the focused words in a step-
ladder tournament. For a given word, the al-
gorithm repeats to compare two candidate heads
and finds the most plausible head in the series
of a tournament. On each comparison, the win-
ner is chosen by an SVM binary classifier with
a quadratic polynomial kernel
2
. The model uses
different algorithms for training example gener-
ation and parsing. Figures 2 and 3 show train-
ing example generation and parsing algorithm, re-
spectively. Time complexity of both algorithms is
O(n
2
) for the number of words in an input sen-
tence. Below, we present the features for SVM
// N: # of tokens in input sentence
// true_head[j]: token j?s head at
// training data
// gen(j,i1,i2,LEFT): generate an example
// where token j is dependent of i1
// gen(j,i1,i2,RIGHT): generate an example
// where token j is dependent of i2
// Token 0 is the virtual ROOT.
for j = 1 to N-1 do
h = true_head[j];
for i = 0 to h-1 do
if i!=j then gen(j,i,h,RIGHT);
for i = h+1 to N do
if i!=j then gen(j,h,i,LEFT);
end-for;
Figure 2: Pseudo Code of Training Example Gen-
eration
disambiguation was done by Asahara, and all tasks were su-
pervised by Matsumoto.
2
We use TinySVM as an SVM classifier. chasen.org/
?
taku/software/TinySVM/
// N: # of tokens in input sentence
// head[]: (analyzed-) head of tokens
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
// cands.push_back(k): add token index k
// to the end of cands.
// cands.erase(i): remove i-th element
// from cands.
for j = 1 to N do
cands = [];
for i = 0 to N do
if i!=j then cands.push_back(i);
end-for;
while cands.size() > 1 do
if classify(j,cands[0],
cands[1]) = LEFT then
cands.erase(1);
else
cands.erase(0);
end-if;
end-while;
head[j] = cands[0];
end-for;
Figure 3: Pseudo Code of Parsing Algorithm
in our tournament model. The FORM, LEMMA,
GPOS(for training), PPOS(for testing, instead of
GPOS), SPLIT FORM, SPLIT LEMMA, PPOSS
in the following tokens were used as the features:
? Dependent, candidate1, candidate2
? Immediately-adjacent tokens of dependent, candidate1,
candidate2, respectively
? All tokens between dependent-candidate1, dependent-
candidate2, candidate1-candidate2, respectively
We also used the distance feature: distance (1 or
2-5 or 6+ tokens) between dependent-candidate1,
dependent-candidate2, and candidate1-candidate2.
Features corresponding to the candidates, includ-
ing the distance feature, have a prefix that indicates
its side: ?L-?(the candidate appears on left-hand-
side of the dependent) or ?R-?(appears on right-
hand-side of the dependent). Training an SVM
model with all examples is time-consuming, and
split the examples by the dependent GPOS for
training (PPOS for testing, instead of GPOS
3
) to
run SVM training in parallel. Since the number of
examples with the dependent PPOS:IN, NN, NNP
3
We cannot use GPOS for testing due to the shared task
regulation.
229
is still large, we used only first 1.5 million exam-
ples for the dependent GPOS. Note that, the algo-
rithm does not check the well-formedness of de-
pendency trees
4
.
2.2 Dependency Label Classification
This phase labels a dependency relation label to
each word in a parse tree produced in the preced-
ing phase. (McDonald et al, 2006) suggests that
edges of head x
i
and its dependents x
j1
, ..., x
jM
are highly correlated, and capturing these corre-
lation improves classification accuracy. In their
approach, edges of a head and its dependents
e
i,j1
, ..., e
i,jM
are classified sequentially, and then
Viterbi algorithm is performed to find the highest
scoring label sequence. We take a similar approach
with some simplification. In our system, each edge
is classified deterministically, and the previous de-
cision is used as a feature for the subsequent clas-
sification.
We use an online passive aggressive algorithm
(Crammer et al, 2006)
5
for dependency label clas-
sification since it converges fast, gives good per-
formance and can be implemented easily. The fea-
tures used in this phase are primarily similar to that
of (McDonald et al, 2006).
Word features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the head and the dependent.
Position: Position relation between the head and the depen-
dent (Is the head anterior to dependent?). Is the word
top of the sentence? Is the word last of the sentence?
Context features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the nearest left/right word. SPLIT LEMMA
and PPOS bigram (ww, wp, pw, pp) of the head and the
dependent (window size 5).
Sibling features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the dependent?s nearest left and right siblings
in the dependency tree.
Other features: The number of dependent?s children.
Whether the dependent and the dependent?s grand
parent SPLIT LEMMA/PPOS are the same. The
previous classification result (previous label).
2.3 Predicate Identification
This phase solves which word can be a predi-
cate. In the predicate spotting, the linear-chain
4
We tried to make a k-best cascaded model among the
modules. The latter module can check the well-formedness
of the tree. The current implementation skips this well-
formedness checking.
5
We use PA algorithm among PA, PA-I and PA-II in
(Crammer et al, 2006).
CRF (Lafferty et al, 2001) annotates whether the
word is a predicate or not. The FORM, LEMMA
(itself, and whether the LEMMA is registered in
the PropBank/NomBank frames), SPLIT FORM,
SPLIT LEMMA, PPOSS within 5 token window
size are used as the features. We also use bigram
features within 3 token window size and trigram
features within 5 token window size for FORM,
LEMMA, SPLIT FORM, SPLIT LEMMA, and
PPOSS. The main reason why we use a sequence
labeling method for predicate identification was
to relax the effect of the tagging error of PPOS
and PPOSS. However, we will show later that this
module aggravates the total performance.
2.4 Word Sense Disambiguation
For the word sense disambiguation, we use 1-
nearest neighbour method in a compressed fea-
ture space by probabilistic latent semantic index-
ing (PLSI). We trained the word sense disambigua-
tion model from the example sentences in the train-
ing/development data and PropBank/NomBank
frames. The metric in the nearest neighbour
method is based on the occurrence of LEMMA
in the example sentences. However, the exam-
ples in the PropBank/NomBank do not contain the
lemma information. To lemmatize the words in
the PropBank/NomBank, we compose a lemma-
tizer from the FORM-LEMMA table in the train-
ing and development.
6
Since the metric space
is very sparse, PLSI (Hofmann, 1999) is used to
reduce the metric space dimensions. We used
KL-divergence between two examples of P (d
i
|z
k
)
of P (d
i
, w
j
) =
?
k
P (d
i
|z
k
)P (w
j
|z
k
)P (z
k
) as
hemi-metric for the nearest neighbour method
7
,
in which d
i
? D is an example sentence in the
training/devel/test data and PropBank/NomBank
frames; w
j
? W is LEMMA; and z
k
? Z is a
latent class. We use |Z| = 100, which gave the
best performance in the development data. Note,
we transductively used the test data for the PLSI
modeling within the test run period.
2.5 Semantic Role Labeling
While semantic role labeling task is generally per-
formed by two phases: argument identification and
argument classification, we did not divide the task
6
We are not violating the closed track regulation to build
the lemmatizer. If a word in the PropBank/NomBank is not in
the training/development data, we give up lemmatization.
7
We useD
KL
=
?
k
P (d
input data
|z
k
)log
P (d
input data
|z
k
)
P (d
1-nearest data
|z
k
)
as hemi-metric. It is a non-commutative measure.
230
into the two phases. That is, argument candidates
are directly assigned a particular semantic role la-
bel. We did not employ any candidate filtering pro-
cedure, so argument candidates consist of words in
any predicate-word pair. The argument candidates
that have no roles are assigned ?NONE? label. For
the reason that described in Section 2.2 (fast con-
vergence and good performance), we use an on-
line passive aggressive algorithm for learning the
semantic role classifiers.
Useful features for argument classification of
verb and noun predicates are different. For exam-
ple, voice (active or passive) is essential for verb
predicate?s argument classification. On the other
hand, presence of a genitive word is useful for
noun predicate?s argument classification. For this
reason, we created twomodels: argument classifier
for verb predicates and that for noun predicates.
Semantic frames are useful information for se-
mantic role classification. Generally, obligatory
arguments not included in semantic frames do not
appear in actual texts. For this reason, we use
PropBank/NomBank semantic frames for seman-
tic role pruning. Suppose semantic roles in the se-
mantic frame are F
i
= {A0, A1, A2, A3}. Since
obligatory arguments are {A0...AA}, the remain-
ing arguments {A4, A5, AA} are removed from
label candidates.
For verb predicates, the features used in our sys-
tem are based on (Hacioglu, 2004). We also em-
ployed some other features proposed in (Gildea
and Jurafsky, 2002; Pradhan et al, 2004b). For
noun predicates, the features are primarily based
on (Pradhan et al, 2004a). The features that we
defined for semantic role labeling are as follows:
Word features: SPLIT LEMMA and PPOS of the predicate,
dependent and dependent?s head, and its conjunctions.
Dependency label: The dependency label between the argu-
ment candidate and the its head.
Family: The position of the argument candidate with respect
to the predicate position over the dependency tree (e.g.,
child, sibling).
Position: The position of the head of the dependency relation
with respect to the predicate position in the sentence.
Pattern: The left-to-right chain of the PPOS/dependency la-
bels of the predicate?s children.
Context features: PPOS of the nearest left/right word.
Path features: SPLIT LEMMA, PPOS and dependency la-
bel paths between the predicate and the argument can-
didate, and its path bi-gram.
Distance: The number of paths between the predicate and
the argument candidate.
Voice: Voice of the predicate (active or passive) and voice-
position conjunction (for verb predicates).
Is predicate plural: Whether the predicate is singular or
plural (for noun predicates).
Genitives between the predicate and the argument: Is
there a genitive word between the predicate and the
argument? (for noun predicates)
3 Results
Table 1 shows the result of our system. The pro-
posed method was effective in dependency pars-
ing (rank 3rd), but was not good in semantic role
labeling (rank 9th). One reason of the result of
semantic role labeling could be usages of Prop-
Bank/NomBank frames. We did not achieve the
maximum use of the resources, hence the design of
features and the choice of learning algorithm may
not be optimal.
Figure 4: Overview of the Modified System
The other reason is the design of the pipeline.
We changed the design of the pipeline after the
test run. The overview of the modified system
is illustrated in Figure 4. After the syntactic de-
pendency parsing, we limited the predicate can-
didates as verbs and nouns by PPOSS, and fil-
tered the argument candidates by Xue?s method
(Xue and Palmer, 2004). Next, the candidate pair
of predicate-argument was classified by an online
passive-aggressive algorithm as shown in Section
2.5. Finally, the word sense of the predicate is de-
termined by the module in Section 2.4. The new
result is scores with ? in Table 1. The result means
that the first design was not the best for the task.
Acknowledgements
We would like to thank the CoNLL-2008 shared
task organizers and the data providers (Surdeanu
et al, 2008).
231
Problem All WSJ Brown Rank
Complete Problem 79.10 (80.89
?
) 80.30 (82.06
?
) 69.29 (71.32
?
) 9th
Semantic Dependency 70.84 (74.53
?
) 72.37 (76.01
?
) 58.21 (62.41
?
) 9th
Semantic Role Labeling 67.92 (72.31
?
) 69.31 (73.62
?
) 56.42 (61.64
?
) -
Predicate Identification & Word Sense Disambiguation 77.20 (79.17
?
) 79.02 (80.99
?
) 62.10 (64.03
?
) -
Syntactic Dependency (Labeled) 87.18 88.06 80.17 3rd
Syntactic Label Accuracy 91.63 92.31 86.26 -
Unlabeled Syntactic Dependency Unlabeled 90.20 90.73 85.94 -
The scores with ? mark are our post-evaluation results.
Table 1: The Results ? Closed Challenge
References
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149?164.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwarz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of Machine
Learning Research, 7:551?585.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Hacioglu, Kadri. 2004. Semantic role labeling using
dependency trees. In COLING-2004: Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 1273?1276.
Hofmann, Thomas. 1999. Probabilistic Latent Seman-
tic Indexing. In SIGIR-1999: Proceedings of the
22nd Annual International ACM SIGIR Conference
on Research and Development in Informatino Re-
trieval, pages 50?57.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ?The Computational Treatment of
Anaphora?, pages 23?30.
Iwatate, Masakazu, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese Dependency Parsing Using
a Tournament Model. In COLING-2008: Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (To Appear).
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML-1001: Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282?289.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In CoNLL-
2006: Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, pages 216?
220.
Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915?932.
Pradhan, Sameer, Honglin Sun, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2004a. Parsing Argu-
ments of Nominalizations in English and Chinese. In
HLT-NAACL-2004: Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 141?144.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004b. Shal-
low Semantic Parsing Using Support Vector Ma-
chines. In HLT-NAACL-2004: Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 233?240.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL-2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In EMNLP-
2004: Proceedings of 2004 Conference on Empirical
Methods in Natural Language Processing, pages 88?
94.
232
