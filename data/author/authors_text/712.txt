187
188
189
190
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 225?232, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Error Handling in the RavenClaw Dialog Management Framework 
Dan Bohus 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, PA, 15213 
dbohus@cs.cmu.edu 
Alexander I. Rudnicky 
Computer Science Department 
Carnegie Mellon University 
Pittsburgh, PA, 15213 
air@cs.cmu.edu 
 
 
Abstract 
We describe the error handling architect-
ture underlying the RavenClaw dialog 
management framework. The architecture 
provides a robust basis for current and fu-
ture research in error detection and recov-
ery. Several objectives were pursued in its 
development: task-independence, ease-of-
use, adaptability and scalability. We de-
scribe the key aspects of architectural de-
sign which confer these properties, and 
discuss the deployment of this architect-
ture in a number of spoken dialog systems 
spanning several domains and interaction 
types. Finally, we outline current research 
projects supported by this architecture. 
1 Introduction 
Over the last decade, improvements in speech rec-
ognition and other component technologies have 
paved the way for the emergence of complex task-
oriented spoken dialog systems. While traditionally 
the research community has focused on building 
information-access and command-and-control 
systems, recent efforts aim towards building more 
sophisticated language-enabled agents, such as 
personal assistants, interactive tutors, open-domain 
question answering systems, etc. At the other end 
of the complexity spectrum, simpler systems have 
already transitioned into day-to-day use and are 
becoming the norm in the phone-based customer-
service industry. 
Nevertheless, a number of problems remain in 
need of better solutions. One of the most important 
limitations in today?s spoken language interfaces is 
their lack of robustness when faced with under-
standing errors. This problem appears across all 
domains and interaction types, and stems primarily 
from the inherent unreliability of the speech recog-
nition process. The recognition difficulties are 
further exacerbated by the conditions under which 
these systems typically operate: spontaneous spe-
ech, large vocabularies and user populations, and 
large variability in input line quality. In these set-
tings, average word-error-rates of 20-30% (and up 
to 50% for non-native speakers) are quite common. 
Left unchecked, speech recognition errors can 
lead to two types of problems in a spoken dialog 
system: misunderstandings and non-understand-
ings. In a misunderstanding, the system obtains an 
incorrect semantic interpretation of the user?s turn. 
In the absence of robust mechanisms for assessing 
the reliability of the decoded inputs, the system 
will take the misunderstanding as fact and will act 
based on invalid information. In contrast, in a non-
understanding the system fails to obtain an inter-
pretation of the input altogether. Although no false 
information is incorporated in this case, the situa-
tion is not much better: without an appropriate set 
of recovery strategies and a mechanism for diag-
nosing the problem, the system?s follow-up options 
are limited and uninformed. In general, unless 
mitigated by accurate error awareness and robust 
recovery mechanisms, speech recognition errors 
exert a strong negative impact on the quality and 
ultimately on the success of the interactions (Sand-
ers et al 2002). 
Two pathways towards increased robustness 
can be easily envisioned. One is to improve the 
accuracy of the speech recognition process. The 
second is to create mechanisms for detecting and 
gracefully handling potential errors at the conver-
sation level. Clearly, these two approaches do not 
225
stand in opposition and a combined effort would 
lead to the best results. The error handling archi-
tecture we describe in this paper embodies the sec-
ond approach: it aims to provide the mechanisms 
for robust error handling at the dialog management 
level of a spoken dialog system. 
The idea of handling errors through conversa-
tion has already received a large amount of atten-
tion from the research community. On the theore-
tical side, several models of grounding in commu-
nication have been proposed (Clark and Schaefer, 
1989; Traum, 1998). While these models provide 
useful insights into the grounding process as it 
happens in human-human communication, they 
lack the decision-making aspects required to drive 
the interaction in a real-life spoken dialog system. 
In the Conversational Architectures project, Paek 
and Horvitz (2000) address this challenge by de-
veloping a computational implementation of the 
grounding process using Bayesian belief networks. 
However, questions still remain: the structure and 
parameters of the belief networks are handcrafted, 
as are the utilities for the various grounding ac-
tions; due to scalability and task-representation 
issues, it is not known yet how the proposed ap-
proach would transfer and scale to other domains.  
Three ingredients are required for robust error 
handling: (1) the ability to detect the errors, (2) a 
set of error recovery strategies, and (3) a 
mechanism for engaging these strategies at the 
appropriate time. For some of these issues, various 
solutions have emerged in the community. For 
instance, systems generally rely on recognition 
confidence scores to detect potential misunder-
standings (e.g. Krahmer et al, 1999; Walker et al, 
2000) and use explicit and implicit confirmation 
strategies for recovery. The decision to engage 
these strategies is typically based on comparing the 
confidence score against manually preset thresh-
olds (e.g. Kawahara and Komatani, 2000). For 
non-understandings, detection is less of a problem 
(systems know by definition when non-understand-
ings occur). Strategies such as asking the user to 
repeat or rephrase, providing help, are usually en-
gaged via simple heuristic rules. 
At the same time, a number of issues remain 
unsolved: can we endow systems with better error 
awareness by integrating existing confidence an-
notation schemes with correction detection mecha-
nisms? Can we diagnose the non-understanding 
errors on-line? What are the tradeoffs between the 
various non-understanding recovery strategies? 
Can we construct a richer set of such strategies? 
Can we build systems which automatically tune 
their error handling behaviors to the characteristics 
of the domains in which they operate? 
We have recently engaged in a research pro-
gram aimed at addressing such issues. More gener-
ally, our goal is to develop a task-independent, 
easy-to-use, adaptive and scalable approach for 
error handling in task-oriented spoken dialog sys-
tems. As a first step in this program, we have 
developed a modular error handling architecture, 
within the larger confines of the RavenClaw dialog 
management framework (Bohus and Rudnicky, 
2003). The proposed architecture provides the in-
frastructure for our current and future research on 
error handling. In this paper we describe the pro-
posed architecture and discuss the key aspects of 
architectural design which confer the desired prop-
erties. Subsequently, we discuss the deployment of 
this architecture in a number of spoken dialog sys-
tems which operate across different domains and 
interaction types, and we outline current research 
projects supported by the proposed architecture. 
2 RavenClaw Dialog Management 
We begin with a brief overview of the RavenClaw 
dialog management framework, as it provides the 
larger context for the error handling architecture.  
RavenClaw is a dialog management framework 
for task-oriented spoken dialog systems. To date, it 
has been used to construct a large number of sys-
tems spanning multiple domains and interaction 
types (Bohus and Rudnicky, 2003): information 
access (RoomLine, the Let?s Go Bus Information 
System), guidance through procedures (LARRI), 
command-and-control (TeamTalk), taskable agents 
(Vera). Together with these systems, RavenClaw 
provides the larger context as well as a test-bed for 
evaluating the proposed error handling architec-
ture. More generally, RavenClaw provides a robust 
basis for research in various other aspects of dialog 
management, such as learning at the task and dis-
course levels, multi-participant dialog, timing and 
turn-taking, etc. 
A key characteristic of the RavenClaw frame-
work is the separation it enforces between the do-
main-specific and domain-independent aspects of 
dialog control. The domain-specific dialog control 
logic is described by a Dialog Task Specification, 
226
essentially a hierarchical dialog plan provided by 
the system author. A fixed, domain-independent 
Dialog Engine manages the conversation by exe-
cuting the given Dialog Task Specification. In the 
process, the Dialog Engine also contributes a set of 
domain-independent conversational skills, such as 
error handling (discussed extensively in Section 4), 
timing and turn-taking, etc. The system authoring 
effort is therefore minimized and focused entirely 
on the domain-specific aspects of dialog control. 
2.1 The Dialog Task Specification 
A Dialog Task Specification consists of a tree of 
dialog agents, where each agent manages a sub-
part of the interaction. Figure 1 illustrates a portion 
of the dialog task specification from RoomLine, a 
spoken dialog system which can assist users in 
making conference room reservations. The root 
node subsumes several children: Welcome, which 
produces an introductory prompt, GetQuery which 
obtains the time and room constraints from the 
user, DoQuery which performs the database query, 
and DiscussResults which handles the follow-up 
negotiation dialog. Going one level deeper in the 
tree, GetQuery contains GetDate which requests the 
date for the reservation, GetStartTime and GetEnd-
Time which request the times, and so on. This type 
of hierarchical task representation has a number of 
advantages: it scales up gracefully, it can be 
dynamically extended at runtime, and it implicitly 
captures a notion of context in dialog.  
The agents located at the leaves of the tree are 
called basic dialog agents, and each of them im-
plements an atomic dialog action (dialog move). 
There are four types of basic dialog agents: Inform 
? conveys information to the user (e.g. Welcome), 
Request ? asks a question and expects an answer 
(e.g. GetDate), Expect ? expects information with-
out explicitly asking for it, and EXecute ? imple-
ments a domain specific operation (e.g. DoQuery). 
The agents located at non-terminal positions in the 
tree are called dialog agencies (e.g. RoomLine, 
GetQuery). Their role is to plan for and control the 
execution of their sub-agents. For each agent in the 
tree, the system author may specify preconditions, 
completion criteria, effects and triggers; various 
other functional aspects of the dialog agents (e.g. 
state-specific language models for request-agents, 
help-prompts) are controlled through parameters. 
The information the system acquires and ma-
nipulates in conversation is captured in concepts, 
associated with various agents in the tree (e.g. date, 
start_time). Each concept maintains a history of 
previous values, information about current candi-
date hypotheses and their associated confidence 
scores, information about when the concept was 
last updated, as well as an extended set of flags 
which describe whether or not the concept has 
been conveyed to the user, whether or not the con-
cept has been grounded, etc. This rich representa-
tion provides the necessary support for concept-
level error handling. 
Dialog Stack 
Dialog Engine 
Dialog Task 
Specification 
Expectation Agenda 
start_time: [start_time] [time] 
date: [date] 
start_time: [start_time] [time] 
end_time: [end_time] [time] 
date: [date] 
start_time: [start_time] [time] 
end_time: [end_time] [time] 
location: [location] 
network: [with_network]->true,  
                [without_network]->false 
? ? ? 
System: For when do you need the room? 
User:  let?s try two to four p.m. 
Parse:  [time](two) [end_time](to four pm) 
User Input 
RoomLine 
GetQuery 
GetStartTime 
date 
end_time start_time 
RoomLine 
I: Welcome GetQuery 
R: GetDate 
Start-Over 
R: GetStartTime R: GetEndTime 
DiscussResults X: DoQuery 
Figure 1: RavenClaw architecture 
227
2.2 The Dialog Engine 
The Dialog Engine is the core domain-independent 
component which manages the interaction by exe-
cuting a given Dialog Task Specification. The con-
trol algorithms are centered on two data-structures: 
a dialog stack, which captures the dialog structure 
at runtime, and an expectation agenda, which cap-
tures the system?s expectations for the user input at 
each turn in the dialog. The dialog is controlled by 
interleaving Execution Phases with Input Phases. 
During an Execution Phase, dialog agents from 
the tree are placed on, and executed from the dia-
log stack. At the beginning of the dialog, the root 
agent is placed on the stack. Subsequently, the en-
gine repeatedly takes the agent on the top of the 
stack and executes it. When dialog agencies are 
executed, they typically schedule one of their sub-
agents for execution by placing it on the stack. The 
dialog stack will therefore track the nested struc-
ture of the dialog at runtime. Ultimately, the execu-
tion of the basic dialog agents on the leaves of the 
tree generates the system?s responses and actions. 
During an Input Phase, the system assembles 
the expectation agenda, which captures what the 
system expects to hear from the user in a given 
turn. The agenda subsequently mediates the trans-
fer of semantic information from the user?s input 
into the various concepts in the task tree. For the 
interested reader, these mechanisms are described 
in more detail in (Bohus and Rudnicky, 2003) 
Additionally, the Dialog Engine automatically 
provides a number of conversational strategies, 
such as the ability to handle various requests for 
help, repeating the last utterance, suspending and 
resuming the dialog, starting over, reestablishing 
the context, etc. These strategies are implemented 
as library dialog agencies. Their corresponding 
sub-trees are automatically added to the Dialog 
Task Specification provided by the system author 
(e.g. the Start-Over agency in Figure 1.) The auto-
matic availability of these strategies lessens devel-
opment efforts and ensures a certain uniformity of 
behavior both within and across tasks. 
3 The Error Handling Architecture 
The error handling architecture in the RavenClaw 
dialog management framework subsumes two 
main components: (1) a set of error handling 
strategies (e.g. explicit and implicit confirmation, 
asking the user to repeat, etc.) and (2) an error 
handling process which engages these strategies. 
The error handling strategies are implemented 
as library dialog agents. The decision process 
which engages these strategies is part of the Dialog 
Engine. This design, in which both the strategies 
and the decision process are decoupled from the 
dialog task, as well as from each other, provides a 
number of advantages. First, it ensures that the er-
ror handling mechanisms are reusable across dif-
ferent dialog systems. Second, the approach 
guarantees a certain uniformity and consistency in 
error handling behaviors both within and across 
systems. Third, as new error handling strategies are 
developed, they can be easily plugged into any ex-
isting system. Last, but not least, the approach sig-
nificantly lessens the system authoring effort by 
allowing developers to focus exclusively on de-
scribing the dialog control logic. 
The responsibility for handling potential under-
standing errors1 is delegated to the Error Handling 
Process which runs in the Dialog Engine (see Fig-
ure 2). At each system turn, this process collects 
evidence and makes a decision with respect to en-
gaging any of the error handling strategies. When 
necessary, it will insert an error handling strategy 
on the dialog stack (e.g. the ExplicitConfirm 
(start_time) strategy in Figure 2), thus modifying 
on-the-fly the task originally specified by the sys-
tem author. The strategy executes and, once com-
pleted, it is removed from the stack and the dialog 
resumes from where it was left off. 
                                                          
1
 Note that the proposed framework aims to handle 
understanding errors. The corresponding strategies are generic 
and can be applied in any domain. Treatment of domain or 
task-specific errors (e.g. database access error, etc) still needs 
to be implemented as part of the dialog task specification.  
Error Handling 
Strategies 
Error Handling  
Process 
Explicit  
Confirm 
RoomLine 
GetQuery 
GetStartTime 
ExplicitConfirm 
(start_time) 
Dialog Stack 
Evidence 
Figure 2: Error Handling ? Block Diagram 
 
Dialog Task Specification 
 
Dialog Engine 
228
3.1 Error Handling Strategies 
The error handling strategies can be divided into 
two groups: strategies for handling potential mis-
understandings and strategies for handling non-
understandings. 
For handling potential misunderstandings, three 
strategies are currently available: Explicit Confir-
mation, Implicit Confirmation and Rejection. 
For non-understandings, a larger number of er-
ror recovery strategies are currently available: 
AskRepeat ? the system asks the user to repeat; 
AskRephrase ? the system asks the user to re-
phrase; Reprompt ? the system repeats the previous 
prompt; DetailedReprompt ? the system repeats a 
more verbose version of the previous prompt, 
Notify ? the system simply notifies the user that a 
non-understanding has occurred; Yield ? the sys-
tem remains silent, and thus implicitly notifies the 
user that a non-understanding has occurred; 
MoveOn ? the system tries to advance the task by 
giving up on the current question and moving on 
with an alternative dialog plan (note that this strat-
egy is only available at certain points in the dia-
log); YouCanSay ? the system gives an example of 
what the user could say at this point in the dialog; 
FullHelp ? the system provides a longer help mes-
sage which includes an explanation of the current 
state of the system, as well as what the user could 
say at this point. An in-depth analysis of these 
strategies and their relative tradeoffs is available in 
(Bohus and Rudnicky, 2005a). Several sample 
dialogs illustrating these strategies are available 
on-line (RoomLine, 2003).  
3.2 Error Handling Process 
The error handling decision process is imple-
mented in a distributed fashion, as a collection of 
local decision processes. The Dialog Engine auto-
matically associates a local error handling process 
with each concept, and with each request agent in 
the dialog task tree, as illustrated in Figure 3. The 
error handling processes running on individual 
concepts are in charge of recovering from misun-
derstandings on those concepts. The error handling 
processes running on individual request agents are 
in charge or recovering from non-understandings 
on the corresponding requests.  
At every system turn, each concept- and 
request-agent error handling process computes and 
forwards its decision to a gating mechanism, which 
queues up the actions (if necessary) and executes 
them one at a time. For instance, in the example in 
Figure 3, the error handling decision process for 
the start_time concept decides to engage an explicit 
confirmation on that concept, while the other deci-
sion processes do not take any action. In this case 
the gating mechanism creates a new instance of an 
explicit confirmation agency, passes it the pointer 
to the concept to be confirmed (start_time), and 
places it on the dialog stack. On completion, the 
strategy updates the confidence score of the con-
firmed hypothesis in light of the user response, and 
the dialog resumes from where it was left off.  
The specific implementation of the local deci-
sion processes constitutes an active research issue. 
Currently, they are modeled as Markov Decision 
Processes (MDP). The error handling processes 
running on individual concepts (concept-MDPs in 
end_time 
date 
start_time 
Explicit Confirm 
No Action 
Figure 3: A Distributed Error Handling Process 
ExplicitConfirm 
(start_time) 
 
 
 
 
 
 
 
 
 
 
 
Gating 
Mechanism 
Error Handling  
Decision Proc. 
[Concept-MDP] 
No Action 
No Action 
  
 
GetQuery 
R: GetDate 
R: GetStartTime 
R: GetEndTime 
RoomLine 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Error Handling  
Decision Proc. 
[Concept-MDP] 
Error Handling  
Decision Proc. 
[Request-MDP] 
Error Handling  
Decision Proc. 
[Concept-MDP] 
229
Figure 3) are partially-observable MDPs, with 3 
underlying hidden states: correct, incorrect and 
empty. The belief state is constructed at each time 
step from the confidence score of the top-hypothe-
sis for the concept. For instance, if the top 
hypothesis for the start_time concept is 10 a.m. with 
confidence 0.76, then the belief state for the 
POMDP corresponding to this concept is: 
{P(correct)=0.76, P(incorrect)=0.24, P(empty)=0}. 
The action-space for these models contains the 
three error recovery strategies for handling poten-
tial misunderstandings, and no-action. The third 
ingredient in the model is the policy.  A policy de-
fines which action the system should take in each 
state, and is indirectly described by specifying the 
utility of each strategy in each state. Currently, a 
number of predefined policies (e.g. always-
explicit-confirm, pessimistic, and optimistic) are 
available in the framework. Alternatively, system 
authors can specify and use their own policies. 
The error handling processes running on re-
quest agents (request-MDPs in Figure 3) are in 
charge of handling non-understandings on those 
requests. Currently, two types of models are avail-
able for this purpose. The simplest model has three 
states: non-understanding, understanding and 
inactive. A second model also includes information 
about the number of consecutive non-understand-
ings that have already happened. In the future, we 
plan to identify more features which carry useful 
information about the likelihood of success of in-
dividual recovery strategies and use them to create 
more complex models. The action-space is defined 
by the set of non-understanding recovery strategies 
presented in the previous subsection, and no-
action. Similar to the concept-MDPs, a number of 
default policies are available; alternatively, system 
authors can specify their own policy for engaging 
the strategies. 
While the MDP implementation allows us to 
encode various expert-designed policies, our ulti-
mate goal is to learn such policies from collected 
data using reinforcement learning. Reinforcement 
learning has been previously used to derive dialog 
control policies in systems operating with small 
tasks (Scheffler and Young, 2002; Singh et al 
2000). The approaches proposed to date suffer 
however from one important shortcoming, which 
has so far prevented their use in large, practical 
spoken dialog systems. The problem is lack of 
scalability: the size of the state space grows very 
fast with the size of the dialog task, and this ren-
ders the approach unfeasible in complex domains. 
A second important limitation of reinforcement 
learning techniques proposed to date is that the 
learned policies cannot be reused across tasks. For 
each new system, a new MDP has to be con-
structed, new data has to be collected, and a new 
training phase is necessary. This requires a signifi-
cant amount of expertise and effort from the sys-
tem author. 
We believe that the error handling architecture 
we have described addresses these issues in several 
ways. The central idea behind the distributed na-
ture of the approach is to keep the learning prob-
lem tractable by leveraging independence relation-
ships between different parts of the dialog. First, 
the state and action-spaces can be maintained rela-
tively small since we are only focusing on making 
error handling decisions (as opposed to other dia-
log control decisions). A more complex task 
translates into a larger number of MDP instantia-
tions rather than a more complex model structure. 
Second, both the model structure and parameters 
(i.e. the transition probabilities) can be tied across 
models: for instance the MDP for grounding the 
start_time concept can be identical to the one for 
grounding the end_time concept; all models for 
grounding Yes/No concepts could be tied together, 
etc. Model tying has the potential to greatly im-
prove scalability since data is polled together and 
the total number of model parameters to be learned 
grows sub-linearly with the size of the task. Third, 
since the individual MDPs are decoupled from the 
actual system task, the policies learned in a par-
ticular system can potentially be reused in other 
systems (e.g. we expect that grounding yes/no con-
cepts functions similarly at different locations in 
the dialog, and across domains). Last but not least, 
the approach can easily accommodate dynamic 
task generation. In traditional reinforcement 
learning approaches the state and action-spaces of 
the underlying MDP are task-specific. The task 
therefore has to be fixed, known in advance: for 
instance the slots that the system queries the user 
about (in a slot-filling system) are fixed. In con-
trast, in the RavenClaw architecture, the dialog 
task tree (e.g. the dialog plan) can be dynamically 
expanded at runtime with new questions and con-
cepts, and the corresponding request- and concept-
MDPs are automatically created by the Dialog En-
gine. 
230
4 Deployment and Current Research 
While a quantitative evaluation of design charac-
teristics such as task-independence, scalability, and 
ease-of-use is hard to perform, a first-order empiri-
cal evaluation of the proposed error handling ar-
chitecture can be accomplished by using it in 
different systems and monitoring the system au-
thoring process and the system?s operation.  
To date, the architecture has been successfully 
deployed in three different spoken dialog systems. 
A first system, RoomLine (2003), is a phone-based 
mixed-initiative system that assists users in making 
conference room reservations on campus. A sec-
ond system, the Let?s Go! Bus Information System 
(Raux et al 2005), provides information about bus 
routes and schedules in the greater Pittsburgh area 
(the system is available to the larger public). Fi-
nally, Vera is a phone-based taskable agent that 
can be instructed to deliver messages to a third 
party, make wake-up calls, etc. Vera actually con-
sists of two dialog systems, one which handles in-
coming requests (Vera In) and one which performs 
message delivery (Vera Out). In each of these sys-
tems, the authoring effort with respect to error 
handling consisted of: (1) specifying which models 
and policies should be used for the concepts and 
request-agents in the dialog task tree, and (2) 
writing the language generation prompts for ex-
plicit and implicit confirmations for each concept.  
Even though the first two systems operate in 
similar domains (information access), they have 
very different user populations: students and fac-
ulty on campus in the first case versus the entire 
Pittsburgh community in the second case. As a 
result, the two systems were configured with dif-
ferent error handling strategies and policies (see 
Table 1). RoomLine uses explicit and implicit con-
firmations with an optimistic policy to handle po-
tential misunderstandings. In contrast, the Let?s Go 
Public Bus Information System always uses ex-
plicit confirmations, in an effort to increase robust-
ness (at the expense of potentially longer dialogs). 
For non-understandings, RoomLine uses the full 
set of non-understanding recovery strategies pre-
sented in section 3.1. The Let?s Go Bus Informa-
tion system uses the YouCanSay and FullHelp 
strategies. Additionally a new GoToAQuieterPlace 
strategy was developed for this system (and is now 
available for use into any other RavenClaw-based 
system). This last strategy asks the user to move to 
a quieter place, and was prompted by the observa-
tion that a large number of users were calling the 
system from noisy environments. 
While the first two systems were developed by 
authors who had good knowledge of the Raven-
Claw dialog management framework, the third sys-
tem, Vera, was developed as part of a class project, 
by a team of six students who had no prior experi-
ence with RavenClaw. Modulo an initial lack of 
documentation, no major problems were encoun-
tered in configuring the system for automatic error 
handling. Overall, the proposed error handling ar-
chitecture adapted easily and provided the desired 
functionality in each of these domains: while new 
strategies and recovery policies were developed for 
some of the systems, no structural changes were 
required in the error handling architecture. 
Table 1: Spoken dialog systems using the RavenClaw error handling architecture 
 RoomLine Let?s Go Public Vera In / Out 
Domain room reservations bus route information task-able agent 
Initiative type mixed system mixed / mixed 
Task size: #agents ; #concepts 110 ; 25 57 ; 19 29 ; 4 / 31 ; 13 
Strategies for misunderstandings explicit and implicit explicit explicit and implicit / 
explicit only 
Policy for misunderstandings optimistic always-explicit optimistic /  
always-explicit 
Strategies for non-understandings all strategies 
(see Section 3.1) 
go-to-quieter-place, 
you-can-say, help 
all strategies /  
repeat prompt 
Policy for non-understandings choose-random author-specified  
heuristic policy 
choose-random /  
always-repeat-prompt 
Sessions collected so far 1393 2836 72 / 131 
Avg. task success rate 75% 52% (unknown) 
% Misunderstandings 17% 28% (unknown) 
% Non-understandings 13% 27% (unknown) 
% turns when strategies engage 41% 53% 36% / 44% 
231
5 Conclusion and Future Work 
We have described the error handling architecture 
underlying the RavenClaw dialog management 
framework. Its design is modular: the error han-
dling strategies as well as the mechanisms for en-
gaging them are decoupled from the actual dialog 
task specification. This significantly lessens the 
development effort: system authors focus exclu-
sively on the domain-specific dialog control logic, 
and the error handling behaviors are generated 
transparently by the error handling process running 
in the core dialog engine. Furthermore, we have 
argued that the distributed nature of the error han-
dling process leads to good scalability properties 
and facilitates the reuse of policies within and 
across systems and domains.  
The proposed architecture represents only the 
first (but an essential step) in our larger research 
program in error handling. Together with the sys-
tems described above, it sets the stage for a number 
of current and future planned investigations in er-
ror detection and recovery. For instance, we have 
recently conducted an extensive investigation of 
non-understanding errors and the ten recovery 
strategies currently available in the RavenClaw 
framework. The results of that study fall beyond 
the scope of this paper and are presented separately 
in (Bohus and Rudnicky, 2005a). In another pro-
ject supported by this architecture, we have devel-
oped a model for updating system beliefs over 
concept values in light of initial recognition confi-
dence scores and subsequent user responses to 
system actions. Initially, our confirmation strate-
gies used simple heuristics to update the system?s 
confidence score for a concept in light of the user 
response to the verification question. We have 
showed that a machine learning based approach 
which integrates confidence information with cor-
rection detection information can be used to con-
struct significantly more accurate system beliefs 
(Bohus and Rudnicky, 2005b). Our next efforts 
will focus on using reinforcement learning to 
automatically derive the error recovery policies. 
References 
Bohus, D., Rudnicky, A., 2003 ? RavenClaw: Dialogue 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech-2003, Geneva, Switzerland 
Bohus, D., Rudnicky, A., 2005a ? Sorry, I didn?t Catch 
That! An Investigation into Non-understandings and 
Recovery Strategies, to appear in SIGDial-2005, Lis-
bon, Portugal 
Bohus, D., Rudnicky, A., 2005b ? Constructing Accu-
rate Beliefs in Spoken Dialog Systems, submitted to 
ASRU-2005, Cancun, Mexico 
Clark, H.H., Schaefer, E.F., 1989 ? Contributing to Dis-
course, in Cognitive Science, vol 13, 1989. 
Kawahara, T., Komatani, K., 2000 ? Flexible mixed-
initiative dialogue management using concept-level 
confidence measures of speech recognizer output, in 
Proc. of COLING, Saarbrucken, Germany, 2000. 
Krahmer, E., Swerts, M., Theune, M., Weegels, M., 
1999 - Error Detection in Human-Machine Interac-
tion, Speaking. From Intention to Articulation, MIT 
Press, Cambridge, Massachusetts, 1999 
Paek, T., Horvitz, E., 2000 ? Conversation as Action 
Under Uncertainty, in Proceedings of the Sixteenth 
Conference on Uncertainty and Artificial Intelli-
gence, Stanford, CA, June 2000. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M., 2005 ? Let?s Go Public! Taking a Spoken Dialog 
System to the Real World, submitted to Interspeech-
2005, Lisbon, Portugal 
RoomLine web site, as of June 2005 ?  
www.cs.cmu.edu/~dbohus/RoomLine 
Sanders, G., Le, A., Garofolo, J., 2002 ? Effects of Word 
Error Rate in the DARPA Communicator Data Dur-
ing 2000 and 2001, in Proceedings of ICSLP?02, 
Denver, Colorado, 2002. 
Scheffler, K., Young, S., 2002 ? Automatic learning of 
dialogue strategy using dialogue simulation and re-
inforcement learning, in Proceedings of HLT-2002. 
Singh, S., Litman, D., Kearns, M., Walker, M., 2000 ? 
Optimizing Dialogue Management with Reinforce-
ment Learning: Experiments with the NJFun System, 
in Journal of Artificial Intelligence Research, vol. 16, 
pp 105-133, 2000. 
Traum, D., 1998 ? On Clark and Schaefer?s Contribu-
tion Model and its Applicability to Human-Computer 
Collaboration, in Proceedings of the COOP?98, May 
1998. 
Walker, M., Wright, J., Langkilde, I., 2000 ? Using 
Natural Language Processing and Discourse Fea-
tures to Identify Understanding Errors in a Spoken 
Dialogue System, in Proc. of the 17?th International 
Conference of Machine Learning, pp 1111-1118.  
 
232
Proceedings of NAACL HLT 2007, pages 276?283,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Estimating the Reliability of MDP Policies: A Confidence Interval Approach
Joel R. Tetreault
University of Pittsburgh
LRDC
Pittsburgh PA, 15260, USA
tetreaul@pitt.edu
Dan Bohus
Carnegie Mellon University
Dept. of Computer Science
Pittsburgh, PA, 15213, USA
dbohus@cs.cmu.edu
Diane J. Litman
University of Pittsburgh
Dept. of Computer Science
LRDC
Pittsburgh PA, 15260, USA
litman@cs.pitt.edu
Abstract
Past approaches for using reinforcement
learning to derive dialog control policies
have assumed that there was enough col-
lected data to derive a reliable policy. In
this paper we present a methodology for
numerically constructing confidence inter-
vals for the expected cumulative reward
for a learned policy. These intervals are
used to (1) better assess the reliability
of the expected cumulative reward, and
(2) perform a refined comparison between
policies derived from different Markov
Decision Processes (MDP) models. We
applied this methodology to a prior ex-
periment where the goal was to select the
best features to include in the MDP state-
space. Our results show that while some
of the policies developed in the prior work
exhibited very large confidence intervals,
the policy developed from the best feature
set had a much smaller confidence interval
and thus showed very high reliability.
1 Introduction
NLP researchers frequently have to deal with issues
of data sparsity. Whether the task is machine transla-
tion or named-entity recognition, the amount of data
one has to train or test with can greatly impact the re-
liability and robustness of one?s models, results and
conclusions.
One research area that is particularly sensitive to
the data sparsity issue is machine learning, specifi-
cally in using Reinforcement Learning (RL) to learn
the optimal action for a dialogue system to make
given any user state. Typically this involves learn-
ing from previously collected data or interacting in
real-time with real users or user simulators. One of
the biggest advantages to this machine learning ap-
proach is that it can be used to generate optimal poli-
cies for every possible state. However, this method
requires a thorough exploration of the state-space to
make reliable conclusions on what the best actions
are. States that are infrequently visited in the train-
ing set could be assigned sub-optimal actions, and
therefore the resulting dialogue manager may not
provide the best interaction for the user.
In this work, we present an approach for esti-
mating the reliability of a policy derived from col-
lected training data. The key idea is to take into ac-
count the uncertainty in the model parameters (MDP
transition probabilities), and use that information to
numerically construct a confidence interval for the
expected cumulative reward for the learned policy.
This confidence interval approach allows us to: (1)
better assess the reliability of the expected cumula-
tive reward for a given policy, and (2) perform a re-
fined comparison between policies derived from dif-
ferent MDP models.
We apply the proposed approach to our previous
work (Tetreault and Litman, 2006) in using RL to
improve a spoken dialogue tutoring system. In that
work, a dataset of 100 dialogues was used to de-
velop a methodology for selecting which user state
features should be included in the MDP state-space.
But are 100 dialogues enough to generate reliable
policies? In this paper we apply our confidence in-
276
terval approach to the same dataset in an effort to in-
vestigate how reliable our previous conclusions are,
given the amount of available training data.
In the following section, we discuss the prior
work and its data sparsity issue. In section 3, we
describe in detail our confidence interval methodol-
ogy. In section 4, we show how this methodology
works by applying it to the prior work. In sections 5
and 6, we present our conclusions and future work.
2 Previous Work
Past research into using RL to improve spoken di-
alogue systems has commonly used Markov Deci-
sion Processes (MDP?s) (Sutton and Barto, 1998)
to model a dialogue (such as (Levin and Pieraccini,
1997) and (Singh et al, 1999)).
A MDP is defined by a set of states {si}i=1..n,a set of actions {ak}k=1..p, and a set of transitionprobabilities which reflect the dynamics of the en-
vironment {p(si|sj, ak)}k=1..pi,j=1..n: if the model is attime t in state sj and takes action ak, then it willtransition to state si with probability p(si|sj , ak).Additionally, an expected reward r(si, sj , ak) is de-fined for each transition. Once these model parame-
ters are known, a simple dynamic programming ap-
proach can be used to learn the optimal control pol-
icy pi?, i.e. the set of actions the model should take
at each state, to maximize its expected cumulative
reward.
The dialog control problem can be naturally cast
in this formalism: the states {si}i=1..n in the MDPcorrespond to the dialog states (or an abstraction
thereof), the actions {ak}k=1..p correspond to theparticular actions the dialog manager might take,
and the rewards r(si, sj , ak) are defined to reflecta particular dialog performance metric. Once the
MDP structure has been defined, the model param-
eters {p(si|sj, ak)}k=1..pi,j=1..n are estimated from a cor-pus of dialogs (either real or simulated), and, based
on them, the policy which maximizes the expected
cumulative reward is computed.
While most work in this area has focused on de-
veloping the best policy (such as (Walker, 2000),
(Henderson et al, 2005)), there has been relatively
little work done with respect to selecting the best
features to include in the MDP state-space. For in-
stance, Singh et al (1999) showed that dialogue
length was a useful state feature and Frampton and
Lemon (2005) showed that the user?s last dialogue
act was also useful. In our previous work, we com-
pare the worth of several features. In addition, Paek
and Chickering?s (2005) work showed how a state-
space can be reduced by only selecting features that
are relevant to maximizing the reward function.
The motivation for this line of research is that if
one can properly select the most informative fea-
tures, one develops better policies, and thus a bet-
ter dialogue system. In the following sections we
summarize our past data, approach, results, and is-
sue with policy reliability.
2.1 MDP Structure
For this study, we used an annotated corpus of
human-computer spoken dialogue tutoring sessions.
The fixed-policy corpus contains data collected from
20 students interacting with the system for five prob-
lems (for a total of 100 dialogues of roughly 50 turns
each). The corpus was annotated with 5 state fea-
tures (Table 1). It should be noted that two of the
features, Certainty and Frustration, were manually
annotated while the other three were done automat-
ically. All features are binary except for Certainty
which has three values.
State Values
Correctness Student is correct or incorrect
in the current turn
Certainty Student is certain, neutral
or uncertain in the current turn
Concept Repetition A particular concept is either new
or repeated
Frustration Student is frustrated or not
in the current turn
Percent Correct Student answers over 66% of
questions correctly in dialogue
so far, or less
Table 1: State Features in Tutoring Corpus
For the action set {ak}k=1..p, we looked at whattype of question the system could ask the student
given the previous state. There are a total of four
possible actions: ask a short answer question (one
that requires a simple one word response), a com-
plex answer question (one that requires a longer,
deeper response), ask both a simple and complex
question in the same turn, or do not ask a question
at all (give a hint). The reward function r was the
277
learning gain of each student based on a pair of tests
before and after the entire session of 5 dialogues.
The 20 students were split into two groups (high
and low learners) based on their learning gain, so
10 students and their respective five dialogues were
given a positive reward of +100, while the remain-
der were assigned a negative reward of -100. The
rewards were assigned in the final dialogue state, a
common approach when applying RL in spoken di-
alogue systems.
2.2 Approach and Results
To investigate the usefulness of different features,
we took the following approach. We started with
two baseline MDPs. The first model (Baseline 1)
used only the Correctness feature in the state-space.
The second model (Baseline 2) included both the
Correctness and Certainty features. Next we con-
structed 3 new models by adding each of the remain-
ing three features (Frustration, Percent Correct and
Concept Repetition) to the Baseline 2 model.
We defined three metrics to compare the policies
derived from these MDPs: (1) Diff?s: the number of
states whose policy differs from the Baseline 2 pol-
icy, (2) Percent Policy change (P.C.): the weighted
amount of change between the two policies (100%
indicates total change), and (3) Expected Cumula-
tive Reward (or ECR) which is the average reward
one would expect in that MDP when in the state-
space.
The intuition is that if a new feature were rele-
vant, the corresponding model would lead to a dif-
ferent policy and a better expected cumulative re-
ward (when compared to the baseline models). Con-
versely, if the features were not useful, one would
expect that the new policies would look similar
(specifically, the Diff?s count and % Policy Change
would be low) or produce similar expected cumula-
tive rewards to the original baseline policy.
The results of this analysis are shown in Table 2 1
The Diff?s and Policy Change metrics are undefined
for the two baselines since we only use these two
metrics to compare the other three features to Base-
1Please note that to due to refinements in code, there is a
slight difference between the ECR?s reported in this work and
the ECR?s reported in the previous work, for the three features
added to Baseline 2. These changes did not alter the rankings
of these models, or the conclusions of the previous work.
line 2. All three metrics show that the best feature
to add to the Baseline 2 model is Concept Repetition
since it results in the most change over the Baseline
2 policy, and also the expected reward is the highest
as well. For the remainder of this paper, when we
refer to Concept Repetition, Frustration, or Percent
Correctness, we are referring to the model that in-
cludes that feature as well as the Baseline 2 features
Correctness and Certainty.
State Feature # Diff?s % P.C. ECR
Baseline 1 N/A N/A 6.15
Baseline 2 N/A N/A 31.92
B2 + Concept Repetition 10 80.2% 42.56
B2 + Frustration 8 66.4% 32.99
B2 + Percent Correctness 4 44.3% 28.50
Table 2: Feature Comparison Results
2.3 Problem with Reliability
However, the approach discussed above assumes
that given the size of the data set, the ECR and poli-
cies are reliable. If the MDP model were very frag-
ile, that is the policy and expected cumulative reward
were very sensitive to the quality of the transition
probability estimates, then the metrics could reveal
quite different rankings. Previously, we used a qual-
itative approach of tracking how the worth of each
state (V-value) changed over time. The V-values
indicate how much reward one would expect from
starting in that state to get to a final state. We hy-
pothesized that if the V-values stabilized as data in-
creased, then the learned policy would be more reli-
able.
So is this V-value methodology adequate for as-
sessing if there is enough data to determine a sta-
ble policy, and also for assessing if one model is
better than another? Since our approach for state-
space selection is based on comparing a new pol-
icy with a baseline policy, having a stable policy is
extremely important since instability could lead to
different conclusions. For example, in one compar-
ison, a new policy could differ with the baseline in
8 out of 10 states. But if the MDP were unstable,
adding just a little more data could result in a differ-
ence of only 4 out of 10 states. Is there an approach
that can categorize whether given a certain data size,
278
that the expected cumulative reward (and thus the
policy) is reliable? In the next section we present a
new methodology for numerically constructing con-
fidence intervals for these value function estimates.
Then, in the following section, we reevaluate our
prior work with this methodology and discuss the
results.
3 Confidence Interval Methodology
3.1 Policy Evaluation with Confidence
Intervals
The starting point for the proposed methodology
is the observation that for each state sj and ac-tion ak in the MDP, the set of transition probabili-ties {p(si|sj, ak)}i=1..n are modeled as multinomialdistributions that are estimated from the transition
counts in the training data:
p?(si|sj, ak) =
c(si, sj, ak)
?n
i=1 c(si, sj , ak)
(1)
where n is the number of states in the model, and
c(si, sj , ak) is the number of times the system wasin state sj , took action ak, and transitioned to state
si in the training data.It is important to note that these parameters are
just estimates. The reliability of these estimates
clearly depends on the amount of training data, more
specifically on the transition counts c(si, sj, ak). Forinstance, consider a model with 3 states and 2 ac-
tions. Say the model was in state s1 and took action
a1 ten times. Out of these, three times the modeltransitioned back to state s1, two times it transi-tioned to state s2, and five times to state s3. Thenwe have:
p?(si|s1, a1) = ?0.3; 0.2; 0.5? = ? 310 ;
2
10 ;
5
10 ? (2)
Additionally, let?s say the same model was in state
s2 and took action a2 1000 times. Following that ac-tion, it transitioned 300 times to state s1, 200 timesto state s2, and 500 times to state s3.
p?(si|s2, a2) = ?0.3; 0.2; 0.5? = ? 3001000 ;
200
1000 ;
500
1000 ? (3)
While both sets of transition parameters have the
same value, the second set of estimates is more reli-
able. The central idea of the proposed approach is to
model this uncertainty in the system parameters, and
use it to numerically construct confidence intervals
for the value of the optimal policy.
Formally, each set of transition probabilities
{p(si|sj , ak)}i=1..n is modeled as a multinomial dis-tribution, estimated from data2. The uncertainty of
multinomial estimates are commonly modeled by
means of a Dirichlet distribution. The Dirichlet dis-
tribution is characterized by a set of parameters ?1,
?2, ..., ?n, which in this case correspond to thecounts {c(si, sj , ak)}i=1..n. For any given j, thelikelihood of the set of multinomial transition pa-
rameters {p(si|sj, ak)}i=1..n is then given by:
P ({p(si|sj , ak)}i=1..n|D) =
= 1Z(D)
?n
i=1 p(si|sj , ak)?i?1 (4)
where Z(D) =
?n
i=1 ?(?i)
?(
?n
i=1 ?i)
and ?i = c(si, sj , ak).
Note that the maximum likelihood estimates for the
formula above correspond to the frequency count
formula we have already described:
p?ML(si|sj, ak) =
?i
?n
i=1 ?i
= c(si, sj, ak)?n
i=1 c(si, sj , ak)(5)
To capture the uncertainty in the model parame-
ters, we therefore simply need to store the counts
of the observed transitions c(si, sj , ak). Based onthis model of uncertainty, we can numerically con-
struct a confidence interval for the value of the opti-
mal policy pi?. Instead of computing the value of the
policy based on the maximum likelihood transition
estimates T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n, we gen-
erate a large number of transition matrices T?1, T?1,... T?m by sampling from the Dirichlet distributionscorresponding to the counts observed in the train-
ing data (in the experiments reported in this paper,
we used m = 1000). We then compute the value
of the optimal policy pi? in each of these models
{Vpi?(T?i)}i=1..m. Finally, we numerically constructthe 95% confidence interval for the value function
based on the resulting value estimates: the bounds
for the confidence interval are set at the lowest and
highest 2.5 percentile of the resulting distribution of
the values for the optimal policy {Vpi?(T?i)}i=1..m.The algorithm is outlined below:
2By p we will denote the true model parameters; by p? we
will denote data-driven estimates for these parameters
279
1. compute transition counts from the training set:
C = {c(si, sj, ak)}k=1..pi,j=1..n (6)
2. compute maximum likelihood estimates for
transition probability matrix:
T?ML = {p?ML(si|sj , ak)}k=1..pi,j=1..n (7)
3. use dynamic programming to compute the op-
timal policy pi? for model T?ML
4. sample m transition matrices {T?k}k=1..m, us-ing the Dirichlet distribution for each row:
{p?i(si|sj, ak)}i=1..n =
= Dir({c(si, sj , ak)}i=1..n) (8)
5. evaluate the optimal policy pi? in each of these
m models, and obtain Vpi?(T?i)
6. numerically build the 95% confidence interval
for Vpi? from these estimates.
To summarize, the central idea is to take into ac-
count the reliability of the transition probability esti-
mates and construct a confidence interval for the ex-
pected cumulative reward for the learned policy. In
the standard approach, we would compute an esti-
mate for the expected cumulative reward, by simply
using the transition probabilities derived from the
training set. Note that these transition probabilities
are simply estimates which are more or less accu-
rate, depending on how much data is available. The
proposed methodology does not fully trust these es-
timates, and asks the question: given that the real
world (i.e. real transition probabilities) might actu-
ally be a bit different than we think it is, how well
can we expect the learned policy to perform? Note
that the confidence interval we construct, and there-
fore the conclusions we draw, are with respect to the
policy learned from the current estimates, i.e. from
the current training set. If more data becomes avail-
able, a different optimal policy might emerge, about
which we cannot say much.
3.2 Related Work
Given the stochastic nature of the models, confi-
dence intervals are often used to estimate the reli-
ability of results in machine learning experiments,
e.g. (Rivals and Personnaz, 2002), (Schapire, 2002)
and (Dumais et al, 1998). In this work we use a
confidence interval methodology in the context of
MDPs. The idea of modeling the uncertainty of
the transition probability estimates using Dirichlet
models also appears in (Jaulmes et al, 2005). In
that work, the authors used the uncertainty in model
parameters to develop active learning strategies for
partially observable MDPs, a topic not previously
addressed in the literature. In our work we rely on
the same model of uncertainty for the transition ma-
trix, but use it to derive confidence intervals for the
expected cumulative reward for the learned optimal
policy, in an effort to assess the reliability of this
policy.
4 Results
Our previous results indicated that Concept Repe-
tition was the best feature to add to the Baseline 2
state-space model, but also that Percent Correctness
and Frustration (when added to Baseline 2) offered
an improvement over the Baseline MDP?s. How-
ever, these conclusions were based on a very quali-
tative approach for determining if a policy is reliable
or not. In the following subsection, we apply our ap-
proach of confidence intervals to empirically deter-
mine if given this data set of 100 dialogues, whether
the estimates of the ECR are reliable, and whether
the original rankings and conclusions hold up under
this refined analysis. In subsection 4.2, we provide
a methodology for pinpointing when one model is
better than another.
4.1 Quantitative Analysis of ECR Reliability
For our first investigation, we look at the confidence
intervals of each MDP?s ECR over the entire data set
of 20 students (later in this section we show plots for
the confidence intervals as data increases). Table 3
shows the upper and lower bounds for the ECR orig-
inally reported in Table 2. The first column shows
the original, estimated ECR of the MDP and the last
column is the width of the bound (the difference be-
tween the upper and lower bound).
So what conclusions can we make about the reli-
ability of the ECR, and hence of the learned policies
for the different MDP?s, given this amount of train-
ing data? The confidence interval for the ECR for
280
State Feature ECR Lower Bound Upper Bound Width
Baseline 1 6.15 0.21 23.73 23.52
Baseline 2 (B2) 31.92 -5.31 60.48 65.79
B2 + Concept Repetition 42.56 28.37 59.29 30.92
B2 + Frustration 32.99 -4.12 61.30 65.42
B2 + Percent Correctness 28.50 -5.89 57.82 63.71
Table 3: Confidence Intervals with complete dataset
the Baseline 1 model ranges from 0.21 to 23.73. Re-
call that the final states are capped at +100 and -100,
and are thus the maximum and minimum bounds
that one can see in this experiment. These bounds
tell us that, if we take into account the uncertainty
in the model estimates (given the small training set
size), with probability 0.95 the actual true ECR for
this policy will be greater than 0.21 and smaller than
23.73. The width of this confidence interval is 23.52.
For the Baseline 2 model, the bounds are much
wider: from -5.31 to 60.48, for a total width of
65.79. While the ECR estimate is 31.92 (which
is seemingly larger than 6.15 for the Baseline 1
model), the wide confidence interval tells us that this
estimate is not very reliable. It is possible that the
policy derived from this model with this amount of
data could perform poorly, and even get a negative
reward. From the dialogue system designer?s stand-
point, a model like this is best avoided.
Of the remaining three models ? Concept Repeti-
tion, Frustration, and Percent Correctness, the first
one exhibits a tighter confidence interval, indicat-
ing that the estimated expected cumulative reward
(42.56) is fairly reliable: with 95% probability of
being between 28.37 and 59.29. The ECR for the
other two models (Frustration and Percent Correct-
ness) again shows a wide confidence interval once
we take into account the uncertainty in the model
parameters.
These results shed more light on the shortcom-
ings of the ECR metric used to evaluate the models
in prior work. This estimate does not take into ac-
count the uncertainty of the model parameters. For
example, a model can have an optimal policy with
a very high ECR value, but have very wide confi-
dence bounds reaching even into negative rewards.
On the other hand, another model can have a rela-
tively lower ECR but if its bounds are tighter (and
the lower bound is not negative), one can know that
that policy is less affected by poor parameter esti-
mates stemming from data sparsity issues. Using the
confidence intervals associated with the ECR gives a
much more refined, quantitative estimate of the reli-
ability of the reward, and hence of the policy derived
from that data.
An extension of this result is that confidence in-
tervals can also allow us to make refined judgments
about the comparative utility of different features,
the original motivation of our prior study. Basi-
cally, a model (M1) is better than another (M2) if
M1?s lower bound is greater than the upper bound of
M2. That is, one knows that 95% of the time, the
worst case situation of M1 (the lower bound) will
always yield a higher reward than the best case of
M2. In our data, this happens only once, with Con-
cept Repetition being empirically better than Base-
line 1, since the lower bound of Concept Repetition
is 28.37 and the upper bound of Baseline 1 is 23.73.
Given this situation, Concept Repetition is a useful
feature which, when included in the model, leads to
a better policy than simply using Correctness. We
cannot draw any conclusions about the other fea-
tures, since their bounds are generally quite wide.
Given this amount of training data, we cannot say
whether Percent Correctness and Frustration are bet-
ter features than the Baseline MDP?s. Although their
ECR?s are higher, there is too much uncertainty to
definitely conclude they are better.
4.2 Pinpointing Model Cross-over
The previous analysis focused on a quantitative
method of (1) determining the reliability of the MDP
ECR estimate and policy, as well as (2) assessing
whether one model is better than another. In this
section, we present an extension to the second con-
tribution by answering the question: given that one
model is more reliable than another, is it possible
to determine at which point one model?s estimates
become more reliable than another model?s? In our
281
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 1
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
0 2 4 6 8 10 12 14 16 18 20
?100
?80
?60
?40
?20
0
20
40
60
80
100
Baseline 2 +Concept Repetition
# of students
EC
R
 
 
Confidence Bounds
Calculated ECR
Figure 1: Confidence Interval Plots
case, we want to know at what point Concept Rep-
etition becomes more reliable than Baseline 1. To
do this, we investigate how the confidence interval
changes as the amount of training data increases in-
stead of looking at the reliability estimate at only one
particular data size.
We incrementally increase the amount of train-
ing data (adding the data from one new student at a
time), and calculate the corresponding optimal pol-
icy and confidence interval for the expected cumula-
tive reward for that policy. Figure 1 shows the con-
fidence interval plots as data is added to the MDP
for the Baseline 1 and Concept Repetition MDP?s.
For reference, Baseline 2, Percent Correctness and
Frustration plots did not exhibit the same converg-
ing behavior as these two, which is not surprising
given how wide the final bounds are. For each plot,
the bold lines represent the upper and lower bounds,
and the dotted line represents the calculated ECR.
Analyzing the two MDP?s, we find that the confi-
dence intervals for Baseline 1 and Concept Repeti-
tion converge as more data is added, which is an ex-
pected trend. One useful result from observing the
change in confidence intervals is that one can de-
termine the point in one which one model becomes
empirically better than another. Superimposing the
upper and lower bounds (Figure 2) reveals that after
we include the data from the first 13 students, the
lower bound of Concept Repetition crosses over the
upper bound of Baseline 1.
Observing this behavior is especially useful for
performing model switching. In automatic model
switching, a dialogue manager runs in real time and
as it collects data, it can switch from using a sim-
ple dialogue model to a complex model. Confidence
intervals can be used to determine when to switch
from one model to the next by checking if a complex
model?s bounds cross over the bounds of the current
model. Basically, the dialogue manager switches
when it can be sure that the more complex model?s
ECR is not only higher, but statistically significantly
so.
0 2 4 6 8 10 12 14 16 18 20
?50
0
50
100
# of students
EC
R
Baseline 1 and Concept Repetition Superimposed
 
 
Baseline 1
B2 + Concept Repetition
Figure 2: Baseline 1 and Concept Repetition Bounds
5 Conclusions
Past work in using MDP?s to improve spoken dia-
logue systems have usually glossed over the issue of
whether or not there was enough training data to de-
velop reliable policies. In this work, we present a
numerical method for building confidence intervals
for the expected cumulative reward for a learned pol-
icy. The proposed approach allows one to (1) better
282
assess the reliability of the expected cumulative re-
ward for a given policy, and (2) perform a refined
comparison between policies derived from different
MDP models.
We applied this methodology to a prior experi-
ment where the objective was to select the best fea-
tures to include in the MDP state-space. Our results
show that policies constructed from the Baseline 1
and Concept Repetition models are more reliable,
given the amount of data available for training. The
Concept Repetition model (which is composed of
the Concept Repetition, Certainty and Correctness
features) was especially useful, as it led to a policy
that outperformed the Baseline 1 model, even when
we take into account the uncertainty in the model
estimates caused by data sparsity. In contrast, for
the Baseline 2, Percent Correctness, and Frustration
models, the estimates for the expected cumulative
reward are much less reliable, and no conclusion can
be reliably drawn about the usefulness of these fea-
tures. In addition, we showed that our confidence
interval approach has applications in another MDP
problem: model switching.
6 Future Work
As an extension of this work, we are currently inves-
tigating in more detail what makes some MDP?s reli-
able or unreliable for a certain data size (such as the
case where Baseline 2 does not converge but a more
complicated model does, such as Concept Repeti-
tion). Our initial findings indicate that, as more data
becomes available the bounds tighten for most pa-
rameters in the transition matrix. However, for some
of the parameters the bounds can remain wide, and
that is enough to keep the confidence interval for the
expected cumulative reward from converging.
Acknowledgments
We would like to thank Jeff Schneider, Drew Bag-
nell, Pam Jordan, as well as the ITSPOKE and Pitt
NLP groups, and the Dialog on Dialogs group for
their help and comments. Finally, we would like to
thank the four anonymous reviewers for their com-
ments on the initial version of this paper. Support for
this research was provided by NSF grants #0325054
and #0328431.
References
S. Dumais, J. Platt, D. Heckerman, and M. Sahami. 1998.Inductive learning algorithms and representations fortext categorization. In Conference on Information and
Knowledge Management.
M. Frampton and O. Lemon. 2005. Reinforcement learn-ing of dialogue strategies using the user?s last dialogueact. In IJCAI Wkshp. on K&R in Practical Dialogue
Systems.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybridreinforcement/supervised learning for dialogue poli-cies from communicator data. In IJCAI Wkshp. on
K&R in Practical Dialogue Systems.
R. Jaulmes, J. Pineau, and D. Precup. 2005. Active learn-ing in partially observable markov decision processes.In European Conference on Machine Learning.
E. Levin and R. Pieraccini. 1997. A stochastic model of
computer-human interaction for learning dialogues. In
Proc. of EUROSPEECH ?97.
T. Paek and D. Chickering. 2005. The markov assump-tion in spoken dialogue management. In 6th SIGDial
Workshop on Discourse and Dialogue.
I. Rivals and L. Personnaz. 2002. Construction of con-fidence intervals for neural networks based on leastsquares estimation. In Neural Networks.
R. Schapire. 2002. The boosting approach to machinelearning: An overview. In MSRI Workshop on Nonlin-
ear Estimation and Classification.
S. Singh, M. Kearns, D. Litman, and M. Walker. 1999.
Reinforcement learning for spoken dialogue systems.In Proc. NIPS ?99.
R. Sutton and A. Barto. 1998. Reinforcement Learning.The MIT Press.
J. Tetreault and D. Litman. 2006. Comparing the utility
of state features in spoken dialogue using reinforce-ment learning. In NAACL.
M. Walker. 2000. An application of reinforcement learn-ing to dialogue strategy selection in a spoken dialogue
system for email. JAIR, 12.
283
Proceedings of NAACL HLT 2007, Companion Volume, pages 9?12,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Conquest ? an Open-Source Dialog System for Conferences 
 
Dan Bohus, Sergio Grau Puerto, David Huggins-Daines, Venkatesh Keri,  
Gopala Krishna,  Rohit Kumar, Antoine Raux, Stefanie Tomko 
School of Computer Science 
Carnegie Mellon University 
{ dbohus, sgrau, dhuggins, vkeri, gopalakr, rohitk, antoine, stef }@ cs.cmu.edu 
 
 
  
Abstract 
We describe ConQuest, an open-source, 
reusable spoken dialog system that pro-
vides technical program information dur-
ing conferences. The system uses a 
transparent, modular and open infrastruc-
ture, and aims to enable applied research 
in spoken language interfaces. The con-
ference domain is a good platform for ap-
plied research since it permits periodical 
redeployments and evaluations with a real 
user-base. In this paper, we describe the 
system?s functionality, overall architec-
ture, and we discuss two initial deploy-
ments.  
1 Introduction  
Conducting applied spoken language interface re-
search is generally a costly endeavor. Developing, 
deploying and maintaining real-world spoken lan-
guage interfaces requires an existing infrastructure, 
a significant amount of engineering effort, and can 
greatly benefit from the availability of certain re-
sources such as transcribed in-domain data.  
In an effort to enable applied research and to 
lower this high cost of entry, we have developed 
ConQuest (Conference Questions) an open-source 
spoken dialog system that provides access to 
schedule and technical program information during 
conferences. We believe the conference domain 
has a number of good properties for applied re-
search: it includes a number of tasks of different 
complexities, it provides regular access to a real-
world user population; it permits periodical rede-
ployments and evaluations and therefore can pro-
vide a natural common evaluation task for the 
spoken language interfaces community.  
The ConQuest system is constructed on top of 
the open, transparent and modular Olympus dialog 
system framework (2007), and can be easily reused 
across different conferences. To date, the system 
has been deployed in two conferences: InterSpeech 
2006 and IJCAI 2007. Together with corpora col-
lected from these deployments, the system is freely 
available for download (Conquest, 2007).  
We begin by describing the ConQuest function-
ality in the next section. Then, in section 3 we pro-
vide an overview of the system architecture and 
discuss the development process. In section 4 we 
briefly discuss the two deployment efforts. Finally, 
in section 5 we discuss related work and draw a 
number of conclusions.   
2 Functionality 
As Figure 1 illustrates, ConQuest is a mixed-
initiative spoken dialog system that provides ac-
cess to schedule and technical program information 
during conferences.  
Users can browse the schedule and find details 
about various papers or sessions of interest by pro-
viding identifying information, such as topics, ses-
sion names, special events, paper titles, author 
names, specific dates and times, specific locations, 
or a combination thereof (e.g. turns 2, 4, 14). Addi-
tionally, the system also allows uses to listen to 
current announcements and to cast their votes for a 
Best Paper Award (e.g. turns 10-17).  
The interaction is mixed-initiative; at any point, 
the user may switch the current focus of the con-
versation (e.g. turn 8). ConQuest employs an adap-
tive information presentation strategy that allows 
9
users to easily navigate the schedule (see turns 3, 5 
and 15). The system uses a rich repertoire of error 
recovery strategies to handle potential errors, in-
cluding several fall-back strategies (e.g. turn 13).  
3 System Architecture  
The ConQuest system was built using RavenClaw/ 
Olympus (2007), an open-source framework that 
facilitates research and development in task ori-
ented conversational spoken language interfaces. 
Olympus consists of a collection of components 
for recognition, language understanding, dialog 
management, language generation, speech synthe-
sis, etc., and the corresponding communication 
infrastructure. To date, Olympus has been used to 
develop and deploy a number of other systems 
spanning different domains and interaction types 
(Bohus and Rudnicky, 2003).  
A key characteristic of the Olympus framework 
is a clear separation between the domain independ-
ent programs (or components) and domain specific 
resources. This decoupling promotes reusability 
and significantly lessens the system development 
effort. In ConQuest, the authoring effort was fo-
cused on developing resources such as the lexicon, 
language model, grammar, dialog task specifica-
tion, etc. Some interesting, unanticipated engineer-
ing challenges we faced during development were 
dealing with foreign names and accented charac-
ters and performing text normalization on various 
fields (e.g. Alex Smith and Alexander Smith are 
the same author), while at the same time ensuring 
consistency between these various resources. Be-
low, we briefly comment of each component and 
the corresponding resources. Figure 2 provides a 
top-level architectural view. 
Speech Recognition. ConQuest uses a recogni-
tion server coupled to a set of parallel recognition 
engines: two SPHINX-II decoders (Huang et al, 
1992) that use gender-specific acoustic models, 
and a DTMF (touch-tone decoder). Each recogni-
tion engine uses class-based (e.g. paper titles, au-
thor names, etc.), state-specific trigram-language 
models. We started with an initial language model 
built using data collected with an early text-only 
prototype. We then internally deployed a speech 
based system, collected more data, transcribed it, 
and used it to retrain the language models. The 
Figure 1. A sample interaction with ConQuest 
1 S: Welcome to ConQuest [?] I can give you information about papers, sessions, special events and announcements at Inter-
Speech 2006. You can also cast a vote for the best paper award. How can I help you? 
2 U: are there any papers on spoken dialog systems today 
3 S:  There are 23 papers on spoken dialog systems today. These papers are being presented in 3 sessions: Dialog and Discourse, 
Language Modeling for Spoken Dialog Systems and Spoken Dialog Systems I. To browse this list of papers, please tell me the 
author name, the paper title, the time or the venue you?re looking for. If you?re not interested in any of these papers say ?quit?. 
4 U: which ones are in the afternoon 
5 S:  There will be 11 papers on spoken dialog systems this afternoon. Would you like to hear information about all these papers? 
6 U: yes 
7 S:  The list of papers on spoken dialog systems this afternoon is: Paper 1 - ?Dynamic Extension of a Grammar-based Dialogue 
System: Constructing an All-Recipes Knowing Robot?, Paper 2 ?  [?] Would you like more information about any of these? 
8 U: tell me more about the third one 
9 S:  Paper 3 is titled ?System- versus user-initiative dialog strategy for driver information systems?, and was presented yesterday at 2 
pm in the poster session in Allegheny Ballroom Foyer. This paper was written by Chantal Ackermann, and Marion Libossek. 
Would you like more information about any of these? 
8 U: no when is the banquet 
9 S:  The Banquet will be held Wednesday from 7pm to 9:30 pm in the Heintz History Center. The Heinz History Center is located on 
Smallman street between 12th and 13th streets. The History Center is a museum that relates and interprets 250 years of history 
in the Western Pennsylvania area. Is there anything else I can do for you? 
10 U: yes I?d like to cast a vote 
11 S:  Please tell me the title of the paper you?d like to vote for.. 
12 U: I don?t really know the title but I think it had Diane as one of the main authors ? [non-understanding] 
13 S:  Sorry, I didn?t catch that. Please dial the first six letters from the title of the paper you would like to vote for.  
14 U: I don?t know it it?s a paper by Diane Litman 
15 S:  There are 3 papers by Diane Litman. Paper 1 ? ?Discourse Structure and Speech Recognition Problems?, Paper 2 ? [?], Paper 
3 ? [?] Would you like to vote for any of these? 
16 U: Discourse Structure and Speech Recognition Problems 
17 S:  Okay, I?ve recorded your vote for the paper entitled ?Discourse Structure and Speech Recognition Problems? What else can I do 
for you?  
18 U: [?] 
10
final language models used during the InterSpeech 
deployment were trained from on a corpus of 6350 
utterances. The system operated with a lexicon of 
4795 words, which included 659 lexicalized (con-
catenated) paper titles, and 1492 lexicalized author 
names, and 78 lexicalized session names. The pro-
nunciations were generated using CMU Dictionary 
and later manually corrected.  
Language understanding. The system uses the 
Phoenix (Ward and Issar, 1994) robust parser to 
extract concepts from the recognition results. A 
domain-specific shallow semantic grammar was 
developed and concatenated with a domain-
independent grammar for generic expressions like 
[Yes], [No], [Date], [Time], etc.  
Dialog management. ConQuest uses a Raven-
Claw-based dialog manager (Bohus and Rudnicky, 
2003). We developed a dialog task specification 
for the conference schedule domain, expressed as a 
hierarchical plan for the interaction, which the 
RavenClaw engine uses to drive the dialog. In the 
process, the RavenClaw engine automatically pro-
vides additional generic conversational skills such 
as error recovery strategies and support for various 
universal dialog mechanisms (e.g. repeat, start-
over, what-can-I-say, etc.)  
Backend/Database. A backend agent looks up 
schedule information from the database (stored as 
a flat text file). The backend agent also performs 
domain specific pre-lookup normalization (e.g. 
mapping author names to their canonical forms), 
and post-lookup processing of the returned records 
(e.g. clustering papers by sessions). The database 
file serves as starting point for constructing a 
number of other system resources (e.g. language 
model classes, lexicon, etc.)  
Figure 2. The Olympus dialog system reference architecture (a typical system) 
Temporal reference resolution agent. Apart 
from the database agent, the dialog manager also 
communicates with an agent that resolves temporal 
expressions (e.g. tomorrow at four p.m.) into ca-
nonical forms.  
Language generation. ConQuest uses Rosetta, 
a template-based language generation component. 
The authoring effort at this level consisted of writ-
ing various templates for the different system ques-
tions and information presentation prompts.  
Speech synthesis. ConQuest uses the Cepstral 
(2005) speech synthesis engine, configured with an 
open-domain unit selection voice. We manually 
checked and corrected pronunciations for author 
names, various technical terms and abbreviations.  
4 Development and Deployment 
The first development of ConQuest system was 
done for the Interspeech 2006 conference held in 
Pittsburgh, PA. The iterative development process 
involved regular interaction with potential users 
i.e. researchers who regularly attend conferences. 
Seven developers working half time participated in 
this development for about three months. An esti-
mated one man-year of effort was spent. This esti-
mate does not include the effort involved in 
transcribing the data collected after the conference. 
Two systems were deployed at the Interspeech 
2006 conference: a desktop system using a close-
talking microphone placed by the registration desk, 
and a telephone-based system. Throughout the 
conference we collected a corpus of 174 sessions. 
We have orthographically transcribed the user ut-
11
terances and are currently analyzing the data; we 
plan to soon release it to the community, together 
with detailed statistics, the full system logs as well 
as the full system source code (Conquest, 2007). 
Following Interspeech 2006, ConQuest was re-
deployed at IJCAI 2007 conference held in Hy-
derabad, India. The second deployment took an 
estimated two man-months: three developers work-
ing half-time for over a month. The significant 
parts of the second deployment involved incorpo-
rating scheduling data for the IJCAI 2007 and im-
plementing two new requirements i.e. support for 
workshops and Indian English speech recognition. 
The IJCAI development had fewer iterations than 
the first effort. The two desktop systems set up at 
the conference venue collected 129 sessions of 
data. This data is currently being transcribed and 
will soon be released to the community through the 
Conquest website (Conquest, 2007). 
Through these two deployments of ConQuest 
the system specifications have been refined and we 
expect the development time to asymptote to less 
than a month after a few more deployments. 
5 Discussion and Conclusion  
Our primary goal in developing ConQuest was to 
enable research by constructing and releasing an 
open-source, full-fledged dialog system, as well as 
an initial corpus collected with this system. The 
system is built on top of an open, transparent and 
modular infrastructure that facilitates research in 
spoken language interfaces (Olympus, 2007).  
There have been a number of other efforts to 
collect and publish dialog corpora, for instance 
within the DARPA Communicator project. A more 
recent project, that operates in a domain similar to 
ConQuest is DiSCoH, a Dialog System for Confer-
ence Help developed by researchers at AT&T, 
ICSI and Edinburgh University, and deployed dur-
ing the SLT-2006 workshop (Adreani et al, 2006). 
While their goals are similar, i.e. to enable re-
search, DiSCoH and ConQuest differ in a number 
of dimensions. Functionality-wise, DiSCoH offers 
general conference information about the venue, 
accommodation options and costs, paper submis-
sion, etc., while ConQuest provides access to the 
technical schedule and allows participants to vote 
for a best paper award. DiSCoH is built using 
AT&T technology and a call-routing approach; 
ConQuest relies on a plan-based dialog manage-
ment framework (RavenClaw) and an open-source 
infrastructure (Olympus). Finally, the DiSCoH ef-
fort aims to develop a richly annotated dialog cor-
pus to be used for research; ConQuest?s aim is to 
provide both the full system and an initial tran-
scribed and annotated corpus to the community. 
The conference domain is interesting in that it 
allows for frequent redeployment and in theory 
provides regular access to a certain user-base. It 
should therefore facilitate research and periodical 
evaluations. Unfortunately, the dialog corpora col-
lected so far using DiSCoH and ConQuest have 
been somewhat smaller than our initial expecta-
tions. We believe this is largely due to the fact that 
the systems provide information that is already 
accessible to users by other means (paper confer-
ence program, web-sites, etc.). Perhaps combining 
the functionalities of these two systems, and ex-
panding into directions where the system provides 
otherwise hard-to-access information (e.g. local 
restaurants, transportation, etc.) would lead to in-
creased traffic.  
References  
Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D., 
Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS-
CoH: Collecting an Annotated Open Corpus with 
Dialogue Acts and Reward Signals for Natural Lan-
guage Helpdesk, in Proceedings of IEEE SLT-2006 
Workshop, Aruba Beach, Aruba. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech 2003, Geneva, Switzerland. 
Cepstral, LLC, 2005, SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Conquest, 2007, http://www.conquest-dialog.org. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Olympus/RavenClaw web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proceedings of the ARPA Human Language Tech-
nology Workshop, pages 213?216, Plainsboro, NJ. 
12
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32?39,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Olympus: an open-source framework 
for conversational spoken language interface research 
 
 
Dan Bohus, Antoine Raux, Thomas K. Harris, 
Maxine Eskenazi, Alexander I. Rudnicky 
School of Computer Science 
Carnegie Mellon University 
{dbohus, antoine, tkharris, max, air}@cs.cmu.edu 
 
 
 
 
Abstract 
We introduce Olympus, a freely available 
framework for research in conversational 
interfaces. Olympus? open, transparent, 
flexible, modular and scalable nature fa-
cilitates the development of large-scale, 
real-world systems, and enables research 
leading to technological and scientific ad-
vances in conversational spoken language 
interfaces. In this paper, we describe the 
overall architecture, several systems 
spanning different domains, and a number 
of current research efforts supported by 
Olympus.  
1 Introduction  
Spoken language interfaces developed in industrial 
and academic settings differ in terms of goals, the 
types of tasks and research questions addressed, 
and the kinds of resources available.  
In order to be economically viable, most indus-
try groups need to develop real-world applications 
that serve large and varied customer populations. 
As a result, they gain insight into the research 
questions that are truly significant for current-
generation technologies. When needed, they are 
able to focus large resources (typically unavailable 
in academia) on addressing these questions. To 
protect their investments, companies do not gener-
ally disseminate new technologies and results. 
In contrast, academia pursues long-term scien-
tific research goals, which are not tied to immedi-
ate economic returns or customer populations. As a 
result, academic groups are free to explore a larger 
variety of research questions, even with a high risk 
of failure or a lack of immediate payoff. Academic 
groups also engage in a more open exchange of 
ideas and results. However, building spoken lan-
guage interfaces requires significant investments 
that are sometimes beyond the reach of academic 
researchers. As a consequence, research in acade-
mia is oftentimes conducted with toy systems and 
skewed user populations. In turn, this raises ques-
tions about the validity of the results and hinders 
the research impact.  
In an effort to address this problem and facilitate 
research on relevant, real-world questions, we have 
developed Olympus, a freely available framework 
for building and studying conversational spoken 
language interfaces. The Olympus architecture, 
described in Section 3, has its roots in the CMU 
Communicator project (Rudnicky et al, 1999). 
Based on that experience and subsequent projects, 
we have engineered Olympus into an open, trans-
parent, flexible, modular, and scalable architecture. 
To date, Olympus has been used to develop and 
deploy a number of spoken language interfaces 
spanning different domains and interaction types; 
these systems are presented in Section 4. They are 
currently supporting research on diverse aspects of 
spoken language interaction. Section 5 discusses 
three such efforts: error handling, multi-participant 
conversation, and turn-taking. 
We believe that Olympus and other similar tool-
kits, discussed in Section 6, are essential in order 
to bridge the gap between industry and academia. 
Such frameworks lower the cost of entry for re-
32
search on practical conversational interfaces. They 
also promote technology transfer through the reuse 
of components, and support direct comparisons 
between systems and technologies.  
2 Desired characteristics 
While developing Olympus, we identified a num-
ber of characteristics that in our opinion are neces-
sary to effectively support and foster research. The 
framework should be open, transparent, flexible, 
modular, and scalable.  
Open. Complete source code should be avail-
able for all the components so that researchers and 
engineers can inspect and modify it towards their 
ends. Ideally, source code should be free for both 
research and commercial purposes and grow 
through contributions from the user community. 
Transparent / Analytic. Open source code 
promotes transparency, but beyond that researchers 
must be able to analyze the system?s behavior. To 
this end, every component should provide detailed 
accounts of their internal state. Furthermore, tools 
for data visualization and analysis should be an 
integral part of the framework. 
Flexible. The framework should be able to ac-
commodate a wide range of applications and re-
search interests, and allow easy integration of new 
technologies. 
Modular / Reusable. Specific functions (e.g. 
speech recognition, parsing) should be encapsu-
lated in components with rich and well-defined 
interfaces, and an application-independent design. 
This will promote reusability, and will lessen the 
system development effort.  
Scalable. While frameworks that rely on sim-
ple, well established approaches (e.g. finite-state 
dialogs in VoiceXML) allow the development of 
large-scale systems, this is usually not the case for 
frameworks that provide the flexibility and trans-
parency needed for research. However, some re-
search questions are not apparent until one moves 
from toy systems into large-scale applications. The 
framework should strive to not compromise scal-
ability for the sake of flexibility or transparency. 
3 Architecture  
At the high level, a typical Olympus application 
consists of a series of components connected in a 
classical, pipeline architecture, as illustrated by the 
bold components in Figure 1. The audio signal for 
the user utterance is captured and passed through a 
speech recognition module that produces a recog-
nition hypothesis (e.g., two p.m.). The recognition 
hypothesis is then forwarded to a language under-
standing component that extracts the relevant con-
cepts (e.g., [time=2p.m.]), and then through a 
confidence annotation module that assigns a confi-
dence score. Next, a dialog manager integrates this 
semantic input into the current context, and pro-
duces the next action to be taken by the system in 
the form of the semantic output (e.g., {request 
end_time}). A language generation module pro-
duces the corresponding surface form, which is 
subsequently passed to a speech synthesis module 
and rendered as audio.  
Galaxy communication infrastructure. While 
the pipeline shown in bold in Figure 1 captures the 
logical flow of information in the system, in prac-
tice the system components communicate via a 
centralized message-passing infrastructure ? Gal-
axy (Seneff et al, 1998). Each component is im-
plemented as a separate process that connects to a 
traffic router ? the Galaxy hub. The messages are 
sent through the hub, which forwards them to the 
appropriate destination. The routing logic is de-
scribed via a configuration script. 
Speech recognition. Olympus uses the Sphinx 
decoding engine (Huang et al, 1992). A recogni-
tion server captures the audio stream, forwards it to 
a set of parallel recognition engines, and collects 
the corresponding recognition results. The set of 
best hypotheses (one from each engine) is then 
forwarded to the language understanding compo-
nent. The recognition engines can also generate n-
best lists, but that process significantly slows down 
the systems and has not been used live. Interfaces 
to connect Sphinx-II and Sphinx-III engines, as 
well as a DTMF (touch-tone) decoder to the recog-
nition server are currently available. The individual 
recognition engines can use either n-gram- or 
grammar-based language models. Dialog-state 
specific as well as class-based language models are 
supported, and tools for constructing language and 
acoustic models from data are readily available. 
Most of the Olympus systems described in the next 
section use two gender-specific Sphinx-II recog-
nizers in parallel. Other parallel decoder configura-
tions can also be created and used.  
Language understanding is performed by 
Phoenix, a robust semantic parser (Ward and Issar, 
33
1994). Phoenix uses a semantic grammar to parse 
the incoming set of recognition hypotheses. This 
grammar is assembled by concatenating a set of 
reusable grammar rules that capture domain-
independent constructs like [Yes], [No], [Help], 
[Repeat], and [Number], with a set of domain-
specific grammar rules authored by the system de-
veloper. For each recognition hypothesis the output 
of the parser consists of a sequence of slots con-
taining the concepts extracted from the utterance.  
Confidence annotation. From Phoenix, the set 
of parsed hypotheses is passed to Helios, the con-
fidence annotation component. Helios uses features 
from different knowledge sources in the system 
(e.g., recognition, understanding, dialog) to com-
pute a confidence score for each hypothesis. This 
score reflects the probability of correct understand-
ing, i.e. how much the system trusts that the cur-
rent semantic interpretation corresponds to the 
user?s intention. The hypothesis with the highest 
score is forwarded to the dialog manager.  
Dialog management. Olympus uses the Raven-
Claw dialog management framework (Bohus and 
Rudnicky, 2003). In a RavenClaw-based dialog 
manager, the domain-specific dialog task is repre-
sented as a tree whose internal nodes capture the 
hierarchical structure of the dialog, and whose 
leaves encapsulate atomic dialog actions (e.g., ask-
ing a question, providing an answer, accessing a 
database). A domain-independent dialog engine 
executes this dialog task, interprets the input in the 
current dialog context and decides which action to 
engage next. In the process, the dialog manager 
may exchange information with other domain-
specific agents (e.g., application back-end, data-
base access, temporal reference resolution agent). 
Language generation. The semantic output of 
the dialog manager is sent to the Rosetta template-
based language generation component, which pro-
duces the corresponding surface form. Like the 
Phoenix grammar, the language generation tem-
plates are assembled by concatenating a set of pre-
defined, domain-independent templates, with 
manually authored task-specific templates.  
Speech synthesis. The prompts are synthesized 
by the Kalliope speech synthesis module. Kalliope 
can be currently configured to use Festival (Black 
and Lenzo, 2000), which is an open-source speech 
synthesis system, or Cepstral Swift (Cepstral 
2005), a commercial engine. Finally, Kalliope also 
supports the SSML markup language.  
Other components. The various components 
briefly described above form the core of the Olym-
pus dialog system framework. Additional compo-
nents have been created throughout the 
development of various systems, and, given the 
modularity of the architecture, can be easily re-
used. These include a telephony component, a text 
Parsing 
PHOENIX 
Recognition 
Server 
Lang. Gen 
ROSETTA 
Synthesis 
KALLIOPE 
? 
SPHINX SPHINX 
SPHINX 
Confidence 
HELIOS 
HUB 
Text I/O 
TTYSERVER 
Application 
Back-end 
Dialog. Mgr. 
RAVENCLAW 
Date-Time 
resolution 
Process 
Monitor 
Until what time 
would you like  
the room? 
{request end_time} 
Figure 1. The Olympus dialog system reference architecture (a typical system) 
two p.m. [time=2pm] [time=2pm]/0.65 
34
input-and-output interface, and a temporal refer-
ence resolution agent that translates complex date-
time expressions (including relative references, 
holidays, etc.) into a canonical form. Recently, a 
Jabber interface was implemented to support inter-
actions via the popular GoogleTalk internet mes-
saging system. A Skype speech client component 
is also available.  
Data Analysis. Last but not least, a variety of 
tools for logging, data processing and data ana-
lytics are also available as part of the framework. 
These tools have been used for a wide variety of 
tasks ranging from system monitoring, to trends 
analysis, to training of internal models. 
A key characteristic shared by all the Olympus 
components is the clear separation between do-
main-independent programs and domain-specific 
resources. This decoupling promotes reuse and 
lessens the system development effort. To build a 
new system, one can focus simply on developing 
resources (e.g., language model, grammar, dialog 
task specification, generation templates) without 
having to do any programming. On the other hand, 
since all components are open-source, any part of 
the system can be modified, for example to test 
new algorithms or compare approaches. 
4 Systems 
To date, the Olympus framework has been used to 
successfully build and deploy several spoken dia-
log systems spanning different domains and inter-
action types (see Table 1). Given the limited space, 
we discuss only three of these systems in a bit 
more detail: Let?s Go!, LARRI, and TeamTalk. 
More information about the other systems can be 
found in (RavenClaw-Olympus, 2007). 
4.1 Let?s Go! 
The Let?s Go! Bus Information System (Raux et al
2005; 2006) is a telephone-based spoken dialog 
system that provides access to bus schedules. In-
teraction with the system starts with an open 
prompt, followed by a system-directed phase 
where the user is asked the missing information. 
Each of the three or four pieces of information 
provided (origin, destination, time of travel, and 
optional bus route) is explicitly confirmed. The 
system knows 12 bus routes, and about 1800 place 
names. 
Originally developed as an in-lab research sys-
tem, Let?s Go! has been open to the general public 
since March, 2005. Outside of business hours, calls 
to the bus company are transferred to Let?s Go!, 
providing a constant flow of genuine dialogs 
(about 40 calls per weeknight and 70 per weekend 
night). As of March, 2007, a corpus of about 
30,000 calls to the system has been collected and 
partially transcribed and annotated. In itself, this 
publicly available corpus constitutes a unique re-
source for the community. In addition, the system 
itself has been modified for research experiments 
(e.g., Raux et al, 2005, Bohus et al, 2006). Be-
tween-system studies have been conducted by run-
ning several versions of the system in parallel and 
picking one at random for every call. We have re-
cently opened this system to researchers from other 
groups who wish to conduct their own experi-
ments. 
4.2 LARRI 
LARRI (Bohus and Rudnicky, 2002a) is a multi-
modal system for support of maintenance and re-
pair activities for F/A-18 aircraft mechanics. The 
system implements an Interactive Electronic Tech-
nical Manual.  
LARRI integrates a graphical user interface for 
easy visualization of dense technical information 
(e.g., instructions, schematics, video-streams) with 
a spoken dialog system that facilitates information 
access and offers assistance throughout the execu-
tion of procedural tasks. The GUI is accessible via 
a translucent head-worn display connected to a 
wearable client computer. A rotary mouse (dial) 
provides direct access to the GUI elements.  
After an initial log-in phase, LARRI guides the 
user through the selected task, which consists of a 
sequence of steps containing instructions, option-
ally followed by verification questions. Basic steps 
can include animations or short video sequences 
that can be accessed by the user through the GUI 
or through spoken commands. The user can also 
take the initiative and access the documentation, 
either via the GUI or by simple commands such as 
?go to step 15? or ?show me the figure?. 
The Olympus architecture was easily adapted 
for this mobile and multi-modal setting. The wear-
able computer hosts audio input and output clients, 
as well as the graphical user interface. The Galaxy 
hub architecture allows us to easily connect these 
35
components to the rest of the system, which runs 
on a separate server computer. The rotary-mouse 
events from the GUI are rendered as semantic in-
puts and are sent to Helios which in turn forwards 
the corresponding messages to the dialog manager.  
4.3 TeamTalk 
TeamTalk (Harris et al, 2005) is a multi-modal 
interface that facilitates communication between a 
human operator and a team of heterogeneous ro-
bots, and is designed for a multi-robot-assisted 
treasure-hunt domain. The human operator uses 
spoken language in concert with pen-gestures on a 
shared live map to elicit support from teams of ro-
bots. This support comes in the forms of mapping 
unexplored areas, searching explored areas for ob-
jects of interest, and leading the human to said ob-
jects. TeamTalk has been built as a fully functional 
interface to real robots, including the Pioneer 
P2DX and the Segway RMP. In addition, it can 
interface with virtual robots within the high-
fidelity USARSim (Balakirsky et al, 2006) simula-
tion environment. TeamTalk constitutes an excel-
lent platform for multi-agent dialog research. 
To build TeamTalk, we had to address two chal-
lenges to current architecture. The multi-
participant nature of the interaction required multi-
ple dialog managers; the live map with pen-
gestured references required a multi-modal integra-
tion. Again, the flexibility and transparency of the 
Olympus framework allowed for relatively simple 
solutions to both of these challenges. To accom-
modate multi-participant dialog, each robot in the 
domain is associated with its own RavenClaw-
based dialog manager, but all robots share the 
other Olympus components: speech recognition, 
language understanding, language generation and 
speech synthesis. To accommodate the live map 
GUI, a Galaxy server was built in Java that could 
send the user?s inputs to Helios and receive outputs 
from RavenClaw. 
5 Research 
The Olympus framework, along with the systems 
developed using it, provides a robust basis for re-
search in spoken language interfaces. In this sec-
tion, we briefly outline three current research 
efforts supported by this architecture. Information 
about other supported research can be found in 
(RavenClaw-Olympus, 2007). 
5.1 Error handling  
A persistent and important problem in today?s spo-
ken language interfaces is their lack of robustness 
when faced with understanding errors. This prob-
lem stems from current limitations in speech rec-
ognition, and appears across most domains and 
interaction types. In the last three years, we con-
ducted research aimed at improving robustness in 
spoken language interfaces by: (1) endowing them 
with the ability to accurately detect errors, (2) de-
System name Domain / Description Genre 
RoomLine 
(Bohus and Rudnicky 2005) 
telephone-based system that provides support for conference 
room reservation and scheduling within the School of Com-
puter Science at CMU. 
information access (mixed 
initiative) 
Let?s Go! Public 
(Raux et al2005) 
telephone-based system that provides access to bus schedule 
information in the greater Pittsburgh area. 
information access 
(system initiative) 
LARRI 
(Bohus and Rudnicky 2002) 
multi-modal system that provides assistance to F/A-18 aircraft 
personnel during maintenance tasks. 
multi-modal task guidance 
and procedure browsing 
Intelligent Procedure  
Assistant 
(Aist et al2002) 
early prototype for a multi-modal system aimed at providing 
guidance and support to the astronauts on the International 
Space Station during the execution of procedural tasks and 
checklists. 
multi-modal task guidance 
and procedure browsing 
TeamTalk 
(Harris et al2005) 
multi-participant spoken language command-and-control inter-
face for a team of robots in the treasure-hunt domain. 
multi-participant command-
and-control 
VERA telephone-based taskable agent that can be instructed to de-liver messages to a third party and make wake-up calls. 
voice mail / message deliv-
ery 
Madeleine text-based dialog system for medical diagnosis. diagnosis 
ConQuest 
(Bohus et al2007) 
telephone-based spoken dialog system that provides confer-
ence schedule information. 
information access 
(mixed-initiative) 
RavenCalendar 
(Stenchikova et al2007). 
multimodal dialog system for managing personal calendar 
information, such as meetings, classes, appointments and 
reminders (uses Google Calendar as a back-end)  
information access and 
scheduling 
 Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) 
36
veloping a rich repertoire of error recovery strate-
gies and (3) developing scalable, data-driven ap-
proaches for building error recovery policies1. Two 
of the dialog systems from Table 1 (Let?s Go! and 
RoomLine) have provided a realistic experimental 
platform for investigating these issues and evaluat-
ing the proposed solutions.   
With respect to error detection, we have devel-
oped tools for learning confidence annotation 
models by integrating information from multiple 
knowledge sources in the system (Bohus and Rud-
nicky, 2002b). Additionally, Bohus and Rudnicky 
(2006) proposed a data-driven approach for con-
structing more accurate beliefs in spoken language 
interfaces by integrating information across multi-
ple turns in the conversation. Experiments with the 
RoomLine system showed that the proposed belief 
updating models led to significant improvements 
(equivalent with a 13.5% absolute reduction in 
WER) in both the effectiveness and the efficiency 
of the interaction.  
With respect to error recovery strategies, we 
have developed and evaluated a large set of strate-
gies for handling misunderstandings and non-
understandings (Bohus and Rudnicky, 2005). The 
strategies are implemented in a task-decoupled 
manner in the RavenClaw dialog management 
framework. 
Finally, in (Bohus et al, 2006) we have pro-
posed a novel online-learning based approach for 
building error recovery policies over a large set 
of non-understanding recovery strategies. An em-
pirical evaluation conducted in the context of the 
Let?s Go! system showed that the proposed ap-
proach led to a 12.5% increase in the non-
understanding recovery rate; this improvement was 
attained in a relatively short (10-day) time period.  
The models, tools and strategies developed 
throughout this research can and have been easily 
reused in other Olympus-based systems. 
5.2 Multi-participant conversation  
Conversational interfaces are generally built for 
one-on-one conversation. This has been a workable 
assumption for telephone-based systems, and a 
useful one for many single-purpose applications. 
However this assumption will soon become 
strained as a growing collection of always-
                                                          
1
 A policy specifies how the system should choose between 
different recovery strategies at runtime.  
available agents (e.g., personal trainers, pedestrian 
guides, or calendar systems) and embodied agents 
(e.g., appliances and robots) feature spoken lan-
guage interfaces. When there are multiple active 
agents that wish to engage in spoken dialog, new 
issues arise. On the input side, the agents need to 
be able to identify the addressee of any given user 
utterance. On the output side, the agents need to 
address the problem of channel contention, i.e., 
multiple participants speaking over each other. 
Two architectural solutions can be envisioned: 
(1) the agents share a single interface that under-
stands multi-agent requirements, or (2) each agent 
uses its own interface and handles multi-participant 
behavior. Agents that provide different services 
have different dialog requirements, and we believe 
this makes a centralized interface problematic. Fur-
thermore, the second solution better fits human 
communication behavior and therefore is likely to 
be more natural and habitable.  
TeamTalk is a conversational system that fol-
lows the second approach: each robot has its own 
dialog manager. Processed user inputs are sent to 
all dialog managers in the system; each dialog 
manager decides based on a simple algorithm 
(Harris et al, 2004) whether or not the current in-
put is addressed to it. If so, an action is taken. Oth-
erwise the input is ignored; it will be processed and 
responded to by another robot. On the output side, 
to address the channel contention problem, each 
RavenClaw output message is augmented with in-
formation about the identity of the robot that gen-
erated it. The shared synthesis component queues 
the messages and plays them back sequentially 
with the corresponding voice. 
We are currently looking into two additional 
challenges related to multi-participant dialog. We 
are interested in how to address groups and sub-
groups in addition to individuals of a group, and 
we are also interested in how to cope with multiple 
humans in addition to multiple agents. Some ex-
periments investigating solutions to both of these 
issues have been conducted. Analysis of the results 
and refinements of these methods are ongoing. 
5.3 Timing and turn-taking  
While a lot of research has focused on higher lev-
els of conversation such as natural language under-
standing and dialog planning, low-level inter-
actional phenomena such as turn-taking have not 
37
received as much attention. As a result, current 
systems either constrain the interaction to a rigid 
one-speaker-at-a-time style or expose themselves 
to interactional problems such as inappropriate 
delays, spurious interruptions, or turn over-taking 
(Raux et al, 2006). To a large extent, these issues 
stem from the fact that in common dialog architec-
tures, including Olympus, the dialog manager 
works asynchronously from the real world (i.e., 
utterances and actions that are planned are as-
sumed to be executed instantaneously). This means 
that user barge-ins and backchannels are often in-
terpreted in an incorrect context, which leads to 
confusion, unexpected user behavior and potential 
dialog breakdowns. Additionally, dialog systems? 
low-level interactional behavior is generally the 
result of ad-hoc rules encoded in different compo-
nents that are not precisely coordinated. 
In order to investigate and resolve these is-
sues, we are currently developing version 2 of the 
Olympus framework. In addition to all the compo-
nents described in this paper, Olympus 2 features 
an Interaction Manager which handles the precise 
timing of events perceived from the real world 
(e.g., user utterances) and of system actions (e.g., 
prompts). By providing an interface between the 
actual conversation and the asynchronous dialog 
manager, Olympus 2 allows a more reactive behav-
ior without sacrificing the powerful dialog man-
agement features offered by RavenClaw. Olympus 
2 is designed so that current Olympus-based sys-
tems can be upgraded with minimal effort.  
This novel architecture, initially deployed in 
the Let?s Go system, will enable research on turn-
taking and other low-level conversational phenom-
ena. Investigations within the context of other ex-
isting systems, such as LARRI and TeamTalk, will 
uncover novel challenges and research directions.  
6 Discussion and conclusion 
The primary goal of the Olympus framework is to 
enable research that leads to technological and sci-
entific advances in spoken language interfaces.  
Olympus is however by no means a singular ef-
fort. Several other toolkits for research and devel-
opment are available to the community. They 
differ on a number of dimensions, such as objec-
tives, scientific underpinnings, as well as techno-
logical and implementation aspects. Several 
toolkits, both commercial, e.g., TellMe, BeVocal, 
and academic, e.g., Ariadne (2007), SpeechBuilder 
(Glass et al, 2004), and the CSLU toolkit (Cole, 
1999), are used for rapid development. Some, e.g., 
CSLU and SpeechBuilder, have also been used for 
educational purposes. And yet others, such as 
Olympus, GALATEEA (Kawamoto et al, 2002) 
and DIPPER (Bos et al, 2003) are primarily used 
for research. Different toolkits rely on different 
theories and dialog representations: finite-state, 
slot-filling, plan-based, information state-update. 
Each toolkit balances tradeoffs between complex-
ity, ease-of-use, control, robustness, flexibility, etc. 
We believe the strengths of the Olympus 
framework lie not only in its current components, 
but also in its open, transparent, and flexible na-
ture. As we have seen in the previous sections, 
these characteristics have allowed us to develop 
and deploy practical, real-world systems operating 
in a broad spectrum of domains. Through these 
systems, Olympus provides an excellent basis for 
research on a wide variety of spoken dialog issues. 
The modular construction promotes the transfer 
and reuse of research contributions across systems.  
While desirable, an in-depth understanding of 
the differences between all these toolkits remains 
an open question. We believe that an open ex-
change of experiences and resources across toolkits 
will create a better understanding of the current 
state-of-the-art, generate new ideas, and lead to 
better systems for everyone. Towards this end, we 
are making the Olympus framework, as well as a 
number of systems and dialog corpora, freely 
available to the community. 
Acknowledgements 
We would like to thank all those who have brought 
contributions to the components underlying the 
Olympus dialog system framework. Neither Olym-
pus nor the dialog systems discussed in this paper 
would have been possible without their help. We 
particularly wish to thank Alan W Black for his 
continued support and advice. Work on Olympus 
components and systems was supported in part by 
DARPA, under contract NBCH-D-03-0010, Boe-
ing, under contract CMU-BA-GTA-1, and the US 
National Science Foundation under grant number 
0208835. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the authors and do not necessarily reflect 
the views of the funding agencies.  
38
References  
Aist, G., Dowding, J., Hockey, B.A., Rayner, M., 
Hieronymus, J., Bohus, D., Boven, B., Blaylock, N., 
Campana, E., Early, S., Gorrell, G., and Phan, S., 
2003. Talking through procedures: An intelligent 
Space Station procedure assistant, in Proc. of EACL-
2003, Budapest, Hungary 
Ariadne, 2007, The Ariadne web-site, as of January 
2007, http://www.opendialog.org/. 
Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M. 
2006. UsarSim: providing a framework for multi-
robot performance evaluation, in Proc. of PerMIS. 
Black, A. and Lenzo, K., 2000. Building Voices in the 
Festival Speech System, http://festvox.org/bsv/, 2000. 
Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri, 
V., Krishna, G., Kumar, K., Raux, A., Tomko, S., 
2007. Conquest ? an Open-Source Dialog System for 
Conferences, in Proc. of HLT 2007, Rochester, USA. 
Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi, 
M., Rudnicky, A.  2006.  Online Supervised Learning 
of Non-understanding Recovery Policies, in Proc. of 
SLT-2006, Aruba.  
Bohus, D., and Rudnicky, A.  2006.  A K-hypotheses + 
Other Belief Updating Model, in Proc. of the AAAI 
Workshop on Statistical and Empirical Methods in 
Spoken Dialogue Systems, 2006. 
Bohus, D., and Rudnicky, A.,  2005.  Sorry I didn?t 
Catch That: An Investigation of Non-understanding 
Errors and Recovery Strategies, in Proc. of SIGdial-
2005, Lisbon, Portugal. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proc. of Eu-
rospeech 2003, Geneva, Switzerland. 
Bohus, D., and Rudnicky, A., 2002a. LARRI: A Lan-
guage-based Maintenance and Repair Assistant, in 
Proc. of IDS-2002, Kloster Irsee, Germany. 
Bohus, D., and Rudnicky, A., 2002b. Integrating Multi-
ple Knowledge Sources in the CMU Communicator 
Dialog System, Technical Report CMU-CS-02-190. 
Bos, J., Klein, E., Lemon, O., and Oka, T., 2003. 
DIPPER: Description and Formalisation of an In-
formation-State Update Dialogue System Architec-
ture, in Proc. of SIGdial-2003, Sapporo, Japan 
Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Cole, R., 1999. Tools for Research and Education in 
Speech Science, in Proc. of the International Confer-
ence of Phonetic Sciences, San Francisco, USA. 
Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004. 
A Framework for Developing Conversational Inter-
faces, in Proc. of CADUI, Funchal, Portugal. 
Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J. 
Bodine, K., and Black, A. 2004. A Research Platform 
for Multi-Agent Dialogue Dynamics, in Proc. of The 
IEEE International Workshop on Robotics and Hu-
man Interactive Communications, Kurashiki, Japan. 
Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Hetero-
genous Multi-Robot Dialogues for Search Tasks, in 
AAAI Spring Symposium: Dialogical Robots, Palo 
Alto, California. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Kawamoto, S.,  Shimodaira, H., Nitta, T., Nishimoto, 
T., Nakamura, S., Itou, K., Morishima, S., Yotsukura, 
T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T., 
Tokuda, K., Hirose, K., Minematsu, N., Yamada, A., 
Den, Y., Utsuro, T., and Sagayama, S., 2002. Open-
source software for developing anthropomorphic 
spoken dialog agent, in Proc. of PRICAI-02, Interna-
tional Workshop on Lifelike Animated Agents. 
Raux, A., Langner, B., Bohus, D., Black, A., and Eske-
nazi, M.  2005, Let's Go Public! Taking a Spoken 
Dialog System to the Real World, in Proc. of Inter-
speech 2005, Lisbon, Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. 2006 Doing Research on a Deployed Spoken 
Dialogue System: One Year of Let's Go! Experience, 
in Proc. of Interspeech 2006, Pittsburgh, USA. 
RavenClaw-Olympus web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., 
Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Cre-
ating natural dialogs in the Carnegie Mellon Com-
municator system, in Proc. of Eurospeech 1999. 
Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and 
Zue V. 1998 Galaxy-II: A reference architecture for 
conversational system development, in Proc. of 
ICSLP98, Sydney, Australia. 
Stenchikova, S., Mucha, B., Hoffman, S., Stent, A., 
2007. RavenCalendar: A Multimodal Dialog System 
for Managing A Personal Calendar, in Proc. of HLT 
2007, Rochester, USA.  
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proc. of the ARPA Human Language Technology 
Workshop, pages 213?216, Plainsboro, NJ. 
39
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 21?28,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Learning N-Best Correction Models from Implicit User Feedback  
in a Multi-Modal Local Search Application 
 
 
Dan Bohus, Xiao Li, Patrick Nguyen, Geoffrey Zweig 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
{dbohus, xiaol, panguyen, gzweig}@microsoft.com 
 
 
 
 
 
 
Abstract 
We describe a novel n-best correction model 
that can leverage implicit user feedback (in 
the form of clicks) to improve performance in 
a multi-modal speech-search application. The 
proposed model works in two stages. First, the 
n-best list generated by the speech recognizer 
is expanded with additional candidates, based 
on confusability information captured via user 
click statistics. In the second stage, this ex-
panded list is rescored and pruned to produce 
a more accurate and compact n-best list. Re-
sults indicate that the proposed n-best correc-
tion model leads to significant improvements 
over the existing baseline, as well as other tra-
ditional n-best rescoring approaches.  
1 Introduction 
Supported by years of research in speech recogni-
tion and related technologies, as well as advances 
in mobile devices, speech-enabled mobile applica-
tions are finally transitioning into day-to-day use. 
One example is Live Search for Windows Mobile 
(2008), a speech-enabled application that allows 
users to get access to local information by speaking 
a query into their device. Several other systems 
operating in similar domains have recently become 
available (TellMeByMobile, 2008; Nuance Mobile 
Search, 2008; V-Lingo Mobile, 2008; VoiceSignal 
Search, 2008.) 
Traditionally, multi-modal systems leverage the 
additional input channels such as text or buttons to 
compensate for the current shortcomings of speech 
recognition technology. For instance, after the user 
speaks a query, the Live Search for Windows Mo-
bile application displays a confirmation screen that 
contains the n-best recognition results. The user 
selects the correct hypothesis using the buttons on 
the device, and only then the system displays the 
corresponding search results (see Figure 1.) 
We argue that ideally multi-modal systems 
could use the additional, more accurate input chan-
nels not only for confirmation or immediate cor-
rection, but also to learn from the interaction and 
improve their performance over time, without ex-
plicit human supervision. For example, in the inte-
raction paradigm described above, apart from 
providing the means for selecting the correct rec-
ognition result from an n-best list, the user click on 
a hypothesis can provide valuable information 
about the errors made by system, which could be 
exploited to further improve performance.  
Consider for instance the following numbers 
from an analysis of logged click data in the Live 
Search for Windows Mobile system. Over a certain 
period of time, the results Beer and Gear were dis-
played together in an n-best list 122 times. Out of 
these cases, Beer was clicked 67% of the time, and 
Gear was never clicked. In 25% of the cases when 
Beer was selected, Gear was incorrectly presented 
above (i.e. higher than) Beer in the n-best list. 
More importantly, there are also 82 cases in which 
Gear appears in an n-best list, but Beer does not. A 
manual inspection reveals that, in 22% of these 
cases, the actual spoken utterance was indeed Beer. 
The clicks therefore indicate that the engine often 
misrecognizes Gear instead of Beer.  
21
Ideally, the system should be able to take advan-
tage of this information and use the clicks to create 
an automatic positive feedback loop. We can envi-
sion several ways in which this could be accom-
plished. A possible approach would be to use all 
the clicked results to adapt the existing language or 
acoustic models. Another, higher-level approach is 
to treat the recognition process as a black-box, and 
use the click feedback (perhaps also in conjunction 
with other high-level information) to post-process 
the results recognition results. 
While both approaches have their merits, in this 
work we concentrate on the latter paradigm. We 
introduce a novel n-best correction model that le-
verages the click data to improve performance in a 
speech-enabled multi-modal application. The pro-
posed model works in two stages. First, the n-best 
list generated by the speech recognizer is expanded 
with additional candidates, based on results confu-
sability information captured by the click statistics. 
For instance, in the 82 cases we mentioned above 
when Gear was present in the n-best list but Beer 
was not, Beer (as well as potentially other results) 
would also be added to form an expanded n-best 
list. The expanded list is then rescored and pruned 
to construct a corrected, more accurate n-best list.  
The proposed approach, described in detail in 
Section 3, draws inspiration from earlier work in 
post-recognition error-correction models (Ringger 
and Allen, 1996; Ringger and Allen, 1997) and n-
best rescoring (Chotimongkol and Rudnicky, 2001; 
Birkenes et al, 2007). The novelty of our approach 
lies in: (1) the use of user click data in a deployed 
multi-modal system for creating a positive feed-
back loop, and (2) the development of an n-best 
correction model based on implicit feedback that 
outperforms traditional rescoring-only approaches. 
Later on, in Section 5, we will discuss in more de-
tail the relationship of the proposed approach to 
these and other works previously reported in the 
literature.  
Before moving on to describe the n-best correc-
tion model in more detail, we give a high-level 
overview of Live Search for Windows Mobile, the 
multi-modal, mobile local search application that 
provided the test-bed for evaluating this work.  
2 Live Search for Windows Mobile  
Live Search for Windows Mobile is an application 
that enables local web-search on mobile devices. In 
its current version, it allows users to find informa-
tion about local businesses and restaurants, to ob-
tain driving directions, explore maps, view current 
traffic, get movie show-times, etc. A number of 
screen-shots are illustrated in Figure 1. 
Recently, Live Search for Windows Mobile has 
been extended with a speech interface (notice the 
Speak button assigned to the left soft-key in Figure 
1.a.) The speech-based interaction with the system 
proceeds as follows: the user clicks the Speak but-
ton and speaks the name of a local business, for 
instance A-B-C Hauling, or a general category such 
as Vietnamese Restaurants. The application end-
points the audio and forwards it over the data 
channel to a server (Figure 1.b.) Recognition is 
performed on the server side, and the resulting n-
best list is sent back to the client application, where 
it is displayed to the user (Figure 1.c.) The user can 
select the correct item from the n-best list, re-speak 
the request, or abandon the interaction altogether 
by pressing Cancel. Once the user selects an item in 
the n-best list, the corresponding search results are 
displayed (Figure 1.d.) 
(a) (b) (c) (d) 
Figure 1. Windows Live Search for Mobile. (a) initial screen; (b) user is speaking a request; (c) n-best list 
is presented; (d) final search results are displayed 
22
Apart from business names, the system also 
handles speech input for addresses, as well as 
compound requests, such as Shamiana Restaurant 
in Kirkland, Washington. For the latter cases, a 
two-tier recognition and confirmation process is 
used. In the first stage a location n-best list is gen-
erated and sent to the client for confirmation. After 
the user selects the location, a second recognition 
stage uses a grammar tailored to that specific loca-
tion to re-recognize the utterance. The client then 
displays the final n-best list from which the user 
can select the correct result. 
Several details about the system architecture and 
the structure of the recognition process have been 
omitted here due to space considerations. For the 
interested reader, a more in-depth description of 
this system is available in (Acero et al, 2008).  
3 Approach 
We now turn our attention to the proposed n-best 
correction model 
3.1 Overview 
The model works in two stages, illustrated in Fig-
ure 2. In the first stage the n-best list produced by 
the speech recognizer is expanded with several 
alternative hypotheses. In the second stage, the 
expanded n-best list is rescored to construct the 
final, corrected n-best list.  
The n-best expansion step relies on a result con-
fusion matrix, constructed from click information. 
The matrix, which we will describe in more detail 
in the following subsection, contains information 
about which result was selected (clicked) by the 
user when a certain result was displayed. For in-
stance, in the example from Figure 2, the matrix 
indicates that when Burlington appeared in the n-
best list, Bar was clicked once, Bowling was 
clicked 13 times, Burger King was clicked twice, 
and Burlington was clicked 15 times (see hashed 
row in matrix.) The last element in the row indi-
cates that there were 7 cases in which Burlington 
was decoded, but nothing (?) was clicked. Essen-
tially, the matrix captures information about the 
confusability of different recognition results.  
The expansion step adds to an n-best list gener-
ated by the recognizer all the results that were pre-
viously clicked in conjunction with any one of the 
items in the given n-best list. For instance, in the 
example from Figure 2, the n-best list contains 
Sterling, Stirling, Burlington and Cooling. Based 
on the confusion matrix, this list will be expanded 
to also include Bar, Bowling, Burger King, Tow-
ing, and Turley. In this particular case, the correct 
recognition result, Bowling, is added in the ex-
panded n-best list.  
In the final step, the expanded list is rescored. In 
the previous example, for simplicity of explana-
tion, a simple heuristic for re-scoring was used: 
add all the counts on the columns corresponding to 
each expanded result. As a consequence, the cor-
Burlington 
Cooling 
Sterling 
Stirling 
0  ?   7    0     0    ?     0    0  ?   1   0    9 
0  ?   4    0     0    ?   10    1  ?   2   2    5 
0  ?   4    0     0    ?     4    1  ?   0   0    9 
B
u
rl
in
g
to
n
 
B
o
w
li
n
g
 
B
u
rg
er
 K
in
g
 
T
o
w
in
g
 
T
u
rl
ey
 
S
ti
rl
in
g
 
B
ar
 
S
te
rl
in
g
 
Sterling 
Stirling 
Burlington 
Cooling + ? 
Bowling  28 
Burlington  15 
Sterling  14 
Towing  3 
Burger King  2 
Stirling  2 
Turley  2 
Bar  1 
 
Bar 
Bowling 
Burger King 
Burlington 
Sterling  
Stirling 
Towing  
Turley 
 
? 
Result Confusion Matrix 
Initial  
N-Best 
Expanded 
N-Best 
Corrected 
(expanded & 
rescored) 
N-Best 
Figure 2. A confusion-based n-best correction model 
1  ? 13    2   15    ?     0    0  ?   0   0    7 
?
 
Stage 1: Expansion Stage 2: Rescoring 
23
rect recognition result, Bowling, was pushed to the 
top of the n-best list.  
We begin by formally describing the construc-
tion of the results confusability matrix and the ex-
pansion process in the next two sub-sections. Then, 
we describe three rescoring approaches. The first 
one is based on an error-correction model con-
structed from the confusion matrix. The other two, 
are more traditional rescoring approaches, based 
on language model adaptation.  
3.2 The Result Confusion Matrix 
The result confusion matrix is computed in a sim-
ple traversal of the click logs. The rows in the ma-
trix correspond to decoded results, i.e. results that 
have appeared in an n-best list. The columns in the 
matrix correspond to clicked (or intended) results, 
i.e. results that the user has clicked on in the n-best 
list. The entries at the intersection of row ? and 
column ? correspond to the number of times result 
? was clicked when result ? was decoded: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
In addition, the last column in the matrix, de-
noted ? contains the number of times no result was 
clicked when result ? was displayed: 
 
?? ,? = #(??????? = ?, ??????? = ?). 
 
The rows in the matrix can therefore be used to 
compute the maximum likelihood estimate for the 
conditional probability distribution: 
 
???(?|?) =
?? ,?
 ?? ,??
 . 
 
The full dimensions of the result confusion ma-
trix can grow very large since the matrix is con-
structed at the result level (the average number of 
words per displayed result is 2.01). The number of 
rows equals the number of previously decoded re-
sults, and the number of columns equals the num-
ber of previously clicked results. However, the 
matrix is very sparse and can be stored efficiently 
using a sparse matrix representation. 
3.3 N-Best Expansion 
The first step in the proposed n-best correction 
model is to expand the initial n-best list with all 
results that have been previously clicked in con-
junction with the items in the current n-best list. 
Let?s denote by ? = {??}?=1..?  the initial n-best 
list produced by the speech recognizer. Then, the 
expanded n-best list ?? will contain all ?? , as well 
as all previously clicked results ? such that there 
exists ? with ??? ,? > 0. 
3.4 Confusion Matrix Based Rescoring  
Ideally, we would like to rank the hypotheses in 
the expanded list ?? according to ?(?|?), where ? 
represents the intended result and ? represents the 
acoustics of the spoken utterance. This can be re-
written as follows: 
 
                       ? ? ? =  ?(?|?) ? ?(?|?)? .             [1] 
 
The first component in this model is an error-
correction model ?(?|?). This model describes the 
conditional probability that the correct (or in-
tended) result is ? given that result ? has been de-
coded. While this conditional model cannot be 
constructed directly, we can replace it by a proxy - 
?(?|?), which models the probability that the re-
sult ? will be clicked, given that result ? was de-
coded. As mentioned earlier in subsection 3.2, this 
conditional probability distribution can be com-
puted from the result confusion matrix. In replac-
ing ? ? ?  with ?(?|?), we are making the 
assumption that the clicks correspond indeed to the 
correct, intended results, and to nothing else1. 
Notice that the result confusion matrix is gener-
ally very sparse. The maximum likelihood estima-
tor ???(?|?) will therefore often be inappropriate. 
To address this data sparsity issue, we linearly in-
terpolate the maximum likelihood estimator with 
an overall model ??(?|?): 
 
? ? ? =  ???? ? ? + (1? ?)?? ? ? . 
 
The overall model is defined in terms of two 
constants, ? and ?, as follows: 
 
?? ? ? =  
?, ?? ? = ?
?, ?? ? ? ?
  
 
where ? is the overall probability in the whole 
dataset of clicking on a given decoded result, and 
? is computed such that ?? ? ?  normalizes to 1. 
                                                          
1 While this assumption generally holds, we have also ob-
served cases where it is violated: sometimes users (perhaps 
accidentally) click on an incorrect result; other times the cor-
rect result is in the list but nothing is clicked (perhaps the user 
was simply testing out the recognition capabilities of the sys-
tem, without having an actual information need) 
24
Finally, the ? interpolation parameter is determined 
empirically on the development set.  
The second component in the confusion based 
rescoring model from equation [1] is ?(?|?). This 
is the recognition score for hypothesis ?. The n-
best rescoring model from [1] becomes: 
 
? ? ? =   ???? ? ?? + (1? ?)?? ? ??  ? ?(?? |?)
????
 
3.5 Language Model Based Rescoring 
A more traditional alternative for n-best rescoring 
is to adapt the bigram language model used by the 
system in light of the user click data, and re-rank 
the decoded results by: 
 
? ? ? ? ? ??  ? ? ? ? ?? ?(??) 
 
Here ? ? ??  is the acoustic score assigned by 
the recognizer to hypothesis ?? , and ?(??) is the 
adapted language model score for this hypothesis.  
A simple approach for adapting the system?s 
language model is to add the word sequences of 
the user-clicked results to the original training sen-
tences and to re-estimate the language model ?(?). 
We will refer to this method as maximum likelih-
ood (ML) estimation. A second approach, referred 
to as conditional maximum likelihood (CML) es-
timation, is to adapt the language model such as to 
directly maximize the conditional likelihood of the 
correct result given acoustics, i.e., 
 
? ? ? =
? ? ? ?(?)
 ? ? ?? ?(??)????
 
 
Note that this is the same objective function as 
the one used in Section 3.4, except that here the 
click data is used to estimate the language model 
instead of the error correction model. Again, in 
practice we assume that users click on correct re-
sults, i.e. ? = ?. 
4 Experiments  
We now discuss a number of experiments and the 
results obtained using the proposed n-best correc-
tion approach.  
4.1 Data 
For the purposes of the experiments described be-
low we extracted just over 800,000 queries from 
the server logs in which the recognizer had gener-
ated a simple n-best list2. For each recognition 
event, we collected from the system logs the n-best 
list, and the result clicked by the user (if the user 
clicked on any result).  
In addition, for testing purposes, we also make 
use of 11529 orthographically transcribed user re-
quests. The transcribed set was further divided into 
a development set containing 5680 utterances and 
a test set containing 5849 utterances.  
4.2 Initial N-Best Rescoring 
To tease apart the effects of expansion and rescor-
ing in the proposed n-best correction model, we 
began by using the rescoring techniques on the 
initial n-best lists, without first expanding them. 
Since the actual recognition confidence scores 
?(?? |?) were not available in the system logs, we 
replaced them with an exponential probability den-
sity function based on the rank of the hypothesis:  
 
? ??  ? = 2
??  
 
We then rescored the n-best lists from the test 
set according to the three rescoring models de-
scribed earlier: confusion matrix, maximum like-
lihood (ML), and conditional maximum likelihood 
(CML). We computed the sentence level accuracy 
for the rescored n-best list, at different cutoffs. The 
accuracy was measured by comparing the rescored 
hypotheses against the available transcripts. 
Note that the maximum depth of the n-best lists 
generated by the recognizer is 10; this is the max-
imum number of hypotheses that can be displayed 
on the mobile device. However, the system may 
generate fewer than 10 hypotheses. The observed 
average n-best list size in the test set was 4.2.  
The rescoring results are illustrated in Figure 3 
and reported in Table 1. The X axis in Figure 3 
shows the cutoff at which the n-best accuracy was 
computed. For instance in the baseline system, the 
correct hypothesis was contained in the top result 
in 46.2% of cases, in the top-2 results in 50.5% of 
the cases and in the top-3 results in 51.5% of the 
cases. The results indicate that all the rescoring 
models improve performance relative to the base-
                                                          
2 We did not consider cases where a false-recognition event 
was fired (e.g. if no speech was detected in the audio signal) ? 
in these cases no n-best list is generated. We also did not con-
sider cases where a compound n-best was generated (e.g. for 
compound requests like Shamiana in Kirkland, Washington) 
25
line. The improvement is smallest for the maxi-
mum likelihood (ML) language model rescoring 
approach, but is still statistically significant 
(? = 0.008 in a Wilcoxon sign-rank test.) The con-
fusion-matrix based rescoring and the CML rescor-
ing models perform similarly well, leading to a 1% 
absolute improvement in 1-best and 2-best sen-
tence-level accuracy from the baseline (? < 10?5). 
No statistically significant difference can be de-
tected between these two models. At the same 
time, they both outperform the maximum likelih-
ood rescoring model (? < 0.03). 
4.3 N-Best Correction 
Next, we evaluated the end-to-end n-best correc-
tion approach. The n-best lists were first expanded, 
as described in section 3.3, and the expanded lists 
were ranked using the confusion matrix based res-
coring model described in Section 3.4.  
The expansion process enlarges the original n-
best lists. Immediately after expansion, the average 
n-best size grows from 4.2 to 96.9. The oracle per-
formance for the expanded n-best lists increases to 
59.8% (versus 53.5% in the initial n-best lists.) 
After rescoring, we trimmed the expanded n-best 
lists to a maximum of 10 hypotheses: we still want 
to obey the mobile device display constraint. The 
resulting average n-best size was 7.09 (this is low-
er than 10 since there are cases when the system 
cannot generate enough expansion hypotheses.) 
The sentence-level accuracy of the corrected n-
best lists is displayed in line 4 from Table 1. A di-
rect comparison with the rescoring-only models or 
with the baseline is however unfair, due to the 
larger average size of the corrected n-best lists. To 
create a fair comparison and to better understand 
the performance of the n-best correction process, 
we pruned the corrected n-best lists by eliminating 
all hypotheses with a score below a certain thre-
shold. By varying this rejection threshold, we can 
therefore control the average depth of the resulting 
corrected n-best lists. At a rejection threshold of 
0.004, the average corrected n-best size is 4.15, 
comparable to the baseline of 4.2 .  
The performance for the corresponding cor-
rected (and pruned) n-best lists is shown in line 5 
from Table 1 and illustrated in Figure 4. In contrast 
to a rescoring-only approach, the expansion pro-
cess allows for improved performance at higher 
depths in the n-best list. The maximum n-best per-
formance (while keeping the average n-best size at 
4.15), is 56.5%, a 3% absolute improvement over 
the baseline (? < 10?5).  
Figure 5 provides more insight into the relation-
ship between the sentence-level accuracy of the 
corrected (and pruned) n-best lists and the average 
n-best size (the plot was generated by varying the 
rejection threshold.) The result we discussed above 
can also be observed here: at the same average n-
best size, the n-best correction model significantly 
outperforms the baseline. Furthermore, we can see 
that we can attain the same level of accuracy as the 
baseline system while cutting the average n-best 
size by more than 50%, from 4.22 to 2. In the op-
posite direction, if we are less sensitive to the 
number of items displayed in the n-best list (except 
for the 10-maximum constraint we already obey), 
we can further increase the overall performance by 
another 0.8% absolute to 57.3%; this overall accu-
racy is attained at an average n-best size of 7.09.  
Figure 3. Initial n-best rescoring (test-set) 
Table 1. Test-set sentence-level n-best accuracy; 
(0) baseline; (1)-(3) initial n-best rescoring;  
(4)-(5) expansion + rescoring 
 Model 1-
Best 
2-
Best 
3-
Best 
10-
Best 
0 Baseline 46.2 50.5 51.5 53.5 
1 ML Rescoring  46.8 50.9 52.1 53.5 
2 CML Rescoring 47.4 51.4 52.6 53.5 
3 Confusion Matrix Resc. 47.3 51.5 52.5 53.5 
4 Expansion + Rescoring 
(size=7.09) 
46.8 52.3 54.5 57.3 
5 Expansion + Rescoring 
(size=4.15) 
46.8 52.3 54.4 56.5 
 
26
Finally, we also investigated rescoring the ex-
panded n-best lists using the CML approach. To 
apply CML, an initial ranking of the expanded n-
best lists is however needed. If we use the ranking 
produced by the confusion-matrix based model 
discussed above, no further performance improve-
ments can be observed.  
5 Related work 
The n-best correction model we have described in 
this paper draws inspiration from earlier works on 
post-recognition error correction models, n-best 
rescoring and implicitly supervised learning. In 
this section we discuss some of the similarities and 
differences between the proposed approach and 
previous work. 
The idea of correcting speech recognition errors 
in a post-processing step has been proposed earlier 
by (Ringger and Allen, 1996; Ringger and Allen, 
1997). The authors showed that, in the presence of 
transcribed data, a translation-based post-processor 
can be trained to correct the results of a speech 
recognizer, leading to a 15% relative WER im-
provement in a corpus of TRAINS-95 dialogues.  
The n-best correction approach described here is 
different in two important aspects. First, instead of 
making use of transcripts, the proposed error-
correction model is trained using implicit user 
feedback obtained in a multi-modal interface (in 
this case user clicks in the n-best list.) This is a less 
costly endeavor, as the system automatically ob-
tains the supervision signal directly from the inte-
raction; no transcripts are necessary. Second, the 
approach operates on the entire n-best list, rather 
than only on the top hypothesis; as such, it has ad-
ditional information that can be helpful in making 
corrections. At Figure 2 illustrates, there is a poten-
tial for multiple incorrect hypotheses to point to-
wards and reinforce the same correction 
hypothesis, leading to improved performance (in 
this example, Burlington, Cooling, Sterling and 
Stirling were all highly confusable with Bowling, 
which was the correct hypothesis). 
The n-best correction model we have described 
includes a rescoring step. N-best rescoring ap-
proaches have been investigated extensively in the 
speech recognition community. In the dialog 
community, n-best rescoring techniques that use 
higher-level, dialog features have also been pro-
posed and evaluated (Chotimongkol and Rudnicky, 
2001). Apart from using the click feedback, the 
novelty in our approach lies in the added expansion 
step and in the use of an error-correction model for 
rescoring. We have seen that the confusability-
based n-best expansion process leads to signifi-
cantly improved performance, even if we force the 
model to keep the same average n-best size. 
Finally, the work discussed in this paper has 
commonalities with previous works on lightly su-
pervised learning in the speech community, e.g. 
(Lamel and Gauvain, 2002) and leveraging implicit 
feedback for learning from interaction, e.g. (Baner-
jee and Rudnicky, 2007; Bohus and Rudnicky, 
2007). In all these cases, the goal is to minimize 
the need for manually-labeled data, and learn di-
Figure 5. Overall n-best accuracy as a function of 
the average n-best size  
53.5% 
56.5% 
57.3% 
Figure 4. N-Best correction (test-set) 
27
rectly from the interaction. We believe that in the 
long term this family of learning techniques will 
play a key role towards building autonomous, self-
improving systems. 
6 Conclusion and future work 
We have proposed and evaluated a novel n-best 
correction model that leverages implicit user feed-
back in a multi-modal interface to create a positive 
feedback loop. While the experiments reported 
here were conducted in the context of a local 
search application, the approach is applicable in 
any multi-modal interface that elicits selection in 
an n-best list from the user.  
The proposed n-best correction model works in 
two stages. First, the n-best list generated by the 
speech recognizer is expanded with additional hy-
potheses based on confusability information cap-
tured from previous user clicks. This expanded list 
is then rescored and pruned to create a more accu-
rate and more compact n-best list. Our experiments 
show that the proposed n-best correction approach 
significantly outperforms both the baseline and 
other traditional n-best rescoring approaches, with-
out increasing the average length of the n-best lists.  
Several issues remain to be investigated. The 
models discussed in this paper focus on post-
recognition processing. Other ways of using the 
click data can also be envisioned. For instance, one 
approach would be to add all the clicked results to 
the existing language model training data and 
create an updated recognition language model. In 
the future, we plan to investigate the relationship 
between these two approaches, and to whether they 
can be used in conjunction. Earlier related work 
(Ringger and Allen, 1997) suggests that this should 
indeed be the case. 
Second, the click-based error-correction model 
we have described in section 3.4 operates at the 
result level. The proposed model is essentially a 
sentence level, memory-based translation model. 
In the future, we also plan to investigate word-
level error-correction models, using machine trans-
lation techniques like the ones discussed in (Ring-
ger and Allen, 1997; Li et al, 2008). 
Finally, we plan to investigate how this process 
of learning from implicit feedback in a multi-
modal interface can be streamlined, such that the 
system continuously learns online, with a minimal 
amount of human intervention.  
Acknowledgments 
This work would have not been possible without 
the help of a number of other people. We would 
like to especially thank Oliver Scholz, Julian 
Odell, Christopher Dac, Tim Paek, Y.C. Ju, Paul 
Bennett, Eric Horvitz and Alex Acero for their 
help and for useful conversations and feedback. 
References  
Acero, A., N. Bernstein, et al (2008). "Live Search for 
Mobile: Web Services by Voice on the Cellphone". 
ICASSP'08. Las Vegas, NV. 
Banerjee, S. and A. Rudnicky (2007). "Segmenting 
Meetings into Agenda Items by Extracting Implicit 
Supervision from Human Note-Taking". IUI'2007. 
Honolulu, Hawaii. 
Birkenes, O., T. Matsui, et al (2007). "N-Best Rescor-
ing for Speech Recognition using Penalized Logis-
tic Regression Machines with Garbage Class". 
ICASSP'2007, Honolulu, Hawaii. 
Bohus, D. and A. Rudnicky (2007). "Implicitly-
supervised learning in spoken language interfaces: 
an application to the confidence annotation prob-
lem". SIGdial 2007, Antwerp, Belgium. 
Chotimongkol, A. and A. Rudnicky (2001). "N-best 
Speech Hypotheses Reordering Using Linear Re-
gression". Eurospeech'2001, Aalborg, Denmark. 
Lamel, L. and J.-L. Gauvain (2002). "Lightly Super-
vised and Unsupervised Acoustic Model Training." 
Computer Speech and Language 16: 115-129. 
Li, X., Y.-C. Ju, et al (2008). "Language Modeling for 
Voice Search: a Machine Translation Approach". 
ICASSP'08, Las Vegas, NV. 
Live Search for Windows Mobile (2008): 
 http://mobile.search.live.com 
Nuance Mobile Search (2008): 
http://www.nuance.com/mobilesearch. 
Ringger, E. and J. Allen (1996). "Error Correction via 
Post-Processor for Continuous Speech Recogni-
tion". ICASSP'96, Atlanta, GA. 
Ringger, E. and J. Allen (1997). "Robust Error Correc-
tion of Continuous Speech Recognition". ESCA-
NATO Workshop on Robust Speech Recognition 
for Unknown Communication Channels, Pont-a-
Mousson, France. 
TellMeByMobile (2008): 
http://www.tellme.com/products/tellmebymobile. 
V-Lingo Mobile. (2008): 
http://www.vlingomobile.com/downloads.html. 
VoiceSignal Search. (2008): 
http://www.voicesignal.com/solutions/vsearch.php. 
 
28
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 225?234,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Models for Multiparty Engagement in Open-World Dialog 
Dan Bohus 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
dbohus@microsoft.com 
Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
horvitz@microsoft.com 
 
 
Abstract 
We present computational models that allow 
spoken dialog systems to handle multi-
participant engagement in open, dynamic envi-
ronments, where multiple people may enter and 
leave conversations, and interact with the sys-
tem and with others in a natural manner. The 
models for managing the engagement process 
include components for (1) sensing the en-
gagement state, actions and intentions of mul-
tiple agents in the scene, (2) making engage-
ment decisions (i.e. whom to engage with, and 
when) and (3) rendering these decisions in a set 
of coordinated low-level behaviors in an embo-
died conversational agent. We review results 
from a study of interactions "in the wild" with a 
system that implements such a model.  
1 Introduction 
To date, nearly all spoken dialog systems research has 
focused on the challenge of engaging single users on 
tasks defined within a relatively narrow context.  Efforts 
in this realm have led to significant progress including 
large-scale deployments that now make spoken dialog 
systems common features in the daily lives of millions 
of people. However, research on dialog systems has 
largely overlooked important challenges with the initia-
tion, maintenance, and suspension of conversations that 
are common in the course of natural communication and 
collaborations among people. In (Bohus and Horvitz, 
2009) we outlined a set of core challenges for extending 
traditional closed-world dialog systems to systems that 
have competency in open-world dialog. The work de-
scribed here is part of a larger research effort aimed at 
addressing these challenges, and constructing computa-
tional models to support the core interaction skills re-
quired for open-world dialog. In particular, we focus our 
attention in this paper on the challenges of managing 
engagement ? ?the process by which two (or more) par-
ticipants establish, maintain and end their perceived 
connection during interactions they jointly undertake?, 
cf. Sidner et al (2004) in open-world settings.  
We begin by reviewing the challenges of managing 
engagement in the open-world in the next section. In 
Section 3, we survey the terrain of related efforts that 
provides valuable context for the new work described in 
this paper. In Section 4, we introduce a computational 
model for multiparty situated engagement. The model 
harnesses components for sensing the engagement state, 
actions, and intentions of people in the scene for making 
high-level engagement decisions (whom to engage with, 
and when), and for rendering these decisions into a set 
of low-level coordinated behaviors (e.g., gestures, eye 
gaze, greetings, etc.). Then, we describe an initial ob-
servational study with the proposed model, and discuss 
some of the lessons learned through this experiment. 
Finally, in Section 6, we summarize this work and out-
line several directions for future research.  
2 Engagement in Open-World Dialog 
In traditional, single-user systems the engagement prob-
lem can often be resolved in a relatively simple manner. 
For instance, in telephony-based applications, it is typi-
cally safe to assume that a user is engaged with a dialog 
system once a call has been received. Similarly, push-
to-talk buttons are often used in multimodal mobile ap-
plications. Although these solutions are sufficient and 
even natural in closed, single-user contexts, they be-
come inappropriate for open-world systems that must 
operate continuously in open, dynamic environments, 
such as robots, interactive billboards, or embodied con-
versational agents.  
Interaction in the open-world is characterized by two 
aspects that capture key departures from assumptions 
traditionally made in spoken dialog systems (Bohus and 
Horvitz, 2009). The first one is the dynamic, multiparty 
nature of the interaction, i.e., the world typically con-
tains not just one, but multiple agents that are relevant 
225
to the interactive system. Engagements in open worlds 
are often dynamic and asynchronous, i.e. relevant agents 
may enter and leave the observable world at any time, 
may interact with the system and with each other, and 
their goals, needs, and intentions may change over time. 
Managing the engagement process in this context re-
quires that a system explicitly represents, models, and 
reasons about multiple agents and interaction contexts, 
and maintains and leverages long-term memory of the 
interactions to provide support and assistance.  
A second important aspect that distinguishes open-
world from closed-world dialog is the situated nature of 
the interaction, i.e., the fact that the surrounding physi-
cal environment provides rich, streaming context that is 
relevant for conducting and organizing the interactions. 
Situated interactions among people often hinge on 
shared information about physical details and relation-
ships, including structures, geometric relationships and 
pathways, objects, topologies, and communication af-
fordances.  The often implicit, yet powerful physicality 
of situated interaction, provides opportunities for mak-
ing inferences in open-world dialog systems, and chal-
lenges system designers to innovate across a spectrum 
of complexity and sophistication. Physicality and em-
bodiment also provide important affordances that can be 
used by a system to support the engagement process. 
For instance, the use of a rendered or physically embo-
died avatar in a spoken dialog system provides a natural 
point of visual engagement between the system and 
people, and allows the system to employ natural signal-
ing about attention and engagement with head pose, 
gaze, facial expressions, pointing and gesturing. 
We present in this paper methods that move beyond 
the realm of closed-world dialog with a situated multi-
party engagement model that can enable a computation-
al system to fluidly engage, disengage and re-engage 
one or multiple people, and support natural interactions 
in an open-world context. 
3 Related Work 
The process of engagement between people, and be-
tween people and computational systems has received a 
fair amount of attention. Observational studies in the 
sociolinguistics and conversational analysis communi-
ties have revealed that engagement is a complex, mixed-
initiative, highly-coordinated process that often involves 
a variety of non-verbal cues and signals, (Goffman, 
1963; Kendon, 1990), spatial trajectory and proximity 
(Hall, 1966; Kendon, 1990b), gaze and mutual attention 
(Argyle and Cook, 1976), head and hand gestures (Ken-
don, 1990), as well as verbal greetings. 
A number of researchers have also investigated is-
sues of engagement in human-computer and human-
robot interaction contexts. Sidner and colleagues (2004) 
define engagement as ?the process by which two (or 
more) participants establish, maintain and end their per-
ceived connection during interactions they jointly un-
dertake?, and focus on the process of maintaining en-
gagement. They show in a user study (Sidner et al, 
2004; 2005) that people directed their attention to a ro-
bot more often when the robot made engagement ges-
tures throughout the interaction (i.e. tracked the user?s 
face, and pointed to relevant objects at appropriate times 
in the conversation.) Peters (2005; 2005b) uses an alter-
native definition of engagement as ?the value that a par-
ticipant in an interaction attributes to the goal of being 
together with the other participant(s) and of continuing 
the interaction,? and present the high-level schematics 
for an algorithm for establishing and maintaining en-
gagement. The algorithm highlights the importance of 
mutual attention and eye gaze and relies on a heuristi-
cally computed ?interest level? to decide when to start a 
conversation. Michalowski and colleagues (2006) pro-
pose and conduct experiments with a model of engage-
ment grounded in proxemics (Hall, 1966) which classi-
fies relevant agents in the scene in four different catego-
ries (present, attending, engaged  and interacting) based 
on their distance to the robot. The robot?s behaviors are 
in turn conditioned on the four categories above.  
In our work, we follow Sidner?s definition of en-
gagement as a process (Sidner et al, 2004) and describe 
a computational model for situated multiparty engage-
ment. The proposed model draws on several ideas from 
the existing body of work, but moves beyond it and 
provides a more comprehensive framework for manag-
ing the engagement process in a dynamic, open-world 
context, where multiple people with different and 
changing goals may enter and leave, and communicate 
and coordinate with each other and with the system.  
4 Models for Multiparty Engagement 
The proposed framework for managing engagement is 
centered on a reified notion of interaction, defined here 
as a basic unit of sustained, interactive problem-solving. 
Each interaction involves two or more participants, and 
this number may vary in time; new participants may 
join an existing interaction, or current participants may 
leave an interaction at any point in time. The system is 
actively engaged in at most one interaction at a time 
(with one or multiple participants), but it can simulta-
neously keep track of additional, suspended interactions. 
In this context, engagement is viewed as the process 
subsuming the joint, coordinated activities by which 
participants initiate, maintain, join, abandon, suspend, 
resume, or terminate an interaction. Appendix A shows 
by means of an example the various stages of an interac-
tion and the role played by the engagement process.  
Successfully modeling the engagement process in a 
situated, multi-participant context requires that the sys-
tem (1) senses and reasons about the engagement state, 
226
 not-engaged engaged 
EA=disengage | 
SEA=disengage 
Figure 2. Engagement state transition diagram. EA is the 
agent?s engagement action; SEA is the system?s action. 
EA=maintain & 
SEA=maintain 
EA=engage & 
SEA=engage 
EA=no-action | 
SEA=no-action 
Figure 3. Graphical model showing key variables and 
dependencies in managing engagement. 
ES 
EA 
EI 
t   t+1 
SEA 
ES 
? 
EI 
A 
? 
a
d
d
it
io
n
a
l 
c
o
n
te
x
t 
e
n
g
a
g
e
m
e
n
t 
s
e
n
s
in
g
 
? 
G 
A 
G 
EA 
SEB 
? 
actions and intentions of multiple agents in the scene, 
(2) makes high-level engagement control decisions (i.e. 
about whom to engage or disengage with, and when) 
and (3) executes and signals these decisions to the other 
participants in an appropriate and expected manner (e.g. 
renders them in a set of coordinated behaviors such as 
gestures, greetings, etc.). The proposed model subsumes 
these three components, which we discuss in more de-
tail in the following subsections. 
4.1 Engagement State, Actions, Intentions 
As a prerequisite for making informed engagement de-
cisions, a system must be able to recognize various en-
gagement cues, and to reason about the engagement 
actions and intentions of relevant agents in the scene. To 
accomplish this, the sensing subcomponent of the pro-
posed engagement model tracks over time three related 
engagement variables for each agent ? and interaction ?: 
the engagement state ???
? (?) , the engagement action 
???
? (?) and the engagement intention ???
? (?).  
The engagement state, ???
? (?), captures whether an 
agent ? is engaged in interaction ? and is modeled as a 
deterministic variable with two possible values: en-
gaged and not-engaged. The state is updated based on 
the joint actions of the agent and the system (see Figures 
3 and 4). Since engagement is a collaborative process, 
the transitions to the engaged state require that both the 
agent and the system take either an engage action (if the 
agent was previously not engaged) or a maintain action 
(if the agent was already engaged); we discuss these 
actions in more detail shortly. On the other hand, disen-
gagement can be a unilateral act: an agent transitions to 
the not-engaged state if either the agent or the system 
take a disengage action or a no-action. 
The second engagement variable, ???
? (?), models the 
actions that an agent takes to initiate, maintain or termi-
nate engagement. There are four engagement actions: 
engage, no-action, maintain, disengage. The first two 
are possible only from the not-engaged state, while the 
last two are possible only from the engaged state. The 
engagement actions are estimated based on a condition-
al probabilistic model of the form: 
 
?(???
? (?)|???
?  ? ,???
?  ? ? 1 , ????
?  ? ? 1 ,?(?)) 
 
The inference is conditioned on the current engage-
ment state, on the previous agent and system actions, 
and on additional sensory evidence ?(t). ? t  includes 
the detection of explicit engagement cues such as: salu-
tations (e.g. ?Hi!?, ?Bye bye?); calling behaviors (e.g. 
?Laura!?); the establishment or the breaking of an F-
formation (Kendon, 1990b), i.e. the agent approaches 
and positions himself in front of the system and attends 
to the system; an expected, opening dialog move (e.g. 
?Come here!?). Note that each of these cues is explicit, 
and marks a committed engagement action.  
A third variable in the proposed model, ???
? (?) , 
tracks the engagement intention of an agent with respect 
to a conversation. Like the engagement state, the inten-
tion can either be engaged or not-engaged. Intentions 
are tracked separately from actions since an agent might 
intend to engage or disengage the system, but not yet 
take an explicit engagement action. For instance, let us 
consider the case in which the system is already en-
gaged in an interaction and another agent is waiting in 
line to interact with the system.  Although the waiting 
agent does not take an explicit, committed engagement 
action, she might still intend to engage in a new conver-
sation with the system once the opportunity arises. She 
might also signal this engagement intention via various 
cues (e.g. pacing around, glances that make brief but 
clear eye contact with the system, etc.) More generally, 
the engagement intention variable captures whether or 
not an agent would respond positively should the system 
initiate engagement. In that sense, it roughly corres-
ponds to Peters? (2005; 2005b) ?interest level?, i.e. to 
the value the agent attaches to being engaged in a con-
versation with the system.  
Like engagement actions, engagement intentions are 
inferred based on a direct conditional model: 
 
227
?(???
? (?)|???
?  ? ,???
?  ? ,????
?  ? ? 1 ,???
?  t? 1 ,?(?)) 
 
This model leverages information about the current 
engagement state, the previous agent and system ac-
tions, the previous engagement intention, as well as ad-
ditional evidence ?(?)  capturing implicit engagement 
cues. Such cues include the spatiotemporal trajectory of 
the participant and the level of sustained mutual atten-
tion. The models for inferring engagement actions and 
intentions are generally independent of the application. 
They capture the typical behaviors and cues by which 
people signal engagement, and, as such, should be reus-
able across different domains. In other work (Bohus and 
Horvitz, 2009b), we describe these models in more de-
tail and show how they can be learned automatically 
from interaction data. 
4.2 Engagement Control Policy 
Based on the inferred state, actions and intentions of the 
agents in the scene, as well as other additional evidence 
to be discussed shortly, the proposed model outputs 
high-level engagement actions, denoted by SEA deci-
sion node in Figure 3. The action-space on the system 
side contains the same four actions previously dis-
cussed: engage, disengage, maintain and no-action. 
Each action is parameterized with a set of agents {??} 
and an interaction ?. Additional parameters that control 
the lower level execution of these actions, such as spe-
cific greetings, waiting times, urgency, etc. may also be 
specified. The actual execution mechanisms are dis-
cussed in more detail in the following subsection. 
In making engagement decisions in an open-world 
setting, a conversational system must balance the goals 
and needs of multiple agents in the scene and resolve 
various tradeoffs (for instance between continuing the 
current interaction or interrupting it temporarily to ad-
dress another agent), all the while observing rules of 
social etiquette in interaction. Apart from the detected 
engagement state, actions and intentions of an agent 
??
? =  ???
? ,???
? ,???
?   , the control policy can be en-
hanced through leveraging additional observational evi-
dence, including high-level information ??  about the 
various agents in the scene, such as their long-term 
goals and activities, as well as other global context (?), 
including the multiple tasks at hand, the history of the 
interactions, relationships between various agents in the 
scene (e.g. which agents are in a group together), etc. 
For instance, a system might decide to temporarily 
refuse engagement even though an agent takes an en-
gage action, because it is currently involved in a higher 
priority interaction. Or, a system might try to take the 
initiative and engage an agent based on the current con-
text (e.g. the system has a message to deliver) and activ-
ity of the agent (e.g. the agent is passing by), even 
though the agent has no intention to engage.  
Engagement control policies have therefore the form,  
 
????({??
? }? ,? ,  ??  ? ,?) 
 
where we have omitted the time index for simplicity. In 
contrast to the models for inferring engagement inten-
tions and action, the engagement control policy can of-
ten be application specific. Such policies can be au-
thored manually to capture the desired system behavior.  
We will discuss a concrete example of this in Section 
5.2. In certain contexts, a more principled solution can 
be developed by casting the control of engagement as an 
optimization problem for scheduling collaborations with 
multiple parties under uncertainties about the estimated 
goals and needs, the duration of the interactions, time 
and frustration costs, social etiquette, etc. We are cur-
rently exploring such models, where the system also 
uses information-gathering actions (e.g. ?Are the two of 
you together?? ?Are you here for X?,? etc.), based on 
value-of-information computations to optimize in the 
nature and flow of attention and collaboration in multi-
party interactions. 
4.3 Behavioral Control Policy 
At the lower level, the engagement decisions taken by 
the system have to be executed and rendered in an ap-
propriate manner. With the use of a rendered or physical 
embodied agent, these actions are translated into a set of 
coordinated lower-level behaviors, such as head ges-
tures, making and breaking eye contact, facial expres-
sions, salutations, interjections, etc. The coordination of 
these behaviors is governed by a behavioral control pol-
icy, conditioned on the estimated engagement state, 
actions and intentions of the considered agents, as well 
as other information extracted from the scene: 
 
????(???, {??
? }? ,? ,?) 
 
For example, in the current implementation, the en-
gage system action subsumes three sub-behaviors per-
formed in a sequence: EstablishAttention, Greeting, and 
Monitor. First, the system attempts to establish sus-
tained mutual attention with the agent(s) to be engaged. 
This is accomplished by directing the gaze towards the 
agents, and if the agent?s focus of attention is not on the 
system, triggering an interjection like ?Excuse me!? 
Once mutual attention is established, on optional Greet-
ing behavior is performed; a greeting can be specified as 
an execution parameter of the engage action. Finally, 
the system enters a Monitor behavior, in which it moni-
tors for the completion of engagement. The action com-
pletes successfully once the agent(s) are in an engaged 
state. Alternatively if a certain period of time elapses 
and the agent(s) have not yet transitioned to the engaged 
state, the engage system action completes with failure 
(which is signaled to the engagement control layer).  
Like the high-level engagement control policies, the 
behavioral control policies can either be authored ma-
nually, or learned from data, either in a supervised (e.g. 
228
from a human-human interaction corpus) or unsuper-
vised learning setting. Also, like the engagement sens-
ing component, the behavioral control component is 
decoupled from the task at hand, and should be largely 
reusable across multiple application domains.  
5 Observational Study 
As an initial step towards evaluating the proposed si-
tuated multiparty engagement models, we conducted a 
preliminary observational study with a spoken dialog 
system that implements these models. The goals of this 
study were (1) to investigate whether a system can use 
the proposed engagement models to effectively create 
and conduct multiparty interactions in an open-world 
setting, (2) to study user behavior and responses in this 
setting, and (3) to identify some of the key technical 
challenges in supporting multiparty engagement and 
dialog in open-world context. In this section, we de-
scribe this study and report on the lessons learned.  
5.1 Experimental platform 
Studying multiparty engagement and more generally 
open-world interaction poses significant challenges. 
Controlled laboratory studies are by their very nature 
closed-world. Furthermore, providing participants with 
instructions, such as ?Go interact with this system?, or 
?Go join the existing interaction? can significantly 
prime and alter the engagement behaviors they would 
otherwise display upon encountering the system in an 
unconstrained setting. This can in turn cast serious 
doubts on the validity of the results. Open-world inte-
raction is best observed in the open-world.  
To provide an ecologically valid basis for studying 
situated, multiparty engagement we therefore developed 
a conversational agent that implements the proposed 
model, and deployed it in the real-world. The system, 
illustrated in Figure 4, takes the form of an interactive 
multi-modal kiosk that displays a realistically rendered 
avatar head which can interact via natural language. The 
avatar can engage with one or more participants and 
plays a simple game, in which the users have to respond 
to multiple-choice trivia questions.  
The system?s hardware and software architecture is 
illustrated in Figure 4. Data gathered from a wide-angle 
camera, a 4-element linear microphone array, and a 19? 
touch-screen is forwarded to a scene analysis module 
that fuses the incoming streams and constructs in real-
time a coherent picture of the dynamics in the surround-
ing environment. The system detects and tracks the lo-
cation of multiple agents in the scene, tracks the head 
pose for engaged agents, tracks the current speaker, and 
infers the focus of attention, activities, and goals of each 
agent, as well as the group relationships among different 
agents. An in-depth description of the hardware and 
scene analysis components falls beyond the scope of 
this paper, but details are available in (Bohus and Hor-
vitz, 2009). The scene analysis results are forwarded to 
the control level, which is structured in a two-layer reac-
tive-deliberative architecture. The reactive layer imple-
ments and coordinates various low-level behaviors, in-
cluding engagement, conversational floor management 
and turn-taking, and coordinating spoken and gestural 
outputs. The deliberative layer plans the system?s dialog 
moves and high-level engagement actions. 
Overall, the game task was purposefully designed to 
minimize challenges in terms of speech recognition or 
dialog management, and allow us to focus our attention 
on the engagement processes. The avatar begins the 
interactions by asking the engaged user if they would 
like to play a trivia game. If the user agrees, the avatar 
goes through four multiple-choice questions, one at a 
time. After each question, the possible answers are dis-
played on the screen (Figure 4) and users can respond 
by either speaking an answer or by touching it. When 
the answer provided by the user is incorrect, the system 
provides a short explanation regarding the correct an-
swer before moving on to the next question.  
The system also supports multi-participant interac-
tions. The engagement policy used to attract and engage 
Dialog Management 
Behavioral Control 
Scene Analysis Output Planning 
Vision Speech Synthesis Avatar 
wide-angle camera 
4-element linear microphone array  
touch screen 
speakers 
Figure 4. Trivia game dialog system: prototype, architectural overview, and runtime scene analysis 
229
multiple users in a game is the focus of this observa-
tional study, and is discussed in more detail in the next 
subsection. Once the system is engaged with multiple 
users, it uses a multi-participant turn taking model 
which allows it to continuously track who the current 
speaker is, and who has the conversational floor (Bohus 
and Horvitz, 2009). At the behavioral level, the avatar 
orients its head pose and gaze towards the current 
speaker, or towards the addressee(s) of its own utter-
ances. During multiplayer games, the avatar alternates 
between the users when asking questions. Also, after a 
response is received from one of the users, the avatar 
confirms the answer with the other user(s), e.g. ?Do you 
agree with that?? A full sample interaction with the sys-
tem is described in Appendix A, and the corresponding 
video is available online (Situated Interaction, 2009).  
5.2 Multiparty Engagement Policy 
The trivia game system implements the situated, multi-
party engagement model described in Section 4. The 
sensing and behavioral control components are applica-
tion independent and were previously described. We 
now describe the system?s engagement policy, which is 
application specific.  
As previously discussed, apart from using the in-
ferred engagement state, actions and intentions for the 
agents in the scene, the proposed model also uses in-
formation about the high-level goals and activities of 
these agents when making engagement decisions. Spe-
cifically, the system tracks the goal of each agent in the 
scene, which can be play, watch, or other, and their cur-
rent activity, which can be passing-by, interacting, play-
ing, watching, or departing. The goal and activity rec-
ognition models are application specific, and in this case 
are inferred based on probabilistic conditional models 
that leverage information about the spatiotemporal tra-
jectory of each agent and their spoken utterances, as 
well as global scene information (e.g. is the system en-
gaged in an active interaction, etc.).  
Initially, when the system is idle, it uses a conserva-
tive engagement policy and waits for the user to initiate 
engagement via an explicit action. Such actions include 
the user approaching and entering in an F-formation 
(Kendon, 1990b) with the system, i.e. standing right in 
front of it, swiping their badge, or pushing the start but-
ton (in the idle state the GUI displays ?swipe your 
badge or press here to begin? below the avatar head).  
While engaged in an interaction, the system attempts 
to engage bystanders in an effort to create a collabora-
tive, multi-participant game. In this case, the engage-
ment policy is conditioned on the inferred activities of 
the agents in the scene. Specifically, if a watching bys-
tander is detected, the system temporarily disengages 
the current participant, and engages and attempts to 
?convince? the watching bystander to join the existing 
game. The prompts in this side interaction depend on 
the current game context, as shown in Table 1. If the 
watching bystander agrees to join in, the system adds 
him to the existing interaction, and continues a multi-
participant game (see Table 1.A.) Conversely, if the 
bystander refuses, the system re-engages the previous 
participant and resumes the single-user game (see Table 
1.B.) Additional examples are available in Appendix A. 
Finally, if the system is already engaged and a watch-
ing bystander is detected but only during the last ques-
tion, the system engages them temporarily to let them 
know that the current game will end shortly and, if they 
wait, they can also start a new game (see Table 1.D).  
5.3 Results and Lessons Learned 
We deployed the system described above for 20 days 
near one of the kitchenettes in our building. The system 
attracted attention of passer-bys with the tracking mo-
tion of its virtual face that followed people as they 
passed by. Most people that interacted with the system 
did so for the first time; only a small number of people 
interacted several times. No instructions were provided 
for interacting with the system. We shall now review 
results from analysis of the collected data.  
Throughout the 20 days of deployment, the system 
engaged in a total of 121 interactive events. Of these, in 
54 cases (44%), a participant engaged the system but 
did not play the game. Typically, the participant would 
approach and enter in an F-formation with the system, 
If a watching bystander B is detected during the first 3 questions  
 and the engaged participant E has not made any mistakes 
A 
 [S to E]: Hold on a second. 
[S to B]: Hi. Excuse me, would you like to join in? 
[B]:  Okay.  
[S to E&B]: Great. So let?s see how well you can do together. 
 Next question [?] 
 and the engaged participant E has just made a mistake, and 
the system did not previously engage the bystander B. 
B 
 [S to E]: Perhaps you can use some help. Hold on a 
 second. 
[S to B]: Hi. Do you think you could help out? 
[B]:  No thanks 
[S to B]: Okay. Well, I guess you can just watch for now 
 and if you wait for a bit we?ll be done and you can 
 start a new game yourself 
[S to E]: Guess you?re still on your own. Next question [?] 
 and  the engaged participant E has just made a mistake, and 
the system did previously attempt to engage the bystander B. 
C 
 [S to E]: I think you could really use some help. Hold on a 
 second. 
[S to B]: Are you sure you don?t want to help out? Come 
 on, this is fun. 
[B]:  Sure 
[S to E&B]: Great. So let?s see how well you can do together. 
 Next question [?] 
If a watching bystander B is detected during the last question 
D 
[S to E]: Excuse me for one moment. 
[S to B]: We?re almost done here. If you wait for a bit we can start 
 a new game right after 
[S to E]: Sorry about that [?] 
 Table 1. Multiparty engagement policy 
230
but, once the system engaged and asked if they would 
like to play the trivia game, they responded negatively 
or left without responding. In 49 cases (40%), a single 
participant engaged and played the game, but no bys-
tanders were observed during these interactions. In one 
case, two participants approached and engaged simulta-
neously; the system played a multi-participant game, but 
no other bystanders were observed. Finally, in the re-
maining 17 cases (14% of all engagements, 25% of ac-
tual interactions), at least one bystander was observed 
and the system engaged in multiparty interaction. These 
multiparty interactions are the focus of our observation-
al analysis, and we will discuss them in more detail.  
In 2 of these 17 cases, bystanders appeared only late 
in the interaction, after the system had already asked the 
last question. In these cases, according to its engage-
ment policy, the system notified the bystander that they 
would be attended to momentarily (see Table 1.D), and 
then proceeded to finish the initial game. In 8 of the 
remaining 15 cases (53%), the system successfully per-
suaded bystanders to join the current interaction and 
carried on a multi-participant game. In the remaining 7 
cases (47%), bystanders turned down the offer to join 
the existing game. Although this corpus is still relatively 
small, these statistics indicate that the system can suc-
cessfully engage bystanders and create and manage 
multi-participant interactions in the open world.  
Next, we analyzed more closely the responses and 
reactions from bystanders and already engaged partici-
pants to the system?s multiparty engagement actions. 
Throughout the 17 multiparty interactions, the system 
planned and executed a total of 23 engagement actions 
soliciting a bystander to enter the game, and 6 engage-
ment actions letting a bystander know that they will be 
engaged momentarily. The system actions and res-
ponses from bystanders and engaged participants are 
visually summarized in Figure 5, and are presented in 
full in Appendix B. Overall, bystanders successfully 
recognize that they are being engaged and solicited by 
the system and respond (either positively or negatively) 
in the large majority of cases (20 out of 23). In 2 of the 
remaining 3 cases, the previously engaged participant 
responded instead of the bystander; finally, in one case 
the bystander did not respond and left the area.  
While bystanders generally respond when engaged 
by the system, the system?s engagement actions towards 
bystanders also frequently elicits spoken responses from 
the already engaged participants; this happened in 14 
out of 23 cases (61%). The responses are sometimes 
addressed to the system e.g. ?Yes he does,? or towards 
the bystander, e.g. ?Say yes!?, or they reflect general 
comments, e.g. ?That?s crazy!? These results show that, 
when creating the side interaction to solicit a bystander 
to join the game, the system should engage both the 
bystander and the existing user in this side interaction, 
or at least allow the previous user to join this side inte-
raction (currently the system engages only the bystander 
in this interaction; see example from Appendix A.)  
Furthermore, we noticed that, in several cases, bys-
tanders provided responses to the system?s questions 
even prior to the point the system engaged them in inte-
raction (sometimes directed toward the system, some-
times toward the engaged participant.) We employed a 
system-initiative engagement policy towards bystanders 
in the current experiment. The initiative being taken by 
participants highlights the potential value of implement-
ing a mixed-initiative policy for engagement. If a rele-
vant response is detected from a bystander, this can be 
interpreted as an engagement action (recall from subsec-
tion 4.1 that engagement actions subsume expected 
opening dialog moves), and a mixed-initiative policy 
can respond by engaging the bystander, e.g. ?Did you 
want to join in?? or ?Please hang on, let?s let him finish. 
We can play a new game right after that.? This policy 
could be easily implemented under the proposed model.  
We also noted side comments by both bystander and 
the existing participant around the time of multiparty 
engagement. These remarks typically indicate surprise 
and excitement at the system?s multiparty capabilities. 
Quotes include: ?That?s awesome!?, ?Isn?t that great!?, 
?That?s funny!?, ?Dude!?, ?Oh my god that?s creepy!?, 
?That?s cool!?, ?It multitasks!?, ?That is amazing!?, 
?That?s pretty funny?, plus an abundance of laughter 
and smiles. Although such surprise might be expected 
today with a first-time exposure to an interactive system 
that is aware of and can engage with multiple parties, 
we believe that expectations will change in the future, as 
these technologies become more commonplace.  
Figure 5. System multiparty engagement actions and responses from bystanders and already engaged participants.  
For bystander responses,     denotes a positive response;      denotes a negative response;    denotes no response. For responses 
from previously engaged participant,      denotes utterances addressed to the bystander,      denotes side comments,      denotes 
responses directed to the system 
response from  
previously engaged 
participant 
Excuse me for one second ? Hi, would you like to join in? 
[12 cases] 
Y Y Y N Y N T N N N N N N N N N N Y Y Y Y 
Perhaps you can use some 
help? Do you think you could 
help out? [6 cases] 
[after non-understanding] 
Sorry, did you want to join 
in? [5 cases] 
We?re almost done here. If you 
wait for a bit we can start a new 
game right after  [6 cases] 
response from  
solicited bystander 
system 
prompt 
B B 
Y N 
B
B B S S S S S S C C C C C 
SC
231
Overall, this preliminary study confirmed that the 
system can effectively initiate engagement in multiparty 
settings, and also highlighted several core challenges for 
managing engagement and supporting multiparty inte-
ractions in the open world. A first important challenge 
we have identified is developing robust models for 
tracking the conversational dynamics in multiparty situ-
ations, i.e. identifying who is talking to whom at any 
given point. Secondly, the experiment has highlighted 
the opportunity for using more flexible, mixed-initiative 
engagement policies. Such policies will rely heavily on 
the ability to recognize engagement intentions; in (Bo-
hus and Horvitz, 2009b), we describe the automated 
learning of engagement intentions from interaction data. 
Finally, another lesson we learned from these initial 
experiments is the importance of accurate face tracking 
for supporting multiparty interaction. Out of the 17 mul-
tiparty interactions, 7 were affected by vision problems 
(e.g. the system momentarily lost a face, or swapped the 
identity of two faces); 4 of these were fatal errors that 
eventually led to interaction breakdowns.  
6 Summary and Future Work 
We have described a computational model for managing 
engagement decisions in open-world dialog. The model 
harnesses components for sensing and reasoning about 
the engagement state, actions, and intentions of multiple 
participants in the scene, for making high-level en-
gagement control decisions about who and when to en-
gage, and for executing and rendering these actions in 
an embodied agent. We reviewed an observational study 
that showed that, when weaved together, these compo-
nents can provide support for effectively managing en-
gagement, and for creating and conducting multiparty 
interactions in an open-world context.  
We believe that the components and policies we have 
presented provide a skeleton for engagement and inte-
raction in open-world settings. However, there are im-
portant challenges and opportunities ahead. Future re-
search includes developing methods for fine tuning and 
optimizing each of these subcomponents and their inte-
ractions. Along these lines, there are opportunities to 
employ machine learning to tune and adapt multiple 
aspects of the operation of the system. In (Bohus and 
Horvitz, 2009b) we introduce and evaluate an approach 
to learning models for inferring engagement actions and 
intentions online, through interaction.  On another direc-
tion, we are investigating the use of decision-theoretic 
approaches for optimizing mixed-initiative engagement 
policies by taking into account the underlying uncertain-
ties, the costs and benefits of interruption versus contin-
uing collaboration, queue etiquette associated with ex-
pectations of fairness, etc. Another difficult challenge is 
the creation of accurate low-level behavioral models, 
including the fine-grained control of pose, gesture, and 
facial expressions. Developing such methods will likely 
have subtle, yet powerful influences on the effectiveness 
of signaling and overall grounding in multiparty set-
tings. We believe that research on these and other prob-
lems of open-world dialog will provide essential and 
necessary steps towards developing computational sys-
tems that can embed interaction deeply into the natural 
flow of everyday tasks, activities, and collaborations. 
Acknowledgments  
We thank George Chrysanthakopoulos, Zicheng Liu, 
Tim Paek, Cha Zhang, and Qiang Wang for discussions 
and feedback in the development of this work.  
References 
M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-
bridge University Press, New York 
D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-
lenges, Directions and Prototype, to appear in KRPD?09, 
Pasadena, CA 
D. Bohus and E. Horvitz, 2009b, An Implicit-Learning Based 
Model for Detecting Engagement Intentions, submitted to 
SIGdial?09, London, UK 
E. Goffman, 1963, Behaviour in public places: notes on the 
social order of gatherings, The Free Press, New York 
E.T. Hall, 1966, The Hidden Dimension: man?s use of space in 
public and private, New York: Doubleday. 
A. Kendon, 1990, A description of some human greetings, 
Conducting Interaction: Patterns of behavior in focused en-
counters, Studies in International Sociolinguistics, Cam-
bridge University Press 
A. Kendon, 1990b, Spatial organization in social encounters: 
the F-formation system, Conducting Interaction: Patterns of 
behavior in focused encounters, Studies in International 
Sociolinguistics, Cambridge University Press 
M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial 
model of engagement for a social robot, in 9th IEEE Work-
shop on Advanced Motion Control, pp. 762-767 
C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005, 
A model of attention and interest using gaze behavior, Lec-
ture Notes in Computer Science, pp. 229-240. 
C. Peters, 2005b, Direction of Attention Perception for Con-
versation Initiation in Virtual Environments, in Intelligent 
Virtual Agents, 2005, pp. 215-228.  
C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to 
Look: A Study of Human-Robot Engagement, IUI?2004, pp. 
78-84, Madeira, Portugal 
C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005, 
Explorations in engagement for humans and robots, Artifi-
cial Intelligence, 166 (1-2), pp. 140-164 
Situated Interaction, 2009, Project page: 
http://research.microsoft.com/~dbohus/research_situated_i
nteraction.html 
R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001, 
Eye gaze patterns in conversations: there is more to con-
versational agents than meets the eyes, CHI?01 
232
  
 
1: 
 
S: 
 
 
S: 
 
 
1: 
S: 
 
1:  
S:  
 
 
1: 
S: 
1: 
S: 
 
S: 
 
 
S: 
 
 
S: 
2: 
S: 
2: 
S: 
 
 
 
S: 
 
S: 
 
 
S: 
1: 
S: 
1: 
S: 
1: 
S: 
1: 
S: 
 
 
 
 
 
S: 
 
S: 
 
S: 
 
2: 
S: 
2: 
S: 
 
S: 
 
S: 
 
 
 
 
 
1: 
S: 
2: 
S: 
 
 
 
1: 
S: 
2: 
S: 
2: 
S: 
 
 
1: 
[approaching] 
 
[engaging]  
Hi! 
 
Would you like to play a trivia game? If you answer three 
out of four questions correctly I can enter you in a raffle 
for a Zune player. 
Sure. 
Okay. First, could you please swipe your badge below 
so I can email you the raffle ticket? 
[swipes badge; in the meantime, bystander 2 appears] 
Got it! Here is the first question.  
The state with the most shark attacks each year is 
California, Florida or Hawaii? 
Florida. 
Florida. Is that final? 
Yup 
That?s right 
 
[disengaging] 
Excuse me for one second.  
 
[engaging] 
Hi 
 
Would you like to join in? 
Uh. No thanks. 
Sorry, did you want to join in? 
No.  
Well, you can just watch for now. And if you wait for a 
bit, we?ll be done and then you can start a new game 
yourself 
 
[disengaging] 
 
[engaging] 
Sorry about that.  
 
Next question.Which mammal lives the longest? 
The human 
You think it?s human, right? 
Uh no, no no.  
So what?s the correct answer? 
Elephant.  
Elephant. Is that final? 
Yup.  
Sorry but that?s actually incorrect. Current scientific 
thought is that the longest possible human life span is 
120 years, making humans the longest lived among the 
mammals. Perhaps you could use some help. Hold on 
one more second.  
 
[disengaging] 
 
[engaging] 
 
Are you sure you don?t want to help out? Come on, this 
is fun.  
Okay. 
Sorry, so did you wanna join in? 
Yes.  
Great 
 
[disengaging] 
 
[re-engaging with both 1 and 2] 
 
So let?s see if you can do better together. So far you 
have one correct answer. Now on to the next question. 
Is it true that Black Friday, the Friday after Thanksgiving 
is the busiest shopping day in the year? 
I heard that it wasn?t true actually? false. 
Do you also think that?s false? 
I guess. 
You got it! Okay, so far you have 2 out of 3 correct 
answers. If you get this last question right you can enter 
the raffle to win a Zune. How many people are killed by 
vending machines at the beach in a typical year? 
It?s probably more than one? I?ll say thirteen.  
Do you think that?s correct? 
No, thirteen.  
Is that final? 
Yes.  
Congratulations, you had 3 correct answers out of 4 
questions. I will record an entry for you in the Zune 
raffle.  
Great, thanks 
 
Bye-bye 
 
 
A
ct
iv
e 
t1 
t2 
 
t3 
t4 
t5 
t6 
t7 
t8 
t9 
t10 
t11 
t12 
t13 
t14 
t15 
Engage({1},i1) 
Maintain({1},i1) 
Disengage({1},i1) 
Engage({2},i2) 
Maintain({2},i2) 
Diseng ({2},i2) 
Engage({1},i1) 
Maintain({1},i1) 
Disengage({1},i1) 
Engage({2},i3) 
Maintain({2},i3) 
Diseng({2},i3) 
Engage({1,2},i4) 
Maintain({1,2},i4) 
Disengage({1,2},i4) 
t0 
 
Appendix A. Sample multiparty interaction with trivia game dialog system (not part of the experiment) 
 
 1 
 2 
 1 
 2 
 1 
 2 
 1 
 1 
 1 
 2 
A
ct
iv
e 
S
us
pe
nd
ed
 
S
us
pe
nd
ed
 
 1 
 2 
first person engages -  
around time t2 
bystander appears ? prior to t3 
system engages bystander ~ t5 
participants play together ~ t14 
system 
agent 
active conversa-
tion suspended conver-
sation other conversation 
A
ct
iv
e 
A
ct
iv
e 
A
ct
iv
e 
233
  
Appendix B. User responses to multiparty engagement actions.  
 
S denotes the system, E denotes the already engaged participant, B denotes a watching bystander.  
 
Actions and Resposes Response from B Response from E Timing 
[S to E]: Hold on one second. 
[S to B]: Excuse me, would you like to join in? 
   
4 positive answers from B 
7 negative answers from B 
1 no answer from B (E answers) 
Yes  B only 
Yes Say yes Overlap 
Sure  B only 
Yes [to B]: Would you like to join 
in? 
E first 
No That?s crazy! B first 
Oh, no. No + [moves away] That?s funny! B first 
No thank you  B only 
No No? B first 
Woah, no.  That?s cool! B first 
No + [moves away] + That?s 
pretty funny. 
[laughs looking at B] B first 
[laughs] [laughs] Yes. Oh yes. E only 
[S to E]: Perhaps you could use some help. 
 Excuse me for one second. 
[S to B]: Hi, do you think you could help out? 
 
3 positive answers from B 
2 negative answers from B 
1 no-answer from B (moves away) 
Yes. Yes. B first 
Yes Yes he does. Overlap 
[laughs] + No.  B only 
[to E]: Isn?t that weird? 
[to S]: No. 
[to E]: Isn?t that great? 
[to B]: That is amazing! B first 
[laughs] + [moves out] Quit E only 
[laughs] + Sure Sure B first 
If the initial response from B was not unders-
tood by the system, system asks one more time 
[S to B]: Sorry, did you want to join in? 
 
1 positive answer from B 
3 negative answer from B 
1 no-answer from B (E answers) 
No. 
Please. 
Yes, I don?t know, help me! B first 
No.  B only 
No.  B only 
No.  B only 
 No. E only 
[S to B]: We?re almost done here. If you wait 
 for a bit we can start a new game 
 right after. 
 
1 answer from B 
1 answer from E 
4 no-answer from either B or E 
Great, thanks.  B only 
 That?s awesome E only 
   
   
   
   
 
234
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 244?252,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Learning to Predict Engagement with a                                                        
Spoken Dialog System in Open-World Settings 
Dan Bohus 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
dbohus@microsoft.com 
Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
horvitz@microsoft.com 
 
 
Abstract 
We describe a machine learning approach that 
allows an open-world spoken dialog system to 
learn to predict engagement intentions in situ, 
from interaction. The proposed approach does 
not require any developer supervision, and le-
verages spatiotemporal and attentional features 
automatically extracted from a visual analysis 
of people coming into the proximity of the sys-
tem to produce models that are attuned to the 
characteristics of the environment the system is 
placed in. Experimental results indicate that a 
system using the proposed approach can learn 
to recognize engagement intentions at low false 
positive rates (e.g. 2-4%) up to 3-4 seconds 
prior to the actual moment of engagement.  
1 Introduction 
We address the challenge of predicting the forthcoming 
engagement of people with open-world conversational 
systems (Bohus and Horvitz, 2009a), i.e. systems that 
operate in relatively unconstrained environments, where 
multiple participants might come and go, establish, 
maintain and break the communication frame, and si-
multaneously interact with a system and with others. 
Examples of such systems include interactive billboards 
in a mall, robots in a home environment, intelligent 
home control systems, interactive systems that provide 
assistance and support during procedural tasks, etc. 
In traditional closed-world dialog systems the en-
gagement problem is generally resolved via simple, un-
ambiguous signals.  For example, engagement is gener-
ally assumed once a phone call is answered by a tele-
phony dialog system. Similarly, a push-to-talk button 
can provide a clear engagement signal for a speech 
enabled mobile application. These solutions are howev-
er inappropriate for systems that must operate conti-
nuously in open, dynamic environments, and engage 
with multiple people and groups over time. Such sys-
tems should ideally be ready to initiate dialog in a fluid, 
natural manner. They should manage engagement with 
participants who are close by, and with those who are at 
a distance, with participants who have a standing plan to 
interact with a system, and with those whom opportu-
nistically decide to engage, in-stream with their other 
ongoing activities.  In recognizing engagement inten-
tions, such systems need to minimize false positives, 
while also minimizing the unnatural delays and discon-
tinuities that come with false negatives about engage-
ment intentions. 
The work described in this paper is set in the larger 
context of a computational model for supporting fluid 
engagement in open-world dialog systems that we have 
previously described in (Bohus and Horvitz, 2009b). 
The above mentioned model harnesses components for 
sensing the engagement state, actions, and intentions of 
multiple participants in the scene, for making engage-
ment control decisions, and for rendering these deci-
sions into coordinated low-level behaviors, such as the 
changing pose and expressions of the face of an embo-
died agent. In this paper, we focus on the sensing sub-
component of this larger model and describe an ap-
proach for automatically learning to detect engagement 
intentions from interaction. 
2 Related Work 
The challenges of engagement between people, and be-
tween people and computational systems, have already 
received some attention in the conversational analysis, 
sociolinguistics, and human-computer interaction com-
munities. For instance, in an early treatise Goffman 
(1963) discusses how people use cues to detect engage-
ment in an effort to avoid the social costs of engaging in 
interaction with an unwilling participant. In later work, 
Kendon (1990a) presents a detailed investigation of 
video sequences of greetings in human-human interac-
tion, and identifies several stages of complex coordi-
nated action (pre-sighting, sighting, distance salutation, 
244
approach, close salutation), together with the head and 
body gestures that they typically involve. In (1990b), 
Kendon also introduces the notion of an F-formation, a 
pattern said to arise when ?two or more people sustain a 
spatial and orientational relationship in which they have 
equal, direct, and exclusive access,? and discusses the 
role of F-formations in establishing and maintaining 
social interactions. Argyle and Cook (1976) as well as 
others (e.g., Duncan, 1972; Vertegaal et al, 2001) have 
identified and discussed the various functions of eye 
gaze in maintaining social and communicative engage-
ment. Overall, this body of work suggests that engage-
ment is a rich, mixed-initiative, and well-coordinated 
process that involves non-verbal cues and signals, such 
as spatial trajectory and proximity, gaze and mutual 
attention, head and hand gestures, and verbal greetings.  
More recently, several researchers have investigated 
issues of engagement in human-computer and human-
robot interaction contexts. Sidner et al (2004; 2005) 
define engagement as ?the process by which two (or 
more) participants establish, maintain and end their per-
ceived connection during interactions they jointly un-
dertake,? and conduct a user study that explores the 
process of maintaining engagement. They show that 
people direct their attention to a robot more often when 
the robot makes engagement gestures throughout an 
interaction, i.e. tracks the user?s face, and points to rele-
vant objects at appropriate times in the conversation. 
Peters et al(2005a; 2005b) use an alternative defini-
tion of engagement as ?the value that a participant in an 
interaction attributes to the goal of being together with 
the other participant(s) and of continuing the interac-
tion,? and present the high-level schematics for an algo-
rithm for establishing and maintaining engagement. The 
proposed algorithm highlights the importance of eye 
gaze and mutual attention in this process and relies on a 
heuristically computed interest level to decide when to 
begin a conversation.  
Michalowski et al(2006) propose and conduct expe-
riments with a spatial model of engagement, grounded 
in proxemics (Hall, 1966). Their model classifies rele-
vant agents in the scene in four different categories 
based on their distance to the robot: present (standing 
far), attending (standing closer), engaged (next to the 
robot), and interacting (standing right in front of the 
robot). The robot?s behaviors are in turn conditioned on 
these categories: the robot turns towards attending 
people, greets engaged people and verbally prompts 
interacting people for input. The authors discuss several 
lessons learned from an observational study conducted 
with this robot in a building lobby.  They find that the 
fast-paced movements of people in the environment 
pose a number of challenges: often the robot greeted 
people too late (earlier anticipation was needed), or 
greeted people that did not intend to engage (more accu-
rate anticipation was needed). The authors recognize 
that these limitations stem partly from their reliance on 
static models, and hypothesize that temporal informa-
tion such as speed and trajectory may provide additional 
cues regarding a person?s engagement with the robot. 
In this paper, we expand on our previous work on a 
situated multiparty engagement model (Bohus and Hor-
vitz, 2009b). Specifically, we focus on a key subcom-
ponent in this model: detecting whether or not a user 
intends to engage in an interaction with a system. We 
introduce an approach that improves upon the existing 
work (Peters 2005a, 2005b; Michalowski et. al, 2006) in 
several significant ways. First, the approach is data-
driven: the use of machine learning techniques allows 
the system to adapt to the specific characteristics of its 
physical location and to the behaviors of the surround-
ing population of potential participants. Second, we 
leverage a wide array of observations, including tem-
poral features. Finally, no developer supervision is re-
quired for training the model: the supervision signal is 
extracted automatically, in-stream with the interactions, 
allowing for online learning and adaptation.  
3 Situated Multiparty Engagement Model 
To set the broader context for the work described in this 
paper, we now briefly review the overall model for 
managing engagement in an open-world setting intro-
duced in (Bohus and Horvitz, 2009b). The model is cen-
tered on a reified notion of interaction, defined as a ba-
sic unit of sustained, interactive problem-solving. Each 
interaction can involve two or more participants, and 
this number may vary in time; new participants may 
join an existing interaction and current participants may 
leave an interaction at any point in time. The system is 
actively engaged in at most one interaction at a time 
(with one or multiple participants), but it can simulta-
neously keep track of additional, suspended interactions. 
In this context, engagement is viewed as the process 
subsuming the joint, coordinated activities by which 
participants initiate, maintain, join, abandon, suspend, 
resume, or terminate an interaction. 
Successfully managing this process requires that the 
system (1) senses and reasons about the engagement 
state, actions and intentions of multiple agents in the 
scene, (2) makes high-level engagement control deci-
sions (i.e. about whom to engage or disengage with, and 
when) and (3) executes and signals these decisions to 
the other participants in an appropriate manner (e.g. via 
a set of coordinated behaviors such as gestures, greet-
ings, etc.) The proposed model, illustrated in Figure 1, 
subsumes these three components. 
The sensing subcomponent in the model tracks the 
engagement state, engagement actions, and engagement 
intention for each agent in the visual scene. The en-
gagement state, ???
? (?), denotes whether an agent ? is 
245
Figure 1. Graphical model showing key variables and 
dependencies in managing engagement. 
ES 
EA 
EI 
t   t+1 
SEA 
ES 
? 
EI 
A 
? 
a
d
d
it
io
n
a
l 
c
o
n
te
x
t 
e
n
g
a
g
e
m
e
n
t 
s
e
n
s
in
g
 
? 
G 
A 
G 
EA 
SEB 
? 
engaged in interaction ? and is modeled as a determinis-
tic variable with two possible values: engaged and not-
engaged. The state is updated based on the joint actions 
of the system and the agent.  
A second engagement variable, ???
? (?), models the 
actions that an agent takes to initiate, maintain or termi-
nate engagement. There are four possible engagement 
actions: engage, no-action, maintain, disengage. These 
actions are tracked by means of a conditional probabilis-
tic model that takes into account the engagement state 
???
? (?), the previous agent and system actions, as well 
as additional sensory evidence ? capturing committed 
engagement actions, such as: salutations (e.g. ?Hi!?); 
calling behaviors (e.g. ?Laura!?); the establishment or 
the breaking of an F-formation (Kendon, 1990b); ex-
pected opening dialog moves (e.g. ?Come here!?) etc.  
A third variable in the proposed model, ???
? (?) , 
tracks whether or not each agent intends to be engaged 
in a conversation with the system. Like the engagement 
state, the intention can either be engaged or not-
engaged. Intentions are tracked separately from actions 
since an agent might intend to engage or disengage the 
system, but not yet take an explicit engagement action. 
For instance, let us consider the case in which the sys-
tem is already engaged in an interaction and another use 
is waiting in line to interact with the system: although 
the waiting user does not take an explicit, committed 
engagement action, she might signal (e.g. via a glance 
that makes brief but clear eye contact with the interac-
tive system) that her intention is to engage in a new 
conversation once the opportunity arises. More general-
ly, the engagement intention captures whether or not an 
agent would respond positively should the system in-
itiate engagement. In that sense, it roughly corresponds 
to Peters? (2005; 2005b) ?interest level?, i.e. to the value 
the agent attaches to being engaged in a conversation 
with the system. Like engagement actions, engagement 
intentions are inferred based on probabilistic models 
that take into account the current engagement state, the 
previous agent and system actions, the previous en-
gagement intention, as well as additional evidence that 
captures implicit engagement cues, e.g. the spatiotem-
poral trajectory of the participant, the level of sustained 
mutual attention, etc.  
Based on the inferred engagement state, actions, and 
intentions of the agents in the scene, as well as other 
additional high-level evidence such as the agents? in-
ferred goals (?), activities (?) and relationships (?), the 
proposed model outputs engagement actions ? denoted 
by the ??? decision node in Figure 1. The action-space 
consists of the same four actions previously discussed: 
engage, disengage, maintain and no-action. At the low-
er level, the engagement decisions taken by the system 
are translated into a set of coordinated lower-level be-
haviors (???) such as head gestures, making eye con-
tact, facial expressions, salutations, interjections, etc. 
In related work (Bohus and Horvitz, 2009a; 2009b), 
we have demonstrated how this model can be used to 
effectively create and support multiparty interactions in 
an open-world context. Here, we focus on one specific 
subcomponent of this framework: the model for detect-
ing engagement intentions.  
4 Approach 
To illustrate the problem of detecting engagement inten-
tions, consider for instance a situated conversational 
system that examines through its sensors the scenes 
from Figure 3. How can such a system detect whether 
the person in the image intends to engage in a conversa-
tion or is just passing-by? Studies of human-human 
conversational engagement (Goffman, 1963; Argyle and 
Cook, 1976; Duncan, 1972; Kendon, 1990, 1990b) indi-
cate that people signal and detect engagement intentions 
by producing and monitoring for a variety of cues, in-
cluding gaze and sustained attention, trajectory and 
proximity, head and hand gestures, body pose, etc.  
In the proposed approach, we use machine learning 
techniques, and leverage a wide array of observations 
from the sensors to create a model that allows an open-
world interactive system to detect the specific patterns 
characterizing an engagement intention. Existing work 
on detecting engagement intentions has focused on stat-
ic heuristic models that leverage proximity and attention 
features (Peters, 2005, 2005b; Michalowski, 2006). As 
previously discussed, psychologists have shown the 
important role played by geometric relationships, trajec-
tories, and sustained attention in signaling and detecting 
engagement. The use of machine learning allows us to 
consider a wide array of such features, including trajec-
tory, speed, and the attention of agents over time. 
246
Figure 3. Placement and visual fields of view for  
side (right) and front (left) orientations. 
pillar 
Kitchenette 
Corridor 
pillar 
Kitchenette 
Corridor 
In general, as discussed in the previous section, the 
engagement intentions of an agent may evolve tempo-
rally under the proposed model, as a function of the 
various system actions and behaviors (e.g. an embodied 
system that makes eye contact, or smiles, or moves to-
ward a participant might alter the engagement intention 
of that participant). In this work we concentrate on a 
simplified problem, in which the system?s behavior is 
fixed (e.g. system always tracks people that pass by), 
and the engagement intention can be assumed constant 
within a limited time window. 
The central idea of the proposed approach is to start 
by using a very conservative (i.e., low false-positives) 
detector for engagement intentions, such as a push-to-
engage button, and automatically gather sensor data 
surrounding the moments of engagement, together with 
labels that indicate whether someone actually engaged 
or not. Note that the system eventually finds out if a 
person becomes engaged with it. If we assume that an 
intention to engage existed for a limited window of time 
prior to the moment of engagement, the collected data 
can be used to learn a model for predicting this intention 
ahead of the actual moment of engagement. The pro-
posed approach therefore enables a system to learn in-
situ models for predicting forthcoming engagement, and 
the models are attuned to the specifics of the environ-
ment the system is in. No explicit developer supervision 
is required, as the training labels are extracted automati-
cally from interaction.  
5 Experimental Setup 
To provide an ecologically valid basis for data collec-
tion and for evaluating the proposed approach, we de-
veloped a situated conversational agent and deployed it 
in the real-world. The system, illustrated in Figure 2, is 
an interactive multimodal kiosk that displays a realisti-
cally rendered avatar head. The avatar can engage and 
interact via natural language with one or more partici-
pants, and plays a simple game in which the users have 
to respond to multiple-choice trivia questions. The sys-
tem, and sample interactions are described in more de-
tail in (Bohus and Horvitz, 2009.) 
The hardware and software architecture is also illu-
strated in Figure 2. Data gathered from a wide-angle 
camera, a 4-element linear microphone array, and a 19? 
touch-screen is forwarded to a scene analysis module 
that fuses the incoming streams and constructs in real-
time a coherent picture of the dynamics in the surround-
ing environment. The system detects and tracks the lo-
cation of multiple agents in the scene, tracks the head 
pose for engaged agents, and infers the focus of atten-
tion, activities, goals and (group) relationships among 
different agents in the scene. An in-depth description of 
these scene analysis components falls beyond the scope 
of this paper, but more details are available in (Bohus 
and Horvitz, 2009). The scene analysis results are for-
warded to the control level, which is structured in a two-
layer reactive-deliberative architecture. The reactive 
layer implements and coordinates low-level behaviors, 
including engagement, conversational floor manage-
ment and turn-taking, and coordinating spoken and ges-
tural outputs. The deliberative layer plans the system?s 
dialog moves and high-level engagement actions. 
We deployed the system described above in an open-
space near the kitchenette area in our building. As we 
were interested in exploring the influence of the spatial 
setup on the engagement models, we deployed the sys-
tem in two different spatial orientations, illustrated to-
gether with the resulting visual fields of view in Figure 
3. Even though the location is similar, the two orienta-
tions create considerable differences in the relative tra-
jectories of people that go by (dashed lines) and people 
that engage with the system (continuous lines). In the 
side orientation, people typically enter the system?s field 
Figure 2. System prototype and architectural overview. 
Dialog Management 
Behavioral Control 
Scene Analysis Output Planning 
Vision Speech Synthesis Avatar 
wide-angle camera 
4-element linear microphone array  
touch screen 
speakers 
247
of view and approach it from the sides. In the front 
orientation, people enter the field of view and approach 
either frontally, or from the immediate right side.  
6 Data and Modeling 
The system was deployed during regular business hours 
for 10 days in each of the two orientations described 
above, for a total of 158 hours and 32 minutes. No in-
structions were provided and most people that interacted 
with the system did so for the first time.  
6.1 Corpus and Implicit Labels 
Throughout the data collection, the system used a con-
servative heuristic to detect engagement intentions: it 
considered that a user wanted to engage when they ap-
proached the system and entered in an F-formation 
(Kendon, 1990b) with it. Specifically, if a sufficiently 
large (close by) frontal face was detected in front of it, 
the system triggered an engaging action and started the 
interaction. We found this F-formation heuristic to be 
fairly robust, having a false-positive rate of 0.18% (6 
false engagements out of 3274 total faces tracked). In 2 
of these cases the face tracker committed an error and 
falsely identified a large nearby face, and in 4 cases a 
person passed by very close to the system but without 
any visible intention to engage.   
Although details on false-negative statistics have not 
yet been calculated (this would require a careful exami-
nation of all 158 hours of data), our experience with the 
face detector suggests this number is near 0. In months 
of usage, we never observed a case where the system 
failed to detect a close by, frontal face. At the same time, 
we note that there is an important distinction between 
people who actually engage with the system, and people 
who intend to engage, but perhaps not come in close-
enough proximity for the system to detect this intention 
(according to the heuristic described above). In this 
sense, while our heuristic can detect people who engage 
at a 0 false-negative rate, the false-negative rate with 
respect to engagement intentions is non-zero. Despite 
these false-negatives, we found that the proposed heu-
ristic still represents a good starting point for learning to 
detect engagement intentions. As we shall see later, em-
pirical results indicate that, by learning to detect who 
actually engages, the system can learn to also detect 
people who might intend to engage, but who ultimately 
do not engage with the dialog system.  
In the experiments described here, we focus on de-
tecting engagement intentions for people that ap-
proached while the system was idle. We therefore au-
tomatically eliminated all faces that were temporally 
overlapping with the periods when the system was al-
ready engaged in an interaction. For the remaining face 
traces, we automatically generate labels as follows: 
? if a person entered in an F-formation and became 
engaged in interaction with the system at time ?? , 
the corresponding face trace was labeled with a 
positive engagement intention label from ??-20sec; 
until ?? ; the initial portion of the trace, from the 
moment it was detected until ??-20sec was marked 
with a negative engagement intention label. Final-
ly, the remainder of the trace (from ??  until the 
face disappeared) was discarded, as the user was 
actively engaged with the system during this time.  
? if the face was never engaged in interaction (i.e. a 
person was just passing by), the entire trace was 
labeled with a negative engagement intention.  
Note that in training the models described below we 
used these automatic labels, which are not entirely accu-
rate: they include a small number of false-positives, as 
discussed above. However, for evaluation purposes, we 
used the corrected labels (no false-positives). 
6.2 Models 
To review, the task at hand is to learn a model for pre-
dicting engagement intentions, based on information 
that can be extracted at runtime from face traces, includ-
ing spatiotemporal trajectory and cues about attention. 
We cast this problem as a frame-by-frame binary classi-
fication task: at each frame, the model must classify 
each visible face as either intending to engage or not. 
We used a maximum entropy model to make this pre-
diction:  
 
? ?? ? =
1
?(?)
???   ?? ? ??(?)
?
  
 
The key role in the proposed maximum entropy 
model is played by the set of features ??(?), which must 
capture cues that are relevant for detecting an engage-
ment intention. We designed several subsets of features, 
summarized in Table 2. The location subset, loc, in-
cludes the x and y location of the detected face in the 
visual scene, and the width and height of the face region, 
which indirectly reflect the proximity of the agent. The 
second feature subset, loc+ff, also includes a probability 
score (and a binarized version of it) produced by the 
face detector which reflects the confidence that the face 
is frontal and thus provides an automatic measure of the 
focus-of-attention of the agent. Apart from these auto-
Table 1. Corpus statistics. 
 Side Front Total 
Size (hours:minutes) 83:16 75:15 158:32 
# face traces 2025 1249 3274 
# engaged 
% engaged  
72 
3.55% 
74 
5.92% 
146 
4.46% 
# false-positive engaged 
% false-positive engaged 
1 
0.04% 
5 
0.40% 
6 
0.18% 
# not-engaged  
% not-engaged  
1953 
96.45% 
1175 
94.08% 
3128 
95.54% 
 
248
matically generated attention features, we also experi-
mented with a manually annotated binary attention 
score, attn. The attention of each detected face was ma-
nually tagged throughout the entire dataset. This infor-
mation is not available to the system at runtime; we use 
it only to identify an upper performance baseline.   
The maximum entropy model is not temporally struc-
tured. The temporal structure of the spatial and atten-
tional trajectory is captured via a set of additional fea-
tures, derived as follows. Given an existing feature f, we 
compute a set of trajectory features traj.w(f) by accumu-
lating aggregate statistics for the feature f over a past 
window of size w frames. We explored windows of size 
5, 10, 20, 30. For continuous features, the trajectory 
statistics include the min, max, mean, and variance of 
the features in the specified window. In addition, we 
performed a linear and a quadratic fit of f in this window, 
and used the resulting coefficients (2 for the linear fit 
and 3 for the quadratic fit) as features (see the example 
in Figure 4). For the binary features, the trajectory sta-
tistics include the number and proportion of times the 
feature had a value of 1 in the given window, and the 
number of frames since the feature last had a value of 1.  
7 Experimental Results 
We trained and evaluated (using a 10-fold cross-
validation process) a set of models for each of the two 
system orientations shown in Figure 3 and for each of 
the 5 feature subsets shown in Table 2. The results on 
the per-frame classification task, including the ROC 
curves for the different models are presented and dis-
cussed in more detail in Appendix A.  
At runtime, the system uses these frame-based mod-
els to predict across time the likelihood that a given 
agent intends to engage (see Figure 5). In this context, 
an evaluation that counts the errors per person (i.e., per 
trace), rather than errors per frame is more informative. 
Furthermore, since early detection is important for sup-
porting a natural engagement process, an informative 
evaluation should also capture how soon a model can 
detect a positive engagement intention (see Figure 5).  
Making decisions about an agent?s engagement in-
tentions typically involves comparing the probability of 
engagement against a preset threshold. Given a thre-
shold, we can compute for each model the number of 
false-positives at the trace level: if the prediction ex-
ceeds the threshold at any point in the trace, we consider 
that a positive detection. We note that, if we aim to 
detect people who will actually engage, there are no 
false negatives at the trace level. The system can use the 
machine learned models in conjunction with the pre-
vious heuristic (a user is detected standing in front of 
the system), to eventually detect when people engage. 
Also, given a threshold, we can identify how early a 
model can correctly detect the intention to engage 
(compared to the existing F-formation heuristic that 
defined the moment of engagement in the training data). 
These durations are illustrated for a threshold of 0.5 in 
Figure 5, and are referred to in the sequel as early detec-
tion time. By varying the threshold between 0 and 1, we 
can obtain a profile that links the false-positive rate at 
the trace level to how early the system can detect en-
gagement, i.e. to the mean early detection time.  
Figure 6 shows the false-positive rate as a function of 
the mean early detection time for models trained using 
each of the five feature subsets shown in Table 2, in the 
side orientation. The model that uses only location in-
formation (including the size of the face and proximity) 
performs worst. Adding automatically extracted infor-
mation about attention leads only to a marginal im-
provement. However, adding information about the tra-
Feature sets Description [total # of features in set] 
Loc location features: x, y, width and height [4] 
loc+ff 
location features plus a confidence score indicat-
ing whether the face is frontal (ff), as well as a 
binary version of this score (ff=1) [6] 
traj(loc) 
location features plus trajectory of location fea-
tures over windows of 5, 10, 20, 30 frames [118] 
traj(loc+ff) 
location and face frontal features, as well as 
trajectory of location and of face-frontal features 
over windows of 5, 10, 20, 30 frames [172] 
traj(loc+attn) 
location and manually labeled attention features, 
as well as trajectory of location and of attention 
over windows of 5, 10, 20, 30 frames [133] 
 
Table 2. Feature sets for detecting engagement intention. 
0 10 20 30 40 50
100
200
300
400
500
600
30 frame window 
current frame 
x 
Figure 4. Trajectory features extracted by fitting linear and 
quadratic functions. 
Figure 5. Example predictions for three different models. 
0 5 10 15
0
0.5
1
0
50
100
0
640
0
0.5
1
x 
width 
frontal 
traj(loc+ff) 
traj(loc) 
loc early detection time = 10.4 sec 
5.4 sec 
4.0 sec 
249
jectory of location and of attention, leads to larger cu-
mulative gains. Adding the more accurate (manually 
tagged) information about attention yields the best mod-
el. The relative performance of these models (which can 
be observed at the frame-level in Appendix A) confirms 
our expectations and the importance of trajectory fea-
tures (both spatial and attentional) in detecting engage-
ment intentions. The results also indicate that the differ-
ences, and hence the importance of these features, are 
larger when trying to detect engagement early on, i.e. at 
larger early detection times. Tables 3 and 4 further high-
light these differences. For instance, when detecting 
engagement intentions at a mean early detection above 3 
seconds, the model that uses trajectory information, 
traj(loc+ff), decreases the false positive rate by a factor of 
3 compared to the location-only model.  
Figure 7 and Tables 5 and 6 show the results for the 
front orientation. The relative trends are similar to those 
observed in the side orientation, highlighting again the 
importance of trajectory features. At the same time, the 
models are performing slightly worse in absolute terms, 
which is consistent with the increased difficulty of the 
task. Several contributing factors can be identified in 
Figure 3: people may simply pass by in closer proximity 
to the system; people who come from the corridor are 
generally frontally oriented towards the system, making 
frontal face cues less informative; and finally, people 
who will engage need to deviate less from the regular 
trajectory of people who are just passing by.   
Next, we review how well the models trained gene-
ralize across the two different setups, by evaluating the 
trajectory models traj(loc+ff) across the two datasets. The 
results indicate that the models are attuned to the dataset 
they are trained on (see Figure 7). As we discussed ear-
lier, we expect this result given the different geometry 
of the relative trajectories of engagement in the two 
orientations. These results highlight the importance of 
learning in situ, and show that the proposed approach 
can be used to learn the specific patterns of engagement 
in a given environment automatically, without explicit 
developer supervision.  
Finally, we performed an error analysis. We focused 
on the side orientation and visually inspected the 79 
(4%) false-positive errors committed by the traj(loc+ff) 
Model 
Early detection time 
FP=2.5% FP=5% FP=10% FP=20% 
loc 1.14 1.97 2.29 2.92 
loc+ff 1.70 2.25 2.74 3.18 
traj(loc) 1.93 2.57 3.13 3.66 
traj(loc+ff) 1.99 2.64 3.44 4.02 
traj(loc+attn) 1.97 2.47 3.52 4.15 
 
Model 
Early detection time 
FP=2.5% FP=5% FP=10% FP=20% 
loc 2.18 2.72 3.09 3.59 
loc+ff 2.25 2.74 3.08 3.63 
traj(loc) 2.51 3.03 3.53 4.07 
traj(loc+ff) 2.68 3.20 3.68 4.22 
traj(loc+attn) 3.08 3.52 4.13 4.49 
 
Figure 6. False-positives vs. early detection time (side). 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time (seconds) 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time (seconds) 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+attn)
0 1 2 3 4 5
0%
10%
20%
30%
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
raj(loc+attn)
Table 3. *False-positive rate at different EDT (side) Table 5. *False-positive rate at different EDT (front) 
Table 4.*Early detection times at different FP rates (side). Table 6 * Early detection times at different FP rates (front). 
 
Figure 7. False-positives vs. early detection time (front). 
Model 
False positive rate 
EDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4 
loc 0.31% 1.6% 4.3% 9.4% 18.4% 32.6% 
loc+ff 0.31% 1.5% 4.1% 8.7% 18.3% 28.6% 
traj(loc) 0.31% 1.1% 2.6% 4.8% 9.3% 18.6% 
traj(loc+ff) 0.15% 0.9% 2.0% 4.0% 7.1% 14.3% 
traj(loc+attn) 0.26% 0.6% 1.1% 2.2% 5.1% 8.9% 
 
Model 
False positive rate 
EDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4 
loc 2.3% 5.8% 11.3% 23.0% 35.2% 44.5% 
loc+ff 1.6% 3.7% 7.3% 15.8% 28.5% 41.7% 
traj(loc) 1.1% 3.1% 4.7% 8.2% 15.6% 36.8% 
traj(loc+ff) 1.2% 2.7% 4.7% 7.2% 10.9% 19.8% 
traj(loc+attn) 0.8% 2.9% 5.4% 5.4% 10.3% 16.1% 
 
*shaded cells in Tables 3-6 show statistically significant improvements in performance (p<0.05) over the corresponding model that uses the immediately previous 
feature set (e.g. the cell right above). The traj(loc), traj(loc+ff), traj(loc+attn) always statistically significantly (p<0.05) improve upon the loc models 
250
model when using a threshold corresponding to a mean 
early detection time of 3 seconds. This analysis indi-
cates that in 22 out of these 79 errors (28%) the person 
did actually exhibit behaviors consistent with an inten-
tion to engage the system, such as stopping by or turn-
ing around after passing the system, and approaching 
and maintaining sustained attention for a significant 
amount of time. These cases represent false-negatives 
committed by our conservative F-formation heuristic 
with respect to engagement intention; the user did not 
approach close enough for the system to trigger en-
gagement. The actual false-positive rate of the trained 
model is therefore 2.9% rather than 4%. The system was 
able to correctly identify these cases because the beha-
vioral patterns are similar to the ones exhibited by 
people who did approach close enough for the heuristic 
detector to fire. We plan to assess the false-negative rate 
of the current heuristic more closely and explore how 
many false negatives are actually recovered by the 
trained model.  This analysis will require that multiple 
judges assess engagement intentions on all 3274 traces.  
8 Summary and Future Work 
We described an approach to learning engagement in-
tentions in a situated conversational system. The pro-
posed models fit into a larger framework for supporting 
multiparty, situated engagement and open-world dialog 
(Bohus and Horvitz, 2009a; 2009b). Experimental re-
sults indicate that a system using the proposed approach 
can learn to detect engagement intentions at low false 
positive rates up to 3-4 seconds prior to the actual mo-
ment of engagement. The models leverage features that 
capture spatiotemporal and attentional cues that are 
tuned to the specifics of the physical environment in 
which the system operates. Furthermore, the models can 
be trained in previously unseen environments, without 
any explicit developer supervision. 
We believe the methods and results described 
represent a first step towards supporting fluid, natural 
engagement in open-world interaction. Numerous chal-
lenges remain. While we confirmed the importance of 
spatiotemporal and attentional features in detecting en-
gagement intentions, we believe that leveraging addi-
tional and more accurate sensory information (e.g. body 
pose, eye gaze, more accurate depth information, agent 
identity coupled with longer term memory features) 
may improve performance. Secondly, while the current 
models where trained in a batch fashion, the proposed 
method naturally lends itself to an online approach, 
where the system starts with a prior model for detecting 
engagement intentions, and refines this model online. 
More importantly, rather than just learning to detect 
engagement intentions, we plan to focus on the more 
general problem of controlling the engagement process: 
how should the system time its actions (i.e. gaze and 
sustained attention, smiles, greeting, etc.) to create natu-
ral, fluid engagements in the open world. Introducing 
mobility to dialog systems brings yet another interesting 
dimension to this problem: how can a mobile system, 
such as a robot, detect engagement intentions and re-
spond to support a natural engagement process? We 
believe that there is great opportunity to address these 
challenges by learning predictive models from data.  
References 
M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-
bridge University Press, New York 
D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-
lenges, Directions and Prototype, to appear in KRPD?09, 
Pasadena, CA 
D. Bohus and E. Horvitz, 2009b, Computational Models for 
Multiparty Engagement in Open-World Dialog, submitted 
to SIGdial?09, London, UK.  
E. Goffman, 1963, Behaviour in public places: notes on the 
social order of gatherings, The Free Press, New York 
E.T. Hall, 1966, The Hidden Dimension: man?s use of space in 
public and private, New York: Doubleday. 
A. Kendon, 1990a, A description of some human greetings, 
Conducting Interaction: Patterns of behavior in focused en-
counters, Studies in International Sociolinguistics, Cam-
bridge University Press 
A. Kendon, 1990b, Spatial organization in social encounters: 
the F-formation system, Conducting Interaction: Patterns of 
behavior in focused encounters, Studies in International 
Sociolinguistics, Cambridge University Press 
M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial 
model of engagement for a social robot, in 9th IEEE Work-
shop on Advanced Motion Control, pp. 762-767 
C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005a, 
A model of attention and interest using gaze behavior, Lec-
ture Notes in Computer Science, pp. 229-240. 
C. Peters, 2005b, Direction of Attention Perception for Con-
versation Initiation in Virtual Environments, in Intelligent 
Virtual Agents, 2005, pp. 215-228.  
C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to 
Look: A Study of Human-Robot Engagement, IUI?2004, pp. 
78-84, Madeira, Portugal 
C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005, 
Explorations in engagement for humans and robots, Artifi-
cial Intelligence, 166 (1-2), pp. 140-164 
R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001, 
Eye gaze patterns in conversations: there is more to con-
versational agents than meets the eyes, CHI?01  
Figure 7. Model evaluation across orientations. 
 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
traj(loc+ff) trained on side data
traj(loc+ff) trained on front data
F
a
ls
e
 p
o
s
it
iv
e
s
 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
traj(loc+ff) trained on front data
traj(loc+ff) traine  on side data
Mean early detection time 
Evaluation on side data Evaluation on front data 
251
  
 
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+attn)
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+a tn)
Appendix A. Per-frame evaluation of maximum entropy models for detecting engagement intentions 
 
Model Avg. log-likelihood Hard error 
Base Train CV Base Train CV 
loc -0.1651 -0.1222 -0.1259 3.91% 3.22% 3.25% 
loc+ff -0.1651 -0.0962 -0.0984 3.91% 3.01% 3.07% 
traj(loc) -0.1651 -0.0947 -0.1073 3.91% 2.88% 3.06% 
traj(loc+ff) -0.1651 -0.0836 -0.0904 3.91% 2.69% 2.85% 
traj(loc+attn) -0.1651 -0.0765 -0.0810 3.91% 2.47% 2.56% 
 
Figure 1. Per-frame ROC for side orientation models 
T
ru
e
 p
o
s
it
iv
e
s
 (
s
e
n
s
it
iv
it
y
) 
False positives (1-specificity) 
Figure 2. Per-frame ROC for front orientation models 
 
False positives (1-specificity) 
T
ru
e
 p
o
s
it
iv
e
s
 (
s
e
n
s
it
iv
it
y
) 
Model Avg. log-likelihood Hard error 
Base Train CV Base Train CV 
loc -0.1875 -0.1451 -0.1498 4.63% 4.58% 4.72% 
loc+ff -0.1875 -0.1326 -0.1392 4.63% 4.22% 4.39% 
traj(loc) -0.1875 -0.1262 -0.1338 4.63% 3.99% 4.24% 
traj(loc+ff) -0.1875 -0.1159 -0.1298 4.63% 3.91% 4.38% 
traj(loc+attn) -0.1875 -0.1150 -0.1267 4.63% 4.04% 4.47% 
 
Table 1. Baseline, training-set and cross-validation 
performance (data average log-likelihood and classifi-
cation error) for side orientation models 
Table 2. Baseline, training-set and cross-validation 
performance (data average log-likelihood and classifi-
cation error) for front orientation models 
252
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 466?475,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative state tracking for spoken dialog systems
Angeliki Metallinou1?, Dan Bohus2, and Jason D. Williams2
1University of Southern California, Los Angeles, CA, USA
2Microsoft Research, Redmond, WA, USA
metallin@usc.edu dbohus@microsoft.com jason.williams@microsoft.com
Abstract
In spoken dialog systems, statistical state
tracking aims to improve robustness to
speech recognition errors by tracking a
posterior distribution over hidden dialog
states. Current approaches based on gener-
ative or discriminative models have differ-
ent but important shortcomings that limit
their accuracy. In this paper we discuss
these limitations and introduce a new ap-
proach for discriminative state tracking
that overcomes them by leveraging the
problem structure. An offline evaluation
with dialog data collected from real users
shows improvements in both state track-
ing accuracy and the quality of the pos-
terior probabilities. Features that encode
speech recognition error patterns are par-
ticularly helpful, and training requires rel-
atively few dialogs.
1 Introduction
Spoken dialog systems interact with users via nat-
ural language to help them achieve a goal. As the
interaction progresses, the dialog manager main-
tains a representation of the state of the dialog
in a process called dialog state tracking. For ex-
ample, in a bus schedule information system, the
dialog state might indicate the user?s desired bus
route, origin, and destination. Dialog state track-
ing is difficult because automatic speech recog-
nition (ASR) and spoken language understand-
ing (SLU) errors are common, and can cause the
system to misunderstand the user?s needs. At
the same time, state tracking is crucial because
the system relies on the estimated dialog state to
choose actions ? for example, which bus schedule
information to present to the user.
The dialog state tracking problem can be for-
malized as follows (Figure 1). Each system turn
in the dialog is one datapoint. For each datapoint,
the input consists of three items: a set of K fea-
tures that describes the current dialog context, G
dialog state hypotheses, and for each dialog state
hypothesis, M features that describe that dialog
state hypothesis. The task is to assign a probabil-
ity distribution over the G dialog state hypotheses,
plus a meta-hypothesis which indicates that none
of the G hypotheses is correct.
Note that G varies across turns (datapoints) ?
for example, in the first turn of Figure 1, G = 3,
and in the second and third turns G = 5. Also
note that the dialog state tracker is not predicting
the contents of the dialog state hypotheses; the di-
alog state hypotheses contents are given by some
external process, and the task is to predict a proba-
bility distribution over them, where the probability
assigned to a hypothesis indicates the probability
that it is correct. It is a requirement that the G
hypotheses are disjoint; with the special ?every-
thing else? meta-hypothesis, exactly one hypoth-
esis is correct by construction. After the dialog
state tracker has output its distribution, this distri-
bution is passed to a separate, downstream process
that chooses what action to take next (e.g., how to
respond to the user).
Dialog state tracking can be seen an analogous
to assigning a probability distribution over items
on an ASR N-best list given speech input and the
recognition output, including the contents of the
N-best list. In this task, the general features de-
scribe the recognition overall (such as length of
utterance), and the hypothesis-specific features de-
scribe each N-best entry (such as decoder cost).
? Work done while at Microsoft Research
466
Another analogous task is assigning a probabil-
ity distribution over a set of URLs given a search
query and the URLs. Here, general features de-
scribe the whole set of results, e.g., number of
words in the query, and hypothesis-specific fea-
tures describe each URL, e.g., the fraction of
query words contained in page.
For dialog state tracking, most commercial sys-
tems use hand-crafted heuristics, selecting the
SLU result with the highest confidence score,
and discarding alternatives. In contrast, statisti-
cal approaches compute a posterior distribution
over many hypotheses for the dialog state. The
key insight is that dialog is a temporal process in
which correlations between turns can be harnessed
to overcome SLU errors. Statistical state track-
ing has been shown to improve task completion
in end-to-end spoken dialog systems (Bohus and
Rudnicky (2006); Young et al (2010); Thomson
and Young (2010)).
Two types of statistical state tracking ap-
proaches have been proposed. Generative ap-
proaches (Horvitz and Paek (1999); Williams and
Young (2007); Young et al (2010); Thomson and
Young (2010)) use generative models that capture
how the SLU results are generated from hidden
dialog states. These models can be used to track
an arbitrary number of state hypotheses, but can-
not easily incorporate large sets of potentially in-
formative features (e.g. from ASR, SLU, dialog
history), resulting in poor probability estimates.
As an illustration, in Figure 1, a generative model
might fail to assign the highest score to the correct
hypothesis (61C) after the second turn. In contrast,
discriminative approaches use conditional mod-
els, trained in a discriminative fashion (Bohus and
Rudnicky (2006)) to directly estimate the distribu-
tion over a set of state hypotheses based on a large
set of informative features. They generally pro-
duce more accurate distributions, but in their cur-
rent form they can only track a handful of state hy-
potheses. As a result, the correct hypothesis may
be discarded: for instance, in Figure 1, a discrim-
inative model might consider only the top 2 SLU
results, and thus fail to consider the correct 61C
hypothesis at all.
The main contribution of this paper is to de-
velop a new discriminative model for dialog state
tracking that can operate over an arbitrary number
of hypotheses and still compute accurate proba-
bility estimates. We also explore the relative im-
portance of different feature sets for this task, and
measure the amount of data required to reliably
train our model.
2 Data and experimental design
We use data from the public deployment of two
systems in the Spoken Dialog Challenge (Black
et al (2010)) which provide bus schedule infor-
mation for Pittsburgh, USA. The systems, DS1
and DS2, were fielded by AT&T, and are de-
scribed in Williams et al (2010) and Williams
(2012). Both systems followed a highly directed
flow, separately collecting 5 slots. All users were
asked for their bus route, origin, and destination;
then, they were optionally prompted for a date and
time. Each slot was explicitly or implicitly con-
firmed before collecting the next. At the end, bus
times were presented. The two systems differed in
acoustic models, confidence scoring model, state
tracking method and parameters, number of sup-
ported routes (8 vs 40, for DS1 and DS2 respec-
tively), presence of minor bugs, and user popu-
lation. These differences yield distinctions in the
distributions in the two corpora (Williams (2012)).
In both systems, a dialog state hypothesis con-
sists of a value of the user?s goal for a certain
slot: for example, a state hypothesis for the origin
slot might be ?carnegie mellon university?. The
number G of state hypotheses (e.g. slot values)
observed so far depends on the dialog, and turn
within that dialog. For instance, in Fig. 1, G pro-
gressively takes values 3, 5 and 5. Dialog state
hypotheses with identical contents (e.g., the same
bus route) are merged. The correctness of the SLU
results was manually labeled by professional an-
notators.
2.1 Experimental setup
To perform a comparative analysis of various state
tracking algorithms, we test them offline, i.e., by
re-running state tracking against the SLU results
from deployment. However, care must be taken:
when the improved state-tracker is installed into a
dialog system and used to drive action selection,
the distribution of the resulting dialog data (which
is an input for the state tracker) will change. In
other words, it is known a priori that the train
and test distributions will be mismatched. Hence,
when conducting offline experiments, if train and
test data were drawn from the same matched dis-
tribution, this may overstate performance.
467
Figure 1: Overview of dialog state tracking. In this example, the dialog state contains the user?s desired
bus route. At each turn, the system produces a spoken output. The user?s spoken response is processed
to extract a set of spoken language understanding (SLU) results, each with a local confidence score. A
set of G dialog state hypotheses is formed by considering all SLU results observed so far, including
the current turn and all previous turns. For each state hypothesis, a feature extractor produces a set of
M hypothesis-specific features, plus a single set of K general features that describes the current dialog
context. The dialog state tracker uses these features to produce a distribution over theG state hypotheses,
plus a meta-hypothesis rest which accounts for the possibility that none of the G hypotheses are correct.
dataset train set test set
MATCH1 half calls from DS2 remaining calls in DS2
MATCH2 half calls from DS1,
half from DS2
remaining calls from
DS1 and DS2
MISMATCH all calls from DS1 all calls from DS2
Table 1: Train-test data splits
To account for this effect, we explicitly study
train/test mismatch through three partitions of data
from DS1 and DS2 (see Table 1): MATCH1 con-
tains matched train/test data from the DS2 dataset;
MATCH2 contains matched train/test data from
both datasets; finally, MISMATCH contains mis-
matched train/test data. While the MISMATCH
condition may not identically replicate the mis-
match observed from deploying a new state tracker
online (since online characteristics depend on user
behavior) training on DS1 and testing on DS2 at
least ensures the presence of some real-world mis-
match.
We assess performance via two metrics: accu-
racy and L2 norm. Accuracy indicates whether the
state hypothesis with the highest assigned proba-
bility is correct, where rest is correct iff none of
the SLU results prior to the current turn include the
user?s goal. High accuracy is important as a dialog
system must ultimately commit to a single inter-
pretation of the user?s needs ? e.g., it must commit
to a route in order to provide bus timetable infor-
mation. In addition, the L2 norm (or Brier score,
Murphy (1973)) also captures how well calibrated
the output probabilities are, which is crucial to de-
cision theoretic methods for action selection. The
L2 norm is computed between the output poste-
rior and the ground-truth vector, which has 1 in
the position of the correct item and 0 elsewhere.
Both metrics are computed for each slot in each
turn, and reported by averaging across all turns
and slots.
468
2.2 Hand-crafted baseline state tracker
As a baseline, we construct a hand-crafted state
tracking rule that follows a strategy common in
commercial systems: it returns the SLU result
with the maximum confidence score, ignoring all
other hypotheses. Although this is very a simple
rule, it is very often effective. For example, if the
user says ?no? to an explicit confirmation or ?go
back? to an implicit confirmation, they are asked
the same question again, which gives an opportu-
nity for a higher confidence score. Of the G pos-
sible hypotheses for a slot, we denote the number
actually assigned a score by a model as G?, so in
this heuristic baseline G? = 1.
The performance of this baseline (BASELINE
in Table 3) is relatively strong because the top
SLU result is by far most likely to be correct, and
because the confidence score was already trained
with slot-specific speech data (Williams and Bal-
akrishnan (2009), Williams (2012)). However,
this simple rule can?t make use of SLU results on
the N-best list, or statistical priors; these limita-
tions motivate the use of statistical state trackers,
introduced next.
3 Generative state tracking
Generative state tracking approaches leverage
models that describe how SLU results are gener-
ated from a hidden dialog state, denoted g. The
user?s true (unobserved) action u is conditioned on
g and the system action a via a user action model
P (u|g, a), and also on the observed SLU result
u? via a model of how SLU results are generated
P (u?|u). Given a prior distribution b(g) and a re-
sult u?, an updated distribution b?(g) can be com-
puted by summing over all hidden user actions u:
b?(g) = ?
?
u
P (u?|u) ? P (u|g, a)b(g) (1)
where ? is a normalizing constant (Williams and
Young (2007)). Generative approaches model the
posterior over all possible dialog state hypotheses,
including those not observed in the SLU N-best
lists. In general this is computationally intractable
because the number of states is too large. One ap-
proach to scaling up is to group g into a few par-
titions, and to track only states suggested by ob-
served SLU results (Young et al (2010); Williams
(2010); Gas?ic? and Young (2011)). Another ap-
proach is to factor the components of a dialog
state, make assumptions about conditional inde-
pendence between the components, and apply ap-
proximate inference techniques such as loopy be-
lief propagation (Thomson and Young (2010)).
In deployment, DS1 and DS2 used the AT&T
Statistical Dialog Toolkit (ASDT) for dialog state
tracking (Williams (2010); AT&T Statistical Dia-
log Toolkit). ASDT implements a generative up-
date of the form of Eq 1, and uses partitions to
maintain tractability. Component models were
learned from dialog data from a different dia-
log system. A maximum of G? = 20 state hy-
potheses were tracked for each slot. The per-
formance (GENONLINE in Table 3), was worse
than BASELINE: an in-depth analysis attributed
this to the mismatch between train and test data
in the component models, and to the underlying
flawed assumption of eq. 1 that observations at
different turns are independent conditioned on the
dialog state ? in practice, confusions made by
speech recognition are highly correlated (Williams
(2012)).
For all datasets, we re-estimated the models on
the train set and re-ran generative tracking with
an unlimited number of partitions (i.e., G? = G);
see GENOFFLINE in Table 3. The re-estimated
tracker improved accuracy in MATCH conditions,
but degraded accuracy in the MISMATCH condi-
tion. This can be partly attributed to the difficulty
in estimating accurate initial priors b(g) for MIS-
MATCH, where the bus route, origin, and destina-
tion slot values in train and test systems differed
significantly.
4 Discriminative State Tracking:
Preliminaries and existing work
In contrast to generative models, discriminative
approaches to dialog state tracking directly predict
the correct state hypothesis by leveraging discrim-
inatively trained conditional models of the form
b(g) = P (g|f), where f are features extracted
from various sources, e.g. ASR, SLU, dialog his-
tory, etc. In this work we will use maximum en-
tropy models. We begin by briefly introducing
these models in the next subsection. We then de-
scribe the features used, and finally review exist-
ing discriminative approaches for state tracking
which serve as a starting point for the new ap-
proach we introduce in Section 5.
469
4.1 Maximum entropy models
The maximum entropy framework (Berger et al
(1996)) models the conditional probability distri-
bution of the label y given features x, p(y|x) via
an exponential model of the form:
P (y|x, ?) = exp(
?
i?I ?i?i(x, y))?
y?Y exp(
?
i?I ?i?i(x, y))
(2)
where ?i(x, y) are feature functions jointly de-
fined on features and labels, and ?i are the model
parameters. The training procedure optimizes the
parameters ?i to maximize the likelihood over the
data instances subject to regularization penalties.
In this work, we optimize the L1 penalty using a
cross-validation process on the train set, and we
use a fixed L2 penalty based on heuristic based on
the dataset size. The same optimization is used for
all models.
4.2 Features
Discriminative approaches for state tracking rely
on informative features to predict the correct di-
alog state. In this work we designed a set of
hypothesis-specific features that convey informa-
tion about the correctness of a particular state hy-
pothesis, and a set of general features that convey
information about the correctness of the rest meta-
hypothesis.
Hypothesis-specific features can be grouped
into 3 categories: base, history and confusion fea-
tures. Base features consider information about
the current turn, including rank of the current SLU
result (current hypothesis), the SLU result confi-
dence score(s) in the current N-best list, the differ-
ence between the current hypothesis score and the
best hypothesis score in the current N-best list, etc.
History features contain additional useful informa-
tion about past turns. Those include the number of
times an SLU result has been observed before, the
number of times an SLU result has been observed
before at a specific rank such as rank 1, the sum
and average of confidence scores of SLU results
across all past recognitions, the number of possi-
ble past user negations or confirmations of the cur-
rent SLU result etc.
Confusion features provide information about
likely ASR errors and confusability. Some recog-
nition results are more likely to be incorrect than
others ? background noise tends to trigger certain
results, especially short bus routes like ?p?. More-
over, similar sounding phrases are more likely to
be confused. The confusion features were com-
puted on a subset of the training data. For each
SLU result we computed the fraction of the time
that the result was correct, and the binomial 95%
confidence interval for that estimate. Those two
statistics were pre-computed for all SLU results
in the training data subset, and were stored in a
lookup table. At runtime, when an SLU hypoth-
esis is recognized, its statistics from this lookup
table are used as features. Similar statistics were
computed for prior probability of an SLU result
appearing on an N-best list, and prior probability
of SLU result appearance at specific rank positions
of an N-best list, prior probability of confusion be-
tween pairs of SLU results, and others.
General features provide aggregate information
about dialog history and SLU results, and are
shared across different SLU results of an N-best
list. For example, from the current turn, we use
the number of distinct SLU results, the entropy
of the confidence scores, the best path score of
the word confusion network, etc. We also include
features that contain aggregate information about
the sequence of all N-best lists up to the current
turn, such as the mean and variance of N-best list
lengths, the number of distinct SLU results ob-
served so far, the entropy of their corresponding
confidence scores, and others.
We denote the number of hypothesis-specific
features as M , and the number of general features
asK. K andM are each in the range of 100?200,
although M varies depending on whether history
and confusion features are included. For a given
dialog turn with G state hypotheses, there are a to-
tal of G ?M +K distinct features.
4.3 Fixed-length discriminative state
tracking
In past work, Bohus and Rudnicky (2006) intro-
duced discriminative state tracking, casting the
problem as standard multiclass classification. In
this setup, each turn constitutes one data instance.
Since in dialog state tracking the number of state
hypotheses varies across turns, Bohus and Rud-
nicky (2006) chose a subset of G? state hypothe-
ses to score. In this work we used a similar
setup, where we considered the top G1 SLU re-
sults from the current N-best list at turn t, and the
top G2 and G3 SLU results from the previous N-
best lists at turns t ? 1 and t ? 2. The problem
can then be formulated as multiclass classification
470
over G?+1 = G1+G2+G3+1 classes, where the
correct class indicates which of these hypotheses
(or rest) is correct. We experimented with differ-
ent values and found that G1 = 3, G2 = 2, and
G3 = 1 (G? = 6) yielded the best performance.
Feature functions are defined in the standard
way, with one feature function ? and weight ? for
each (feature,class) pair. Formally, ? of eq. 2 is
defined as ?i,j(x, y) = xi?(y, j), where ?(y, j) =
1 if y = j and 0 otherwise. i indexes over the
G?M +K features and j over the G? + 1 classes.1
The two-dimensional subscript i, j if used for clar-
ity of notation, but is otherwise identical in role to
the one-dimension subscript i in Eq 2. Figure 2 il-
lustrates the relationship between hypotheses and
weights.
Results are reported as DISCFIXED in Table 3.
In the MATCH conditions, performance is gener-
ally higher than the other baselines, particularly
when confusion features are included. In the MIS-
MATCH condition, performance is worse that the
BASELINE.
A strength of this approach is that it enables
features from every hypothesis to independently
affect every class. However, the total number
of feature functions (hence weights to learn) is
(G? + 1) ? (G?M +K), which increases quadrat-
ically with the number of hypotheses considered
G?. Although regularization can help avoid over-
fitting per se, it becomes a more challenging task
with more features. Learning weights for each
(feature,class) pair has the drawback that the ef-
fect of hypothesis-specific features such as confi-
dence have to be learned separately for every hy-
pothesis. Also, although we know in advance that
posteriors for a dialog state hypothesis are most
dependent on the features corresponding to that
hypothesis, in this approach the features from all
hypotheses are pooled together and the model is
left to discover these correspondences via learn-
ing. Furthermore, items lower down on the SLU
N-best list are much less likely to be correct: an
item at a very deep position (say 19) might never
be correct in the training data ? when this occurs,
it is unreasonable to expect posteriors to be esti-
mated accurately.
As a result of these issues, in practice G? is lim-
ited to being a small number ? here we found that
increasing G? > 6 degraded performance. Yet with
1Although in practice, maximum entropy model con-
straints render weights for one class redundant.
G? = 6, we found that in 10% of turns, the correct
state hypothesis was present but was being dis-
carded by the model, which substantially reduces
the upper-bound on tracker performance. In the
next section, we introduce a novel discriminative
state tracking approach that addresses the above
limitations, and enables jointly considering an ar-
bitrary number of state hypotheses, by exploiting
the structure inherent in the dialog state tracking
problem.
5 Dynamic discriminative state tracking
The key idea in the proposed approach is to use
feature functions that link hypothesis-specific fea-
tures to their corresponding dialog state hypoth-
esis. This approach makes it straightforward to
model relationships such as ?higher confidence for
an SLU result increases the probability of its cor-
responding state hypothesis being correct?. This
formulation also decouples the number of models
parameters (i.e. weights to learn) from the number
of hypotheses considered, allowing an arbitrary
number of dialog states hypotheses to be scored.
Figure 2: The DISCFIXED model is a traditional
maximum entropy model for classification. Every
feature in every hypothesis is linked to every hy-
pothesis, requiring (G?+ 1)(G?M +K) weights.
We begin by re-stating how features are in-
dexed. Recall each dialog state hypothesis has M
hypothesis-specific features; for each hypothesis,
we concatenate these M features with the K gen-
eral features, which are identical for all hypothe-
ses. For the meta-hypothesis rest, we again con-
catenateM+K features, where theM hypothesis-
specific features take special undefined values. We
write xgi to refer to the ith feature of hypothesis g,
where i ranges from 1 to M +K and g from 1 to
G+ 1.
471
Figure 3: The DISCDYN model presented in this
paper exploits the structure of the state tracking
problem. Features are linked to only their own
hypothesis, and weights are shared across all hy-
potheses, requiring M +K weights.
algorithm description
BASELINE simple hand-crafted rule
GENONLINE generative update, in deployed system
GENOFFLINE generative update, re-trained and run offline
DISCFIXED discr. fixed size multiclass (7 classes)
DISCDYN1 discr. joint dynamic estimation
DISCDYN2 discr. joint dynamic estimation, using indicator
encoding of ordinal features
DISCDYN3 discr. joint dynamic estimation, using indicator
encoding and ordinal-ordinal conjunctions
DISCIND discr. separate estimation
Table 2: Description of the various implemented
state tracking algorithms
The model is based on M + K feature func-
tions. However, unlike in traditional maximum
entropy models such as the fixed-position model
above, these features functions are dynamically
defined when presented with each turn. Specif-
ically, for a turn with G hypotheses, we define
?i(x, y = g) = xgi , where y ranges over the
set of possible dialog states G + 1 (and as above
i ? 1 . . .M +K). The feature function ?i is dy-
namic in that the domain of y ? i.e., the number of
dialog state hypotheses to score ? varies from turn
to turn. With feature functions defined this way,
standard maximum entropy optimization is then
applied to learn the corresponding set of M + K
weights, denoted ?i. Fig. 3 shows the relationship
of hypotheses and weights.
In practice, this formulation ? in which general
features are duplicated across every dialog state
hypothesis ? may require some additional feature
engineering: for every hypothesis g and general
feature i, the value of that general feature xgi will
be multiplied by the same weight ?i. The result
is that any setting of ?i affects all scores identi-
cally, with no net change to the resulting poste-
rior. Nonetheless, general features do contain use-
ful information for state tracking; to make use of
them, we add conjunctions (combinations) of gen-
eral and hypothesis-specific features.
We use 3 different feature variants. In DIS-
CDYN1, we use the original feature set, ignor-
ing the problem described above (so that the gen-
eral features contribute no information), result-
ing in M + K weights. DISCDYN2 adds indi-
cator encodings of the ordinal-valued hypothesis-
specific features. For example, rank is encoded
as a vector of boolean indicators, where the first
indicator is nonzero if rank = 1, the second is
nonzero if rank = 2, and the third if rank ?
3. This provides a more detailed encoding of
the ordinal-valued hypothesis-specific features, al-
though it still ignores information from the gen-
eral features. This encoding increases the number
of weights to learn to about 2(M +K).
Finally, DISCDYN3 extends DISCDYN2 by in-
cluding conjunctions of the ordinal-valued general
features with ordinal-valued hypothesis-specific
features. For example, if the 3-way hypothesis-
specific indicator feature for rank described above
were conjoined with a 4-way general indicator
feature for dialog state, the result would be an in-
dicator of dimension 3 ? 4 = 12. This expansion
results in approximately 10(M + K) weights to
learn in DISCDYN3.2
For comparison, we also estimated a simpler
alternative model, called DISCIND. This model
consists of 2 binary classifiers: the first one
scores each hypothesis in isolation, using the M
hypothesis-specific features for that hypothesis +
the K general features for that turn, and outputs a
(single) probability that the hypothesis is correct.
For this classifier, each hypothesis (not each turn)
defines a data instance. The second binary clas-
sifier takes the K general features, and outputs a
probability that the rest meta-hypothesis is correct.
For this second classifier, each turn defines one
data instance. The output of these two models is
then calibrated with isotonic regression (Zadrozny
and Elkan (2002)) and normalized to generate the
posterior over all hypotheses.
2We explored adding all possible conjunctions, including
real-valued features, but this increased memory and computa-
tional requirements dramatically without performance gains.
472
Metric Accuracy (larger numbers better) L2 (smaller numbers better)
Dataset MATCH1 MATCH2 MISMATCH MATCH1 MATCH2 MISMATCH
Features b bc bch b bc bch b bc bch b bc bch b bc bch b bc bch
BASELINE 61.5 61.5 61.5 63.4 63.4 63.4 62.5 62.5 62.5 27.1 27.1 27.1 25.5 25.5 25.5 27.3 27.3 27.3
GENONLINE 54.4 54.4 54.4 55.8 55.8 55.8 54.8 54.8 54.8 34.8 34.8 34.8 32.0 32.0 32.0 34.8 34.8 34.8
GENOFFLINE 57.1 57.1 57.1 60.1 60.1 60.1 51.8 51.8 51.8 37.6 37.6 37.6 33.4 33.4 33.4 42.0 42.0 42.0
DISCFIXED 61.9 66.7 65.3 63.6 69.7 68.8 59.1 61.9 59.3 27.2 23.6 24.4 25.8 21.9 22.4 28.9 27.8 27.8
DISCDYN1 62.0 70.9 71.1 64.4 72.4 72.9 59.4 61.8 62.3 26.3 21.3 20.9 25.0 20.4 20.1 27.7 26.3 25.9
DISCDYN2 62.6 71.3 71.5 65.7 72.1 72.2 61.9 63.2 63.1 26.3 21.4 21.2 24.4 20.5 20.4 26.9 25.8 25.4
DISCDYN3 63.6 70.1 70.9 65.9 72.1 70.7 60.7 62.1 62.9 26.2 21.5 21.4 24.3 20.6 20.7 27.1 25.9 26.1
DISCIND 62.4 69.8 70.5 63.4 71.5 71.8 59.9 63.3 62.2 26.7 23.3 22.5 25.7 21.8 20.7 28.4 27.3 28.8
Table 3: Performance of the different algorithms on each dataset using three feature combinations. Base
features are denoted as b, ASR/SLU confusion features as c and history features as h. Performance for
the feature combinations bh is omitted for space; it is between b and bc.
6 Results and discussion
The implemented state tracking methods are sum-
marized in Table 2, and our results are presented in
Table 3. These results suggest several conclusions.
First, discriminative approaches for state track-
ing broadly outperform generative methods. Since
discriminative methods incorporate many features
and are trained directly to optimize performance,
this is perhaps unsurprising for the MATCH con-
ditions. It is interesting that discriminative meth-
ods are also superior in the more realistic MIS-
MATCH setting, albeit with smaller gains. This
result suggests that discriminative methods have
good promise when deployed into real systems,
where mismatch between training and test distri-
butions is expected.
Second, the dynamic discriminative DISCDYN
models also outperformed the fixed-length dis-
criminative methods. This shows the benefit of
a model which can score every dialog state hy-
potheses, rather than a fixed subset. Third, the
three variants of the DISCDYN model, which pro-
gressively contain more detailed feature encoding
and conjunctions, perform similarly. This suggests
that a relatively simple encoding is sufficient to
achieve good performance, as the feature indica-
tors and conjunctions present in DISCDYN2 and
DISCDYN3 give only a small additional increase.
Among the discriminative models, the jointly-
optimized DISCDYN versions also slightly out-
perform the simpler, independently-optimized DI-
SCIND version. This is to be expected, for two rea-
sons: first, DISCIND is trained on a per-hypothesis
basis, while the DISCDYN models are trained on a
per-turn basis, which is the true performance met-
ric. For example, some turns have 1 hypothesis
and others have 100, but DISCIND training counts
all hypotheses equally. Second, model parameters
in DISCIND are trained independently of compet-
ing hypotheses. However, they should rather be
adjusted specifically so that the correct item re-
ceives a larger score than incorrect items ? not
merely to increase scores for correct items and de-
crease scores for incorrect items in isolation ? and
this is what is done in the DISCDYN models.
The analysis of various feature sets indicates
that the ASR/SLU error correlation (confusion)
features yield the largest improvement ? c.f. fea-
ture set bc compared to b in Table 3. The im-
provement is smallest for MISMATCH, which un-
derscores the challenges of mismatched train and
test conditions during a realistic runtime scenario.
Note, however, that we have constructed a highly
mismatched case where we train on DS1 (that sup-
ports just 8 routes) and test on DS2 (that supports
40 routes). Therefore, many route, origin and des-
tination slot values in the test data do not appear
in the training data. Hence, it is unsurprising that
the positive effect of confusion features would de-
crease.
While Table 3 shows performance measures av-
eraged across all turns, Table 4 breaks down per-
formance measures by slot, using the full feature
set bch and the realistic MISMATCH dataset. Re-
sults here show a large variation in performance
across the different slots. For the date and time
slots, there is an order of magnitude less data than
for the other slots; however performance for dates
is quite good, whereas times is rather poor. We
believe this is because the SLU confusion features
can be estimated well for slots with small cardinal-
ities (there are 7 possible values for the day), and
less well for slots with large cardinalities (there are
24 ? 60 = 1440 possible time values). This sug-
473
Accuracy (larger numbers better)
algorithms rout origin dest. date time
BASELINE 53.81 66.49 67.78 71.88 52.32
GENONLINE 50.02 54.11 59.05 75.78 35.02
GENOFFLINE 48.12 58.82 58.98 72.66 20.25
DISCFIXED 52.83 67.81 70.67 71.88 33.34
DISCDYN1 54.28 68.24 68.53 79.69 40.51
DISCDYN2 56.18 68.42 70.10 80.47 40.51
DISCDYN3 54.52 66.24 67.96 82.81 43.04
DISCIND 54.25 68.84 70.79 78.13 38.82
L2 metric (smaller numbers better)
algorithms route origin dest. date time
BASELINE 33.15 24.67 24.68 21.61 32.35
GENONLINE 35.50 35.10 31.13 19.86 52.58
GENOFFLINE 46.42 35.73 37.76 19.97 70.30
DISCFIXED 34.09 23.92 23.35 17.59 40.15
DISCDYN1 31.30 23.01 23.07 15.29 37.02
DISCDYN2 30.53 22.40 22.74 13.58 37.59
DISCDYN3 31.58 23.86 23.68 13.93 37.52
DISCIND 36.50 23.45 23.41 15.20 45.43
Table 4: Performance per slot on dataset MIS-
MATCH using the full feature set bch.
(a) MISMATCH dataset (b) MATCH2 dataset
Figure 4: Accuracy vs. amount of training data
gests that the amount of data required to estimate a
good model may depend on the cardinality of slot
values.
Finally, in Figure 4 we show how performance
varies with different amounts of training data for
the MATCH2 and MISMATCH datasets, where the
full training set size is approximately 5600 and
4400 turns, respectively. In both cases asymptotic
performance is reached after about 2000 turns, or
about 150 dialogs. This is particularly encour-
aging, as it suggests models could be learned or
adapted online with relatively little data, or could
even be individually tailored to particular users.
7 Conclusion and Future Work
Dialog state tracking is crucial to the successful
operation of spoken dialog systems. Recently de-
veloped statistical approaches are promising as
they fully utilize the dialog history, and can in-
corporate priors from past usage data. However,
existing methodologies are either limited in their
accuracy or their coverage, both of which hamper
performance.
In this paper, we have introduced a new model
for discriminative state tracking. The key idea is to
exploit the structure of the problem, in which each
dialog state hypothesis has features drawn from
the same set. In contrast to past approaches to dis-
criminative state tracking which required a num-
ber of parameters quadratic in the number of state
hypotheses, our approach uses a constant number
of parameters, invariant to the number of state hy-
potheses. This is a crucial property that enables
generalization and dealing with an unlimited num-
ber of hypotheses, overcoming a key limitation in
previous models.
We evaluated the proposed method and com-
pared it to existing generative and discrimina-
tive approaches on a corpus of real-world human-
computer dialogs chosen to include a mismatch
between training and test, as this will be found
in deployments. Results show that the proposed
model exceeds both the accuracy and probabil-
ity quality of all baselines when using the rich-
est feature set, which includes information about
common ASR confusions and dialog history. The
model can be trained efficiently, i.e. only about
150 training dialogs are necessary.
The next step is to incorporate this approach
into a deployed dialog system, and use the esti-
mated posterior over dialog states as input to the
action selection process. In future, we also hope
to explore unsupervised online adaptation, where
the trained model can be updated as test data is
processed.
Acknowledgments
We thank Patrick Nguyen for helpful discussions
regarding maximum entropy modeling and fea-
ture functions for handling structured and dynamic
output classification problems.
References
AT&T Statistical Dialog Toolkit. AT&T Statistical
Dialog Toolkit. http://www2.research.
att.com/sw/tools/asdt/, 2013.
Adam Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. A maximum entropy approach
to natural language processing. Computational
Linguistics, 22:39?71, 1996.
474
Alan W. Black, S. Burger, B. Langner, G. Par-
ent, and M. Eskenazi. Spoken dialog challenge
2010. In Proc. of Workshop on Spoken Lan-
guage Technologies (SLT), 2010.
Dan Bohus and Alex Rudnicky. A k hypothe-
ses + other belief updating model. In Proc.
of AAAI Workshop on Statistical and Empirical
Approaches to Spoken Dialog Systems, 2006.
Milica Gas?ic? and Steve Young. Effective handling
of dialogue state in the hidden information state
pomdp dialogue manager. ACM Transactions
on Speech and Language Processing, 7, 2011.
Eric Horvitz and Tim Paek. A computational ar-
chitecture for conversation. In Proc. of the 7th
Intl. Conf. on User Modeling, 1999.
Allan H Murphy. A new vector partition of the
probability score. Journal of Applied Meteorol-
ogy, 12:595?600, 1973.
Blaise Thomson and Steve Young. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588, 2010.
Jason D. Williams. Incremental partition recombi-
nation for efficient tracking of multiple dialogue
states. In Proc. of ICASSP, 2010.
Jason D. Williams. Challenges and opportuni-
ties for state tracking in statistical spoken dialog
systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Pro-
cessing, Special Issue on Advances in Spoken
Dialogue Systems and Mobile Interface, 6(8):
959?970, 2012.
Jason D. Williams and Suhrid Balakrishnan. Esti-
mating probability of correctness for asr n-best
lists. In Proc. SigDial Conference, 2009.
Jason D. Williams and Steve Young. Partially ob-
servable markov decision processes for spoken
dialog systems. Computer Speech and Lan-
guage, 21:393?422, 2007.
Jason D. Williams, Iker Arizmendi, and Alistair
Conkie. Demonstration of AT&T Let?s Go: A
production-grade statistical spoken dialog sys-
tem. In Proc of Workshop on Spoken Language
Technologies (SLT), 2010.
Steve Young, Milica Gas?ic?, Simon Keizer,
Franc?ois Mairesse, Jost Schatzmann, Blaise
Thomson, and Kai Yu. The hidden informa-
tion state model: a practical framework for
POMDP-based spoken dialogue management.
Computer Speech and Language, 24(2):150?
174, 2010.
Bianca Zadrozny and Charles Elkan. Transform-
ing classifier scores into accurate multiclass
probability estimates. In Proc. of the eighth
ACM SIGKDD Intl. Conf on Knowledge Dis-
covery and Data mining, pages 694?699, 2002.
475
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 98?109,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Multiparty Turn Taking in Situated Dialog: 
Study, Lessons, and Directions 
 
 
Dan Bohus Eric Horvitz 
Microsoft Research Microsoft Research 
One Microsoft Way One Microsoft Way 
Redmond, WA, 98052 Redmond, WA, 98052 
dbohus@microsoft.com horvitz@microsoft.com 
 
 
 
 
 
 
Abstract 
We report on an empirical study of a multiparty 
turn-taking model for physically situated spo-
ken dialog systems. We present subjective and 
objective performance measures that show how 
the model, supported with a basic set of sensory 
competencies and turn-taking policies, can en-
able interactions with multiple participants in a 
collaborative task setting. The analysis brings 
to the fore several phenomena and frames chal-
lenges for managing multiparty turn taking in 
physically situated interaction.  
1. Introduction 
Effective dialog relies on the coordination of con-
tributions by participants in a conversation via turn 
taking. The complexity of understanding and man-
aging turns grows significantly in moving from 
dyadic to multiparty settings, including situations 
where groups of people converse as they collabo-
rate on shared goals. We are exploring computa-
tional methods that can endow dialog systems with 
the ability to participate in a natural, fluid manner 
in conversations involving several people.  
In Bohus and Horvitz (2010a), we presented a 
computational model for managing multiparty turn 
taking. The model harnesses multisensory percep-
tion and reasoning and includes a set of compo-
nents and representations.  These include methods 
for tracking multiparty conversational dynamics, 
for making turn-taking decisions, and for rendering 
decisions about turns into an appropriate set of 
low-level, coordinated gaze, gesture and speech 
behaviors. We implemented the model and have 
been testing it in several domains. The investiga-
tions have been aimed at characterizing the sys-
tem?s performance in complex multiparty settings.  
In Bohus and Horvitz (2010b), we examine data 
collected during a user study to evaluate the ability 
of the system to shape the flow of multiparty con-
versational dynamics.  In this paper, we focus our 
attention on the performance of the inference and 
decision-making models.  We analyze the accuracy 
of current turn-taking inferences, the influence of 
inference errors on decisions, and the overall effec-
tiveness of the system?s decision making. We re-
port on subjective and objective measures of the 
system?s turn-taking performance. We find that the 
turn-taking methodology enables our system to 
successfully participate in multiparty interactions, 
even when relying on relatively coarse models for 
inference and decision making.  The analysis high-
lights several general phenomena including stand-
ing bottlenecks and difficulties, and opportunities 
for enhancing multiparty turn taking in dialog sys-
tems. Based on the results, we discuss challenges 
and directions for research on turn taking in physi-
cally situated dialog.   
2. Related Work 
We begin by placing this work within the larger 
context of research on multiparty interaction and 
turn taking. In a seminal paper on turn taking in 
natural conversations, Sacks, Schegloff and Jeffer-
son (1974) proposed a basic model for the organi-
98
zation of turns in conversation. The model is cen-
tered on the notion of turn-constructional-units, 
separated by transition relevance places that pro-
vide opportunities for speaker changes. In later 
work, Schegloff (2000) elaborates on several as-
pects of this model, including interruptions and 
overlap resolution devices. Other researchers in 
conversational analysis and psycho-linguistics 
have highlighted the important role played by gaze, 
gesture, and other non-verbal communication 
channels in regulating turn taking. For instance, 
Duncan (1972) discusses the role of non-verbal 
signals, and proposes that turn taking is mediated 
via a set of verbal and non-verbal cues. Wiemann 
and Knapp (1975) survey prior investigations on 
turn-taking cues in several conversational settings, 
in an effort to elucidate differences. Goodwin 
(1980) discusses various aspects of the relationship 
between turn taking and attention. More recently, 
Hjalmarsson (2011) investigates the additive effect 
turn-taking cues have on listeners in both human 
and synthetic voices.  
 
Figure 1. Components of turn-taking model. 
S(s) A(s) 
Sensing 
CONTRIB 
Decisions 
Behavioral Control 
Behaviors and  
Output Management 
System 
Floor 
Action 
D
ia
lo
g 
m
an
ag
em
en
t 
Contribute Audio-visual 
evidence 
Dialog 
Context 
Speech 
Gaze 
Gesture 
Semantic 
Input 
Semantic 
Output 
FS(p) FI(p) 
FA(p) 
? 
Tu
rn
 m
an
ag
em
en
t 
Within the dialog systems community, efforts 
have been made on designing and implementing 
computational models for managing turn taking 
(e.g., Traum, 1994; Thorriss?n, 2002; Raux and 
Eskenazi, 2009; Selfridge and Heeman, 2010). 
Moving beyond the dyadic setting, Traum and 
Rickel (2002) describe a turn management compo-
nent for supporting dialog between a trainee and 
multiple virtual humans. Kronlid (2006) describes 
a Harel state-chart implementation of the original 
SSJ model. Researchers studying human-robot in-
teraction have developed prototype robots that can 
interact with multiple human participants (e.g. Ma-
tsusaka et al, 2001; Bennewitz et al, 2005). In our 
previous work Bohus and Horvitz (2009; 2010a; 
2010b), we describe a platform that leverages mul-
timodal perception and reasoning to support multi-
party dialog in open-world settings. 
3. Multiparty Turn-Taking Model 
We engaged in a set of experiments to probe the 
inference and decision making competencies of a 
computational model for multiparty turn taking 
(Bohus and Horvitz 2010a; 2010b). To set the 
stage for the analysis to follow, we briefly review 
the proposed approach. 
We model turn taking as an interactive, collabo-
rative process by which participants in a conversa-
tion monitor one another and take coordinated ac-
tions to ensure that (generally) only one person 
speaks at a given time. The participant ratified to 
speak via this process is said to have the floor. 
Each participant engaged in the interaction con-
tinuously produces (i.e. at every time tick) one of 
four floor management actions: a hold action indi-
cates that a participant is maintaining the floor; a 
release action indicates that the participant is 
yielding the floor to a set of other participants 
(which could be void, allowing for self-selection 
next turn allocation); a take action indicates that 
the participant is trying to acquire the floor; finally, 
a null action indicates that a participant is not mak-
ing any floor claims. The floor shifts from one par-
ticipant to another as the result of the joint, coop-
erative floor management actions taken by the par-
ticipants. Specifically, a release action must be met 
with a take action for a floor shift to occur; in all 
other cases the floor stays with the participant that 
currently holds it.  
Figure 1 illustrates the main components and 
key abstractions in the model. The sensing sub-
component tracks the conversational dynamics, 
and includes models for detecting spoken signals s, 
inferring the source S(s) and the set of addressees 
A(s) for each signal, as well as the floor state 
FS(p), actions FA(p) and intentions FI(p) of each 
participant p engaged in a conversation. This in-
formation is used in conjunction with higher-level 
dialog context to decide when the system should 
generate new contributions and which floor action 
should be produced at each point in time. Finally, 
floor actions are rendered by a behavioral compo-
nent into a set of coordinated gaze, gesture and 
speech behaviors. By harnessing these different 
components, the proposed model can enable an 
99
embodied conversational agent to handle a broad 
spectrum of turn-taking phenomena.  
Figure 2.  Questions game: screen and kiosk. 
4. User Study 
We implemented an initial set of turn-taking infer-
ence and decision making models in the context of 
a multiparty dialog system, and we conducted a 
large-scale multiparty interaction user study with 
this system. The study, described in more detail 
below, was designed to fulfill two goals: (1) to as-
certain an initial performance baseline and identify 
current bottlenecks and challenges to be addressed 
moving forward, and (2)  to collect a large set of 
multiparty human-computer dialog data that can be 
used to study and improve multiparty turn taking in 
dialog systems.  
4.1. System 
The platform used in these experiments, described 
in detail in Bohus and Horvitz (2009), takes the 
form of a multimodal interactive kiosk that dis-
plays an avatar head which plays a questions game 
with multiple participants. The system leverages 
audiovisual information and employs components 
for visually tracking multiple people in the scene, 
sound source localization, speech recognition, 
conversational scene analysis, behavioral control 
and dialog management. Figure 2 shows a screen 
generated by the system, with the rendered avatar 
and a sample challenge question. Users can col-
laborate on selecting an answer, and, after a con-
firmation, the system provides an explanation if the 
answer is incorrect, before moving on to the next 
question. Sample interactions are found in Appen-
dix C and videos are available online (Situated In-
teraction, 2011). 
4.2. Turn-Taking Inference and Decisions 
In the current system, a voice activity detector is 
used to identify and segment spoken utterances. 
The source of each utterance is assumed to be the 
participant who is closest in the horizontal plane to 
the sound direction identified by the microphone 
array. The set of addressees is identified by fusing 
information probabilistically about the focus of 
attention of the source, as obtained through face 
detection and head pose tracking, while the utter-
ance is being detected. In addition, the system as-
sumes that non-understandings are addressed to 
other engaged participants, since initial tests indi-
cated that in this domain about 80% of utterances 
that led to non-understandings were in fact ad-
dressed to others. Similarly, the system assumes 
that utterances longer than three seconds are ad-
dressed to others (responses addressed to the sys-
tem tend to be short in this domain) 
Floor management actions are inferred as fol-
lows. If a participant has the floor, we assume they 
are performing a hold action if speaking and a re-
lease action otherwise. The release is assumed to 
be towards the addressees of the last spoken utter-
ance. Although the latter assumption on releases 
may not hold in the most general case, it is a rea-
sonable one for the questions game domain. If a 
participant does not have the floor, the system as-
sumes they perform a take action if speaking or a 
null action otherwise. The system also assumes 
that the floor intentions are fully reflected by the 
floor actions, i.e., a participant intends to have the 
floor if and only if she performs a hold or take ac-
tion. Floor states are updated based on the joint, 
coordinated floor actions of all participants, as de-
scribed earlier.  
Turn-taking decisions are based on a simple 
heuristic policy. The system takes the floor if (1) 
the floor is being released to it or (2) a participant 
releases the floor to someone else, but no one 
claims the floor for a preset duration. In most cas-
es, this duration is set to 3.5 seconds. However, if 
the floor is released to someone else after the sys-
tem is interrupted during a question dialog act, the 
system will try to quickly reacquire the floor 
should no one else be speaking, so as to finish or 
restate its question. The waiting duration is set in 
the latter case to 500 milliseconds. If after 500ms, 
when the system tries to take the floor another con-
flict occurs (followed by a floor release to someone 
else), the waiting duration is increased again to 3.5 
seconds. Finally, if a third consecutive conflict oc-
100
curs when the system tries to acquire the floor, the 
waiting duration is set to a longer, 20 seconds. 
The system releases the floor at the end of its 
own outputs. In addition, it has to decide whether it 
should release the floor when a user performs a 
take action (i.e. barges in) while the system is 
speaking. The heuristic policy currently imple-
mented by the system releases the floor only for 
barge-ins occurring during question dialog acts. 
Finally, the behavioral models employ policies 
informed by the existing literature on the role of 
gaze in regulating turn taking. In particular, the 
system?s gaze is directed towards the speaking par-
ticipant, or, if the system is speaking, towards the 
addressees of the system?s utterance. During si-
lences, the system?s gaze is directed towards the 
participants that the floor is being released to. 
The models and policies described above repre-
sent a starting point for inference and action, con-
structed to enable data collection and an initial 
evaluation in this domain. We are working to up-
date the turn-taking architecture with more sophis-
ticated evidential reasoning and utility-theoretic 
decision making.  Nevertheless, when harnessed as 
an ensemble within the turn-taking approach that 
we have described, the current procedures provide 
for an array of complex, multiparty turn-taking 
behaviors. For instance, the system can address 
each participant individually or all participants as a 
group via controlling the orientation of its head 
pose. When participants talk amongst themselves, 
the system can monitor their exchanges and wait 
until the floor is being released back to it. If an 
answer is heard during such a side conversation 
(e.g., one participant suggests an answer to an-
other), the system highlights it on the screen (see 
Figure 2). If a significant pause is detected during 
this side conversation, the avatar takes the floor 
and the initiative, e.g., ?So, what do you think is 
the correct answer?? Once a participant provides 
an answer, the system seeks confirmation from 
another participant before moving on. In some 
cases, the avatar passes back the floor and seeks 
confirmation non-verbally, by simply turning to-
wards another participant and raising its eyebrows. 
The system can try to require the floor immediately 
after being interrupted, but can also back off, giv-
ing the participants a chance to finish a side con-
versation, if successive floor conflicts occur. Sam-
ple interactions can be viewed in Appendix C and 
online (Situated Interaction, 2011). 
4.3. Study Design 
The user study was conducted in a usability lab 
and involved a total of 60 participants recruited as 
pairs of people from the general population who 
previously knew one another (30 male and 30 fe-
male, with ages between 18 and 61). The study 
was structured in 15 one-hour sessions, with each 
session involving four participants, i.e., two pairs 
of two previously acquainted participants. In each 
session, we formed all possible subgroups of size 
two (6 subgroups) and of size three (4 subgroups) 
with the four participants. Each subgroup played 
one game with the system. This setup allowed us to 
collect a large set of multiparty interactions under 
diverse conditions (e.g., all-male, all-female, 
mixed-gender groups; groups where people were 
previously acquainted vs. not, etc.). At the end of 
each session, participants filled in a subjective as-
sessment survey. 
4.4.  Corpus, Annotations, and Cost Assessment 
In total, 150 multiparty interactions were collected: 
90 with two participants and the system, and 60 
with three participants and the system. A profes-
sional annotator transcribed the utterances detected 
by the system at runtime, and labeled them with 
source and addressee information. 
    The system was noted to commit several types 
of turn-taking errors. To expand the error analysis 
beyond occurrence statistics and to characterize the 
impact of various types of errors, we conducted a 
follow-up study.  In this second study, a set of ad-
ditional participants were recruited to review vid-
eos of interactions from the first study and asked to 
(1) identify the turn-taking errors committed by the 
system and (2) to assess the costliness of the error 
on a five-point scale.  
A total of 9 interactions (5 with two participants 
and system; 4 with three participants and system) 
were randomly sampled from the collected corpus, 
while ensuring that each turn-taking outcome of 
interest (discussed in Section 5 and summarized in 
Table 1) was sufficiently represented. Nine partici-
pants were recruited via an email request to em-
ployees at our organization.  Each participant re-
viewed three interactions, and each interaction was 
reviewed by three different participants. Prior to 
the experiment, each of the annotators received a 
brief review of the turn-taking process in human-
human interaction. Next, they used a multimodal 
101
annotation tool that we created to review the inter-
action videos. As each video played, the annotator 
pushed a button at each point they believed that the 
system had committed a turn-taking error. In a se-
cond pass, each annotator was asked to review the 
errors that they had previously identified and to 
assess the relative cost of the error, on a scale from 
0 (?no error?) to 5 (?worst error?). In a final step, 
the authors manually aligned each identified turn-
taking error with a turn-taking decision made by 
the system and its corresponding outcome. 
5. Evaluation 
We now focus on the various types of turn-taking 
errors, the outcomes that these errors lead to, and 
the costs assessed for the outcomes.  We begin by 
focusing on diarization challenges described in 
Section 5.1.  In Sections 5.2 and 5.3, we review the 
accuracy of the system?s turn-taking inferences and 
decisions, and their corresponding outcomes.  Fi-
nally, in Section 5.4, we turn our attention to the 
subjective assessment results obtained via the post-
experiment user survey.  
Before diving into the details, we note that we 
eliminated 7 out of the total 150 interactions from 
the analysis due to significant problems with 
acoustic echo cancellation. In the remaining 143 
interactions, we also identified and eliminated 24 
utterances in the transitional engagement stages, 
e.g., when the users were not ready or properly 
setup in front of the system. The analysis below is 
based on the remaining 4379 utterances.  
5.1. Diarization 
The system uses a voice activity detector which 
leverages energy, acoustics and grammar to detect 
spoken utterances. Our experiments indicate that 
this type of black-box solution can make diariza-
tion errors, especially in multiparty settings where 
people may speak simultaneously, at a fast pace, 
and address each other with language outside the 
system?s grammar. Results show that only 72% of 
the detected segments contain speech from a single 
participant. Another 2% contain background noises 
incorrectly identified as speech. Most often these 
are instances where the system heard itself due to 
acoustic echo-cancellation problems; the ratio 
grows to about 6% among all utterances detected 
while the system is speaking. The remaining 26% 
contain overlapping or successive utterances from 
multiple speakers. Inspection of the data reveals 
that some utterances spoken softly by participants 
were not detected and that segmentation boundary 
errors are also sometimes present. While such er-
rors may be mitigated by inferences at higher lev-
els in the turn-taking model, they can significantly 
influence the system?s ability to track the conver-
sational dynamics and make appropriate turn-
taking decisions. We plan to pursue more robust 
audiovisual diarization methods that integrate 
sound localization as detected by a microphone 
array, along with higher-level interaction context.  
5.2. Take versus Null  
We now turn our attention to the system?s floor 
control decisions. The analysis below is based on 
the utterances and segmentation detected by the 
system at runtime. We note that a more precise 
analysis could be conducted with a ground truth 
segmentation of utterances. Utterances detected by 
the system can be classified into three categories, 
based on their relationship to system outputs, as 
shown in Figure 3: overlaps, which start and end 
during a system?s output, continuers, which begin 
during but finish after a system output has ended, 
and responses, which do not overlap anywhere.   
With the current policy, the system chooses 
whether it should take the floor following each 
detected continuer and response. The dataset con-
tains a total of 3265 such instances. The system?s 
decision at each of these points hinges on the re-
sults of its inferences about the participants? floor 
actions, and thus of inferences about the addressees 
of each utterance. Table 1 displays a tabulation of 
the release actions performed by the participants 
versus the actions identified by the system. The 
release actions are determined from labels assigned 
manually by the professional annotator. Recall that 
we make an assumption that the release is towards 
the set of addressees of an utterance. For segments 
that were labeled as containing multiple utterances, 
the release is made to the addressee of the last ut-
terance. The last row in Table 1 corresponds to 
background noises and system speech incorrectly 
Figure 3. Schematic of different classes of overlap.
turn-initial overlap (TIO) response continuer turn-internal overlap 
System speech 
Detected utterances 
Actual utterances 
102
Inferred Addressee / Release Action  
To System  Not to System 
 
2063 (64%) 
  
277 (9%) 
Take + Verbal Contribution 
1796 (87%) 
Delayed System Take 
59 (21%) 
To
 S
ys
te
m
 
 
Turn-initial 
overlap 
182 (10%) 
[17 Echo] 
 
 
0.25 No turn-initial 
overlap 
1614 (90%) 
 
 
0.00 
 
 
Take+ 
Non-verbal 
Release 
267 (13%) 
 
 
 
 
0.42 Turn-initial 
overlap  
22 (37%) 
[0 Echo] 
 
1.83 
 
 
No turn-initial 
overlap 
37 (63%) 
 
 
2.58 
 
 
Other 
Takes 
218 (79%) 
 
 
 
 
0.85 
 
305 (9%) 
  
588 (18%) 
Take + Verbal Contribution 
242 (79%) 
Delayed System Take 
131 (22%) 
La
be
le
d 
Ad
dr
es
se
e 
/ R
el
ea
se
 A
ct
io
n 
 
No
t t
o 
Sy
st
em
 
 
Turn-initial 
overlap 
101 (42%) 
[0 Echo] 
 
 
1.76 No turn-initial 
overlap 
141 (58%) 
 
 
0.42 
 
 
Take+ 
Non-verbal 
Release 
63 (21%) 
 
 
 
 
0.00 
Turn-initial 
overlap 
38 (29%) 
[3 Echo] 
 
0.55 
 
 
No turn-initial 
overlap 
93 (71%) 
 
 
0.00 
 
 
Other 
Takes 
457 (78%) 
 
 
 
 
0.03 
 
 
 
10 (<1%) 
  
22 (<1%) 
Take + Verbal Contribution 
9 (90%) 
Delayed System Take 
13 (59%) 
Ba
ck
gr
ou
nd
 
Turn-initial  
overlap 
3 (33%) 
[0 Echo] 
No turn-initial  
overlap 
6 (67%) 
Take+ 
Non-verbal  
Release 
1 (10%) 
Turn-initial  
overlap  
7 (54%) 
[4 Echo] 
No turn-initial  
overlap 
6 (46%) 
Other 
Takes 
9 (41%) 
Table 1. Decisions to take floor (vs. null), outcomes, and estimated costs (bar graph with confidence intervals).  
  Echo denotes cases where the turn initial overlap is created by utterances where the system hears itself because         
of errors with echo cancellation.  
     
  
     
    
    
identified as utterances.  
On the task of detecting addressees, and thus 
floor release actions, the results show an error rate 
of 18%, including 305 false-positives (erroneous 
detections) and 277 false-negatives (missed detec-
tions) of floor releases to the system. These errors 
influence the quality of turn taking in a variety of 
ways and underscore the need for more robust in-
ferences about speech source and target, and floor 
release actions. We believe that more sophisticated 
models learned from audiovisual information (e.g., 
prosody, head and body pose, etc.) and attributes 
of the interaction context (e.g., who spoke last, 
where is the system looking, etc.) can reduce errors 
significantly. 
Table 1 indicates that in 305 (9%) of the cases 
the system incorrectly inferred that the floor was 
being released to it. In 79% of these cases, the sys-
tem took the floor and produced a verbal contribu-
tion. Since the floor was not released to the system, 
such errors can lead to significant turn-taking prob-
lems, which often manifest as floor conflicts 
marked by turn-initial overlaps, where a partici-
pant and the system start speaking around the same 
time (see Figure 3). Operationally, we define turn-
initial overlaps as all detected overlaps with an 
actual onset of less than 300 milliseconds from the 
beginning of the system?s utterance (see discussion 
in Appendix A); the other overlaps are dubbed 
turn-internal. We note that the time at which an 
overlap is detected by the system lags behind the 
actual onset of the utterance by an average of about 
700 milliseconds, due to core latencies in our audio 
and speech processing pipeline. Accounting for 
these computational lags, and others arising at dif-
ferent places in processing pipelines, raise chal-
lenges for turn taking in spoken dialog systems. 
42% of the verbal takes performed incorrectly 
by the system led to turn-initial overlaps. This is 
not surprising, as the system starts speaking when 
the floor was not released to it. In some of these 
cases the same participant continues (e.g., diariza-
tion errors incorrectly segmented the utterance), or 
someone else starts speaking. The cost assessment 
experiment confirmed the impact of these errors ? 
the average estimated cost was 1.76. If no turn-
initial overlap occurred after the system incorrectly 
took the floor, the average cost was 0.42. Clearly 
103
floor conflicts come with a cost. The specific cost 
assessments we obtained are perhaps influenced to 
a degree by the role of game mediator played by 
the system. With this role, taking the floor in cases 
when the system was not addressed is perhaps not 
as costly as it might be in other domains.  
Note that 182 turn-initial overlaps also occur 
when the system takes the floor after correctly 
identifying that the floor was released to it (upper-
left quadrant in Table 1). 17 of them are created by 
the system hearing itself as it starts speaking, due 
to errors in acoustic echo cancellation; these in-
stances are marked Echo in Table 1. While the rel-
ative percentage of turn-initial overlaps is smaller 
after a floor release to the system (~10%), the ma-
jority of all turn-initial overlaps (shaded cells in 
Table 1) occur in this context, because of the larger 
incidence of the situation. Often, these utterances 
contain an immediate answer or a short confirma-
tion from another participant. The cost of these 
turn-initial overlaps is also much lower: 0.25 ver-
sus 1.76 (again, the cost structure is probably sen-
sitive to details of the domain). 
We believe the turn-initial overlaps that occur 
when the floor is released to the system can be ex-
plained in part by the interpretation of the system?s 
short delay in responding (per processing) as a sig-
nal that the system is not taking the floor, leading 
other participants to take initiative. As another fac-
tor, turn taking is a mixed-initiative process, and 
other participants might vie for the floor and issue 
their own contributions immediately after an an-
swer directed to the system. These observations 
bring to the fore two questions: (1) how can we 
minimize the number of turn-initial overlaps, and 
(2) how can the system gracefully handle such 
overlaps once they occur?  
One approach to minimizing turn-initial over-
laps is to reduce the system?s response delays via 
faster processing or via the use of predictive mod-
els to anticipate the end of turns (e.g. Ferrer et al, 
2003; Schlangen, 2006; Raux and Eskenazi, 2008; 
Skantze and Schlangen, 2009). Multiparty settings 
require methods for forecasting not only when a 
current speaker will finish, but also whether any 
participant will try to take (or release) the floor 
within a small window of time in the future, i.e., 
accurately modeling all floor intentions. Our turn-
taking framework includes components for repre-
senting and modeling floor intentions, but these are 
not used in the current system.  We believe there is 
promise in learning models to predict floor inten-
tions and the timing of ends of utterances from in-
teraction data. The availability of such predictions 
can fuel additional turn-taking strategies and also 
pave the way to more graceful handling of turn-
initial overlaps after they occur. For instance, if the 
system can anticipate that someone else might start 
speaking, it might still decide to take the floor but 
it might start with a filler, e.g., ?So [pause] What 
do you think?? constructing a natural opportunity 
for resolving a potential conflict after ?So? We 
plan to investigate the use of decision-theoretic 
methods to anticipate and resolve such conflicts by 
introducing and modulating an array of strategies, 
including the use of fillers, restarts, and acknowl-
edgment gestures.  
In 21% of the 305 incorrectly detected floor re-
leases to the system, our system immediately per-
formed a non-verbal floor release to another par-
ticipant by turning the avatar?s face towards them 
and raising its eyebrows (Take + Non-verbal Release in 
Table 1). These situations are not costly, as the 
system?s action does not interrupt the flow of the 
conversation. Indeed they were never penalized in 
the cost assessment experiment that we conducted. 
However, the same action, performed when the 
floor is actually released to the system (13% of 
2063 cases), has the potential to create problems if 
not properly recognized by the targeted participant 
as a floor release by the system; the average cost 
assessed in this case was 0.42. 
The right-hand column in Table 1 shows cases 
where the system detected that the floor was not 
released to it. In these cases, the system waits (per-
forms null) for a specified duration. The cost as-
sessment indicates that waiting in this situation is 
overall costly, and the cost depends on the ultimate 
outcome. If no one else takes the floor, the system 
will eventually do so (Delayed System Take cases in 
Table 1). In some of these cases, turn-initial over-
laps also occur. The 277 cases in which the system 
fails to detect that the floor was in fact released to 
it lead to no immediate response from the system. 
In these cases the system can be perceived as unre-
sponsive and the participants eventually repeat 
themselves. We believe that performance can be 
improved with the use of an ongoing decision-
theoretic analysis that continuously reassesses the 
situation while the system waits.  Such an analysis 
would consider the delay, floor holder?s previous 
actions, inferences about participants? floor inten-
104
tions, and cost-benefit tradeoffs of different floor 
actions. 
5.3. Release versus Hold 
We now turn our attention to the system?s deci-
sions to release the floor. Recall that, according to 
the current policy, the system performs a floor hold 
while it is speaking and a floor release at the end of 
its outputs. In addition, if an overlap (i.e., barge-in) 
was detected during question dialog acts, the sys-
tem performed a floor release immediately, inter-
rupting its own output and allowing for the user 
barge-in.  
Since such barge-ins were allowed only during 
the question dialog acts, as Table 2 shows, the cur-
rent policy leads to an abundance of cases in which 
the system performs hold when an overlap is de-
tected. Some of these cases are continuers: the 
overlap only happens at the very end of the sys-
tem?s output. These cases do not create significant 
turn-taking problems, as the floor still transitions to 
the participant relatively quickly (the system re-
leases at the end of its output). However, in a sig-
nificant number of cases the system appears to ig-
nore the participants (shaded cells in Table 2). 
About three quarters of these overlaps occur while 
the system is providing an explanation after an in-
correct answer. Observations of the data indicate 
that in these cases participants may discuss or give 
their opinion on the answer or some aspect of the 
system?s explanation, while ignoring the system as 
it blindly continues the explanation.  
We have separated in Table 2 turn-initial from 
turn-internal overlaps. The two types of overlaps 
reflect different phenomena. As we have discussed, 
turn-initial overlaps mark floor conflicts, and vari-
ous strategies could be used to negotiate such con-
flicts (e.g., Yang and Heeman, 2010). In contrast, 
turn-internal overlaps may reflect efforts by other 
participants to take the floor, or might simply be 
backchannels, laughter, exclamations or other lexi-
cal or non-lexical events that do not mark a claim 
for the floor. Making appropriate floor control de-
cisions in this case will require models for reliably 
distinguishing between the two, i.e., between the 
take or null floor actions of the participants. This is 
an especially challenging inference problem as 
decisions need to be made as early as possible after 
the onset of an utterance.  
We note the relatively large incidence of failures 
in echo cancellation in our microphone array. On 
the utterances marked Echo in Table 2, the system 
heard itself and thought a user was speaking. We 
believe these failures could be significantly re-
duced with better acoustic echo cancellation.  
5.4. Subjective Assessment  
Finally, we present results from a subjective as-
sessment of the system by participants, based on a 
post-experiment survey. The survey included sev-
eral 7-point Likert scale questions related to turn 
taking, which are displayed in Figure 4, together 
with the mean user responses and the correspond-
ing 95% confidence intervals. Generally, partici-
pants rated the system?s turn-taking abilities fa-
vorably, with scores around 4.5-5. No statistically 
significant differences were detected in assess-
ments across the participant?s gender or previous 
familiarity with speech recognition systems. We 
also note that a parallel human?human interaction 
study would help us characterize better the sys-
tem?s performance relative to human dialog.  
I knew when the avatar
was addressing me
I knew when the avatar
was addressing others
I knew whom the avatar
was talking to
I knew when it was 
my time to speak
The avatar knew when 
I was speaking to it
The avatar knew when 
I was speaking to others
The avatar knew when 
it was its time to speak
The avatar interrupted 
us at the wrong time
The avatar waited too 
long before taking its turn 
I felt left out or excluded 
during the games
The interaction 
was natural 
I enjoyed playing 
the game
Figure 4. Results of subjective assessments. 
5.1 
5.0 
4.8 
4.6 
4.9 
4.8 
4.0 
3.2 
3.0 
2.0 
4.0 
5.5 
[ lower is better ]
[ lower is better ] 
 [ lower is better ]
1 - Never 7 - Always 2 3 4 5 6 
Action performed by system when overlap detected  
HOLD RELEASE 
315 (23%) 
Tu
rn
 
In
iti
al
 
Overlap 
285 (90%) 
[14 Echo] 
Continuer 
30 (10%) 
[3 Echo] 
43 (3%) 
[7 Echo] 
968 (69%) 
O
ve
rla
p 
Ty
pe
 
Tu
rn
  
In
te
rn
al
 
Overlap 
828 (86%) 
[44 Echo] 
Continuer 
140 (14%) 
[7 Echo] 
73 (5%) 
[13 Echo] 
Table 2. Decisions to release floor (vs. hold).
105
In addition to the survey questions, participants 
were invited to describe in their own words what 
they liked best and the first thing they would 
change about the system. 21 of the 60 participants 
mentioned aspects of multiparty interaction in the 
?what I liked best? category, such as the system?s 
ability to track the speaking participant and address 
people individually. Other frequent answers to this 
question called out the overall experience with the 
integrative intelligence of the system (15 answers), 
the fun/educational nature of the game (14), and 
aspects of speech recognition (11). On the ?first 
thing you would change,? the majority of answers 
(32) included references to shortcomings in render-
ing the avatar, while 13 answers included refer-
ences to problematic aspects of the multiparty turn 
taking. Other answers included task domain sug-
gestions (6) and comments about improving the 
speech recognition (5). A sampling of answers is 
presented in Appendix B.   
6. Summary and Future Work 
We reported on a user study of a multiparty turn-
taking model. Objective measures of system per-
formance and subjective assessments by partici-
pants indicate that the approach can enable suc-
cessful multiparty turn taking in the questions 
game domain. When the correct turn-taking deci-
sions are made, the multiparty interaction is seam-
less and resembles human-human collaboration. 
The conversations exhibit fluid exchanges among 
people and the system, including mixed-initiative, 
multiparty floor control, fluid back offs and re-
starts, natural use of non-verbal cues, such as par-
ticipants? utterances being triggered by a turn of 
the avatar?s head or a lift of the eyebrows. In con-
trast, turn-taking failures lead to a striking loss of 
fluidity and a qualitative jump out of an engaged 
process, where the system rapidly shifts from a 
collaborating participant into a distant and uncoor-
dinated appliance.  
The results we have discussed are based on an 
initial set of coarse perceptual and decision-making 
models and thus reflect an initial baseline; there is 
significant room for improvements. A careful dis-
section of the outcomes demonstrates the subtleties 
of multiparty turn taking and highlights several 
directions we plan to address in future work. First, 
our experiments have highlighted the importance 
of accurate diarization in multiparty dialog set-
tings. Minimizing errors requires rich perceptual 
and inferential competencies, leveraging audiovis-
ual evidence, general patterns of human discourse, 
and attributes of the task-specific goals and con-
text. We plan to explore the use of machine learn-
ing procedures for constructing predictive models 
that harness richer streams of evidence to identify 
and segment utterances, and to make inferences 
about their sources and targets, and the floor state, 
actions and intentions of all participants. Better 
turn-taking decisions can also be supported by in-
ferences about social norms, roles and dynamics, 
pace of interaction, and engagement.  
Although handcrafted turn-taking policies went 
a long way in this domain, enabling more general 
multiparty turn taking will require continuous in-
ference and decision making under uncertainty that 
considers subtleties of intention and timing, and 
that takes into consideration tradeoffs associated 
with different courses of actions. We foresee the 
value of extending the current decision models 
with richer temporal reasoning for performing such 
ongoing analyses. Challenges include a more in-
depth understanding of the cost of different types 
of turn-taking errors; the development of a wider 
array of graded strategies and behaviors for taking, 
releasing, or holding the floor, and for gracefully 
negotiating floor conflicts; and finally, the ability 
to reason about uncertainty in the world as well as 
in the system?s own processing delays in order to 
resolve tradeoffs between taking timely action and 
delaying for additional evidence that promises to 
enhance the accuracies of decisions. 
Much also remains to be done with the corre-
sponding generation of subtle verbal and non-
verbal cues for enhanced signaling and naturalness 
of conversation, including the use of fillers, re-
starts, backchannels, and envelope feedback. We 
are excited about tackling these and other chal-
lenges on the path to fielding systems that can en-
gage in fluid multiparty dialog.  
Acknowledgments 
We thank Anne Loomis Thompson, Ece Kamar, 
Qin Cai, Cha Zhang, and Zicheng Liu for their 
contributions. We also thank our colleagues who 
participated in pilot experiments for the user study. 
106
References 
Bennewitz, M., Faber, F., Joho, D., Schreiber, M., and 
Behnke, S., 2005. Integrating vision and speech for 
Conversations with Multiple Persons, in Proc. of 
IROS?05 
Bohus, D., and Horvitz, E., 2009. Dialog in the Open-
World: Platform and Applications, in Proc ICMI?09. 
Bohus, D., and Horvitz, E., 2010a. Computational 
Models for Multiparty Turn Taking, Microsoft 
Research Technical Report MSR-TR 2010-115. 
Bohus, D., and Horvitz, E., 2010b. Facilitating 
Multiparty Dialog with Gaze, Gesture and Speech, in 
Proc ICMI?10. 
Duncan, S. 1972. Some Signals and Rules for Taking 
Speaking Turns in Conversation, Journal of 
Personality and Social Psychology 23, 283-292. 
Ferrer, L., Shriberg, E., and Stolcke, A. 2003. A 
Prosody-Based Approach to End-Of-Utterance 
Detection That Does Not Require Speech 
Recognition, in Proc. ICASSP?03. 
Goodwin, C. 1980. Restarts, pauses and the 
achievement of mutual gaze at turn-beginning, 
Sociological Inquiry, 50(3-4). 
Hjalmarsson, A., 2011. The additive effect of turn-
taking cues in human and synthetic voice, in Speech 
Communication, vol. 53, issue 1. 
Kronlid, F., 2006. Turn Taking for Artificial Conversa-
tional Agents, in Cooperative Information Agents X, 
LNAI 4149, Springer-Verlag 
Matsusaka, Y., Fujie, S., and Kobayashi, T., 2001. 
Modeling of conversational strategy for the robot 
participating in the group conversation, in Proc of 
EuroSpeech?01. 
Raux, A., and Eskenazi, M. 2008. Optimizing endp-
ointing thresholds using dialogue features in a spoken 
dialogue system, in Proc of SIGdial-2008. 
Raux, A. and Eskenazi, M., 2009. A Finite-State Turn-
Taking Model for Spoken Dialog Systems, in Proc. 
HLT?09. 
Sacks, H., Schegloff. E., and Jefferson, G. 1974. A 
simplest systematics for the organization of turn-
taking in conversation, Language, 50, 696-735.  
Schegloff, E. 2000. Overlapping talk and the 
organization of turn-taking in conversation, 
Language in Society, 29, 1-63. 
Schlangen, D., 2006. From reaction to prediction: 
Experiments with computational models of turn-
taking, in Proc. Interspeech?06, Panel on Prosody of 
Dialogue Acts and Turn-Taking  
Selfridge, E., and Heeman, P., 2010. Importance-Driven 
Turn-Bidding for Spoken Dialogue Systems, in Proc. 
of ACL-2010, Uppsala, Sweden 
Skantze, G., and Schlangen, D., 2009. Incremental 
dialogue processing in a micro-domain, in Proc. of 
EACL-2009.  
Situated Interaction, 2011. Project web page:  
http://research.microsoft.com/~dbohus/si.html 
Thorisson, K.R. 2002. Natural Turn-Taking Needs No 
Manual: Computational Theory and Model, from 
Perceptions to Action, Multimodality in Language 
and Speech Systems, Kluwer Academic Publishers. 
Traum, D., 1994. A Computational Theory of Ground-
ing in Natural Language Conversation, TR-545, U. 
of Rochester. 
Traum, D., and Rickel, J., 2002. Embodied Agents for 
Multi-party Dialogue in Immersive Virtual World, in 
Proc. AAMAS?02.  
Wiemann, J., and Knapp, M., 1975. Turn-taking in 
conversation, Journal of Communication, 25, 75-92. 
Yang, F., and Heeman, P., 2010. Initiative Conflicts in 
Task-Oriented Dialogue, in Computer, Speech and 
Language, vol. 24, issue 2.  
107
Appendix A. Details on derivation of operational definition of turn-initial overlaps.  
 
As described in Section 5.2, we operationally define turn-initial overlaps as 
detected user utterances that have an actual onset of less than 0.3 seconds from 
the beginning of a system utterance. Figure 5 shows the histogram of the onset 
time for user speech with respect to system utterances (start of system utter-
ance is at 0 seconds), for overlapping utterances, where this onset is between -
2 and +5 seconds. If multiple user utterances overlap with a single system 
utterance, only the first user utterance, i.e. the first overlap, is considered in 
computing this histogram. As Figure 5 shows, the onset distribution has a bi-
modal character. We believe that the two modes may reflect two different 
phenomena in terms of the floor transition. The early-onset mode corresponds 
to situations in which a user starts to speak right around (before or immedi-
ately after) the time the system also started speaking; this indicates a situation 
where there is contention for the floor and the system cannot assume it has 
successfully acquired the floor. In contrast, user utterances starting at later 
times represent cases where the floor did first transition to the system and the 
user is aware of this transition.  In producing an utterance the user is attempt-
ing to barge-in and take the floor back from the system (unless the user utter-
ance is a backchannel). The threshold of 0.3 seconds on the onset for turn-
initial overlaps was selected based on the shape of this distribution.  
-2 -1 0 1 2 3 4 5
0%
2%
4%
6%
8%
10%
12%
onset (seconds) 
Figure 5. Histogram of onsets for first  
overlaps. 
 
Appendix B. Sample responses from survey 
 
Category # Example comment 
Please describe what you liked best about interacting with the system 
Multiparty 
interaction 
prowess 
21 
- I enjoyed how it recognized who was speaking and actually looked at you 
- I liked how the avatar tracked the players; how it understood speech 
- It was great to play a game where you don?t have to use your hands, just your mind. The way the avatar would recognize 
position of who spoke was nice. The blinking action at the avatar made her more realistic but she needed more than her face.  
- That it would look right at you and ask a question 
- I liked how the avatar made eye contact with each person playing the game?
Overall  
experience 
with system 
15 
- It was very new and thus it was fun. I don?t play computer games often and I did enjoy this one. Which is rare for me.  
- It was different than any other trivia game I?ve played in the past 
- I think this is a great way for a human to interact with a computer?
- It?s cool interacting with the avatar?
Rewarding 
task 14 
- I liked the challenge of the questions 
- It?s a great fun way to improve knowledge 
- New experience that I found enjoyable. I enjoyed thinking about choices and having an interaction with the avatar 
Speech and 
language 11 
- Voice recognition was fairly accurate, no need to repeat 
- The ability of it to understand what I was saying. Plus it?s pretty cool.  
- I liked it because it wasn?t really hard for the system to understand what we were saying. Even though we have an accent. 
If there was one thing you could change about this system, what would it be? 
Avatar  
rendering 32 
- The avatar should be more friendly ? she came off a bit austere ? she didn?t smile even when we got 5 out of 6 questions 
right, it was only ?pretty good?.  
- The way it moves its lips needs to be better 
- The avatar seemed a little to ?stiff?. It needs to be more natural in movement and speech 
- The face was a ?warmer face?. Smiling perhaps. 
Multiparty 
failures 13 
- Extend the time limit when questions haven?t been fully answered. It would sometimes say we were correct or false before we 
had confirmed our answer 
- Sometimes it skips and pauses and making it difficult to understand 
- Consistency in waiting and asking player to confirm answer instead of overhearing conversations and choosing an answer 
itself 
Task domain  6 - It would be cool if it could remember our names. Also, 6 questions was a little short. I think 8 or 10 questions would be better.  - I think the questions should be more pop culture related 
Speech and 
language 5 
- I enjoyed her. I would like her to understand a little easier. We had to repeat answers on occasion which wasn?t too bad. 
Overall I really liked it. Perhaps it could ask our names and call us by name when speaking to us 
 
 
 
 
 
 
 
 
 
 
108
Appendix C. Excerpts from interactions with the system. We present and discuss two segments from an interaction 
with the questions game system. The segments illustrate challenges for diarization, tracking conversational dynamics (e.g. infer-
ring speech source, target, floor actions, etc.) and decisions making for multiparty turn taking. The video for this entire interac-
tion, as well as an additional interaction are available online at (Situated Interaction, 2011) 
 
1 S?P1 Hi. Would you like to play a questions 
game? 
 
1 S?P16 In the USA hospitals have a red 
cross, what sign do they use in the 
Arab World? 
This segment starts with the system asking a question 
2 P16?P16 Hospitals have a red cross what sign 
do they user in the Arab World? 
3 P17?S Solid blue circle 
4 S?P16 Is that correct? 
5 P17?S No 
6 S?P16,P17 So what?s the correct answer? 
7 P17?P16 Red crescent moon 
8 P17?S Red crescent moon 
9 S?P16 [turns to P16, lifts eyebrows] 
10 P16?S Yes 
11 S?P16,P17 That?s right. Okay, so far you?re two 
out of three [?] 
P16 echoes back the question to himself in a low voice. The system detects this utterance, cor-
rectly inferred that the floor is not being released to it (the utterance leads to a non-
understanding), and the system turns towards the other participant, which produces ?Solid blue 
circle? (3). The utterance and corresponding floor release to the system are correctly understood, 
and the system turns back to P16 for confirmation. Next, the system detects an utterance from P17, 
who has changed her mind (5). The system takes the floor and asks both participants (by directing 
gaze towards them) what the correct answer is (6), and releases the floor to both participants o. 
P17 speaks first towards P16 (7) ? this utterance is low energy and not detected by the system, and 
then towards the system (8). The system recognizes the floor is released to it after (8), takes the 
floor and directs it back towards P16 non-verbally, by turning towards him and raising eyebrows. 
P16 responds immediately, and the system takes the floor and continues to the next question. 
   
17 S?P16,P17 Next question [?] The system then moves on to the next question 
2 P17?S Hi
3 P16?S Hello
4 P17?S Yes 
5 P18?S Yes 
6 S?P16 Here is the first question. The study of 
the size and shape of the skull as an 
indicator of a person?s character is 
known as: telepathy, graphology, or 
phrenology? 
7 P16?S I?ll go? Graphology 
8 S?P17 What do you think? 
9 P17?P17 graph- 
10 P16?P17 That doesn?t sound right but 
11 P17?S Phrenology
12 S?P16,P17 I?m sorry but that?s incorrect. Popular 
during the 19th century but not dis-
credited, phrenology is a theory that a 
person?s character and intelligence 
can be determined by the size and 
shape of their skull.  
13 P16?P17 phrenology
14 P16?P17 She did not wait for your answer
15 P17?P16 I guess not
16 P16?S You didn?t wait for my answer
Immediately after the system?s greeting, the two participants also say ?Hi? and ?Hello?. Their 
greetings are detected as a single utterance by the system which partially overlaps with the be-
ginning of the system?s follow-up question (overlaps are underlined in the examples to the left). 
According to the current policy, the system does not release the floor on this interruption and 
continues with its question. The ?Yes? responses from (4) and (5) are overlapping with each other 
and are detected by the system as a single utterance which is correctly decoded. 
The system correctly infers the floor was addressed to it, and therefore takes the floor and pro-
duces the first question 
Given the pause between ?I?ll go? and ?Graphology? the response in (7) is in fact detected as two 
separate utterances by the system. The first part is non-understood, hence assumed addressed to 
other and the system does not take the floor. The system correctly understands and takes the 
floor after ?Graphology?, and moves to ask for confirmation (8). Next, while the system asks the 
other participant for confirmation, due to imperfections in echo cancellation, the system hears a 
noise at the beginning of its utterance, but ignores the detected ?barge-in?. 
P17 softly says to herself ?graph-?. This utterance is not actually detected by the system. 
Next, the system misunderstands the utterance in (10) as ?that sounds right? and incorrectly infers 
that the utterance was addressed to it. It therefore takes the floor and continues. This leads to a 
turn-initial overlap with the ?Phrenology? utterance immediately produced by P17 (11) 
 
 
The follow-up utterances and discussion between participants (13-16) overlap with portions of the 
system?s explanation. They indicate the high cost of the misunderstanding and of the system?s 
incorrect inference and decision to take the floor (admonished by the user in (16) n, as well as 
the shortcomings of the current policy to not release the floor for barge-ins detected during expla-
nations. This example highlights the need for more robust inferences, but also better policies for 
releasing back the floor and for machinery that would allow the system to gracefully backing from 
detected floor conflicts. 
 
 P arrow shows  
direction of 
attention 
 P P has floor 
 P P is speaking 
 P P is an 
addressee 
n In the first segment, while the system is 
speaking to both participants (12), P17 leans 
in as she produces utterance (16) 
o In the second segment, the system re-
leases the floor to both participants after 
producing (6) 
Illustrations of conversational scene 
analysis performed by the system in real-
time, at runtime. 
 
109
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 13?14,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Towards Situated Collaboration 
 
Dan Bohus, Ece Kamar, Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052, USA 
{dbohus,eckamar,horvitz@microsoft.com} 
 
 
 
 
 
 
Abstract 
We outline a set of key challenges for dialog 
management in physically situated interactive 
systems, and propose a core shift in perspec-
tive that places spoken dialog in the context of 
the larger collaborative challenge of managing 
parallel, coordinated actions in the open 
world.  
Multiple models for dialog management have been 
proposed, studied, and evaluated in the research 
community (i.a. Allen et al 2001; Bohus and Rud-
nicky, 2009; Rich and Sidner, 1998; Traum and 
Larsson, 2003; Williams and Young, 2007). In the 
process, a diverse set of problems have come to 
light and have been pursued. These include the 
challenges of modeling initiative in interaction, 
contextual interpretation and processing, the man-
agement of uncertainty, grounding, error handling 
and recovery, turn-taking and, more recently, in-
cremental processing in dialog systems. Analyses 
of existing approaches (Allen et. al, 2001; Church-
er et. al, 1997; McTear 2002; Paek and Pieraccini, 
2008) reveal a constellation of benefits but also 
shortcomings along multiple dimensions, where no 
single technique provides the benefits of all. 
While taking incremental, focused steps is im-
portant for making progress within a mature disci-
pline, we believe that the current scope and 
conceptual borders of work in spoken dialog con-
strains thinking about possibilities and gets in the 
way of achieving breakthrough advances. Research 
to date on dialog management has focused almost 
exclusively on dyadic settings, where a single user 
interacts with a system over a relatively narrow, 
speech-only channel. Characteristics of this domi-
nant and shared worldview on dialog research have 
driven modeling and architectural choices, and of-
ten done so in an implicit, hidden manner. For in-
stance, dialog is often viewed as a collection of 
dialog moves that are timed in a relatively well-
structured, sequential fashion. As a consequence, 
dialog management models typically operate on a 
?per-turn? basis: inputs are assumed to arrive se-
quentially and are processed one at a time; for each 
received input, discourse understanding is per-
formed, and a corresponding response is generated.  
In reality, interactions among actors situated in 
the open, physical world depart deeply from com-
mon assumptions made in spoken dialog research 
and bring into focus an array of important, new 
challenges (Horvitz, 2007; Bohus and Horvitz, 
2010; Bohus, Horvitz, Kanda et al, eds., 2010).  
We describe some of the challenges with respect to 
dialog management, and re-frame this problem as 
an instance of the larger collaborative challenge of 
managing parallel, coordinated actions amidst a 
dynamically changing physical world.  
As an example, consider a robot that has been 
given the responsibility of greeting, interacting, 
and escorting visitors in a building. In this setting, 
reasoning about the actors, objects and events and 
relationships in the scene can play a critical role in 
understanding and organizing the interactions. The 
surrounding environment provides rich, continu-
ously streaming situational context that is relevant 
for determining the best way an agent might con-
tribute to interactions. Because the situational con-
text can evolve asynchronously with respect to 
turns in the conversation, systems that operate in 
the open world must be able to plan continuously, 
13
in stream, rather than on a ?per-turn? basis. Inter-
action and collaboration in these settings is best 
viewed as a flow of coordinated, parallel actions. 
The sequential structure of turns in dyadic interac-
tions is but one example of such coordination, fo-
cused solely on linguistic actions. However, to 
successfully interact and collaborate with multiple 
participants in physically situated settings, an agent 
must be able to recognize, plan, and produce both 
linguistic and non-linguistic actions, and reason 
about potentially complex patterns of coordination 
between actions, in-stream?as they are being pro-
duced by the participants in the collaboration. 
We argue that attaining the dream of fluid, 
seamless spoken language interaction with ma-
chines requires a fundamental shift in how we view 
dialog management. First, we need to move from 
per-turn to continual in-stream planning. Second, 
we need to move from reasoning about sequential 
actions to reasoning about parallel and coordinat-
ed actions and their influence on states in the 
world. And third, we need models that can track 
and leverage the streaming situational context, 
from noisy observations, to make decisions about 
how to best contribute to collaborations.  
Spoken dialog is an important channel for ex-
pressing coordinative information. However, we 
need to recognize and begin to tackle head on the 
larger challenge of situated collaborative activity 
management.  We understand that taking this per-
spective introduces new complexities?and that 
some of our colleagues will view diving into the 
larger problems in advance of solving simpler ones 
as being unwise. However, we believe that we 
must embrace the larger goals to make significant 
progress on the struggles with the simpler ones, 
and that the investment in solving challenges with 
physically situated collaboration will have eventual 
payoffs in enabling progress in spoken dialog.   
Making progress on the broader challenge re-
quires technical innovations, tools, and data. Con-
sider for instance one sub-problem of belief 
tracking in these systems: continuously updating 
beliefs over the state of the collaborative activity 
and the situational context requires the develop-
ment of new types of models that can combine 
streaming evidence about context collected 
through sensors, with discrete evidence about the 
actions performed or the turns spoken collected 
through speech, gesture or other action-recognition 
components. In addition, progress hinges on identi-
fying a set of relevant problem domains, and coor-
dinating efforts in the community to collect data, 
and comparatively evaluate proposed approaches. 
New tools geared towards analysis, visualization 
and debugging with streaming multimodal data are 
also required.   
We propose a core shift of perspective and as-
sociated research agenda for moving from dialog 
management to situated collaborative activity 
management. We invite discussion on these ideas.  
References  
Allen, J.F., Byron, D.K., Dzikovska, M., Ferguson, G., 
Galescu, L., and Stent, A. 2001. Towards Conversa-
tional Human-Computer Interaction, AI Magazine, 
22(3) 
Bohus, D., and Rudnicky, A. 2009. The Ravenclaw dia-
log management framework: Architecture and sys-
tems, in Computer, Speech and Language, 23(3). 
Bohus, D., and Horvitz, E. 2010. On the Challenges and 
Opportunities of Physically Situated Dialog, AAAI 
Symposium on Dialog with Robots, Arlington, VA. 
Bohus, D., Horvitz, E., Kanda, T., Mutlu, B., Raux, A., 
editors, 2010. Special Issue on ?Dialog with Robots?, 
AI Magazine 32(4). 
Churcher, G. E., Atwell, E.S, and Souter, C. 1997 Dia-
logue Management Systems: a Survey and Overview, 
Technical Report, University of Leeds, Leeds, UK. 
Horvitz, E., 2007. Reflections on Challenges and Prom-
ises of Mixed-Initiative Interaction, AI Magazine 28, 
pp. 19-22. 
McTear, M.F. 2002. Spoken dialogue technology: ena-
bling the conversational user interface, ACM Compu-
ting Surveys, 34(1):90-169. 
Paek, T., and Pierracini, R. 2008. Automating Spoken 
Dialogue Management design using machine learn-
ing: An industry perspective, Speech Communica-
tion, 50(8-9):716-729. 
Rich, C., and Sidner, C.L. 1998. Collagen: A Collabora-
tion Manager for a Collaborative Interface Agent, 
User Modelling and User Assisted Interaction, 7(3-
4):315-350, Kluwer Academic Publishers. 
Traum, D., and Larsson, S. 2003. The Information State 
Approach to Dialogue Management. Current and 
New Directions in Discourse and Dialogue, Text 
Speech and Language Technology, 22:325-353. 
Williams, J., and Young, S., 2007. Partially Observable 
Markov Decisions Processes for Spoken Dialog Sys-
tems, Computer, Speech and Language, 21(2). 
Young, S. 2006. Using POMDPs for Dialog Manage-
ment, in Proc. of SLT-2006, Palm Beach, Aruba. 
14
Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 16?24,
Philadelphia, Pennsylvania, 19 June 2014. c 2014 Association for Computational Linguistics
Crowdsourcing Language Generation Templates for Dialogue Systems
Margaret Mitchell
Microsoft Research
Redmond, WA USAmemitc@microsoft.com Dan BohusMicrosoft ResearchRedmond, WA USAdbohus@microsoft.com Ece KamarMicrosoft ResearchRedmond, WA USAeckamar@microsoft.com
Abstract
We explore the use of crowdsourcing to
generate natural language in spoken dia-
logue systems. We introduce a method-
ology to elicit novel templates from the
crowd based on a dialogue seed corpus,
and investigate the effect that the amount
of surrounding dialogue context has on the
generation task. Evaluation is performed
both with a crowd and with a system de-
veloper to assess the naturalness and suit-
ability of the elicited phrases. Results indi-
cate that the crowd is able to provide rea-
sonable and diverse templates within this
methodology. More work is necessary be-
fore elicited templates can be automati-
cally plugged into the system.
1 Introduction
A common approach for natural language gener-
ation in task-oriented spoken dialogue systems is
template-based generation: a set of templates is
manually constructed by system developers, and
instantiated with slot values at runtime. When
the set of templates is limited, frequent interac-
tions with the system can quickly become repet-
itive, and the naturalness of the interaction is lost.
In this work, we propose and investigate a
methodology for developing a corpus of natural
language generation templates for a spoken dia-
logue system via crowdsourcing. We use an ex-
isting dialogue system that generates utterances
from templates, and explore how well a crowd
can generate reliable paraphrases given snippets
from the system?s original dialogues. By utiliz-
ing dialogue data collected from interactions with
an existing system, we can begin to learn differ-
ent ways to converse while controlling the crowd
to stay within the scope of the original system.
The proposed approach aims to leverage the sys-
tem?s existing capabilities together with the power
of the crowd to expand the system?s natural lan-
guage repertoire and create richer interactions.
Our methodology begins with an existing cor-
pus of dialogues, extracted from a spoken dia-
logue system that gives directions in a building.
Further details on this system are given in ?4.1.
The extracted dialogue corpus contains phrases
the system has generated, and crowd-workers con-
struct alternates for these phrases, which can be
plugged back into the system as crowd templates.
We investigate via crowdsourcing the effect of the
amount of surrounding context provided to work-
ers on the perceived meaning, naturalness, and di-
versity of the alternates they produce, and study
the acceptability of these alternates from a sys-
tem developer viewpoint. Our results indicate that
the crowd provides reasonable and diverse tem-
plates with this methodology. The developer eval-
uation suggests that additional work is necessary
before we can automatically plug crowdsourced
templates directly into the system.
We begin by discussing related work in ?2. In?3, we detail the proposed methodology. In ?4, we
describe the experimental setup and results. Di-
rections for future work are discussed in ?5.
2 Related Work
Online crowdsourcing has gained popularity in
recent years because it provides easy and cheap
programmatic access to human intelligence. Re-
searchers have proposed using crowdsourcing
for a diverse set of natural language process-
ing tasks, including paired data collection for
training machine translation systems (Zaidan and
Callison-Burch, 2011), evaluation of NLP systems
(Callison-Burch and Dredze, 2010) and speech
transcriptions (Parent and Eskenazi, 2010). A
popular task targeting language diversity is para-
phrase generation, which aims at collecting di-
verse phrases while preserving the original mean-
ing. Crowdsourcing paraphrase generation has
16
been studied for the purposes of plagiarism detec-
tion (Burrows and Stein, 2013), machine transla-
tion (Buzek et al., 2010), and expanding language
models used in mobile applications (Han and Ju,
2013). Automated and crowd-based methods have
been proposed for evaluating paraphrases gener-
ated by the crowd (Denkowski and Lavie, 2010;
Tschirsich and Hintz, 2013). Researchers have
proposed workflows to increase the diversity of
language collected with crowd-based paraphrase
generation (Negri et al., 2012) and for reducing
the language bias in generation by initiating gen-
eration with visual input (Chen and Dolan, 2011).
While paraphrase generation typically aims to pre-
serve the meaning of a phrase without considering
its use beyond the sentence level, we focus on col-
lecting diverse language to be used directly in a
dialogue system in a way that agrees with the full
dialogue context.
Manually authoring dialogue systems has been
identified as a challenging and time-consuming
task (Ward and Pellom, 1999), motivating re-
searchers to explore opportunities to use the crowd
to improve and evaluate dialogue systems. Wang
et al. (2012) proposed methods to acquire corpora
for NLP systems using semantic forms as seeds,
and for analyzing the quality of the collected cor-
pora. Liu et al. (2010) used crowdsourcing for
free-form language generation and for semantic
labeling, with the goal of generating language cor-
pora for new domains. Crowd-workers contribute
to dialogue generation in real-time in the Chorus
system by providing input about what the system
should say next (Lasecki et al., 2013). Crowd-
sourcing has also been used with some success for
dialogue system evaluation (Jurc???c?ek et al., 2011).
Previous work on increasing language diversity
in dialogue systems with crowdsourcing has fo-
cused on learning about diversity in user input
to improve components such as speech recogni-
tion and language understanding (e.g., Wang et al.
(2012)). Instead, our work focuses on adding di-
versity to system outputs. Mairesse et al. (2010)
followed a similar approach to the work reported
here, using crowdsourcing to collect paraphrases
for a dialogue system in the restaurant domain.
However, the focus of the Mairesse et al. work was
on training an NLG module using this data. Our
work focuses on crowdsourcing techniques to ex-
tract relevant paraphrases, examining the effect of
context on their suitability and generalizability.
3 Methodology
Our methodology for developing natural language
generation templates is illustrated by the pipeline
in Figure 1. This pipeline is designed for di-
alogue systems that use a template-based natu-
ral language generation component. It assumes
that the given system has an initial set of lan-
guage generation templates that have been man-
ually authored, and expands from there. The ini-
tial system is used to collect a corpus of dialogues,
which we will refer to as the dialogue seed cor-
pus, through interactions with users. Based on the
dialogue seed corpus, we automatically construct
a set of generation HITs, web-based crowdsourc-
ing tasks that are used to elicit paraphrases from
crowd-workers for instantiated system templates.
A generation HIT displays one of the system turns
extracted from a system dialogue, with a phrase
highlighted, and different amounts of surround-
ing context in different conditions. The worker is
asked to replace the phrase with another one that
keeps the same meaning and the coherence of the
interaction. If slots are marked in the original, they
must be preserved by the worker, which allows us
to easily convert the elicited paraphrases to crowd
templates. Once a corpus of crowd templates are
collected in this fashion, a system developer may
filter and decide which to add as viable alternatives
to the system?s existing list of language generation
templates (top path in the pipeline from Figure 1).
We also construct a set of evaluation HITs and
post them to the crowd to assess the suitability and
relative naturalness of the crowd templates (bot-
tom path in the pipeline from Figure 1.) We study
how the scores obtained in this crowd-evaluation
may be used to help filter the set of new templates
that are presented as candidates to the system de-
veloper. In the following subsections, we describe
each of the pipeline components in detail.
3.1 Dialogue Seed Corpus
We assume as a starting point an existing dialogue
system that uses a template-based language gener-
ation component. The system uses a set of tem-
plates T , which are instantiated with slots filled to
generate system phrases. A system turn may con-
tain one or more such phrases connected together.
For instance, in the dialogue fragments shown in
Figure 2, the template ?Sorry, that was [Place]
you wanted, right?? generates at runtime ?Sorry,
that was Ernestine Patrick?s office you wanted,
17
Figure 1: Pipeline for crowd-based development of natural language generation templates.
right??. Statistics on the dialogue seed corpus
used in this study are provided in ?4.2.
The proposed methodology does not require
transcriptions of user utterances in the dialogue
seed corpus; instead, it utilizes the recognition re-
sults for each user turn. The primary reason be-
hind this choice is that a dialogue that contains
recognized user turns may be more coherent than
one that contains transcripts and can be generated
automatically, as the dialogue manager generates
system responses based on the recognition results.
However, turn-overtaking issues and recognition
problems sometimes resulted in incoherent dia-
logue interactions. Improving speech recognition
remains an area for future work.
3.2 Generation HITs
We use the dialogue seed corpus to produce gener-
ation HITs to elicit paraphrases for system phrases
from crowd-workers. In the simplest form, a gen-
eration HIT might present a single system phrase
to the worker. We hypothesize that the surround-
ing context may be an important factor in facili-
tating the construction of appropriate paraphrases,
affecting their diversity, naturalness, generaliz-
ability, etc.; we therefore investigate the effect of
presenting varying amounts of dialogue context to
the worker.
Specifically, given a system phrase correspond-
ing to a template t instantiated in a dialogue, we
investigate six different dialogue context condi-
tions. A phrase in a condition presented to a
crowd-worker will be referred to as a seed, p. Ex-
amples of seeds in each condition are illustrated in
Figure 2. In the first condition, denoted Phrase,
a seed is presented to the worker in isolation. In
the second condition, denoted S, the entire sys-
tem turn containing p is presented to the worker,
with p highlighted. In the next 4 conditions, de-
noted suS, suSu, susuS, susuSu, seeds are pre-
sented in increasingly larger contexts including
one or two previous system and user turns (de-
noted with lowercase ?s? and ?u? in the encoding
Figure 2: Generation HIT excerpts in six different
context conditions (w/o instructions, examples).
above), followed by the system turn S that con-
tains the highlighted seed p, followed in two con-
ditions (susuSu and suSu) by another user turn.
Not all context conditions are applicable for each
instantiated template, e.g., conditions that require
previous context, such as suS, cannot be con-
structed for phrases appearing in the first system
turn. We follow a between-subjects design, such
18
that each worker works on only a single condition.
Each generation HIT elicits a paraphrase for a
seed. The HIT additionally contains instructions
and examples of what workers are expected to do
and not to do.1 We instruct workers to read the
dialogue presented and rephrase the highlighted
phrase (seed) so as to preserve the meaning and
the cohesion of the interaction. To identify slots
accurately in the crowd-generated paraphrases, we
mark slot values in the given seed with bold italics
and instruct workers to keep this portion exactly
the same in their paraphrases (see Figure 2). These
paraphrases are then turned into crowd templates
following 3 basic steps: (1) Spelling error cor-
rection; (2) Normalization;2 and (3) Replacing
filled slots in the worker?s paraphrase with the slot
name. We ask workers to provide paraphrases (in
English) that differ from the original phrase more
substantially than by punctuation changes, and im-
plement controls to ensure that workers enter slot
values.
In completing the generation tasks, the crowd
produces a corpus of paraphrases, one paraphrase
for each seed. For example, ?I apologize, are you
looking for Ernestine Patrick?s office??, is a para-
phrase for the highlighted seed shown in Figure 2.
As we have asked the workers not to alter slot val-
ues, crowd templates can easily be recovered, e.g.,
?I apologize, are you looking for [Place]??
3.3 Evaluation HITs
A good crowd template must minimally satisfy
two criteria: (1) It should maintain the meaning
of the original template; and (2) It should sound
natural in any dialogue context where the original
template was used by the dialogue manager, i.e., it
should generalize well, beyond the specifics of the
dialogue from which it was elicited.
To assess crowd template quality, we construct
evaluation HITs for each crowd template. Instan-
tiated versions of the original template and the
crowd template are displayed as options A and
B (with randomized assignment) and highlighted
as part of the entire dialogue in which the origi-
nal template was used (see Figure 3). In this in-
context (IC) evaluation HIT, the worker is asked
whether the instantiated crowd template has the
same meaning as the original, and which is more
natural. In addition, because the original dialogues
1Instructions available at m-mitchell.com/corpora.html.
2We normalize capitalization, and add punctuation identi-
cal to the seed when no punctuation was provided.
Figure 3: Example evaluation HIT excerpt.
were sometimes incoherent (see ?3.1), we also
asked the evaluation workers to judge whether the
given phrases made sense in the given context.
Finally, in order to assess how well the crowd
template generalizes across different dialogues,
we use a second, out-of-context (OOC) eval-
uation HIT. For each crowd template, we ran-
domly selected a new dialogue where the tem-
plate t appeared. The out-of-context evaluation
HIT presents the instantiated original template and
crowd template in this new dialogue. The crowd-
workers thus assess the crowd template in a dia-
logue context different from the one in which it
was collected. We describe the evaluation HITs in
further detail in ?4.
3.4 Developer Filtering
While a crowd-based evaluation can provide in-
sights into the quality of the crowd templates, ul-
timately, whether or not a template is appropriate
for use in the dialogue system depends on many
other factors (e.g., register, style, expectations,
system goals, etc.). The last step in the proposed
methodology is therefore a manual inspection of
the crowd templates by a system developer, who
assesses which are acceptable for use in the sys-
tem without changes.
19
Figure 4: Directions Robot system.
4 Experiments and Results
We now describe our experiments and results. We
aim to discover whether there is an effect of the
amount of surrounding context on perceived crowd
template naturalness. We additionally explore
whether the crowd template retains the meaning
of the original template, whether they both make
sense in the given context, and the diversity of
the templates that the crowd produced for each
template type. We report results when the tem-
plates are instantiated in-context, in the original
dialogue; and out-of-context, in a new dialogue.
We first describe the experimental test-bed and the
corpora used and collected below.
4.1 Experimental Platform
The test-bed for our experiments is Directions
Robot, a situated dialogue system that provides
directions to peoples? offices, conference rooms,
and other locations in our building (Bohus et al.,
2014). The system couples a Nao humanoid
robot with a software infrastructure for multi-
modal, physically situated dialogue (Bohus and
Horvitz, 2009) and has been deployed for several
months in an open space, in front of the elevator
bank on the 3rd floor of our building (see Figure
4). While some of the interactions are need-based,
e.g., visitors coming to the building for meetings,
many are also driven by curiosity about the robot.
The Directions Robot utilizes rule-based natu-
ral language generation, with one component for
giving directions based on computed paths, and
another component with 38 templates for the rest
of the dialogue. Our experimentation focuses on
these 38 templates. As the example shown in Fig-
ure 2 illustrates, slots are dynamically filled in at
run-time, based on the dialogue history.
We conducted our experiments on a general-
Cond.
Crowd Generation Crowd Eval.
# Gen # w Time/ # Uniq. # Eval Time/
HITs HIT Para. HITs HIT
(? 3) (sec) (? 5) (sec)
Phrase 767 26 34.7 1181 1126 29.4
S 860 28 30.8 1330 1260 39.2
suS 541 26 33.3 1019 772 30.5
suSu 265 24 38.8 531 392 32.6
susuS 360 24 41.0 745 572 32.3
susuSu 296 28 42.9 602 440 34.4
Total 3089 - - 5408 4562 -
Average - 26 36.9 - - 33.1
Table 1: Statistics for the crowd-based generation
and evaluation processes. Each generation HIT
was seen by 3 unique workers and each evaluation
HIT was seen by 5 unique workers. #w represents
number of workers. For evaluation, #w = 231.
purpose crowdsourcing marketplace, the Univer-
sal Human Relevance System (UHRS).3 The mar-
ketplace connects human intelligence tasks with a
large population of workers across the globe. It
provides controls for selecting the country of res-
idence and native languages for workers, and for
limiting the maximum number of tasks that can be
done by a single worker.
4.2 Crowd-based Generation
Dialogue seed corpus We used 167 dialogues
collected with the robot over a period of one week
(5 business days) as the dialogue seed corpus. The
number of turns in these dialogues (including sys-
tem and user) ranges from 1 to 41, with a mean of
10 turns. 30 of the 38 templates (79%) appeared
in this corpus.
Generation HITs We used the dialogue seed
corpus to construct generation HITs, as described
in ?3.2. In a pilot study, we found that for every
10 instances of a template submitted to the crowd,
we received approximately 6 unique paraphrases
in return, with slightly different ratios for each of
the six conditions. We used the ratios observed for
each condition in the pilot study to down-sample
the number of instances we created for each tem-
plate seen more than 10 times in the corpus. The
total number of generation HITs resulting for each
condition is shown in Table 1.
Crowd generation process Statistics on crowd
generation are shown in Table 1. Each worker
could complete at most 1/6 of the total HITs for
that condition. We paid 3 cents for each genera-
3This is a Microsoft-internal crowdsourcing platform.
20
tion HIT, and each HIT was completed by 3 unique
workers. From this set, we removed corrupt re-
sponses, and all paraphrases for a generation HIT
where at least one of the 3 workers did not cor-
rectly write the slot values. This yielded a total of
9123 paraphrases, with 5408 unique paraphrases.
4.3 Crowd-based Evaluation
Evaluation HITs To keep the crowd evaluation
tractable, we randomly sampled 25% of the para-
phrases generated for all conditions to produce
evaluation HITs. We excluded paraphrases from
seeds that did not receive paraphrases from all 3
workers or were missing required slots. As dis-
cussed in ?3, paraphrases were converted to crowd
templates, and each crowd template was instanti-
ated in the original dialogue, in-context (IC) and
in a randomly selected out-of-context (OOC) dia-
logue. The OOC templates were instantiated with
slots relevant to the chosen dialogue. This process
yielded 2281 paraphrases, placed into each of the
two contexts.
Crowd evaluation process As discussed in?3.3, instantiated templates (crowd and original)
were displayed as options A and B, with random-
ized assignment (see Figure 3). Workers were
asked to judge whether the original and the crowd
template had the same meaning, and whether they
made sense in the dialogue context. Workers then
rated which was more natural on a 5-point ordi-
nal scale ranging from -2 to 2, where a -2 rating
marked that the original was much more natural
than the crowd template. Statistics on the judg-
ments collected in the evaluation HITs are shown
in Table 1. Workers were paid 7 cents for each
HIT. Each worker could complete at most 5% of
all HITs, and each HIT was completed by 5 unique
workers.
Outlier elimination One challenge with crowd-
sourced evaluations is noise introduced by spam-
mers. While questions with known answers may
be used to detect spammers in objective tasks, the
subjective nature of our evaluation tasks makes
this difficult: a worker who does not agree with the
majority may simply have different opinions about
the paraphrase meaning or naturalness. Instead of
spam detection, we therefore seek to identify and
eliminate outliers; in addition, as previously dis-
cussed, each HIT was performed by 5 workers, in
an effort to increase robustness.
We focused attention on workers who per-
formed at least 20 HITs (151 of 230 workers, cov-
ering 98% of the total number of HITs). Since
we randomized the A/B assignment of instantiated
original templates and crowd templates, we expect
to see a symmetric distribution over the relative
naturalness scores of all judgments produced by a
worker. To identify workers violating this expec-
tation, we computed a score that reflected the sym-
metry of the histogram of the naturalness votes for
each worker. We considered as outliers 6 work-
ers that were more than z=1.96 standard deviations
away from the mean on this metric (corresponding
to a 95% confidence interval). Secondly, we com-
puted a score that reflected the percentage of tasks
where a worker was in a minority, i.e., had the
single opposing vote to the other workers on the
same meaning question. We eliminated 4 work-
ers, who fell in the top 97.5 percentile of this dis-
tribution. We corroborated these analyses with a
visual inspection of scatterplots showing these two
metrics against the number of tasks performed by
each judge.4 As one worker failed on both criteria,
overall, 9 workers (covering 9% of all judgements)
were considered outliers and their responses were
excluded.
4.4 Crowd Evaluation Results
Meaning and Sense Across conditions, we find
that most crowd templates are evaluated as hav-
ing the same meaning as the original and mak-
ing sense by the majority of workers. Evaluation
percentages are shown in Table 2, and are around
90% across the board. This suggests that in most
cases, the generation task yields crowd templates
that meet the goal of preserving the meaning of the
original template.
Naturalness To evaluate whether the amount of
surrounding context has an effect on the perceived
naturalness of a paraphrase relative to the original
phrase, we use a Kruskal-Wallis (KW) test on the
mean scores for each of the paraphrases, setting
our significance level to .05. A Kruskal-Wallis
test is a non-parametric test useful for significance
testing when the independent variable is categor-
ical and the data is not assumed to be normally
distributed. We find that there is an effect of con-
dition on the relative naturalness score (KW chi-
squared = 15.9156, df = 5, p = 0.007) when crowd
4Scatterplots available at m-mitchell.com/corpora.html.
21
Crowd Evaluation Developer Evaluation
Cond. % Same % Makes Avg. Relative Avg. % Dev. Avg.
Meaning Sense Naturalness D-score Accepted D-score
IC OOC IC OOC IC OOC IC OOC All Seen>1
Phrase 92 91 90 90 -.54 (.66) -.50 (.61) .67 .67 37 67 .30
S 91 89 88 88 -.50 (.65) -.47 (.66) .68 .64 35 53 .29
suS 84 87 85 87 -.37 (.65) -.37 (.61) .70 .70 40 63 .41
suSu 88 85 95 88 -.48 (.62) -.43 (.61) .76 .71 38 50 .39
susuS 94 94 91 94 -.43 (.70) -.39 (.67) .81 .80 38 78 .34
susuSu 91 89 92 86 -.40 (.61) -.38 (.66) .73 .74 45 67 .42
Table 2: % same meaning, % makes sense, and average relative naturalness (standard deviation in paren-
theses), measured in-context (IC) and out-of-context (OOC); crowd-based and developer-based diversity
score (D-score); developer acceptance rate computed over all templates, and those seen more than once.
The susuS condition yields the most diverse templates using crowd-based metrics; removing templates
seen once in the evaluation corpus, this condition has the highest acceptance in the developer evaluation.
templates are evaluated in-context, but not out-of-
context (KW chi-squared = 9.4102, df = 5, p-value
= 0.09378). Average relative naturalness scores in
each condition are shown in Table 2.
Diversity We also assess the diversity of the
templates elicited from the crowd, based on the
evaluation set. Specifically, we calculate a diver-
sity score (D-score) for each template type t. We
calculate this score as the number of unique crowd
template types for t voted to make sense and have
the same meaning as the original by the majority,
divided by the total number of seeds for t with
evaluated crowd templates. More formally, let P
be the original template instantiations that have
evaluated crowd templates, M the set of unique
crowd template types voted as having the same
meaning as the original template by the majority
of workers, and S the set of unique crowd tem-
plate types voted as making sense in the dialogue
by the majority of workers. Then:
D-score(t) =
|M \ S||P |
The average diversity scores across all tem-
plates for each condition are shown in Table 2.
We find the templates that yield the most di-
verse crowd templates include WL Retry ?Where
are you trying to get to in this building?? andOK Help, ?Okay, I think I can help you with
that?, which have a diversity rating of 1.0 in sev-
eral conditions: for each template instance we in-
stantiate (i.e., each generation HIT), we get a new,
unique crowd template back. Example crowd tem-
plates for the OK Help category include ?I be-
lieve I can help you find that? and ?I can help
you ok?. The templates with the least diversity are
those for Hi, which has a D-score around 0.2 in
the S and Phrase conditions.
4.5 Developer Acceptability Results
For the set of crowd templates used in the crowd-
based evaluation process, one of the system de-
velopers5 provided binary judgments on whether
each template could be added (without making any
changes) to the system or not. The developer had
access to the original template, extensive knowl-
edge about the system and domain, and the way in
which each of these templates are used.
Results indicate that the developer retained 487
of the 1493 unique crowd templates that were used
in crowd-evaluation (33%). A breakdown of this
acceptance rate by condition is shown in Table 2.
When we eliminate templates seen only once in
the evaluation corpus, acceptability increases, at
the expense of recall. We additionally calculate
a diversity score from those templates accepted
by the developer, which is simply the number of
crowd template types accepted by the developer,
divided by the total number of seeds used to elicit
the crowd templates in the developer?s evaluation,
for each template type t.
The developer evaluation revealed a wide range
of reasons for excluding crowd templates. Some
of the most common were lack of grammatical-
ity, length (some paraphrases were too long/short),
stylistic mismatch with the system, and incorrect
punctuation. Other reasons included register is-
sues, e.g., too casual/presumptive/impolite, issues
of specificity, e.g., template was too general, and
issues of incompatibility with the dialogue state
and turn construction process. Overall, the de-
veloper interview highlighted very specific system
5The developer was not an author of this paper.
22
Figure 5: Precision and recall for heuristics.
and domain knowledge in the selection process.
4.6 Crowd-based Evaluation and Developer
Acceptability
We now turn to an investigation of whether statis-
tics from the crowd-based generation and evalu-
ation processes can be used to automatically fil-
ter crowd templates. Specifically, we look at two
heuristics, with results plotted in Figure 5. These
heuristics are applied across the evaluation cor-
pus, collating data from all conditions. The first
heuristic, Heur1, uses a simple threshold on the
number of times a crowd template occurred in the
evaluation corpus.6 We hypothesize that more fre-
quent paraphrases are more likely to be acceptable
to the developer, and in fact, as we increase the
frequency threshold, precision increases and recall
decreases.
The second heuristic, Heur2, combines the
threshold on counts with additional scores col-
lected in the out-of-context crowd-evaluation: It
only considers templates with an aggregated judg-
ment on the same meaning question greater than
50% (i.e., the majority of the crowd thought the
paraphrase had the same meaning as the origi-
nal), and with an aggregated relative naturalness
score above the overall mean. As Figure 5 illus-
trates, different tradeoffs between precision and
recall can be achieved via these heuristics, and by
varying the count threshold.
These results indicate that developer filtering re-
mains a necessary step for adding new dialogue
system templates, as the filtering process cannot
yet be replaced by the crowd-evaluation. This is
not surprising since the evaluation HITs did not
6Since the evaluation corpus randomly sampled 25% of
the generation HITs output, this is a proxy for the frequency
with which that template was generated by the crowd.
express all the different factors that we found the
developer took into account when selecting tem-
plates, such as style decisions and how phrases are
combined in the system to form a dialogue. Future
work may consider expanding evaluation HITs to
reflect some of these aspects. By using signals ac-
quired through crowd generation and evaluation,
we should be able to reduce the load for the de-
veloper by presenting a smaller and more precise
candidate list at the expense of reductions in recall.
5 Discussion
We proposed and investigated a methodology for
developing a corpus of natural language genera-
tion templates for a spoken dialogue system via
crowdsourcing. We investigated the effect of the
context we provided to the workers on the per-
ceived meaning, naturalness, and diversity of the
alternates obtained, and evaluated the acceptabil-
ity of these alternates from a system developer
viewpoint.
Our results show that the crowd is able to pro-
vide suitable and diverse paraphrases within this
methodology, which can then be converted into
crowd templates. However, more work is nec-
essary before elicited crowd templates can be
plugged directly into a system.
In future work, we hope to continue this pro-
cess and investigate using features from the crowd
and judgments from system developers in a ma-
chine learning paradigm to automatically identify
crowd templates that can be directly added to the
dialogue system. We would also like to extend be-
yond paraphrasing single templates to entire sys-
tem turns. With appropriate controls and feature
weighting, we may be able to further expand dia-
logue capabilities using the combined knowledge
of the crowd. We expect that by eliciting lan-
guage templates from multiple people, as opposed
to a few developers, the approach may help con-
verge towards a more natural distribution of al-
ternative phrasings in a dialogue. Finally, future
work should also investigate the end-to-end effects
of introducing crowd elicited templates on the in-
teractions with the user.
Acknowledgments
Thanks to members of the ASI group, Chit W.
Saw, Jason Williams, and anonymous reviewers
for help and feedback with this research.
23
References
D. Bohus and E. Horvitz. 2009. Dialog in the open
world: Platform and applications. Proceedings of
ICMI?2009.
Dan Bohus, C. W. Saw, and Eric Horvitz. 2014. Di-
rections robot: In-the-wild experiences and lessons
learned. Proceedings of AAMAS?2014.
Martin Potthast Burrows, Steven and Benno Stein.
2013. Paraphrase acquisition via crowdsourcing and
machine learning. ACM Transactions on Intelligent
Systems and Technology (TIST), 43.
Olivia Buzek, Philip Resnik, and Benjamin B. Beder-
son. 2010. Error driven paraphrase annotation using
mechanical turk. Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with amazon?s me-
chanical turk. Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1.
Michael Denkowski and Alon Lavie. 2010. Exploring
normalization techniques for human judgments of
machine translation adequacy collected using ama-
zon mechanical turk. Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk.
Matthai Philipose Han, Seungyeop and Yun-Cheng Ju.
2013. Nlify: lightweight spoken natural language
interfaces via exhaustive paraphrasing. Proceedings
of the 2013 ACM international joint conference on
Pervasive and ubiquitous computing.
Filip Jurc???c?ek, Simon Keizer, Milica Gas?ic?, Franc?ois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2011. Real user evaluation of spoken dia-
logue systems using amazon mechanical turk. Pro-
ceedings of INTERSPEECH, 11.
Walter S. Lasecki, Rachel Wesley, Jeffrey Nichols,
Anand Kulkarni, James F. Allen, and Jeffrey P.
Bigham. 2013. Chorus: a crowd-powered con-
versational assistant. Proceedings of the 26th an-
nual ACM symposium on User interface software
and technology.
Sean Liu, Stephanie Seneff, and James Glass. 2010.
A collective data generation method for speech lan-
guage models. Spoken Language Technology Work-
shop (SLT), IEEE.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation
using graphical models and active learning. Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012.
Chinese whispers: Cooperative paraphrase acquisi-
tion. Proceedings of LREC.
Gabriel Parent and Maxine Eskenazi. 2010. To-
ward better crowdsourced transcription: Transcrip-
tion of a year of the let?s go bus information sys-
tem data. Spoken Language Technology Workshop
(SLT), IEEE.
Martin Tschirsich and Gerold Hintz. 2013. Leveraging
crowdsourcing for paraphrase recognition. LAW VII
& ID, 205.
William Yang Wang, Dan Bohus, Ece Kamar, and
Eric Horvitz. 2012. Crowdsourcing the acquisi-
tion of natural language corpora: Methods and ob-
servations. Spoken Language Technology Workshop
(SLT), IEEE.
W. Ward and B. Pellom. 1999. The cu communicator
system. Proceedings of IEEE ASRU.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1.
24
