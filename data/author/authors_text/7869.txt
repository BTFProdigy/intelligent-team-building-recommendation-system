Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090?1099,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Soft-Supervised Learning for Text Classification
Amarnag Subramanya & Jeff Bilmes
Dept. of Electrical Engineering,
University of Washington, Seattle, WA 98195, USA.
{asubram,bilmes}@ee.washington.edu
Abstract
We propose a new graph-based semi-
supervised learning (SSL) algorithm and
demonstrate its application to document
categorization. Each document is represented
by a vertex within a weighted undirected
graph and our proposed framework minimizes
the weighted Kullback-Leibler divergence
between distributions that encode the class
membership probabilities of each vertex. The
proposed objective is convex with guaranteed
convergence using an alternating minimiza-
tion procedure. Further, it generalizes in
a straightforward manner to multi-class
problems. We present results on two stan-
dard tasks, namely Reuters-21578 and
WebKB, showing that the proposed algorithm
significantly outperforms the state-of-the-art.
1 Introduction
Semi-supervised learning (SSL) employs small
amounts of labeled data with relatively large
amounts of unlabeled data to train classifiers. In
many problems, such as speech recognition, doc-
ument classification, and sentiment recognition,
annotating training data is both time-consuming
and tedious, while unlabeled data are easily ob-
tained thus making these problems useful appli-
cations of SSL. Classic examples of SSL algo-
rithms include self-training (Yarowsky, 1995) and
co-training (Blum and Mitchell, 1998). Graph-
based SSL algorithms are an important class of SSL
techniques that have attracted much of attention of
late (Blum and Chawla, 2001; Zhu et al, 2003).
Here one assumes that the data (both labeled and
unlabeled) is embedded within a low-dimensional
manifold expressed by a graph. In other words,
each data sample is represented by a vertex within
a weighted graph with the weights providing a mea-
sure of similarity between vertices.
Most graph-based SSL algorithms fall under one
of two categories ? those that use the graph structure
to spread labels from labeled to unlabeled samples
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002) and those that optimize a loss function
based on smoothness constraints derived from the
graph (Blum and Chawla, 2001; Zhu et al, 2003;
Joachims, 2003; Belkin et al, 2005). Sometimes the
two categories are similar in that they can be shown
to optimize the same underlying objective (Zhu and
Ghahramani, 2002; Zhu et al, 2003). In general
graph-based SSL algorithms are non-parametric and
transductive.1 A learning algorithm is said to be
transductive if it is expected to work only on a closed
data set, where a test set is revealed at the time of
training. In practice, however, transductive learners
can be modified to handle unseen data (Zhu, 2005a;
Sindhwani et al, 2005). A common drawback of
many graph-based SSL algorithms (e.g. (Blum and
Chawla, 2001; Joachims, 2003; Belkin et al, 2005))
is that they assume binary classification tasks and
thus require the use of sub-optimal (and often com-
putationally expensive) approaches such as one vs.
rest to solve multi-class problems, let alne struc-
tured domains such as strings and trees. There are
also issues related to degenerate solutions (all un-
labeled samples classified as belonging to a single
1Excluding Manifold Regularization (Belkin et al, 2005).
1090
class) (Blum and Chawla, 2001; Joachims, 2003;
Zhu and Ghahramani, 2002). For more background
on graph-based and general SSL and their applica-
tions, see (Zhu, 2005a; Chapelle et al, 2007; Blitzer
and Zhu, 2008).
In this paper we propose a new algorithm for
graph-based SSL and use the task of text classifica-
tion to demonstrate its benefits over the current state-
of-the-art. Text classification involves automatically
assigning a given document to a fixed number of se-
mantic categories. Each document may belong to
one, many, or none of the categories. In general,
text classification is a multi-class problem (more
than 2 categories). Training fully-supervised text
classifiers requires large amounts of labeled data
whose annotation can be expensive (Dumais et al,
1998). As a result there has been interest is us-
ing SSL techniques for text classification (Joachims,
1999; Joachims, 2003). However past work in semi-
supervised text classification has relied primarily on
one vs. rest approaches to overcome the inherent
multi-class nature of this problem. We believe such
an approach may be sub-optimal because, disregard-
ing data overlap, the different classifiers have train-
ing procedures that are independent of one other.
In order to address the above drawback we pro-
pose a new framework based on optimizing a loss
function composed of Kullback-Leibler divergence
(KL-divergence) (Cover and Thomas, 1991) terms
between probability distributions defined for each
graph vertex. The use of probability distributions,
rather than fixed integer labels, not only leads to a
straightforward multi-class generalization, but also
allows us to exploit other well-defined functions of
distributions, such as entropy, to improve system
performance and to allow for the measure of uncer-
tainty. For example, with a single integer, at most all
we know is its assignment. With a distribution, we
can continuously move from knowing an assignment
with certainty (i.e., an entropy of zero) to expres-
sions of doubt or multiple valid possibilities (i.e., an
entropy greater than zero). This is particularly use-
ful for document classification as we will see. We
also show how one can use the alternating minimiza-
tion (Csiszar and Tusnady, 1984) algorithm to op-
timize our objective leading to a relatively simple,
fast, easy-to-implement, guaranteed to converge, it-
erative, and closed form update for each iteration.
2 Proposed Graph-Based Learning
Framework
We consider the transductive learning problem, i.e.,
given a training setD = {Dl,Du}, whereDl andDu
are the sets of labeled and unlabeled samples respec-
tively, the task is to infer the labels for the samples
in Du. In other words, Du is the ?test-set.? Here
Dl = {(xi, yi)}li=1, Du = {xi}
l+u
i=l+1, xi ? X (the
input space of the classifier, and corresponds to vec-
tors of features) and yi ? Y (the space of classifier
outputs, and for our case is the space of non-negative
integers). Thus |Y| = 2 yields binary classifica-
tion while |Y| > 2 yields multi-class. We define
n = l + u, the total number of samples in the train-
ing set. Given D, most graph-based SSL algorithms
utilize an undirected weighted graph G = (V,E)
where V = {1, . . . , n} are the data points in D
and E = V ? V are the set of undirected edges
between vertices. We use wij ? W to denote the
weight of the edge between vertices i and j. W is
referred to as the weight (or affinity) matrix of G.
As will be seen shortly, the input features xi effect
the final classification results via W, i.e., the graph.
Thus graph construction is crucial to the success of
any graph-based SSL algorithm. Graph construction
?is more of an art, than science? (Zhu, 2005b) and
is an active research area (Alexandrescu and Kirch-
hoff, 2007). In general the weights are formed as
wij = sim(xi,xj)?(j ? K(i)). Here K(i) is the set
of i?s k-nearest-neighbors (KNN), sim(xi,xj) is a
given measure of similarity between xi and xj , and
?(c) returns a 1 if c is true and 0 otherwise. Getting
the similarity measure right is crucial for the success
of any SSL algorithm as that is what determines the
graph. Note that setting K(i) = |V | = n results
in a fully-connected graph. Some popular similarity
measures include
sim(xi,xj) = e
?
?xi?xj?
2
2
?2 or
sim(xi,xj) = cos(xi,xj) =
?xi,xj?
? xi ?2
2
? xj ?2
2
where ? xi ?2 is the L2 norm, and ?xi,xj? is the
inner product of xi and xj . The first similarity mea-
sure is an RBF kernel applied on the squared Eu-
clidean distance while the second is cosine similar-
ity. In this paper all graphs are constructed using
cosine similarity.
1091
We next introduce our proposed approach. For
every i ? V , we define a probability distribution pi
over the elements of Y. In addition let rj , j = 1 . . . l
be another set of probability distributions again over
the elements of Y (recall, Y is the space of classi-
fier outputs). Here {rj}j represents the labels of the
supervised portion of the training data. If the label
for a given labeled data point consists only of a sin-
gle integer, then the entropy of the corresponding rj
is zero (the probability of that integer will be unity,
with the remaining probabilities being zero). If, on
the other hand, the ?label? for a given labeled data
point consists of a set of integers (e.g., if the object
is a member of multiple classes), then rj is able to
represent this property accordingly (see below). We
emphasize again that both pi and rj are probability
distributions, with rj fixed throughout training. The
goal of learning in this paper is to find the best set
of distributions pi, ?i that attempt to: 1) agree with
the labeled data rj wherever it is available; 2) agree
with each other (when they are close according to a
graph); and 3) be smooth in some way. These cri-
teria are captured in the following new multi-class
SSL optimization procedure:
min
p
C
1
(p), where C
1
(p) =
[
l?
i=1
DKL
(
ri||pi
)
+?
n?
i
?
j
wijDKL
(
pi||pj
)
? ?
n?
i=1
H(pi)
?
?
,
(1)
and where p , (p
1
, . . . , pn) denotes the en-
tire set of distributions to be learned, H(pi) =
?
?
y
pi(y) log pi(y) is the standard Shannon en-
tropy function of pi, DKL(pi||qj) is the KL-
divergence between pi and qj , and ? and ? are hy-
perparameters whose selection we discuss in section
5. The distributions ri are derived from Dl (as men-
tioned above) and this can be done in one of the fol-
lowing ways: (a) if y?i is the single supervised label
for input xi then ri(y) = ?(y = y?i), which means
that ri gives unity probability for y equaling the la-
bel y?i; (b) if y?i = {y?
(1)
i , . . . , y?
(k)
i }, k ? |Y| is a set
of possible outputs for input xi, meaning an object
validly falls into all of the corresponding categories,
we set ri(y) = (1/k)?(y ? y?i) meaning that ri is
uniform over only the possible categories and zero
otherwise; (c) if the labels are somehow provided
in the form of a set of non-negative scores, or even
a probability distribution itself, we just set ri to be
equal to those scores (possibly) normalized to be-
come a valid probability distribution. Among these
three cases, case (b) is particularly relevant to text
classification as a given document many belong to
(and in practice may be labeled as) many classes.
The final classification results, i.e., the final labels
for Du, are then given by y? = argmax
y?Y
pi(y).
We next provide further intuition on our objective
function. SSL on a graph consists of finding a la-
belingDu that is consistent with both the labels pro-
vided in Dl and the geometry of the data induced
by the graph. The first term of C
1
will penalize
the solution pi i ? {1, . . . , l}, when it is far away
from the labeled training data Dl, but it does not in-
sist that pi = ri, as allowing for deviations from ri
can help especially with noisy labels (Bengio et al,
2007) or when the graph is extremely dense in cer-
tain regions. As explained above, our framework al-
lows for the case where supervised training is uncer-
tain or ambiguous. We consider it reasonable to call
our approach soft-supervised learning, generalizing
the notion of semi-supervised learning, since there
is even more of a continuum here between fully su-
pervised and fully unsupervised learning than what
typically exists with SSL. Soft-supervised learning
allows uncertainty to be expressed (via a probability
distribution) about any of the labels individually.
The second term of C
1
penalizes a lack of con-
sistency with the geometry of the data and can be
seen as a graph regularizer. If wij is large, we prefer
a solution in which pi and pj are close in the KL-
divergence sense. While KL-divergence is asym-
metric, given that G is undirected implies W is sym-
metric (wij = wji) and as a result the second term
is inherently symmetric.
The last term encourages each pi to be close to
the uniform distribution if not preferred to the con-
trary by the first two terms. This acts as a guard
against degenerate solutions commonly encountered
in SSL (Blum and Chawla, 2001; Joachims, 2003).
For example, consider the case where part of the
graph is almost completely disconnected from any
labeled vertex (which is possible in the k-nearest
neighbor case). In such situations the third term en-
1092
sures that the nodes in this disconnected region are
encouraged to yield a uniform distribution, validly
expressing the fact that we do not know the labels of
these nodes based on the nature of the graph. More
generally, we conjecture that by maximizing the en-
tropy of each pi, the classifier has a better chance of
producing high entropy results in graph regions of
low confidence (e.g. close to the decision boundary
and/or low density regions). This overcomes a com-
mon drawback of a large number of state-of-the-art
classifiers that tend to be confident even in regions
close to the decision boundary.
We conclude this section by summarizing some of
the features of our proposed framework. It should
be clear that C
1
uses the ?manifold assumption?
for SSL (see chapter 2 in (Chapelle et al, 2007))
? it assumes that the input data can be embed-
ded within a low-dimensional manifold (the graph).
As the objective is defined in terms of probability
distributions over integers rather than just integers
(or to real-valued relaxations of integers (Joachims,
2003; Zhu et al, 2003)), the framework general-
izes in a straightforward manner to multi-class prob-
lems. Further, all the parameters are estimated
jointly (compare to one vs. rest approaches which
involve solving |Y| independent problems). Fur-
thermore, the objective is capable of handling label
training data uncertainty (Pearl, 1990). Of course,
this objective would be useless if it wasn?t possible
to efficiently and easily optimize it on large data sets.
We next describe a method that can do this.
3 Learning with Alternating Minimization
As long as ?, ? ? 0, the objective C
1
(p) is con-
vex. This follows since DKL(pi||pj) is convex in
the pair (pi, pj) (Cover and Thomas, 1991), nega-
tive entropy is convex, and a positive-weighted lin-
ear combination of a set of convex functions is con-
vex. Thus, the problem of minimizing C
1
over the
space of collections of probability distributions (a
convex set) constitutes a convex programming prob-
lem (Bertsekas, 2004). This property is extremely
beneficial since there is a unique global optimum
and there are a variety of methods that can be used
to yield that global optimum. One possible method
might take the derivative of the objective along with
Lagrange multipliers to ensure that we stay within
the space of probability distributions. This method
can sometimes yield a closed form single-step an-
alytical expression for the globally optimum solu-
tion. Unfortunately, however, our problem does not
admit such a closed form solution because the gra-
dient of C
1
(p) with respect to pi(y) is of the form,
k
1
pi(y) log pi(y) + k2pi(y) + k3 (where k1, k2, k3
are fixed constants). Sometimes, optimizing the dual
of the objective can also produce a solution, but un-
fortunately again the dual of our objective also does
not yield a closed form solution. The typical next
step, then, is to resort to iterative techniques such
as gradient descent along with modifications to en-
sure that the solution stays within the set of proba-
bility distributions (the gradient of C
1
alone will not
necessarily point in the direction where p is still a
valid distribution) - one such modification is called
the method of multipliers (MOM). Another solu-
tion would be to use computationally complex (and
complicated) algorithms like interior point methods
(IPM). While all of the above methods (described
in detail in (Bertsekas, 2004)) are feasible ways to
solve our problem, they each have their own draw-
backs. Using MOM, for example, requires the care-
ful tuning of a number of additional parameters such
as learning rates, growth factors, and so on. IPM in-
volves inverting a matrix of the order of the number
of variables and constraints during each iteration.
We instead adopt a different strategy based on al-
ternating minimization (Csiszar and Tusnady, 1984).
This approach has a single additional optimization
parameter (contrasted with MOM), admits a closed
form solution for each iteration not involving any
matrix inversion (contrasted with IPM), and yields
guaranteed convergence to the global optimum. In
order to render our approach amenable to AM, how-
ever, we relax our objective C
1
by defining a new
(third) set of distributions for all training samples qi,
i = 1, . . . , n denoted collectively like the above us-
ing the notation q , (q
1
, . . . , qn). We define a new
objective to be optimized as follows:
min
p,q
C
2
(p, q), where C
2
(p, q) =
[
l?
i=1
DKL
(
ri||qi
)
+?
n?
i=1
?
j?N (i)
w
?
ijDKL
(
pi||qj
)
? ?
n?
i=1
H(pi)
?
?
.
1093
Before going further, the reader may be wondering
at this juncture how might it be desirable for us to
have apparently complicated the objective function
in an attempt to yield a more computationally and
methodologically superior machine learning proce-
dure. This is indeed the case as will be spelled out
below. First, in C
2
we have defined a new weight
matrix [W ?]ij = w?ij of the same size as the original
where W ? = W + ?In, where In is the n? n iden-
tity matrix, and where ? ? 0 is a non-negative con-
stant (this is the optimization related parameter men-
tioned above). This has the effect that w?ii ? wii.
In the original objective C
1
, wii is irrelevant since
DKL(p||p) = 0 for all p, but since there are now two
distributions for each training point, there should be
encouragement for the two to approach each other.
Like C
1
, the first term of C
2
ensures that the la-
beled training data is respected and the last term is
a smoothness regularizer, but these are done via dif-
ferent sets of distributions, q and p respectively ?
this choice is what makes possible the relatively sim-
ple analytical update equations given below. Next,
we see that the two objective functions in fact have
identical solutions when the optimization enforces
the constraint that p and q are equal:
min
(p,q):p=q
C
2
(p, q) = min
p
C
1
(p).
Indeed, as ? gets large, the solutions considered vi-
able are those only where p = q. We thus have that:
lim
???
min
p,q
C
2
(p, q) = min
p
C
1
(p).
Therefore, the two objectives should yield the same
solution as long as ? ? wij for all i, j. A key advan-
tage of this relaxed objective is that it is amenable to
alternating minimization, a method to produce a se-
quence of sets of distributions (pn, qn) as follows:
pn = argmin
p
C
2
(p, qn?1), qn = argmin
q
C
2
(pn, q).
It can be shown (we omit the rather lengthy proof
due to space constraints) that the sequence gener-
ated using the above minimizations converges to the
minimum of C
2
(p, q), i.e.,
lim
n??
C
2
(p(n), q(n)) = inf
p,q
C
2
(p, q),
provided we start with a distribution that is initial-
ized properly q(0)(y) > 0 ? y ? Y. The update
equations for p(n) and q(n) are given by
p
(n)
i (y) =
1
Zi
exp
?
(n?1)
i (y)
?i
,
q
(n)
i (y) =
ri(y)?(i ? l) + ?
?
j w
?
jip
(n)
j (y)
?(i ? l) + ?
?
j w
?
ji
,
where
?i = ? + ?
?
j
w
?
ij ,
?
(n?1)
i (y) = ?? + ?
?
j
w
?
ij(log q
(n?1)
j (y)? 1)
and where Zi is a normalizing constant to ensure pi
is a valid probability distribution. Note that each it-
eration of the proposed framework has a closed form
solution and is relatively simple to implement, even
for very large graphs. Henceforth we refer to the
proposed objective optimized using alternating min-
imization as AM.
4 Connections to Other Approaches
Label propagation (LP) (Zhu and Ghahramani,
2002) is a graph-based SSL algorithms that per-
forms Markov random walks on the graph and has
a straightforward extension to multi-class problems.
The update equations for LP (which also we use for
our LP implementations) may be written as
p
(n)
i (y) =
ri(y)?(i ? l) + ?(i > l)
?
j wijp
(n?1)
j (y)
?(i ? l) + ?(i > l)
?
j wij
Note the similarity to the update equation for q(n)i in
our AM case. It has been shown that the squared-
loss based SSL algorithm (Zhu et al, 2003) and LP
have similar updates (Bengio et al, 2007).
The proposed objective C
1
is similar in spirit to
the squared-loss based objective in (Zhu et al, 2003;
Bengio et al, 2007). Our method, however, differs
in that we are optimizing the KL-divergence over
probability distributions. We show in section 5 that
KL-divergence based loss significantly outperforms
the squared-loss. We believe that this could be due
1094
to the following: 1) squared loss is appropriate un-
der a Gaussian loss model which may not be opti-
mal under many circumstances (e.g. classification);
2) KL-divergence DKL(p||q) is based on a relative
(relative to p) rather than an absolute error; and 3)
under certain natural assumptions, KL-divergence is
asymptotically consistent with respect to the under-
lying probability distributions.
AM is also similar to the spectral graph trans-
ducer (Joachims, 2003) in that they both attempt
to find labellings over the unlabeled data that re-
spect the smoothness constraints of the graph. While
spectral graph transduction is an approximate solu-
tion to a discrete optimization problem (which is NP
hard), AM is an exact solution obtained by optimiz-
ing a convex function over a continuous space. Fur-
ther, while spectral graph transduction assumes bi-
nary classification problems, AM naturally extends
to multi-class situations without loss of convexity.
Entropy Minimization (EnM) (Grandvalet and
Bengio, 2004) uses the entropy of the unlabeled data
as a regularizer while optimizing a parametric loss
function defined over the labeled data. While the
objectives in the case of both AM and EnM make
use of the entropy of the unlabeled data, there are
several important differences: (a) EnM is not graph-
based, (b) EnM is parametric whereas our proposed
approach is non-parametric, and most importantly,
(c) EnM attempts to minimize entropy while the pro-
posed approach aims to maximize entropy. While
this may seem a triviality, it has catastrophic conse-
quences in terms of both the mathematics and mean-
ing. The objective in case of EnM is not convex,
whereas in our case we have a convex formulation
with simple update equations and convergence guar-
antees.
(Wang et al, 2008) is a graph-based SSL al-
gorithm that also employs alternating minimiza-
tion style optimization. However, it is inherently
squared-loss based which our proposed approach
out-performs (see section 5). Further, they do not
provide or state convergence guarantees and one
side of their update approximates an NP-complete
optimization procedure.
The information regularization (IR) (Corduneanu
and Jaakkola, 2003) algorithm also makes use of
a KL-divergence based loss for SSL. Here the in-
put space is divided into regions {Ri} which might
or might not overlap. For a given point xi ? Ri,
IR attempts to minimize the KL-divergence between
pi(yi|xi) and p?Ri(y), the agglomerative distribution
for region Ri. Given a graph, one can define a re-
gion to be a vertex and its neighbor thus making IR
amenable to graph-based SSL. In (Corduneanu and
Jaakkola, 2003), the agglomeration is performed by
a simple averaging (arithmetic mean). While IR sug-
gests (without proof of convergence) the use of al-
ternating minimization for optimization, one of the
steps of the optimization does not admit a closed-
form solution. This is a serious practical drawback
especially in the case of large data sets. (Tsuda,
2005) (hereafter referred to as PD) is an extension of
the IR algorithm to hypergraphs where the agglom-
eration is performed using the geometric mean. This
leads to closed form solutions in both steps of the al-
ternating minimization. There are several important
differences between IR and PD on one side and our
proposed approach: (a) neither IR nor PD use an
entropy regularizer, and (b) the update equation for
one of the steps of the optimization in the case of
PD (equation 13 in (Tsuda, 2005)) is actually a spe-
cial case of our update equation for pi(y) and may
be obtained by setting wij = 1/2. Further, our work
here may be easily extended to hypergraphs.
5 Results
We compare our algorithm (AM) with other
state-of-the-art SSL-based text categorization al-
gorithms, namely, (a) SVM (Joachims, 1999),
(b) Transductive-SVM (TSVM) (Joachims, 1999),
(c) Spectral Graph Transduction (SGT) (Joachims,
2003), and (d) Label Propagation (LP) (Zhu and
Ghahramani, 2002). Note that only SGT and LP
are graph-based algorithms, while SVM is fully-
supervised (i.e., it does not make use of any of the
unlabeled data). We implemented SVM and TSVM
using SVM Light (Joachims, b) and SGT using SGT
Light (Joachims, a). In the case of SVM, TSVM and
SGT we trained |Y| classifiers (one for each class) in
a one vs. rest manner precisely following (Joachims,
2003).
5.1 Reuters-21578
We used the ?ModApte? split of the Reuters-21578
dataset collected from the Reuters newswire in
1095
1987 (Lewis et al, 1987). The corpus has 9,603
training (not to be confused with D) and 3,299 test
documents (which representsDu). Of the 135 poten-
tial topic categories only the 10 most frequent cate-
gories are used (Joachims, 1999). Categories outside
the 10 most frequent were collapsed into one class
and assigned a label ?other?. For each document i
in the training and test sets, we extract features xi in
the following manner: stop-words are removed fol-
lowed by the removal of case and information about
inflection (i.e., stemming) (Porter, 1980). We then
compute TFIDF features for each document (Salton
and Buckley, 1987). All graphs were constructed us-
ing cosine similarity with TFIDF features.
For this task Y = { earn, acq, money, grain,
crude, trade, interest, ship, wheat, corn, average}.
For LP and AM, we use the output space Y? = Y?{
other }. For documents in Dl that are labeled with
multiple categories, we initialize ri to have equal
non-zero probability for each such category. For
example, if document i is annotated as belonging
to classes { acq, grain, wheat}, then ri(acq) =
ri(grain) = ri(wheat) = 1/3.
We created 21 transduction sets by randomly sam-
pling l documents from the training set with the con-
straint that each of 11 categories (top 10 categories
and the class other) are represented at least once in
each set. These samples constitute Dl. All algo-
rithms used the same transduction sets. In the case
of SGT, LP and AM, the first transduction set was
used to tune the hyperparameters which we then held
fixed for all the remaining 20 transduction sets. For
all the graph-based approaches, we ran a search over
K ? {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note
K = n represents a fully connected graph). In addi-
tion, in the case of AM, we set ? = 2 for all exper-
iments, and we ran a search over ? ? {1e?8, 1e?4,
0.01, 0.1, 1, 10, 100} and ? ? {1e?8, 1e?6, 1e?4,
0.01, 0.1}, for SGT the search was over c ? {3000,
3200, 3400, 3800, 5000, 100000} (see (Joachims,
2003)).
We report precision-recall break even point
(PRBEP) results on the 3,299 test documents in Ta-
ble 1. PRBEP has been a popular measure in infor-
mation retrieval (see e.g. (Raghavan et al, 1989)).
It is defined as that value for which precision and
recall are equal. Results for each category in Ta-
ble 1 were obtained by averaging the PRBEP over
Category SVM TSVM SGT LP AM
earn 91.3 95.4 90.4 96.3 97.9
acq 67.8 76.6 91.9 90.8 97.2
money 41.3 60.0 65.6 57.1 73.9
grain 56.2 68.5 43.1 33.6 41.3
crude 40.9 83.6 65.9 74.8 55.5
trade 29.5 34.0 36.0 56.0 47.0
interest 35.6 50.8 50.7 47.9 78.0
ship 32.5 46.3 49.0 26.4 39.6
wheat 47.9 44.4 59.1 58.2 64.3
corn 41.3 33.7 51.2 55.9 68.3
average 48.9 59.3 60.3 59.7 66.3
Table 1: P/R Break Even Points (PRBEP) for the top
10 categories in the Reuters data set with l = 20 and
u = 3299. All results are averages over 20 randomly
generated transduction sets. The last row is the macro-
average over all the categories. Note AM is the proposed
approach.
the 20 transduction sets. The final row ?average?
was obtained by macro-averaging (average of av-
erages). The optimal value of the hyperparame-
ters in case of LP was K = 100; in case of AM,
K = 2000, ? = 1e?4, ? = 1e?2; and in the case
of SGT, K = 100, c = 3400. The results show
that AM outperforms the state-of-the-art on 6 out of
10 categories and is competitive in 3 of the remain-
ing 4 categories. Further it significantly outperforms
all other approaches in case of the macro-averages.
AM is significant over its best competitor SGT at
the 0.0001 level according to the difference of pro-
portions significance test.
Figure 1 shows the variation of ?average? PRBEP
against the number of labeled documents (l). For
each value of l, we tuned the hyperparameters over
the first transduction set and used these values for
all the other 20 sets. Figure 1 also shows error-
bars (? standard deviation) all the experiments. As
expected, the performance of all the approaches
improves with increasing number of labeled docu-
ments. Once again in this case, AM, outperforms
the other approaches for all values of l.
5.2 WebKB Collection
World Wide Knowledge Base (WebKB) is a collec-
tion of 8282 web pages obtained from four academic
1096
0 50 100 150 200 250 300 350 400 450 50045
50
55
60
65
70
75
80
85
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 1: Average PRBEP over all classes vs.
number of labeled documents (l) for Reuters data
set
0 100 200 300 400 500 600
20
30
40
50
60
70
80
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 2: Average PRBEP over all classes vs.
number of labeled documents (l) for WebKB col-
lection.
domains. The web pages in the WebKB set are la-
beled using two different polychotomies. The first
is according to topic and the second is according to
web domain. In our experiments we only consid-
ered the first polychotomy, which consists of 7 cat-
egories: course, department, faculty, project, staff,
student, and other. Following (Nigam et al, 1998)
we only use documents from categories course, de-
partment, faculty, project which gives 4199 docu-
ments for the four categories. Each of the documents
is in HTML format containing text as well as other
information such as HTML tags, links, etc. We used
both textual and non-textual information to construct
the feature vectors. In this case we did not use ei-
ther stop-word removal or stemming as this has been
found to hurt performance on this task (Nigam et al,
1998). As in the the case of the Reuters data set
we extracted TFIDF features for each document and
constructed the graph using cosine similarity.
As in (Bekkerman et al, 2003), we created four
roughly-equal random partitions of the data set. In
order to obtain Dl, we first randomly choose a split
and then sample l documents from that split. The
other three splits constitute Du. We believe this is
more realistic than sampling the labeled web-pages
from a single university and testing web-pages from
the other universities (Joachims, 1999). This method
of creating transduction sets allows us to better eval-
uate the generalization performance of the various
algorithms. Once again we create 21 transduction
sets and the first set was used to tune the hyperpa-
rameters. Further, we ran a search over the same grid
as used in the case of Reuters. We report precision-
Class SVM TSVM SGT LP AM
course 46.5 43.9 29.9 45.0 67.6
faculty 14.5 31.2 42.9 40.3 42.5
project 15.8 17.2 17.5 27.8 42.3
student 15.0 24.5 56.6 51.8 55.0
average 23.0 29.2 36.8 41.2 51.9
Table 2: P/R Break Even Points (PRBEP) for the WebKB
data set with l = 48 and u = 3148. All results are aver-
ages over 20 randomly generated transduction sets. The
last row is the macro-average over all the classes. AM is
the proposed approach.
recall break even point (PRBEP) results on the 3,148
test documents in Table 2. For this task, we found
that the optimal value of the hyperparameter were:
in the case of LP, K = 1000; in case of AM,
K = 1000, ? = 1e?2, ? = 1e?4; and in case of
SGT, K = 100, c = 3200. Once again, AM is sig-
nificant at the 0.0001 level over its closest competi-
tor LP. Figure 2 shows the variation of PRBEP with
number of labeled documents (l) and was generated
in a similar fashion as in the case of the Reuters data
set.
6 Discussion
We note that LP may be cast into an AM-like frame-
work by using the following sequence of updates,
p
(n)
i (y) = ?(i ? l)ri(y) + ?(i > l)q
(n?1)
i ,
q
(n)
i (y) =
?
j wijp
(n)
i (y)
?
j wij
1097
To compare the behavior of AM and LP, we ap-
plied this form of LP along with AM on a simple
5-node binary-classification SSL graph where two
nodes are labeled (node 1 and 2) and the remaining
nodes are unlabeled (see Figure 3, top). Since this is
binary classification (|Y | = 2), each distribution pi
or qi can be depicted using only a single real num-
ber between 0 and 1 corresponding to the probability
that each vertex is class 2 (yes two). We show how
both LP and AM evolve starting from exactly the
same random starting point q0 (Figure 3, bottom).
For each algorithm, the figure shows that both algo-
rithms clearly converge. Each alternate iteration of
LP is such that the labeled vertices oscillate due to
its clamping back to the labeled distribution, but that
is not the case for AM. We see, moreover, qualitative
differences in the solutions as well ? e.g., AM?s so-
lution for the pendant node 5 is less confident than is
LP?s solution. More empirical comparative analysis
between the two algorithms of this sort will appear
in future work.
We have proposed a new algorithm for semi-
supervised text categorization. Empirical results
show that the proposed approach significantly out-
performs the state-of-the-art. In addition the pro-
posed approach is relatively simple to implement
and has guaranteed convergence properties. While
in this work, we use relatively simple features to
construct the graph, use of more sophisticated fea-
tures and/or similarity measures could lead to further
improved results.
Acknowledgments
This work was supported by ONR MURI grant
N000140510388, by NSF grant IIS-0093430, by
the Companions project (IST programme under EC
grant IST-FP6-034434), and by a Microsoft Re-
search Fellowship.
References
Alexandrescu, A. and Kirchhoff, K. (2007). Data-driven
graph construction for semi-supervised graph-based
learnning in nlp. In Proc. of the Human Language
Technologies Conference (HLT-NAACL).
Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y.
(2003). Distributional word clusters vs. words for text
categorization. J. Mach. Learn. Res., 3:1183?1208.
0.8
0.6 0.2
0.8
0.8
Node 1
Label 1
Node 2
Label 2
Node 3
Unlabeled
Node 4
Unlabeled
Node 5
Unlabeled
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
AM iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
LP iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
 
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
Figure 3: Graph (top), and alternating values of pn, qn
for increasing n for AM and LP.
1098
Belkin, M., Niyogi, P., and Sindhwani, V. (2005). On
manifold regularization. In Proc. of the Conference on
Artificial Intelligence and Statistics (AISTATS).
Bengio, Y., Delalleau, O., and Roux, N. L. (2007). Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
Bertsekas, D. (2004). Nonlinear Programming. Athena
Scientific Publishing.
Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on
Semi-Supervised learning. http://ssl-acl08.
wikidot.com/.
Blum, A. and Chawla, S. (2001). Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19?
26. Morgan Kaufmann, San Francisco, CA.
Blum, A. and Mitchell, T. (1998). Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learning
Theory.
Chapelle, O., Scholkopf, B., and Zien, A. (2007). Semi-
Supervised Learning. MIT Press.
Corduneanu, A. and Jaakkola, T. (2003). On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Cover, T. M. and Thomas, J. A. (1991). Elements of In-
formation Theory. Wiley Series in Telecommunica-
tions. Wiley, New York.
Csiszar, I. and Tusnady, G. (1984). Information Geome-
try and Alternating Minimization Procedures. Statis-
tics and Decisions.
Dumais, S., Platt, J., Heckerman, D., and Sahami, M.
(1998). Inductive learning algorithms and represen-
tations for text categorization. In CIKM ?98: Proceed-
ings of the seventh international conference on Infor-
mation and knowledge management, New York, NY,
USA.
Grandvalet, Y. and Bengio, Y. (2004). Semi-supervised
learning by entropy minimization. In Advances in
Neural Information Processing Systems (NIPS).
Joachims, T. SGT Light. http://sgt.joachims.
org.
Joachims, T. SVM Light. http://svmlight.
joachims.org.
Joachims, T. (1999). Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Joachims, T. (2003). Transductive learning via spectral
graph partitioning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Lewis, D. et al (1987). Reuters-21578. http:
//www.daviddlewis.com/resources/
testcollections/reuters21578.
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.
(1998). Learning to classify text from labeled and un-
labeled documents. In AAAI ?98/IAAI ?98: Proceed-
ings of the fifteenth national/tenth conference on Arti-
ficial intelligence/Innovative applications of artificial
intelligence, pages 792?799.
Pearl, J. (1990). Jeffrey?s Rule, Passage of Experience
and Neo-Bayesianism in Knowledge Representation
and Defeasible Reasoning. Kluwer Academic Pub-
lishers.
Porter, M. (1980). An algorithm for suffix stripping. Pro-
gram, 14(3):130?137.
Raghavan, V., Bollmann, P., and Jung, G. S. (1989). A
critical investigation of recall and precision as mea-
sures of retrieval system performance. ACM Trans.
Inf. Syst., 7(3):205?229.
Salton, G. and Buckley, C. (1987). Term weighting ap-
proaches in automatic text retrieval. Technical report,
Ithaca, NY, USA.
Sindhwani, V., Niyogi, P., and Belkin, M. (2005). Be-
yond the point cloud: from transductive to semi-
supervised learning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Szummer, M. and Jaakkola, T. (2001). Partially la-
beled classification with Markov random walks. In
Advances in Neural Information Processing Systems,
volume 14.
Tsuda, K. (2005). Propagating distributions on a hyper-
graph by dual information regularization. In Proceed-
ings of the 22nd International Conference on Machine
Learning.
Wang, J., Jebara, T., and Chang, S.-F. (2008). Graph
transduction via alternating minimization. In Proc. of
the International Conference on Machine Learning
(ICML).
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
Zhu, X. (2005a). Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
Zhu, X. (2005b). Semi-Supervised Learning with
Graphs. PhD thesis, Carnegie Mellon University.
Zhu, X. and Ghahramani, Z. (2002). Learning from
labeled and unlabeled data with label propagation.
Technical report, Carnegie Mellon University.
Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
1099
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 995?1002, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Vocal Joystick: A Voice-Based Human-Computer Interface for
Individuals with Motor Impairments?
Jeff A. Bilmes?, Xiao Li?, Jonathan Malkin?, Kelley Kilanski?, Richard Wright?,
Katrin Kirchhoff?, Amarnag Subramanya?, Susumu Harada?, James A.
Landay?, Patricia Dowden?, Howard Chizeck?
?Dept. of Electrical Engineering
?Dept. of Computer Science & Eng.
?Dept. of Linguistics
?Dept. of Speech & Hearing Science
University of Washington
Seattle, WA
Abstract
We present a novel voice-based human-
computer interface designed to enable in-
dividuals with motor impairments to use
vocal parameters for continuous control
tasks. Since discrete spoken commands
are ill-suited to such tasks, our interface
exploits a large set of continuous acoustic-
phonetic parameters like pitch, loudness,
vowel quality, etc. Their selection is opti-
mized with respect to automatic recogniz-
ability, communication bandwidth, learn-
ability, suitability, and ease of use. Pa-
rameters are extracted in real time, trans-
formed via adaptation and acceleration,
and converted into continuous control sig-
nals. This paper describes the basic en-
gine, prototype applications (in particu-
lar, voice-based web browsing and a con-
trolled trajectory-following task), and ini-
tial user studies confirming the feasibility
of this technology.
1 Introduction
Many existing human-computer interfaces (e.g.,
mouse and keyboard, touch screens, pen tablets,
etc.) are ill-suited to individuals with motor
impairments. Specialized (and often expensive)
human-computer interfaces that have been devel-
oped specifically for this target group include sip
and puff switches, head mice, eye-gaze devices, chin
joysticks and tongue switches. While many indi-
viduals with motor impairments have complete use
?This material is based on work supported by the National
Science Foundation under grant IIS-0326382.
of their vocal system, these devices make little use
of it. Sip and puff switches, for example, have low
communication bandwidth, making it impossible to
achieve more complex control tasks.
Natural spoken language is often regarded as
the obvious choice for a human-computer inter-
face. However, despite significant research efforts
in automatic speech recognition (ASR) (Huang et
al., 2001), existing ASR systems are still not suf-
ficiently robust to a wide variety of speaking condi-
tions, noise, accented speakers, etc. ASR-based in-
terfaces are therefore often abandoned by users after
a short initial trial period. In addition, natural speech
is optimal for communication between humans but
sub-optimal for manipulating computers, windows-
icons-mouse-pointer (WIMP) interfaces, or other
electro-mechanical devices (such as a prosthetic ro-
botic arm). Standard spoken language commands
are useful for discrete but not for continuous op-
erations. For example, in order to move a cursor
from the bottom-left to the upper-right of a screen,
the user might have to repeatedly utter ?up? and
?right? or ?stop? and ?go? after setting an initial tra-
jectory and rate, which is quite inefficient. For these
reasons, we are developing alternative and reusable
voice-based assistive technology termed the ?Vocal
Joystick? (VJ).
2 The Vocal Joystick
The VJ approach has three main characteristics:
1) Continuous control parameters: Unlike standard
speech recognition, the VJ engine exploits continu-
ous vocal characteristics that go beyond simple se-
quences of discrete speech sounds (such as syllables
or words) and include e.g., pitch, vowel quality, and
loudness, which are then mapped to continuous con-
995
trol parameters.
2) Discrete vocal commands: Unlike natural speech,
the VJ discrete input language is based on a pre-
designed set of sounds. These sounds are selected
with respect to acoustic discriminability (maximiz-
ing recognizer accuracy), pronounceability (reduc-
ing potential vocal strain), mnemonic characteris-
tics (reducing cognitive load), robustness to environ-
mental noise, and application appropriateness.
3) Reusable infrastructure: Our goal is not to create
a single application but to provide a modular library
that can be incorporated by developers into a variety
of applications that can be controlled by voice. The
VJ technology is not meant to replace standard ASR
but to enhance and be compatible with it.
2.1 Vocal Characteristics
Three continuous vocal characteristics are extracted
by the VJ engine: energy, pitch, and vowel qual-
ity, yielding four specifiable continuous degrees of
freedom. The first of these, localized acoustic en-
ergy, is used for voice activity detection. In addi-
tion, it is normalized relative to the current vowel
detected (see Section 3.3), and is used by our cur-
rent VJ-WIMP application (Section 4) to control the
velocity of cursor movements. For example, a loud
voice causes a faster movement than does a quiet
voice. The second parameter, pitch, is also extracted
but is currently not mapped to any control dimension
in the VJ-WIMP application but will be in the future.
The third parameter is vowel quality. Unlike conso-
nants, which are characterized by a greater degree of
constriction in the vocal tract, vowels have much in-
herent signal energy and are therefore well-suited to
environments where both high accuracy and noise-
robustness are crucial. Vowels can be characterized
using a 2-D space parameterized by F1 and F2, the
first and second vocal-tract formants (resonant fre-
quencies). We initially experimented with directly
extracting F1/F2 and using them for directly spec-
ifying 2-D continuous control. While we have not
ruled out the use of F1/F2 in the future, we have
so far found that even the best F1/F2 detection al-
gorithms available are not yet accurate enough for
precise real-time specification of movement. There-
fore, we classify vowels directly and map them onto
the 2-D vowel space characterized by degree of con-
striction (i.e., tongue height) and tongue body posi-
tion (Figure 1). In our VJ-WIMP application, we use
Deg
ree 
of C
ons
trict
ion Front Central Back
High
Mid
Low
Tongue Body Position
[iy ] [ix ] [uw ]
[ey] [ax ] [ow ]
[ae ] [a] [aa ]
Figure 1: Vowel configurations as a function of their
dominant articulatory configurations.
the four corners of this chart to map to the 4 princi-
ple directions of up, down, left, and right as shown
in Figure 2 (note that the two figures are flipped and
rotated with respect to each other). We have four
different VJ systems running: A) a 4-class system
allowing only the specification of the 4 principle di-
rections; B) a 5-class system that also includes the
phone [ax] to act as a carrier when wishing to vary
only pitch and loudness; C) a 8-class system that in-
cludes the four diagonal directions; and D) a 9-class
system that includes all phones and directions. Most
of the discussion in this paper refers to the 4-class
system.
A fourth vocal characteristic is also extracted
by the VJ engine, namely discrete sounds. These
sounds may correspond to button presses as on a
mouse or joystick. The choice of sounds depends
on the application and are chosen according to char-
acteristic 2 above.
3 The VJ Engine
Our system-level design goals are modularity, low
latency, and maximal computational efficiency. For
this reason, we share common signal processing
operations in multiple signal extraction modules,
which yields real-time performance but leaves con-
siderable computational headroom for the back-end
applications being driven by the VJ engine.
Figure 3 shows the VJ engine architecture having
three modules: signal processing, pattern recogni-
tion, and motion control.
3.1 Signal Processing
The goal of the signal processing module is to ex-
tract low-level acoustic features that can be used in
996
[iy ]
[ix ]
[uw ]
[ey]
[ow ]
[ae ]
[a]
[aa ][ax ]
Figure 2: Vowel-direction mapping: vowels corre-
sponding to directions.
AcousticWaveform FeatureExtraction
Features:EnergyNCCFF1/F2MFCC
SignalProcessing
Energy
VowelClassification
PatternRecognition
PitchTracking
Discrete SoundRecognition
MotionParameters:
xy-directions,Speed,Acceleration,
Motion Control
SpaceTransformationMotion
ComputerInterfaceDriver Adaptation
Figure 3: System organization
estimating the vocal characteristics. The features we
use are energy, normalized cross-correlation coeffi-
cients (NCCC), formant estimates, Mel-frequency
cepstral coefficients (MFCCs), and formant esti-
mates. To extract features, the speech signal is PCM
sampled at a rate of Fs =16,000Hz. Energy is mea-
sured on a frame-by-frame basis with a frame size
of 25ms and a frame step of 10ms. Pitch is ex-
tracted with a frame size of 40ms and a frame step of
10ms. Multiple pattern recognition tasks may share
the same acoustic features: for example, energy and
NCCCs are used for pitch tracking, and energy and
MFCCs can be used in vowel classification and dis-
crete sound recognition. Therefore, it is more ef-
ficient to decouple feature extraction from pattern
recognition, as is shown in Figure 3.
3.2 Pattern Recognition
The pattern recognition module uses the acoustic
features to extract desired parameters. The estima-
tion and classification system must simultaneously
perform energy computation (available from the in-
put), pitch tracking, vowel classification, and dis-
crete sound recognition.
Many state-of-the-art pitch trackers are based on
dynamic programming (DP). This, however, often
requires the meticulous design of local DP cost func-
tions. The forms of these cost functions are usu-
ally empirically determined and/or their parameters
are tuned by algorithms such as gradient descent
(D.Talkin, 1995). Since different languages and ap-
plications may follow very different pitch transition
patterns, the cost functions optimized for certain lan-
guages and applications may not be the most appro-
priate for others. Our VJ system utilizes a graphi-
cal model mechanism to automatically optimize the
parameters of these cost functions, and has been
shown to yield state-of-the-art performance (X.Li et
al., 2004; J.Malkin et al, 2005).
For frame-by-frame vowel classification, our de-
sign constraints are the need for extremely low la-
tency and low computational cost. Probability es-
timates for vowel classes thus need to be obtained
as soon as possible after the vowel has been uttered
or after any small change in voice quality has oc-
curred. It is well known that models of vowel clas-
sification that incorporate temporal dynamics such
as hidden Markov models (HMMs) can be quite ac-
curate. However, the frame-by-frame latency re-
quirements of VJ make HMMs unsuitable for vowel
classification since HMMs estimate the likelihood
of a model based on the entire utterance. An alter-
native is to utilize causal ?HMM-filtering?, which
computes likelihoods at every frame based on all
frames seen so far. We have empirically found,
however, that slightly non-causal and quite local-
ized estimates of the vowel category probability
is sufficient to achieve user satisfaction. Specifi-
cally, we obtain probability estimates of the form
p(Vt|Xt?? , . . . , Xt+? ), where V is a vowel class,
and Xt?? , . . . , Xt+? are feature frames within a
length 2? + 1 window of features centered at time
t. After several empirical trials, we decided on
neural networks for vowel classification because of
the availability of efficient discriminative training al-
gorithms and their computational simplicity. Specif-
ically we use a simple 2-layer multi-layer percep-
tron (Bishop, 1995) whose input layer consists of
26 ? 7 = 182 nodes, where 26 is the dimension of
Xt, the MFCC feature vector, and 2? + 1 = 7 is the
997
number of consecutive frames, and that has 50 hid-
den nodes (the numbers 7 and 50 were determined
empirically). The output layer has 4 output nodes
representing 4 vowel probabilities. During training,
the network is optimized to minimize the Kullback-
Leibler (K-L) divergence between the output and the
true label distribution, thus achieving the aforemen-
tioned probabilistic interpretation.
The VJ engine needs not only to detect that the
user is specifying a vowel (for continuous control)
but also a consonant-vowel-consonant (CVC) pat-
tern (for discrete control) quickly and with a low
probability of confusion (a VJ system also uses C
and CV patterns for discrete commands). Requir-
ing an initial consonant will phonetically distinguish
these sounds from the pure vowel segments used
for continuous control ? the VJ system constantly
monitors for changes that indicate the beginning of
one of the discrete control commands. The vowel
within the CV and CVC patterns, moreover, can help
prevent background noise from being mis-classified
as a discrete sound. Lastly, each such pattern cur-
rently requires an ending silence, so that the next
command (a new discrete sound or continuous con-
trol vowel) can be accurately initiated. In all cases, a
simple threshold-based rejection mechanism is used
to reduce false positives.
To recognize the discrete control signals, HMMs
are employed since, as in standard speech recogni-
tion, time warping is necessary to normalize for dif-
ferent signal durations corresponding to the same
class. Specifically, we embed phone HMMs into
?word? (C, CV, or CVC) HMMs. In this way, it
is possible to train phone models using a training
set that covers all possible phones, and then con-
struct an application-specific discrete command vo-
cabulary without retraining by recombining existing
phone HMMs into new word HMMs. Therefore,
each VJ-driven application can have its own appro-
priate discrete sound set.
3.3 Motion Control: Direction and Velocity
The VJ motion control module receives several pat-
tern recognition parameters and processes them to
produce output more appropriate for determining 2-
D movement in the VJ-WIMP application.
Initial experiments suggested that using pitch to
affect cursor velocity (Igarashi and Hughes, 2001)
would be heavily constrained by an individual?s vo-
cal range. Giving priority to a more universal user-
independent VJ system, we instead focused on rela-
tive energy. Our observation that users often became
quiet when trying to move small amounts confirmed
energy as a natural choice. Drastically different in-
trinsic average energy levels for each vowel, how-
ever, meant that comparing all sounds to a global av-
erage energy would create a large vowel-dependent
bias. To overcome this, we distribute the energy per
frame among the different vowels, in proportion to
the probabilities output by the neural network, and
track the average energy for each vowel indepen-
dently. By splitting the power in this way, there is
no effect when probabilities are close to 1, and we
smooth out changes during vowel transitions when
probabilities are more evenly distributed.
There are many possible options for determining
velocity (a vector capturing both direction and speed
magnitude) and ?acceleration? (a function determin-
ing how the control-to-display ratio changes based
on input parameters), and the different schemes have
a large impact on user satisfaction. Unlike a standard
mouse cursor, where the mapping is from 2-D hand
movement to a 2-D screen, the VJ system maps from
vocal-tract articulatory movement to a 2-D screen,
and the transformation is not as straightforward. All
values are for the current time frame t unless indi-
cated otherwise. First, a raw direction value is cal-
culated for each axis j ? {x, y} as
dj =
?
i
pi ? ?vi, ej? (1)
in which pi = p(Vt = i|Xt??,...,t+? ) is the proba-
bility for vowel i at time t, vi is a unit vector in the
direction of vowel i, ej is the unit-length positive di-
rectional basis vector along the j axis, and ?v, e? is
the projection of vector v onto unit vector e. To de-
termine movement speed, we first calculate a scalar
for each axis j as
sj =
?
i
max
[
0, gi
(
pi ? f(
E
?i
)
)]
? |?vi, ej?|
where E is the energy in the current frame, ?i is the
average energy for vowel i, and f(?) and gi(?) are
functions used for energy normalization and percep-
tual scaling (such as logs and/or cube-roots). This
therefore allocates frame energy to direction based
on the vowel probabilities. Lastly, we calculate the
velocity for axis j at the current frame as
Vj = ? ? s
?
j ? exp(?sj). (2)
998
where ? represents the overall system sensitivity and
the other values (? and ?) are warping constants, al-
lowing the user to control the shape of the accelera-
tion curve. Typically only one of ? and ? is nonzero.
Setting both to zero results in constant-speed move-
ment along each axis, while ? = 1 and ? = 0
gives a linear mapping that will scale motion with
energy but have no acceleration. The current user-
independent system uses ? = 0.6, ? = 1.0 and sets
? = 0. Lastly, the final velocity along axis j is Vjdj .
Future publications will report on systematic evalu-
ations of different f(?) and gi(?) functions.
3.4 Motion Control: User Adaptation
Since vowel quality is used for continuous control,
inaccuracies can arise due to speaker variability ow-
ing to different speech loudness levels, vocal tract
lengths, etc. Moreover, a vowel class articulated by
one user might partially overlap in acoustic space
with a different vowel class from another user. This
imposes limitations on a purely user-independent
vowel classifier. Differences in speaker loudness
alone could cause significant unpredictability. To
mitigate these problems, we have designed an adap-
tation procedure where each user is asked to pro-
nounce four pre-defined vowel sounds, each last-
ing 2-3 seconds, at the beginning of a VJ ses-
sion. We have investigated several novel adaptation
strategies utilizing both neural networks and support
vector machines (SVM). The fundamental idea be-
hind them both is that an initial speaker-independent
transformation of the space is learned using train-
ing data, and is represented by the first layer of a
neural network. Adaptation data then is used to
transform various parameters of the classifier (e.g.,
all or sub-portions of the neural network, or the para-
meters of the SVM). Further details of some of these
novel adaptation strategies appear in (X.Li et al,
2005), and the remainder will appear in forthcom-
ing publications. Also, the average energy values of
each vowel for each user are recorded and used to
normalize the speed control rate mentioned above.
Preliminary evaluations on the data so far collected
show very good results, with adaptation reducing the
vowel classification error rate by 18% for the 4-class
case, and 35% for the 8-class case. Moreover, infor-
mal studies have shown that users greatly prefer the
VJ system after adaptation than before.
4 Applications and Videos
Our overall intent is for VJ to interface with a va-
riety of applications, and our primary application
so far has been to drive a standard WIMP interface
with VJ controls, what we call the VJ-WIMP ap-
plication. The current VJ version allows left but-
ton clicks (press and release, using the consonant
[k]) as well as left button toggles (using consonant
[ch]) to allow dragging. Since WIMP interfaces
are so general, this allows us to indirectly control
a plethora of different applications. Video demon-
strations are available at the URL: http://ssli.
ee.washington.edu/vj.
One of our key VJ applications is vocal web
browsing. The video (dated 6/2005) shows exam-
ples of two web browsing tasks, one as an exam-
ple of navigating the New York Times web site, the
other using Google Maps to select and zoom in on a
target area. Section 5 describes a preliminary evalu-
ation on these tasks. We have also started using the
VJ engine to control video games (third video ex-
ample), have interfaced VJ with the Dasher system
(Ward et al, 2000) (we call it the ?Vocal Dasher?),
and have also used VJ for figure drawing.
Several additional direct VJ-applications have
also been developed. Specifically, we have directly
interfaced the VJ system into a simple blocks world
environment, where more precise object movement
is possible than via the mouse driver. Specifically,
this environment can draw arbitrary trajectories, and
can precisely measure user fidelity when moving an
object along a trajectory. Fidelity depends both on
positional accuracy and task duration. One use of
this environment shows the spatial direction corre-
sponding to vocal effort (useful for training, forth
video example). Another shows a simple robotic
arm being controlled by VJ. We plan to use this
environment to perform formal and precise user-
performance studies in future work.
5 Preliminary User Study
We conducted a preliminary user study1 to evaluate
the feasibility of VJ and to obtain feedback regard-
ing specific difficulties in using the VJ-WIMP sys-
tem. While this study is not accurate in that: 1) it
does not yet involve the intended target population
1The user study presented here used an earlier version of VJ
than the current improved one described in the preceding pages.
999
of individuals with motor impairments, and: 2) the
users had only a small amount of time to practice and
become adept at using VJ, the study is still indica-
tive of the VJ approach?s overall viability as a novel
voice-based human-computer interface method. The
study quantitatively compares VJ performance with
a standard desktop mouse, and provides qualitative
measurement of the user?s perception of the system.
5.1 Experiment Setup
We recruited seven participants ranging from age 22
to 26, none of whom had any motor impairment.
Of the seven participants, two were female and five
were male. All of them were graduate students in
Computer Science, although none of them had pre-
viously heard of or used VJ. Four of the participants
were native English speakers; the other three had an
Asian language as their mother tongue.
We used a Dell Inspiron 9100 laptop with a 3.2
GHz Intel Pentium IV processor running the Fedora
Core 2 operating system, with a 1280 x 800 24-bit
color display. The laptop was equipped with an ex-
ternal Microsoft IntelliMouse connected through the
USB port which was used for all of the tasks in-
volving the mouse. A head-mounted Amanda NC-
61 microphone was used as the audio input device,
while the audio feedback from the laptop was output
through the laptop speakers. The Firefox browser
was used for all of the tasks, with the browser screen
maximized such that the only portion of the screen
which was not displaying the contents of the web
page was the top navigation toolbar which was 30
pixels high.
5.2 Quantitative and Qualitative Evaluation
At the beginning of the quantitative evaluation, each
participant was given a brief description of the VJ
operations and was shown a demonstration of the
system by a practiced experimenter. The participant
was then guided through an adaptation process dur-
ing which she/he was asked to pronounce the four
directional vowels (Section 3.4). After adaptation,
the participant was given several minutes to practice
using a simple target clicking application. The quan-
titative portion of our evaluation followed a within-
participant design. We exposed each participant to
two experimental conditions which we refer to as
input modalities: the mouse and the VJ. Each par-
ticipant completed two tasks on each modality, with
one trial per task.
The first task was a link navigation task (Link),
in which the participants were asked to start from a
specific web page and follow a particular set of links
to reach a destination. Before the trial, the experi-
menter demonstrated the specified sequence of links
to the participant by using the mouse and clicking at
the appropriate links. The participant was also pro-
vided with a sheet of paper for their reference that
listed the sequence of links that would lead them to
the target. The web site we used was a Computer
Science Department student guide and the task in-
volved following six links with the space between
each successive link including both horizontal and
vertical components.
The second task was map navigation (Map), in
which the participant was asked to navigate an on-
line map application from a starting view (showing
the entire USA) to get to a view showing a partic-
ular campus. The size of the map was 400x400
pixels, and the set of available navigation controls
surrounding the map included ten discrete zoom
level buttons, eight directional panning arrows, and
a click inside the map causing the map to be centered
and zoomed in by one level. Before the trial, a prac-
ticed experimenter demonstrated how to locate the
campus map starting from the USA view to ensure
they were familiar with the geography.
For each task, the participants performed one trial
using the mouse, and one trial using a 4-class VJ.
The trials were presented to the participants in a
counterbalanced order. We recorded the completion
time for each trial, as well as the number of false
positives (system interprets a click when the user
did not make a click sound), missed recognitions
(the user makes a click sound but the system fails to
recognize it as a click), and user errors (whenever
the user clicks on an incorrect link). The recorded
trial times include the time used by all of the above
errors including recovery time.
After the completion of the quantitative evalu-
ation, the participants were given a questionnaire
which consisted of 14 questions related to the partic-
ipants? perception of their experience using VJ such
as the degree of satisfaction, frustration, and embar-
rassment. The answers were encoded on a 7-point
Likert scale. We also included a space where the
participants could write in any comments, and an in-
1000
010
20
30
40
50
60
70
80
90
100
Link Map
Task type
T
as
k 
co
m
p
le
ti
o
n
 t
im
e 
(s
ec
o
n
d
s)
Mouse
Vocal Joystick
Figure 4: Task complement times
0
2
4
6
8
10
12
14
16
18
20
M,
 K
ore
a
M,
 N
ort
he
as
t
M,
 M
idw
es
t
M,
 N
ort
he
as
t
F, 
Mi
d-A
tla
nti
c
F, 
Ch
ina
M,
 C
hin
a
Participant (Gender, Origin)
N
um
be
r o
f m
is
se
d 
re
co
gn
iti
on
s
Link
Map
Figure 5: Missed recognitions by participant
formal post-experiment interview was performed to
solicit further feedback.
5.3 Results
Figure 4 shows the task completion times for Link
and Map tasks, Figure 5 shows the breakdown of
click errors by individual participants, Figure 6
shows the average number of false positive and
missed recognition errors for each of the tasks.
There was no instance of user error in any trial. Fig-
ure 7 shows the median of the responses to each of
the fourteen questionnaire questions (error bars in
each plot show ? standard error). In our measure-
ment of the task completion times, we considered
the VJ?s recognition error rate as a fixed factor, and
thus did not subtract the time spent during those er-
rors from the task completion time.
There were several other interesting observations
that were made throughout the study. We noticed
that the participants who had the least trouble with
missed recognitions for the clicking sound were ei-
0
1
2
3
4
5
6
7
8
9
10
Link Map
Task type
N
um
be
r o
f e
rr
or
s
False positive
Missed Recognition
Figure 6: Average number of click errors per task
1.0
2.0
3.0
4.0
5.0
6.0
7.0
Ea
sy 
to l
ear
n
Ea
sy 
to u
se
Dif
f icu
lt to
 co
ntr
ol
Fru
stra
ting Fu
n
Tir
ing
Em
bar
ras
sin
g
Intu
itiv
e
Err
or 
pro
ne
Se
lf-c
ons
cio
us
Se
lf-c
ons
cio
usn
ess
 de
cre
ase
d
Vo
we
l so
und
s d
isti
ngu
ish
abl
e
Ma
p h
ard
er t
han
 se
arc
h
Mo
tion
 ma
tch
ed 
inte
ntio
n
Strongly
agree
Strongly
disagree
Figure 7: Questionnaire results
ther female or with an Asian language background,
as shown in Figure 5. Our hypothesis regarding the
better performance by female participants is that the
original click sound was trained on one of our fe-
male researcher?s voice. We plan also in future work
to determine how the characteristics of different na-
tive language speakers influence VJ, and ultimately
to correct for any bias.
All but one user explicitly expressed their confu-
sion in distinguishing between the [ae] and [aa] vow-
els. Four of the seven participants independently
stated that their performance would probably have
been better if they had been able to practice longer,
and did not attribute their perceived suboptimal per-
formance to the quality of the VJ?s recognition sys-
tem. Several participants reported that they felt their
vocal cords were strained due to having to produce a
loud sound in order to get the cursor to move at the
desired speed. We suspect this is due either to ana-
log gain problems or to their adapted voice being too
loud, and therefore the system calibrating the nor-
mal speed to correspond to the loud voice. We have
since removed this problem by adjusting our adapta-
1001
tion strategy to express preference for a quiet voice.
In summary, the results from our study suggest
that users without any prior experience were able
to perform basic mouse based tasks using the Vocal
Joystick system with relative slowdown of four to
nine times compared to a conventional mouse. We
anticipate that future planned improvements in the
algorithms underlying the VJ engine (to improve ac-
curacy, user-independence, adaptation, and speed)
will further increase the VJ system?s viability, and
combined with practice could improve VJ enough so
that it becomes a reasonable alternative compared to
a standard mouse?s performance.
6 Related Work
Related voice-based interface studies include
(Igarashi and Hughes, 2001; Olwal and Feiner,
2005). Igarashi & Hughes presented a system where
non-verbal voice features control a mouse system ?
their system requires a command-like discrete sound
to determine direction before initiating a movement
command, where pitch is used to control veloc-
ity. We have empirically found an energy-based
mapping for velocity (as used in our VJ system)
both more reliable (no pitch-tracking errors) and
intuitive. Olwal & Feiner?s system moves the mouse
only after recognizing entire words. de Mauro?s
?voice mouse? http://www.dii.unisi.it/
?maggini/research/voice mouse.html
focuses on continuous cursor movements similar
to the VJ scenario; however, the voice mouse
only starts moving after the vocalization has been
completed leading to long latencies, and it is not
easily portable to other applications. Lastly, the
commercial dictation program Dragon by ScanSoft
includes MouseGridTM(Dra, 2004) which allows
discrete vocal commands to recursively 9-partition
the screen, thus achieving log-command access to a
particular screen point. A VJ system, by contrast,
uses continuous aspects of the voice, has change
latency (about 60ms) not much greater than reaction
time, and allows the user to make instantaneous
directional change using one?s voice (e.g., a user
can draw a ?U? shape in one breath).
7 Conclusions
We have presented new voice-based assistive tech-
nology for continuous control tasks and have
demonstrated an initial system implementation of
this concept. An initial user study using a group
of individuals from the non-target population con-
firmed the feasibility of this technology. We plan
next to further improve our system by evaluating a
number of novel pattern classification techniques to
increase accuracy and user-independence, and to in-
troduce additional vocal characteristics (possibilities
include vibrato, degree of nasality, rate of change
of any of the above as an independent parameter)
to increase the available simultaneous degrees of
freedom controllable via the voice. Moreover, we
plan to develop algorithms to decouple unintended
user correlations of these parameters, and to further
advance both our adaptation and acceleration algo-
rithms.
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion. Clarendon Press, Oxford.
2004. Dragon naturally speaking, MousegridTM, Scan-
Soft Inc.
D.Talkin. 1995. A robust algorithm for pitch track-
ing (RAPT). In W.B.Kleign and K.K.Paliwal, editors,
Speech Coding and Synthesis, pp. 495?515, Amster-
dam. Elsevier Science.
X. Huang, A. Acero, and H.-W. Hon. 2001. Spoken Lan-
guage Processing: A Guide to Theory, Algorithm, and
System Development. Prentice Hall.
T. Igarashi and J. F. Hughes. 2001. Voice as sound: Us-
ing non-verbal voice input for interactive control. In
ACM UIST 2001, November.
J.Malkin, X.Li, and J.Bilmes. 2005. A graphical model
for formant tracking. In Proc. IEEE Intl. Conf. on
Acoustics, Speech, and Signal Processing.
A. Olwal and S. Feiner. 2005. Interaction techniques us-
ing prosodic feature of speech and audio localization.
In Proceedings of the 10th International Conference
on Intelligent User Interfaces, pp. 284?286.
D. Ward, A. F. Blackwell, and D. C. MacKay. 2000.
Dasher - a data entry interface using continuous ges-
tures and language models. In ACM UIST 2000.
X.Li, J.Malkin, and J.Bilmes. 2004. A graphical model
approach to pitch tracking. In Proc. Int. Conf. on Spo-
ken Language Processing.
X.Li, J.Bilmes, and J.Malkin. 2005. Maximum mar-
gin learning and adaptation of MLP classifers. In 9th
European Conference on Speech Communication and
Technology (Eurospeech?05), Lisbon, Portugal, Sep-
tember.
1002
Proceedings of NAACL HLT 2007, Companion Volume, pages 165?168,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Virtual Evidence for Training Speech Recognizers using Partially Labeled
data
Amarnag Subramanya
Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
asubram@u.washington.edu
Jeff Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
bilmes@ee.washington.edu
Abstract
Collecting supervised training data for au-
tomatic speech recognition (ASR) sys-
tems is both time consuming and expen-
sive. In this paper we use the notion of vir-
tual evidence in a graphical-model based
system to reduce the amount of supervi-
sory training data required for sequence
learning tasks. We apply this approach to
a TIMIT phone recognition system, and
show that our VE-based training scheme
can, relative to a baseline trained with
the full segmentation, yield similar results
with only 15.3% of the frames labeled
(keeping the number of utterances fixed).
1 Introduction
Current state-of-the-art speech recognizers use thou-
sands of hours of training data, collected from a
large number of speakers with various backgrounds
in order to make the models more robust. It is well
known that one of the simplest ways of improv-
ing the accuracy of a recognizer is to increase the
amount of training data. Moreover, speech recog-
nition systems can benefit from being trained on
hand-transcribed data where all the appropriate word
level segmentations (i.e., the exact time of the word
boundaries) are known. However, with increasing
amounts of raw speech data being made available, it
is both time consuming and expensive to accurately
segment every word for every given sentence. More-
over, for languages for which only a small amount
of training data is available, it can be expensive and
challenging to annotate with precise word transcrip-
tions ? the researcher may have no choice but to use
partially erroneous training data.
There are a number of different ways to label
data used to train a speech recognizer. First, the
most expensive case (from an annotation perspec-
tive) is fully supervised training, where both word
sequences and time segmentations are completely
specified1 . A second case is most commonly used
in speech recognition systems, where only the word
sequences of utterances are given, but their precise
segmentations are unknown. A third case falls un-
der the realm of semi-supervised approaches. As
one possible example, a previously trained recog-
nizer is used to generate transcripts for unlabeled
data, which are then used to re-train the recog-
nizer based on some measure of recognizer confi-
dence (Lamel et al, 2002).
The above cases do not exhaust the set of possible
training scenarios. In this paper, we show how the
notion of virtual evidence (VE) (Pearl, 1988) may
be used to obtain the benefits of data with time seg-
mentations but using only partially labeled data. Our
method lies somewhere between the first and sec-
ond cases above. This general framework has been
successfully applied in the past to the activity recog-
nition domain (Subramanya et al, 2006). Here we
make use of the TIMIT phone recognition task as an
example to show how VE may be used to deal with
partially labeled speech training data. To the best of
our knowledge, this paper presents the first system to
express training uncertainty using VE in the speech
domain.
2 Baseline System
Figure 1 shows two consecutive time slices of a dy-
namic Bayesian network (DBN) designed for con-
1This does not imply that all variables are observed during
training. While the inter-word segmentations are known, the
model is not given information about intra-word segmentations.
165
Transition
Transition
Phone
Phone
Position
State
Phone
State
C   =1 C =1
S
H
A
R
P
OObservation
t?1
t?1
P
H
S t?1
Rt?1
At?1
Ot?1 t
t
t
t
t
t
tt?1
VE observed child
VE applied 
via this CPT
VE applied 
via this CPT
Figure 1: Training Graph.
text independent (CI) phone recognition. All ob-
served variables are shaded, deterministic depen-
dences are depicted using solid black lines, value
specific dependences are shown using a dot-dash
lines, and random dependencies are represented us-
ing dashed lines. In this paper, given any random
variable (rv) X , x denotes a particular value of that
rv, DX is the domain of X (x ? DX ), and |DX |
represents its cardinality.
In the above model, Pt is the rv representing
the phone variable, Ht models the current po-
sition within a phone, St is the state, Ot the
acoustic observations, At and Rt indicate state
and phone transitions respectively. Here, DXt =
DXt?1 , ?t,?X . In our implementation here,
DHt , DAt ? {0, 1, 2}, DRt ? {0, 1}. Also
?{c1, . . . , cn} is an indicator function that turns on
when all the conditions {c1, . . . , cn} are true (i.e.
a conjunction over all the conditions). The distri-
bution for Ht is given by p(ht|ht?1, rt?1, at?1) =
?{ht=0,rt?1=1} + ?{ht=at?1+ht?1,rt?1=0}, which im-plies that we always start a phone with Ht = 0.
We allow skips in each phone model, and At=0,
indicates no transition, At=1 implies you transi-
tion to the next state, At=2 causes a state to skip
(Ht+1 = Ht + 2). As the TIMIT corpus pro-
vides phone level segmentations, Pt is observed dur-
ing training. However, for reasons that will be-
come clear in the next section, we treat Pt as hid-
den but make it the parent of a rv Ct, with, p(ct =
1|pt) = ?lt=pt where lt is obtained from the tran-scriptions (lt ? DPt). The above formulation hasexactly the same effect as making Pt observed and
setting it equal to lt (Bilmes, 2004). Additional de-
tails on other CPTs in this model may be found in
(Bilmes and Bartels, 2005). We provide more de-
tails on the baseline system in section 4.1.
Our main reason for choosing the TIMIT phone
recognition task is that TIMIT includes both se-
quence and segment transcriptions (something rare
t t t1 4 7
p2p1
Labeled Unlabeled
t5t2 t6tt 8t30
Figure 2: Illustration showing our rendition of Vir-
tual Evidence.
for LVCSR corpora such as Switchboard and
Fisher). This means that we can compare against
a model that has been trained fully supervised. It is
also well known that context-dependent (CD) mod-
els outperform CI models for the TIMIT phone
recognition task (Glass et al, 1996). We used
CI models primarily for the rapid experimental
turnaround time and since it still provides a rea-
sonable test-bed for evaluating new ideas. We
do note, however, that our baseline CI system is
competitive with recently published CD systems
(Wang and Fosler-Lussier, 2006), albeit which uses
many fewer components per mixture (see Sec-
tion 4.1).
3 Soft-supervised Learning With VE
Given a joint distribution over n variables
p(x1, . . . , xn), ?evidence? simply means that
one of the variables (w.l.o.g. x1) is known. We
denote this by x?1, so the probability distribution
becomes p(x?1, . . . , xn) (no longer a function of x1).
Any configuration of the variables where x1 6= x?1
is never considered. We can mimic this behavior
by introducing a new virtual child variable c into
the joint distribution that is always observed to be
one (so c = 1), and have c interact only with x1
via the CPT p(c = 1|x1) = ?x1=x?1 . Therefore,
?
x1 p(c = 1, x1, . . . , xn) = p(x?1, . . . , xn). Nowconsider setting p(c = 1|x1) = f(x1), where
f() is an arbitrary non-negative function. With
this, different treatment can be given to different
assignments to x1, but unlike hard evidence, we
are not insisting on only one particular value. This
represents the general notion of VE. In a certain
sense, the notion of VE is similar to the prior
distribution in Bayesian inference, but it is different
in that VE expresses preferences over combinations
of values of random variables whereas a Bayesian
prior expresses preferences over combinations of
model parameter values. For a more information on
VE, see (Bilmes, 2004; Pearl, 1988).
VE can in fact be used when accurate phone level
segmentations are not available. Consider the illus-
tration in Figure 2. As shown, t1 and t4 are the
166
start and end times respectively for phone p1, while
t4 and t7 are the start and end times for phone p2.
When the start and end times for each phone are
given, we have information about the identity of
the phone that produced each and every observation.
The general training scenario in most large vocabu-
lary speech recognition systems, however, does not
have access to these starting/ending times, and they
are trained knowing only the sequence of phone la-
bels (e.g., that p2 follows p1).
Consider a new transcription based on Figure 2,
where we know that p1 ended at some time t3 ? t4
and that p2 started at sometime t5 > t4. In the
region between t3 and t5 we have no information
on the identity of the phone variable for each
acoustic frame, except that it is either p1 or p2. A
similar case occurs at the start of phone p1 and
the end of phone p2. The above information can
be used in our model (Figure 1) in the following
way (here given only for t2 ? t ? t6): p(Ct =
1|pt) = ?{pt=p1,t2?t?t3} + ?{pt=p2,t5?t?t6} +
ft(p1)?{pt=p1,t3?t?t5} + gt(p2)?{pt=p2,t3?t?t5}.
Here ft(p1) and gt(p2) represent our relative beliefs
at time t in whether the value of Pt is either p1
or p2. It is important to highlight that rather than
the absolute values of these functions, it is their
relative values that have an effect on inference
(Bilmes, 2004). There are number of different
ways of choosing these functions. First, we can set
ft(p1) = gt(p2) = ?, ? > 0. This encodes our
uncertainty regarding the identity of the phone in
this region while still forcing it to be either p1 or
p2, and equal preference is given for both (referred
to as ?uniform over two phones?). Alternatively,
other functions could take into account the fact that,
in the frames ?close? to t3, it is more likely to be
p1, whereas in the frames ?close? to t5, it is more
likely to be p2. This can be represented by using
a decreasing function for ft(p1) and an increasing
function for gt(p2) (for example linearly increasing
or decreasing with time).
As more frames are dropped around transitions
(e.g., as t3 ? t2 decreases), we use lesser amounts
of labeled data. In an extreme situation, we can drop
all the labels (t3 < t2) to recover the case where only
sequence and not segment information is available.
Alternatively, we can have t3 = t2 +1, which means
that only one frame is labeled for every phone in an
utterance ? all other frames of a phone are left un-
transcribed. From the perspective of a transcriber,
this simulates the task of going through an utter-
ance and identifying only one frame that belongs to
0 10 20 30 40 50 60 70 80 90 100
52
54
56
58
60
62
64
% of Unused Segmentation Data
Ph
on
e A
cc
ur
ac
y
 
 
Baseline
Uniform over 2 phones
Linear Interpolation
Figure 3: Virtual Evidence Results
each particular phone without having to identify the
phone boundary. In contrast to the task of determin-
ing the phone boundary, identifying one frame per
word unit is much simpler, less prone to error or dis-
agreement, and less costly (Greenberg, 1995).
4 Experimental Results
4.1 Baseline System
We trained a baseline TIMIT phone recognition sys-
tem that made full use of all phone level segmen-
tations (the fully supervised case). To obtain the
acoustic observations, the signal was first preem-
phasized (? = 0.97) and then windowed using a
Hamming window of size 25ms at 100Hz. We then
extracted MFCC?s from these windowed features.
Deltas and double deltas were appended to the above
observation vector. Each phone is modeled using 3
states, and 64 Gaussians per state. We follow the
standard practice of building models for 48 different
phones and then mapping them down to 39 phones
for scoring purposes (Halberstadt and Glass, 1997).
The decoding DBN graph is similar to the training
graph (Figure 1) except that the variable Ct is re-
moved when decoding. We test on the NIST Core
test set (Glass et al, 1996). All results reported in
this paper were obtained by computing the string
edit (Levenshtein) distance between the hypothesis
and the reference. All models in this paper were
implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Bartels, 2005).
4.2 VE Based Training and Results
We tested various cases of VE-based training by
varying the amount of ?dropped? frame labels on
either side of the transition (the dropped labels be-
came the unlabeled frames of Figure 2). We did this
until there was only one frame left labeled for ev-
ery phone. Moreover, in each of the above cases,
we tested a number of different functions to gener-
167
ate the VE scores (see section 3). The results of our
VE experiments are shown in Figure 3. The curves
were obtained by fitting a cubic spline to the points
shown in the figure. The phone accuracy (PA) of our
baseline system (trained in a fully supervised man-
ner) is 61.4%. If the total number of frames in the
training set is NT , and we drop labels on N frames,
the amount of unused data is given by U = NNT ?100(the x-axis in the figure). Thus U = 0% is the fully
supervised case, whereas U = 100% corresponds
to using only the sequence information. Dropping
the label for one frame on either side of every phone
transition yielded U = 24.5%.
It can be seen that in the case of both ?uniform
over 2 phones? and linear interpolation, the PA ac-
tually improves when we drop a small number (?
5 frames) of frames on either side of the transition.
This seems to suggest that there might be some in-
herent errors in the frame level labels near the phone
transitions. The points on the plot at U=84.7% cor-
respond to using a single labeled frame per phone
in every utterance in the training set (average phone
length in TIMIT is about 7 frames). The PA of the
system using a single label per phone is 60.52%. In
this case, we also used a trapezoidal function defined
as follows: if t = ti were the labeled frames for
phone p1, then ft(p1) = 1, ti ? 1 ? t ? ti + 1, and
a linear interpolation function for the other values
t during the transition to generate the VE weights.
This system yielded a PA of 61.29% (baseline accu-
racy 61.4%). We should highlight that even though
this system used only 15.3% of the labels used by
the baseline, the results were similar! The figure
also shows the PA of the system that used only
the sequence information was about 53% (compare
against baseline accuracy of 61.4%). This lends ev-
idence to the claim that training recognizers using
data with time segmentation information can lead to
improved performance.
Given the procedure we used to drop the frames
around transitions, the single labeled frame for ev-
ery phone is usually located on or around the mid-
point of the phone. This however cannot be guaran-
teed if a transcriber is asked to randomly label one
frame per phone. To simulate such a situation, we
randomly choose one frame to be labeled for every
phone in the utterance. We then trained this system
using the ?uniform over 2 phones? technique and
tested it on the NIST core test set. This experiment
was repeated 10 times, and the PA averaged over the
10 trails was found to be 60.5% (standard deviation
0.402), thus showing the robustness of our technique
even for less carefully labeled data.
5 Discussion
In this paper we have shown how VE can be used
to train a TIMIT phone recognition system using
partially labeled data. The performance of this sys-
tem is not significantly worse than the baseline that
makes use of all the labels. Further, though this
method of data transcription is only slightly more
time consuming that sequence labeling, it yeilds sig-
nificant gains in performance (53% v/s 60.5%). The
results also show that even in the presence of fully
labaled data, allowing for uncertainity at the tran-
sitions during training can be beneficial for ASR
performance. It should however be pointed out
that while phone recognition accuracy is not al-
ways a good predictor of word accuracy, we still
expect that our method will ultimately generalize
to word accuracy as well, assuming we have ac-
cess to a corpus where at least one frame of each
word has been labeled with the word identity. This
work was supported by an ONR MURI grant, No.
N000140510388.
References
[Bilmes and Bartels2005] J. Bilmes and C. Bartels. 2005.
Graphical model architectures for speech recognition. IEEE
Signal Processing Magazine, 22(5):89?100, September.
[Bilmes2004] J. Bilmes. 2004. On soft evidence in Bayesian
networks. Technical Report UWEETR-2004-0016, Univer-
sity of Washington, Dept. of EE.
[Glass et al1996] J. Glass, J. Chang, and M. McCandless. 1996.
A probabilistic framework for feature-based speech recogni-
tion. In Proc. ICSLP ?96, volume 4, Philadelphia, PA.
[Greenberg1995] S Greenberg. 1995. The Switchboard tran-
scription project. Technical report, The Johns Hopkins Uni-
versity (CLSP) Summer Research Workshop.
[Halberstadt and Glass1997] A. K. Halberstadt and J. R. Glass.
1997. Heterogeneous acoustic measurements for phonetic
classification. In Proc. Eurospeech ?97, pages 401?404,
Rhodes, Greece.
[Lamel et al2002] L. Lamel, J. Gauvian, and G. Adda. 2002.
Lightly supervised and unsupervised acoustic model train-
ing. Computer Speech and Language.
[Pearl1988] J. Pearl. 1988. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, Inc.
[Subramanya et al2006] A. Subramanya, A. Raj, J. Bilmes, and
D. Fox. 2006. Recognizing activities and spatial context
using wearable sensors. In Proc. of the Conference on Un-
certainty in Articial Intelligence (UAI).
[Wang and Fosler-Lussier2006] Y. Wang and E. Fosler-Lussier.
2006. Integrating phonetic boundary discrimination explic-
ity into HMM systems. In Proc. of the Interspeech.
168
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167?176,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Efficient Graph-Based Semi-Supervised Learning
of Structured Tagging Models
Amarnag Subramanya
Google Research
Mountain View, CA 94043
asubram@google.com
Slav Petrov
Google Research
New York, NY 10011
slav@google.com
Fernando Pereira
Google Research
Mountain View, CA 94043
pereira@google.com
Abstract
We describe a new scalable algorithm for
semi-supervised training of conditional ran-
dom fields (CRF) and its application to part-
of-speech (POS) tagging. The algorithm uses
a similarity graph to encourage similar n-
grams to have similar POS tags. We demon-
strate the efficacy of our approach on a do-
main adaptation task, where we assume that
we have access to large amounts of unlabeled
data from the target domain, but no additional
labeled data. The similarity graph is used dur-
ing training to smooth the state posteriors on
the target domain. Standard inference can be
used at test time. Our approach is able to scale
to very large problems and yields significantly
improved target domain accuracy.
1 Introduction
Semi-supervised learning (SSL) is the use of
small amounts of labeled data with relatively large
amounts of unlabeled data to train predictors. In
some cases, the labeled data can be sufficient to pro-
vide reasonable accuracy on in-domain data, but per-
formance on even closely related out-of-domain data
may lag far behind. Annotating training data for all
sub-domains of a varied domain such as all of Web
text is impractical, giving impetus to the develop-
ment of SSL techniques that can learn from unla-
beled data to perform well across domains. The ear-
liest SSL algorithm is self-training (Scudder, 1965),
where one makes use of a previously trained model
to annotate unlabeled data which is then used to
re-train the model. While self-training is widely
used and can yield good results in some applica-
tions (Yarowsky, 1995), it has no theoretical guaran-
tees except under certain stringent conditions, which
rarely hold in practice(Haffari and Sarkar, 2007).
Other SSL methods include co-training (Blum
and Mitchell, 1998), transductive support vector ma-
chines (SVMs) (Joachims, 1999), and graph-based
SSL (Zhu et al, 2003). Several surveys cover a
broad range of methods (Seeger, 2000; Zhu, 2005;
Chapelle et al, 2007; Blitzer and Zhu, 2008). A ma-
jority of SSL algorithms are computationally expen-
sive; for example, solving a transductive SVM ex-
actly is intractable. Thus we have a conflict between
wanting to use SSL with large unlabeled data sets
for best accuracy, but being unable to do so because
of computational complexity. Some researchers at-
tempted to resolve this conflict by resorting to ap-
proximations (Collobert et al, 2006), but those lead
to suboptimal results (Chapelle et al, 2007).
Graph-based SSL algorithms (Zhu et al, 2003;
Joachims, 2003; Corduneanu and Jaakkola, 2003;
Belkin et al, 2005; Subramanya and Bilmes, 2009)
are an important subclass of SSL techniques that
have received much attention in the recent past, as
they outperform other approaches and also scale eas-
ily to large problems. Here one assumes that the data
(both labeled and unlabeled) is represented by ver-
tices in a graph. Graph edges link vertices that are
likely to have the same label. Edge weights govern
how strongly the labels of the nodes linked by the
edge should agree.
Most previous work in SSL has focused on un-
structured classification problems, that is, problems
with a relatively small set of atomic labels. There
167
has been much less work on SSL for structured pre-
diction where labels are composites of many atomic
labels with constraints between them. While the
number of atomic labels might be small, there will
generally be exponentially many ways to combine
them into the final structured label. Structured pre-
diction problems over sequences appear for exam-
ple in speech recognition, named-entity recogni-
tion, and part-of-speech tagging; in machine trans-
lation and syntactic parsing, the output may be tree-
structured.
Altun et al (2005) proposed a max-margin ob-
jective for semi-supervised learning over structured
spaces. Their objective is similar to that of manifold
regularization (Belkin et al, 2005) and they make
use of a graph as a smoothness regularizer. However
their solution involves inverting a matrix whose size
depends on problem size, making it impractical for
very large problems. Brefeld and Scheffer (2006)
present a modified version of the co-training algo-
rithm for structured output spaces. In both of the
above cases, the underlying model is based on struc-
tured SVM, which does not scale well to very large
datasets. More recently Wang et al (2009) proposed
to train a conditional random field (CRF) (Lafferty et
al., 2001) using an entropy-based regularizer. Their
approach is similar to the entropy minimization al-
gorithm (Grandvalet and Bengio, 2005). The prob-
lem here is that their objective is not convex and thus
can pose issues for large problems. Further, graph-
based SSL algorithms outperform algorithms based
on entropy minimization (Chapelle et al, 2007).
In this work, we propose a graph-based SSL
method for CRFs that is computationally practical
for very large problems, unlike the methods in the
studies cited above. Our method is scalable be-
cause it trains with efficient standard building blocks
for CRF inference and learning and also standard
graph label propagation machinery. Graph regular-
izer computations are only used for training, so at
test time, standard CRF inference can be used, un-
like in graph-based transductive methods. Briefly,
our approach starts by training a CRF on the source
domain labeled data, and then uses it to decode unla-
beled data from the target domain. The state posteri-
ors on the target domain are then smoothed using the
graph regularizer. Best state sequences for the unla-
beled target data are then created by Viterbi decod-
ing with the smoothed state posteriors, and this au-
tomatic target domain annotation is combined with
the labeled source domain data to retrain the CRF.
We demonstrate our new method in domain adap-
tation for a CRF part-of-speech (POS) tagger. While
POS tagging accuracies have reached the level of
inter-annotator agreement (>97%) on the standard
PennTreebank test set (Toutanova et al, 2003; Shen
et al, 2007), performance on out-of-domain data is
often well below 90%, impairing language process-
ing tasks that need syntactic information. For exam-
ple, on the question domain used in this paper, the
tagging accuracy of a supervised CRF is only 84%.
Our domain adaptation algorithm improves perfor-
mance to 87%, which is still far below in-domain
performance, but a significant reduction in error.
2 Supervised CRF
We assume that we have a set of labeled source do-
main examples Dl = {(xi,yi)}li=1, but only un-
labeled target domain examples Du = {xi}
l+u
i=l+1.
Here xi = x
(1)
i x
(2)
i ? ? ?x
(|xi|)
i is the sequence of
words in sentence i and yi = y
(1)
i y
(2)
i ? ? ? y
(|xi|)
i is
the corresponding POS tag sequence, with y(j)i ? Y
where Y is the set of POS tags. Our goal is to learn
a CRF of the form:
p(yi|xi; ?)?exp
(Ni?
j=1
K?
k=1
?kfk(y
(j?1)
i ,y
(j)
i ,xi, j)
)
for the target domain. In the above equation, ? =
{?1, . . . , ?K} ? RK , fk(y
(j?1)
i , y
(j)
i ,xi, j) is the k-
th feature function applied to two consecutive CRF
states and some window of the input sequence, and
?k is the weight of that feature. We discuss our fea-
tures in detail in Section 6. Given only labeled data
Dl, the optimal feature weights are given by:
??=argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?)+????2
]
(1)
Here ???2 is the squared `2-norm and acts as the
regularizer, and ? is a trade-off parameter whose set-
ting we discuss in Section 6. In our case, we also
have access to the unlabeled data Du from the target
domain which we would like to use for training the
CRF. We first describe how we construct a similarity
168
graph over the unlabeled which will be used in our
algorithm as a graph regularizer.
3 Graph Construction
Graph construction is the most important step in
graph-based SSL. The standard approach for un-
structured problems is to construct a graph whose
vertices are labeled and unlabeled examples, and
whose weighted edges encode the degree to which
the examples they link should have the same la-
bel (Zhu et al, 2003). Then the main graph con-
struction choice is what similarity function to use
for the weighted edges between examples. How-
ever, in structured problems the situation is more
complicated. Consider the case of sequence tag-
ging we are studying. While we might be able to
choose some appropriate sequence similarity to con-
struct the graph, such as edit distance or a string
kernel, it is not clear how to use whole sequence
similarity to constrain whole tag sequences assigned
to linked examples in the learning algorithm. Al-
tun et al (2005) had the nice insight of doing the
graph construction not for complete structured ex-
amples but instead for the parts of structured exam-
ples (also known as factors in graphical model ter-
minology), which encode the local dependencies be-
tween input data and output labels in the structured
problem. However, their approach is too demanding
computationally (see Section 5), so instead we use
local sequence contexts as graph vertices, exploting
the empirical observation that the part of speech of
a word occurrence is mostly determined by its local
context.
Specifically, the set V of graph vertices consists
of all the word n-grams1 (types) that have occur-
rences (tokens) in training sentences (labeled and
unlabeled). We partition V = Vl ? Vu where Vl cor-
responds to n-grams that occur at least once in the
labeled data, and Vu corresponds to n-grams that oc-
cur only in the unlabeled data.
Given a symmetric similarity function between
types to be defined below, we link types u and v with
1We pad the n-grams at the beginning and end of sentences
with appropriate dummy symbols.
Description Feature
Trigram + Context x1 x2 x3 x4 x5
Trigram x2 x3 x4
Left Context x1 x2
Right Context x4 x5
Center Word x2
Trigram ? Center Word x2 x4
Left Word + Right Context x2 x4 x5
Left Context + Right Word x1 x2 x4
Suffix HasSuffix(x3)
Table 1: Features we extract given a sequence of words
?x1 x2 x3 x4 x5? where the trigram is ?x2 x3 x4?.
an edge of weight wuv, defined as:
wuv =
{
sim(u, v) if v ? K(u) or u ? K(v)
0 otherwise
whereK(u) is the set of k-nearest neighbors of u ac-
cording to the given similarity. For all experiments
in this paper, n = 3 and k = 5.
To define the similarity function, for each token
of a given type in the labeled and unlabeled data,
we extract a set of context features. For example,
for the token x2 x3 x4 occurring in the sequence
x1 x2 x3 x4 x5, we use feature templates that cap-
ture the left (x1 x2) and right contexts (x4 x5). Addi-
tionally, we extract suffix features from the word in
the middle. Table 1 gives an overview of the features
that we used. For each n-gram type, we compute the
vector of pointwise mutual information (PMI) val-
ues between the type and each of the features that
occur with tokens of that type. Finally, we use the
cosine distance between those PMI vectors as our
similarity function.
We have thus circumvented the problem of defin-
ing similarities over sequences by defining the graph
over types that represent local sequence contexts.
Since our CRF tagger only uses local features of the
input to score tag pairs, we believe that the graph
we construct captures all significant context infor-
mation. Figure 1 shows an excerpt from our graph.
The figure shows the neighborhoods of a subset of
the vertices with the center word ?book.? To reduce
clutter, we included only closest neighbors and the
edges that involve the nodes of interest.
169
[the conference on]
[whose book on]
[the auction on]
[U.N.-backed conference on]
[the conference speakers]
[to schedule a]
[to postpone a]
VB
[to ace a]
[to book a]
[to run a]
[to start a]
NN
NN
NN
VB
VB
[you book a]
[you rent a]
[you log a]
[you unrar a]
[to book some]
[to approve some]
VB
[to fly some]
[to approve parental-consent]
6
4
3
[the book that]
[the job that]
[the constituition that]
[the movie that]
[the city that]
NN
NN
[a movie agent]
[a clearing agent]
[a book agent]
7
4
6
Figure 1: Vertices with center word ?book? and their local neighborhoods, as well as the shortest-path distance between
them. Note that the noun (NN) and verb (VB) interpretations form two disjoint connected components.
It is remarkable that the neighborhoods are co-
herent, showing very similar syntactic configura-
tions. Furthermore, different vertices that (should)
have the same label are close to each other, form-
ing connected components for each part-of-speech
category (for nouns and verbs in the figure). We ex-
pect the similarity graph to provide information that
cannot be expressed directly in a sequence model.
In particular, it is not possible in a CRF to directly
enforce the constraint that similar trigrams appear-
ing in different sentences should have similar POS
tags. This constraint however is important dur-
ing (semi-supervised) learning, and is what makes
our approach different and more effective than self-
training.
In practice, we expect two main benefits from
our graph-based approach. First, the graph allows
new features to be discovered. Many words occur
only in the unlabeled data and a purely supervised
CRF would not be able to learn feature weights for
those observations. We could use self-training to
learn weights for those features, but self-training just
tends to reinforce the knowledge that the supervised
model already has. The similarity graph on the other
hand can link events that occur only in the unlabeled
data to similar events in the labeled data. Further-
more, because the graph is built over types rather
than tokens, it will encourage the same interpreta-
tion to be chosen for similar trigrams occurring in
different sentences. For example, the word ?unrar?
will most likely not occur in the labeled training
data. Seeing it in the neighborhood of words for
which we know the POS tag will help us learn the
correct POS tag for this otherwise unknown word
(see Figure 1).
Second, the graph propagates adjustments to the
weights of known features. Many words occur only
a handful of times in our labeled data, resulting in
poor estimates of their contributions. Even for fre-
quently occurring events, their distribution in the tar-
get domain might be different from their distribution
in the source domain. While self-training might be
able to help adapt to such domain changes, its ef-
fectiveness will be limited because the model will
always be inherently biased towards the source do-
main. In contrast, labeled vertices in the similar-
ity graph can help disambiguate ambiguous contexts
and correct (some of) the errors of the supervised
model.
4 Semi-Supervised CRF
Given unlabeled data Du, we only have access to
the prior p(x). As the CRF is a discriminative
model, the lack of label information renders the
CRF weights independent of p(x) and thus we can-
not directly utilize the unlabeled data when train-
ing the CRF. Therefore, semi-supervised approaches
to training discriminative models typically use the
unlabeled data to construct a regularizer that is
used to guide the learning process (Joachims, 1999;
Lawrence and Jordan, 2005). Here we use the graph
as a smoothness regularizer to train CRFs in a semi-
supervised manner.
Our algorithm iterates between the following five
170
Algorithm 1 Semi-Supervised CRF Training
?s = crf-train(Dl, ?0)
Set ?(t)0 = ?
(s)
while not converged do
{p} = posterior decode(Du, ?old)
{q} = token to type({p})
{q?} = graph propagate({q})
D(1)u = viterbi decode({q?}, ?old)
?(t)n+1 = crf-train(Dl ? D
(1)
u , ?
(t)
n )
end while
Return last ?(t)
simple (and convex) steps: Given a set of CRF pa-
rameters, we first compute marginals over the un-
labeled data (posterior decode). The marginals
over tokens are then aggregated to marginals over
types (token to type), which are used to initial-
ize the graph label distributions. After running la-
bel propagation (graph propagate), the posteriors
from the graph are used to smooth the state posteri-
ors. Decoding the unlabeled data (viterbi decode)
produces a new set of automatic annotations that can
be combined with the labeled data to retrain the CRF
using the supervised CRF training objective (crf-
train). These steps, summarized in Algorithm 1, are
iterated until convergence.
4.1 Posterior Decoding
Let ?(t)n (t refers to target domain) represent the esti-
mate of the CRF parameters for the target domain af-
ter the n-th iteration.2 In this step, we use the current
parameter estimates to compute the marginal proba-
bilities
p(y(j)i |xi; ?
(t)
n ) 1 ? j ? |xi|, i ? Dl
over POS tags for every word position j for i index-
ing over sentences in Dl ? Du.
4.2 Token-to-Type Mapping
Recall that our graph is defined over types while
the posteriors computed above involve particular to-
kens. We accumulate token-based marginals to cre-
ate type marginals as follows. For a sentence i and
word position j in that sentence, let T (i, j) be the
2In the first iteration, we initialize the target domain param-
eters to the source domain parameters: ?(t)0 = ?
(s).
trigram (graph node) centered at position j. Con-
versely, for a trigram type u, let T?1(u) be the set
of actual occurrences (tokens) of that trigram u; that
is, all pairs (i, j) where i is the index of a sentence
where u occurs and j is the position of the center
word of an occurrence of u in that sentence. We cal-
culate type-level posteriors as follows:
qu(y) ,
1
|T?1(u)|
?
(i,j)?T?1(u)
p(y(j)i |xi; ?
(t)
n ) .
This combination rule connects the token-centered
CRF with the type-centered graph. Other ways
of combining the token marginals, such as using
weights derived from the entropies of marginals,
might be worth investigating.
4.3 Graph Propagation
We now use our similarity graph (Section 3) to
smooth the type-level marginals by minimizing the
following convex objective:
C(q) =
?
u?Vl
?ru ? qu?
2
+ ?
?
u?V,v?N (i)
wuv?qu ? qv?
2 + ?
?
u?V
?qu ? U?
2
s.t.
?
y
qu(y) = 1 ?u & qu(y) ? 0 ?u, y (2)
where q = {q1, q2, . . . q|V |}. The setting of the
hyperparameters ? and ? will be discussed in Sec-
tion 6, N (u) is the set of neighbors of node u, and
ru is the empirical marginal label distribution for tri-
gram u in the labeled data. We use a squared loss to
penalize neighboring nodes that have different label
distributions: ?qu ? qv?2 =
?
y(qu(y) ? qv(y))
2,
additionally regularizing the label distributions to-
wards the uniform distribution U over all possible
labels Y . It can be shown that the above objective is
convex in q.
Our graph propagation objective can be seen as a
multi-class generalization of the quadratic cost crite-
rion (Bengio et al, 2007). The first term in the above
objective requires that we respect the information
in our labeled data. The second term is the graph
smoothness regularizer which requires that the qi?s
be smooth with respect to the graph. In other words,
if wuv is large, then qu and qv should be close in the
171
squared-error sense. This implies that vertices u and
v are likely to have similar marginals over POS tags.
The last term is a regularizer and encourages all type
marginals to be uniform to the extent that is allowed
by the first two terms. If a unlabeled vertex does
not have a path to any labeled vertex, this term en-
sures that the converged marginal for this vertex will
be uniform over all tags, ensuring that our algorithm
performs at least as well as a standard self-training
based algorithm, as we will see later.
While the objective in Equation 2 admits a closed
form solution, it involves inverting a matrix of or-
der |V | and thus we use instead the simple iterative
update given by
q(m)u (y) =
?u(y)
?u
where
?u(y) = ru(y)?(u ? Vl)
+
?
v?N (u)
wuvq
(m?1)
v (y) + ?U(y),
?u = ?(u ? Vl) + ? + ?
?
v?N (u)
wuv (3)
where m is the iteration index and ? is the indica-
tor function that returns 1 if and only if the con-
dition is true. The iterative procedure starts with
q(0)u (y) = qu(y) as given in the previous section.
In all our experiments we run 10 iterations of the
above algorithm, and we denote the type marginals
at completion by q?u(y).
4.4 Viterbi Decoding
Given the type marginals computed in the previous
step, we interpolate them with the original CRF to-
ken marginals. This interpolation between type and
token marginals encourages similar n-grams to have
similar posteriors, while still allowing n-grams in
different sentences to differ in their posteriors. For
each unlabeled sentence i and word position j in it,
we calculate the following interpolated tag marginal:
p?(y(j)i = y|xi) = ?p(y
(j)
i = y|xi; ?
(t)
n )
+ (1? ?)q?T (m,n)(y) (4)
where ? is a mixing coefficient which reflects the
relative confidence between the original posteriors
from the CRF and the smoothed posteriors from the
graph. We discuss how we set ? in Section 6.
The interpolated marginals summarize all the in-
formation obtained so far about the tag distribution
at each position. However, if we were to use them on
their own to select the most likely POS tag sequence,
the first-order tag dependencies modeled by the CRF
would be mostly ignored. This happens because the
type marginals obtained from the graph after label
propagation will have lost most of the sequence in-
formation. To enforce the first-order tag dependen-
cies we therefore use Viterbi decoding over the com-
bined interpolated marginals and the CRF transition
potentials to compute the best POS tag sequence for
each unlabeled sentence. We refer to these 1-best
transcripts as y?i , i ? Du.
4.5 Re-training the CRF
Now that we have successfully labeled the unlabeled
target domain data, we can use it in conjunction with
the source domain labeled data to re-train the CRF:
?(t)n+1 =argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?(t)n )
? ?
l+u?
i=l+1
log p(y?i |xi; ?
(t)
n )+????
2
]
(5)
where ? and ? are hyper-parameters whose setting
we discuss in Section 6. Given the new CRF pa-
rameters ? we loop back to step 1 (Section 4.1) and
iterate until convergence. It is important to note that
every step of our algorithm is convex, although their
combination clearly is not.
5 Related Work
Our work differs from previous studies of
SSL (Blitzer et al, 2006; III, 2007; Huang
and Yates, 2009) for improving POS tagging in
several ways. First, our algorithm can be general-
ized to other structured semi-supervised learning
problems, although POS tagging is our motivating
task and test application. Unlike III (2007), we
do not require target domain labeled data. While
the SCL algorithm (Blitzer et al, 2006) has been
evaluated without target domain labeled data, that
evaluation was to some extent transductive in that
the target test data (unlabeled) was included in the
unsupervised stage of SCL training that creates the
structural correspondence between the two domains.
172
We mentioned already the algorithm of Altun et
al. (2005), which is unlikely to scale up because
its dual formulation requires the inversion of a ma-
trix whose size depends on the graph size. Gupta
et al (2009) also constrain similar trigrams to have
similar POS tags by forming cliques of similar tri-
grams and maximizing the agreement score over
these cliques. Computing clique agreement poten-
tials however is NP-hard and so they propose ap-
proximation algorithms that are still quite complex
computationally. We achieve similar effects by us-
ing our simple, scalable convex graph regularization
framework. Further, unlike other graph-propagation
algorithms (Alexandrescu and Kirchhoff, 2009), our
approach is inductive. While one might be able
to make inductive extensions of transductive ap-
proaches (Sindhwani et al, 2005), these usually re-
quire extensive computational resources at test time.
6 Experiments and Results
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source domain
training set. We follow standard setup procedures
for this task and train on sections 00-18, compris-
ing of 38,219 POS-tagged sentences with a total of
912,344 words. To evaluate our domain-adaptation
approach, we consider two different target domains:
questions and biomedical data. Both target do-
mains are relatively far from the source domain
(newswire), making this a very challenging task.
The QuestionBank (Judge et al, 2006), provides
an excellent corpus consisting of 4,000 questions
that were manually annotated with POS tags and
parse trees. We used the first half as our develop-
ment set and the second half as our test set. Ques-
tions are difficult to tag with WSJ-trained taggers
primarily because the word order is very different
than that of the mostly declarative sentences in the
training data. Additionally, the unknown word rate
is more than twice as high as on the in-domain de-
velopment set (7.29% vs. 3.39%). As our unla-
beled data, we use a set of 10 million questions
collected from anonymized Internet search queries.
These queries were selected to be similar in style
and length to the questions in the QuestionBank.3
3In particular, we selected queries that start with an English
function word that can be used to start a question (what, who,
As running the CRF over 10 million sentences can
be rather cumbersome and probably unnecessary, we
randomly select 100,000 of these queries and treat
this asDu. Because the graph nodes and the features
used in the similarity function are based on n-grams,
data sparsity can be a serious problem, and we there-
fore use the entire unlabeled data set for graph con-
struction. We estimate the mutual information-based
features for each trigram type over all the 10 million
questions, and then construct the graph over only
the set of trigram types that actually occurs in the
100,000 random subset and the WSJ training set.
For our second target domain, we use the Penn
BioTreebank (PennBioIE, 2005). This corpus con-
sists of 1,061 sentences that have been manually an-
notated with POS tags. We used the first 500 sen-
tences as a development set and the remaining 561
sentences as our final test set. The high unknown
word rate (23.27%) makes this corpus very difficult
to tag. Furthermore, the POS tag set for this data is a
super-set of the Penn Treebank?s, including the two
new tags HYPH (for hyphens) and AFX (for com-
mon post-modifiers of biomedical entities such as
genes). These tags were introduced due to the im-
portance of hyphenated entities in biomedical text,
and are used for 1.8% of the words in the test set.
Any tagger trained only on WSJ text will automati-
cally predict wrong tags for those words. For unla-
beled data we used 100,000 sentences that were cho-
sen by searching MEDLINE for abstracts pertaining
to cancer, in particular genomic variations and muta-
tions (Blitzer et al, 2006). Since we did not have ac-
cess to additional unlabeled data, we used the same
set of sentences as target domain unlabeled data,Du.
The graph here was constructed over the 100,000 un-
labeled sentences and the WSJ training set. Finally,
we remind the reader that we did not use label infor-
mation for graph construction in either corpus.
6.1 Baselines
Our baseline supervised CRF is competitive
with state-of-the-art discriminative POS taggers
(Toutanova et al, 2003; Shen et al, 2007), achieving
97.17% on the WSJ development set (sections 19-
21). We use a fairly standard set of features, includ-
ing word identity, suffixes and prefixes and detectors
when, etc.), and have between 30 and 160 characters.
173
Questions Bio
Dev Eval Dev Eval
Supervised CRF 84.8 83.8 86.5 86.2
Self-trained CRF 85.4 84.0 87.5 87.1
Semi-supervised CRF 87.6 86.8 87.5 87.6
Table 2: Domain adaptation experiments. POS tagging accuracies in %.
for special characters such as dashes and digits. We
do not use of observation-dependent transition fea-
tures. Both supervised and semi-supervised models
are regularized with a squared `2-norm regularizer
with weight set to 0.01.
In addition to the supervised baseline trained ex-
clusively on the WSJ, we also consider a semi-
supervised self-trained baseline (?Self-trained CRF?
in Table 2). In this approach, we first train a su-
pervised CRF on the labeled data and then do semi-
supervised training without label propagation. This
is different from plain self-training because it aggre-
gates the posteriors over tokens into posteriors over
types. This aggregation step allows instances of the
same trigram in different sentences to share infor-
mation and works better in practice than direct self-
training on the output of the supervised CRF.
6.2 Domain Adaptation Results
The data set obtained concatenating the WSJ train-
ing set with the 10 million questions had about 20
million trigram types. Of those, only about 1.1 mil-
lion trigram types occurred in the WSJ training set
or in the 100,000 sentence sub-sample. For the
biomedical domain, the graph had about 2.2 mil-
lion trigrams. For all our experiments we set hy-
perparameters as follows: for graph propagation,
? = 0.5, ? = 0.01, for Viterbi decoding mixing,
? = 0.6, for CRF re-training, ? = 0.001, ? = 0.01.
These parameters were chosen based on develop-
ment set performance. All CRF objectives were op-
timized using L-BFGS (Bertsekas, 2004).
Table 2 shows the results for both domains. For
the question corpus, the supervised CRF performs
at only 85% on the development set. While it is al-
most impossible to improve in-domain tagging ac-
curacy and tagging is therefore considered a solved
problem by many, these results clearly show that
the problem is far from solved. Self-training im-
proves over the baseline by about 0.6% on the de-
velopment set. However the gains from self-training
are more modest (0.2%) on the evaluation (test) set.
Our approach is able to provide a more solid im-
provement of about 3% absolute over the super-
vised baseline and about 2% absolute over the self-
trained system on the question development set. Un-
like self-training, on the question evaluation set, our
approach provides about 3% absolute improvement
over the supervised baseline. For the biomedical
data, while the performances of our approach and
self-training are statistically indistinguishable on the
development set, we see modest gains of about 0.5%
absolute on the evaluation set. On the same data, we
see that our approach provides about 1.4% absolute
improvement over the supervised baseline.
7 Analysis & Conclusion
The results suggest that our proposed approach pro-
vides higher gains relative to self-training on the
question data than on the biomedical corpus. We
hypothesize that this caused by sparsity in the graph
generated from the biomedical dataset. For the ques-
tions graph, the PMI statistics were estimated over
10 million sentences while in the case of the biomed-
ical dataset, the same statistics were computed over
just 100,000 sentences. We hypothesize that the lack
of well-estimated features in the case of the biomed-
ical dataset leads to a sparse graph.
To verify the above hypothesis, we measured the
percentage of trigrams that occur in the target do-
main (unlabeled) data that do not have any path to
a trigram in the source domain data, and the aver-
age minimum path length between a trigram in the
target data and a trigram in the source data (when
such a path exists). The results are shown in Ta-
ble 3. For the biomedical data, close to 50% of the
trigrams from the target data do not have a path to
a trigram from the source data. Even when such a
path exists, the average path length is about 22. On
174
Questions Bio
% of unlabeled trigrams
12.4 46.8not connected to
any labeled trigrams
average path length
9.4 22.4
between an unlabeled
trigram and its nearest
labeled trigram
Table 3: Analysis of the graphs constructed for the two
datasets discussed in Section 6. Unlabeled trigrams occur
in the target domain only. Labeled trigrams occur at least
once in the WSJ training data.
the other hand, for the question corpus, only about
12% of the target domain trigrams are disconnected,
and the average path length is about 9. These re-
sults clearly show the sparse nature of the biomed-
ical graph. We believe that it is this sparsity that
causes the graph propagation to not have a more no-
ticeable effect on the final performance. It is note-
worthy that making use of even such a sparse graph
does not lead to any degradation in results, which we
attribute to the choice of graph-propagation regular-
izer (Section 4.3).
We presented a simple, scalable algorithm for
training structured prediction models in a semi-
supervised manner. The approach is based on using
as a regularizer a nearest-neighbor graph constructed
over trigram types. Our results show that the ap-
proach not only scales to large datasets but also pro-
duces significantly improved tagging accuracies.
References
A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In NAACL.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Advances in Neural Information Process-
ing Systems 18, page 18.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In Proc. of the Conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Y. Bengio, O. Delalleau, and N. L. Roux, 2007. Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
D Bertsekas. 2004. Nonlinear Programming. Athena
Scientific Publishing.
J. Blitzer and J. Zhu. 2008. ACL 2008 tutorial on Semi-
Supervised learning.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML06, 23rd
International Conference on Machine Learning.
O. Chapelle, B. Scholkopf, and A. Zien. 2007. Semi-
Supervised Learning. MIT Press.
R. Collobert, F. Sinz, J. Weston, L. Bottou, and
T. Joachims. 2006. Large scale transductive svms.
Journal of Machine Learning Research.
A. Corduneanu and T. Jaakkola. 2003. On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In CAP.
R. Gupta, S. Sarawagi, and A. A. Diwan. 2009. General-
ized collective inference with symmetric clique poten-
tials. CoRR, abs/0907.0589.
G. R. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. In
UAI.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In ACL-IJCNLP ?09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1. Association for Computational Linguistics.
H. Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Thorsten Joachims. 2003. Transductive learning via
spectral graph partitioning. In Proc. of the Interna-
tional Conference on Machine Learning (ICML).
J. Judge, A. Cahill, and J. van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proceedings of the 21st International Conference
on Computational Linguist ics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 497?504.
175
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the In-
ternational Conference on Machine Learning (ICML).
N. D. Lawrence and M. I. Jordan. 2005. Semi-supervised
learning via gaussian processes. In NIPS.
PennBioIE. 2005. Mining the bibliome project. In
http://bioie.ldc.upenn.edu/.
H. J. Scudder. 1965. Probability of Error of some Adap-
tive Pattern-Recognition Machines. IEEE Transac-
tions on Information Theory, 11.
M. Seeger. 2000. Learning with labeled and unlabeled
data. Technical report, University of Edinburgh, U.K.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In ACL ?07.
V. Sindhwani, P. Niyogi, and M. Belkin. 2005. Beyond
the point cloud: from transductive to semi-supervised
learning. In Proc. of the International Conference on
Machine Learning (ICML).
A. Subramanya and J. A. Bilmes. 2009. Entropic graph
regularization in non-parametric semi-supervised clas-
sification. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL ?03.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
176
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1017?1026, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Reading The Web with Learned Syntactic-Semantic Inference Rules
Ni Lao1?, Amarnag Subramanya2, Fernando Pereira2, William W. Cohen1
1Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
2Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA
nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu
Abstract
We study how to extend a large knowledge
base (Freebase) by reading relational informa-
tion from a large Web text corpus. Previous
studies on extracting relational knowledge
from text show the potential of syntactic
patterns for extraction, but they do not exploit
background knowledge of other relations
in the knowledge base. We describe a
distributed, Web-scale implementation of a
path-constrained random walk model that
learns syntactic-semantic inference rules for
binary relations from a graph representation
of the parsed text and the knowledge base.
Experiments show significant accuracy im-
provements in binary relation prediction over
methods that consider only text, or only the
existing knowledge base.
1 Introduction
Manually-created knowledge bases (KBs) often lack
basic information about some entities and their
relationships, either because the information was
missing in the initial sources used to create the
KB, or because human curators were not confident
about the status of some putative fact, and so they
excluded it from the KB. For instance, as we will
see in more detail later, many person entries in
Freebase (Bollacker et al 2008) lack nationality
information. To fill those KB gaps, we might use
general rules, ideally automatically learned, such as
?if person was born in town and town is in country
?This research was carried out during an internship at
Google Research
then the person is a national of the country.? Of
course, rules like this may be defeasible, in this case
for example because of naturalization or political
changes. Nevertheless, many such imperfect rules
can be learned and combined to yield useful KB
completions, as demonstrated in particular with the
Path-Ranking Algorithm (PRA) (Lao and Cohen,
2010; Lao et al 2011), which learns such rules on
heterogenous graphs for link prediction tasks.
Alternatively, we may attempt to fill KB gaps by
applying relation extraction rules to free text. For
instance, Snow et al(2005) and Suchanek et al
(2006) showed the value of syntactic patterns in
extracting specific relations. In those approaches,
KB tuples of the relation to be extracted serve as
positive training examples to the extraction rule
induction algorithm. However, the KB contains
much more knowledge about other relations that
could potentially be helpful in improving relation
extraction accuracy and coverage, but that is not
used in such purely text-based approaches.
In this work, we use PRA to learn weighted
rules (represented as graph path patterns) that
combine both semantic (KB) and syntactic infor-
mation encoded respectively as edges in a graph-
structured KB, and as syntactic dependency edges
in dependency-parsed Web text. Our approach can
easily incorporate existing knowledge in extraction
tasks, and its distributed implementation scales to
the whole of the Freebase KB and 60 million parsed
documents. To the best of our knowledge, this is the
first successful attempt to apply relational learning
methods to heterogeneous data with this scale.
1017
1.1 Terminology and Notation
In this study, we use a simplified KB consisting of a
set C of concepts and a set R of labels. Each label r
denotes some binary relation partially represented in
the KB. The concrete KB is a directed, edge-labeled
graph G = (C, T ) where T ? C ? R ? C is the
set of labeled edges (also known as triples) (c, r, c?).
Each triple represents an instance r(c, c?) of the
relation r ? R. The KB may be incomplete, that
is, r(c, c?) holds in the real world but (c, r, c?) 6? T .
Our method will attempt to learn rules to infer such
missing relation instances by combining the KB
with parsed text.
We denote by r?1 the inverse relation of r:
r(c, c?) ? r?1(c?, c). For instance Parent?1 is
equivalent to Children. It is convenient to take G
as containing triple (c?, r?1, c) whenever it contains
triple (c, r, c?).
A path type in G is a sequence pi = ?r1, . . . , rm?.
An instance of the path type is a sequence of nodes
c0, . . . , cm such that ri(ci?1, ci). For instance, ?the
persons who were born in the same town as the
query person?, and ?the nationalities of persons who
were born in the same town as the query person? can
be reached respectively through paths matching the
following types
pi1 :
?
BornIn,BornIn?1
?
pi2 :
?
BornIn,BornIn?1,Nationality
?
1.2 Learning Syntactic-Semantic Rules with
Path-Constrained Random Walks
Given a query concept s ? C and a relation
r ? R, PRA begins by enumerating a large set of
bounded-length path types. These path types are
treated as ranking ?experts,? each generating some
random instance of the path type starting from s, and
ranking end nodes t by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? by using logistic
regression to predict the probability that the relation
r(s, t) holds.
In this study, we test the hypothesis that PRA can
be used to find useful ?syntactic-semantic patterns?
? that is, patterns that exploit both semantic
and syntactic relationships, thereby using semantic
knowledge as background in interpreting syntactic
 
wrote
She
Mention
dobj
Charlotte
was
nsubj
nsubj
Jane Eyre
Charlotte
Bronte
Mention
Jane Eyre
Mention
Coreference Resolution
Entity 
Resolution
Freebase
News Corpus
Dependency Trees
Write
Patrick Bront?HasFather
?
Profession
Writer
Figure 1: Knowledge base and parsed text as a labeled
graph. For clarity, some word nodes are omitted.
relationships. As shown in Figure 1, we extend the
KB graph G with nodes and edges from text that
has been syntactically analyzed with a dependency
parser1 and where pronouns and other anaphoric
referring expressions have been clustered with their
antecedents. The text nodes are word/phrase
instances, and the edges are syntactic dependencies
labeled by the corresponding dependency type.
Mentions of entities in the text are linked to KB
concepts by mention edges created by an entity
resolution process.
Given for instance the query
Profession(CharlotteBronte, ?), PRA produces
a ranked list of answers that may have the relation
Profession with the query node CharlotteBronte.
The features used to score answers are the
random walk probabilities of reaching a certain
profession node from the query node by paths
with particular path types. PRA can learn path
types that combine background knowledge in
the database with syntactic patterns in the text
corpus. We now exemplify some path types
involving relations described in Table 3. Type
?
M, conj,M?1,Profession
?
is active (matches
paths) for professions of persons who are mentioned
in conjunction with the query person as in
?collaboration between McDougall and Simon
1Stanford dependencies (de Marneffe and Manning, 2008).
1018
Philips?. For a somewhat subtler example, type
?
M,TW,CW?1,Profession?1,Profession
?
is active
for persons who are mentioned by their titles as in
?President Barack Obama?. The type subsequence
?
Profession?1,Profession
?
ensures that only
profession concepts are activated. The features
generated from these path types combine syntactic
dependency relations (conj) and textual information
relations (TW and CW) with semantic relations in
the KB (Profession).
Experiments on three Freebase relations (profes-
sion, nationality and parents) show that exploiting
existing background knowledge as path features
can significantly improve the quality of extraction
compared with using either Freebase or the text
corpus alone.
1.3 Related Work
Information extraction from varied unstructured and
structured sources involves both complex relational
structure and uncertainty at all levels of the extrac-
tion process. Statistical Relational Learning (SRL)
seeks to combine statistical and relational learning
methods to address such tasks. However, most SRL
approaches (Friedman et al 1999; Richardson and
Domingos, 2006) suffer the complexity of inference
and learning when applied to large scale problems.
Recently, Lao and Cohen (2010) introduced Path
Ranking algorithm, which is applicable to larger
scale problems such as literature recommendation
(Lao and Cohen, 2010) and inference on a large
knowledge base (Lao et al 2011).
Much of the previous work on automatic relation
extraction was based on certain lexico-syntactic
patterns. Hearst (1992) first noticed that patterns
such as ?NP and other NP? and ?NP such as NP?
often imply hyponym relations (NP here refers to
a noun phrase). However, such approaches to
relation extraction are limited by the availability of
domain knowledge. Later systems for extracting
arbitrary relations from text mostly use shallow
surface text patterns (Etzioni et al 2004; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002).
The idea of using sequences of dependency edges
as features for relation extraction was explored by
Snow et al(2005) and Suchanek et al(2006). They
define features to be shortest paths on dependency
trees which connect pairs of NP candidates.
This study is most closely related to work of
Mintz et al(2009), who also study the problem of
extending Freebase with extraction from parsed text.
As in our work, they use a logistic regression model
with path features. However, their approach does not
exploit existing knowledge in the KB. Furthermore,
their path patterns are used as binary-values features.
We show experimentally that fractional-valued
features generated by random walks provide much
higher accuracy than binary-valued ones.
Culotta et al(2006)?s work is similar to our
approach in the sense of relation extraction by
discovering relational patterns. However while
they focus on identifying relation mentions in text
(microreading),this work attempts to infer new
tuples by gathering path evidence over the whole
corpus (macroreading). In addition, their work
involves a few thousand examples, while we aim for
Web-scale extraction.
Do and Roth (2010) use a KB (YAGO) to
aid the generation of features from free text.
However their method is designed specifically for
extracting hierarchical taxonomic structures, while
our algorithm can be used to discover relations for
general general graph-based KBs.
In this paper we extend the PRA algorithm along
two dimensions: combining syntactic and semantic
cues in text with existing knowledge in the KB;
and a distributed implementation of the learning and
inference algorithms that works at Web scale.
2 Path Ranking Algorithm
We briefly review the Path Ranking algorithm
(PRA), described in more detail by Lao and Cohen
(2010). Each path type pi = ?r1, r2, ..., r`? specifies
a real-valued feature. For a given query-answer node
pair (s, t), the value of the feature pi is P (s? t;pi),
the probability of reaching t from s by a random
walk that instantiates the type. More specifically,
suppose that the random walk has just reached vi by
traversing edges labeled r1, . . . , ri with s=v0. Then
vi+1 is drawn at random from all nodes reachable
from vi by edges labeled ri+1. A path type pi is
active for pair (s, t) if P (s? t;pi) > 0.
Let B = {?, pi1, ..., pin} be the set of all path
types of length no greater than ` that occur in
the graph together with the dummy type ?, which
1019
represents the bias feature. For convenience, we set
P (s ? t;?) = 1 for any nodes s, t. The score for
whether query node s is related to another node t by
relation r is given by
score(s, t) =
?
pi?B
P (s? t;pi)?pi ,
where ?pi is the weight of feature pi. The model
parameters to be learned are the vector ? =
??pi?pi?B . The procedures used to discover B and
estimate ? are described in the following. Finally,
note that we train a separate PRA model for each
relation r.
Path Discovery: Given a graph and a target
relation r, the total number of path types is an
exponential function of the maximum path length
` and considering all possible paths would be
computationally very expensive. As a result, B is
constructed using only path types that satisfy the
following two constraints:
1. the path type is active for more than K training
query nodes, and
2. the probability of reaching any correct target
node t is larger than a threshold ? on average
for the training query nodes s.
We will discuss how K, ? and the training queries
are chosen in Section 5. In addition to making the
training more efficient, these constraints are also
helpful in removing low quality path types.
Training Examples: For each relation r of inter-
est, we start with a set of node pairs Sr = {(si, ti)}.
From Sr, we create the training setDr = {(xi, yi)},
where xi = ?P (si ? ti;pi)?pi?B is the vector
of path feature values for the pair (si, ti), and yi
indicates whether r(si, ti) holds.
Following previous work (Lao and Cohen, 2010;
Mintz et al 2009), node pairs that are in r in
the KB are legitimate positive training examples2.
One can generate negative training examples by
considering all possible pairs of concepts whose
type is compatible with r (as given by the schema)
and are not present in the KB. However this
2In our experiments we subsample the positive examples.
See section 3.2 for more details.
procedure leads to a very large number of negative
examples (e.g., for the parents relation, any pair of
person concepts which are related by this relation
would be valid negative examples) which not only
makes training very expensive but also introduces
an incorrect bias in the training set. Following
Lao and Cohen (2010) we use a simple biased
sampling procedure to generate negative examples:
first, the path types discovered in the previous (path
discovery) step are used to construct an initial PRA
model (all feature weights are set to 1.0); then, for
each query node si, this model is used to retrieve
candidate answer nodes, which are then sorted in
descending order by their scores; finally, nodes at
the k(k + 1)/2-th positions are selected as negative
samples, where k = 0, 1, 2, ....
Logistic Regression Training: Given a training
set D, we estimate parameters ? by maximizing the
following objective
F(?) =
1
|D|
?
(x,y)?D
f(x, y;?)? ?1???1 ? ?2???22
where ?1 and ?2 control the strength of the L1-
regularization which helps with structure selection
and L22-regularization which prevents overfitting.
The log-likelihood f(x, y;?) of example (x, y) is
given by
f(x, y,?) = y ln p(x,?) + (1? y) ln(1? p(x,?))
p(x,?) =
exp(?Tx)
1 + exp(?Tx)
.
Inference: After a model is trained for a relation
r in the knowledge base, it can be used to produce
new instances of r. We first generate unlabeled
queries s which belong to the domain of r. Queries
which appear in the training set are excluded. For
each unlabeled query node s, we apply the trained
PRA model to generate a list of candidate t nodes
together with their scores. We then sort all the
predictions (s, t) by their scores in descending order,
and evaluate the top ones.
3 Extending PRA
As described in the previous section, the PRA model
is trained on positive and negative queries generated
from the KB. As Freebase contains millions of
1020
concepts and edges, training on all the generated
queries is computationally challenging. Further,
we extend the Freebase graph with parse paths of
mentions of concepts in Freebase in millions of Web
pages. Yet another issue is that the training queries
generated using Freebase are inherently biased
towards the distribution of concepts in Freebase
and may not reflect the distribution of mentions of
these concepts in text data. As one of the goals of
our approach is to learn relation instances that are
missing in Freebase, training on such a set biased
towards the distribution of concepts in Freebase may
not lead to good performance. In this section we
explain how we modified the PRA algorithm to
address those issues.
3.1 Scaling Up
Most relations in Freebase have a large set of
existing triples. For example, for the profession
relation, there are around 2 million persons in
Freebase, and about 0.3 million of them have known
professions. This results in more than 0.3 million
training queries (persons), each with one or more
positive answers (professions), and many negative
answers, which make training computationally
challenging. Generating all the paths for millions
of queries over a graph with millions of concepts
and edges further complicates the computational
issues. Incorporating the parse path features from
the text only exacerbates the matter. Finally once we
have trained a PRA model for a given relation, say
profession, we would like to infer the professions for
all the 1.7 million persons whose professions are not
known to Freebase (and possibly predict changes to
the profession information of the 0.3 million people
whose professions were given).
We use distributed computing to deal with the
large number of training and prediction queries
over a large graph. A key observation is that the
different stages of the PRA algorithm are based
on independent computations involving individual
queries. Therefore, we can use the MapReduce
framework to distribute the computation (Dean and
Ghemawat, 2008). For path discovery, we modify
Lao et als path finding (2011) approach to decouple
the queries: instead of using one depth-first search
that involves all the queries, we first find all paths
up to certain length for each query node in the
map stage, and then collect statistics for each path
from all the query nodes in the reduce stage. We
used a 500-machine, 8GB/machine cluster for these
computations.
Another challenge associated with applying PRA
to a graph constructed using a large amounts of
text is that we cannot load the entire graph on a
single machine. To circumvent this problem, we first
index all parsed sentences by the concepts that they
mention. Therefore, to perform a random walk for a
query concept s, we only load the sentences which
mention s.
3.2 Sampling Training Data
Using the r-edges in the KB as positive examples
distorts the training set. For example, for the
profession relation, there are 0.3 million persons
for whom Freebase has profession information, and
amongst these 0.24 million are either politicians
or actors. This may not reflect the distribution
of professions of persons mentioned in Web data.
Using all of these as training queries will most
certainly bias the trained model towards these
professions as PRA is trained discriminatively. In
other words, training directly with this data would
lead to a model that is more likely to predict
professions that are popular in Freebase. To avoid
this distortion, we use stratified sampling. For each
relation r and concept t ? C, we count the number
of r edges pointing to t
Nr,t = |{(s, r, t) ? T}| .
Given a training query (s, r, t) we sample it
according to
Pr,t = min
(
1,
?
m+Nr,t
Nr,t
)
We fix m = 100 in our experiments. If we take the
profession relation as an example, the above implies
that for popular professions, we only sample about
?
Nr,t out of the Nr,t possible queries that end in t,
whereas for the less popular professions we would
accept all the training queries.
3.3 Text Graph Construction
As we are processing Web text data (see following
section for more detail), the number of mentions
1021
of a concept follows a somewhat heavy-tailed
distribution: there are a small number of very
popular concepts (head) and a large number of not
so popular concepts (tail). For instance the concept
BarackObama is mentioned about 8.9 million times
in our text corpus. To prevent the text graph from
being dominated by the head concepts, for each
sentence that mentions concept c ? C, we accept
it as part of the text graph with probability:
Pc = min
(
1,
?
k + Sc
Sc
)
where Sc is the number of sentences in which c is
mentioned in the whole corpus. In our experiments
we use k = 105. This means that if Sc  k, then we
only sample about
?
Sc of the sentences that contain
a mention of the concept, while if Sc  k, then all
mentions of that concept will likely be included.
4 Datasets
We use Freebase as our knowledge base. Freebase
data is harvested from many sources, including
Wikipedia, AMG, and IMDB.3 As of this writing,
it contains more than 21 million concepts and 70
million labeled edges. For a large majority of con-
cepts that appear both in Freebase and Wikipedia,
Freebase maintains a link to the Wikipedia page of
that concept.
We also collect a large Web corpus and identify
60 million pages that mention concepts relevant
to this study. The free text on those pages
are POS-tagged and dependency parsed with an
accuracy comparable to that of the current Stanford
dependency parser (Klein and Manning, 2003). The
parser produces a dependency tree for each sentence
with each edge labeled with a standard dependency
tag (see Figure 1).
In each of the parsed documents, we use POS tags
and dependency edges to identify potential referring
noun phrases (NPs). We then use a within-document
coreference resolver comparable to that of Haghighi
and Klein (2009) to group referring NPs into
co-referring clusters. For each cluster that contains a
proper-name mention, we find the Freebase concept
or concepts, if any, with a name or alias that matches
3www.wikipedia.org, www.allmusic.com, www.
imdb.com.
Table 1: Size of training and test sets for each relation.
Task Training Set Test Set
Profession 22,829 15,219
Nationality 14,431 9,620
Parents 21,232 14,155
the mention. If a cluster has multiple possible
matching Freebase concepts, we choose a single
sense based on the following simple model. For
each Freebase concept c ? C, we computeN(c,m),
the number of times the concept c is referred by
mention m by using both the alias information
in Freebase and the anchors of the corresponding
Wikipedia page for that concept. Based on N(c,m)
we can calculate the empirical probability p(c|m) =
N(c,m)/
?
c? N(c
?,m). If u is a cluster with
mention set M(u) in the document, and C(m) the
set of concepts in KB with name or alias m, we
assign u to concept c? = argmax
c?C(m),m?M(u)
p(c|m),
provided that there exists at least one c ? C(m) and
m ? M(u) such that p(c|m) > 0. Note that M(c)
only contains the proper-name mentions in cluster c.
5 Results
We use three relations profession, nationality and
parents for our experiments. For each relation, we
select its current set of triples in Freebase, and apply
the stratified sampling (Section 3.2) to each of the
three triple sets. The resulting triple sets are then
randomly split into training (60% of the triples) and
test (the remaining triples). However, the parents
relation yields 350k triples after stratified sampling,
so to reduce experimental effort we further randomly
sub-sample 10% of that as input to the train-test
split. Table 1 shows the sizes of the training and
test sets for each relation.
To encourage PRA to find paths involving the
text corpus, we do not count relation M (which
connects concepts to their mentions) or M?1 when
calculating path lengths. We use L1/L22-regularized
logistic regression to learn feature weights. The
PRA hyperparameters (? and K as defined in
Section 2) and regularizer hyperparameters are
tuned by threefold cross validation (CV) on the
training set. We average the models across all
the folds and choose the model that gives the best
1022
Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and
KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and
text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results
shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions
significance test.
Task KB Text KB+Text KB+Text[b]
Profession 0.532 0.516 0.583 0.453
Nationality 0.734 0.729 0.812 0.693
Parents 0.329 0.332 0.392 0.319
performance on the training set for each relation.
We report results of two evaluations. First, we
evaluate the performance of the PRA algorithm
when trained on a subset of existing Freebase facts
and tested on the rest. Second, we had human
annotators verify facts proposed by PRA that are not
in Freebase.
5.1 Evaluation with Existing Knowledge
Previous work in relation extraction from parsed
text (Mintz et al 2009) has mostly used binary
features to indicate whether a pattern is present in
the sentences where two concepts are mentioned.
To investigate the benefit of having fractional valued
features generated by random walks (as in PRA), we
also evaluate a binarized PRA approach, for which
we use the same syntactic-semantic pattern features
as PRA does, but binarize the feature values from
PRA: if the original fractional feature value was
zero, the feature value is set to zero (equivalent to
not having the feature in that example), otherwise it
is set to 1.
Table 2 shows a comparison of the results
obtained using the PRA algorithm trained using
only Freebase (KB), using only the text corpus
graph (Text), trained with both Freebase and the
text corpus (KB+Text) and the binarized PRA
algorithm using both Freebase and the text corpus
(KB+Text[b]). We report Mean Reciprocal Rank
(MRR) where, given a set of queries Q,
MRR =
1
|Q|
?
q?Q
1
rank of q?s first correct answer
.
Comparing the results of first three columns we
see that combining Freebase and text achieves
significantly better results than using either Freebase
or text alone. Further comparing the results of last
two columns we also observe a significant drop in
MRR for the binarized version of PRA. This clearly
shows the importance of using the random walk
probabilities. It can also be seen that the MRR for
the parents relation is lower than those for other
relations. This is mainly because there are larger
number of potential answers for each query node of
Parent relation than for each query node of the other
two relations ? all persons in Freebase versus all
professions or nationalities. Finally, it is important
to point out that our evaluations are actually lower
bounds of actual performance, because, for instance,
a person might have a profession besides the ones in
Freebase and in such cases, this evaluation does not
give any credit for predicting those professions ?
they are treated as errors. We try to address this issue
with the manual evaluations in the next section.
Table 2 only reports results for the maximum path
length ` = 4 case. We found that shorter maximum
path lengths give worse results: for instance, with
` = 3 for the profession relation, MRR drops to
0.542, from 0.583 for ` = 4 when using both
Freebase and text. This difference is significant
at the 0.0001 level according to a difference of
proportions test. Further we find that using longer
path length takes much longer time to train and test,
but does not lead to significant improvements over
the ` = 4 case. For example, for profession, ` = 5
gives a MRR of 0.589.
Table 3 shows the top weighted features that
involve text edges for PRA models trained on both
Freebase and the text corpus. To make them
easier to understand, we group them based on their
functionality. For the profession and nationality
tasks, the conjunction dependency relation (in group
1,4) plays an important role: these features first find
persons mentioned in conjunction with the query
1023
Table 3: Top weighted path types involving text edges for each task grouped according to functionality. M relations
connect each concept in knowledge base to its mentions in the corpus. TW relations connect each token in a sentence to
the words in the text representation of this token. CW relations connect each concept in knowledge base to the words
in the text representation of this concept. We use lower case names to denote dependency edges, word capitalized
names to denote KB edges, and ??1 ? to denote the inverse of a relation.
Profession Top Weighted Features Comments
1
?
M, conj,M?1,Profession
?
Professions of persons mentioned in conjunction
with the query person: ?McDougall and Simon
Phillips collaborated ...?
?
M, conj?1,M?1,Profession
?
2
?
M,TW,CW?1,Profession?1,Profession
?
Active if a person is mentioned by his profession:
?The president said ...?
3
?
M,TW,TW?1,M?1,Children,Profession
?
First find persons with similar names or
mentioned in similar ways, then aggregate the
professions of their children/parents/advisors:
starting from the concept BarackObama, words
such as ?Obama?, ?leader?, ?president?, and
?he? are reachable through path ?M,TW?
?
M,TW,TW?1,M?1,Parents,Profession
?
?
M,TW,TW?1,M?1,Advisors,Profession
?
Nationality Top Weighted Features Comments
4
?
M, conj,TW,CW?1,Nationality
?
The nationalities of persons mentioned in
conjunction with the query person: ?McDougall
and Simon Phillips collaborated ...?
?
M, conj?1,TW,CW?1,Nationality
?
5
?
M, nc?1,TW,CW?1,Nationality
?
The nationalities of persons mentioned close to
the query person through other dependency
relations.
?
M, tmod?1,TW,CW?1,Nationality
?
?
M, nn,TW,CW?1,Nationality
?
6
?
M, poss, poss?1,M?1,PlaceOfBirth,ContainedBy
?
The birth/death places of the query person with
restrictions to different syntactic constructions.
?
M, title, title?1,M?1,PlaceOfDeath,ContainedBy
?
Parents Top Weighted Features Comments
7
?
M,TW,CW?1,Parents
?
The parents of persons with similar names or
mentioned in similar ways: starting from the
concept CharlotteBronte words such as
?Bronte?, ?Charlotte?, ?Patrick??, and ?she? are
reachable through path ?M,TW?.
8
?
M, nsubj, nsubj?1,TW,CW?1
?
Persons with similar names or mentioned in
similar ways to the query person with various
restrictions or expansions.
?
nsubj, nsubj?1
?
and
?
nc?1, nc
?
require the query to be subject and
noun compound respectively.
?
TW?1,TW
?
expands further by word similarities.
?
M, nsubj, nsubj?1,M?1,CW,CW?1
?
?
M, nc?1, nc,TW,CW?1
?
?
M,TW,CW?1
?
?
M,TW,TW?1,TW,CW?1
?
1024
person, and then find their professions or nation-
alities. The features in group 2 capture the fact
that sometimes people are mentioned by their pro-
fessions. The subpath
?
Profession?1,Profession
?
ensures that only profession related concepts are
activated. Features in group 3 first find persons
with similar names or mentioned in similar ways
to the query person, and then aggregate the
professions of their children, parents, or advisors.
Features in group 6 can be seen as special
versions of feature ?PlaceOfBirth,ContainedBy?
and ?PlaceOfDeath,ContainedBy?. The subpaths
?
M, poss, poss?1,M?1
?
and
?
M, title, title?1,M?1
?
return the random walks back to the query node only
if the mentions of the query node have poss (stands
for possessive modifier, e.g. ?Bill?s clothes?) or title
(stands for person?s title, e.g. ?President Obama?)
edges in text; otherwise these features are inactive.
Therefore, these features are active only for specific
subsets of queries. Features in group 8 generally find
persons with similar names or mentioned in similar
ways to the query person. However, they further
expand or restrict this person set in various ways.
Typically, each trained model includes hundreds
of paths with non-zero weights, so the bulk of
classifications are not based on a few high-precision-
recall patterns, but rather on the combination of
a large number of lower-precision high-recall or
high-precision lower-recall rules.
5.2 Manual Evaluation
We performed two sets of manual evaluations. In
each case, an annotator is presented with the triples
predicted by PRA, and asked if they are correct. The
annotator has access to the Freebase and Wikipedia
pages for the concepts (and is able to issue search
queries about the concepts).
In the first evaluation, we compared the perfor-
mance of two PRA models, one trained using the
stratified sampled queries and another trained using
a randomly sampled set of queries for the profession
relation. For each model, we randomly sample 100
predictions from the top 1000 predictions (sorted by
the scores returned by the model). We found that the
PRA model trained with stratified sampled queries
has 0.92 precision, while the other model has only
0.84 precision (significant at the 0.02 level). This
shows that stratified sampling leads to improved
Table 4: Human judgement for predicted new beliefs.
Task p@100 p@1k p@10k
Profession 0.97 0.92 0.84
Nationality 0.98 0.97 0.90
Parents 0.86 0.81 0.79
performance.
We also evaluated the new beliefs proposed by
the models trained for all the three relations using
stratified sampled queries. We estimated precision
for the top 100 predictions and randomly sampled
100 predictions each from the top 1,000 and 10,000
predictions. Here we use the PRA model trained
using both KB and text. The results of this
evaluation are shown in Table 4. It can be seen
that the PRA model is able to produce very high
precision predications even when one considers the
top 10,000 predictions.
Finally, note that our model is inductive. For
instance, for the profession relation, we are able to
predict professions for the around 2 million persons
in Freebase. The top 1000 profession facts extracted
by our system involve 970 distinct people, the top
10,000 facts involve 8,726 distinct people, and the
top 100,000 facts involve 79,885 people.
6 Conclusion
We have shown that path constrained random walk
models can effectively infer new beliefs from a
large scale parsed text corpus with background
knowledge. Evaluation by human annotators shows
that by combining syntactic patterns in parsed
text with semantic patterns in the background
knowledge, our model can propose new beliefs
with high accuracy. Thus, the proposed random
walk model can be an effective way to automate
knowledge acquisition from the web.
There are several interesting directions to con-
tinue this line of work. First, bidirectional search
from both query and target nodes can be an efficient
way to discover long paths. This would especially
useful for parsed text. Second, relation paths that
contain constant nodes (lexicalized features) and
conjunction of random walk features are potentially
very useful for extraction tasks.
1025
Acknowledgments
We thank Rahul Gupta, Michael Ringgaard, John
Blitzer and the anonymous reviewers for helpful
comments. The first author was supported by a
Google Research grant.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries, DL ?00, pages 85?94, New York, NY, USA.
ACM.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
296?303, New York City, USA, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Chris Manning.
2008. Stanford dependencies. http:
//www.tex.ac.uk/cgi-bin/texfaq2html?
label=citeURL.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113, January.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1099?1109, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th
international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi
Pfeffer. 1999. Learning Probabilistic Relational
Models. In IJCAI, volume 16, pages 1300?1309.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1152?1161, Singapore, August. Association for
Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proceedings
of COLING-92, pages 539?545. Association for
Computational Linguistics, August.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
423?430. Association for Computational Linguistics,
July.
Ni Lao and William Cohen. 2010. Relational retrieval
using a combination of path-constrained random
walks. Machine Learning, 81:53?67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large
scale knowledge base. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 529?539, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question answering
system. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 41?47, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304, Cambridge,
MA. NIPS Foundation, MIT Press.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?06, pages 712?717, New York, NY, USA.
ACM.
1026
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793?803,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cross-Document Coreference Using
Distributed Inference and Hierarchical Models
Sameer Singh? Amarnag Subramanya? Fernando Pereira? Andrew McCallum?
? Department of Computer Science, University of Massachusetts, Amherst MA 01002
? Google Research, Mountain View CA 94043
sameer@cs.umass.edu, asubram@google.com, pereira@google.com, mccallum@cs.umass.edu
Abstract
Cross-document coreference, the task of
grouping all the mentions of each entity in a
document collection, arises in information ex-
traction and automated knowledge base con-
struction. For large collections, it is clearly
impractical to consider all possible groupings
of mentions into distinct entities. To solve
the problem we propose two ideas: (a) a dis-
tributed inference technique that uses paral-
lelism to enable large scale processing, and
(b) a hierarchical model of coreference that
represents uncertainty over multiple granular-
ities of entities to facilitate more effective ap-
proximate inference. To evaluate these ideas,
we constructed a labeled corpus of 1.5 million
disambiguated mentions in Web pages by se-
lecting link anchors referring to Wikipedia en-
tities. We show that the combination of the
hierarchical model with distributed inference
quickly obtains high accuracy (with error re-
duction of 38%) on this large dataset, demon-
strating the scalability of our approach.
1 Introduction
Given a collection of mentions of entities extracted
from a body of text, coreference or entity resolu-
tion consists of clustering the mentions such that
two mentions belong to the same cluster if and
only if they refer to the same entity. Solutions to
this problem are important in semantic analysis and
knowledge discovery tasks (Blume, 2005; Mayfield
et al, 2009). While significant progress has been
made in within-document coreference (Ng, 2005;
Culotta et al, 2007; Haghighi and Klein, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009; Haghighi and Klein, 2010), the larger prob-
lem of cross-document coreference has not received
as much attention.
Unlike inference in other language processing
tasks that scales linearly in the size of the corpus,
the hypothesis space for coreference grows super-
exponentially with the number of mentions. Conse-
quently, most of the current approaches are devel-
oped on small datasets containing a few thousand
mentions. We believe that cross-document coref-
erence resolution is most useful when applied to a
very large set of documents, such as all the news ar-
ticles published during the last 20 years. Such a cor-
pus would have billions of mentions. In this paper
we propose a model and inference algorithms that
can scale the cross-document coreference problem
to corpora of that size.
Much of the previous work in cross-document
coreference (Bagga and Baldwin, 1998; Ravin and
Kazi, 1999; Gooi and Allan, 2004; Pedersen et al,
2006; Rao et al, 2010) groups mentions into entities
with some form of greedy clustering using a pair-
wise mention similarity or distance function based
on mention text, context, and document-level statis-
tics. Such methods have not been shown to scale up,
and they cannot exploit cluster features that cannot
be expressed in terms of mention pairs. We provide
a detailed survey of related work in Section 6.
Other previous work attempts to address some of
the above concerns by mapping coreference to in-
ference on an undirected graphical model (Culotta
et al, 2007; Poon et al, 2008; Wellner et al, 2004;
Wick et al, 2009a). These models contain pair-
wise factors between all pairs of mentions captur-
ing similarity between them. Many of these mod-
els also enforce transitivity and enable features over
793
Filmmaker
Rapper
BEIJING, Feb. 21? Kevin Smith, who played the god of war in the "Xena"...
... The Physiological Basis of Politics,? by Kevin B. Smith, Douglas Oxley, Matthew Hibbing...
The filmmaker Kevin Smith returns to the role of Silent Bob...
Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...
Firefighter Kevin Smith spent almost 20 years preparing for Sept. 11. When he...
...shorthanded backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth...
...were coming,'' said Dallas cornerback Kevin Smith. ''We just didn't know when...
...during the late 60's and early 70's, Kevin Smith worked with several local...
...the term hip-hop is attributed to Lovebug Starski. What does it actually mean...
Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...
Cornerback
Firefighter
Actor
Running back
Author
Figure 1: Cross-Document Coreference Problem: Example mentions of ?Kevin Smith? from New York
Times articles, with the true entities shown on the right.
entities by including set-valued variables. Exact in-
ference in these models is intractable and a number
of approximate inference schemes (McCallum et al,
2009; Rush et al, 2010; Martins et al, 2010) may
be used. In particular, Markov chain Monte Carlo
(MCMC) based inference has been found to work
well in practice. However as the number of men-
tions grows to Web scale, as in our problem of cross-
document coreference, even these inference tech-
niques become infeasible, motivating the need for
a scalable, parallelizable solution.
In this work we first distribute MCMC-based in-
ference for the graphical model representation of
coreference. Entities are distributed across the ma-
chines such that the parallel MCMC chains on the
different machines use only local proposal distribu-
tions. After a fixed number of samples on each ma-
chine, we redistribute the entities among machines
to enable proposals across entities that were pre-
viously on different machines. In comparison to
the greedy approaches used in related work, our
MCMC-based inference provides better robustness
properties.
As the number of mentions becomes large, high-
quality samples for MCMC become scarce. To
facilitate better proposals, we present a hierarchi-
cal model. We add sub-entity variables that repre-
sent clusters of similar mentions that are likely to
be coreferent; these are used to propose composite
jumps that move multiple mentions together. We
also introduce super-entity variables that represent
clusters of similar entities; these are used to dis-
tribute entities among the machines such that similar
entities are assigned to the same machine. These ad-
ditional levels of hierarchy dramatically increase the
probability of beneficial proposals even with a large
number of entities and mentions.
To create a large corpus for evaluation, we iden-
tify pages that have hyperlinks to Wikipedia, and ex-
tract the anchor text and the context around the link.
We treat the anchor text as the mention, the con-
text as the document, and the title of the Wikipedia
page as the entity label. Using this approach, 1.5
million mentions were annotated with 43k entity la-
bels. On this dataset, our proposed model yields a
B3 (Bagga and Baldwin, 1998) F1 score of 73.7%,
improving over the baseline by 16% absolute (corre-
sponding to 38% error reduction). Our experimen-
tal results also show that our proposed hierarchical
model converges much faster even though it contains
many more variables.
2 Cross-document Coreference
The problem of coreference is to identify the sets of
mention strings that refer to the same underlying en-
tity. The identities and the number of the underlying
entities is not known. In within-document corefer-
ence, the mentions occur in a single document. The
number of mentions (and entities) in each document
is usually in the hundreds. The difficulty of the task
arises from a large hypothesis space (exponential in
the number of mentions) and challenge in resolv-
ing nominal and pronominal mentions to the correct
named mentions. In most cases, named mentions
794
are not ambiguous within a document. In cross-
document coreference, the number of mentions and
entities is in the millions, making the combinatorics
even more daunting. Furthermore, naming ambigu-
ity is much more common as the same string can
refer to multiple entities in different documents, and
distinct strings may refer to the same entity in differ-
ent documents.
We show examples of ambiguities in Figure 1.
Resolving the identity of individuals with the same
name is a common problem in cross-document
coreference. This problem is further complicated
by the fact that in some situations, these individ-
uals may belong to the same field. Another com-
mon ambiguity is that of alternate names, in which
the same entity is referred to by different names or
aliases (e.g. ?Bill? is often used as a substitute for
?William?). The figure also shows an example of
the renaming ambiguity ? ?Lovebug Starski? refers
to ?Kevin Smith?, and this is an extreme form of al-
ternate names. Rare singleton entities (like the fire-
fighter) that may appear only once in the whole cor-
pus are also often difficult to isolate.
2.1 Pairwise Factor Model
Factor graphs are a convenient representation for a
probability distribution over a vector of output vari-
ables given observed variables. The model that we
use for coreference represents mentions (M) and en-
tities (E) as random variables. Each mention can
take an entity as its value, and each entity takes a set
of mentions as its value. Each mention also has a
feature vector extracted from the observed text men-
tion and its context. More precisely, the probability
of a configuration E = e is defined by
p(e) ? exp
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
where factor ?a represents affinity between men-
tions that are coreferent according to e, and factor
?r represents repulsion between mentions that are
not coreferent. Different factors are instantiated for
different predicted configurations. Figure 2 shows
the model instantiated with five mentions over a two-
entity hypothesis.
For the factor potentials, we use cosine sim-
ilarity of mention context pairs (?mn) such that
m1
m2
m3
m4
m5
e1
e2
Figure 2: Pairwise Coreference Model: Factor
graph for a 2-entity configuration of 5 mentions.
Affinity factors are shown with solid lines, and re-
pulsion factors with dashed lines.
?a(m,n) = ?mn ? b and ?r(m,n) = ?(?mn ? b),
where b is the bias. While one can certainly make
use of a more sophisticated feature set, we leave this
for future work as our focus is to scale up inference.
However, it should be noted that this approach is
agnostic to the particular set of features used. As
we will note in the next section, we do not need to
calculate features between all pairs of mentions (as
would be prohibitively expensive for large datasets);
instead we only compute the features as and when
required.
2.2 MCMC-based Inference
Given the above model of coreference, we seek the
maximum a posteriori (MAP) configuration:
e? = argmaxe p(e)
= argmaxe
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
Computing e? exactly is intractable due to the
large space of possible configurations.1 Instead,
we employ MCMC-based optimization to discover
the MAP configuration. A proposal function q is
used to propose a change e? to the current config-
uration e. This jump is accepted with the following
Metropolis-Hastings acceptance probability:
?(e, e?) = min
(
1,
(
p(e?)
p(e)
)1/t q(e)
q(e?)
)
(1)
1Number of possible entities is Bell(n) in the number of
mentions, i.e. number of partitions of n items
795
where t is the annealing temperature parameter.
MCMC chains efficiently explore the high-
density regions of the probability distribution. By
slowly reducing the temperature, we can decrease
the entropy of the distribution to encourage con-
vergence to the MAP configuration. MCMC has
been used for optimization in a number of related
work (McCallum et al, 2009; Goldwater and Grif-
fiths, 2007; Changhe et al, 2004).
The proposal function moves a randomly chosen
mention l from its current entity es to a randomly
chosen entity et. For such a proposal, the log-model
ratio is:
log
p(e?)
p(e)
=
?
m?et
?a(l,m) +
?
n?es
?r(l, n)
?
?
n?es
?a(l, n)?
?
m?et
?r(l,m) (2)
Note that since only the factors between mention l
and mentions in es and et are involved in this com-
putation, the acceptance probability of each proposal
is calculated efficiently.
In general, the model may contain arbitrarily
complex set of features over pairs of mentions, with
parameters associated with them. Given labeled
data, these parameters can be learned by Percep-
tron (Collins, 2002), which uses the MAP config-
uration according to the model (e?). There also exist
more efficient training algorithms such as SampleR-
ank (McCallum et al, 2009; Wick et al, 2009b) that
update parameters during inference. However, we
only focus on inference in this work, and the only
parameter that we set manually is the bias b, which
indirectly influences the number of entities in e?. Un-
less specified otherwise, in this work the initial con-
figuration for MCMC is the singleton configuration,
i.e. all entities have a size of 1.
This MCMC inference technique, which has been
used in McCallum and Wellner (2004), offers sev-
eral advantages over other inference techniques: (a)
unlike message-passing-methods, it does not require
the full ground graph, (b) we only have to exam-
ine the factors that lie within the changed entities
to evaluate a proposal, and (c) inference may be
stopped at any point to obtain the current best con-
figuration. However, the super exponential nature of
the hypothesis space in cross-doc coreference ren-
ders this algorithm computationally unsuitable for
large scale coreference tasks. In particular, fruit-
ful proposals (that increase the model score) are ex-
tremely rare, resulting in a large number of propos-
als that are not accepted. We describe methods to
speed up inference by 1) evaluating multiple pro-
posal simultaneously (Section 3), and 2) by aug-
menting our model with hierarchical variables that
enable better proposal distributions (Section 4).
3 Distributed MAP Inference
The key observation that enables distribution is that
the acceptance probability computation of a pro-
posal only examines a few factors that are not com-
mon to the previous and next configurations (Eq. 2).
Consider a pair of proposals, one that moves men-
tion l from entity es to entity et, and the other that
moves mention l? from entity e?s to entity e
?
t. The
set of factors to compute acceptance of the first pro-
posal are factors between l and mentions in es and
et, while the set of factors required to compute ac-
ceptance of the second proposal lie between l? and
mentions in e?s and e
?
t. Since these set of factors
are completely disjoint from each other, and the re-
sulting configurations do not depend on each other,
these two proposals are mutually-exclusive. Differ-
ent orders of evaluating such proposals are equiv-
alent, and in fact, these proposals can be proposed
and evaluated concurrently. This mutual-exclusivity
is not restricted only to pairs of proposals; a set of
proposals are mutually-exclusive if no two propos-
als require the same factor for evaluation.
Using this insight, we introduce the following ap-
proach to distributed cross-document coreference.
We divide the mentions and entities among multiple
machines, and propose moves of mentions between
entities assigned to the same machine. These jumps
are evaluated exactly and accepted without commu-
nication between machines. Since acceptance of a
mention?s move requires examining factors that lie
between other mentions in its entity, we ensure that
all mentions of an entity are assigned the same ma-
chine. Unless specified otherwise, the distribution is
performed randomly. To enable exploration of the
complete configuration space, rounds of sampling
are interleaved by redistribution stages, in which the
entities are redistributed among the machines (see
Figure 3). We use MapReduce (Dean and Ghe-
796
Distributor
Inference
Inference
Figure 3: Distributed MCMC-based Inference:
Distributor divides the entities among the machines,
and the machines run inference. The process is re-
peated by the redistributing the entities.
mawat, 2004) to manage the distributed computa-
tion.
This approach to distribution is equivalent to in-
ference with all mentions and entities on a single
machine with a restricted proposer, but is faster
since it exploits independencies to propose multiple
jumps simultaneously. By restricting the jumps as
described above, the acceptance probability calcu-
lation is exact. Partitioning the entities and propos-
ing local jumps are restrictions to the single-machine
proposal distribution; redistribution stages ensure
the equivalent Markov chains are still irreducible.
See Singh et al (2010) for more details.
4 Hierarchical Coreference Model
The proposal function for MCMC-based MAP infer-
ence presents changes to the current entities. Since
we use MCMC to reach high-scoring regions of the
hypothesis space, we are interested in the changes
that improve the current configuration. But as the
number of mentions and entities increases, these
fruitful samples become extremely rare due to the
blowup in the possible space of configurations, re-
sulting in rejection of a large number of proposals.
By distributing as described in the previous section,
we propose samples in parallel, improving chances
of finding changes that result in better configura-
tions. However, due to random redistribution and a
naive proposal function within each machine, a large
fraction of proposals are still wasted. We address
these concerns by adding hierarchy to the model.
4.1 Sub-Entities
Consider the task of proposing moves of mentions
(within a machine). Given the large number of
mentions and entities, the probability that a ran-
domly picked mention that is moved to a random
entity results in a better configuration is extremely
small. If such a move is accepted, this gives us ev-
idence that the mention did not belong to the pre-
vious entity, and we should also move similar men-
tions from the previous entity simultaneously to the
same entity. Since the proposer moves only a sin-
gle mention at a time, a large number of samples
may be required to discover these fruitful moves.
To enable block proposals that move similar men-
tions simultaneously, we introduce latent sub-entity
variables that represent groups of similar mentions
within an entity, where the similarity is defined by
the model. For inference, we have stages of sam-
pling sub-entities (moving individual mentions) in-
terleaved with stages of entity sampling (moving all
mentions within a sub-entity). Even though our con-
figuration space has become larger due to these ex-
tra variables, the proposal distribution has also im-
proved since it proposes composite moves.
4.2 Super-Entities
Another issue faced during distributed inference is
that random redistribution is often wasteful. For ex-
ample, if dissimilar entities are assigned to a ma-
chine, none of the proposals may be accepted. For a
large number of entities and machines, the probabil-
ity that similar entities will be assigned to the same
machine is extremely small, leading to a larger num-
ber of wasted proposals. To alleviate this problem,
we introduce super-entities that represent groups of
similar entities. During redistribution, we ensure all
entities in the same super-entity are assigned to the
same machine. As for sub-entities above, inference
switches between regular sampling of entities and
sampling of super-entities (by moving entities). Al-
though these extra variables have made the config-
uration space larger, they also allow more efficient
distribution of entities, leading to useful proposals.
4.3 Combined Hierarchical Model
Each of the described levels of the hierarchy are sim-
ilar to the initial model (Section 2.1): mentions/sub-
entities have the same structure as the entities/super-
entities, and are modeled using similar factors. To
represent the ?context? of a sub-entity we take the
union of the bags-of-words of the constituent men-
tion contexts. Similarly, we take the union of sub-
797
Super-Entities
Entities
Mentions
Sub-Entities
Figure 4: Combined Hierarchical Model with factors instantiated for a hypothesis containing 2 super-
entities, 4 entities, and 8 sub-entities, shown as colored circles, over 16 mentions. Dotted lines represent
repulsion factors and solid lines represent affinity factors (the color denotes the type of variable that the
factor touches). The boxes on factors were excluded for clarity.
entity contexts to represent the context of an entity.
The factors are instantiated in the same manner as
Section 2.1 except that we change the bias factor
b for each level (increasing it for sub-entities, and
decreasing it for super-entities). The exact values
of these biases indirectly determines the number of
predicted sub-entities and super-entities.
Since these two levels of hierarchy operate at
separate granularities from each other, we combine
them into a single hierarchical model that contains
both sub- and super-entities. We illustrate this hi-
erarchical structure in Figure 4. Inference for this
model takes a round-robin approach by fixing two
of the levels of the hierarchy and sampling the third,
cycling through these three levels. Unless specified
otherwise, the initial configuration is the singleton
configuration, in which all sub-entities, entities, and
super-entities are of size 1.
5 Experiments
We evaluate our models and algorithms on a number
of datasets. First, we compare performance on the
small, publicly-available ?John Smith? dataset. Sec-
ond, we run the automated Person-X evaluation to
obtain thousands of mentions that we use to demon-
strate accuracy and scalability improvements. Most
importantly, we create a large labeled corpus using
links to Wikipedia to explore the performance in the
large-scale setting.
5.1 John Smith Corpus
To compare with related work, we run an evalua-
tion on the ?John Smith? corpus (Bagga and Bald-
win, 1998), containing 197 mentions of the name
?John Smith? from New York Times articles (la-
beled to obtain 35 true entities). The bias b for
our approach is set to result in the correct number
of entities. Our model achieves B3 F1 accuracy of
66.4% on this dataset. In comparison, Rao et al
(2010) obtains 61.8% using the model most similar
to ours, while their best model (which uses sophis-
ticated topic-model features that do not scale easily)
achieves 69.7%. It is encouraging to note that our
approach, using only a subset of the features, per-
forms competitively with related work. However,
due to the small size of the dataset, we require fur-
ther evaluation before reaching any conclusions.
5.2 Person-X Evaluation
There is a severe lack of labeled corpora for cross-
document coreference due to the effort required
to evaluate the coreference decisions. Related
approaches have used automated Person-X evalu-
ation (Gooi and Allan, 2004), in which unique
person-name strings are treated as the true entity
labels for the mentions. Every mention string is
replaced with an ?X? for the coreference system.
We use this evaluation methodology on 25k person-
name mentions from the New York Times cor-
pus (Sandhaus, 2008) each with one of 50 unique
strings. As before, we set the bias b to achieve the
same number of entities. We use 1 million samples
in each round of inference, followed by random re-
distribution in the flat model, and super-entities in
the hierarchical model. Results are averaged over
five runs.
798
Figure 5: Person-X Evaluation of Pairwise model:
Performance as number of machines is varied, aver-
aged over 5 runs.
Number of Entities 43,928
Number of Mentions 1,567,028
Size of Largest Entity 6,096
Average Mentions per Entity 35.7
Variance of Mentions per Entity 5191.7
Table 1: Wikipedia Link Corpus Statistics. Size
of an entity is the number of mentions of that entity.
Figure 5 shows accuracy compared to relative
wallclock running time for distributed inference on
the flat, pairwise model. Speed and accuracy im-
prove as additional machines are added, but larger
number of machines lead to diminishing returns for
this small dataset. Distributed inference on our hi-
erarchical model is evaluated in Figure 6 against in-
ference on the pairwise model from Figure 5. We
see that the individual hierarchical models perform
much better than the pairwise model; they achieve
the same accuracy as the pairwise model in approx-
imately 10% of the time. Moreover, distributed in-
ference on the combined hierarchical model is both
faster and more accurate than the individual hierar-
chical models.
5.3 Wikipedia Link Corpus
To explore the application of the proposed approach
to a larger, realistic dataset, we construct a corpus
based on the insight that links to Wikipedia that ap-
pear on webpages can be treated as mentions, and
since the links were added manually by the page au-
thor, we use the destination Wikipedia page as the
Figure 6: Person-X Evaluation of Hierarchical
Models: Performance of inference on hierarchical
models compared to the pairwise model. Experi-
ments were run using 50 machines.
entity the link refers to.
The dataset is created as follows: First, we crawl
the web and select hyperlinks on webpages that link
to an English Wikipedia page.2 The anchors of
these links form our set of mentions, with the sur-
rounding block of clean text (obtained after remov-
ing markup, etc.) around each link being its con-
text. We assign the title of the linked Wikipedia
page as the entity label of that link. Since this set
of mentions and labels can be noisy, we use the
following filtering steps. All links that have less
than 36 words in their block, or whose anchor text
has a large string edit distance from the title of the
Wikipedia page, are discarded. While this results in
cases in which ?President? is discarded when linked
to the ?Barack Obama? Wikipedia page, it was nec-
essary to reduce noise. Further, we also discard
links to Wikipedia pages that are concepts (such as
?public_domain?) rather than entities. All enti-
ties with less than 6 links to them are also discarded.
Table 1 shows some statistics about our automat-
ically generated data set. We randomly sampled 5%
of the entities to create a development set, treating
the remaining entities as the test set. Unlike the
John Smith and Person-X evaluation, this data set
also contains non-person entities such as organiza-
tions and locations.
For our models, we augment the factor potentials
with mention-string similarity:
2e.g. http://en.wikipedia.org/Hillary_Clinton
799
?a/r(m,n) = ? (?mn ? b+ wSTREQ(m,n))
where STREQ is 1 if mentions m and n are string
identical (0 otherwise), and w is the weight to this
feature.3 In our experiments we found that setting
w = 0.8 and b = 1e? 4 gave the best results on the
development set.
Due to the large size of the corpus, existing cross-
document coreference approaches could not be ap-
plied to this dataset. However, since a majority
of related work consists of using clustering after
defining a similarity function (Section 6), we pro-
vide a baseline evaluation of clustering with Sub-
Square (Bshouty and Long, 2010), a scalable, dis-
tributed clustering method. Subsquare takes as in-
put a weighted graph with mentions as nodes and
similarity between mentions used as edge weights.
Subsquare works by stochastically assigning a ver-
tex to the cluster of one its neighbors if they have
significant neighborhood overlap. This algorithm
is an efficient form of approximate spectral cluster-
ing (Bshouty and Long, 2010), and since it is given
the same distances between mentions as our models,
we expect it to get similar accuracy. We also gen-
erate another baseline clustering by assigning men-
tions with identical strings to the same entity. This
mention-string clustering is also used as the initial
configuration of our inference.
Figure 7: Wikipedia Link Evaluation: Perfor-
mance of inference for different number of machines
(N = 100, 500). Mention-string match clustering is
used as the initial configuration.
3Note that we do not use mention-string similarity for John
Smith or Person-X as the mention strings are all identical.
Method
Pairwise B3 Score
P/ R F1 P/ R F1
String-Match 30.0 / 66.7 41.5 82.7 / 43.8 57.3
Subsquare 38.2 / 49.1 43.0 87.6 / 51.4 64.8
Our Model 44.2 / 61.4 51.4 89.4 / 62.5 73.7
Table 2: F1 Scores on the Wikipedia Link Data.
The results are significant at the 0.0001 level over
Subsquare according to the difference of proportions
significance test.
Inference is run for 20 rounds of 10 million sam-
ples each, distributed over N machines. We use
N = 100, 500 and the B3 F1 score results obtained
set for each case are shown in Figure 7. It can
be seen that N = 500 converges to a better solu-
tion faster, showing effective use of parallelism. Ta-
ble 2 compares the results of our approach (at con-
vergence for N = 500), the baseline mention-string
match and the Subsquare algorithm. Our approach
significantly outperforms the competitors.
6 Related Work
Although the cross-document coreference problem
is challenging and lacks large labeled datasets, its
ubiquitous role as a key component of many knowl-
edge discovery tasks has inspired several efforts.
A number of previous techniques use scoring
functions between pairs of contexts, which are then
used for clustering. One of the first approaches
to cross-document coreference (Bagga and Bald-
win, 1998) uses an idf-based cosine-distance scor-
ing function for pairs of contexts, similar to the one
we use. Ravin and Kazi (1999) extend this work to
be somewhat scalable by comparing pairs of con-
texts only if the mentions are deemed ?ambiguous?
using a heuristic. Others have explored multiple
methods of context similarity, and concluded that
agglomerative clustering provides effective means
of inference (Gooi and Allan, 2004). Pedersen et
al. (2006) and Purandare and Pedersen (2004) inte-
grate second-order co-occurrence of words into the
similarity function. Mann and Yarowsky (2003) use
biographical facts from the Web as features for clus-
tering. Niu et al (2004) incorporate information ex-
traction into the context similarity model, and anno-
tate a small dataset to learn the parameters. A num-
ber of other approaches include various forms of
800
hand-tuned weights, dictionaries, and heuristics to
define similarity for name disambiguation (Blume,
2005; Baron and Freedman, 2008; Popescu et al,
2008). These approaches are greedy and differ in the
choice of the distance function and the clustering al-
gorithm used. Daume? III and Marcu (2005) propose
a generative approach to supervised clustering, and
Haghighi and Klein (2010) use entity profiles to as-
sist within-document coreference.
Since many related methods use clustering, there
are a number of distributed clustering algorithms
that may help scale these approaches. Datta et
al. (2006) propose an algorithm for distributed k-
means. Chen et al (2010) describe a parallel spectral
clustering algorithm. We use the Subsquare algo-
rithm (Bshouty and Long, 2010) as baseline because
it works well in practice. Mocian (2009) presents a
survey of distributed clustering algorithms.
Rao et al (2010) have proposed an online deter-
ministic method that uses a stream of input mentions
and assigns them greedily to entities. Although it
can resolve mentions from non-trivial sized datasets,
the method is restricted to a single machine, which
is not scalable to the very large number of mentions
that are encountered in practice.
Our representation of the problem as an undi-
rected graphical model, and performing distributed
inference on it, provides a combination of advan-
tages not available in any of these approaches. First,
most of the methods will not scale to the hundreds
of millions of mentions that are present in real-world
applications. By utilizing parallelism across ma-
chines, our method can run on very large datasets
simply by increasing the number of machines used.
Second, approaches that use clustering are limited
to using pairwise distance functions for which ad-
ditional supervision and features are difficult to in-
corporate. In addition to representing features from
all of the related work, graphical models can also
use more complex entity-wide features (Culotta et
al., 2007; Wick et al, 2009a), and parameters can
be learned using supervised (Collins, 2002) or semi-
supervised techniques (Mann and McCallum, 2008).
Finally, the inference for most of the related ap-
proaches is greedy, and earlier decisions are not re-
visited. Our technique is based on MCMC inference
and simulated annealing, which are able to escape
local maxima.
7 Conclusions
Motivated by the problem of solving the corefer-
ence problem on billions of mentions from all of the
newswire documents from the past few decades, we
make the following contributions. First, we intro-
duce distributed version of MCMC-based inference
technique that can utilize parallelism to enable scal-
ability. Second, we augment the model with hierar-
chical variables that facilitate fruitful proposal distri-
butions. As an additional contribution, we use links
to Wikipedia pages to obtain a high-quality cross-
document corpus. Scalability and accuracy gains of
our method are evaluated on multiple datasets.
There are a number of avenues for future work.
Although we demonstrate scalability to more than a
million mentions, we plan to explore performance
on datasets in the billions. We also plan to examine
inference on complex coreference models (such as
with entity-wide factors). Another possible avenue
for future work is that of learning the factors. Since
our approach supports parameter estimation, we ex-
pect significant accuracy gains with additional fea-
tures and supervised data. Our work enables cross-
document coreference on very large corpora, and we
would like to explore the downstream applications
that can benefit from it.
Acknowledgments
This work was done when the first author was an
intern at Google Research. The authors would
like to thank Mark Dredze, Sebastian Riedel, and
anonymous reviewers for their valuable feedback.
This work was supported in part by the Center
for Intelligent Information Retrieval, the Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181., in part by an award
from Google, in part by The Central Intelligence
Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249,
in part by NSF grant #CNS-0958392, and in part
by UPenn NSF medium IIS-0803847. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
801
References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In International Conference on Computational
Linguistics, pages 79?85.
A. Baron and M. Freedman. 2008. Who is who and what
is what: experiments in cross-document co-reference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 274?283.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis (ICIA).
Nader H. Bshouty and Philip M. Long. 2010. Find-
ing planted partitions in nearly linear time using ar-
rested spectral clustering. In Johannes Fu?rnkranz
and Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learning
(ICML-10), pages 135?142, Haifa, Israel, June. Omni-
press.
Yuan Changhe, Lu Tsai-Ching, and Druzdzel Marek.
2004. Annealed MAP. In Uncertainty in Artificial In-
telligence (UAI), pages 628?635, Arlington , Virginia.
AUAI Press.
Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen
Lin, and Edward Y. Chang. 2010. Parallel spectral
clustering in distributed systems. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithm. In Annual Meeting of the
Association for Computational Linguistics (ACL).
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
S. Datta, C. Giannella, and H. Kargupta. 2006. K-Means
Clustering over a Large, Dynamic Network. In SIAM
Data Mining Conference (SDM).
Hal Daume? III and Daniel Marcu. 2005. A Bayesian
model for supervised clustering with the Dirichlet pro-
cess prior. Journal of Machine Learning Research
(JMLR), 6:1551?1577.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. Sympo-
sium on Operating Systems Design & Implementation
(OSDI).
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 744?751.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 9?16.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 870?878.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL
HLT), pages 33?40.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 34?44, Cambridge, MA, October.
Association for Computational Linguistics.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed,
T. Finin, C. Fink, M. Freedman, N. Garera, P. Mc-
Namee, et al 2009. Cross-document coreference res-
olution: A key technology for learning by reading. In
AAAI Spring Symposium on Learning by Reading and
Learning to Read.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Horatiu Mocian. 2009. Survey of Distributed Clustering
Techniques. Ph.D. thesis, Imperial College of London.
802
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004. Weakly
supervised learning for cross-document person name
disambiguation supported by information extraction.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page 597.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 208?222.
Hoifung Poon, Pedro Domingos, and Marc Sumner.
2008. A general method for reducing the complexity
of relational inference and its application to MCMC.
In AAAI Conference on Artificial Intelligence.
Octavian Popescu, Christian Girardi, Emanuele Pianta,
and Bernardo Magnini. 2008. Improving cross-
document coreference. Journe?es Internationales
d?Analyse statistique des Donne?es Textuelles, 9:961?
969.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Conference on Computational Natu-
ral Language Learning (CoNLL), pages 41?48.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?11, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2010. Distributed map in-
ference for undirected graphical models. In Neural
Information Processing Systems (NIPS), Workshop on
Learning on Cores, Clusters and Clouds.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Uncertainty in Artificial
Intelligence (UAI), pages 593?601.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009a. An entity-based
model for coreference resolution. In SIAM Interna-
tional Conference on Data Mining (SDM).
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009b. Samplerank: Learn-
ing preferences from atomic gradients. In Neural In-
formation Processing Systems (NIPS), Workshop on
Advances in Ranking.
803
Tutorial Abstracts of ACL 2012, page 6,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Graph-based Semi-Supervised Learning Algorithms for NLP
Amar Subramanya
Google Research
asubram@google.com
Partha Pratim Talukdar
Carnegie Mellon University
ppt@cs.cmu.edu
Abstract
While labeled data is expensive to prepare, ever in-
creasing amounts of unlabeled linguistic data are
becoming widely available. In order to adapt to
this phenomenon, several semi-supervised learning
(SSL) algorithms, which learn from labeled as well
as unlabeled data, have been developed. In a sep-
arate line of work, researchers have started to real-
ize that graphs provide a natural way to represent
data in a variety of domains. Graph-based SSL al-
gorithms, which bring together these two lines of
work, have been shown to outperform the state-of-
the-art in many applications in speech processing,
computer vision and NLP. In particular, recent NLP
research has successfully used graph-based SSL al-
gorithms for PoS tagging (Subramanya et al, 2010),
semantic parsing (Das and Smith, 2011), knowledge
acquisition (Talukdar et al, 2008), sentiment anal-
ysis (Goldberg and Zhu, 2006) and text categoriza-
tion (Subramanya and Bilmes, 2008).
Recognizing this promising and emerging area of re-
search, this tutorial focuses on graph-based SSL al-
gorithms (e.g., label propagation methods). The tu-
torial is intended to be a sequel to the ACL 2008
SSL tutorial, focusing exclusively on graph-based
SSL methods and recent advances in this area, which
were beyond the scope of the previous tutorial.
The tutorial is divided in two parts. In the first
part, we will motivate the need for graph-based SSL
methods, introduce some standard graph-based SSL
algorithms, and discuss connections between these
approaches. We will also discuss how linguistic data
can be encoded as graphs and show how graph-based
algorithms can be scaled to large amounts of data
(e.g., web-scale data).
Part 2 of the tutorial will focus on how graph-based
methods can be used to solve several critical NLP
tasks, including basic problems such as PoS tagging,
semantic parsing, and more downstream tasks such
as text categorization, information acquisition, and
sentiment analysis. We will conclude the tutorial
with some exciting avenues for future work.
Familiarity with semi-supervised learning and
graph-based methods will not be assumed, and the
necessary background will be provided. Examples
from NLP tasks will be used throughout the tutorial
to convey the necessary concepts. At the end of this
tutorial, the attendee will walk away with the follow-
ing:
? An in-depth knowledge of the current state-of-
the-art in graph-based SSL algorithms, and the
ability to implement them.
? The ability to decide on the suitability of
graph-based SSL methods for a problem.
? Familiarity with different NLP tasks where
graph-based SSL methods have been success-
fully applied.
In addition to the above goals, we hope that this tu-
torial will better prepare the attendee to conduct ex-
citing research at the intersection of NLP and other
emerging areas with natural graph-structured data
(e.g., Computation Social Science).
Please visit http://graph-ssl.wikidot.com/ for details.
References
Dipanjan Das and Noah A. Smith. 2011. Semi-supervised
frame-semantic parsing for unknown predicates. In Proceed-
ings of the ACL: Human Language Technologies.
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing stars when
there aren?t many stars: graph-based semi-supervised learn-
ing for sentiment categorization. In Proceedings of the Work-
shop on Graph Based Methods for NLP.
Amarnag Subramanya and Jeff Bilmes. 2008. Soft-supervised
text classification. In EMNLP.
Amarnag Subramanya, Slav Petrov, and Fernando Pereira.
2010. Graph-based semi-supervised learning of structured
tagging models. In EMNLP.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira.
2008. Weakly supervised acquisition of labeled class in-
stances using graph random walks. In EMNLP.
6
