An Experiment On Incremental Analysis 
Using Robust Parsing Techniques 
Ki l ian  Foth  and Wol fgang Menze l  and Hor ia  F .  Pop  and Ingo  Schr6der  
foth  I menze l  I h fpop  I schroeder@nats . in fonnat ik .mf i -hamburg .de  
Fachbere ich  In fo rmat ik ,  Un ivers i tg t  Hamburg  
Vogt-K611n-Strage 30, 22527 Hamburg ,  Germany 
Abst ract  
The results of an experiment are presented in which 
an approach for robust parsing has been applied in- 
crementally. They confirm that due to the robust na- 
ture  of the underlying technology an arbitrary pre- 
fix of a sentence can be analysed into an interme- 
diate structural description which is able to direct 
the further analysis with a high degree of reliability. 
Most notably, this result can be achieved without 
adapting the gramrnar or the parsing algorithms to 
the case of increinental processing. The resulting 
incremental parsing procedure is significantly faster 
if compared to a non-incremental best-first search. 
Additionally it turns out that longer sentences ben- 
efit most from this acceleration. 
1 I n t roduct ion  
Natural language utterances usually unfold over 
time, i. e., both listening and reading are carried 
out in an incremental left-to-right manner. Model- 
ing a similar type of behaviour in computer-based 
solutions is a challenging aim particularly interest- 
ing task for a number of quite different reasons that 
are most relevant in the context of spoken language 
systems: 
? Without any external signals about the end 
of an utterance, incremental analysis is the 
only means to segment the incoming stream of 
speech input. 
? An incremental analysis mode provides for a 
lnore natural (mixed-initiative) dialogue be- 
haviour because partial results are already avail- 
able well before the end of an utterance. 
? Parsing may already take place in concurrency 
to sentence production. Therefore the speaking 
time becomes available as computing time. 
? Dynmnic expectations about the upcoming 
parts of the utterance might be derived right in 
time to provide guiding hints for other process- 
ing components, e. g., predictions about likely 
word forms for a speech recognizer (Hauenstein 
and Weber, 1994). 
In principle, two alternative strategies can be pur- 
sued when designing all incremental parsing proce- 
dure: 
1. To keel) open all necessary structural hypothe- 
ses required to accomodate every possible con- 
tinuation of an utterance. This is tile strategy 
usually adopted in an incremental chart parser 
(WirSn, 1992). 
2. To commit to one or a limited number of inter- 
pretations where 
(a) either this commitment is made rather 
early and a mechanisnl for partial re- 
analysis is provided (Lombardo, 1992) or 
(b) it is delayed until sufficient information is 
eventually available to take an ultimate de- 
cision (Marcus, 1987). 
The apparent efficiency of human language un- 
derstanding is usually attributed to an early com- 
mitment strategy. 
Our approach, in fact, represents an attempt o 
combine these two strategies: On the one hand, it 
keeps many of the available building blocks for the 
initial part of an utterance and passes an ('updated) 
search space to the following processing step. Ad- 
ditionally, the optimal structural description for the 
data already available is determined. This not only 
makes an actual interpretation (together with expec- 
tations for possible continuations) available to sub- 
sequent processing components, but also opens up 
the possibility to use this information to effectively 
constrain the set of new structural hypotheses. 
Determining the optimal interpretation for a yet 
incomplete sentence, however, requires a parsing ap- 
proach robust enough to analyse an arbitrary sen- 
tencc prefix into a meaningful structure. Therefore 
two closely related questions need to be raised: 
1. Can the necessary degree of robustness be 
achieved? 
2. Is the information contained in the currently 
optimal structure useful to guide the subsequent 
analysis? 
1026 
To answer these questions and to estimate the po- 
tential for search space reductions against a 1)ossible 
loss of accuracy, a series of experiments has been 
conducted. Sections 2 and 3 introduce and motivate 
the framework of the exl)eriments. Section 4 de- 
scribes a number of heuristics for re-use of previous 
solutions and Section 5 1)resents the results. 
2 Robust  Parsing in a Dependency  
Framework  
Our grammar models utterances as depertdeTzcy 
t ryes, which consist of pairs of words so that one 
depends directly on the other. This subordination 
relation can be qualified by a label (e. g. to distin- 
guish conq)lements fl:om modifiers). Since each word 
cml only depend on one other word, a labeled tree 
is formed, usually with the finite verb as its root. 
The decision on which stru(:ture to 1)ostulate for 
an utterance is guided 1)y explicit constrai~,ts, which 
are rel)resented as mfiversally quantified logical for- 
mulas about features of word tbrms and l)artial trees. 
t~r instance, one constraint might t)ostulate that a 
relation labeled as 'Subject' can only occur between 
a noun and a finite verb to its right, or that two dif- 
ferent del)endencies of the same verb may not both 
be labeled 'Sul)ject'. For efficiency reasons, these 
formulas may constrain individual del)endency edges 
or pairs of edges only. The al)plication of constraints 
Call I)egin as soou as the first word of an utterance 
is read; no global information about the utterance is
required for analysis of its beginning. 
Since natm:al language inl)ut will often exhibit ir- 
regularities such as restarts, repairs, hesitations and 
other gr~tmmatical errors, individual errors should 
not make further analysis impossible. Instead, a ro- 
bust 1)arser should continue to build a structure tbr 
the utterance. Ideally, this structure should be close 
to that of a similar, but grammatical utterance. 
This goal is attained by annotating the constraints 
that constitute the grammar with scores ranging 
from 0 to 1. A structure that violates one or more 
constraints i annotated with the product of the co l  
responding scores, and the structure with the highest 
combined score is defined as the solution of the pars- 
lug problem. In general, the higher the scores of the 
constraints, the more irregular constructions can 1)e 
analysed. Parsing an utterance with mmotated or 
soft constraints thus mnounts to multi-dimensional 
optimization. Both complete and heuristic search 
methods can be eml)loyed to solve such a t)roblem. 
Our robust al)proach also provides m~ easy way 
to implement partial parsing. If necessary, e. g., an 
isolated noun labeled as 'Subject' may form the root 
of a del)endency tree, although this would violate 
the first constraint lnentioned above. If a finite verl) 
is available, however, subordinating the noun under 
the verb will avoid the error and thus produce a 
better structure. This capability is crucial for the 
analysis of incomplete utterances. 
Different lcvcls of analysis can be defined to model 
syntactic as well as semantic structures. A depel> 
dency tree is constructed for each of these levels. 
Since constraints can relate the edges in parallel de- 
pendency trees to each other, having several trees 
contributes to the robustness of the approach. Al- 
together, the gramlnar used in the experiments de- 
scribed comprises \]2 levels of analysis and 490 con- 
straints (SchrSder el, al., 2000). 
3 Pref ix Parsing wi th  Weighted 
Const ra in ts  
In general, dependency analysis is well-suited for 
incremental analysis. Since subordinations always 
concern two words rather than full constituents, each 
word can be integrated into the analysis as soon as it 
is read, although not necessarily in the ol)timal way. 
Also, the 1)re-colnl)uted dependency links can easily 
1)e re-used in subsequent i erations. Therefore, de- 
1)endency grammar allows a fine-grained incremental 
analysis (boxnbardo, 1992). 
mr c/"\, r2 " -  mod 
o" ? \o ? 
daal; lassen sie uns doch 
I hen lat you us 
z" c1.,. I j "  
~0 
mod/ /  / .  
rloch einen Ten'nil; 
<pa~t> yet a meeting 
(a) 
-\ 
c "~ mod 
\\ ~o  0 0 / ~ \', 
dana lassen sie uns doch 
Then let you us <part> 
Let's appoint yet another meeting then. 
O" 
cJ -" : 
i IB" 
,,,oa ~ ~ i (b) 
o~ ~ 
noch einoll Terrain ausmachen 
yel a meeting appoint 
lPigure 1: An example for a prefix analysis 
When assigning a det)endency structure to incom- 
plete utterances, the problem arises how to analyse 
words whose governors or complements still lie be- 
yond the time horizon. Two distinct alternatives are 
1)ossible: 
1. The parser can establish a dependency between 
tilt word and a special node representing a im- 
tative word that is assmned to follow in the re- 
maining input. This explicitly models the ex- 
pectations that would be raised by the prefix. 
llowever, unifying new words with these under- 
specified nodes is difficult, particularly when 
1027 
multiple words have been conjectured. Also, 
many constraints cannot be meaningfully ap- 
plied to words with unknown features. 
2. An incomplete prefix can be analyzed directly 
if a grammar is robust enough to allow par- 
tial parsing as discussed in the previous sec- 
tion: If the constraint that forbids multiple 
trees receives a severe but non-zero penalty, 
missing governors or complements are accept- 
able as long as no better structure is possible. 
Experiments in prefix parsing using a dependency 
grammar of German have shown that even complex 
utterances with nested subclauses can be analysed 
in the second way. Figure la provides an example 
of this: Because the infinitive verb 'ausmachen' is 
not yet visible, its coinplement ~Terinin' is analysed 
as an isolated subtree, and the main verb 'lassen' is 
lacking a comI)lement. After the missing verb has 
been read, two additional dependency edges suffice 
to build the correct structure from the partial parse. 
This method allows direct comparison between in- 
cremental and non-incremental parser runs, since 
both methods use tim same grammar. Therefore, 
we will follow up on the second alternative only and 
construct extended structures guided by the struc- 
tures of prefixes, without explicitly modeling missing 
words. 
4 Re-Use  of  Part ia l  Resu l ts  
While a prefix analysis can produce partial parses 
and diagnoses, so far this inforlnation has not been 
used in subsequent i erations. In fact, after a new 
word has been read, another search is conducted on 
all words already available. To reduce this duplica- 
tion of work, we wish to narrow down the problem 
space for these words. Therefore, at each iteration, 
the set of hypotheses has to be updated: 
? By deciding which old dependency hypotheses 
should be kept. 
? By deciding which new dependency hypotheses 
should be added to the search space in order to 
accomodate the incoming word. 
For that purpose, several heuristics have been de- 
vised, based on the following principles: 
P red ic t ion  s t rength .  Restrict the search space as 
much as possible, while maintaining correct- 
ness. 
Economy. Keep as nmch of the previous structure 
as possible. 
Rightmost attachment. Attach the incoming 
word to the most recent words. 
The heuristics are presented here in increasing or- 
der of the size of the problem space they produce: 
A. Keep all dependency edges from the previous 
optimal solution. Add all dependency edges 
where the incoming word modifies, or is modi- 
fied by, another word. 
B. As A, but also keep all links that differ h'om the 
previous optimal solution only in their lexical 
readings. 
C. As B, but also keep all links that differ fl'om the 
previous optimal solution only in the subordi- 
nation of its last word. 
D. As C, but also keep all links that differ from the 
previous optimal solution only in the subordi- 
nation of all the words lying on the path h'om 
the last word to the root of the solution tree. 
E. As D, but for all trees in the previous solution. 
5 Resu l ts  
In order to evaluate the potential of the heuristics 
described above, we have conducted a series of ex- 
periments using a grammar that was designed for 
non-incremental, robust parsing. We tested the in- 
crelnental against a non-incremental parser using 
222 utterances taken from the VERBMOBIL do- 
main (Wahlster, 1993). 
V:\] -o -.\[=\] 
1 r; 
A B C D E 
Heuristics 
Figure 2: Solution quality and processing time for 
different heuristics 
Figure 2 compares the five heuristics with respect 
to tile following criteria: 1
Accuracy.  The accuracy (gray bar) describes how 
many edges of the solutions are correct. 
correct edges 
accuracy = ~ edges found 
tNote that the heuristics provide at most one solution and 
may fail to find any solution. 
1028 
Weak recall .  We base our recall measure - given as 
the black bar - on the number of solutions found 
non-incrementally (which is less than 100%) be- 
cause we want focus on tile impact of our heuris- 
tics, not the coverage of tile grammar. 
weak recall = @ correct edges 
@ edges fouud non-incrementally 
Re la t ive  run- t ime.  The run-time required by the 
incremental procedure as a percentage of the 
time required by the non-iucremental search al- 
gorithm is given as the white bar. 
The difference between tile gray and the black bar 
is due to errors of the heuristic method, i. e., ei- 
ther because of its incal)ability to find the correct 
subordination or due to excessive resource demands 
(which lead to process abortion). 
2Nvo observations can be made: First, all but 
tt,e last heuristics need less time than the non- 
incremental algorithm to complete while maintain- 
ing a relative high degree of quality. Second, the 
more elaborate the heuristics are, the longer they 
need to run (as expected) and the better are the re- 
sults for the accuracy measure. However, the lmuris- 
tics D and E could not complete to parse all sen- 
tences because in some cases a pre-defined time limit 
was exceeded; this leads to the observed decrease 
in weak recall when compared to heuristics C. As 
expected, a trade-off between computing time and 
quality can be found. Overall, heuristics C seems to 
be a good choice because it achieves all accuracy of 
up to 93.7% in only one fifth of the run-time. 
250%: 
200% 
t50% 
~00% 
50% 
Figure 
Figure 
heuristics 
~ Time for incremen- 
tal compared to non- 
incremental method 
D Absolute time for in- 
cremental (16...20 set 
to 100%)  
? The gray bar presents tile normalized time with 
the time for sentence length between 16 and 20 
set to 100%. 
Tile results show that the speedup observed in 
Figure 2 is not evenly distributed. While the incre- 
mental analysis of the short sentences takes longer 
(2.5 times slower) than the non-incremental go- 
rithm, the opposite is true for longer sentences (10 
times faster). However, this is welcome behavior: 
The incremental procedure takes longer only in those 
cases that are solved very fast anyway; tim problem- 
atic cases are parsed more quickly. This behavior is 
a first hint that the incremental nalysis with re-use 
of partial results is a step that alleviates the combi- 
natorial explosion of resource demands. 
-? ~ ~ 
m ~ cd 
100%" c-i , ,  
80%. 
60%" 
40%' 
20% 
O% 
1. . .5  6 . . .10  11 . . .15  16 . . .20  overall 
Sentence Length 
3: Processing time vs. sentence length 
3 coral)ares the time requirements of 
C for different sentence lengths. 
? The relative run-time (as in Figure 2) is given 
as the wlfite bar. 
0%, 
1...5 6...10 11...15 16...20 overall 
Sentence Length 
Figure 4: Accuracy vs. sentence length (colors have 
tile same meaning as ill Figure 2) 
Finally, Figure 4 compares the quality resulting 
from heuristics C for different sentence lengths. It 
turns out that, although a slight decrease is observ- 
able, the accuracy is relatively independent of sen- 
tence length. 
I I 
? ~ r -q~ ~ I ~.1 I ~ ~ 6 Conc lus ions  co c,~ 
m 6 ~ An apl)roach to the incremental parsing of natural 
language utterances has been presented, which is 
based on tlle idea to use robust parsing techniques 
to deal with incomplete sentences. It determines a 
structural description for arbitrary sentence prefixes 
by searching for the optimal combination of local 
hypotheses. This search is conducted in a problem 
space which is repeatedly narrowed down according 
to the optimal solution found in tile preceding step 
of analysis. 
1029 
The results available so far confirm the initial ex- 
pectation that the grammar used is robust enough 
to reliably carry out such a prefix analysis, al- 
though it has originally been developed for the non- 
incremental case. The optimal structure as deter- 
mined by the parser obviously contains relevant in- 
formation about the sentence prefix, so that even 
very simple and cheap heuristics can achieve a con- 
siderable level of accuracy. Therefore, large parts of 
the search space can be excluded fi'oln repeated re- 
analysis, which eventually makes it even faster than 
its non-incremental counterpart. Most importantly, 
the observed speedup grows with the length of the 
utterance. 
On the other hand, none of the used structure- 
based heuristics produces a significant iml)rovement 
of quality even if a large amount of computational 
resources is spent. Quite a number of cases can 
be identified where even the most expensive of our 
heuristics is not strong enough, e. g., the German 
sentence with a topicalized irect object: 
DieNOM,ACe Frau sieht derNOM Mama. 
The woman sees tile man. 
The woman, the man sees. 
Here, when analysing the subsentence die Frau 
sieht, the parser will wrongly consider die Frau as 
the subject, because it appears to have the right 
case and there is a clear preference to do so. Later, 
when the next word comes in, there is no way to 
allow for dic Frau to change its structural interpre- 
tation, because this is not licensed by any of the 
given heuristics. 
Therefore, substantially more i)roblenl-oriellted 
heuristics are required, which should take into ac- 
count not only the ol)timal structure, but also the 
conflicts caused by it. Using a weak but cheap 
heuristics, a fast al)proximation of the optimal struc- 
ture can be obtained within a very restricted search 
space, and then refined by subsequent structural 
transformations (Foth et al, 2000). To a certain 
degree this resembles the idea of applying reason 
maintenance t chniques for conflict resolution in in- 
cremental parsing (Wir6n, 1990). In deciding which 
strategy is good enough to find the necessary first 
approximation the results of this paper might play a 
crucial role, since the I)ossible contribution of indi- 
vidual heuristics in such all extended fi'amework can 
be precisely estimated. 
Acknowledgements 
This research as been partly fimded by the German 
Research Foundation "Deutsche Forschungsgemein- 
schaft" under grant no. Me 1472/1-2. 
References 
Kilian Foth, Wolfgang Menzel, and Ingo SchrSder. 
2000. A transformation-based parsing technique 
with anytime property. In Procecdings of the 
International Workshop on Parsing Technologies 
(IWPT-2000), pages 89-100, Trento, Italy. 
Andreas Hauenstein and Hans Weber. 1994. An in- 
vestigation of tightly coupled time synchronous 
speech language interfaces using a unification 
grammar. In Proceedings of the i2th National 
Conference on Artificial Intelligence: Workshop 
on the Integration of Natural Language and Speech 
Processing, pages 42-49, Seattle, Washington. 
Johannes Heinecke, J/irgen Kunze, Wolfgang Men- 
zel, and Ingo SchrSder. 1998. Eliminative pars- 
ing with graded constraints. In Proceedings of 
the Joint Conference COLING/ACL-98, Mon- 
trial, Canada. 
Vincenzo Lombardo. 1992. Incremental dependency 
parsing. In P~vceedings of the Annual Meeting of 
the ACL, Delaware, Newark, USA. 
Mitchell P. Marcus. 1987. Deterministic pars- 
ing and description theory. In Peter Whitclock, 
Mary McGee Wood, Harold Seiners, Rod John- 
son, and Paul Bennett, editors, Linguistic Theory 
and Computer Applications', pages 69-112. Aca- 
demic Press, London, England. 
Wolfgang Menzel and ingo SchrSder. 1998. Decision 
procedures for dependency parsing using graded 
constraints. In Sylvain Kahane and Alain Pol- 
gu~re, editors, Proceedings of the Joint Confer- 
ence COLING/ACL-98 Workshop: Processing of 
Dependency-based Grammars, pages 78-87, Mon- 
trSal, Canada. 
Ingo SchrSder, Wolfgang Menzel, Kilian Foth, 
and Michael Schulz. 2000. Modeling depen- 
dency grammar with restricted constraints. In- 
ternational Journal J?'aitemcnt Automatique des 
Langues: Grammaires de d@endance, 41(1). 
Wolfgang Wahlster. 1993. Verbmobih Translation 
of face-to-face dialogs. In Proceedings of the 3rd 
European Conference on @eech Communication 
and Technology, pages 29-38, Berlin, Germany. 
Itans Weber. 1995. LR-inkrementelles proba- 
bilistisches Chartparsing von Worthypothesen- 
graphen mit Unifikationsgrammatiken: Eine enge 
Kopplung von Suche und Analyse. Verbmobil- 
Report 52, UniversitAt Erlangen-Niirnberg. 
Mats Wir6n. 1992. Studies in Incremental Natural- 
Language Analysis. Ph.D. tlmsis, Department of 
Computer and Information Science, LinkSping 
University, LinkSping, Sweden. 
Mats WirSn. 1990. Incremental parsing and reason 
maintenance. In Proceedings of the i3th Interna- 
tional Conference on Computational Linguistics 
(COLING-90), pages 287-292, Helsinki, Finland. 
1030 
99
100
101
102
103
104
105
106
Interactive grammar development with WCDG
Kilian A. Foth Michael Daum Wolfgang Menzel
Natural Language Systems Group
Hamburg University
D-22527 Hamburg
Germany
{foth,micha,menzel}@nats.informatik.uni-hamburg.de
Abstract
The manual design of grammars for accurate natu-
ral language analysis is an iterative process; while
modelling decisions usually determine parser be-
haviour, evidence from analysing more or differ-
ent input can suggest unforeseen regularities, which
leads to a reformulation of rules, or even to a differ-
ent model of previously analysed phenomena. We
describe an implementation of Weighted Constraint
Dependency Grammar that supports the grammar
writer by providing display, automatic analysis, and
diagnosis of dependency analyses and allows the di-
rect exploration of alternative analyses and their sta-
tus under the current grammar.
1 Introduction
For parsing real-life natural language reliably, a
grammar is required that covers most syntactic
structures, but can also process input even if it
contains phenomena that the grammar writer has
not foreseen. Two fundamentally different ways
of reaching this goal have been employed various
times. One is to induce a probability model of the
target language from a corpus of existing analyses
and then compute the most probable structure for
new input, i.e. the one that under some judiciously
chosen measure is most similar to the previously
seen structures. The other way is to gather linguis-
tically motivated general rules and write a parsing
system that can only create structures adhering to
these rules.
Where an automatically induced grammar re-
quires large amounts of training material and the
development focuses on global changes to the prob-
ability model, a handwritten grammar could in prin-
ciple be developed without any corpus at all, but
considerable effort is needed to find and formu-
late the individual rules. If the formalism allows
the ranking of grammar rules, their relative impor-
tance must also be determined. This work is usu-
ally much more cyclical in character; after grammar
rules have been changed, intended and unforeseen
consequences of the change must be checked, and
further changes or entirely new rules are suggested
by the results.
We present a tool that allows a grammar writer to
develop and refine rules for natural language, parse
new input, or annotate corpora, all in the same envi-
ronment. Particular support is available for interac-
tive grammar development; the effect of individual
grammar rules is directly displayed, and the system
explicitly explains its parsing decisions in terms of
the rules written by the developer.
2 The WCDG parsing system
The WCDG formalism (Schro?der, 2002) describes
natural language exclusively as dependency struc-
ture, i.e. ordered, labelled pairs of words in the in-
put text. It performs natural language analysis under
the paradigm of constraint optimization, where the
analysis that best conforms to all rules of the gram-
mar is returned. The rules are explicit descriptions
of well-formed tree structures, allowing a modular
and fine-grained description of grammatical knowl-
edge. For instance, rules in a grammar of English
would state that subjects normally precede the finite
verb and objects follow it, while temporal NP can
either precede or follow it.
In general, these constraints are defeasible, since
many rules about language are not absolute, but
can be preempted by more important rules. The
strength of constraining information is controlled by
the grammar writer: fundamental rules that must al-
ways hold, principles of different import that have
to be weighed against each other, and general pref-
erences that only take effect when no other disam-
biguating knowledge is available can all be formu-
lated in a uniform way. In some cases preferences
can also be used for disambiguation by approximat-
ing information that is currently not available to the
system (e.g. knowledge on attachment preferences).
Even the very weak preferences have an influence
on the parsing process; apart from serving as tie-
breakers for structures where little context is avail-
able (e.g. with fragmentary input), they provide an
Figure 1: Display of a simplified feature hierarchy
initial direction for the constraint optimization pro-
cess even if they are eventually overruled. As a con-
sequence, even the best structure found usually in-
curs some minor constraint violations; as long as
the combined evidence of these default expectation
failures is small, the structure can be regarded as
perfectly grammatical.
The mechanism of constraint optimization si-
multaneously achieves robustness against extra-
grammatical and ungrammatical input. There-
fore WCDG allows for broad-coverage parsing with
high accuracy; it is possible to write a grammar
that is guaranteed to allow at least one structure for
any kind of input, while still preferring compliant
over deviant input wherever possible. This graceful
degradation under reduced input quality makes the
formalism suitable for applications where deviant
input is to be expected, e.g. second language learn-
ing. In this case the potential for error diagnosis
is also very valuable: if the best analysis that can
be found still violates an important constraint, this
directly indicates not only where an error occurred,
but also what might be wrong about the input.
3 XCDG: A Tool for Parsing and
Modelling
An implementation of constraint dependency gram-
mar exists that has the character of middleware to al-
low embedding the parsing functionality into other
natural language applications. The program XCDG
uses this functionality for a graphical tool for gram-
mar development.
In addition to providing an interface to a range
of different parsing algorithms, graphical display
of grammar elements and parsing results is possi-
ble; for instance, the hierarchical relations between
possible attributes of lexicon items can be shown.
See Figure 1 for an excerpt of the hierarchy of Ger-
man syntactical categories used; the terminals cor-
respond to those used the Stuttgart-Tu?bingen Tagset
of German (Schiller et al, 1999).
More importantly, mean and end results of pars-
ing runs can be displayed graphically. Dependency
structures are represented as trees, while additional
relations outside the syntax structure are shown as
arcs below the tree (see the referential relationship
REF in Figure 2). As well as end results, inter-
mediate structures found during parsing can be dis-
played. This is often helpful in understanding the
behaviour of the heuristic solution methods em-
ployed.
Together with the structural analysis, instances
of broken rules are displayed below the depen-
dency graph (ordered by decreasing weights), and
the dependencies that trigger the violation are high-
lighted on demand (in our case the PP-modification
between the preposition in and the infinite form
verkaufen). This allows the grammar writer to eas-
ily check whether or not a rule does in fact make the
distinction it is supposed to make. A unique iden-
tifier attached to each rule provides a link into the
grammar source file containing all constraint defi-
nitions. The unary constraint ?mod-Distanz? in
the example of Figure 2 is a fairly weak constraint
which penalizes attachments the stronger the more
distant a dependent is placed from its head. At-
taching the preposition to the preceding noun Bund
would be preferred by this constraint, since the dis-
tance is shorter. However, it would lead to a more
serious constraint violation because noun attach-
ments are generally dispreferred.
To facilitate such experimentation, the parse win-
dow doubles as a tree editor that allows structural,
lexical and label changes to be made to an analysis
by drag and drop. One important application of the
integrated parsing and editing tool is the creation of
large-scale dependency treebanks. With the ability
to save and load parsing results from disk, automat-
ically computed analyses can be checked and hand-
corrected where necessary and then saved as anno-
tations. With a parser that achieves a high perfor-
mance on unseen input, a throughput of over 100 an-
notations per hour has been achieved.
4 Grammar development with XCDG
The development of a parsing grammar based on
declarative constraints differs fundamentally from
that of a derivational grammar, because its rules for-
bid structures instead of licensing them: while a
context-free grammar without productions licenses
nothing, a constraint grammar without constraints
would allow everything. A new constraint must
therefore be written whenever two analyses of the
same string are possible under the existing con-
straints, but human judgement clearly prefers one
over the other.
Figure 2: Xcdg Tree Editor
Most often, new constraints are prompted by in-
spection of parsing results under the existing gram-
mar: if an analysis is computed to be grammati-
cal that clearly contradicts intuition, a rule must be
missing from the grammar. Conversely, if an error
is signalled where human judgement disagrees, the
relevant grammar rule must be wrong (or in need of
clarifying exceptions). In this way, continuous im-
provement of an existing grammar is possible.
XCDG supports this development style through
the feature of hypothetical evaluation. The tree dis-
play window does not only show the result returned
by the parser; the structure, labels and lexical selec-
tions can be changed manually, forcing the parser to
pretend that it returned a different analysis. Recall
that syntactic structures do not have to be specif-
ically allowed by grammar rules; therefore, every
conceivable combination of subordinations, labels
and lexical selections is admissible in principle, and
can be processed by XCDG, although its score will
be low if it contradicts many constraints.
After each such change to a parse tree, all con-
straints are automatically re-evaluated and the up-
dated grammar judgement is displayed. In this way
it can quickly be checked which of two alternative
structures is preferred by the grammar. This is use-
ful in several ways. First, when analysing pars-
ing errors it allows the grammar author to distin-
guish search errors from modelling errors: if the
intended structure is assigned a better score than the
one actually returned by the parser, a search error
occurred (usually due to limited processing time);
but if the computed structure does carry the higher
score, this indicates an error of judgement on the
part of the grammar writer, and the grammar needs
to be changed in some way if the phenomenon is to
be modelled adequately.
If a modelling error does occur, it must be be-
cause a constraint that rules against the intended
analysis has overruled those that should have se-
lected it. Since the display of broken constraints is
ordered by severity, it is immediately obvious which
of the grammar rules this is. The developer can
then decide whether to weaken that rule or extend
it so that it makes an exception for the current phe-
nomenon. It is also possible that the intended anal-
ysis really does conflict with a particular linguistic
principle, but in doing so follows a more important
one; in this case, this other rule must be found and
strengthened so that it will overrule the first one.
The other rule can likewise be found by re-creating
the original automatic analysis and see which of its
constraint violations needs to be given more weight,
or, alternatively, which entirely new rule must be
added to the grammar.
In the decision whether to add a new rule to a con-
straint grammar, it must be discovered under what
conditions a particular phenomenon occurs, so that
a generally relevant rule can be written. The posses-
sion of a large amount of analysed text is often use-
ful here to verify decisions based on mere introspec-
tion. Working together with an external program
to search for specific structures in large treebanks,
XCDG can display multiple sentences in stacked
widgets and highlight all instances of the same phe-
nomenon to help the grammar writer decide what
the relevant conditions are.
Using this tool, a comprehensive grammar of
modern German has been constructed (Foth, 2004)
that employs 750 handwritten well-formedness
rules, and has been used to annotate around 25,000
sentences with dependency structure. It achieves a
structural recall of 87.7% on sentences from the NE-
GRA corpus (Foth et al, submitted), but can be ap-
plied to texts of many other types, where structural
recall varies between 80?90%. To our knowledge,
no other system has been published that achieves
a comparable correctness for open-domain German
text. Parsing time is rather high due to the computa-
tional effort of multidimensional optimization; pro-
cessing time is usually measured in seconds rather
than milliseconds for each sentence.
5 Conclusions
We demonstrate a tool that lets the user parse, dis-
play and manipulate dependency structures accord-
ing to a variant of dependency grammar in a graph-
ical environment. We have found such an inte-
grated environment invaluable for the development
of precise and large grammars of natural language.
Compared to other approaches, c.f. (Kaplan and
Maxwell, 1996), the built-in WCDG parser pro-
vides a much better feedback by pinpointing possi-
ble reasons for the current grammar being unable to
produce the desired parsing result. This additional
information can then be immediately used in subse-
quent development cycles.
A similar tool, called Annotate, has been de-
scribed in (Brants and Plaehn, 2000). This tool
facilitates syntactic corpus annotation in a semi-
automatic way by using a part-of-speech tagger and
a parser running in the background. In compari-
son, Annotate is primarily used for corpus annota-
tion, whereas XCDG supports the development of
the parser itself also.
Due to its ability to always compute the single
best analysis of a sentence and to highlight possible
shortcomings of the grammar, the XCDG system
provides a useful framework in which human design
decisions on rules and weights can be effectively
combined with a corpus-driven evaluation of their
consequences. An alternative for a symbiotic coop-
eration in grammar development has been devised
by (Hockenmaier and Steedman, 2002), where a
skeleton of fairly general rule schemata is instan-
tiated and weighed by means of a treebank anno-
tation. Although the resulting grammar produced
highly competitive results, it nevertheless requires
a treebank being given in advance, while our ap-
proach also supports a simultaneous treebank com-
pilation.
References
Thorsten Brants and Oliver Plaehn. 2000. Interac-
tive corpus annotation. In Proc. 2nd Int. Conf.
on Language Resources and Engineering, LREC
2000, pages 453?459, Athens.
Kilian Foth, Michael Daum, and Wolfgang Men-
zel. submitted. A broad-coverage parser for Ger-
man based on defeasible constraints. In Proc. 7.
Konferenz zur Verarbeitung nat?urlicher Sprache,
KONVENS-2004, Wien, Austria.
Kilian A. Foth. 2004. Writing weighted constraints
for large dependency grammars. In Proc. Recent
Advances in Dependency Grammars, COLING
2004, Geneva, Switzerland.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proc. 40th
Annual Meeting of the ACL, ACL-2002, Philadel-
phia, PA.
Ronald M. Kaplan and John T. Maxwell. 1996.
LFG grammar writer?s workbench. Technical re-
port, Xerox PARC.
Anne Schiller, Simone Teufel, Christine Sto?ckert,
and Christine Thielen. 1999. Guidelines fu?r das
Tagging deutscher Textcorpora. Technical report,
Universita?t Stuttgart / Universita?t Tu?bingen.
Ingo Schro?der. 2002. Natural Language Parsing
with Graded Constraints. Ph.D. thesis, Depart-
ment of Informatics, Hamburg University, Ham-
burg, Germany.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 289?296,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Guiding a Constraint Dependency Parser with Supertags
Kilian Foth, Tomas By, and Wolfgang Menzel
Department fu?r Informatik, Universita?t Hamburg, Germany
foth|by|menzel@informatik.uni-hamburg.de
Abstract
We investigate the utility of supertag infor-
mation for guiding an existing dependency
parser of German. Using weighted con-
straints to integrate the additionally avail-
able information, the decision process of
the parser is influenced by changing its
preferences, without excluding alternative
structural interpretations from being con-
sidered. The paper reports on a series of
experiments using varying models of su-
pertags that significantly increase the pars-
ing accuracy. In addition, an upper bound
on the accuracy that can be achieved with
perfect supertags is estimated.
1 Introduction
Supertagging is based on the combination of two
powerful and influential ideas of natural language
processing: On the one hand, parsing is (at least
partially) reduced to a decision on the optimal se-
quence of categories, a problem for which efficient
and easily trainable procedures exist. On the other
hand, supertagging exploits complex categories,
i.e. tree fragments which much better reflect the
mutual compatibility between neighbouring lexi-
cal items than say part-of-speech tags.
Bangalore and Joshi (1999) derived the notion
of supertag within the framework of Lexicalized
Tree-Adjoining Grammars (LTAG) (Schabes and
Joshi, 1991). They considered supertagging a pro-
cess of almost parsing, since all that needs to be
done after having a sufficiently reliable sequence
of supertags available is to decide on their combi-
nation into a spanning tree for the complete sen-
tence. Thus the approach lends itself easily to pre-
processing sentences or filtering parsing results
with the goal of guiding the parser or reducing its
output ambiguity.
Nasr and Rambow (2004) estimated that perfect
supertag information already provides for a pars-
ing accuracy of 98% if a correct supertag assign-
ment were available. Unfortunately, perfectly re-
liable supertag information cannot be expected;
usually this uncertainty is compensated by run-
ning the tagger in multi-tagging mode, expecting
that the reliability can be increased by not forcing
the tagger to take unreliable decisions but instead
offering a set of alternatives from which a subse-
quent processing component can choose.
A grammar formalism which seems particularly
well suited to decompose structural descriptions
into lexicalized tree fragments is dependency
grammar. It allows us to define supertags on differ-
ent levels of granularity (White, 2000; Wang and
Harper, 2002), thus facilitating a fine grained anal-
ysis of how the different aspects of supertag in-
formation influence the parsing behaviour. In the
following we will use this characteristic to study
in more detail the utility of different kinds of su-
pertag information for guiding the parsing process.
Usually supertags are combined with a parser in
a filtering mode, i.e. parsing hypotheses which
are not compatible with the supertag predic-
tions are simply discarded. Drawing on the abil-
ity of Weighted Constraint Dependency Grammar
(WCDG) (Schro?der et al, 2000) to deal with de-
feasible constraints, here we try another option for
making available supertag information: Using a
score to estimate the general reliability of unique
supertag decisions, the information can be com-
bined with evidence derived from other constraints
of the grammar in a soft manner. It makes possi-
ble to rank parsing hypotheses according to their
plausibility and allows the parser to even override
potentially wrong supertag decisions.
Starting from a range of possible supertag mod-
els, Section 2 explores the reliability with which
dependency-based supertags can be determined on
289
SUBJC
PN
ATTR
DET
PP
OBJA
ATTR
DET
SUBJ
DET
KONJ
AUX
S
EXPL
es mag sein , da? die Franzosen kein schl?ssiges Konzept f?r eine echte Partnerschaft besitzen .
Figure 1: Dependency tree for sentence 19601 of the NEGRA corpus.
different levels of granularity. Then, Section 3 de-
scribes how supertags are integrated into the exist-
ing parser for German. The complex nature of su-
pertags as we define them makes it possible to sep-
arate the different structural predictions made by a
single supertag into components and study their
contributions independently (c.f. Section 4). We
can show that indeed the parser is robust enough to
tolerate supertag errors and that even with a fairly
low tagger performance it can profit from the ad-
ditional, though unreliable information.
2 Supertagging German text
In defining the nature of supertags for depen-
dency parsing, a trade-off has to be made between
expressiveness and accuracy. A simple definition
with very small number of supertags will not be
able to capture the full variety of syntactic con-
texts that actually occur, while an overly expres-
sive definition may lead to a tag set that is so large
that it cannot be accurately learnt from the train-
ing data. The local context of a word to be en-
coded in a supertag could include its edge label,
the attachment direction, the occurrence of obliga-
tory1 or of all dependents, whether each predicted
dependent occurs to the right or to the left of the
word, and the relative order among different de-
pendents. The simplest useful task that could be
asked of a supertagger would be to predict the de-
pendency relation that each word enters. In terms
of the WCDG formalism, this means associating
each word at least with one of the syntactic labels
that decorate dependency edges, such as SUBJ or
DET; in other words, the supertag set would be
identical to the label set. The example sentence
1The model of German used here considers the objects
of verbs, prepositions and conjunctions to be obligatory and
most other relations as optional. This corresponds closely to
the set of needs roles of (Wang and Harper, 2002).
?Es mag sein, da? die Franzosen kein schlu?ssiges Konzept
fu?r eine echte Partnerschaft besitzen.?
(Perhaps the French do not have a viable concept for a true
partnership.)
if analyzed as in Figure 1, would then be de-
scribed by a supertag sequence beginning with
EXPL S AUX ...
Following (Wang and Harper, 2002), we further
classify dependencies into Left (L), Right (R), and
No attachments (N), depending on whether a word
is attached to its left or right, or not at all. We
combine the label with the attachment direction
to obtain composite supertags. The sequence of
supertags describing the example sentence would
then begin with EXPL/R S/N AUX/L ...
Although this kind of supertag describes the role
of each word in a sentence, it still does not spec-
ify the entire local context; for instance, it asso-
ciates the information that a word functions as a
subject only with the subject and not with the verb
that takes the subject. In other words, it does not
predict the relations under a given word. Greater
expressivity is reached by also encoding the la-
bels of these relations into the supertag. For in-
stance, the word ?mag? in the example sentence
is modified by an expletive (EXPL) on its left
side and by an auxiliary (AUX) and a subject
clause (SUBJC) dependency on its right side. To
capture this extended local context, these labels
must be encoded into the supertag. We add the
local context of a word to the end of its su-
pertag, separated with the delimiter +. This yields
the expression S/N+AUX,EXPL,SUBJC. If we
also want to express that the EXPL precedes the
word but the AUX follows it, we can instead
add two new fields to the left and to the right
of the supertag, which leads to the new supertag
EXPL+S/N+AUX,SUBJC.
Table 1 shows the annotation of the example us-
290
Word Supertag model J
es +EXPL/R+
mag EXPL+S/N+AUX,SUBJC
sein +AUX/L+
, +/N+
da? +KONJ/R+
die +DET/R+
Franzosen DET+SUBJ/R+
kein +DET/R+
schlu?ssiges +ATTR/R+
Konzept ATTR,DET+OBJA/R+PP
fu?r +PP/L+PN
eine +DET/R+
echte +ATTR/R+
Partnerschaft ATTR,DET+PN/L+
besitzen KONJ,OBJA,SUBJ+SUBJC/L+
. +/N+
Table 1: An annotation of the example sentence
ST Prediction of #tags Super- Com-
mo- label direc- depen- order tag ponent
del tion dents accuracy accuracy
A yes no none no 35 84.1% 84.1%
B yes yes none no 73 78.9% 85.7%
C yes no oblig. no 914 81.1% 88.5%
D yes yes oblig. no 1336 76.9% 90.8%
E yes no oblig. yes 1465 80.6% 91.8%
F yes yes oblig. yes 2026 76.2% 90.9%
G yes no all no 6858 71.8% 81.3%
H yes yes all no 8684 67.9% 85.8%
I yes no all yes 10762 71.6% 84.3%
J yes yes all yes 12947 67.6% 84.5%
Table 2: Definition of all supertag models used.
ing the most sophisticated supertag model. Note
that the notation +EXPL/R+ explicitly represents
the fact that the word labelled EXPL has no de-
pendents of its own, while the simpler EXPL/R
made no assertion of this kind. The extended con-
text specification with two + delimiters expresses
the complete set of dependents of a word and
whether they occur to its left or right. However, it
does not distinguish the order of the left or right
dependents among each other (we order the la-
bels on either side alphabetically for consistency).
Also, duplicate labels among the dependents on ei-
ther side are not represented. For instance, a verb
with two post-modifying prepositions would still
list PP only once in its right context. This ensures
that the set of possible supertags is finite. The full
set of different supertag models we used is given
in Table 2. Note that the more complicated mod-
els G, H, I and J predict all dependents of each
word, while the others predict obligatory depen-
dents only, which should be an easier task.
To obtain and evaluate supertag predictions, we
used the NEGRA and TIGER corpora (Brants et
al., 1997; Brants et al, 2002), automatically trans-
formed into dependency format with the freely
available tool DepSy (Daum et al, 2004). As
our test set we used sentences 18,602?19,601 of
the NEGRA corpus, for comparability to earlier
work. All other sentences (59,622 sentences with
1,032,091 words) were used as the training set. For
each word in the training set, the local context was
extracted and expressed in our supertag notation.
The word/supertag pairs were then used to train
the statistical part-of-speech tagger TnT (Brants,
2000), which performs trigram tagging efficiently
and allows easy retraining on different data. How-
ever, a few of TnT?s limitations had to be worked
around: since it cannot deal with words that have
more than 510 different possible tags, we system-
atically replaced the rarest tags in the training set
with a generic ?OTHER? tag until the limit was
met. Also, in tagging mode it can fail to process
sentences with many unknown words in close suc-
cession. In such cases, we simply ran it on shorter
fragments of the sentence until no error occurred.
Fewer than 0.5% of all sentences were affected by
this problem even with the largest tag set.
A more serious problem arises when using a
stochastic process to assign tags that partially pre-
dict structure: the tags emitted by the model may
contradict each other. Consider, for instance, the
following supertagger output for the previous ex-
ample sentence:
es: +EXPL/R+ mag: +S/N+AUX,SUBJC
sein: PRED+AUX/L+ ...
The supertagger correctly predicts that the first
three labels are EXPL, S, and AUX. It also pre-
dicts that the word ?sein? has a preceding PRED
complement, but this is impossible if the two pre-
ceding words are labelled EXPL and S. Such con-
tradictory information is not fatal in a robust sys-
tem, but it is likely to cause unnecessary work
for the parser when some rules demand the im-
possible. We therefore decided simply to ignore
context predictions when they contradict the ba-
sic label predictions made for the same sentence;
in other words, we pretend that the prediction
for the third word was just +AUX/L+ rather than
PRED+AUX/L+. Up to 13% of all predictions
were simplified in this way for the most complex
supertag model.
The last columns of Table 2 give the number of
different supertags in the training set and the per-
formance of the retrained TnT on the test set in
single-tagging mode. Although the number of oc-
291
curring tags rises and the prediction accuracy falls
with the supertag complexity, the correlation is not
absolute: It seems markedly easier to predict su-
pertags with complements but no direction infor-
mation (C) than supertags with direction informa-
tion but no complements (B), although the tag set
is larger by an order of magnitude. In fact, the pre-
diction of attachment direction seems much more
difficult than that of undirected supertags in every
case, due to the semi-free word order of German.
The greater tag set size when predicting comple-
ments of each words is at least partly offset by
the contextual information available to the n-gram
model, since it is much more likely that a word
will have, e.g., a ?SUBJ? complement when an ad-
jacent ?SUBJ? supertag is present.
For the simplest model A, all 35 possible su-
pertags actually occur, while in the most compli-
cated model J, only 12,947 different supertags are
observed in the training data (out of a theoretically
possible 1024 for a set of 35 edge labels). Note that
this is still considerably larger than most other re-
ported supertag sets. The prediction quality falls to
rather low values with the more complicated mod-
els; however, our goal in this paper is not to opti-
mize the supertagger, but to estimate the effect that
an imperfect one has on an existing parser. Alto-
gether most results fall into a range of 70?80% of
accuracy; as we will see later, this is in fact enough
to provide a benefit to automatic parsing.
Although supertag accuracy is usually deter-
mined by simply counting matching and non-
matching predictions, a more accurate measure
should take into account how many of the indi-
vidual predictions that are combined into a su-
pertag are correct or wrong. For instance, a word
that is attached to its left as a subject, is pre-
ceded by a preposition and an attributive adjec-
tive, and followed by an apposition would bear
the supertag PP,ATTR+SUBJ/L+APP. Since the
prepositional attachment is notoriously difficult to
predict, a supertagger might miss it and emit the
slightly different tag ATTR+SUBJ/L+APP. Al-
though this supertag is technically wrong, it is in
fact much more right than wrong: of the four pre-
dictions of label, direction, preceding and follow-
ing dependents, three are correct and only one is
wrong. We therefore define the component accu-
racy for a given model as the ratio of correct pre-
dictions among the possible ones, which results
in a value of 0.75 rather than 0 for the exam-
ple prediction. The component accuracy of the su-
pertag model J e. g. is in fact 84.5% rather than
67.6%. We would expect the component accuracy
to match the effect on parsing more closely than
the supertag accuracy.
3 Using supertag information in WCDG
Weighted Constraint Dependency Grammar
(WCDG) is a formalism in which declarative
constraints can be formulated that describe
well-formed dependency trees in a particular
natural language. A grammar composed of such
constraints can be used for parsing by feeding it
to a constraint-solving component that searches
for structures that satisfy the constraints.
Each constraint carries a numeric score or penalty
between 0 and 1 that indicates its importance. The
penalties of all instances of constraint violations
are multiplied to yield a score for an entire anal-
ysis; hence, an analysis that satisfies all rules of
the WCDG bears the score 1, while lower values
indicate small or large aberrations from the lan-
guage norm. A constraint penalty of 0, then, cor-
responds to a hard constraint, since every analysis
that violates such a constraint will always bear the
worst possible score of 0. This means that of two
constraints, the one with the lower penalty is more
important to the grammar.
Since constraints can be soft as well as hard, pars-
ing in the WCDG formalism amounts to multi-
dimensional optimization. Of two possible analy-
ses of an utterance, the one that satisfies more (or
more important) constraints is always preferred.
All knowledge about grammatical rules is encoded
in the constraints that (together with the lexicon)
constitute the grammar. Adding a constraint which
is sensitive to supertag predictions will therefore
change the objective function of the optimiza-
tion problem, hopefully leading to a higher share
of correct attachments. Details about the WDCG
parser can be found in (Foth and Menzel, 2006).
A grammar of German is available (Foth et al,
2004) that achieves a good accuracy on written
German input. Despite its good results, it seems
probable that the information provided by a su-
pertag prediction component could improve the
accuracy further. First, because the optimization
problem that WCDG defines is infeasible to solve
exactly, the parser must usually use incomplete,
292
heuristic algorithms to try to compute the opti-
mal analysis. This means that it sometimes fails
to find the correct analysis even if the language
model accurately defines it, because of search er-
rors during heuristic optimization. A component
that makes specific predictions about local struc-
ture could guide the process so that the correct
alternative is tried first in more cases, and help
prevent such search errors. Second, the existing
grammar rules deal mainly with structural compat-
ibility, while supertagging exploits patterns in the
sequence of words in its input, i. e. both models
contribute complementary information. Moreover,
the parser can be expected to profit from supertags
providing highly lexicalized pieces of information.
Supertag Component Parsing accuracy
Model accuracy accuracy unlabelled labelled
baseline ? ? 89.6% 87.9%
A 84.1% 84.1% 90.8% 89.4%
B 78.9% 85.7% 90.6% 89.2%
C 81.1% 88.5% 91.0% 89.6%
D 76.9% 90.8% 91.1% 89.8%
E 80.6% 91.8% 90.9% 89.6%
F 76.2% 90.9% 91.4% 90.0%
G 71.8% 81.3% 90.8% 89.4%
H 67.9% 85.8% 90.8% 89.4%
I 71.6% 84.3% 91.8% 90.4%
J 67.6% 84.5% 91.8% 90.5%
Table 3: Influence of supertag integration on pars-
ing accuracy.
Parsing accuracy
Constraint penalty unlabelled labelled
0.0 3.7% 3.7%
0.05 85.2% 83.5%
0.1 87.6% 85.7%
0.2 88.9% 87.3%
0.5 91.2% 89.5%
0.7 91.5% 90.1%
0.9 91.8% 90.5%
0.95 91.1% 89.8%
1.0 89.6% 87.9%
Table 4: Parsing accuracy depending on different
strength of supertag integration.
To make the information from the supertag se-
quence available to the parser, we treat the com-
plex supertags as a set of predictions and write
constraints to prefer those analyses that satisfy
them. The predictions of label and direction made
by models A and B are mapped onto two con-
straints which demand that each word in the anal-
ysis should exhibit the predicted label and direc-
tion. The more complicated supertag models con-
strain the local context of each word further. Effec-
tively, they predict that the specified dependents of
a word occur, and that no other dependents occur.
The former prediction equates to an existence con-
dition, so constraints are added which demand the
presence of the predicted relation types under that
word (one for left dependents and one for right de-
pendents). The latter prediction disallows all other
dependents; it is implemented by two constraints
that test the edge label of each word-to-word at-
tachment against the set of predicted dependents
of the regent (again, separately for left and right
dependents). Altogether six new constraints are
added to the grammar which refer to the output
of the supertagger on the current sentence.
Note that in contrast to most other approaches we
do not perform multi-supertagging; exactly one
supertag is assumed for each word. Alternatives
could be integrated by computing the logical dis-
junctions of the predictions made by each su-
pertag, and then adapting the new constraints ac-
cordingly.
4 Experiments
We tested the effect of supertag predictions on
a full parser by adding the new constraints to
the WCDG of German described in (Foth et al,
2004) and re-parsing the same 1,000 sentences
from the NEGRA corpus. The quality of a de-
pendency parser such as this can be measured as
the ratio of correctly attached words to all words
(structural accuracy) or the ratio of the correctly
attached and correctly labelled words to all words
(labelled accuracy). Note that because the parser
always finds exactly one analysis with exactly one
subordination per word, there is no distinction be-
tween recall and precision. The structural accuracy
without any supertags is 89.6%.
To determine the best trade-off between complex-
ity and prediction quality, we tested all 10 supertag
models against the baseline case of no supertags at
all. The results are given in Table 3. Two observa-
tions can be made about the effect of the supertag
model on parsing. Firstly, all types of supertag pre-
diction, even the very basic model A which pre-
dicts only edge labels, improve the overall accu-
racy of parsing, although the baseline is already
quite high. Second, the richer models of supertags
appear to be more suitable for guiding the parser
than the simpler ones, even though their own ac-
curacy is markedly lower; almost one third of the
supertag predictions according to the most compli-
293
cated definition J are wrong, but nevertheless their
inclusion reduces the remaining error rate of the
parser by over 20%.
This result confirms the assumption that if su-
pertags are integrated as individual constraints,
their component accuracy is more important than
the supertag accuracy. The decreasing accuracy of
more complex supertags is more than counterbal-
anced by the additional information that they con-
tribute to the analysis. Obviously, this trend can-
not continue indefinitely; a supertag definition that
predicted even larger parts of the dependency tree
would certainly lead to much lower accuracy by
even the most lenient measure, and a prediction
that is mostly wrong must ultimately degrade pars-
ing performance. Since the most complex model J
shows no parsing improvement over its succes-
sor I, this point might already have been reached.
The use of supertags in WCDG is comparable
to previous work which integrated POS tagging
and chunk parsing. (Foth and Hagenstro?m, 2002;
Daum et al, 2003) showed that the correct bal-
ance between the new knowledge and the exist-
ing grammar is crucial for successful integration.
This is achieved by means of an additional pa-
rameter, modeling how trustworthy supertag pre-
dictions are considered. Its effect is shown in Ta-
ble 4. As expected, making supertag constraints
hard (with a value of 0.0) over-constrains most
parsing problems, so that hardly any analyses can
be computed. Other values near 0 avoid this prob-
lem but still lead to much worse overall perfor-
mance, as wrong or even impossible predictions
too often overrule the normal syntax constraints.
The previously used value of 0.9 actually yields
the best results with this particular grammar.
The fact that a statistical model can improve pars-
ing performance when superimposed on a sophis-
ticated hand-written grammar is of particular in-
terest because the statistical model we used is so
simple, and in fact not particularly accurate; it
certainly does not represent the state of the art
in supertagging. This gives rise to the hope that
as better supertaggers for German become avail-
able, parsing results will continue to see additional
improvements, i.e., future supertagging research
will directly benefit parsing. The obvious ques-
tion is how great this benefit might conceivably
become under optimal conditions. To obtain this
upper limit of the utility of supertags we repeated
Supertag Constraint penalty
model 0.9 0.0
A 92.7% / 92.2% 94.0% / 94.0%
B 94.3% / 93.7% 96.0% / 96.0%
C 92.8% / 92.4% 94.1% / 94.1%
D 94.3% / 93.8% 96.0% / 96.0%
E 93.1% / 92.6% 94.3% / 94.3%
F 94.6% / 94.1% 96.1% / 96.1%
G 94.2% / 93.7% 95.8% / 95.8%
H 95.2% / 94.7% 97.4% / 97.4%
I 97.1% / 96.8% 99.5% / 99.5%
J 97.1% / 96.8% 99.6% / 99.6%
Table 5: Unlabelled and labelled parsing accuracy
with a simulated perfect supertagger.
the process of translating each supertag into addi-
tional WCDG constraints, but this time using the
test set itself rather than TnT?s predictions.
Table 5 again gives the unlabelled and labelled
parsing accuracy for all 10 different supertag mod-
els with the integration strengths of 0 and 0.9.
(Note that since all our models predict the edge
label of each word, hard integration of perfect
predictions eliminates the difference between la-
belled und unlabelled accuracy.) As expected, an
improved accuracy of supertagging would lead
to improved parsing accuracy in each case. In
fact, knowing the correct supertag would solve the
parsing problem almost completely with the more
complex models. This confirms earlier findings for
English (Nasr and Rambow, 2004).
Since perfect supertaggers are not available, we
have to make do with the imperfect ones that do
exist. One method of avoiding some errors intro-
duced by supertagging would be to reject supertag
predictions that tend to be wrong. To this end, we
ran the supertagger on its training set and deter-
mined the average component accuracy of each
occurring supertag. The supertags whose average
precision fell below a variable threshold were not
considered during parsing as if the supertagger had
not made a prediction. This means that a threshold
of 100% corresponds to the baseline of not using
supertags at all, while a threshold of 0% prunes
nothing, so that these two cases duplicate the first
and last line from Table 2.
As Table 6 shows, pruning supertags that are
wrong more often than they are right results in
a further small improvement in parsing accu-
racy: unlabelled syntax accuracy rises up to 92.1%
against the 91.8% if all supertags of model J are
used. However, the effect is not very noticeable,
so that it would be almost certainly more useful to
294
Parsing accuracy
Threshold unlabelled labelled
0% 91.8% 90.5%
20% 91.8% 90.4%
40% 91.9% 90.5%
50% 92.0% 90.7%
60% 92.1% 91.0%
80% 91.4% 90.0%
100% 89.6% 87.9%
Table 6: Parsing accuracy with empirically pruned
supertag predictions.
improve the supertagger itself rather than second-
guess its output.
5 Related work
Supertagging was originally suggested as a
method to reduce lexical ambiguity, and thereby
the amount of disambiguation work done by the
parser. Sakar et al (2000) report that this increases
the speed of their LTAG parser by a factor of 26
(from 548k to 21k seconds) but at the price of only
being able to parse 59% of the sentences in their
test data (of 2250 sentences), because too often the
correct supertag is missing from the output of the
supertagger. Chen et al (2002) investigate differ-
ent supertagging methods as pre-processors to a
Tree-Adjoining Grammar parser, and they claim a
1-best supertagging accuracy of 81.47%, and a 4-
best accuracy of 91.41%. With the latter they reach
the highest parser coverage, about three quarters of
the 1700 sentences in their test data.
Clark and Curran (2004a; 2004b) describe a com-
bination of supertagger and parser for parsing
Combinatory Categorial Grammar, where the tag-
ger is used to filter the parses produced by the
grammar, before the computation of the model pa-
rameters. The parser uses an incremental method:
the supertagger first assigns a small number of cat-
egories to each word, and the parser requests more
alternatives only if the analysis fails. They report
91.4% precision and 91.0% recall of unlabelled
dependencies and a speed of 1.6 minutes to parse
2401 sentences, and claim a parser speedup of a
factor of 77 thanks to supertagging.
The supertagging approach that is closest to ours
in terms of linguistic representations is probably
(Wang and Harper, 2002; Wang and Harper, 2004)
whose ?Super Abstract Role Values? are very sim-
ilar to our model F supertags (Table 2). It is in-
teresting to note that they only report between 328
and 791 SuperARVs for different corpora, whereas
we have 2026 category F supertags. Part of the dif-
ference is explained by our larger label set: 35,
the same as the number of model A supertags
in table 2 against their 24 (White, 2000, p. 50).
Also, we are not using the same corpus. In ad-
dition to determining the optimal SuperARV se-
quence in isolation, Wang and Harper (2002) also
combine the SuperARV n-gram probabilities with
a dependency assignment probability into a depen-
dency parser for English. A maximum tagging ac-
curacy of 96.3% (for sentences up to 100 words) is
achieved using a 4-gram n-best tagger producing
the 100 best SuperARV sequences for a sentence.
The tightly integrated model is able to determine
96.6% of SuperARVs correctly. The parser itself
reaches a labelled precision of 92.6% and a la-
belled recall of 92.2% (Wang and Harper, 2004).
In general, the effect of supertagging in the other
systems mentioned here is to reduce the ambi-
guity in the input to the parser and thereby in-
crease its speed, in some cases dramatically. For
us, supertagging decreases the speed slightly, be-
cause additional constraints means more work for
the parser, and because our supertagger-parser in-
tegration is not yet optimal. On the other hand
it gives us better parsing accuracy. Using a con-
straint penalty of 0.0 for the supertagger integra-
tion (c.f. Table 5) does speed up our parser several
times, but would only be practical with very high
tagging accuracy. An important point is that for
some other systems, like (Sarkar et al, 2000) and
(Chen et al, 2002), parsing is not actually feasible
without the supertagging speedup.
6 Conclusions and future work
We have shown that a statistical supertagging
component can significantly improve the parsing
accuracy of a general-purpose dependency parser
for German. The error rate among syntactic at-
tachments can be reduced by 24% over an al-
ready competitive baseline. After all, the integra-
tion of the supertagging results helped to reach a
quality level which compares favourably with the
state-of-the-art in probabilistic dependency pars-
ing for German as defined with 87.34%/90.38%
labelled/unlabelled attachment accuracy on this
years shared CoNLL task by (McDonald et al,
2005) (see (Foth and Menzel, 2006) for a more de-
tailed comparison). Although the statistical model
used in our system is rather simple-minded, it
clearly captures at least some distributional char-
295
acteristics of German text that the hand-written
rules do not.
A crucial factor for success is the defeasible in-
tegration of the supertagging predictions via soft
constraints. Rather than pursuing a strict filtering
approach where supertagging errors are partially
compensated by an n-best selection, we commit to
only one supertag per word, but reduce its influ-
ence. Treating supertag predictions as weak pref-
erences yields the best results. By measuring the
accuracy of the different types of predictions made
by complex supertags, different weights could also
be assigned to the six new constraints.
Of the investigated supertag models, the most
complex ones guide the parser best, although
their own accuracy is not the best one, even
when measured by the more pertinent component
accuracy. Since purely statistical parsing methods
do not reach comparable parsing accuracy on
the same data, we assume that this trend does
not continue indefinitely, but would stop at some
point, perhaps already reached.
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: an
approach to almost parsing. Computational Linguis-
tics, 25(2):237?265.
T. Brants, R. Hendriks, S. Kramp, B. Krenn, C. Preis,
W. Skut, and H. Uszkoreit. 1997. Das NEGRA-
Annotationsschema. Technical report, Universita?t des
Saarlandes, Computerlinguistik.
S. Brants, St. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc. Work-
shop on Treebanks and Linguistic Theories, Sozopol.
T. Brants. 2000. TnT ? A statistical part-of-speech
tagger. In Proc. the 6th Conf. on Applied Natural Lan-
guage Processing, ANLP-2000, pages 224?231, Seat-
tle, WA.
J. Chen, S. Bangalore, M. Collins, and O. Rambow.
2002. Reranking an N-gram supertagger. In Proc. 6th
Int. Workshop on Tree Adjoining Grammar and Related
Frameworks.
S. Clark and J. R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proc.
20th Int. Conf. on Computational Linguistics.
S. Clark and J. R. Curran. 2004b. Parsing the WSJ us-
ing CCG and log-linear models. In Proc. 42nd Meeting
of the ACL.
M. Daum, K. Foth, and W. Menzel. 2003. Constraint
based integration of deep and shallow parsing tech-
niques. In Proc. 11th Conf. of the EACL, Budapest,
Hungary.
M. Daum, K. Foth, and W. Menzel. 2004. Au-
tomatic transformation of phrase treebanks to depen-
dency trees. In Proc. 4th Int. Conf. on Language Re-
sources and Evaluation, LREC-2004, pages 99?106,
Lisbon, Portugal.
K. Foth and J. Hagenstro?m. 2002. Tagging for robust
parsers. In 2nd Workshop on Robust Methods in Anal-
ysis of Natural Language Data, ROMAND-2002, pages
21 ? 32, Frascati, Italy.
K. Foth and W. Menzel. 2006. Hybrid parsing: Us-
ing probabilistic models as predictors for a symbolic
parser. In Proc. 21st Int. Conf. on Computational Lin-
guistics, Coling-ACL-2006, Sydney.
K. Foth, M. Daum, and W. Menzel. 2004. A broad-
coverage parser for german based on defeasible con-
straints. In 7. Konferenz zur Verarbeitung natu?rlicher
Sprache, KONVENS-2004, pages 45?52, Wien.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. Human Language
Technology Conference, HLT/EMNLP-2005, Vancou-
ver, B.C.
A. Nasr and O. Rambow. 2004. A simple string-
rewriting formalism for dependency grammar. In
Coling-Workshop Recent Advances in Dependency
Grammar, pages 17?24, Geneva, Switzerland.
A. Sarkar, F. Xia, and A. Joshi. 2000. Some experi-
ments on indicators of parsing complexity for lexical-
ized grammars. In Proc. COLING Workshop on Effi-
ciency in Large-Scale Parsing Systems.
Y. Schabes and A. K. Joshi. 1991. Parsing with lexi-
calized tree adjoining grammar. In M. Tomita, editor,
Current Issues in Parsing Technologies. Kluwer Aca-
demic Publishers.
I. Schro?der, W. Menzel, K. Foth, and M. Schulz. 2000.
Modeling dependency grammar with restricted con-
straints. Traitement Automatique des Langues (T.A.L.),
41(1):97?126.
W. Wang and M. P. Harper. 2002. The SuperARV lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. Conf.
on Empirical Methods in Natural Language Process-
ing, EMNLP-2002, pages 238?247, Philadelphia, PA.
W. Wang and M. P. Harper. 2004. A statistical
constraint dependency grammar (CDG) parser. In
Proc. ACL Workshop Incremental Parsing: Bringing
Engineering and Cognition Together, pages 42?49,
Barcelona, Spain.
Ch. M. White. 2000. Rapid Grammar Development
and Parsing: Constraint Dependency Grammar with
Abstract Role Values. Ph.D. thesis, Purdue University,
West Lafayette, IN.
296
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 321?328,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Hybrid Parsing:
Using Probabilistic Models as Predictors for a Symbolic Parser
Kilian A. Foth, Wolfgang Menzel
Department of Informatics
Universita?t Hamburg, Germany
{foth|menzel}@informatik.uni-hamburg.de
Abstract
In this paper we investigate the benefit
of stochastic predictor components for the
parsing quality which can be obtained with
a rule-based dependency grammar. By in-
cluding a chunker, a supertagger, a PP at-
tacher, and a fast probabilistic parser we
were able to improve upon the baseline by
3.2%, bringing the overall labelled accu-
racy to 91.1% on the German NEGRA cor-
pus. We attribute the successful integra-
tion to the ability of the underlying gram-
mar model to combine uncertain evidence
in a soft manner, thus avoiding the prob-
lem of error propagation.
1 Introduction
There seems to be an upper limit for the level
of quality that can be achieved by a parser if it
is confined to information drawn from a single
source. Stochastic parsers for English trained on
the Penn Treebank have peaked their performance
around 90% (Charniak, 2000). Parsing of German
seems to be even harder and parsers trained on the
NEGRA corpus or an enriched version of it still
perform considerably worse. On the other hand,
a great number of shallow components like tag-
gers, chunkers, supertaggers, as well as general or
specialized attachment predictors have been devel-
oped that might provide additional information to
further improve the quality of a parser?s output, as
long as their contributions are in some sense com-
plementory. Despite these prospects, such possi-
bilities have rarely been investigated so far.
To estimate the degree to which the desired syn-
ergy between heterogeneous knowledge sources
can be achieved, we have established an exper-
imental framework for syntactic analysis which
allows us to plug in a wide variety of external
predictor components, and to integrate their con-
tributions as additional evidence in the general
decision-making on the optimal structural inter-
pretation. We refer to this approach as hybrid pars-
ing because it combines different kinds of linguis-
tic models, which have been acquired in totally
different ways, ranging from manually compiled
rule sets to statistically trained components.
In this paper we investigate the benefit of ex-
ternal predictor components for the parsing qual-
ity which can be obtained with a rule-based gram-
mar. For that purpose we trained a range of predic-
tor components and integrated their output into the
parser by means of soft constraints. Accordingly,
the goal of our research was not to extensively op-
timize the predictor components themselves, but
to quantify their contribution to the overall pars-
ing quality. The results of these experiments not
only lead to a better understanding of the utility
of the different knowledge sources, but also allow
us to derive empirically based priorities for fur-
ther improving them. We are able to show that
the potential of WCDG for information fusion is
strong enough to accomodate even rather unreli-
able information from a wide range of predictor
components. Using this potential we were able to
reach a quality level for dependency parsing Ger-
man which is unprecendented so far.
2 Hybrid Parsing
A hybridization seems advantageous even among
purely stochastic models. Depending on their
degree of sophistication, they can and must be
trained on quite different kinds of data collections,
which due to the necessary annotation effort are
available in vastly different amounts: While train-
ing a probabilistic parser or a supertagger usually
321
requires a fully developed tree bank, in the case
of taggers or chunkers a much more shallow and
less expensive annotation suffices. Using a set of
rather simple heuristics, a PP-attacher can even be
trained on huge amounts of plain text.
Another reason for considering hybrid ap-
proaches is the influence that contextual factors
might exert on the process of determining the most
plausible sentence interpretation. Since this influ-
ence is dynamically changing with the environ-
ment, it can hardly be captured from available cor-
pus data at all. To gain a benefit from such con-
textual cues, e.g. in a dialogue system, requires to
integrate yet another kind of external information.
Unfortunately, stochastic predictor components
are usually not perfect, at best producing prefer-
ences and guiding hints instead of reliable certain-
ties. Integrating a number of them into a single
systems poses the problem of error propagation.
Whenever one component decides on the input
of another, the subsequent one will most proba-
bly fail whenever the decision was wrong; if not,
the erroneous information was not crucial anyhow.
Dubey (2005) reported how serious this problem
can be when he coupled a tagger with a subsequent
parser, and noted that tagging errors are by far the
most important source of parsing errors.
As soon as more than two components are in-
volved, the combination of different error sources
migth easily lead to a substantial decrease of the
overall quality instead of achieving the desired
synergy. Moreover, the likelihood of conflicting
contributions will rise tremendously the more pre-
dictor components are involved. Therefore, it is
far from obvious that additional information al-
ways helps. Certainly, a processing regime is
needed which can deal with conflicting informa-
tion by taking its reliability (or relative strength)
into account. Such a preference-based decision
procedure would then allow stronger valued evi-
dence to override weaker one.
3 WCDG
An architecture which fulfills this requirement
is Weighted Constraint Dependency Grammar,
which was based on a model originally proposed
by Maruyama (1990) and later extended with
weights (Schro?der, 2002). A WCDG models nat-
ural language as labelled dependency trees on
words, with no intermediate constituents assumed.
It is entirely declarative: it only contains rules
(called constraints) that explicitly describe the
properties of well-formed trees, but no derivation
rules. For instance, a constraint can state that de-
terminers must precede their regents, or that there
cannot be two determiners for the same regent,
or that a determiner and its regent must agree in
number, or that a countable noun must have a de-
terminer. Further details can be found in (Foth,
2004). There is only a trivial generator compo-
nent which enumerates all possible combinations
of labelled word-to-word subordinations; among
these any combination that satisfies the constraints
is considered a correct analysis.
Constraints on trees can be hard or soft. Of
the examples above, the first two should proba-
bly be considered hard, but the last two could be
made defeasible, particularly if a robust coverage
of potentially faulty input is desired. When two
alternative analyses of the same input violate dif-
ferent constraints, the one that satisfies the more
important constraint should be preferred. WCDG
ensures this by assigning every analysis a score
that is the product of the weights of all instances
of constraint failures. Parsing tries to retrieve the
analysis with the highest score.
The weight of a constraint is usually determined
by the grammar writer as it is formulated. Rules
whose violation would produce nonsensical struc-
tures are usually made hard, while rules that en-
force preferred but not required properties receive
less weight. Obviously this classification depends
on the purpose of a parsing system; a prescrip-
tive language definition would enforce grammat-
ical principles such as agreement with hard con-
straints, while a robust grammar must allow vio-
lations but disprefer them via soft constraints. In
practice, the precise weight of a constraint is not
particularly important as long as the relative im-
portance of two rules is clearly reflected in their
weights (for instance, a misinflected determiner is
a language error, but probably a less severe one
than duplicate determiners). There have been at-
tempts to compute the weights of a WCDG au-
tomatically by observing which weight vectors
perform best on a given corpus (Schro?der et al,
2001), but weights computed completely automat-
ically failed to improve on the original, hand-
scored grammar.
Weighted constraints provide an ideal interface
to integrate arbitrary predictor components in a
soft manner. Thus, external predictions are treated
322
the same way as grammar-internal preferences,
e.g. on word order or distance. In contrast to a
filtering approach such a strong integration does
not blindly rely on the available predictions but is
able to question them as long as there is strong
enough combined evidence from the grammar and
the other predictor components.
For our investigations, we used the ref-
erence implementation of WCDG available
from http://nats-www.informatik.
uni-hamburg.de/download, which allows
constraints to express any formalizable property
of a dependency tree. This great expressiveness
has the disadvantage that the parsing problem
becomes NP-complete and cannot be solved
efficiently. However, good success has been
achieved with transformation-based solution
methods that start out with an educated guess
about the optimal tree and use constraint failures
as cues where to change labels, subordinations,
or lexical readings. As an example we show
intermediate and final analyses of a sentence from
our test set (negra-s18959): ?Hier kletterte die
Marke von 420 auf 570 Mark.? (Here the figure
rose from 420 to 570 DM).
SUBJ
PN
PP
PN
PP
OBJA
DET
S
ADV
hier kletterte die Marke von 420 auf 570 Mark .
In the first analysis, subject and object relations
are analysed wrongly, and the noun phrase ?570
Mark? has not been recognized. The analysis is
imperfect because the common noun ?Mark? lacks
a Determiner.
PN
ATTR
PP
PN
PP
SUBJ
DET
S
ADV
hier kletterte die Marke von 420 auf 570 Mark .
The final analysis correctly takes ?570 Mark? as
the kernel of the last preposition, and ?Marke? as
the subject. Altogether, three dependency edges
had to be changed to arrive at this solution.
Figure 1 shows the pseudocode of the best solu-
tion algorithm for WCDG described so far (Foth et
al., 2000). Although it cannot guarantee to find the
best solution to the constraint satisfaction prob-
lem, it requires only limited space and can be in-
terrupted at any time and still returns a solution.
If not interrupted, the algorithm terminates when
A := the set of levels of analysis
W:= the set of all lexical readings of words in the sentence
L := the set of defined dependency labels
E := A ? W ? W ? L = the base set of dependency edges
D := A ? W = the set of domains da,w of all constraint variables
B := ? = the best analysis found
C := ? = the current analysis
{ Create the search space. }
for e ? E
if eval(e) > 0
then da,w := da,w ? {e}
{ Build initial analysis. }
for da,w ? D
e0 = arg max
e?da,w
score(C ? {e})
C := C ? {e0}
B := C
T := ? = tabu set of conflicts removed so far.
U := ? = set of unremovable conflicts.
i := the penalty threshold above which conflicts are ignored.
n := 0
{ Remove conflicts. }
while ? c ? eval(C) \ U : penalty(c) > i
and no interruption occurred
{ Determine which conflict to resolve. }
cn := arg max
c?eval(C)\U
penalty(c)
T := T ? {c}
{ Find the best resolution set. }
Rn := arg max
R ??domains(cn)
score(replace(C, R))
where replace(C, R) does not cause any c ? T
and |R \ C| <= 2
if no Rn can be found
{ Consider c0 unremovable. }
n := 0, C := B, T := ?, U := U ? {c0}
else
{ Take a step. }
n := n + 1, C := replace(C,Rn)
if score(C) > score(B)
n := 0, B := C, T := ?, U := U ? eval(C)
return B
Figure 1: Basic algorithm for heuristic transfor-
mational search.
no constraints with a weight less than a prede-
fined threshold are violated. In contrast, a com-
plete search usually requires more time and space
than available, and often fails to return a usable re-
sult at all. All experiments described in this paper
were conducted with the transformational search.
For our investigation we use a comprehensive
grammar of German expressed in about 1,000
constraints (Foth et al, 2005). It is intended to
cover modern German completely and to be ro-
323
bust against many kinds of language error. A large
WCDG such as this that is written entirely by hand
can describe natural language with great precision,
but at the price of very great effort for the grammar
writer. Also, because many incorrect analyses are
allowed, the space of possible trees becomes even
larger than it would be for a prescriptive grammar.
4 Predictor components
Many rules of a language have the character of
general preferences so weak that they are eas-
ily overlooked even by a language expert; for in-
stance, the ordering of elements in the German
mittelfeld is subject to several types of preference
rules. Other regularities depend crucially on the
lexical identity of the words concerned; modelling
these fully would require the writing of a spe-
cific constraint for each word, which is all but in-
feasible. Empirically obtained information about
the behaviour of a language would be welcome
in such cases where manual constraints are not
obvious or would require too much effort. This
has already been demonstrated for the case of
part-of-speech tagging: because contextual cues
are very effective in determining the categories of
ambiguous words, purely stochastical models can
achieve a high accuracy. (Hagenstro?m and Foth,
2002) show that the TnT tagger (Brants, 2000)
can be profitably integrated into WCDG parsing:
A constraint that prefers analyses which conform
to TnT?s category predictions can greatly reduce
the number of spurious readings of lexically am-
biguous words. Due to the soft integration of the
tagger, though, the parser is not forced to accept its
predictions unchallenged, but can override them if
the wider syntactic context suggests this. In our
experiments (line 1 in Table 1) this happens 75
times; 52 of these cases were actual errors com-
mitted by the tagger. These advantages taken to-
gether made the tagger the by far most valuable in-
formation source, whithout which the analysis of
arbitrary input would not be feasible at all. There-
fore, we use this component (POS) in all subse-
quent experiments.
Starting from this observation, we extended the
idea to integrate several other external compo-
nents that predict particular aspects of syntax anal-
yses. Where possible, we re-used publicly avail-
able components to make the predictions rather
than construct the best predictors possible; it is
likely that better predictors could be found, but
components ?off the shelf? or written in the sim-
plest workable way proved enough to demonstrate
a positive benefit of the technique in each case.
For the task of predicting the boundaries of
major constituents in a sentence (chunk parsing,
CP), we used the decision tree model TreeTag-
ger (Schmid, 1994), which was trained on arti-
cles from Stuttgarter Zeitung. The noun, verb
and prepositional chunk boundaries that it predicts
are fed into a constraint which requires all chunk
heads to be attached outside the current chunk, and
all other words within it. Obviously such informa-
tion can greatly reduce the number of structural al-
ternatives that have to be considered during pars-
ing. On our test set, the TreeTagger achieves a
precision of 88.0% and a recall of 89.5%.
Models for category disambiguation can easily
be extended to predict not only the syntactic cate-
gory, but also the local syntactic environment of
each word (supertagging). Supertags have been
successfully applied to guide parsing in symbolic
frameworks such as Lexicalised Tree-Adjoning
grammar (Bangalore and Joshi, 1999). To obtain
and evaluate supertag predictions, we re-trained
the TnT Tagger on the combined NEGRA and
TIGER treebanks (1997; 2002). Putting aside the
standard NEGRA test set, this amounts to 59,622
sentences with 1,032,091 words as training data.
For each word in the training set, the local context
was extracted and encoded into a linear represen-
tation. The output of the retrained TnT then pre-
dicts the label of each word, whether it follows or
precedes its regent, and what other types of rela-
tions are found below it. Each of these predictions
is fed into a constraint which weakly prefers de-
pendencies that do not violate the respective pre-
diction (ST). Due to the high number of 12947 su-
pertags in the maximally detailed model, the ac-
curacy of the supertagger for complete supertags
is as low as 67.6%. Considering that a detailed su-
pertag corresponds to several distinct predictions
(about label, direction etc.), it might be more ap-
propriate to measure the average accuracy of these
distinct predictions; by this measure, the individ-
ual predictions of the supertagger are 84.5% accu-
rate; see (Foth et al, 2006) for details.
As with many parsers, the attachment of prepo-
sitions poses a particular problem for the base
WCDG of German, because it is depends largely
upon lexicalized information that is not widely
used in its constraints. However, such information
324
Reannotated Transformed
Predictors Dependencies Dependencies
1: POS only 89.7%/87.9% 88.3%/85.6%
2: POS+CP 90.2%/88.4% 88.7%/86.0%
3: POS+PP 90.9%/89.1% 89.6%/86.8%
4: POS+ST 92.1%/90.7% 90.7%/88.5%
5: POS+SR 91.4%/90.0% 90.0%/87.7%
6: POS+PP+SR 91.6%/90.2% 90.1%/87.8%
7: POS+ST+SR 92.3%/90.9% 90.8%/88.8%
8: POS+ST+PP 92.1%/90.7% 90.7%/88.5%
9: all five 92.5%/91.1% 91.0%/89.0%
Table 1: Structural/labelled parsing accuracy with
various predictor components.
can be automatically extracted from large corpora
of trees or even raw text: prepositions that tend
to occur in the vicinity of specific nouns or verbs
more often than chance would suggest can be as-
sumed to modify those words preferentially (Volk,
2002).
A simple probabilistic model of PP attachment
(PP) was used that counts only the occurrences of
prepositions and potential attachment words (ig-
noring the information in the kernel noun of the
PP). It was trained on both the available tree banks
and on 295,000,000 words of raw text drawn from
the taz corpus of German newspaper text. When
used to predict the probability of the possible
regents of each preposition in each sentence, it
achieved an accuracy of 79.4% and 78.3%, respec-
tively (see (Foth and Menzel, 2006) for details).
The predictions were integrated into the grammar
by another constraint which disprefers all possible
regents to the corresponding degree (except for the
predicted regent, which is not penalized at all).
Finally, we used a full dependency parser in or-
der to obtain structural predictions for all words,
and not merely for chunk heads or prepositions.
We constructed a probabilistic shift-reduce parser
(SR) for labelled dependency trees using the
model described by (Nivre, 2003): from all avail-
able dependency trees, we reconstructed the se-
ries of parse actions (shift, reduce and attach)
that would have constructed the tree, and then
trained a simple maximum-likelihood model that
predicts parse actions based on features of the cur-
rent state such as the categories of the current
and following words, the environment of the top
stack word constructed so far, and the distance be-
tween the top word and the next word. This oracle
parser achieves a structural and labelled accuracy
of 84.8%/80.5% on the test set but can only predict
projective dependency trees, which causes prob-
lems with about 1% of the edges in the 125,000
dependency trees used for training; in the inter-
est of simplicity we did not address this issue spe-
cially, instead relying on the ability of the WCDG
parser to robustly integrate even predictions which
are wrong by definition.
5 Evaluation
Since the WCDG parser never fails on typical tree-
bank sentences, and always delivers an analysis
that contains exactly one subordination for each
word, the common measures of precision, recall
and f-score all coincide; all three are summarized
as accuracy here. We measure the structural (i.e.
unlabelled) accuracy as the ratio of correctly at-
tached words to all words; the labelled accuracy
counts only those words that have the correct re-
gent and also bear the correct label. For compar-
ison with previous work, we used the next-to-last
1,000 sentences of the NEGRA corpus as our test
set. Table 1 shows the accuracy obtained.1
The gold standard used for evaluation was de-
rived from the annotations of the NEGRA tree-
bank (version 2.0) in a semi-automatic procedure.
First, the NEGRA phrase structures were auto-
matically transformed to dependency trees with
the DEPSY tool (Daum et al, 2004). However,
before the parsing experiments, the results were
manually corrected to (1) take care of system-
atic inconsistencies between the NEGRA annota-
tions and the WCDG annotations (e.g. for non-
projectivities, which in our case are used only if
necessary for an ambiguity free attachment of ver-
bal arguments, relative clauses and coordinations,
but not for other types of adjuncts) and (2) to re-
move inconsistencies with NEGRAs own annota-
tion guidelines (e.g. with regard to elliptical and
co-ordinated structures, adverbs and subordinated
main clauses.) To illustrate the consequences of
these corrections we report in Table 1 both kinds
of results: those obtained on our WCDG-conform
annotations (reannotated) and the others on the
raw output of the automatic conversion (trans-
1Note that the POS model employed by TnT was trained
on the entire NEGRA corpus, so that there is an overlap be-
tween the training set of TnT and the test set of the parser.
However, control experiments showed that a POS model
trained on the NEGRA and TIGER treebanks minus the test
set results in the same parsing accuracy, and in fact slightly
better POS accuracy. All other statistical predictors were
trained on data disjunct from the test set.
325
formed), although the latter ones introduce a sys-
tematic mismatch between the gold standard and
the design principles of the grammar.
The experiments 2?5 show the effect of adding
the POS tagger and one of the other predictor com-
ponents to the parser. The chunk parser yields
only a slight improvement of about 0.5% accu-
racy; this is most probably because the baseline
parser (line 1) does not make very many mistakes
at this level anyway. For instance, the relation type
with the highest error rate is prepositional attach-
ment, about which the chunk parser makes no pre-
dictions at all. In fact, the benefit of the PP com-
ponent alone (line 3) is much larger even though
it predicts only the regents of prepositions. The
two other components make predictions about all
types of relations, and yield even bigger benefits.
When more than one other predictor is added to
the grammar, the beneft is generally higher than
that of either alone, but smaller than the sum of
both. An exception is seen in line 8, where the
combination of POS tagging, supertagging and PP
prediction fails to better the results of just POS
tagging and supertagging (line 4). Individual in-
spection of the results suggests that the lexicalized
information of the PP attacher is often counter-
acted by the less informed predictions of the su-
pertagger (this was confirmed in preliminary ex-
periments by a gain in accuracy when prepositions
were exempted from the supertag constraint). Fi-
nally, combining all five predictors results in the
highest accuracy of all, improving over the first
experiment by 2.8% and 3.2% for structural and
labelled accuracy respectively.
We see that the introduction of stochastical in-
formation into the handwritten language model is
generally helpful, although the different predictors
contribute different types of information. The POS
tagger and PP attacher capture lexicalized regular-
ities which are genuinely new to the grammar: in
effect, they refine the language model of the gram-
mar in places that would be tedious to describe
through individual rules. In contrast, the more
global components tend to make the same predic-
tions as the WCDG itself, only explicitly. This
guides the parser so that it tends to check the cor-
rect alternative first more often, and has a greater
chance of finding the global optimum. This ex-
plains why their addition increases parsing accu-
racy even when their own accuracy is markedly
lower than even the baseline (line 1).
6 Related work
The idea of integrating knowledge sources of dif-
ferent origin is not particularly new. It has been
successfully used in areas like speech recognition
or statistical machine translation where acoustic
models or bilingual mappings have to be com-
bined with (monolingual) language models. A
similar architecture has been adopted by (Wang
and Harper, 2004) who train an n-best supertag-
ger and an attachment predictor on the Penn Tree-
bank and obtain an labelled F-score of 92.4%,
thus slightly outperforming the results of (Collins,
1999) who obtained 92.0% on the same sentences,
but evaluating on transformed phrase structure
trees instead on directly computed dependency re-
lations.
Similar to our approach, the result of (Wang
and Harper, 2004) was achieved by integrating
the evidence of two (stochastic) components into
a single decision procedure on the optimal inter-
pretation. Both, however, have been trained on
the very same data set. Combining more than
two different knowledge sources into a system
for syntactic parsing to our knowledge has never
been attempted so far. The possible synergy be-
tween different knowledge sources is often as-
sumed but viable alternatives to filtering or selec-
tion in a pipelined architecture have not yet been
been demonstrated successfully. Therefore, exter-
nal evidence is either used to restrict the space of
possibilities for a subsequent component (Clark
and Curran, 2004) or to choose among the alter-
native results which a traditional rule-based parser
usually delivers (Malouf and van Noord, 2004). In
contrast to these approaches, our system directly
integrates the available evidence into the decision
procedure of the rule-based parser by modifying
the objective function in a way that helps guiding
the parsing process towards the desired interpre-
tation. This seems to be crucial for being able to
extend the approach to multiple predictors.
An extensive evaluation of probabilistic de-
pendency parsers has recently been carried out
within the framework of the 2006 CoNLL
shared task (see http://nextens.uvt.nl/
?conll). Most successful for many of the 13 dif-
ferent languages has been the system described in
(McDonald et al, 2005). This approach is based
on a procedure for online large margin learning
and considers a huge number of locally available
features to predict dependency attachments with-
326
out being restricted to projective structures. For
German it achieves 87.34% labelled and 90.38%
unlabelled attachment accuracy. These results are
particularly impressive, since due to the strictly lo-
cal evaluation of attachment hypotheses the run-
time complexity of the parser is only O(n2).
Although a similar source of text has been used
for this evaluation (newspaper), the numbers can-
not be directly compared to our results since both
the test set and the annotation guidelines differ
from those used in our experiments. Moreover, the
different methodologies adopted for system devel-
opment clearly favour a manual grammar develop-
ment, where more lexical resources are available
and because of human involvement a perfect iso-
lation between test and training data can only be
guaranteed for the probabilistic components. On
the other hand CoNLL restricted itself to the eas-
ier attachment task and therefore provided the gold
standard POS tag as part of the input data, whereas
in our case pure word form sequences are anal-
ysed and POS disambiguation is part of the task
to be solved. Finally, punctuation has been ig-
nored in the CoNLL evaluation, while we included
it in the attachment scores. To compensate for the
last two effects we re-evaluated our parser without
considering punctuation but providing it with per-
fect POS tags. Thus, under similar conditions as
used for the CoNLL evaluation we achieved a la-
belled accuracy of 90.4% and an unlabelled one of
91.9%.
Less obvious, though, is a comparison with re-
sults which have been obtained for phrase struc-
ture trees. Here the state of the art for German is
defined by a system which applies treebank trans-
formations to the original NEGRA treebank and
extends a Collins-style parser with a suffix analy-
sis (Dubey, 2005). Using the same test set as the
one described above, but restricting the maximum
sentence length to 40 and providing the correct
POS tag, the system achieved a labelled bracket
F-score of 76.3%.
7 Conclusions
We have presented an architecture for the fusion of
information contributed from a variety of compo-
nents which are either based on expert knowledge
or have been trained on quite different data col-
lections. The results of the experiments show that
there is a high degree of synergy between these
different contributions, even if they themselves are
fairly unreliable. Integrating all the available pre-
dictors we were able to improve the overall la-
belled accuracy on a standard test set for German
to 91.1%, a level which is as least as good as the
results reported for alternative approaches to pars-
ing German.
The result we obtained also challenges the com-
mon perception that rule-based parsers are neces-
sarily inferior to stochastic ones. Supplied with
appropriate helper components, the WCDG parser
not only reached a surprisingly high level of out-
put quality but in addition appears to be fairly sta-
ble against changes in the text type it is applied to
(Foth et al, 2005).
We attribute the successful integration of dif-
ferent information sources primarily to the funda-
mental ability of the WCDG grammar to combine
evidence in a soft manner. If unreliable informa-
tion needs to be integrated, this possibility is cer-
tainly an undispensible prerequisite for prevent-
ing local errors from accumulating and leading to
an unacceptably low degree of reliability for the
whole system eventually. By integrating the dif-
ferent predictors into the WCDG parsers?s general
mechanism for evidence arbitration, we not only
avoided the adverse effect of individual error rates
multiplying out, but instead were able to even raise
the degree of output quality substantially.
From the fact that the combination of all pre-
dictor components achieved the best results, even
if the individual predictions are fairly unreliable,
we can also conclude that diversity in the selec-
tion of predictor components is more important
than the reliability of their contributions. Among
the available predictor components which could
be integrated into the parser additionally, the ap-
proach of (McDonald et al, 2005) certainly looks
most promising. Compared to the shift-reduce
parser which has been used as one of the pre-
dictor components for our experiments, it seems
particularly attractive because it is able to predict
non-projective structures without any additional
provision, thus avoiding the misfit between our
(non-projective) gold standard annotations and the
restriction to projective structures that our shift-
reduce parser suffers from.
Another interesting goal of future work might
be to even consider dynamic predictors, which
can change their behaviour according to text type
and perhaps even to text structure. This, however,
would also require extending and adapting the cur-
327
rently dominating standard scenario of parser eval-
uation substantially.
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Thorsten Brants, Roland Hendriks, Sabine Kramp,
Brigitte Krenn, Cordula Preis, Wojciech Skut,
and Hans Uszkoreit. 1997. Das NEGRA-
Annotationsschema. Negra project report, Uni-
versita?t des Saarlandes, Computerlinguistik,
Saarbru?cken, Germany.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Thorsten Brants. 2000. TnT ? A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference (ANLP-
2000), Seattle, WA, USA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. NAACL-2000.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proc. 20th Int. Conf. on Computational Lin-
guistics, Coling-2004.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd thesis, Uni-
versity of Pennsylvania, Philadephia, PA.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks
to dependency trees. In Proc. 4th Int. Conf. on Lan-
guage Resources and Evaluation, LREC-2004, Lis-
bon, Portugal.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: parsing German with suffix analysis and
smoothing. In Proc. 43rd Annual Meeting of the
ACL, Ann Arbor, MI.
Kilian Foth and Wolfgang Menzel. 2006. The benefit
of stochastic PP-attachment to a rule-based parser.
In Proc. 21st Int. Conf. on Computational Linguis-
tics, Coling-ACL-2006, Sydney.
Kilian A. Foth, Wolfgang Menzel, and Ingo Schro?der.
2000. A Transformation-based Parsing Technique
with Anytime Properties. In 4th Int. Workshop on
Parsing Technologies, IWPT-2000, pages 89 ? 100.
Kilian Foth, Michael Daum, and Wolfgang Menzel.
2005. Parsing unrestricted German text with defea-
sible constraints. In H. Christiansen, P. R. Skad-
hauge, and J. Villadsen, editors, Constraint Solv-
ing and Language Processing, volume 3438 of Lec-
ture Notes in Artificial Intelligence, pages 140?157.
Springer-Verlag, Berlin.
Kilian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. 21st Int. Conf. on Computational
Linguistics, Coling-ACL-2006, Sydney.
Kilian Foth. 2004. Writing Weighted Constraints
for Large Dependency Grammars. In Proc. Re-
cent Advances in Dependency Grammars, COLING-
Workshop 2004, Geneva, Switzerland.
Jochen Hagenstro?m and Kilian A. Foth. 2002. Tagging
for robust parsers. In Proc. 2nd. Int. Workshop, Ro-
bust Methods in Analysis of Natural Language Data,
ROMAND-2002.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. IJCNLP-04 Workshop Beyond
Shallow Analyses - Formalisms and statistical mod-
eling for deep analyses, Sanya City, China.
Hiroshi Maruyama. 1990. Structural disambiguation
with constraint propagation. In Proc. 28th Annual
Meeting of the ACL (ACL-90), pages 31?38, Pitts-
burgh, PA.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing, HLT/EMNLP-2005, Vancouver, B.C.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proc. 4th Interna-
tional Workshop on Parsing Technologies, IWPT-
2003, pages 149?160.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Int. Conf. on New
Methods in Language Processing, Manchester, UK.
Ingo Schro?der, Horia F. Pop, Wolfgang Menzel, and
Kilian Foth. 2001. Learning grammar weights us-
ing genetic algorithms. In Proceedings Eurocon-
ference Recent Advances in Natural Language Pro-
cessing, pages 235?239, Tsigov Chark, Bulgaria.
Ingo Schro?der. 2002. Natural Language Parsing with
Graded Constraints. Ph.D. thesis, Dept. of Com-
puter Science, University of Hamburg, Germany.
Martin Volk. 2002. Combining unsupervised and su-
pervised methods for pp attachment disambiguation.
In Proc. of COLING-2002, Taipeh.
Wen Wang and Mary P. Harper. 2004. A statistical
constraint dependency grammar (CDG) parser. In
Proc. ACL Workshop Incremental Parsing: Bringing
Engineering and Cognition Together, pages 42?49,
Barcelona, Spain.
328
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 223?230,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Benefit of Stochastic PP Attachment to a Rule-Based Parser
Kilian A. Foth and Wolfgang Menzel
Department of Informatics
Hamburg University
D-22527 Hamburg
Germany
foth|menzel@nats.informatik.uni-hamburg.de
Abstract
To study PP attachment disambiguation as
a benchmark for empirical methods in nat-
ural language processing it has often been
reduced to a binary decision problem (be-
tween verb or noun attachment) in a par-
ticular syntactic configuration. A parser,
however, must solve the more general task
of deciding between more than two alter-
natives in many different contexts. We
combine the attachment predictions made
by a simple model of lexical attraction
with a full-fledged parser of German to de-
termine the actual benefit of the subtask
to parsing. We show that the combination
of data-driven and rule-based components
can reduce the number of all parsing errors
by 14% and raise the attachment accuracy
for dependency parsing of German to an
unprecedented 92%.
1 Introduction
Most NLP applications are either data-driven
(classification tasks are solved by comparing pos-
sible solutions to previous problems and their so-
lutions) or rule-based (general rules are formu-
lated which must be applicable to all cases that
might be encountered). Both methods face obvi-
ous problems: The data-driven approach is at the
mercy of its training set and cannot easily avoid
mistakes that result from biased or scarce data. On
the other hand, the rule-based approach depends
entirely on the ability of a computational linguist
to anticipate every construction that might ever oc-
cur. These handicaps are part of the reason why,
despite great advances, many tasks in computa-
tional linguistics still cannot be performed nearly
as well by computers as by human informants.
Applied to the subtask of syntax analysis, the di-
chotomy manifests itself in the existence of learnt
and handwritten grammars of natural languages.
A great many formalisms have been advanced that
fall into either of the two variants, but even the
best of them cannot be said to interpret arbitrary
input consistently in the same way that a human
reader would. Because the handicaps of differ-
ent methods are to some degree complementary,
it seems likely that a combination of approaches
could yield better results than either alone. We
therefore integrate a data-driven classifier for the
special task of PP attachment into an existing rule-
based parser and measure the effect that the addi-
tional information has on the overall accuracy.
2 Motivation
PP attachment disambiguation has often been
studied as a benchmark test for empirical meth-
ods in natural language processing. Prepositions
allow subordination to many different attachment
sites, and the choice between them is influenced
by factors from many different linguistic levels,
which are generally subject to preferential rather
than rigorous regularities. For this reason, PP at-
tachment is a comparatively difficult subtask for
rule-based syntax analysis and has often been at-
tacked by statistical methods.
Because probabilistic approaches solve PP at-
tachment as a natural subtask of parsing anyhow,
the obvious application of a PP attacher is to in-
tegrate it into a rule-based system. Perhaps sur-
prisingly, so far this has rarely been done. One
reason for this is that many rule-driven syntax an-
alyzers provide no obvious way to integrate un-
certain, statistical information into their decisions.
Another is the traditional emphasis on PP attach-
ment as a binary classification task; since (Hin-
dle and Rooth, 1991), research has concentrated
on resolving the ambiguity in the category pattern
?V+N+P+N?, i.e. predicting the PP attachment to
either the verb or the first noun. It is often assumed
that the correct attachment is always among these
223
two options, so that all problem instances can be
solved correctly despite the simplification. This
task is sufficient to measure the relative quality of
different probability models, but it is quite differ-
ent from what a parser must actually do: It is easier
because the set of possible answers is pre-filtered
so that only a binary decision remains, and the
baseline performance for pure guessing is already
50%. But it is harder because it does not pro-
vide the predictor with all the information needed
to solve many doubtful cases; (Hindle and Rooth,
1991) found that human arbiters consistently reach
a higher agreement when they are given the entire
sentence rather than just the four words concerned.
Instead of the accuracy of PP attachers in the
isolated decision between two words, we investi-
gate the problem of situated PP attachment. In this
task, all nouns and verbs in a sentence are potential
attachment points for a preposition; the computer
must find suitable attachments for one or more
prepositions in parallel, while building a globally
coherent syntax structure at the same time.
3 Methods
Statistical PP attachment is based on the obser-
vation that the identities of content words can be
used to predict which prepositional phrases mod-
ify which words, and achieve better-than-chance
accuracy. This is apparently because, as heads
of their respective phrases, they are representative
enough that they can serve as a crude approxima-
tion of the semantic structure that could be derived
from the phrases. Consider the following example
(the last sentence in our test set):
Die Firmen mu?ssen noch die Bedenken der EU-
Kommission gegen die Fusion ausra?umen. (The compa-
nies have yet to address the Commission?s concerns about
the merger.)
In this sentence, the preferred analysis will pair
the preposition ?gegen? (against, about, versus)
with the noun ?Bedenken? (concerns), since the
proposition is clearly that the concerns pertain to
the merger. A syntax tree of this interpretation is
shown in Figure 1. Note that there are at least
three different syntactically plausible attachment
sites for the preposition. In fact, there are even
more, since a parser can make no initial assump-
tions about the global structure of the syntax tree
that it will construct; for instance, the possibility
that ?gegen? attaches to the noun ?Firmen? (compa-
nies) cannot be ruled out when beginning to parse.
3.1 WCDG
For the following experiments, we used the de-
pendency parser of German described in (Foth et
al., 2005). This system is especially suited to
our goals for several reasons. Firstly, the parser
achieves the highest published dependency-based
accuracy on unrestricted written German input,
but still has a comparatively high error rate for
prepositions. In particular, it mis-attaches the
preposition ?gegen? in the example sentence. Sec-
ond, although rule-based in nature, it uses numer-
ical penalties to arbitrate between different disam-
biguation rules. It is therefore easy to add another
rule of varying strength, which depends on the
output of an external statistical predictor, to guide
the parser when it has no other means of making
an attachment decision. Finally, the parser and
grammar are freely available for use and modi-
fication (http://nats-www.informatik.
uni-hamburg.de/download).
Weighted Constraint Dependency Grammar
(Schro?der, 2002) models syntax structure as la-
belled dependency trees as shown in the exam-
ple. A grammar in this formalism is written as
a set of constraints that license well-formed par-
tial syntax structures. For instance, general projec-
tivity rules ensure that the dependency tree corre-
sponds to a properly nested syntax structure with-
out crossing brackets1 . Other constraints require
an auxiliary verb to be modified by a full verb, or
prescribe morphosyntactical agreement between a
determiner and its regent (the word modified by
the determiner). Although the Constraint Satisfac-
tion Problem that this formalism defines is, in the-
ory, infeasibly hard, it can nevertheless be solved
approximatively with heuristic solution methods,
and achieve competitive parsing accuracy.
To allow the resolution of true ambiguity (the
existence of different structures neither of which is
strictly ungrammatical), weighted constraints can
be written that the solution should satisfy, if this
is possible. The goal is then to build the struc-
ture that violates as few constraints as possible,
and preferentially violates weak rather than strong
constraints. This allows preferences to be ex-
pressed rather than hard rules. For instance, agree-
ment constraints could actually be declared as vio-
lable, since typing errors, reformulations, etc. can
1Some constructions of German actually violate this prop-
erty; exceptions in the projectivity constraints deal with these
cases.
224
AUX
PN
DET
PP
GMOD
DET
OBJA
DET
ADV
S
SUBJ
DET
die
the
Firmen
companies
m?ssen
have to
noch
yet
die
the
Bedenken
concerns
der
the
EU-Kommission
European commission
gegen
about
die
the
Fusion
merger
ausr?umen
address
.
Figure 1: Correct syntax analysis of the example sentence.
and do actually lead to mis-inflected phrases. In
this way robustness against many types of error
can be achieved while still preferring the correct
variant. For more about the WCDG parser, see
(Schro?der, 2002; Foth and Menzel, 2006) .
The grammar of German available for this
parser relies heavily on weighted constraints both
to cope with many kinds of imperfect input and
to resolve true ambiguities. For the example sen-
tence, it retrieves the desired dependencies ex-
cept for constructing the implausible dependency
?ausra?umen?+?gegen? (address against). Let us
briefly review the relevant constraints that cause
this error:
? General structural, valence and agreement
constraints determine the macro structure of
the sentence in the desired way. For in-
stance, the finite and the full verb must com-
bine to form an auxiliary phrase, because this
is the only way of accounting for all words
while satisfying valence and category con-
straints. For the same reasons both deter-
miners must be paired with their respective
nouns. Also, the prepositional phrase itself is
correctly predicted.
? General category constraints ensure that the
preposition can attach to nouns and verbs, but
not, say, to a determiner or to punctuation.
? A weak constraint on adjuncts says that ad-
juncts are usually close to their regent. The
penalty of this constraint varies according to
the length of the dependency that it is applied
to, so that shorter dependencies are generally
preferred.
? A slightly stronger constraint prefers attach-
ment of the preposition to the verb, since
overall verb attachment is more common than
noun attachment in German. Therefore, the
verb attachment leads to the globally best so-
lution for this sentence.
There are no lexicalized rules that capture the
particular plausibility of the phrase ?Bedenken
gegen? (concerns about). A constraint that de-
scribes this individual word pair would be trivial
to write, but it is not feasible to model the general
phenomenon in this way; thousands of constraints
would be needed just to reflect the more impor-
tant collocations in a language, and the exact set
of collocating words is impossible to predict ac-
curately. Data-driven information would be much
more suitable for curing this lexical blind spot.
3.2 The Collocation Measure
The usual way to retrieve the lexical preference of
a word such as ?Bedenken? for ?gegen? is to obtain
a large corpus and assume that it is representative
of the entire language; in particular, that colloca-
tions in this corpus are representative of colloca-
tions that will be encountered in future input. The
assumption is of course not entirely true, but it can
nevertheless be preferable to rely on such uncer-
tain knowledge rather than remain undecided, on
the reasonable assumption that it will lead to more
correct than wrong decisions. Note that the same
reasoning applies to many of the violable con-
straints in a WCDG: although they do not hold on
all possible structures, they hold more often than
they fail, and therefore can be useful for analysing
unknown input.
Different measures have been used to gauge the
strength of a lexical preference, but in general the
efficacy of the statistical approach depends more
on the suitability of the training corpus than on de-
tails of the collocation measure. Since our focus
225
is not on finding the best extraction method, but
on judging the benefit of statistical components to
parsing, we employ a collocation measure related
to the idea of mutual information: a collocation
between a word w and a preposition p is judged
more likely the more often it appears, and the less
often its component words appear. By normalizing
against the total number t of utterances we derive
a measure of Lexical Attraction for each possible
collocation:
LA(w, p) := fw+pt
/
(fw
t ?
fp
t
)
For instance, if we assume that the word ?Be-
denken? occurs in one out of 2,000 sentences of
German and the word ?gegen? occurs in one sen-
tence out of 31 (these figures were taken from
the unsupervised experiment described later), then
pure chance would make the two words co-occur
in one sentence out of 62,000. If the LA score
is higher than 1, i. e. we observe a much higher
frequency of co-occurrences in a large corpus, we
can assume that the two events are not statisti-
cally independent ? in other words, that there is a
positive correlation between the two words. Con-
versely, we would expect a much lower score for
the implausible collocation ?Bedenken?+?fu?r?, in-
dicating a dispreference for this attachment.
4 Experiments
4.1 Sources
To obtain the counts to base our estimates of at-
traction on, we first turned to the dependency tree-
bank that accompanies the WCDG parsing suite.
This corpus contains some 59,000 sentences with
1,000,000 words with complete syntactic annota-
tions, 61% of which are drawn from online tech-
nical newscasts, 33% from literature and 6% from
law texts. We used the entire corpus except for the
test set as a source for counting PP attachments di-
rectly. All verbs, nouns and prepositions were first
reduced to their base forms in order to reduce the
parameter space. Compound nouns were reduced
to their base nouns, so that ?EU-Kommission? is
treated the same as ?Kommission?, on the assump-
tion that the compound exerts similar attractions as
the base noun. In contrast, German verbs with pre-
fixes usually differ markedly in their preferences
from the base verb. Since forms of verbs such as
?ausra?umen? (address) can be split into two parts
(w, p) fw+p fw LA
?Firma?+?gegen? 72 76492 0.03
?Bedenken?+?gegen? 1529 9618 4.96
?Kommission?+?gegen? 223 52415 0.13
?ausra?umen?+?gegen? 130 2342 1.73
(where fp = 566068, t = 17657329)
Table 1: Example calculation of lexical attraction.
(?NP ra?umte NP aus?), such separated verbs were
reassembled before stemming.
Although the information retrieved from com-
plete syntax trees is valuable, it is clearly insuf-
ficient for estimating many valid collocations. In
particular, even for a comparatively strong collo-
cation such as ?Bedenken?+?gegen? we can expect
only very few instances. (There are, in fact, 4
such instances, well above chance level but still
a very small number.) Therefore we used the
archived text from 18 volumes of the newspaper
tageszeitung as a second source. This corpus con-
tains about 295,000,000 words and should allow
us to detect many more collocations. In fact, we
do find 2338 instances of ?Bedenken?+?gegen? in
the same sentence.
Of course, since we have no syntactic annota-
tions for this corpus (and it would be infeasible to
create them even by fully automatic parsing), not
all of these instances may indicate a syntactic de-
pendency. (Ratnaparkhi, 1998) solved this prob-
lem by regarding only prepositions in syntactically
unambiguous configurations. Unfortunately, his
patterns cannot directly be applied to German sen-
tences because of their freer word order. As an
approximation it would be possible to count only
pairs of adjacent content words and prepositions.
However, this would introduce systematic biases
into the counts, because nouns do in fact very often
occur adjacently to prepositions that modify them,
but many verbs do not. For instance, the phrase
?jmd. anklagen wegen etw.? (to sue s.o. for s.th.)
gives rise to a strong collocation between the verb
?anklagen? and the preposition ?wegen?; however,
in the predominant sentence types of German, the
two words are virtually never adjacent, because ei-
ther the preposition kernel or the direct object must
intervene. Therefore, we relax the adjacency con-
dition for verb attachment and also count prepo-
sitions that occur within a fixed distance of their
suspected regent.
Table 1 shows the detailed values when judg-
ing the example sentence according to the un-
parsed corpus. The strong collocation that we
would expect for ?Bedenken?+?gegen? is indeed
226
Value of i Recall for V for N overall
1 96.2% 39.8% 65.2%
2 96.2% 52.0% 71.9%
5 88.8% 66.3% 76.4%
8 80.0% 79.6% 79.8%
10 67.5% 82.7% 75.8%
Table 2: Influence of noun factor on solving isolated attach-
ment decisions.
observed, with a value of 4.96. However, the
verb attachment also has a score above 1, indicat-
ing that ?gegen?+?ausra?umen? (to address about)
are also positively correlated. This is almost cer-
tainly a misleading figure, since those two words
do not form a plausible verb phrase; it is much
more probable that the very strong, in fact id-
iomatic, correlation ?Bedenken ausra?umen? (to ad-
dress concerns) causes many co-occurrences of all
three words. Therefore our figures falsely suggest
that ?gegen? would often attach to ?ausra?umen?,
when it is in fact the direct object of that verb that
it is attracted to.
(Volk, 2002) already suggested that this count-
ing method introduced a general bias toward verb
attachment, and when comparing the results for
very frequent words (for which more reliable evi-
dence is available from the treebank) we find that
verb attachments are in fact systematically over-
estimated. We therefore adopted his approach and
artificially inflated all noun+preposition counts by
a constant factor i. To estimate an appropriate
value for this factor, we extracted 178 instances of
the standard verb+noun+preposition configuration
from our corpus, of which 80 were verb attach-
ments (V) and 98 were noun attachments (N).
Table 2 shows the performance of the predictor
for this binary decision task. Taken as it is, it re-
trieves most verb attachments, but less than half of
the noun attachments, while higher values of i can
improve the recall both for noun attachments and
overall. The performance achieved falls somewhat
short of the highest figures reported previously for
PP attachment for German (Volk, 2002); this is
at least in part due to our simple model that ig-
nores the kernel noun of the PP. However, it could
well be good enough to be integrated into a full
parser and provide a benefit to it. Also, the syntac-
tical configuration in this standard benchmark is
not the predominant one in complete German sen-
tences; in fact fewer than 10% of all prepositions
occur in this context. The best performance on the
triple task is therefore not guaranteed to be the best
choice for full parsing. In our experiments, we
1.0
0.8
1 3 5
weight
LA
Figure 2: Mapping lexical attraction values to penalties
used a value of i = 8, which seems to be suited
best to our grammar.
4.2 Integration Method
To add our simple collocation model to the parser,
it is sufficient to write a single variable-strength
constraint that judges each PP dependency by how
strong the lexical attraction between the regent and
the dependent is. The only question is how to map
our lexical attraction values to penalties for this
constraint. Their predicted relative order of plausi-
bility should of course be reflected, so that depen-
dencies with a high lexical attraction are preferred
over those with lower lexical attraction. At the
same time, the information should not be given too
much weight compared to the existing grammar
rules, since it is heuristic in nature and should cer-
tainly not override important principles such as va-
lence or agreement. The penalties of WCDG con-
straints range from 0.0 (hard constraint) through
1.0 (a constraint with this penalty has no effect
whatsoever and is only useful for debugging).
We chose an inverse mapping based on the log-
arithm of lexical attraction (cf. Figure 2):
p(w, p) = max(1,min(0.8,1?(2?log3(LA(w,p)))/50))?
where ? is a normalization constant that scales
the highest occurring value of LA to 1. For in-
stance, this mapping will interpret a strong lex-
ical attraction of 5 as the penalty 0.989 (almost
perfect) and a lexical attraction of only 0.5 as the
penalty 0.95 (somewhat dispreferred). The overall
range of PP attachment penalties is limited to the
interval [0.8 ? 1.0], which ensures that the judge-
ment of the statistical module will usually come
into play only when no other evidence is available;
preliminary experiments showed that a stronger
integration of the component yields no additional
advantage. In any case, the exact figure depends
closely on the valuation of the existing constraints
of the grammar and is of little importance as such.
227
Label occurred retrieved errors accuracy
PP 1892 1285 607 67.9%
ADV 1137 951 186 83.6%
OBJA 775 675 100 87.1%
APP 659 567 92 86.0%
SUBJ 1338 1251 87 93.5%
S 1098 1022 76 93.1%
KON 481 406 75 84.4%
REL 167 107 60 64.1%
overall 17719 16073 1646 90.7
Table 3: Performance of the original parser on the test set.
Besides adding the new constraint ?PP attach-
ment? to the grammar, we also disabled several
of the existing constraints that apply to preposi-
tions, since we assume that our lexicalized model
is superior to the unlexicalized assumptions that
the grammar writers had made so far. For instance,
the constraint mentioned in Section 3 that glob-
ally prefers verb attachment to noun attachment
is essentially a crude approximation of lexical at-
traction, whose task is now taken over entirely by
the statistical predictor. We also assume that lex-
ical preference exerts a stronger influence on at-
tachment than mere linear distance; therefore we
changed the distance constraint so that it exempts
prepositions from the normal distance penalties
imposed on adjuncts.
4.3 Corpus
For our parsing experiments, we used the first
1,000 sentences of technical newscasts from the
dependency treebank mentioned above. This test
set has an average sentence length of 17.7 words,
and from previous experiments we estimate that it
is comparable in difficulty to the NEGRA corpus
to within 1% of accuracy. Although online articles
and newspaper copy follow some different con-
ventions, we assume the two text types are similar
enough that collocations extracted from one can
be used to predict attachments in the other.
For parsing we used the heuristic trans-
formation-based search described in (Foth et al,
2000). Table 3 illustrates the structural accuracy2
of the unmodified system for various subordina-
tion types. For instance, of the 1892 dependency
edges with the label ?PP? in the gold standard,
1285 are attached correctly by the parser, while
607 receive an incorrect regent. We see that PP at-
tachment decisions are particularly prone to errors
2Note that the WCDG parser always succeeds in assign-
ing exactly one regent to each word, so that there is no dif-
ference between precision and recall. We refer to structural
accuracy as the ratio of words which have been attached cor-
rectly to all words.
Method PP accuracy overall accuracy
baseline 67.9% 90.7%
supervised 79.4% 91.9%
unsupervised 78.3% 91.9%
backed-off 78.9% 92.2%
Table 4: Structural accuracy of PP edges and all edges.
both in absolute and in relative terms.
4.4 Results
We trained the PP attachment predictor both with
the counts acquired from the dependency treebank
(supervised) and those from the newspaper cor-
pus (unsupervised). We also tested a mode of op-
eration that uses the more reliable data from the
treebank, but backs off to unsupervised counts if
the hypothetical regent was seen fewer than 1,000
times in training.
Table 4 shows the results when parsing with the
augmented grammar. Both the overall structural
accuracy and the accuracy of PP edges are given;
note that these figures result from the general sub-
ordination task, therefore they correspond to Ta-
ble 3 and not to Table 2. As expected, lexical-
ized preference information for prepositions yields
a large benefit to full parsing: the attachment error
rate is decreased by 34% for prepositions, and by
14% overall. In this experiment, where much more
unsupervised training data was available, super-
vised and unsupervised training achieved almost
the same level of performance (although many in-
dividual sentences were parsed differently).
A particular concern with corpus-based deci-
sion methods is their applicability beyond the
training corpus. In our case, the majority of the
material for supervised training was taken from
the same newscast collection as the test set. How-
ever, comparable results are also achieved when
applying the parser to the standard test set from the
NEGRA corpus of German, as used by (Schiehlen,
2004; Foth et al, 2005): adding the PP predic-
tor trained on our dependency treebank raises the
overall attachment accuracy from 89.3% to 90.6%.
This successful reuse indicates that lexical prefer-
ence between prepositions and function words is
largely independent of text type.
5 Related Work
(Hindle and Rooth, 1991) first proposed solving
the prepositional attachment task with the help of
statistical information, and also defined the preva-
lent formulation as a binary decision problem with
three words involved. (Ratnaparkhi et al, 1994)
228
extended the problem instances to quadruples by
also considering the kernel noun of the PP, and
used maximum entropy models to estimate the
preferences.
Both supervised and unsupervised training pro-
cedures for PP attachment have been investigated
and compared in a number of studies, with su-
pervised methods usually being slightly superior
(Ratnaparkhi, 1998; Pantel and Lin, 2000), with
the notable exception of (Volk, 2002), who ob-
tained a worse accuracy in the supervised case,
obviously caused by the limited size of the avail-
able treebank. Combining both methods can lead
to a further improvement (Volk, 2002; Kokkinakis,
2000), a finding confirmed by our experiments.
Supervised training methods already applied to
PP attachment range from stochastic maximum
likelihood (Collins and Brooks, 1995) or maxi-
mum entropy models (Ratnaparkhi et al, 1994)
to the induction of transformation rules (Brill and
Resnik, 1994), decision trees (Stetina and Nagao,
1997) and connectionist models (Sopena et al,
1998). The state-of-the-art is set by (Stetina and
Nagao, 1997) who generalize corpus observations
to semantically similar words as they can be de-
rived from the WordNet hierarchy.
The best result for German achieved so far is
the accuracy of 80.89% obtained by (Volk, 2002).
Note, however, that our goal was not to optimize
the performance of PP attachment in isolation but
to quantify the contribution it can make to the per-
formance of a full parser for unrestricted text.
The accuracy of PP attachment has rarely been
evaluated as a subtask of full parsing. (Merlo et al,
1997) evaluate the attachment of multiple preposi-
tions in the same sentence for English; 85.3% ac-
curacy is achieved for the first PP, 69.6% for the
second and 43.6% for the third. This is still rather
different from our setup, where PP attachment is
fully integrated into the parsing problem. Closer
to our evaluation scenario comes (Collins, 1999)
who reports 82.3%/81.51% recall/precision on PP
modifications for his lexicalized stochastic parser
of English. However, no analysis has been carried
out to determine which model components con-
tributed to this result.
A more application-oriented view has been
adopted by (Schwartz et al, 2003), who devised
an unsupervised method to extract positive and
negative lexical evidence for attachment prefer-
ences in English from a bilingual, aligned English-
Japanese corpus. They used this information to re-
attach PPs in a machine translation system, report-
ing an improvement in translation quality when
translating into Japanese (where PP attachment is
not ambiguous and therefore matters) and a de-
crease when translating into Spanish (where at-
tachment ambiguities are close to the original ones
and therefore need not be resolved).
Parsing results for German have been published
a number of times. Combining treebank transfor-
mation techniques with a suffix analysis, (Dubey,
2005) trained a probabilistic parser and reached a
labelled F-score of 76.3% on phrase structure an-
notations for a subset of the sentences used here
(with a maximum length of 40). For dependency
parsing a labelled accuracy of 87.34% and an un-
labelled one of 90.38% has been achieved by ap-
plying the dependency parser described in (Mc-
Donald et al, 2005) to German data. This system
is based on a procedure for online large margin
learning and considers a huge number of locally
available features, which allows it to determine
the optimal attachment fully deterministically. Us-
ing a stochastic variant of Constraint Dependency
Grammar (Wang and Harper, 2004) reached a
92.4% labelled F-score on the Penn Treebank,
which slightly outperforms (Collins, 1999) who
reports 92.0% on dependency structures automati-
cally derived from phrase structure results.
6 Conclusions and future work
Corpus-based data has been shown to provide a
significant benefit when used to guide a rule-based
dependency parser of German, reducing the er-
ror rate for situated PP attachment by one third.
Prepositions still remain the largest source of at-
tachment errors; many reasons can be tracked
down for individual errors, such as faulty POS
tagging, misinterpreted global sentence structure,
genuinely ambiguous constructions, failure of the
attraction heuristics, or simply lack of process-
ing time. However, considering that even human
arbiters often agree only on 90% of PP attach-
ments, the results appear promising. In particu-
lar, many attachment errors that strongly disagree
with human intuition (such as in the example sen-
tence) were in fact prevented. Thus, the addition
of a corpus-based knowledge source to the sys-
tem yielded a much greater benefit than could have
been achieved with the same effort by writing in-
dividual constraints.
229
One obvious further task is to improve our
simple-minded model of lexical attraction. For in-
stance, some remaining errors suggest that taking
the kernel noun into account would yield a higher
attachment precision; this will require a redesign
of the extraction tools to keep the parameter space
manageable. Also, other subordination types than
?PP? may benefit from similar knowledge; e.g., in
many German sentences the roles of subject and
object are syntactically ambiguous and can only
be understood correctly through world knowledge.
This is another area in which synergy between
lexical attraction estimates and general symbolic
rules appears possible.
References
E. Brill and P. Resnik. 1994. A rule-based approach to
prepositional phrase attachment disambiguation. In
Proc. 15th Int. Conf. on Computational Linguistics,
pages 1198 ? 1204, Kyoto, Japan.
M. Collins and J. Brooks. 1995. Prepositional attach-
ment through a backed-off model. In Proc. of the
3rd Workshop on Very Large Corpora, pages 27?38,
Somerset, New Jersey.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Phd thesis, University
of Pennsylvania, Philadephia, PA.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proc. 43rd Annual Meeting of the ACL, Ann Ar-
bor, MI.
K. Foth and W. Menzel. 2006. Hybrid parsing: Us-
ing probabilistic models as predictors for a symbolic
parser. In Proc. 21st Int. Conf. on Computational
Linguistics, Coling-ACL-2006, Sydney.
K. Foth, W. Menzel, and I. Schro?der. 2000. A
Transformation-based Parsing Technique with Any-
time Properties. In 4th Int. Workshop on Parsing
Technologies, IWPT-2000, pages 89 ? 100.
K. Foth, M. Daum, and W. Menzel. 2005. Parsing un-
restricted German text with defeasible constraints.
In H. Christiansen, P. R. Skadhauge, and J. Vil-
ladsen, editors, Constraint Solving and Language
Processing, volume 3438 of LNAI, pages 140?157.
Springer-Verlag, Berlin.
D. Hindle and M. Rooth. 1991. Structural Ambiguity
and Lexical Relations. In Meeting of the Association
for Computational Linguistics, pages 229?236.
D. Kokkinakis. 2000. Supervised pp-attachment dis-
ambiguation for swedish; (combining unsupervised
supervised training data). Nordic Journal of Lin-
guistics, 3.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. Human Lan-
guage Technology Conference / Conference on Em-
pirical Methods in Natural Language Processing,
HLT/EMNLP-2005, Vancouver, B.C.
P. Merlo, M. Crocker, and C. Berthouzoz. 1997. At-
taching Multiple Prepositional Phrases: General-
ized Backed-off Estimation. In Proc. 2nd Conf. on
Empirical Methods in NLP, pages 149?155, Provi-
dence, R.I.
P. Pantel and D. Lin. 2000. An unsupervised approach
to prepositional phrase attachment using contextu-
ally similar words. In Proc. 38th Meeting of the
ACL, pages 101?108, Hong Kong.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
Maximum Entropy Model for Prepositional Phrase
Attachment. In Proc. ARPA Workshop on Human
Language Technology, pages 250 ?255.
A. Ratnaparkhi. 1998. Statistical models for unsu-
pervised prepositional phrase attachment. In Proc.
17th Int. Conf. on Computational Linguistics, pages
1079?1085, Montreal.
M. Schiehlen. 2004. Annotation Strategies for Proba-
bilistic Parsing in German. In Proceedings of COL-
ING 2004, pages 390?396, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
I. Schro?der. 2002. Natural Language Parsing with
Graded Constraints. Ph.D. thesis, Department of In-
formatics, Hamburg University, Hamburg, Germany.
L. Schwartz, T. Aikawa, and C. Quirk. 2003. Disam-
biguation of english PP-attachment using multilin-
gual aligned data. In Machine Translation Summit
IX, New Orleans, Louisiana, USA.
J. M. Sopena, A. LLoberas, and J. L. Moliner. 1998.
A connectionist approach to prepositional phrase at-
tachment for real world texts. In Proc. 17th Int.
Conf. on Computational Linguistics, pages 1233?
1237, Montreal.
J. Stetina and M. Nagao. 1997. Corpus based PP at-
tachment ambiguity resolution with a semantic dic-
tionary. In Jou Shou and Kenneth Church, editors,
Proc. 5th Workshop on Very Large Corpora, pages
66?80, Hong Kong.
M. Volk. 2002. Combining Unsupervised and Super-
vised Methods for PP Attachment Disambiguation.
In Proc. of COLING-2002, Taipeh.
W. Wang and M. P. Harper. 2004. A statistical
constraint dependency grammar (CDG) parser. In
Proc. ACL Workshop Incremental Parsing: Bringing
Engineering and Cognition Together, pages 42?49,
Barcelona, Spain.
230
Robust Parsing: More with Less
Kilian Foth, Wolfgang Menzel
Fachbereich Informatik, Universita?t Hamburg, Germany
foth|menzel@informatik.uni-hamburg.de
Abstract
Covering as many phenomena as possible is a
traditional goal of parser development, but the
broader a grammar is made, the blunter it may
become, as rare constructions influence the be-
haviour on simple sentences that were already
solved correctly. We observe the effects of in-
tentionally removing support for specific con-
structions from a broad-coverage grammar of
German. We show that accuracy of analysing
sentences from the NEGRA corpus can be im-
proved not only for sentences that do not need
the extra coverage, but even when including
those that do.
1 Introduction
Traditionally, broad coverage has always been consid-
ered to be a desirable property of a grammar: the more
linguistic phenomena are treated properly by the gram-
mar, the better results can be expected when applying
it to unrestricted text (c.f. (Grover et al, 1993; Doran
et al, 1994)). With the advent of empirical methods
and the corresponding evaluationmetrics, however, this
view changed considerably. (Abney, 1996) was among
the first who noted that the relationship between cover-
age and statistical parsing quality is a more complex
one. Adding new rules to the grammar, i.e. increas-
ing its coverage, does not only allow the parser to deal
with more phenomena, hence more sentences; at the
same time it opens up new possibilities for abusing
the newly introduced rules to mis-analyse constructions
which were already treated properly before. As a con-
sequence, a net reduction in parsing quality might be
observed for simple statistical reasons, since the gain
usually is obtained for relatively rare phenomena,while
the adverse effects might well affect frequent ones.
(Abney, 1996) uses this observation to argue in favour
of stochastic models which attempt to choose the opti-
mal structural interpretation instead of only providing
a list of equally probable alternatives. However, using
such an optimization procedure is not necessarily a suf-
ficient precondition to completely rule out the effect.
Compared to traditional handwritten grammars, suc-
cessful stochastic models like (Collins, 1999; Charniak,
2000) open up an even greater space of alternatives for
the parser and accordingly offer a great deal of oppor-
tunities to construct odd structural descriptions from
them. Whether the guidance of the stochastic model
can really prevent the parser from making use of these
unwanted opportunities so far remains unclear.
In the following we make a first attempt to quantify the
consequences that different degrees of coverage have
for the output quality of a wide-coverage parser. For
this purpose we use a Weighted Constraint Dependency
Grammar (WCDG), which covers even relatively rare
syntactic phenomena of German and performs reliably
across a wide variety of different text genres (Foth et
al., 2005). By combining hand-written rules with an
optimization procedure for hypothesis selection, such
a parser makes it possible to successively exclude cer-
tain rare phenomena from the coverage of the grammar
and to study the impact of these modifications on its
output quality
2 Some rare phenomena of German
What are good candidates of ?rare? phenomena that
might be intentionally removed from the coverage of
our grammar?One possibility is to remove coverage for
constructions that are already slightly dispreferred. For
instance, apposition and coordination of noun phrases
often violate the principle of projectivity:
?I got a sled for Christmas, a parrot and a motor-bike.?
This is quite a common construction, but still ?rare? in
the sense that the great majority of appositions does re-
spect projectivity, so that the example seems at least
slightly unusual. But there are also syntactic relations
that are quite rare but nevertheless appear perfectly nor-
mal when they do occur, such as direct appellations:
?James, please open the door.?
This might be because their frequency varies consid-
erably between text types; everyone is familiar with
personal appellation from everyday conversation, but
it would be surprising to hear it from the mouth of a
television news reader.
Finally, some constructions form variants e.g. by omit-
ting certain words:
?I bought a new broom [in order] to clean the drive-
25
No. Phenomenon Example f/1000
1 Mittelfeld extraposition ?Es strahlt u?ber DVB-T neben dem Fernsehprogramm auch seinen Dig-
itext aus, einen Videotext-a?hnlichen Informationsdienst.?
32.5
2 ethical dative ?Noch erobere sich der PC neue Ka?uferschichten, hei?t es weiter.? 18.5
3 Nominalization ?Ta?glich kommen rund 1000 neue hinzu.? 13.4
4 Vocative ?So nicht, ICANN!? 9.7
5 Parenthetical matrix
clause
?Bis zum Jahresende 2002, prognostiziert Roland Berger, werden
die am Neuen Markt gelisteten Unternehmen 200.000 Mitarbeiter
bescha?ftigen.?
8.8
6 verb-first subclause ?Erfu?llt ein Mitgliedstaat keines oder nur eines dieser Kriterien, so
erstellt die Kommission einen Bericht.?
8.3
7 Headline phrase ?Lehrer kaum auf Computer vorbereitet? 3.9
8 coordination cluster ?Auf den Webseiten der Initiative ko?nnen Spender PCs anbieten und
Schulen ihren Bedarf anmelden.?
3.1
9 Adverbial pronoun ?Ihre Sprachen sollen alle gleichberechtigt sein.? 2.6
10 um omission ?Und Dina ging aus, die To?chter des Landes zu sehen.? 2.1
11 Metagrammatical
usage
?Die Bezugnahmen auf die gemeinsame Agrarpolitik oder auf die Land-
wirtschaft und die Verwendung des Wortes ?landwirtschaftlich? sind
in dem Sinne zu verstehen, dass damit unter Beru?cksichtigung der
besonderen Merkmale des Fischereisektors auch die Fischerei gemeint
ist.?
1.8
12 Auxiliary flip ?Die Gescha?digten werfen Ricardo nun eine erhebliche Mitschuld vor,
da gro??erer Schaden ha?tte verhindert werden ko?nnen, wenn der An-
bieter sofort gesperrt worden wa?re.?
1.1
13 Adjectival subclause ?Die Union unterha?lt ferner, soweit zweckdienlich, Beziehungen zu an-
deren internationalen Organisationen.?
0.9
14 Suffix drop ?Ein freundlich Wort, das Maslo intervenieren lie?:? 0.5
15 Elliptical genitive ?Martins war auch nicht besser.? 0.3
16 Adverbial noun ?Sie stehen sich Auge in Auge gegenu?ber.? 0.1
17 Verb/particle mismatch ?Au?er Windows 9x selbst ko?nnen auch andere Hard- und Soft-
warekomponenten eines PC mit zu viel Hauptspeicher manchmal nicht
zurecht.?
0.1
18 Vorfeld extraposition ?Der Verdacht liegt nahe, da? hier Schwarzarbeit betrieben wird.? 0.1
19 double relative subject ?Ich bin der Herr, der ich dich aus ?Agyptenland herausgefu?hrt habe.? 0.02
20 Relative subject clause ?Die dir fluchen, seien verflucht, und die dich segnen, seien gesegnet!? 0.04
21 NP extraposition ?Die Verpflichtungen und die Zusammenarbeit in diesem Bereich
bleiben im Einklang mit den im Rahmen der Nordatlantikvertrags-
Organisation eingegangenen Verpflichtungen, die fu?r die ihr
angeho?renden Staaten weiterhin das Fundament ihrer kollektiven
Verteidigung und das Instrument fu?r deren Verwirklichung ist.?
0.01
Table 1: Some rare phenomena in modern German.
way.?
Here the longer variant is unambiguously a subclause
expressing purpose, while the shorter might be mis-
taken for a prepositional phrase, so it could be regarded
as misleading for the parser.
The selection is necessarily subjective, not only be-
cause the delimitation of a phenomenon is subjective
(are all kinds of ellipsis fundamentally the same phe-
nomenon or not?) but also because we can remove only
those phenomena that are already covered in the first
place. Therefore we have selected phenomena
? that were explicitly added to the grammar at some
point in order to deal with actually occurring un-
foreseen constructions,
? that can easily be removed from the grammar
without affecting other phenomena,
? and that are relatively rare in all the texts we have
investigated.
Table 1 shows the 21 phenomena that we consider in
this paper. (Note that the three earlier example sen-
tences correspond to lines 1, 4, and 10 in this table, but
that not all lines have exact counterparts in English.)
The last column gives the overall frequency per 1,000
sentences of each phenomenon when measured across
all trees in our collection.
The collection contains sections of Bible text (Genesis
1?50), law text (the constitutions of Federal Germany
and of the EuropeanUnion), online technical newscasts
(www.heise.de), novel text, and sentences from the
NEGRA corpus of newspaper articles. Table 2 shows
the sentence counts of the different sections and the
frequency per 1000 of all 21 phenomena in each text
type. It can be seen that most of the constructions re-
main quite rare overall, but often the frequency depends
heavily on the text type, so that a high influence of the
corpus can be expected for our experiments.
26
f /1000 Bible Law Online Novel News overall
Phen. (2,709) (3,722) (55,327) (20,253) (4,000) (86,011)
1 93.6 24.6 29.0 36.7 28.2 32.6
2 59.6 17.5 12.2 31.3 16.2 18.6
3 21.0 22.7 12.3 12.4 19.5 13.4
4 18.4 0.0 0.1 38.2 1.2 9.7
5 1.1 0.0 5.8 18.2 15.8 8.8
6 3.4 51.4 7.8 2.6 6.8 8.3
7 0.7 3.6 4.8 1.3 7.2 3.9
8 7.1 4.4 3.3 2.4 1.8 3.1
9 7.1 0.5 1.6 5.0 3.5 2.6
10 12.7 1.9 1.9 1.2 1.2 2.0
11 0.4 0.3 2.2 0.5 4.8 1.8
12 1.5 0.0 0.9 1.8 1.5 1.1
13 2.2 0.8 1.0 0.5 0.2 0.9
14 0.7 0.0 0.6 1.2 0.2 0.7
15 1.9 0.0 0.7 0.0 1.0 0.5
16 0.4 0.3 0.2 0.0 0.0 0.1
17 1.1 0.0 0.1 0.0 0.0 0.1
18 0.0 0.0 0.1 0.0 0.2 0.1
19 0.7 0.0 0.0 0.0 0.2 0.0
20 0.7 0.0 0.0 0.0 0.0 0.0
21 0.0 0.3 0.0 0.0 0.0 0.0
Table 2: Frequency of phenomena by text type.
3 Weighted Constraint Dependency
Grammar
In WCDG (Schro?der, 2002), natural language is mod-
elled as labelled dependency trees, in which each word
is assigned exactly one other word as its regent (only
the root of the syntax tree remains unsubordinated)
and a label that describes the nature of their relation.
The set of acceptable trees is defined not by way of
generative rules, but only through constraints on well-
formed structures. Every possible dependency tree is
considered correct unless one of its edges or edge pairs
violates a constraint. This permissiveness extends to
many properties that other grammar formalisms con-
sider non-negotiable; for instance, a WCDG can allow
non-projective (or, indeed, cyclical) dependencies sim-
ply by not forbidding them. Since the constraints can
be arbitrary logical formulas, a grammar rule can also
allow some types of non-projective relations and for-
bid others, and in fact the grammar in question does
precisely that.
Weighted constraints can be written to express the fact
that a construction is considered acceptable but not
fully so. This mechanism is used extensively to achieve
robustness against proper errors such as wrong inflec-
tion, ellipsis or mis-ordering; all of these are in fact ex-
pressed through defeasible constraints. But it can also
express more subtle dispreferences against a specific
phenomenonby writing only a weak constraint that for-
bids it; most of the phenomena listed in Table 1 are as-
sociated with such constraints to ensure that the parser
assumes a rare construction only when this is neces-
sary.
We employ a previously existing wide-coverage
WCDG of modern German (Foth et al, 2005)
that covers all of the presented rare phenom-
ena. It comprises about 1,000 constraints, 370
of which are hard constraints. The entire parser
and the grammar of German are publicly avail-
able at http://nats-www.informatik.
uni-hamburg.de/Papa/PapaDownloads.
The optimal structure could be defined as the tree that
violates the least important constraint (as in Optimality
Theory), or the tree that violates the fewest constraints;
in fact a multiplicative measure is used that combines
both aspects byminimizing the collective dispreference
for all phenomena in a sentence. Unfortunately, the re-
sulting combinatorial problem isNP-complete and ad-
mits of no efficient exact solution algorithm. However,
variants of a heuristic local search can be used, which
try to find the optimal tree by constructing a complete
tree and then changing it in those places that violate im-
portant constraints. This involves a trade-off between
parsing accuracy and processing time, because the cor-
rect structure is more likely to be found if there is more
time to try out more alternatives. Given enough time,
the method works well enough that the overall system
exhibits a competitive accuracy even though the theo-
retical accuracy of the languagemodel may be compro-
mised by search errors.
As an example of the process, consider the following
analysis of the German proverb ?Wer anderen eine
Grube gra?bt, fa?llt selbst hinein.? (He who digs a hole
for others, will fall into it himself.) The transformation
starts with the following initial assumption
AVZ
ADV
SS
OBJA
DET
ETH
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.000001892
which, besides producing two isolated fragments
instead of a spanning tree, also lacks a subject for the
second clause.
AVZ
ADV
SS
OBJA
DET
ETH
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.0001888
To mend this problem the relative pronoun from the
first clause has been taken as a subject for the second
one, with the result that the conflict has simply been
moved to the first part of the sentence. Nevertheless,
the global score improved considerably, since the
verb-second condition for German main clauses is
violated less often.
AVZ
ADV
SS
OBJA
DET
SUBJ
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.0004871
Here, the indefinite plural pronoun ?anderen? is taken
as the subject for the second clause, creating, however,
an agreement error with the finite verb, which is
singular. Both subclauses have still not been integrated
into a single spanning tree.
27
AVZ
ADV
KON
S
APP
DET
OBJA
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.002566
The integration is then achieved, but unfortunately
as a coordination without an appropriate conjunction
being available. Moreover there is a problem with the
hypothesized main clause, since it again does not obey
the verb-second condition of German.
AVZ
ADV
KON
REL
APP
DET
OBJA
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.1026
Therefore the interpretation is changed to a relative
clause, which however cannot appear in isolation.
The valency requirements of the verb ?gra?bt? are
satisfied by taking the indefinite pronoun ?anderen? as
a direct object with the true object (?eine Grube?) as a
(mal-formed) apposition.
AVZ
ADV
S
SUBJC
APP
DET
OBJA
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.5502
Finally, the analysis switches to an interpretation
which accepts the second part of the sentence as the
main clause and subordinates the first part as a subject
clause. The problem with the apposition reading
persists.
AVZ
ADV
S
SUBJC
OBJA
DET
ETH
SUBJ
wer anderen eine Grube gr?bt , f?llt selbst hinein .
global score: 0.7249
By interpreting the indefinite pronoun as an ethical da-
tive, the direct object valence is freed for the NP ?eine
Grube?. Although this structure still violates some con-
straints (e.g. the ethical dative is slightly penalized for
being somewhat unusual) a better one cannot be found.
Note that the algorithm does not take the shortest pos-
sible transformation sequence; in fact, the first analysis
could have been transformed directly into the last by
only one exchange. Because the algorithm is greedy, it
chooses a different repair at that point, but it still finds
the solution in about three seconds on a 3 GHz Pentium
machine.
In contrast to stochastic parsing approaches, a WCDG
can be modified in a specifically targeted manner. It
therefore provides us with a grammar formalism which
is particularly well suited to precisely measure the con-
tributions of different linguistic knowledge sources to
the overall parsing quality. In particular it allows us to
1. switch off constraints, i.e. increase the space of ac-
ceptable constructions and/or syntactic structures,
2. weaken constraints, by changing the weight in a
way that it makes the violation of the constraint
condition more easily acceptable,
3. introduce additional dependency labels into the
model,
4. remove existing dependency labels from the
model
5. reinforce constraints, by removing guards for ex-
ceptional cases from them,
6. reinforce constraints, by strengthening their
weights or making the constraint non-defeasible
in the extreme case, and
7. introducing new constraints, to prohibit certain
constructions and/or syntactic structures.
Since for the purpose of our experiments, we start
with a fairly broad-coverage grammar of German, from
which certain rare phenomenawill be removed, options
4 to 7 are most important for us.
4 Robust behaviour under limited
coverage
In general, it is not easy to predict the possible outcome
of a parsing run when using a grammar with a reduced
coverage. Whether a sentence can be analysed at all
solely depends on the available alternatives for struc-
turing it. Which structural description it can receive,
however, is influenced by the scores resulting from
rule applications or constraint violations. Moreover,
the transformation-based solution method used for the
WCDG-experiments introduces yet another condition:
since it is based on a limited heuristics for candidate
generation, the grammar must license not only the fi-
nal parsing result for a sentence, but also all the inter-
mediate transformation steps with a sufficiently high
score. This might exclude some structural interpreta-
tions from being considered at all if the grammar is not
tolerant enough to accommodate highly deviant struc-
tures.
28
Thus, the ability to deal with extragrammatical input
in a robust manner is a crucial property if we are go-
ing to use a grammar with coverage limitations. Un-
fortunately, robust behaviour is usually achieved by ex-
tending instead of reducing the coverage of the model
and compensating the resulting increase in ambiguity
by an appropriately designed scoring scheme together
with an optimization procedure.
To deal with these opposing tendencies, it is obviously
important to determine which parts of the model need
to be relaxed to achieve a sufficient degree of robust-
ness, and which ones can be reinforced to limit the
space of alternatives in a sensible way. Excluding phe-
nomena from the grammar which never occur in a cor-
pus should always give an advantage, since this reduces
the number of alternatives to consider at each step with-
out forbidding any of the correct ones.
On the other hand, removing support for a construc-
tion that is actually needed forces the parser to choose
an incorrect solution for at least some part of a sen-
tence, so that a deterioration might occur instead. But
even if coverage is reduced below the strictly necessary
amount, a net gain in accuracy could occur for two rea-
sons:
1. Leaking: The grammar overgenerates the con-
struction in question, so that forbidding it prevents
errors occurring on ?normal? sentences.
2. Focussing: Due to a more restricted search space,
the parser is not led astray by rare hypotheses, thus
saving processing time which can be used to come
closer to the optimum.
4.1 Experiment 1: More with less
In our first experiment, we analysed 10,000 sentences
of online newscast texts both with the normal grammar
and with the 21 rare phenomena explicitly excluded. As
usual for dependency parsers, we measure the parsing
quality by computing the structural accuracy (the ratio
of correct subordinations to all subordinations) and la-
belled accuracy (the ratio of all correct subordinations
that also bear the correct label to all subordinations).
Note that the WCDG parser always establishes exactly
one subordination for each word of a sentence, so that
no distinction between precision and recall arises. Also,
the grammar is written in such a way that even if a
necessary phenomenon is removed, the parser will at
least find some analysis, so that the coverage is always
100%.
As expected, those ?rare? sentences in which at least
one of these constructions does actually occur are an-
alyzed less accurately than before: structural and la-
belled accuracy drop by about 2 percent points (see
Table 3). However, the other sentences receive slightly
better analyses, and since they are in the great majority,
the overall effect is an increase in parsing quality. Note
also that the ?rare? sentences appear to be more difficult
to analyze in the first place.
Grammar: Normal Reduced
Online newscasts
rare (717) 87.6%/85.2% 85.8%/85.8%
normal (9,283) 91.0%/89.8% 91.4%/90.4%
overall (10,000) 91.0%/89.4% 91.3%/89.7%
NEGRA corpus
rare (91) 85.5%/83.7% 84.0%/81.4%
normal (909) 91.2%/89.3% 91.5%/89.7%
overall (1,000) 90.5%/88.6% 90.6%/88.7%
Table 3: Structural and labelled accuracy when parsing
the same text with reduced coverage.
The net gain in accuracy might be due to plugged leaks
(misleading structures that used to be found are rejected
in favor of correct structures) or to focussing (structures
that were preferred but missed through search errors are
now found). A point in case of the latter explanation
is the fact that the average runtime decreases by 10%
with the reduced grammar. Also, if we consider only
those sentences on which the local search originally
exceeded the time limit of 500 s and therefore had to
be interrupted, the accuracy rises from 85.2%/83.0% to
86.5%/84.4%, i.e. even more pronounced than overall.
4.2 Experiment 2: Stepwise refinement
For comparison with previous work and to investi-
gate corpus-specific effects, we repeated the experi-
ment with the test set of the NEGRA corpus as defined
by (Dubey and Keller, 2003). For that purpose the NE-
GRA annotations were automatically transformed to
dependency trees with the freely available tool DEPSY
(Daum et al, 2004). Some manual corrections were
made to its output to conform to the annotation guide-
lines of the WCDG of German; altogether, 1% of all
words had their regents changed for this purpose.
Table 3 shows that the proportion of sentences with rare
phenomena is somewhat higher in the NEGRA sen-
tences, and consequently the net gain in parsing accu-
racy is smaller; apparently the advantage of reducing
the problem size is almost cancelled by the disadvan-
tage of losing necessary coverage.
To test this theory, we then reduced the coverage of the
grammar in smaller steps. Since constraints allow us to
switch off each of the 21 rare phenomena individually,
we can test whether the effects of reducing coverage
are merely due to the smaller number of alternatives
to consider or whether some constructions affect the
parser more than others, if allowed.
We first took the first 3,000 sentences of the NEGRA
corpus as a training set and counted how often each
construction actually occurs there and in the test set.
Table 4 shows that the two parts of the corpus, while
different, seem similar enough that statistics obtained
29
Frequency per 1000 on
Nr Phenomenon training set test set
1 Mittelfeld extraposition 33.3 13.0
2 ethical dative 16.7 15.0
3 Nominalization 20.3 17.0
4 Vocative 1.0 2.0
5 Parenthetical matrix clause 13.3 23.0
6 verb-first subclause 8.0 3.0
7 Headline phrase 6.7 9.0
8 coordination cluster 1.7 2.0
9 Adverbial pronoun 4.0 2.0
10 um omission 1.3 1.0
11 Metagrammatical usage 5.7 2.0
12 Auxiliary flip 2.0 0.0
13 Adjectival subclause 0.0 1.0
14 Suffix drop 1.0 1.0
15 Elliptical genitive 0.0 1.0
16 Adverbial noun 0.0 0.0
17 Verb/particle mismatch 0.0 0.0
18 Vorfeld extraposition 0.0 1.0
19 double relative subject 0.0 0.0
20 Relative subject clause 0.3 0.0
21 NP extraposition 0.0 0.0
Table 4: Comparison of training and test set.
on the one could be useful for processing the other.
The test set was then parsed again with the coverage
successively reduced in several steps: first, all construc-
tions were removed that never occur in the training set,
then those which occur less than 10 times or 100 times
respectively were also removed. We also performed
the opposite experiment, first removing support for the
least rare phenomena and only then for the really rare
ones.
Phenomena structural labelled
removed accuracy accuracy
none 90.5% 88.6%
= 0 90.5% 88.7%
< 10 90.6% 88.8%
< 100 90.7% 88.6%
>= 100 90.5% 88.6%
>= 10 90.4% 88.5%
> 0 90.5% 88.6%
all 90.6% 88.7%
Table 5: Parsing with coverage reduced stepwise.
Table 5 shows the results of parsing the test set in this
way (the first and last lines are repetitions from Ta-
ble 3). The resulting effects are very small, but they do
suggest that removing coverage for the very rare con-
structions is somewhat more profitable: the first three
new experiments tend to yield better accuracy than the
original grammar, while in the last three it tends to
drop.
4.3 Experiment 3: Plugging known leaks
The previous experiment used only counts from the
treebank annotations to determine how rare a phe-
nomenon is supposed to be, but it might also be im-
portant how rare the parser actually assumes it to be.
The fact that a particular construction never occurs in a
corpus does not prevent the parser from using it in its
analyses, perhaps more often than another construction
that is much more common in the annotations. In other
words, we should measure how much each construc-
tion actually leaks. To this end, we parsed the training
set with the original grammar and grouped all 21 phe-
nomena into three classes:
A: Phenomena that are predicted much more often
than they are annotated
B: Phenomena that are predicted roughly the right
number of times
C: Phenomena that are predicted less often than an-
notated (or in fact not at all).
?Much more often? here means ?by a factor of two or
more?; constructions which were never predicted or an-
notated at all were grouped into class C.
There are different reasons why a phenomenon might
leak more or less. Some constructions depend on par-
ticular combinations of word forms in the input; for
instance, an auxiliary flip can only be predicted when
the finite verb does in fact precede the full verb (phe-
nomenon 12 in Table 1), so that covering it should not
change the behaviour of the system much. But most
sentences contain more than one noun phrase which the
parser might possibly misrepresent as a non-projective
extraposition (phenomenon 1). Also, some rare phe-
nomena are dispreferred more than others even when
they are allowed. We did not investigate these reasons
in detail.
Phenomena structural labelled
removed accuracy accuracy
none 90.5% 88.6%
A (1,3,4,6?10,13,16,18?21) 90.9% 89.0%
B (2,5,11,12) 90.4% 88.5%
C (14,15,17) 90.4% 88.6%
1?21 90.6% 88.7%
Table 6: Parsing with coverage reduced by increasing
leakage.
Table 6 shows an interesting asymmetry: of our 21 con-
structions, 14 regularly leak into sentences where they
have no place, while 4 work more or less as designed.
Only 3 are predicted too seldom. This is consistent with
our earlier interpretation that most added coverage is in
fact unhelpful when judging a parser solely by its em-
pirical accuracy on a corpus.
30
Accordingly, it is in fact more helpful to judge con-
structions by their observed tendency to leak than just
by their annotated frequency: the first experiment (A)
yields the highest accuracy for the newspaper text.
Conversely, removing those constructions which actu-
ally work largely as intended (B) reduces even the over-
all accuracy, and not just the accuracy on ?rare? sen-
tences. The third class contains only three very rare
phenomena, and removing them from the grammar
does not influence parsing very much at all.
Note that this result was obtained although the distribu-
tion of the phenomena differs between parser predic-
tions on the training set and the test set; had we clas-
sified them according to their behaviour on the test set
itself, the class A would have contained only 9 items (of
which 7 overlap with the classification actually used).
5 Related work
The fact that leaking is an ubiquitous property of natu-
ral language grammars has been noted as early as 80
years ago by (Sapir, 1921). Since no precise defini-
tion was given, the notion offers room for interpreta-
tion. In general linguistics, leaking is usually under-
stood as the underlying reason for the apparent im-
possibility to write a grammar which is complete, in
the sense that it covers all sentences of a language,
while maintaining a precise distinction between correct
an incorrect word form sequences (see e.g. (Sampson,
forthcoming)). In Computational Linguistics, attention
was first drawn to the resulting consequences for ob-
taining parse trees when it became obvious that all at-
tempts to build wide-coverage grammars led to an in-
crease in output ambiguity, and that even more fine-
grained feature-based descriptions were not able solve
the problem. Stochastic approaches are usually consid-
ered to provide a powerful countermeasure (Manning
and Schu?tze, 1999). However, as (Steedman, 2004) al-
ready noted, stochastic models do not address the prob-
lem of overgeneration directly.
Disregarding rare phenomena is something that can be
achieved in a stochastic framework by putting a thresh-
old on the minimum number of occurrences to be con-
sidered. Such an approach is mainly used to either ex-
clude rare phenomena in grammar induction (c.f. (Sol-
sona et al, 2002)) or to prune the search space by ad-
justing a beam width during parsing itself (Goodman,
1997). The direct use of thresholding techniques at the
level of the stochastic model, however, has not been in-
vestigated extensively so far. Stochastic models of syn-
tax suffer to such a degree from data sparseness that in
effect strong efforts in the opposite direction become
necessary: instead of ignoring rare events in the train-
ing data, even unseen events are included by smoothing
techniques. The only experimental investigation of the
impact of rare events we are aware of is (Bod, 2003),
where heuristics are explored to constrain the model
in the DOP framework by ignoring certain tree frag-
ments. Contrary to the results of our experiments, very
few constraints have been found that do not decrease
the parse accuracy. In particular, no improvement by
disregarding selected observations was possible.
The tradeoff between processing time and output qual-
ity which our transformation-based problem solving
strategy exhibits, is also a fundamental property of all
beam-search procedures. While a limited beam width
might cause search errors, widening the beam in or-
der to improve the quality requires investing more com-
putational resources (see e.g. (Collins, 1999)). In con-
trast to our transformation-based procedure, however,
the commonly used Viterbi search is not interruptible
and therefore not in a position to really profit from the
tradeoff. Thus, focussing as a possibility to increase
output quality to our knowledge has never been inves-
tigated elsewhere.
6 Conclusions and future work
We have investigated the effect of systematically reduc-
ing the coverage of a general grammar of German. By
removing support for 21 rare phenomena, the overall
parsing accuracy could be improved.We confirmed the
initial assumption about the effects that broad cover-
age has on the parser: while it allows some special sen-
tences to be analysed more accurately, it also causes
a slight decrease on the much more numerous normal
sentences.
This result shows that at least with respect to this par-
ticular grammar, more coverage can indeed lead to less
parsing accuracy. In the first experiment we measured
the overall loss through adding coverage where it is not
needed as about 0.4% of structural accuracy on news-
cast text, and 0.1% on NEGRA sentences. This fig-
ure can be interpreted as the result of overgenerating
or ?leaking? of rare constructions into sentences where
they are not wanted.
Although we found that it makes little difference
whether to remove support for very rare or for some-
what rare phenomena, judging constructions by how
many leaks they actually cause leads to a greater im-
provement. On the NEGRA test set, removing the
?known troublemakers? leads to a greater increase of in
accuracy of 0.4%, reducing the error rate for structural
attachment by 4.2%.
Of course, removing rare phenomena is not a viable
technique to substantially improve parser accuracy, if
only for the simple fact that it does not scale up. How-
ever, it confirms that as soon as a certain level of cov-
erage has been reached, robustness, i.e. the ability to
deal with unexpected data, is more crucial than cover-
age itself to achieve high quality results on unrestricted
input.
On the other hand, the improvement we obtained is not
31
very large, compared to the already rather high over-
all performance of the parser. This may be due to the
consistent use of weighted constraints in the original
grammar, which slightly disprefer many of the 21 phe-
nomena even when they are allowed, and we assume
that the original grammar is already reasonably effec-
tive at preventing leaks. This claim might be confirmed
by reversing the experiment: if all phenomena were al-
lowed and all dispreferences switched off, we would
expect even more leaks to occur.
To carry out comparable experiments on generative
stochastic models presents us with the difficulty that
it would first be necessary to determine which of its
parameters are responsible for covering a specific phe-
nomenon, and whether they can be modified as to re-
move the construction from the coverage without af-
fecting others as well. Even in WCDG it is difficult
to quantify how much of the observed improvement
results from plugged leaks, and how much from fo-
cussing. This could only be done by observing all in-
termediate steps in the solution algorithm, and counting
how many trees that were used as intermediate results
or considered as alternatives exhibit each phenomenon.
The most promising result from the last experiment is
that it is possible to detect particularly detracting phe-
nomena, which are prime candidates for exclusion, in
one part of a corpus and use them on another. This sug-
gests itself to be exploited as a method to automatically
adapt a broad-coverage grammar more closely to the
characteristics of a particular corpus.
References
Steven Abney. 1996. Statistical Methods and Lin-
guistics. In Judith Klavans and Philip Resnik, editors,
The Balancing Act: Combining Symbolic and Statis-
tical Approaches to Language, pages 1?26. The MIT
Press, Cambridge, Massachusetts.
Rens Bod. 2003. Do all fragments count? Natural
Language Engineering, 9(4):307?323.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proc. 1st Meeting of the North
American Chapter of the ACL, NAACL-2000, Seattle,
WA.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. PhD thesis, Univer-
sity of Pennsylvania, Philadephia, PA.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks to
dependency trees. In Proc. 4th Int. Conf. on Language
Resources and Evaluation, pages 99?106, Lisbon, Por-
tugal.
Christy Doran, Dania Egedi, Beth Ann Hockey,
B. Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem - A Wide Coverage Grammar for English. In
Proc. 15th Int. Conf. on Computational Linguistics,
COLING-1994, pages 922 ? 928, Kyoto, Japan.
Amit Dubey and Frank Keller. 2003. Probabilistic
Parsing for German using Sister-Head Dependencies.
In Proc. 41st Annual Meeting of the Association of
Computational Linguistics, ACL-2003, Sapporo, Japan.
Kilian Foth, Michael Daum, and Wolfgang Menzel.
2005. Parsing unrestricted German text with defeasible
constraints. In H. Christiansen, P. R. Skadhauge, and
J. Villadsen, editors, Constraint Solving and Language
Processing, volume 3438 of Lecture Notes in Artificial
Intelligence, pages 88?101, Berlin. Springer-Verlag.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proc. 2nd Int. Conf. on Em-
prical Methods in NLP, EMNLP-1997, Boston, MA.
C. Grover, J. Carroll, and E. Briscoe. 1993. The Alvey
natural language tools grammar (4th release). Tech-
nical Report 284, Computer Laboratory, University of
Cambridge.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Natural Language Processing. MIT
Press, Cambridge etc.
Geoffrey Sampson. forthcoming. Grammar with-
out grammaticality. Corpus Linguistics and Linguistic
Theory.
Edward Sapir. 1921. Language: An Introduction to the
Study of Speech. Harcourt Brace, New York.
Ingo Schro?der. 2002. Natural Language Parsing with
Graded Constraints. Ph.D. thesis, Department of In-
formatics, Hamburg University, Hamburg, Germany.
Roger Argiles Solsona, Eric Fosler-Lussier, Hong-
Kwang J. Kuo, Alexandros Potamianos, and Imed Zi-
touni. 2002. Adaptive language models for spoken
ddialogue systems. In Proc. Int. Conf. on Acoustics,
Seech, and Signal Processing, ICASSP-2002, Orlando,
FL.
Mark Steedman. 2004. Wide Coverage Pars-
ing with Combinatory Grammars. Slides of a
seminar presentation, Melbourne University, Aus-
tralia. http://www.cs.mu.oz.au/research/
lt/seminars/steedman.pdf. Last time visited:
2006-01-06.
32
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Syllable-Based Speech Recognition for Amharic
Solomon Teferra Abate
solomon_teferra_7@yahoo.com
Wolfgang Menzel
menzel@informatik.uni-hamburg.de
Uniformity of Hamburg, Department of Informatik Natural Language Systems Groups
Vogt-K?lln-Strasse. 30, D-22527 Hamburg, Germany
Abstract
Amharic  is  the  Semitic  language  that  has  the 
second  large  number  of  speakers  after  Arabic 
(Hayward and Richard 1999). Its writing system is 
syllabic  with  Consonant-Vowel  (CV)  syllable 
structure. Amharic orthography has more or less a 
one  to  one correspondence with  syllabic  sounds. 
We have used this feature of Amharic to develop a 
CV syllable-based speech recognizer,  using  Hid-
den  Markov  Modeling  (HMM),  and  achieved 
90.43% word recognition accuracy.
1 Introduction
Most of the Semitic languages are technologically 
unfavored. Amharic is one of these languages that 
are looking for technological considerations of re-
searchers and developers in the area of natural lan-
guage  processing  (NLP).  Automatic  Speech  Re-
cognition (ASR) is one of the major areas of NLP 
that is understudied in Amharic. Only few attempts 
(Solomon,  2001;  Kinfe,  2002;  Zegaye,  2003; 
Martha,  2003;  Hussien  and  Gamb?ck,  2005; 
Solomon et al, 2005; Solomon, 2006)  have been 
made.
We have  developed an  ASR for  the  language 
using  CV  syllables  as  recognition  units.  In  this 
paper  we  present  the  development  and  the 
recognition  performance  of  the  recognizer 
following  a  brief  description  of  the  Amharic 
language and speech recognition technology.
2 The Amharic Language
Amharic,  which belongs to the Semitic  language 
family, is the official language of Ethiopia. In this 
family,  Amharic  stands  second in  its  number  of 
speakers  after  Arabic  (Hayward  and  Richard 
1999). Amharic has five dialectical variations (Ad-
dis  Ababa,  Gojjam,  Gonder,  Wollo,  and  Menz) 
spoken in different regions of the country (Cowley, 
et.al.  1976).  The  speech  of  Addis  Ababa  has 
emerged as the standard dialect and has wide cur-
rency  across  all  Amharic-speaking  communities 
(Hayward and Richard 1999).
As with all of the other languages, Amharic has 
its own characterizing phonetic, phonological and 
morphological properties. For example, it has a set 
of  speech sounds that  is  not  found in  other lan-
guages. For example the following sounds are not 
found in English: [p`], [t?`], [s`], [t`], and [q].
Amharic also has its  own inventory of  speech 
sounds.  It  has  thirty  one  consonants  and  seven 
vowels. The consonants are generally classified as 
stops, fricatives,  nasals,  liquids, and semi-vowels 
(Leslau 2000). Tables 1 and 2 show the classifica-
tion of Amharic consonants and vowels1.
Man 
of 
Art 
Voic
ing 
Place of Articulation
Lab Den Pal Vel Glot
Stops Vs  [p] [t] [t?] [k] [?]
Vd [b] [d] [d?] [g] 
Glott [p`] [t`] [t?`] [q] 
Rd       [kw]
[gw]
[qw] 
Fric Vs [f] [s] [?]   [h]
Vd   [z] [?]   
Glott   [s`]     
Rd         [hw]
Nas-
als 
Vd [m] [n] [?]   
Liq   Vd   [l] 
[r]
Sv Vd [w]     [j] 
Table 1: Amharic Consonants
Key: Lab = Labials; Den = Dentals; Pal = Palat-
als; Vel = Velars; Glot = Glottal; Vs = Voiceless; 
1International  Phonetic  Association's  (IPA)  standard  has 
been used for representation.
33
Vd = Voiced; Rd = Rounded; Fric = Fricatives; Liq 
= Liquids; Sv = Semi-Vowels.
Positions front center back
high            [i]    [u]
mid             [e]    [?] [o]
low [a]
Table 2: Amharic Vowels
Amharic is one of the languages that have their 
own writing system, which is used across all Am-
haric dialects. Getachew (1967) stated that the Am-
haric writing system is phonetic. It allows any one 
to write Amharic texts if s/he can speak Amharic 
and has knowledge of the Amharic alphabet. Un-
like most known languages, no one needs to learn 
how  to  spell  Amharic  words.  In  support  of  the 
above  point,  Leslaw  (1995)  noted  that  no  real 
problems exist in Amharic orthography, as there is 
more or less, a one-to-one correspondence between 
the sounds and the graphic symbols, except for the 
gemination  of  consonants and  some  redundant 
symbols.
Many (Bender 1976; Cowley 1976; Baye 1986) 
have claimed the Amharic orthography as a syllab-
ary for a relatively long period of time. Recently, 
however, Taddesse (1994) and Baye (1997), who 
apparently modified his view, have argued it is not. 
Both of these arguments are based on the special 
feature of the orthography; the possibility of rep-
resenting  speech  using  either  isolated  phoneme 
symbols or concatenated symbols.
In the concatenated feature, commonly known to 
most of the population, each orthographic symbol 
represents a consonant and a vowel, except for the 
sixth order2, which is sometimes realized as a con-
sonant without a vowel and at other times a con-
sonant with a vowel. This representation of concat-
enated speech sounds by a single symbol has been 
the basis for the claim made of the writing system, 
as syllabary. 
Amharic  orthography  does  not  indicate 
gemination,  but  since  there  are  relatively  few 
2An order in Amharic writing system is a combination of a 
consonant with a vowel represented by a symbol. A consonant 
has therefore, 7 orders or different symbols that represent its 
combination with 7 Amharic vowels.
minimal pairs of geminations, Amharic readers do 
not find this to be a problem. This property of the 
writing  system  is  analogous  to  the  vowels  of 
Arabic  and  Hebrew,  which  are  not  normally 
indicated in writing.
The Amharic orthography, as represented in the 
Amharic  Character  set  -  also called [fid?lI]  con-
sists of 276 distinct symbols. In addition, there are 
twenty numerals and eight  punctuation marks.  A 
sample  of  the  orthographic  symbols  is  given  in 
Table 3.
? u i a e o
h ? ? ? ? ? ? ?
l ? ? ? ? ? ? ?
m ? ? ? ? ? ? ?
r ? ? ? ? ? ? ?
Table 3: Some Orthographic Symbols of Amharic 
However, research in speech recognition should 
only consider distinct sounds instead of all the or-
thographic symbols, unless there is a need to de-
velop a dictation machine that includes all of the 
orthographic symbols. Therefore, redundant ortho-
graphic  symbols  that  represent  the  same syllabic 
sounds can be eliminated. Thus, by eliminating re-
dundant graphemes, we are left with a total of 233 
distinct  CV syllable  characters.  In  our  work,  an 
HMM model has been developed for each of these 
CV syllables.
3 HMM-Based Speech Recognition
The  most  well  known  and  well  performing  ap-
proach for speech recognition are Hidden Markov 
Models (HMM). An HMM can be classified on the 
basis  of  the type of its  observation  distributions, 
the structure in its transition matrix and the number 
of states.
The observation distributions of HMMs can be 
either discrete, or continuous. In discrete HMMs, 
distributions are defined on finite spaces while in 
continuous  HMMs,  distributions  are  defined  as 
probability  densities  on  continuous  observation 
spaces,  usually  as  a  mixture  of  several  Gaussian 
distributions.
The model topology that is generally adopted for 
speech recognition is a left-to-right or Bakis model 
34
because the speech signal varies in time from left 
to right (Deller, Proakis and Hansen 1993).
An HMM is flexible in its size, type, or architec-
ture to model words as well as any sub-word unit.
3.1 Sub-word Units of Speech Recognition
Large Vocabulary Automatic Speech Recognition 
Systems (LVASRSs)  require modeling of  speech 
in smaller  units  than words  because the acoustic 
samples of most words will never be seen during 
training,  and  therefore,  can  not  be  trained. 
Moreover,  in  LVASRSs  there  are  thousands  of 
words  and most  of  them occur very rarely,  con-
sequently  training of  models  for  whole  words  is 
generally impractical.  That  is  why LVASRSs re-
quire a segmentation of each word in the vocabu-
lary into sub-word units that occur more frequently 
and can be trained more robustly than words. Us-
ing sub-word based models enables us to deal with 
words  which have not  been seen during training 
since they can just  be decomposed into the sub-
word units. As a word can be decomposed in sub-
word units of different granularities, there is a need 
to choose the most suitable sub-word unit that fits 
the purpose of the system.
Lee et al (1992) pointed out that there are two 
alternatives  for  choosing  the  fundamental  sub-
word units, namely acoustically-based and linguist-
ically-based units . The acoustic units are the labels 
assigned  to  acoustic  segment  models,  which  are 
defined on the basis of procuring a set of segment 
models that spans the acoustic space determined by 
the given, unlabeled training data. The linguistic-
ally-based  units  include  the  linguistic  units,  e.g. 
phones, demi-syllables, syllables and morphemes.
It should be clear that there is no ideal (perfect) 
set  of  sub-word units.  Although phones  are very 
small in number and relatively easy to train, they 
are much more sensitive to contextual influences 
than larger units. The use of triphones, which mod-
el both the right and left context of a phone, has 
become the dominant  solution to  the problem of 
the context sensitivity of phones. 
Triphones  are  also  relatively  inefficient  sub-
word units due to their large number.  Moreover, 
since a triphone unit spans a short time-interval, it 
is  not  suitable for the integration of spectral and 
temporal dependencies.
An other alternative is the syllable. Syllables are 
longer and less context sensitive than phones and 
capable of exploiting both the spectral and tempor-
al  characteristics  of  continuous  speech 
(Ganapathiraju et al 1997). Moreover, the syllable 
has  a  close  connection  to  articulation,  integrates 
some co-articulation phenomena, and has the po-
tential  for  a  relatively  compact  representation  of 
conversational speech.
Therefore, different attempts have been made to 
use syllables as a unit of recognition for the devel-
opment of ASR. To mention a few: Ganapathiraju 
et al (1997) have explored techniques to accentu-
ate the strengths of syllable-based modeling with a 
primary interest of integrating finite-duration mod-
eling and monosyllabic word modeling. Wu et al 
(1998) tried to extract the features of speech over 
the syllabic duration (250ms), considering syllable-
length interval to be 100-250ms. Hu et al (1996) 
used  a  pronunciation  dictionary  of  syllable-like 
units that are created from sequences of phones for 
which the boundary is difficult to detect. Kanok-
phara  (2003)  used  syllable-structure-based  tri-
phones as speech recognition units for Thai.
However, syllables are too many in a number of 
languages, such as English, to be trained properly. 
Thus  ASR researchers  in  languages  like  English 
are led to choose phones where as for Amharic it 
seems promising to consider syllables as an altern-
ative, because Amharic has only 233 distinct CV 
syllables.
4 Syllable-Based Speech  Recognition  for 
Amharic
In  the  development  of  syllable-based  LVASRSs 
for Amharic we need to deal with a  language mod-
el,  pronunciation  dictionary,  initialization  and 
training of the HMM models, and identification of 
the proper HMM topologies that can be properly 
trained  with  the  available  data.  This  section 
presents the development and the performance of 
syllable based speech recognizers.
4.1 The Language Model
One of the required elements in the development of 
LVASRSs is the language model.  As there is no 
usable  language  model  for  Amharic,  we  have 
trained  bigram language  models  using  the  HTK 
statistical  language  model  development  modules. 
Due to  the  inflectional  and derivativational  mor-
phological feature of Amharic our language mod-
els have relatively high perplexities.
35
4.2 The Pronunciation Dictionary
The development of a large vocabulary speaker in-
dependent recognition system requires the availab-
ility of an appropriate pronunciation dictionary. It 
specifies the finite set of words that may be output 
by the speech recognizer and gives, at  least,  one 
pronunciation for each. A pronunciation dictionary 
can be classified as a canonical or alternative on 
the basis of the pronunciations it includes.
A  canonical  pronunciation  dictionary  includes 
only  the  standard  phone  (or  other  sub-word)  se-
quence assumed to be pronounced in read speech. 
It does not consider pronunciation variations such 
as speaker variability, dialect, or co-articulation in 
conversational  speech.  On the other hand,  an al-
ternative pronunciation dictionary uses the actual 
phone (or other sub-word) sequences pronounced 
in speech. In an alternative pronunciation diction-
ary,  various  pronunciation  variations  can  be  in-
cluded (Fukada et al 1999).
We have used the pronunciation dictionary that 
has been developed by Solomon et al (2005). They 
have developed a canonical and an alternative pro-
nunciation dictionaries. Their canonical dictionary 
transcribes  50,000 words  and the  alternative  one 
transcribes 25,000 words in terms of CV syllables. 
Both  these  pronunciation  dictionaries  do  not 
handle the difference between geminated and non-
geminated consonants; the variation of the pronun-
ciation  of  the  sixth  order  grapheme,  with  or 
without vowel; and the absence or presence of the 
glottal  stop  consonant.  Gemination  of  Amharic 
consonants  range  from  a  slight  lengthening  to 
much  more  than  doubling.  In  the  dictionary, 
however, they are represented with the same tran-
scription symbols. 
The sixth order grapheme may be realized with 
or without vowel but the pronunciation dictionaries 
do not  indicate  this  difference.  For  example,  the 
dictionaries used the same symbol for the syllable 
[rI]  in  the  word  [d??m?rInI]  'we  started',  whose 
vowel part may not be realized, and in the word 
[b?rIzo] 'he diluted with water' that is always real-
ized with its vowel sound. That forces a syllable 
model to capture two different sounds: a sound of a 
consonant followed by a vowel, and a sound of the 
consonant only. A similar problem occurs with the 
glottal stop consonant [?] which may be uttered or 
not. 
A sample of pronunciations in the canonical and 
alternative  pronunciation  dictionaries  is  given  in 
Table 43. The alternative pronunciation dictionary 
contains up to 25 pronunciation variants per word 
form. Table  5  illustrates  some cases of  the vari-
ation.
        
Words
 Canonical  Pro-
nunciation
 Alternative Pronun-
ciation
CAmA 
  
 CA mA sp 
 
 CA mA sp
 Ca mA sp
Hitey-
oPeyA 
  
  
  
Hi te yo Pe yA 
sp 
 Hi te yo Pe yA sp
 Hi te yo Pi yA sp
 Hi to Pe yA sp
 te yo Pe yA sp
 to Pe yA sp
Table 4: Canonical and Alternative Pronunciation
Words   Number of pronun-
ciation variants
HiteyoPeyAweyAne       25
HiheHadEge       16
yaHiteyoPeyAne       7
miniseteru       7
yaganezabe       6
HegeziHabehEre       6
yehenene       5
Table 5: Number of Pronunciation variants
Although it does not handle gemination and pro-
nunciation  variabilities,  the  canonical  pronunci-
ation dictionary contains all  233 distinct CV syl-
lables of Amharic, which is 100% syllable cover-
age. 
Pronunciation  dictionaries  of  development  and 
evaluation test sets have been extracted from the 
canonical pronunciation dictionary. These test dic-
tionaries have 5,000 and 20,000 words each.
4.3  The Acoustic Model
For training and evaluation of our recognizers, we 
have used the Amharic read speech corpus that has 
been developed by  Solomon et al (2005). 
The speech corpus consists of a training set, a 
speaker adaptation set, development test sets (for 
5,000 and 20,000 vocabularies), and evaluation test 
sets (for 5,000 and 20,000 vocabularies).   It  is  a 
medium size speech corpus of 20 hours of training 
speech that has been read by 100 training speakers 
who  read  a  total  of  10850  different  sentences. 
Eighty of the training speakers are from the Addis 
3In tables 4 and 5, we used our own transcription
36
Ababa dialect while the other twenty are from the 
other four dialects.
Test and speaker adaptation sets were read by 
twenty other speakers of the Addis Ababa dialect 
and four speakers of the other four dialects. Each 
speaker read 18 different sentences for the 5,000 
vocabulary (development and evaluation sets each) 
and 20 different sentences for the 20,000 vocabu-
lary  (development  and  evaluation  sets  each)  test 
sets. For the adaptation set al of these readers read 
53 adaptation sentences that consist of all Amharic 
CV syllables.
Initialization:  Training  HMM  models  starts 
with initialization. Initialization of the model for a 
set of sub-word HMMs prior to re-estimation can 
be achieved in two different ways: bootstrapping 
and flat start. The latter implies that during the first 
cycle of embedded re-estimation, each training ut-
terance will be uniformly segmented. The hope of 
using such a procedure is that in the second and 
subsequent iterations, the models align as intended.
We have initialized HMMs with both methods 
and trained them in the same way. The HMMs that 
have been initialized with the flat start method per-
formed better (40% word recognition accuracy) on 
development test set of 5,000 words.
The problem with the bootstrapping approach is 
that  any  error  of  the  labeler  strongly  affects  the 
performance of the resulting model because con-
secutive training steps are influenced by the initial 
value of the model. As a result, we did not benefit 
from the use of the segmented speech, which has 
been transcribed with a speech recognizer that has 
low word recognition accuracy, and edited by non-
linguist  listeners.  We  have,  therefore,  continued 
our subsequent experiments with the flat start ini-
tialization method.
Training: We have used the Baum-Welch re-es-
timation procedure for the training. In training sub-
word HMMs that are initialized using the flat-start 
procedure,  this  re-estimation  procedure  uses  the 
parameters of continuously spoken utterances as an 
input source. A transcription, in terms of sub-word 
units, is also needed for each input utterance. Us-
ing the speech parameters and their transcription, 
the complete set of sub-word HMMs are re-estim-
ated  simultaneously.  Then  all  of  the  sub-word 
HMMs  corresponding  to  the  sub-word  list  are 
joined together to make a single composite HMM. 
It is important to emphasize that in this process the 
transcriptions are only needed to identify the se-
quence  of  sub-word  units  in  each  utterance.  No 
boundary  information  is  required  (Young  et  al. 
2002). 
The major problem with HMM training is that it 
requires a great amount of speech data. To over-
come  the  problem  of  training  with  insufficient 
speech data, a variety of sharing mechanisms can 
be  implemented.  For  example,  HMM parameters 
are tied together so that the training data is pooled 
and more robust estimates result. It is also possible 
to restrict the model to a variance vector for the de-
scription of output probabilities,  instead of a full 
covariance matrix. Rabiner and Juang(1993) poin-
ted out that for the continuous HMM models, it is 
preferable to use diagonal covariance matrices with 
several mixtures, rather than fewer mixtures with 
full covariance matrices to perform reliable re-es-
timation of the components of the model from lim-
ited  training  data.  The  diagonal  covariance 
matrices have been used in our work.
HMM Topologies:  To our knowledge, there is 
no topology of HMM model that can be taken as a 
rule of thumb for modeling syllable HMMs, espe-
cially, for Amharic CV syllables. To have a good 
HMM model for Amharic CV syllables, one needs 
to conduct experiments to select the optimal model 
topology. Designing  an HMM topology  has to be 
done with proper consideration of the size of the 
unit of recognition and the amount of the training 
speech data. This is because as the size of the re-
cognition unit increases and the size of the model 
(in terms of the number of parameters to be re-es-
timated) grows, the model requires more training 
data.
We,  therefore,  carried  out  a  series  of  experi-
ments using a left-to-right HMM with and without 
jumps and skips, with a different number of emit-
ting states (3, 5, 6, 7, 8, 9, 10 and 11) and different 
number of Gaussian mixtures (from 2 to 98).  By 
jump we mean skips  from the first  non-emitting 
state  to  the  middle  state  and/or  from the  middle 
state to the last non-emitting state. Figure 1 shows 
a  left-to-right  HMM  of  5  emitting  states  with 
jumps and skips.
Figure 1: An example of HMM topologies
37
We have assumed that the problem of gemina-
tion  may  be  compensated  by  the  looping  state 
transitions of the HMM. Accordingly, CV syllables 
containing  geminated  consonants  should  have  a 
higher  loop  probability  than those  with the  non-
geminated consonants. 
To develop a solution for the problem of the ir-
regularities  in  the  realization  of  the  sixth  order 
vowel [I] and the glottal stop consonant [?], HMM 
topologies with jumps have been used. 
We conducted an experiment using HMMs with 
a jump from the middle state to the last (non-emit-
ting) state for all of the CV syllables with the sixth 
order  vowel,  and  a  jump from the  first  emitting 
state to the middle state for all of the CV syllables 
with the glottal stop consonant.  The CV syllable 
with the  glottal  stop consonant  and the  6th order 
vowel  have  both  jumps.  These  topologies  have 
been chosen so that the models recognize the ab-
sence of the vowel and the glottal stop consonant 
of  CV syllables.  This assumption was confirmed 
by  the  observation  that  the  trained models  favor 
such a jump. A model, which has 5 emitting states, 
of the glottal stop consonant with the sixth order 
vowel tends to start emitting with the 3rd emitting 
state with a probability of 0.72. The model also has 
accumulated  a  considerable  probability  (0.38)  to 
jump from the 3rd emitting state to the last (non-
emitting) state.
A similar model of this consonant with the other 
vowels (our example is the 5th order vowel) tend to 
start  emitting  with  the  3rd emitting  state  with  a 
probability of 0.68. This is two times the probabil-
ity (0.32) of its transition from the starting (non-
emitting state) to the 1st emitting state.
The  models  of  the  other  consonants  with  the 
sixth order vowel,  which are  exemplified by  the 
model of the syllable [jI], tend to jump from the 3rd 
emitting state to the last (non-emitting) state with a 
probability of 0.39, which is considerably greater 
than that of continuing with the next state (0.09).
Since the amount of available training speech is 
not enough to train transition probabilities for skip-
ping two or more states, the number of states to be 
skipped have been limited to one. 
To determine the  optimal  number of  Gaussian 
mixtures for the syllable models, we have conduc-
ted a series of experiments by adding two Gaussian 
mixtures for all the models until the performance 
of the model starts to degrade. Considering the dif-
ference in the frequency of the CV syllables,  a hy-
brid number of Gaussian mixtures has been tried. 
By hybrid, we mean that Gaussian mixtures are as-
signed  to  different  syllables  based  on  their  fre-
quency.  For example:  the frequent  syllables,  like 
[nI], are assigned up to fifty-eight while rare syl-
lables, like [p`i], are assigned not more than two 
Gaussian mixtures.
4.4 Performance of the Recognizers
We present recognition results of only those recog-
nizers which have competitive performance to the 
best performing models. For example: the perform-
ance  of  the  model  with  11  emitting  states  with 
skips and hybrid Gaussian mixtures is more com-
petitive  than those with 7,  8,  9,  and 10 emitting 
states. We have also systematically left out test res-
ults which are worse than those presented in Table 
6. Table 64 shows evaluation results made on the 
5k development test set. 
States Transition 
Topolo-
gies
Mix. Models
AM AM + 
LM
AM + 
LM + 
SA
3 No skip
and jump
18 62.85 88.82
Hy 60.87 87.63 88.50
skip 12 69.20
jump 12 43.74 79.94
5 No skip
and jump
12 69.29 88.99 89.80
Hy 60.04
skip 12 85.77
jump 12 54.53 84.60
11 skip 12 55.04
Hy 71.83 89.21 89.04
Table 6: Recognition Performance on 5k Develop-
ment test set
From Table 6, we can see that the models with 
five  emitting  states,  with  twelve  Gaussian  mix-
tures,  without  skips  and  jumps  has  the  best 
(89.80%)  word  recognition  accuracy.  It  has 
87.69% word recognition accuracy on the 20k de-
velopment test set.
Since  the  most  commonly  used  number  of 
HMM states for phone-based speech recognizers is 
three emitting states, one may expect a model of 
six emitting states to be the best for an HMM of 
4In tables 6 and 7, States refers to the number of emitting 
states;  Mix  refers  to  the  number  of  Gaussian  mixtures  per 
state; Hy refers to hybrid; AM refers to acoustic model; LM 
refers to language model; and SA refers to speaker adaptation.
38
concatenated consonant and vowel. But the result 
of our experiment shows that a CV syllable-based 
recognizer with only five emitting states performed 
better than all the other recognizers.
As we can see from Table 6, models with three 
emitting states do have a competitive performance 
with 18 and hybrid Gaussian mixtures. They have 
the least number of states of all our models. Never-
theless,  they  require  more  storage  space  (33MB 
with 18 Gaussian mixtures and 34MB with hybrid 
Gaussian mixtures) than the best performing mod-
els (32MB). Models with three emitting states also 
have  larger  number  of  total  Gaussian  mixtures5 
(30,401  with  18  Gaussian  mixtures  and  31,384 
with hybrid Gaussian mixtures) than the best per-
forming models (13,626 Gaussian mixtures).
The other model topology that is competitive in 
word  recognition  performance is  the  model  with 
eleven emitting states, with skip and hybrid Gaus-
sian mixtures, which has a word recognition accur-
acy  of  89.21%.  It  requires  the  biggest  memory 
space (40MB) and uses the largest number of total 
Gaussian mixtures (36,619) of all the models we 
have developed.
We have evaluated the top two models with re-
gard  to  their  word  recognition  accuracy  on  the 
evaluation test sets. Their performance is presented 
in Table 7. As it can be seen from the table, the 
models with the better performance on the devel-
opment test sets also showed better results with the 
evaluation test sets. We can, therefore, say that the 
model with five emitting states without skips and 
twelve  Gaussian  mixtures  is  preferable  not  only 
with regard to its word recognition accuracy, but 
also with regard to its memory requirements.
Sta
tes
Mix. Models
AM + LM AM + LM + SA
5k 20k 5k 20k
5 12 90.43 87.26
11 Hy 89.36 87.13   
Table 7:  Recognition Performance on 5k and 20k 
Evaluation test sets
For a comparison purpose, we have developed a 
baseline  word-internal  triphone-based  recognizer 
using the same corpus. The models of 3 emitting 
states, 12 Gaussian mixtures, with skips have the 
5We  counted  the  Gaussian  mixtures  that  are  physically 
saved, instead of what should actually be. 
best word recognition accuracy (91.31%) of all the 
other triphone-based recognizers that we have de-
veloped. This recognizer  also has better word re-
cognition accuracy than that of our syllable-based 
recognizer (90.43%). But tying is applied only for 
the triphone-based recognizers. 
However the triphone-based recognizer requires 
much  more  storage  space  (38MB)  than  the  syl-
lable-based  recognizer  that  requires  only  15MB 
space. With regard to their speed of processing, the 
syllable-based  model  was  37%  faster  than  tri-
phone-based one. 
These  are  encouraging  results  as  compared  to 
the performance  reported by Afify et al (2005) for 
Arabic speech recognition (14.2% word error rate). 
They have used a trigram language model with a 
lexicon of 60k vocabulary.
4.5 Conclusions  and  Research  Areas  in  the 
Future
We conclude  that  the  use  of  CV  syllables  is  a 
promising  alternative  in  the  development  of 
ASRSs for Amharic. Although there are still pos-
sibilities  of  performance  improvement,  we  have 
got  an  encouraging  word  recognition  accuracy 
(90.43%). Some of the possibilities of performance 
improvement are:
? The pronunciation dictionary that we have used 
does not  handle the  problem of gemination of 
consonants  and  the  irregular  realization  of  the 
sixth order vowel and the glottal stop consonant, 
which has a direct effect on the quality of the 
sub-word transcriptions.  Proper editing (use of 
phonetic transcription) of the pronunciation dic-
tionaries  which,  however,  requires  a  consider-
able amount of work, certainly will  result  in a 
higher  quality  of  sub-word  transcription  and 
consequently in the improvement of the recog-
nizers'  performance.  By  switching  from  the 
grapheme-based  recognizer  to  phonetic-based 
recognizer in Arabic, Afif et  al.  (2005) gained 
relative  word  error  rate  reduction  of  10%  to 
14%.
? Since tying is one way of minimizing the prob-
lem of shortage of training speech, tying the syl-
lable-based  models  would  possibly  result  in  a 
gain of  some degree  of  performance improve-
ment. 
39
5 References
Afif, Mohamed, Long Nguyen, Bing Xiang, Sherif Ab-
dou, and John Makhoul. 2005. Recent progress in Ar-
abic broadcast news transcription at BBN. In INTER?
SPEECH?2005, 1637-1640
Baye  Yimam and  TEAM 503  students.  1997.  "??? 
?????" Ethiopian Journal of Languages and Literat-
ure 7(1997): 1-32.
Baye Yimam.  1986.  "????? ????".  Addis  Ababa. 
?.?.?.?.?. 
Bender,  L.M.  and  Ferguson  C.  1976.  The  Ethiopian 
Writing System. In Language in Ethiopia. Edited by 
M.L.  Bender,  J.D.  Bowen,  R.L.  Cooper,  and  C.A. 
Ferguson. London: Oxford University press.
Cowley, Roger, Marvin L. Bender and Charles A. Fer-
gusone. 1976.  The Amharic Language-Description. 
In  Language  in  Ethiopia.  Edited  by  M.L.  Bender, 
J.D. Bowen, R.L. Cooper, and C.A. Ferguson. Lon-
don: Oxford University press.
Deller, J.R. Jr., Hansen, J.H.L. and Proakis, J.G., Dis-
crete-time Processing of Speech Signals. Macmillan 
Publishing Company, New York, 2000.
Fukada, Toshiaki, Takayoshi Yoshimura and Yoshinori 
Sagisa. 1999. Automatic generation of multiple pro-
nunciations based on neural networks. Speech Com-
munication  27:63?73 
http://citeseer.ist.psu.edu/fukada99automatic.html.
Ganapathiraju,  Aravind; Jonathan Hamaker;  Mark Or-
dowski; and George R. Doddington. 1997. Joseph Pi-
cone.  Syllable-based  Large  Vocabulary  Continuous 
Speech Recognition.
Getachew Haile.  1967. The Problems of the Amharic 
Writing System. A paper presented in advance for the 
interdisciplinary seminar of the Faculty of Arts and 
Education. HSIU.
Hayward, Katrina and Richard J. Hayward. 1999. Am-
haric. In Handbook of the International Phonetic As-
sociation:  A  guide  to  the  use  of  the  International 
Phonetic Alphabet. Cambridge: the University Press.
Hu, Zhihong; Johan Schalkwyk; Etienne Barnard; and 
Ronald Cole. 1996. Speech recognition using syllable 
like units. Proc. Int'l Conf. on Spoken Language Pro-
cessing (ICSLP), 2:426-429.
Kanokphara,  Supphanat;  Virongrong  Tesprasit  and 
Rachod Thongprasirt.  2003. Pronunciation Variation 
Speech  Recognition  Without  Dictionary  Modifica-
tion on Sparse Database, IEEE International Confer-
ence  on  Acoustics,  Speech,  and  Signal  Processing 
(ICASSP 2003, Hong Kong).
Kinfe Tadesse.  2002. Sub-word Based Amharic Word 
Recognition: An Experiment  Using Hidden Markov 
Model  (HMM),  M.Sc  Thesis.   Addis  Ababa  Uni-
versity Faculty of Informatics. Addis Ababa. 
Lee, C-H., Gauvain, J-L., Pieraccini, R. and Rabiner, L. 
R.. 1992. Large vocabulary speech recognition using 
subword units. Proc. ICSST-92, Brisbane, Australia, 
pp. 342-353.
Leslau,  W.  2000.  Introductory  Grammar  of  Amharic, 
Wiesbaden: Harrassowitz.
Martha Yifiru. 2003. Application of Amharic speech re-
cognition system  to command and control computer: 
An experiment with Microsoft  Word, M.Sc Thesis. 
Addis Ababa University Faculty of Informatics. Ad-
dis Ababa.
Rabiner, L. and Juang, B. 1993. Fundamentals of speech 
recognition. Englewood Cliffs, NJ.
Hussien Seid and Bj?rn. Gamb?ck  2005. A Speaker In-
dependent  Continuous  Speech  Recognizer  for  Am-
haric. In: INTERSPEECH 2005,  9th European Con-
ference on Speech Communication and Technology. 
Lisbon, September 4-9.
Solomon Birihanu. 2001. Isolated Amharic Consonant-
Vowel (CV) Syllable Recognition, M.Sc Thesis.  Ad-
dis Ababa University Faculty of Informatics. Addis 
Ababa.
Solomon Teferra Abate.  2006.  Automatic  Speech Re-
cognition for  Amharic.  Ph.D. Thesis.  University  of 
Hamburg. Hamburg.
Solomon Teferra  Abate,  Wolfgang Menzel  and  Bairu 
Tafla.  2005. An Amharic Speech Corpus for Large 
Vocabulary Continuous Speech Recognition. In: IN-
TERSPEECH  2005,  9th  European  Conference  on 
Speech  Communication  and  Technology.  Lisbon, 
September 4-9.
Tadesse Beyene. 1994. The Ethiopian Writing System. 
Paper presented at the 12th International Conference 
of Ethiopian Studies, Michigan State University.
Wu, Su-Lin. 1998. Incorporating Information from Syl-
lable-length Time Scales into Automatic Speech Re-
cognition.  PhD  thesis,  University  of  California, 
Berkeley, CA.
Young, Steve; Dan Kershaw; Julian Odell and Dave Ol-
lason. 2002. The HTK Book.
Zegaye Seyifu. 2003. Large vocabulary, speaker inde-
pendent, continuous  Amharic speech recognition, 
M.Sc Thesis.  Addis Ababa University Faculty of In-
formatics. Addis Ababa.
40
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 99?107,
Paris, October 2009. c?2009 Association for Computational Linguistics
Co-Parsing with Competitive Models
Lidia Khmylko
Natural Language Systems Group
University of Hamburg, Germany
khmylko@informatik.uni-hamburg.de
Kilian A. Foth
smartSpeed GmbH & Co. KG
Hamburg, Germany
kilian.foth@smartspeed.com
Wolfgang Menzel
Natural Language Systems Group
University of Hamburg, Germany
menzel@informatik.uni-hamburg.de
Abstract
We present an asymmetric approach to
a run-time combination of two parsers
where one component serves as a predic-
tor to the other one. Predictions are inte-
grated by means of weighted constraints
and therefore are subject to preferential
decisions. Previously, the same architec-
ture has been successfully used with pre-
dictors providing partial or inferior infor-
mation about the parsing problem. It has
now been applied to a situation where the
predictor produces exactly the same type
of information at a fully competitive qual-
ity level. Results show that the combined
system outperforms its individual compo-
nents, even though their performance in
isolation is already fairly high.
1 Introduction
Machine learning techniques for automatically ac-
quiring processing models from a data collec-
tion and traditional methods of eliciting linguistic
knowledge from human experts are usually con-
sidered as two alternative roadmaps towards nat-
ural language processing solutions. Since the re-
sulting components exhibit quite different perfor-
mance characteristics with respect to coverage, ro-
bustness and output quality, they might be able to
provide some kind of complementary information,
which could even lead to a notable degree of syn-
ergy between them when combined within a single
system solution.
For the task of dependency parsing, the high
potential for such a synergy has indeed been
demonstrated already (e.g. Zeman and Z?abokrtsky?
(2005), Foth and Menzel (2006)).
A popular approach for combining alterna-
tive decision procedures is voting (Zeman and
Z?abokrtsky?, 2005). It makes use of a symmet-
ric architecture, where a meta component chooses
from among the available candidate hypotheses by
means of a (weighted) voting scheme. Such an ap-
proach not only requires the target structures of all
components to be of the same kind, but in case of
complex structures like parse trees also requires
sophisticated decision procedures which are able
to select the optimal hypotheses with respect to ad-
ditional global constraints (e.g. the tree property).
Since this optimization problem has to be solved
by the individual parser anyhow, an asymmetric
architecture suggests itself as an alternative.
In asymmetric architectures, a master compo-
nent, i.e. a full fledged parser, is solely in charge of
deciding on the target structure, whilst the others
(so called helper or predictor components) provide
additional evidence which is integrated into the
global decision by suitable means. Such a scheme
has been extensively investigated for the Weighted
Constraint Dependency Grammar, WCDG (Foth,
2006). External evidence from the predictor com-
ponents is integrated by means of constraints,
which check for compatibility between a local
structure and a prediction, and penalize this hy-
pothesis in case of a conflict. So far, however,
all the additional information sources which have
been considered in this research differed consider-
ably from the master component: They either fo-
cused on particular aspects of the parsing problem
(e.g. POS tagging, chunking, PP attachment), or
used a simplified scheme for structural annotation
(e.g. projective instead of non-projective trees).
This paper takes one step further by investigat-
ing the same architecture under the additional con-
dition that (1) the helper component provides the
99
very same kind of target structure as the master,
and (2) the quality levels of each of the compo-
nents in isolation are considered.
As a helper component MSTParser (McDon-
ald, 2006), a state-of-the-art dependency parser for
non-projective structures based on a discrimina-
tive learning paradigm, is considered. The accu-
racy of MSTParser differs insignificatly from that
of WCDG with all the previously used helper com-
ponents active.
Section two introduces WCDG with a special
emphasis on the soft integration of external ev-
idence while section three describes MSTParser
which is used as a new predictor component. Since
parsing results for these systems have been re-
ported in quite different experimental settings we
first evaluate them under comparable conditions
and provide the results of using MSTParser as a
guiding predictor for WCDG in section four and
discuss whether the expected synergies have re-
ally materialized. Section five concentrates on a
comparative error analysis.
2 WCDG
The formalism of a Constraint Dependency Gram-
mar was first introduced by Maruyama (1990)
and suggests modeling natural language with the
help of constraints. Schro?der (2002) has extended
the approach to Weighted Constraint Dependency
Grammar, WCDG, where weights are used to fur-
ther disambiguate between competing structural
alternatives. A WCDG models natural language
as labeled dependency trees and is entirely declar-
ative. It has no derivation rules ? instead, con-
straints license well-formed tree structures. The
reference implementation of WCDG for the Ger-
man language used for the experiments described
below contains about 1, 000 manually compiled
constraints.1
Every constraint of the WCDG carries a weight,
also referred to as a penalty, in the interval from
zero to one, a lower value of the weight re-
flects its greater importance. Constraints having
zero weights are referred to as hard and are used
for prohibitive rules. Constraints with a weight
greater than zero, also called defeasible, may ex-
press universal principles or vague preferences for
language phenomena.
1Freely available from http://nats-www.
informatik.uni-hamburg.de/view/CDG/
DownloadPage
Attempts have been made to compute the
weights of a WCDG automatically by observing
which weight vectors perform best on a given cor-
pus, but the computations did not bring any sig-
nificant improvements to the manually assigned
scored (Schro?der et al, 2001). Empirically, the
absolute values of defeasible constraints usually
do not matter greatly as long as the relative impor-
tance of the rules remains preserved so that typical
constructions are preferred, but seldom variations
are also allowed. Thus, the values of weights of
the WCDG constraints have to be determined by
the grammar writer experimentally.
If a set of dependency edges in a parse found by
the system violates any of the constraints, it is reg-
istered as a constraint violation between the struc-
ture and the rules of the language. The score of an
analysis is the product of all the weights for con-
straint violations occurring in the structure. It be-
comes possible to differentiate between the qual-
ity of different parse results: the analysis with a
higher score is considered preferable. Although,
under these conditions, an analysis having only a
few grave conflicts may be preferred by the system
against another one with a great number of smaller
constraint violations, but it ensures that an analysis
which violates any of the hard constraints always
receives the lowest possible score.
The parsing problem is being treated in the
WCDG system as a Constraint Satisfaction Prob-
lem. While a complete search is intractable for
such a problem, transformation-based solution
methods provide a reliable heuristic alternative.
Starting with an initial guess about the optimal
tree, changes of labels, subordinations, or lexi-
cal variants are applied, with constraint violations
used as a control mechanism guiding the transfor-
mation process (Foth et al, 2000).
A transformation-based search cannot guaran-
tee to find the best solution to the constraint sat-
isfaction problem. Compared to the resource re-
quirements of a complete search, however, it is not
only more efficient, but can also be interrupted at
any time. Even if interrupted, it will always return
an analysis, together with a list of constraint viola-
tions it was not able to remove. The algorithm ter-
minates on its own if no violated constraints with
a weight above a predefined threshold remain. Al-
ternatively, a timeout condition can be imposed.
The same kind of constraints that describe
grammar rules, can also be used as an interface
100
to external predictor components. Thus, the for-
malism turned out to be flexible enough to incor-
porate other sources of knowledge into the de-
cision process on the optimal structural interpre-
tation. Foth and Menzel (2006) have reported
about five additional statistical components that
have been successfully integrated into WCDG:
POS tagger, chunker, supertagger, PP attacher and
a shift-reduce oracle parser. They have also shown
that the accuracy improves if multiple compo-
nents interact and consistent predictions no longer
can be guaranteed. Even thought previously in-
tegrated predictor components have an accuracy
that is mostly ? with the exception of the tag-
ger ? below that of the parser itself, WCDG not
only avoids error propagation successfully, it also
improves its results consistently with each compo-
nent added.
3 MSTParser
MSTParser (McDonald, 2006) is a state-of-the-art
language independent data-driven parser. It pro-
cesses the input in two separate stages. In the first,
the dependency structure is determined, labeling is
applied to it successively in the second. The rea-
sons of its efficiency lie in the successful combi-
nation of discriminative learning with graph-based
solution methods for the parsing problem.
In this edge-factored graph-based model, each
edge of the dependency graph is assigned a real-
valued score by its linear model. The score of the
graph is defined as the sum of its edge scores.
If a scoring function for edges is known, the
parsing problem becomes equivalent to finding the
highest scoring directed spanning tree in the com-
plete graph over the given sentence, and the cor-
rect parse can be obtained by searching the space
of valid dependency graphs for a tree with a max-
imum score.
This formalism allows to find efficient solutions
for both projective and non-projective trees. When
only features over single edges are taken into ac-
count, the complexity falls to O(n2) (McDonald
et al, 2005).
Not only a single edge, but also adjacent edges
may be included into the scoring function. As a
result, intractability problems arise for the non-
projective algorithm, but an efficient approximate
algorithm based on exhaustive search is provided
for this case (McDonald et al, 2006). This algo-
rithm was also used for our experiments.2
The parsing model of MSTParser has the advan-
tage that it can be trained globally and eventually
be applied with an exact inference algorithm. On
the other hand, the parser has only limited access
to the history of parsing decisions. To avoid com-
plexity problems, the scores (and the feature rep-
resentations) are restricted to a single edge or ad-
jacent edges. Outsourcing labeling into a separate
stage comes at the price of not being able to com-
bine knowledge about the label and the structure it
is attached to. Such combined evidence, however,
might be helpful for some disambiguation prob-
lems.
4 Guiding WCDG by Predictions of
MSTParser
MSTParser predictions are integrated into the de-
cision procedure of WCDG by means of two ad-
ditional constraints, which monitor each depen-
dency hypothesis for being in accord with the pre-
diction and penalize it if a mismatch has been
found. One of the constraints checks the attach-
ment point being the same, while the other takes
care of the dependency label.
To properly adjust the weights of these con-
straints, it has to be determined how valuable the
information of the predictor is relative to the infor-
mation already present in the system. This grada-
tion is needed to establish a balance between the
influence of the grammar and the predictor. Ac-
cording to the scoring principles of WCDG, a low
weight strongly deprecates all deviations from the
prediction, thus forcing the system to follow them
almost without exception. Higher weights, on the
other hand, enable the grammar to override a pre-
diction. This, however, also means that predic-
tions have less guiding effect of the transformation
process. Typically for WCDG, the best suitable
weights have to be tuned on development data.
To determine the best constraint weights the
WCDG grammar has been extended with three
additional constraints similar to those used for
the shift-reduce predictor in the previous experi-
ments (Foth, 2006). Two of them advise WCDG
on the structural information available from the
MSTParser result and one fetches the edge label
predicted.
As a result of these experiments, the optimum
2MSTParser is freely available from http://
sourceforge.net/projects/mstparser
101
weight for the attachment predictions has been ad-
justed to 0.75. Compared to a weight of 0.9 for
the shift-reduce parser, this is a rather strong in-
fluence, which also reflects the differences in the
reliability of these two information sources. With
a weight of 0.9, the integration of the label predic-
tions is considerably weaker, which is consistent
with their lower degree of accuracy.
Evaluation
The most common general measures for the qual-
ity of dependency trees are structural accuracy
that points out the percentage of words correctly
attached to their head word, and labeled accuracy
which is the ratio of the correctly attached words
which also have the correct label. Still, it is dif-
ficult to directly compare the results reported for
different parsers, as the evaluation results are in-
fluenced by the data used during the experiment,
the domain of the data, and different annotation
guidelines. Moreover, the particular kind of POS
information might be relevant, which either can be
obtained from the manual annotations or be pro-
vided by a real tagger. Even such a condition
as the treatment of punctuation has not yet be-
come a standard. Following the evaluation proce-
dure in the CoNLL-X shared task (Buchholz and
Marsi, 2006), we will not include punctuation into
the performance measures, as was done in previ-
ous WCDG experiments (Foth and Menzel, 2006).
The source of POS tagging information will need
to be specified in each individual case.
All the evaluations were performed on a thou-
sand sentences (18, 602 ? 19, 601) from the
NEGRA treebank, the same data set that was pre-
viously used in the performance evaluations of
WCDG, e.g. in (Foth, 2006). The NEGRA
treebank is a collection of newspaper articles; in
the original, it stores phrase structure annotations.
These have been automatically translated into de-
pendency trees and then manually corrected to
bring them in accord with the annotation guide-
lines of WCDG. The major difference consists
in a different treatment of non-projectivity, where
WCDG only allows non-projectivity in the attach-
ment of verbal arguments, relative clauses and co-
ordinations, i.e., the cases where it helps to de-
crease ambiguity. Furthermore, corrections were
applied when the annotations of NEGRA itself
turned out to be inconsistent (usually in connec-
tion with co-ordinated or elliptical structures, ad-
verbs and subclauses).
Unfortunately, these manually corrected data
were only available for a small part (3, 000 sen-
tences) of the NEGRA corpus, which is not
sufficient for training MSTParser on WCDG-
conforming tree structures. Previous evaluations
of the MSTParser have used much larger train-
ing sets. E.g., during the CoNLL-X shared
task 39,216 sentences from the TIGER Treebank
(Brants et al, 2002) were used.
Therefore, we used 20, 000 sentences from the
online archive of www.heise.de as an alterna-
tive training set. They have been manually an-
notated according to the WCDG guidelines (and
are referred to heiseticker in the following)3.
The texts in this corpus are all from roughly the
same domain as in NEGRA, and although very
many technical terms and proper nouns are used,
the sentences have only a slightly longer mean
length compared to the NEGRA corpus.
Using POS tags from the gold annotations,
MSTParser achieves 90.5% structural and 87.5%
labeled accuracy on the aforementioned NEGRA
test set (Table 1). Even a model trained on the
inconsistent NEGRA data excluding the test set
reaches state-of-the-art 90.5 and 87.3% for struc-
tural and labeled accuracy respectively, despite
the obvious mismatch between training and test
data. This performance is almost the same as the
90.4%/87.3% reported on the TIGER data during
the CoNLL-X 2006 shared task.
Experiment structural labeled
MSTParser-h 90.5 87.5
MSTParser-N 90.5 87.3
MSTParser(CoNLL-X) 90.4 87.3
WCDG + MST 92.9 91.3
WCDG + MST + 5P 93.3 92.0
Table 1: Structural/labeled accuracy results with
POS tagging from the gold standard. WCDG
? no statistical enhancements used. MSTParser-
h ? MSTParser trained on the heiseticker.
MSTParser-N ? MSTParser trained on NEGRA.
5P ? with all five statistical predictors of WCDG.
As is to be expected, if a real POS tagger is used
in the experiments with MSTParser, the accuracy
is reduced quite expectedly by approximately one
3The heiseticker dependency treebank is under
preparation and will be available soon.
102
percent to 89.5%/86.0% (Table 2 (B)). All the re-
sults obtained with a real POS tagger are summa-
rized in Table 2. For comparison, under the same
evaluation conditions, the performance of WCDG
with different predictors is summarized in Table 2
(A).
Experiment structural labeled
(A) WCDG 88.0 86.0
CP 88.6 86.5
PP 89.4 87.3
ST 90.8 89.2
SR 90.0 88.4
PP+SR 90.2 88.6
ST+SR 91.0 89.4
ST+PP 90.8 89.2
5P 91.3 90.0
(B) MSTParser 89.5 86.0
(C) WCDG + MST 92.0 90.5
PP 92.0 90.6
CP 92.1 90.6
SR 92.2 90.6
ST 92.4 90.9
CP+SR 92.3 90.7
CP+ST 92.6 91.0
ST+SR 92.9 91.4
PP+CP+ST 92.6 91.1
PP+ST+SR 92.8 91.3
CP+ST+SR 92.9 91.4
5P 92.9 91.4
Table 2: Structural/labeled accuracy results with
a real POS tagger. (A) WCDG experiments with
different statistical enhancements (B) MSTParser
experiment with a real POS tagger. (C) Com-
bined experiments of WCDG and MSTParser with
other statistical enhancements of WCDG. CP ?
chunker, ST ? supertagger, PP ? prepositional
attacher, SR ? shift-reduce oracle parser, 5P ?
POS + CP + PP + ST + SR.
The combined experiments in which MSTParser
was used as a predictor for WCDG have achieved
higher accuracy than each of the combined com-
ponents in isolation: the structural accuracy rises
to 92.0% while the labeled accuracy also gets over
the 90%-boundary (WCDG + MST experiment in
Table 2 (C)) .
Finally, the MSTParser predictor was evaluated
in combination with the other predictors avail-
able for WCDG. The results of the experiments
are shown in Table 2 (C). Every combination of
MSTParser with other predictors (first four exper-
iments) improves the accuracy. The increase is
highest (0.4%) for the combination with the su-
pertagger. This confirms earlier experiments with
WCDG, in which the supertagger also contributed
the largest gains.
The experimental results again confirm that
WCDG is a reliable platform for information in-
tegration. Although the use of multiple predictors
does not lead to an accumulation of the individual
improvements, the performance of predictor com-
binations is always higher that using them sepa-
rately. A maximum performance of 92.9%/91.4%
is reached with all the six available predictors ac-
tive. For comparison, the same experiment with
POS tags from the gold standard has achieved even
better results of 93.3%/92.0% (Table 1).
Unfortunately, the PP attacher brings accuracy
reductions when it is working parallel to the shift-
reduce predictor (experiment PP + CP + SR in Ta-
ble 2 (C)). This effect has already been observed
in the experiments that combined the two alone
(experiment PP + SR in Table 2 (A)). When MST
was combined with the PP attacher (experiment
PP in Table 2 (C)), the increase of the performance
was also below a tenth of a percent. The possible
reasons why the use of an additional information
source does not improve the performance in this
case may be the disadvantages of the PP attacher
compared to a full parser.
5 Error Analysis
A very useful property of WCDG is that it not only
can be used as a parser, but also as a diagnostic
tool for dependency structures. Applied to a given
dependency tree, any constraint violation reported
by the constraint solver indicates an inconsistency
between the structure and the WCDG constraint
grammar.
Among the most frequent hard constraint vio-
lations found in the MSTParser results are double
subjects, double objects and direct objects in pas-
sive, projectivity violations, conjunctions without
a clause as well as subordinate clause without con-
junction.
These findings are in line with the analysis of
103
McDonald and Nivre (2007). For example, the
errors in distinguishing noun complements of the
verb may be due to the fact that MSTParser is
more precise for longer dependency arcs and has
no access to the parsing history.
In absolute figures, MSTParser commits 1509
attachment errors of which 902 are corrected by
WCDG. On the other hand, WCDG adds another
542 errors of its own, so that the final result still
contains 1149 errors.
For most labels, accuracy of the predictor com-
bination is higher than in each of the parsers
alone. A particularly large gain has been observed
for coordinated elements (KON and CJ), subor-
dinate (NEB) and relative (REL) clauses, indi-
rect accusative objects (OBJA), genitive modifiers
(GMOD) and apposition (APP). Table 3) summa-
rizes the values of structural precision, the ratio of
the number of correct attachment of a given label
to the number of all the predictions for that label
made by the parser, and label recall, the ratio be-
tween the number of correct labeling decisions and
desired labeling.
In this respect, the increase in the structural pre-
cision of the PP attachment seems worth men-
tioning. MSTParser attaches 79.3% of PPs cor-
rectly on the used test set. Although MSTParser
does not use any special PP-attachment resolu-
tion mechanisms, it is comparable with the re-
sult of WCDG combined with the PP attacher that
achieves 78.7% structural precision for PP edges.
If MSTParser is trained on NEGRA exclud-
ing the test set ? the rest of NEGRA lacking
consistence mentioned above ? it performs even
better, attaching 80.4% of PP-s correctly. Thus,
MSTParser as a statistical parser trained on a full
corpus becomes a strong competitor for a PP at-
tacher that has been trained on restricted four-
tuples input.
As for the errors in the MSTParser output that
are most often corrected in the hybrid experiment,
this happens for both the structural precision and
label recall of most verb complements, such as di-
rect and indirect objects, or clausal objects as well
as for subordinate and relative clauses for such
subordinate clauses.
It even comes to one case in which the synergy
took place in spite of the incorrect predictions. Al-
though MSTParser has predicted possessive modi-
fiers more seldom than WCDG alone (the label re-
call of MSTParser for possessive modification was
(1) (2) (3)
Label p r p r p r
DET 98.4 99.3 98.7 99.5 99.3 99.5
PN 97.4 97.4 98.0 98.0 98.0 98.7
PP 67.6 98.1 78.3 97.4 80.1 98.5
ADV 76.6 94.7 79.4 95.4 82.2 97.2
SUBJ 94.0 90.9 91.3 86.4 95.8 94.0
ATTR 95.2 95.8 97.7 98.2 98.3 98.4
S 89.2 90.1 89.3 90.5 90.5 91.0
AUX 95.9 94.2 98.6 97.8 98.7 97.6
OBJA 87.9 83.9 83.8 72.5 92.5 88.7
APP 85.1 88.5 88.9 90.9 90.9 94.0
KON 78.9 88.1 78.9 88.3 86.0 89.2
CJ 85.6 86.5 90.9 91.4 93.0 93.5
GMOD 90.7 90.7 89.0 85.3 96.3 95.8
KONJ 88.6 91.9 91.9 95.7 95.1 95.7
PRED 90.3 75.0 85.4 60.4 91.7 76.4
NEB 68.9 82.8 73.0 66.4 79.5 90.2
REL 64.8 77.9 59.0 77.0 68.9 86.9
Table 3: Per label structural precision (p,
%) and label recal (r, %) in comparison for
the experiments with the real POS tagger (1)
WCDG, (2) MSTParser, (3) WCDG combined
with MSTParser
over 5% below that of WCDG) its structural pre-
cision and label recall in the combined experiment
are by around 6% greater than WCDG result.
Cases in which WCDG performs worse with
the predictor than its predictor alone can hardly be
found. Still, one may observe many cases in which
the predictor has a negative influence on the per-
formance of WCDG, such as for different kinds of
objects (indirect objects, object clauses and infini-
tive objects) and parenthetic matrix clauses. For
all, the result of MSTParser was below that of
the baseline WCDG with only the POS tagger ac-
tive. Same can be said about the labeled accu-
racy for split verb prefixes and nominal time ex-
pressions. This worsening effect can be attributed
to the lower values of the WCDG constraints for
the corresponding labels and edges than for the
MSTParser predictor. Thus, the search could not
find a decision scoring better than that when the
MSTParser prediction has been followed.
Around 15% of the sentences in the test set are
104
not projective. The accuracy of MSTParser on the
projective sentences of the test set is higher than
that on the non-projective sentences by more than
3 percent (Table 4), although these values can-
not be compared directly as the mean length of
non-projective sentences is longer (25.0 vs. 15.3
words).
Experiment Non-proj. Proj.
MSTParser (POS) 88.2 91.7
WCDG (POS) 87.2 90.2
WCDG (POS + SR) 88.7 92.2
WCDG (POS + MST) 91.3 93.6
Table 4: Structural accuracy, (%), for different
parsing runs for non-projective vs. projective sen-
tences.
MSTParser generally tends to find many more
non-projective edges than the data has, while the
precision remains restricted. The number of non-
projective edges was determined by counting how
often an edge crosses some other edge. Thus, if
a non-projective edge crossed three other edges
the number of non-projective edges equals three.
For MSTParser experiments with a real POS tag-
ger (MSTParser POS-experiment in Table 5), the
non-projective edge recall, the ratio of the non-
projective edges found in the experiment to the
corresponding value in the gold standard, is at
23% and non-projective edge precision, the ratio
of the correctly found non-projective edges to all
non-projective edges found, is also only 36% (sec-
ond column in Table 5).
Edges Sentences
Experiment r p r p
MSTParser (POS) 23 36 35 44
WCDG (POS) 37 53 51 63
WCDG (POS + SR) 41 47 57 55
WCDG (POS + MST) 48 53 61 61
Table 5: Recall (r, %) and precision (p, %) of the
non-projective edges and sentences for different
parsing runs.
Precision and recall of non-projective sentences
is a less rigid measure. If at least one edge-
crossing is correctly identified in a non-projective
sentence, it is added to the correctly identified
non-projective sentences, even if the identified
edge-crossing is not the one annotated in the gold
standard and the ratios are calculated respectively
(right column of Table 5). Under these relaxed
conditions, MSTParser correctly identifies slightly
less than a half of the non-projective sentences
and over a third of non-projective edges. In fact,
WCDG under the same conditions (WCDG POS-
experiment in Table 5) has a non-projective sen-
tence precision of 63% and a non-projective edge
precision of 53%. Still, WCDG misses a consid-
erable amount of non-projectivities. More impor-
tantly, as the present shift-reduce predictor has not
been designed for non-projective parsing, its in-
clusion reduces the non-projective sentence and
edge precision of WCDG ? to 55% and 47% re-
spectively ? WCDG (POS+SR) in Table 5.
The expected benefits for the non-projective
sentences have not yet been observed to the full
extent. The precision of the combined system to
find non-projective sentences and edges remained
limited by the performance that WCDG was able
to achieve alone (WCDG (POS+MST) in Table 5).
While MSTParser in many cases predicts non-
projectivity correctly WCDG is seldom capable of
accepting this external evidence. On the contrary,
WCDG often accepts an incorrect projective solu-
tion of the predictor instead of relying on its own
cues. In its interaction with external predictors
WCDG should typically decide about the alterna-
tives.
6 Related Work
So far, approaches to hybrid parsing have been
mainly based on the idea of a post-hoc selec-
tion which can be carried out for either complete
parses, or individual constituents and dependency
edges, respectively. The selection component it-
self can be based on heuristics, like a majority
vote. Alternatively, a second-level classifier is
trained to decide which component to trust under
which conditions and therefore the approach is of-
ten referred to as classifier stacking.
In a series of experiments, Henderson and Brill
(1999) combined three constituency-based parsers
by a selection mechanism for either complete pars-
ing results (parser switching) or individual con-
stituents (parse hybridization), using both a heuris-
tic decision rule as well as a na??ve Bayesian clas-
sifier in each case. Among the heuristics consid-
ered were majority votes for constituents and a
105
similarity-based measure for complete trees. Tests
on Penn Treebank data showed a clear improve-
ment of the combined results over the best individ-
ual parser. Constituent selection outperformed the
complete parse selection scheme, and Bayesian se-
lection was slightly superior.
Instead of coupling different data-driven parsers
which all provide comparable analyses for com-
plete sentences, Rupp et al (2000) combined dif-
ferently elaborated structural descriptions (namely
chunks and phrase structure trees) obtained by
data-driven components with the output of a
HPSG-parser. Driven by the requirements of the
particular application (speech-to-speech transla-
tion), the focus was not only on parse selection,
but also on combining incomplete results. How-
ever, no quantitative evaluation of the results has
been published.
Zeman and Z?abokrtsky? (2005) applied the se-
lection idea to dependency structures and extended
it by using more context features. They com-
bined seven different parsers for Czech, among
them also a system based on a manually com-
piled rule set. Some of the individual parsers had
a fairly poor performance, but even a simple vot-
ing scheme on single edges contributed a signifi-
cant improvement while the best results have been
obtained for a combination that did not include
the worst components. Alternatively the authors
experimented with a trained selection component
which not only had access to the alternative local
parsing results, but also to their structural context.
Neither a memory-based approach nor a model
based on decision trees did result in further gains.
In two separate experiments, Sagae and Lavie
(2006) combined a number of dependency and
constituent parsers, respectively. They created a
new weighted search space from the results of
the individual component parsers using different
weighting schemes for the candidates. They then
reparsed this search space and found a consistent
improvement for the dependency structures, but
not for the constituent-based ones.
While all these approaches attempt to integrate
the available evidence at parse time, Nivre and
McDonald (2008) pursued an alternative architec-
ture, where integration is achieved already at train-
ing time. They combined the two state-of-the-
art data-driven dependency parsers, MaltParser
(Nivre et al, 2006) and MSTParser (McDonald et
al., 2006), by integrating the features of each of the
classifiers into the parsing model of the other one
at training time. Since the two parsers are based
on quite different model types (namely a history-
based vs. a structure-based one), they exhibit a
remarkable complementary behavior (McDonald
and Nivre, 2007). Accordingly, significant mutual
benefits have been observed. Note, however, that
one of the major benefits of MaltParser, its incre-
mental left-to-right processing, is sacrificed under
such a combination scheme.
Martins et al (2008) use stacked learning to
overcome the restriction to the single-edge fea-
tures in both MaltParser and MSTParser. They
suggest an architecture with two layers, where the
output of a standard parser in the first level pro-
vides new features for a parser in the subsequent
level. During the training phase, the second parser
learns to correct mistakes made by the first one. It
allows to involve higher-order predicted edges to
simulate non-local features in the second parser.
The results are competitive with McDonald and
Nivre (2007) while O(n2) runtime of the spanning
tree algorithm is preserved.
7 Conclusion
Integrating MSTParser as a full predictor with
WCDG is beneficial for both of them. Since these
systems take their decisions based on completely
different sources of knowledge, combining both
helps avoid many mistakes each of them commits
in isolation. Altogether, with a real POS tagger, an
accuracy level of 92.9%/91.3% has been reached
(the last row in Table 2 (C)), which is higher than
what any of the parsers achieved alone. With POS
tagging from the gold standard, the accuracy has
been at 93.3%/92.0% (the last row in Table 1). To
the knowledge of the authors, these accuracy val-
ues are also better than any previous parsing re-
sults on the NEGRA test set.
WCDG can profit from the combination not
only with ancillary predictors for specific parsing
subtasks, but also with another full parser. This
result was achieved even though the second parser
is very similar to WCDG with respect to both the
richness and the accuracy of its target structures.
The probable reason lies in the considerable dif-
ference in the error profiles of both systems as re-
gards specific linguistic phenomena. WCDG was
also used as a diagnostic tool for the errors of
MSTParser.
Possibly, a higher degree of synergy could be
106
achieved if a stronger coupling of the compo-
nents were established by also using the scores of
MSTParser as additional information for WCDG,
reflecting the intuitive notion of preference or
plausibility of the predictions. This could be done
for the optimal parse tree alone as well as for the
complete hypothesis space. Alternatively, the out-
put of MSTParser can be used as a initial state
for the transformation procedure of WCDG. Vice
versa, MSTParser could be enriched with addi-
tional features based on the output of WCDG, sim-
ilar to the feature-based integration of data-driven
parsers evaluated by Nivre and McDonald (2008).
At the moment, the integration constraints treats
all attachment and label predictions as being uni-
formly reliable. To individualize them with re-
spect to their type or origin could not only make
the system sensitive to qualitative differences be-
tween predictions (for instance, with respect to
different labels). It would also allow the parser
to accommodate multiple oracle predictors and to
carefully distinguish between typical configura-
tions in which one prediction should be preferred
over an alternative one. MaltParser (Nivre et al,
2006) is certainly a good candidate for carrying
out such experiments.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In: Proceedings of the First Workshop on
Treebanks and Linguistic Theories (TLT), pages 24?
41.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. CoNLL, pages 149 ? 164.
Kilian A. Foth and Wolfgang Menzel. 2006. Hybrid
parsing: using probabilistic models as predictors for
a symbolic parser. In Proc. 21st Int. Conference on
Computational Linguistics and ACL-44, pages 321?
328.
Kilian A. Foth, Wolfgang Menzel, and Ingo Schro?der.
2000. A Transformation-based Parsing Technique
with Anytime Properties. In 4th Int. Workshop on
Parsing Technologies, IWPT-2000, pages 89 ? 100.
Kilian A. Foth. 2006. Hybrid Methods of Natural Lan-
guage Analysis. Doctoral thesis, Hamburg Univer-
sity.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proceedings 4th Conference on Empiri-
cal Methods in Natural Language Processing, pages
187?194.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking Dependency Parsers.
In Proc. of the 2008 Conf. on Empirical Methods in
Natural Language Processing, pages 157 ? 166.
Hiroshi Maruyama. 1990. Structural disambiguation
with constraint propagation. In Proc. 28th Annual
Meeting of the ACL (ACL-90), pages 31?38.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proc. EMNLP-CoNLL, pages 122 ? 131.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Proc.
HLT/EMNLP, pages 523 ? 530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. CoNLL, pages
216 ? 220.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
PhD dissertation, University of Pennsylvania.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. ACL-08: HLT, pages 950?958.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen
Eryig?it, and Svetoslav Marinov. 2006. Labelled
pseudo-projective dependency parsing with support
vector machines. In Proc. CoNLL-2006, pages 221?
225.
Christopher G. Rupp, Jo?rg Spilker, Martin Klarner, and
Karsten L. Worm. 2000. Combining analyses from
various parsers. In Wolfgang Wahlster, editor, Verb-
mobil: Foundations of Speech-to-Speech Transla-
tion, pages 311?320. Springer-Verlag, Berlin etc.
Kenji Sagae and Alon Lavie. 2006. Parser combi-
nations by reparsing. In Proc. HLT/NAACL, pages
129?132.
Ingo Schro?der. 2002. Natural Language Parsing with
Graded Constraints. Ph.D. thesis, Dept. of Com-
puter Science, University of Hamburg, Germany.
Ingo Schro?der, Horia F. Pop, Wolfgang Menzel, and
Kilian A. Foth. 2001. Learning grammar weights
using genetic algorithms. In Proc. Euroconference
Recent Advances in Natural Language Processing,
pages 235 ? 239.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Improv-
ing parsing accuracy by combining diverse depen-
dency parsers. In Proc. 9th International Workshop
on Parsing Technologies (IWPT-2005), pages 171?
178, Vancouver, B.C.
107
Proceedings of the ACL Student Research Workshop, pages 130?135,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A New Syntactic Metric for Evaluation of Machine Translation 
 
Melania Duma 
Department of Computer 
Science 
University of Hamburg 
Vogt-K?lln-Stra?e 30 
22527 Hamburg 
duma@informatik.uni
-hamburg.de 
Cristina Vertan 
Faculty for Language, 
Literature and Media 
University of Hamburg 
Von Melle Park 6 
20146 Hamburg 
cristina.vertan@uni
-hamburg.de 
Wolfgang Menzel 
Department of Computer 
Science 
University of Hamburg 
Vogt-K?lln-Stra?e 30 
22527 Hamburg 
menzel@informatik.uni
-hamburg.de 
 
  
Abstract 
Machine translation (MT) evaluation aims at 
measuring the quality of a candidate 
translation by comparing it with a reference 
translation. This comparison can be 
performed on multiple levels: lexical, 
syntactic or semantic. In this  paper, we 
propose a new syntactic metric for MT 
evaluation based on the comparison of the 
dependency structures of the reference and 
the candidate translations. The dependency 
structures are obtained by means of a 
Weighted Constraints Dependency Grammar 
parser. Based on  experiments performed on 
English to German translations, we show that 
the new metric correlates well with human 
judgments at the system level. 
1 Introduction 
Research in automatic machine translation (MT) 
evaluation has the goal of developing a set of 
computer-based methods that measure accurately 
the correctness of the output generated by a MT 
system. However, this task is a difficult one 
mainly because there is no unique reference 
output that can be used in the comparison with 
the candidate translation. One sentence can have 
several correct translations. Thus, it is difficult to 
decide if the deviation from an existing reference 
translation is a matter of style (the use of 
synonymous words, different syntax etc.) or a 
real translation error.  
Most of the automatic evaluation metrics 
developed so far are focused on the idea of 
lexical matching between the tokens of one or 
more reference translations and the tokens of a 
candidate translation. However, structural 
similarity between a reference translation and a 
candidate one cannot be captured by lexical 
features. Therefore, research in MT evaluation 
experiences a gradual shift of focus from lexical  
metrics to structural ones, whether they are 
syntactic or semantic or a combination of both.  
This paper introduces a new syntactic 
automatic MT evaluation method. At this stage 
of research the new metric is evaluating 
translations from any source language into 
German. Given that a set of constraint-based 
grammar rules are available for that language, 
extensions to other target languages are anytime 
possible. The chosen tool for providing syntactic 
information for German is the Weighted 
Constraints Dependency Grammar (WCDG) 
parser (Menzel and Schr?der, 1998), which is 
preferred over other parsers because of its 
robustness to ungrammatical input, as it is typical 
for MT output. The rest of this paper is organized 
as follows. In Section 2 the state of the art in MT 
evaluation is presented, while in Section 3 the 
new syntactic metric is described. The 
experimental setup and results are presented in 
Section 4. The last section deals with the 
conclusions and future work. 
2 State of the art 
Automatic evaluation of MT systems relies on 
the existence of at least one reference1 created by 
a human annotator. Using an automatic method 
of evaluation a score is computed, based on the 
similarity between the output of the MT system 
and the reference. This similarity can be 
computed at different levels: lexical, syntactic or 
semantic. At the lexical level, the metrics 
developed so far can be divided into two major 
categories: n-gram based and edit distance based. 
                                                          
1 We will use the term reference for the reference 
translation and the term translation for the candidate 
translation. 
130
Among the n-gram based metrics, one of the 
most popular methods of evaluation is BLEU 
(Papineni et al, 2001). It provides a score that is 
computed as the summed number of n-grams 
shared by the references and the output, divided 
by the total number of n-grams. Lexical metrics 
that use the edit distance are constructed using 
the Levenshtein distance applied at the word 
level. Among these metrics, WER (Niessen et 
al., 2000) is the one which is used more 
frequently; it calculates the minimal number of 
insertion, substitutions and deletions needed to 
transform the candidate translation into a 
reference.  
Metrics based on lexical matching suffer from 
not being able to consider the variation  
encountered in natural language. Thus, they 
reward a low score to an otherwise fluent and 
syntactically correct candidate translation, if it 
does not share a certain number of words with 
the set of references. Because of this, major 
disagreements between the scores assigned by 
BLEU and human judgments have been reported 
in Koehn and Monz (2006) and Callison-Burch 
et al (2006). Another disadvantage is that many 
of them cannot be applied at the segment level, 
which is often needed in order to better assess 
the quality of MT output and to determine which 
improvements should be made to the MT system. 
Because of these disadvantages there is an 
increasing need for other approaches to MT 
evaluation that go beyond the lexical level of the 
phrases compared. 
 In Liu and Gildea (2005),  three syntactic 
evaluation metrics are presented. The first of 
these metrics, the Subtree Metric (SMT), is 
based on determining the number of subtrees that 
can be found in both the candidate translation 
and the reference phrase structure trees. The 
second metric, which is a kernel-based subtree 
metric, is defined as the maximum of the cosine 
measure between the MT output and the set of 
references. The third metric proposed computes 
the number of matching n-grams between the 
headword chains of the reference and the 
candidate translation dependency trees obtained 
using the parser described in (Collins, 1999).  
The idea of syntactic similarity is further 
exploited in Owczarzak et al (2007) which uses 
a Lexical Functional Grammar (LFG) parser. 
The similarity between the translation and the 
reference is computed using the precision and the 
recall of the dependencies that illustrate the pair 
of sentences. Furthermore, paraphrases are used 
in order to improve the correlation with human 
judgments. Another  set of syntactic metrics has 
been  introduced in Gimenez (2008); some of 
them are based on analyzing different types of 
linguistic information (i.e. part-of-speech or 
lemma).  
3 A new syntactic automatic metric 
In this section we introduce the  new syntactic 
metric  which is based on constraint dependency 
parsing. In the first subsection, the WCDG parser 
is presented, together with the advantages of 
using this parser over the other ones available, 
while the second subsection provides a detailed 
description of the new metric. 
3.1 Weighted Constraint Dependency 
Grammar Parser 
Our research was performed using a dependency 
parser. We decided on this type of parser 
because, as opposed to constituent parsers, it 
offers the possibility of better representing non-
projective structures. Moreover, it has been 
shown in Kuebler and Prokic (2006) that, at least 
in the case of German, the results achieved by a 
dependency parser are more accurate than the 
ones obtained when parsing using constituent 
parsers, and this is because dependency parsers 
can handle better long distance relations and 
coordination. 
   The goal of constraint dependency 
grammars (CDG) is to create dependency 
structures that represent a given phrase (Schr?der 
et al, 2000) on parallel levels of analysis. A 
relation between two words in a sentence is 
represented using an edge, which connects the 
regent and the dependent. Edges are annotated 
using labels in order to distinguish between 
different types of relations. A constraint is made 
up of a logical formula that describes properties 
of the tree. One property, for example, that is 
always enforced is that no word can have more 
than one regent on any level at a time. During the 
analysis, each of the constraints is applied to 
every edge or every pair of edges belonging to 
the constructed dependency parse tree. The main 
advantage of using constraint dependency 
grammars over dependency grammars based on 
generative rules is that they can deal better with 
free word order languages (Foth, 2004). 
Weighted Constraint Dependency Grammar 
(WCDG) (Menzel and Schr?der, 1998) assigns 
different weights to the constraints of the 
grammar. Every constraint in WCDG is assigned 
a score which is a number between 0.0 and 1.0, 
131
while the general score of a parse is calculated as 
the product of all the scores of all the instances 
of constraints that have not been satisfied. Rules 
that have a score of 0 are called hard rules, 
meaning that they cannot be ignored, which is 
the case of the one regent only rule mentioned 
earlier. The advantage of using graded 
constraints, as opposed to crisp ones, stems from 
the fact that weights allow the parser to tolerate 
constraint violations, which, in turn, makes the 
parser robust against ungrammaticality. The 
parser was evaluated using different types of 
texts, and the results show that it has an accuracy 
between 80% and 90% in computing correct 
dependency attachments depending on the type 
of text (Foth et al, 2004a). 
The benefit of using WCDG over other parsers 
is that it provides further information on a parse, 
like the general score of the parse and the 
constraints that are violated by the final result. 
This information can be further explored in order 
to perform an error analysis. Moreover, because 
of the fact that the candidate translations are 
sometimes not well-formed, parsing them 
represents a challenge. However, WCDG will 
always provide a final result, in the form of a 
dependency structure, even though it might have 
a low score due to the violated constraints. 
3.2 Description of the metric 
In order to define a  new syntactic metric for MT 
evaluation, we have incorporated the WCDG 
parser in the process of evaluation. Because the 
output of the WCDG parser is a dependency tree, 
we have looked into techniques of measuring 
how similar two trees are. Our aim was to 
determine whether a tree similarity metric 
applied on the two dependency parse trees would 
prove to be an efficient way of capturing the 
similarity between the reference and the 
translation. Let us consider this example, in 
which the reference sentence is ?Die schwarze 
Katze springt schnell auf den roten Stuhl.?(engl. 
The black cat jumps quickly on the red chair) 
and the candidate translation is?Auf den roten 
Stuhl schnell springt die schwarze Katze?(engl. 
On the red chair quickly jumps the red cat). Even 
though the word order of the two segments is 
quite different, and the translation has an 
incorrect syntax, they roughly have the same 
meaning. We present in Figure 1 the dependency 
parse trees obtained using WCDG for the 
sentences considered. We can observe that the 
general structure of the translation is similar to 
that of the reference, the only difference being 
the reverse order between the left subtree and the 
right subtree. The tree similarity measure that we 
chose to use was the All Common Embedded 
Subtrees (ACET) (Lin et al, 2008) similarity. 
Given a tree T, an embedded subtree is obtained 
by removing one or more nodes, except for the 
root, from the tree T. The idea behind ACET is 
that, the more substructures two trees share, the 
more similar they are. Therefore, ACET is 
defined as the number of common embedded 
subtrees shared between two trees. The results 
reported in Lin et al (2008) show that ACET 
outperforms tree edit distance (Zhang and 
Shasha, 1989) in terms of efficiency. 
   
 
Figure 1.  Example of dependency parse trees for 
reference and candidate translations 
 
In our experiments, we have applied the ACET 
algorithm, and computed the number of common 
embedded subtrees between the dependency 
parse trees of the hypothesis and the reference. 
Because of the additional information provided 
by the parsing, pre-processing of the output of 
the WCDG parser was necessary in order to 
transform the dependency tree into a general tree. 
We first removed the labels assigned to every 
edge, but maintained the nodes and the left to 
right order between them. 
In the following, we will refer to the new 
proposed metric using CESM (Common 
Embedded Subtree Metric). CESM was 
computed using the precision, the recall and the 
F-measure of the common embedded subtrees of 
the reference and the translation: 
 
           
                      
                     
 
         
                      
                     
 
132
          
                  
                
 
 
where treeref and treehyp represent the 
preprocessed dependency trees of the reference 
and the hypothesis translations.  
4 Experimental setup and evaluation 
In order to determine how accurate CESM is in 
capturing the similarity between references and 
translations, we evaluated it at the system level 
and at the segment level. The evaluation was 
conducted using data provided by the NAACL 
2012 WMT workshop (Callison-Burch et al, 
2012). The test data for the workshop consisted 
of 99 translated news articles in English, 
German, French, Spanish and Czech.  
At the system level, the initial German test set 
provided at the workshop was filtered according 
to the length of segments. This was done in order 
to limit the time requirements of WCDG. As a 
result, 500 segments with a length between 50 
and 80 characters were extracted from the 
German reference file. In the next step, we 
arbitrarily selected the outputs of 7 of the 15 
systems that were submitted for evaluation in the 
English to German translation task: DFKI  
(Vilar, 2012), JHU (Ganitkevitch et al, 2012), 
KIT (Niehues et al, 2012), UK (Zeman, 2012) 
and three anonymized system outputs referred to 
as OnlineA, OnlineB, OnlineC.  
After this initial step of filtering the data, the 7 
systems were evaluated by calculating the CESM 
score for every pair of reference and translation 
segments corresponding to a system. The 
average scores obtained are depicted in Table 1. 
Evaluation of the metric at the system level was 
performed by measuring the correlation of the 
CESM metric with human judgments using 
Spearman's rank correlation coefficient ?:  
 
    
    
 
       
 
 
where n represents the number of MT systems 
considered during evaluation, and di
2 represents 
the difference between the ranks, assigned to a 
system, by the metric and the human judgments. 
The minimum value of ? is -1, when there is no 
correlation between the two rankings, while the 
maximum value is 1, when the two rankings 
correlate perfectly (Callison-Burch et al, 2012).  
In order to compute the ? score, the scores 
attributed to every system by CESM, were 
converted into ranks. From the different ranking 
strategies that were presented by the WMT12 
workshop, the standard ranking order was 
chosen. The ? rank correlation coefficient was 
calculated as being ? = 0.92, which shows there 
is a strong correlation between the results of 
CESM and the human judgments. In order to 
better assess the quality of CESM, the test set 
was also evaluated using NIST (Doddington, 
2002), which managed to obtain the same rank 
correlation coefficient of ? = 0.92. 
 
No. System 
name 
CESM 
score 
NIST 
score 
1 DFKI  0.069 4.7709 
2 JHU 0.073 4.9904 
3 KIT 0.090 5.1358 
4 OnlineA 0.093 5.3039 
5 OnlineB 0.091 5.3039 
6 OnlineC 0.085 4.8022 
7 UK 0.075 4.6579 
Table 1. System level evaluation results 
 
The first step in evaluating at the segment level 
was filtering the initial test set provided by the 
WMT12 workshop. For this purpose, 2500 
reference and translation segments were selected 
with a length between 50 and 80 characters. The 
Kendall tau rank correlation coefficient was 
calculated in order to measure the correlation 
with human judgments, where Kendall tau 
(Callison-Burch et al, 2012) is defined as: 
 
   
                                     
                  
 
 
In order to compute the value of Kendall tau, we 
determined the number of concordant pairs and 
the number of discordant pairs of judgments. 
Similarly to the guideline followed during the 
WMT12 workshop (Callison-Burch et al, 2012), 
we penalized ties given by CESM and ignored 
ties assigned by the human judgments. The 
obtained result was a correlation of 0.058. As a 
term of comparison, the highest correlation for 
segment level reported in Callinson-Burch et al 
(2012) was 0.19 obtained by TerrorCat (Fishel et 
al., 2012) and the lowest was BlockErrCats 
(Popovic, 2012) with 0.040. However, these 
results were obtained by evaluating on the entire 
test set. The rather low correlation result we 
obtained can be partially explained by the fact 
that only one judgment of a pair of reference and 
translation was taken into account. It will be 
133
interesting to see how the averaging of the ranks 
of a translation influences the correlation 
coefficient.  
5 Conclusions and future work 
In this paper, a new evaluation metric for MT 
was introduced, which is based on the 
comparison of dependency parse trees. The 
dependency trees were obtained using the 
WCDG German parser. The reason why we 
chose this parser was that, due to its architecture, 
it is able to handle  ungrammatical and 
ambiguous input data. The experiments 
conducted so far show that using the data made 
available at the NAACL 2012 WMT workshop, 
CESM correlates well with the human judgments 
at the system level. One of the future 
experiments that we intend to perform is to 
assess metric quality on the entire evaluation set. 
Moreover, we plan to compare CESM with other 
tree-based MT metrics. Furthermore, the 
WMT12 workshop offers different ranking 
possibilities, like the ones presented in Bojar et 
al (2011) and in Lopez (2012). It will be 
determined how much are the segment level 
evaluation results influenced by these ranking 
orders. 
One limitation of the proposed metric is that, 
at the moment it is restricted to translations from 
any source language to German as a target 
language. Because of this reason, we plan to 
extend the metric to other languages and see how 
well it performs in different settings. In further 
experiments we also intend to test CESM using 
statistical based dependency  parsers, like the 
Malt Parser (Nivre et al, 2007) and the MST 
parser (McDonald et al, 2006), in order to 
decide whether the choice of parser influences 
the performance of the metric.  
Another approach that we will explore for 
improving CESM is to compare dependency 
parse trees using the base form and the part-of-
speech of the tokens, instead of the exact lexical 
match. We will try this approach in order to 
avoid penalizing lexical variation. 
The accuracy of CESM can be further 
increased by the use of paraphrases, which can 
be obtained by using a German thesaurus or a 
lexical resource like GermaNet (Hamp and 
Feldweg, 1997). Furthermore, a technique like 
the one described in Owczarzak (2008) can be 
implemented for generating domain specific 
paraphrases. The results reported show that the 
use of this kind of paraphrases in order to 
produce new references has increased the BLEU 
score, therefore this is an approach that will be 
further investigated. 
 
Acknowledgments 
 
This work was funded by the University of 
Hamburg Doctoral Fellowships in accordance 
with the Hamburg Act for the Promotion of 
Young Researchers and Artists (HmbNFG), and 
the EAMT Project ?Using Syntactic and 
Semantic Information in the Evaluation of 
Corpus-based Machine Translation?.   
Reference 
O. Bojar, M. Ercegov?evi?, M Popel and O. Zaidan. 
2011. A Grain of Salt for the WMT Manual 
Evaluation. Proceedings of the Sixth Workshop 
on Statistical Machine Translation. 
C. Callison-Burch, M. Osborne and P. Koehn. 2006. 
Re-evaluating the Role of Bleu in Machine 
Translation Research. Proceedings of EACL-
2006. 
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. 
Soricut and L. Specia. 2012.  Findings of the 
2012 Workshop on Statistical Machine 
Translation. Proceedings of WMT12. 
M. J. Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. 
thesis, University of Pennsylvania. 
G. Doddington. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram 
Co-Occurrence Statistics. Proceedings of the 
2nd International Conference on Human Language 
Technology. 
K. Foth. 2004. Writing weighted constraints for 
large de-pendency grammars. Recent Advances 
in De-pendency Grammar, Workshop COLING 
2004. 
K. Foth, M. Daum and W. Menzel. 2004a. A broad-
coverage parser for German based on 
defeasible constraints. KONVENS 2004, 
Beitr?ge zur 7, Konferenz zur Verarbeitung 
nat?rlicher Sprache, Wien. 
K. Foth, M. Daum and W. Menzel. 2004b. 
Interactive grammar development with 
WCDG. Proc. of the 42nd Annual Meeting of the 
Association for Com-putational Linguistics. 
K. Foth, T. By and W. Menzel. 2006. Guiding a 
con-straint dependency parser with supertags. 
Proceedings of the 21st Int. Conf. on 
Computational Linguistics. 
134
M. Fishel, R. Sennrich, M. Popovic and O. Bojar. 
2012. TerrorCat: a translation error 
categorization-based MT quality metric. 
Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
J. Ganitkevitch, Y. Cao, J. Weese, M. Post and C. 
Callison-Burch. 2012. Joshua 4.0: Packing, 
PRO, and paraphrases. Proceedings of the 
Seventh Workshop on Statistical Machine 
Translation. 
J. Gimenez. 2008. Empirical Machine Translation 
and its Evaluation. Ph. D. thesis. 
B. Hamp and H. Feldweg. 1997. GermaNet - a 
Lexical-Semantic Net for German. Proc. of 
ACL workshop Automatic Information Extraction 
and Building of Lexical Semantic Resources for 
NLP Applications. 
P. Koehn and C. Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation 
between European Languages. NAACL 2006 
Workshop on Statistical Machine Translation. 
P. Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press. 
S. K?bler and J. Prokic. 2006. Why is German 
Dependency Parsing more Reliable than 
Constituent Parsing?. Proceedings of the Fifth 
International Work-shop on Treebanks and 
Linguistic Theories. 
Z. Lin, H. Wang, S. McClean and C. Liu. 2008. All 
Common Embedded Subtrees for Measuring 
Tree Similarity. International Symposium on 
Computational Intelligence and Design. 
D. Liu and D. Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation. ACL 2005 
Workshop on Intrinsic and Extrinsic Evaluation 
Measures for Machine Translation and/or 
Summarization. 
A. Lopez. 2012. Putting human assessments of 
machine translation systems in order. 
Proceedings of the Seventh Workshop on 
Statistical Machine Translation.  
R. McDonald, K. Lerman and F. Pereira. 2006. 
Multilingual Dependency Parsing with a Two-
Stage Discriminative Parser. Tenth Conference 
on Computational Natural Language Learning. 
W. Menzel and I. Schr?der. 1998. Decision 
Procedures for Dependency Parsing Using 
Graded Constraints. Workshop On Processing 
Of Dependency-Based Grammars. 
J. Niehues, Y. Zhang, M. Mediani, T. Herrmann, E. 
Cho and A. Waibel. 2012. The karlsruhe institute 
of technology translation systems for the WMT 
2012. Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
S. Niessen, F. J. Och, G. Leusch  and H. Ney. 2000. 
An Evaluation Tool for Machine Translation: 
Fast Evaluation for MT Research. Proceedings 
of the 2nd International Conference on Language 
Resources and Evaluation (LREC). 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov  and E. Marsi. 2007. 
MaltParser: A language-independent system 
for data-driven dependency parsing. Natural 
Language Engineering. 
K. Owczarzak, J. van Genabith and A. Way. 2007. 
Dependency-based automatic evaluation for 
machine translation. Proceedings of SSST, 
NAACL-HLT 2007 / AMTA Workshop on Syntax 
and Structure in Statistical Translation. 
K. Owczarzak. 2008. A Novel Dependency-Based 
Evaluation Metric for Machine Translation, 
Ph.D. thesis. 
K. Papineni, S. Roukos, T. Ward and W.-J. Zhu. 
2001. Bleu: a method for automatic evaluation 
of machine translation. RC22176 (Technical 
Report), IBM T.J. Watson Research Center. 
M. Popovic. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of 
the Seventh Workshop on Statistical Machine 
Translation. 
I. Schr?der, W. Menzel, K. Foth and M. Schulz. 2000. 
Modeling dependency grammar with 
restricted constraints.  Traitement Automatique 
des Langues. 
I. Schr?der, H. Pop, W. Menzel and K. Foth. 2001. 
Learning grammar weights using genetic 
algorithms. Proceedings Euroconference Recent 
Advances in Natural Language Processing. 
I. Schr?der. 2002. Natural Language Parsing with 
Graded Constraints. Ph.D. thesis, Dept. of 
Computer Science, University of Hamburg. 
D. Vilar. 2012.  DFKI?s SMT system for WMT 
2012. Proceedings of the Seventh Workshop on 
Statistical Machine Translation. 
D. Zeman. 2012. Data issues of the multilingual 
translation matrix. Proceedings of the Seventh 
Workshop on Statistical Machine Translation. 
K. Zhang and D. Shasha. 1989. Simple fast 
algorithms for the editing distance between 
trees and related problems. SIAM Journal on 
Computing. 
 
135
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 803?808,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Incremental Predictive Parsing with TurboParser
Arne K
?
ohn and Wolfgang Menzel
Fachbereich Informatik
Universit?at Hamburg
{koehn, menzel}@informatik.uni-hamburg.de
Abstract
Most approaches to incremental parsing
either incur a degradation of accuracy or
they have to postpone decisions, yield-
ing underspecified intermediate output. We
present an incremental predictive depen-
dency parser that is fast, accurate, and
largely language independent. By extend-
ing a state-of-the-art dependency parser,
connected analyses for sentence prefixes
are obtained, which even predict properties
and the structural embedding of upcoming
words. In contrast to other approaches, ac-
curacy for complete sentence analyses does
not decrease.
1 Introduction
When humans communicate by means of a natu-
ral language, utterances are not produced at once
but evolve over time. Human interaction benefits
from this property by processing yet unfinished
utterances and reacting on them. Computational
parsing on the other hand is mostly performed on
complete sentences, a processing mode which ren-
ders a responsive interaction based on incomplete
utterances impossible.
When spoken language is analyzed, a mismatch
between speech recognition and parsing occurs:
If parsing does not work incrementally, the over-
all system loses all the desirable properties made
possible by incremental processing. For speech di-
alogue systems, this leads to increased reaction
times and an unnatural ping-pong style of interac-
tion (Schlangen and Skantze, 2011).
1.1 Desirable features of incremental parsers
Dependency parsing assigns a head and a depen-
dency label to each word form of an input sentence
and the resulting analysis of the sentence is usually
required to form a tree. An incremental dependency
parser processes a sentence word by word, building
analyses for sentence prefixes (partial dependency
analyses, PDA), which are extended and modified
in a piecemeal fashion as more words become avail-
able.
A PDA should come with three important (but
partly contradictory) properties: beyond being ac-
curate, it should also be as stable and informative as
possible. Stability can be measured as the amount
of structure (attachments and their labels) of a PDA
a
i
which is also part of the analysis a
n
of the whole
sentence. To be maximally informative, at least all
available word forms should be integrated into the
prefix PDA. Even such a simple requirement cannot
easily be met without predicting a structural skele-
ton for the word forms in the upcoming part of the
sentence(bottom-up prediction). Other predictions
merely serve to satisfy completeness conditions
(i.e. valency requirements) in an anticipatory way
(top-down predictions). In fact, humans are able to
derive such predictions and they do so during sen-
tence comprehension (Sturt and Lombardo, 2005).
Without prediction, the sentence prefix ?John
drives a? of ?John drives a car? can only be parsed
as a disconnected structure:
John drives
a
S
B
J
The determiner remains unconnected to the rest of
the sentence, because a possible head is not yet
available. However, the determiner could be inte-
grated into the PDA if the connection is established
by means of a predicted word form, which has not
yet been observed. Beuck et al (2011) propose to
use virtual nodes (VNs) for this purpose. Each VN
represents exactly one upcoming word. Its lexical
instantiation and its exact position remain unspeci-
fied. Using a VN, the prefix ?John drives a? could
then be parsed as follows, creating a fully con-
nected analysis, which also satisfies the valency
requirements of the finite verb.
803
John drives
a
[VirtNoun]
S
B
J
D
E
T
O
B
J
This analysis is clearly more informative but still
restricted to the existence of a noun filling the ob-
ject role of ?drives? without predicting its position.
Although a VN does not specify the lexical identity
of the word form it represents, it can nonetheless
carry some information such as a coarse-grained
part-of-speech category.
1.2 Related work
Parsers that produce incremental output are rel-
atively rare: PLTag (Demberg-Winterfors, 2010)
aims at psycholinguistic plausibility. It makes trade-
offs in the field of accuracy and coverage (they
report 6.2 percent of unparseable sentences on sen-
tences of the Penn Treebank with less than 40
words). Due to its use of beam search, the incre-
mental results are non-monotonic. Hassan et al
(2009) present a CCG-based parser that can parse
in an incremental mode. The parser guarantees
that every parse of an increment extends the previ-
ous parse monotonically. However, using the incre-
mental mode without look-ahead, parsing accuracy
drops from 86.70% to 59.01%. Obviously, insist-
ing on strict monotonicity (a
i
? a
n
) is too strong
a requirement, since it forces the parser to keep
attachments that later turn out to be clearly wrong
in light of new evidence.
Being a transition-based parser, Maltparser
(Nivre et al, 2007) does incremental parsing by
design. It is, however, not able to predict upcom-
ing structure and therefore its incremental output is
usually fragmented into several trees. In addition,
Maltparser needs a sufficiently large look-ahead to
achieve high accuracy (Beuck et al, 2011).
Beuck et al (2011) introduced incremental and
predictive parsing using Weighted Constraint De-
pendency Grammar. While their approach does not
decrease in incremental mode, it is much slower
than most other parsers. Another disadvantage is
its hand-written grammar which prevents the parser
from being adapted to additional languages by sim-
ply training it on an annotated corpus and which
makes it difficult to derive empirically valid con-
clusions from the experimental results.
2 Challenges for predictive parsing
Extending a dependency parser to incremental pars-
ing with VNs introduces a significant shift in the
problem to be solved: While originally the problem
was where to attach each word to (1), in the incre-
mental case the additional problem arises, which
VNs to include into the analysis (2). Problem (2),
however, depends on the syntactic structure of the
sentence prefix. Therefore, it is not possible to de-
termine the VNs before parsing commences, but
the decision has to be made while parsing is going
on. We can resolve this issue by transforming prob-
lem (2) into problem (1) by providing the parser
with an additional node, named unused. It is always
attached to the special node 0 (the root node of ev-
ery analysis) and it can only dominate VNs. unused
and every VN it dominates are not considered part
of the analysis. Using this idea, the problem of
whether a VN should be included into the analysis
is now reduced to the problem of where to attach
that VN:
John drives
a
[VirtNoun] [VirtVerb] [unused]
S
B
J
D
E
T
O
B
J
To enable the parser to include VNs into PDAs,
a set of VNs has to be provided. While this set
could include any number of VNs, we only in-
clude a set that covers most cases of prediction
since rare virtual nodes have a very low a-priori
probability of being included and additional VNs
make the parsing problem more complex. This set
is language-dependent and has to be determined in
advance. It can be obtained by generating PDAs
from a treebank and counting the occurrences of
VNs in them. Eventually, a set of VNs is used that
is a super-set of a large enough percentage (> 90%)
of the observed sets.
3 Gold annotations for sentence prefixes
Annotating sentence prefixes by hand is pro-
hibitively costly because the number of increments
is a multitude of the number of sentences in the
corpus. Beuck and Menzel (2013) propose an ap-
proach to automatically generate predictive depen-
dency analyses from the annotation of full sen-
tences. Their method tries to generate upper bounds
for predictability which are relatively tight. There-
fore, not everything that is deemed predictable by
the algorithm is predictable in reality, but every-
thing that is predictable should be deemed as pre-
dictable: Let W be all tokens of the sentence and P
the set of tokens that lie in the prefix for which an
incremental analysis should be generated. A word
w ?W \P is assumed to be predictable (w ? Pr) if
one of the following three criteria is met:
804
40
60
80
100
0 1 2 3 4 5 final
p
e
r
c
e
n
t
a
g
e
relative time point
German
correct
correct structural prediction
wrong structural prediction
wrong
40
60
80
100
0 1 2 3 4 5 final
p
e
r
c
e
n
t
a
g
e
relative time point
English
Figure 1: Results for TurboParser for German and English with gold standard PoS (labeled)
bottom-up prediction w lies on the path from
some w
?
? P to 0. E. g., given the sentence prefix
?The?, an upcoming noun and a verb is predicted:
The
[VirtNoun] [VirtVerb]
top down prediction pi(w), the head of w, is in
P?Pr, and w fills a syntactic role ? encoded by its
dependency label ? that is structurally determined.
That means w can be predicted independently of
the lexical identity of pi(w). An example for this
is the subject label: If pi(w) is in Pr and w is its
subject, w is assumed to be predictable.
lexical top-down prediction pi(w) ? P and w
fills a syntactic role that is determined by an already
observed lexical item, e.g. the object role: If pi(w)
is a known verb and w is its object, w ? Pr because
it is required by a valency of the verb.
While this procedure is language-independent,
some language-specific transformations must be
applied nonetheless. For English, parts of gapping
coordinations can be predicted whereas others can
not. For German, the transformations described in
(Beuck and Menzel, 2013) have been used with-
out further changes. Both sets of structurally and
lexically determined roles are language dependent.
The label sets for German have been adopted from
(Beuck and Menzel, 2013), while the sets for En-
glish have been obtained by manually analyzing
the PTB (Marcus et al, 1994) for predictability.
For words marked as predictable their existence
and word class, but not their lexicalization and
position can be predicted. Therefore, we replace
the lexical item with ?[virtual]? and generalize the
part-of-speech tag to a more coarse grained one.
4 Predictive parsing with TurboParser
We adapt TurboParser (Martins et al, 2013) for
incremental parsing because it does not impose
structural constraints such as single-headedness in
its core algorithm. For each parsing problem, it
creates an integer linear program ? in the form of a
factor graph ? with the variables representing the
possible edges of the analyses.
Since well-formedness is enforced by factors,
additional constraints on the shape of analyses can
be imposed without changing the core algorithm of
the parser. We define three additional restrictions
with respect to VNs: 1) A VN that is attached to
unused may not have any dependents. 2) A VN
may not be attached to 0 if it has no dependents. 3)
Only VNs may be attached to the unused node.
For a given sentence prefix, let A be the set of
possible edges, V the set of all vertices, N ? V
the VNs and u ?V the unused node. Moreover, let
B? A be the set of edges building a well-formed
analysis and z
a
, I(a ? B), where I(.) is the indica-
tor function. The three additional conditions can be
expressed as linear constraints which ensure that
every output is a valid PDA:
z
?n, j?
+ z
?u,n?
? 1, n ? N, j ?V (1)
z
?0,n?
?
?
j?V
z
?n, j?
, n ? N (2)
z
?u,i?
= 0, i ?V \N (3)
The current implementation is pseudo-incremen-
tal. It reinitializes the ILP for every increment with-
out passing intermediate results from one incremen-
tal processing step to the next, although this might
be an option for further optimization.
High quality incremental parsing results can not
be expected from models which have only been
trained on whole-sentence annotations. If a parser
is trained on gold-standard PDAs (generated as de-
scribed in section 3), it would include every VN
into every analysis because that data does not in-
clude any non-attached VNs. We therefore add non-
attached VNs to the generated PDAs until they
contain at least the set of VNs that is later used
during parsing. For instance, each German training
increment contains at least one virtual verb and
805
two virtual nouns and each English one at least one
virtual verb and one virtual noun. This way, the per-
centage of VNs of a specific type being attached in
the training data resembles the a priori probability
that a VN of that type should be included by the
parser while parsing.
TurboParser is trained on these extended PDAs
and no adaptation of the training algorithm is
needed. The training data is heavily skewed be-
cause words at the beginning of the sentences are
more prevalent than the ones at the end. As a com-
parison with a version trained on non-incremental
data shows, this has no noticeable effect on the
parsing quality.
5 Evaluation
The usual methods to determine the quality of a
dependency parser ? labeled and unlabeled attach-
ment scores (AS) ? are not sufficient for the evalu-
ation of incremental parsers. If the AS is computed
for whole sentences, all incremental output is dis-
carded and not considered at all. If every intermedi-
ate PDA is used, words at the start of a sentence are
counted more often than the ones at the end. No in-
formation becomes available on how the accuracy
of attachments evolves while parsing proceeds, and
the prediction quality (i.e. the VNs) is completely
ignored. Therefore, we adopt the enhanced mode
of evaluation proposed by Beuck et al (2013): In
addition to the accuracy for whole sentences, the
accuracies of the n newest words of each analy-
sis are computed. This yields a curve that shows
how good a word can be assumed to be attached
depending on its distance to the most recent word.
Let ?V,G? be the gold standard analysis of an
increment and ?V
?
,P? the corresponding parser out-
put. V and V
?
are the vertices and G and P the
respective edges of the analyses. Let V
?
p
and V
?
v
be
the in-prefix and virtual subset of V
?
, respectively.
To evaluate the prediction capabilities of a parser,
for each increment an optimal partial, surjective
mapping
1
V
?
?V from the output produced by the
parser to the (automatically generated) gold stan-
dard is computed, where each non-virtual element
of V
?
has to be mapped to the corresponding ele-
ment in V . Let M be the set of all such mappings.
Then the best mapping is defined as follows:
? = argmax
m?M
?
w?V
?
I(pi(m(w)) = m(pi(w)))
1
The mapping is partial because for some VNs in V
?
there
might be no corresponding VN in the gold standard.
We define a word w as correctly attached (ignor-
ing the label) if pi(?(w)) = ?(pi(w)). In an incre-
mental analysis, an attachment of a word w can be
classified into four cases:
correct pi(?(w)) = ?(pi(w)), pi(w) ?V
?
p
corr. pred. pi(?(w)) = ?(pi(w)), pi(w) ?V
?
v
wrong pred. pi(?(w)) 6= ?(pi(w)), pi(w) ?V
?
v
wrong pi(?(w)) 6= ?(pi(w)), pi(w) ?V
?
p
We can count the number of VNs that have been
correctly attached: Let T be the set of all analyses
produced by the parser and ?
t
the best mapping as
defined above for each t ? T . Furthermore, let vn(t)
be the set of VNs in t. The total number of correct
predictions of VNs is then defined as:
corr =
?
t?T
?
v?vn(t)
I(pi(?
t
(v)) = ?
t
(pi(v)))
Precision and recall for the prediction with VNs
can be computed by dividing corr by the number
of predicted VNs and the number of VNs in the
gold standard, respectively.
Evaluation has been carried out on the PTB con-
verted to dependency structure using the LTH con-
verter (Johansson and Nugues, 2007) and on the
Hamburg Dependency Treebank (Foth et al, 2014).
From both corpora predictive PDAs padded with
unused virtual nodes have been created for training.
For English, the sentences of part 1-9 of the PTB
were used, for German the first 50,000 sentences
of the HDT have been selected. Testing was done
using one virtual noun and one virtual verb for En-
glish and two virtual nouns and one virtual verb for
German because these sets cover about 90% of the
prefixes in both training sets.
Figure 1 shows the evaluation results for pars-
ing German and English using TurboParser. For
both languages the attachment accuracy rises with
the amount of context available. The difference be-
tween the attachment accuracy of the most recent
word (relative time point 0, no word to the right
of it) and the second newest word (time point 1)
is strongest, especially for English. The word five
elements left of the newest word (time point 5) gets
attached with an accuracy that is nearly as high as
the accuracy for the whole sentence (final).
The types of errors made for German and En-
glish are similar. For both German and English the
unlabeled precision reaches more than 70% (see
Table 1). Even the correct dependency label of up-
coming words can be predicted with a fairly high
precision. TurboParser parses an increment in about
0.015 seconds, which is much faster than WCDG
806
40
60
80
100
0 1 2 3 4 5 final
p
e
r
c
e
n
t
a
g
e
relative time point
TurboParser
40
60
80
100
0 1 2 3 4 5 final
p
e
r
c
e
n
t
a
g
e
relative time point
jwcdg
Figure 2: Results for TurboParser and jwcdg for German with tagger (labeled).
English German German&tagger German (jwcdg)
labeled unlabeled labeled unlabeled labeled unlabeled labeled unlabeled
precision 75.47% 78.55% 67.42% 75.90% 65.21% 73.39% 32.95% 42.23%
recall 57.92% 60,29% 46.77% 52.65% 45.79% 51.54% 35.90% 46.00%
Table 1: Precision and recall for the prediction of virtual nodes
time point 0 time point 5
unlabeled labeled unlabeled labeled
En 89.28% 84.92% 97.32% 97.11%
De 90.91% 88.96% 96.11% 95.65%
Table 2: Stability measures
where about eight seconds per word are needed to
achieve a good accuracy (K?ohn and Menzel, 2013).
The prediction recall is higher for English than for
German which could be due to the differences in
gold-standard annotation.
Training TurboParser on the non-incremental
data sets results in a labeled whole-sentence accu-
racy of 93.02% for German.The whole-sentence
accuracy for parsing with VNs is 93.33%. This
shows that the additional mechanism of VNs has
no negative effects on the overall parsing quality.
To compare TurboParser and WCDG running
both in the predictive incremental mode, we use
jwcdg, the current implementation of this approach.
jwcdg differs from most other parsers in that it
does not act on pre-tagged data but runs an exter-
nal tagger itself in a multi-tag mode. To compare
both systems, TurboParser needs to be run in a
tagger-parser pipeline. We have chosen TurboTag-
ger without look-ahead for this purpose. Running
TurboParser in this pipeline leads to only slightly
worse results compared to the use of gold-standard
tags (see Figure 2). TurboParser?s attachment ac-
curacy is about ten percentage points better than
jwcdg?s across the board. In addition, its VN pre-
diction is considerably better.
To measure the stability, let P
i
be a prefix of the
sentence P
n
and a
i
and a
n
be the corresponding
analyses produced by the parser. An attachment
of a word w ? P
i
is stable if either w?s head is the
same in a
i
and a
n
or w?s head is not part of P
i
in
both a
i
and a
n
. The second part covers the case
where the parser predicts the head of w to lie in the
future and it really does, according to the final parse.
Table 2 shows the attachment stability of the newest
word at time point 0 compared to the word five
positions to the left of time point 0. TurboParser?s
stability turns out to be much higher than jwcdg?s:
For German Beuck et al (2013) report a stability
of only 80% at the most recent word. Interestingly,
labeling the newest attachment for English seems
to be much harder than for German.
6 Conclusion
Using a parser based on ILP, we were able to an-
alyze sentences incrementally and produce con-
nected dependency analyses at every point in time.
The intermediate structures produced by the parser
are highly informative, including predictions for
properties and structural embeddings of upcom-
ing words. In contrast to previous approaches, we
achieve state-of-the-art accuracy for whole sen-
tences by abandoning strong monotonicity and aim
at high stability instead, allowing the parser to im-
prove intermediate results in light of new evidence.
The parser is trained on treebank data for whole
sentences from which prefix annotations are de-
rived in a fully automatic manner. To guide this
process, a specification of structurally and lexically
determined dependency relations and some addi-
tional heuristics are needed. For parsing, only a set
of possible VNs has to be provided. These are the
only language specific components required. There-
fore, the approach can be ported to other languages
with quite modest effort.
807
References
Niels Beuck and Wolfgang Menzel. 2013. Structural
prediction in incremental dependency parsing. In
Alexander Gelbukh, editor, Computational Linguis-
tics and Intelligent Text Processing, volume 7816 of
Lecture Notes in Computer Science, pages 245?257.
Springer Berlin Heidelberg.
Niels Beuck, Arne K?ohn, and Wolfgang Menzel. 2011.
Incremental parsing and the evaluation of partial de-
pendency analyses. In Proceedings of the 1st In-
ternational Conference on Dependency Linguistics.
Depling 2011.
Niels Beuck, Arne K?ohn, and Wolfgang Menzel. 2013.
Predictive incremental parsing and its evaluation. In
Kim Gerdes, Eva Haji?cov?a, and Leo Wanner, editors,
Computational Dependency Theory, volume 258 of
Frontiers in Artificial Intelligence and Applications,
pages 186 ? 206. IOS press.
Vera Demberg-Winterfors. 2010. A Broad-Coverage
Model of Prediction in Human Sentence Processing.
Ph.D. thesis, University of Edinburgh.
Kilian A. Foth, Niels Beuck, Arne K?ohn, and Wolfgang
Menzel. 2014. The Hamburg Dependency Tree-
bank. In Proceedings of the Language Resources
and Evaluation Conference 2014. LREC, European
Language Resources Association (ELRA).
Hany Hassan, Khalil Sima?an, and Andy Way. 2009.
Lexicalized semi-incremental dependency parsing.
In Proceedings of the International Conference
RANLP-2009, pages 128?134, Borovets, Bulgaria,
September. Association for Computational Linguis-
tics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for En-
glish. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Arne K?ohn and Wolfgang Menzel. 2013. Incremental
and predictive dependency parsing under real-time
conditions. In Proceedings of the International Con-
ference Recent Advances in Natural Language Pro-
cessing RANLP 2013, pages 373?381, Hissar, Bul-
garia, September. INCOMA Ltd. Shoumen, BUL-
GARIA.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate
argument structure. In Proceedings of the Workshop
on Human Language Technology, HLT ?94, pages
114?119, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617?622, Sofia, Bulgaria, August.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Davin Schlangen and Gabriel Skantze. 2011. A gen-
eral, abstract model of incremental dialogue process-
ing. Dialogue and Discourse, 2(1):83?111.
Patrick Sturt and Vincenzo Lombardo. 2005. Process-
ing coordinated structures: Incrementality and con-
nectedness. Cognitive Science, 29(2):291?305.
808
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12?16,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Large-scale CCG Induction from the Groningen Meaning Bank
Sebastian Beschke
?
, Yang Liu
?
and Wolfgang Menzel
?
?
Department of Informatics, University of Hamburg, Germany
{beschke,menzel}@informatik.uni-hamburg.de
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
Abstract
In present CCG-based semantic parsing
systems, the extraction of a semantic
grammar from sentence-meaning exam-
ples poses a computational challenge. An
important factor is the decomposition of
the sentence meaning into smaller parts,
each corresponding to the meaning of a
word or phrase. This has so far limited
supervised semantic parsing to small, spe-
cialised corpora. We propose a set of
heuristics that render the splitting of mean-
ing representations feasible on a large-
scale corpus, and present a method for
grammar induction capable of extracting a
semantic CCG from the Groningen Mean-
ing Bank.
1 Introduction
Combinatory Categorial Grammar (CCG) forms
the basis of many current approaches to semantic
parsing. It is attractive for semantic parsing due
to its unified treatment of syntax and semantics,
where the construction of the meaning representa-
tion directly follows the syntactic analysis (Steed-
man, 2001). However, the supervised induction of
semantic CCGs?the inference of a CCG from a
corpus of sentence-meaning pairs?has so far only
been partially solved. While approaches are avail-
able that work on small corpora focused on spe-
cific domains (such as Geoquery and Freebase QA
for question answering (Zelle and Mooney, 1996;
Cai and Yates, 2013)), we are not aware of any
approach that allows the extraction of a seman-
tic CCG from a wide-coverage corpus such as the
Groningen Meaning Bank (GMB) (Basile et al.,
2012). This work attempts to fill this gap.
Analogous to the work of Kwiatkowski et al.
(2010), we view grammar induction as a series of
splitting steps, each of which essentially reverses
a CCG derivation step. However, we diverge
from their approach by applying novel heuristics
for searching the space of possible splits. The
combination of alignment consistency and single-
branching recursion turns out to produce a man-
ageable number of lexical items for most sen-
tences in the GMB, while statistical measures
and manual inspection suggest that many of these
items are also plausible.
2 Searching the space of CCG
derivations
Our search heuristics are embedded into a very
general splitting algorithm, Algorithm 1. Given
a sentence-meaning pair, it iterates over all possi-
ble sentence-meaning splits in two steps. First, a
split index in the sentence is chosen along with a
binary CCG-combinator to be reversed (the syn-
tactic split). Then, the meaning representation is
split accordingly to reverse the application of the
selected combinator (the semantic split). E. g., for
the forward application combinator, the meaning
representation z is split into f, g so that z = fg
(modulo ?, ?, ? conversions). By identifying f
with the left half l of the sentence and g with the
right half r, we obtain two new phrase-meaning
pairs, which are then split recursively.
This algorithm combines two challenging
search problems. Recursive syntactic splitting
searches the space of syntactic CCG derivations
that yield the sentence, which is exponential in the
length of the sentence. Semantic splitting, given
the flexibility of ?-calculus, has infinitely many
solutions. The crucial question is how to prune
the parts of the search space that are unlikely to
lead to good results.
Our strategy to address this problem is to apply
heuristics that constrain the results returned by se-
mantic splitting. By yielding no results on certain
inputs, this at the same time constrains the syntac-
tic search space. The following descriptions there-
12
fore relate to the implementation of the SEMSPLIT
function.
Algorithm 1 A general splitting algorithm. C is
the set of binary CCG combinators. The SEM-
SPLIT function returns possible splits of a meaning
representation according to the reverse application
of a combinator.
function SPLIT(x, z)
if |x| = 1 then
return {(x, z)}
else
G? ?
for 0 < i ? |x| ? 1 and c ? C do
l? x
0
. . . x
i?1
r ? x
i
. . . x
|x|?1
S ? SEMSPLIT(c, z)
for (f, g) ? S do
G? G ? SPLIT(l, f)
? SPLIT(r, g)
end for
end for
return G
end if
end function
2.1 Alignment consistency
The first heuristic we introduce is borrowed from
the field of statistical machine translation. There,
alignments between words of two languages are
used to identify corresponding phrase pairs, as
in the well-known GHKM algorithm (Galley et
al., 2004). In order to apply the same strategy
to meaning representations, we represent them as
their abstract syntax trees. Following Li et al.
(2013), we can then align words in the sentence
and nodes in the meaning representation to iden-
tify components that correspond to each other.
This allows us to impose an extra constraint on
the generation of splits: We require that nodes in f
not be aligned to any words in the right sentence-
half r, and conversely, that nodes in g not be
aligned to words in l.
Alignment consistency helps the search to fo-
cus on more plausible splits by grouping elements
of the meaning representation with the words that
evoked them. However, by itself it does not signif-
icantly limit the search space, as it is still possible
to extract infinitely many semantic splits from any
sentence at any splitting index.
Example: Given the word-to-meaning
?x
?y
?
mia(y)love(x, y)vincent(x)
lovesVincent Mia
Figure 1: An example word-to-meaning align-
ment. Splits across any of the alignment edges are
prohibited. E. g., we cannot produce a split whose
meaning representation contains both vincent and
mia.
alignment from Figure 1, a split that is
excluded by the alignment criterion is:
(Vincent : ?g.?x.?y.vincent(x) ? love(x, y) ?
g(y)), (loves Mia : ?y.mia(y)). This is because
the node ?love? (in f ) is aligned to the word
?loves? (in r).
2.2 Single-branching recursive splitting
The second heuristic is best described as a search
strategy over possible semantic splits. In the fol-
lowing presentation, we presume that alignment
consistency is being enforced. Again, it is helpful
to view the meaning representation as an abstract
syntax tree.
Recall that our goal is to find two expressions
f, g to be associated with the sentence halves l, r.
In a special case, this problem is easily solved: If
we can find some split node X which governs all
nodes aligned to words in r, but no nodes aligned
to words in l, we can simply extract the sub-tree
rooted at X and replace it with a variable. E. g.,
z = a(bc) can be split into f = ?x.a(xc) and
g = b, which can be recombined by application.
However, requiring the existence of exactly two
such contiguous components can be overly restric-
tive, as Figure 2 illustrates. Instead, we say that we
decompose z into a hierarchy of components, with
a split node at the root of each component. These
components are labelled as f - and g-components
in an alternating fashion.
In this hierarchy, the members of an f -
component are not allowed to have alignments to
words in l. A corresponding requirement holds for
13
@@
ed
@
@
cb
a
x
0
x
1
x
2
x
3
x
4
Figure 2: Illustration of single-branching recur-
sion: Assume that the leaves of the meaning rep-
resentation a(bc)(de) are aligned as given to the
words x
0
. . . x
4
, and that we wish to split the sen-
tence at index 2. The indicated split partitions the
meaning representation into three hierarchically
nested components and yields f = ?x.xc(de) and
g = ?y.a(by), which can be recombined using ap-
plication.
g-components.
The single-branching criterion states that all
split nodes lie on a common path from the root,
or in other words, every component is the parent
of at most one sub-component.
In comparison to more flexible strategies,
single-branching recursive splitting has the advan-
tage of requiring a minimum of additionally gen-
erated structure. For every component, we only
need to introduce one new bound variable for the
body plus one for every variable that occurs free
under the split node.
Together with the alignment consistency cri-
terion, single-branching recursive splitting limits
the search space sufficiently to make a full search
tractable in many cases.
2.3 Other heuristics
The following heuristics seem promising but are
left to be explored in future work.
Min-cut splitting In this strategy, we place no
restriction on which split nodes are chosen. In-
stead, we require that the overall count of split
nodes is minimal, which is equivalent to saying
that the edges cut by the split form a minimum cut
separating the nodes aligned to the left and right
halves of the sentence, respectively. This strat-
egy has the advantage of being able to handle any
alignment/split point combination, but requires a
more complex splitting pattern and thus more ad-
ditional structure than single-branching recursion.
Syntax-driven splitting Since CCG is based
on the assumption that semantic and syntactic
derivations are isomorphic, we might use syntac-
tic annotations to guide the search of the deriva-
tion space and only consider splits along con-
stituent boundaries. Syntactic annotations might
be present in the data or generated by standard
tools. However, initial tests have shown that this
requirement is too restrictive when combined with
our two main heuristics.
Obviously, an effective combination of heuris-
tics needs to be found. One particular configura-
tion which seems promising is alignment consis-
tency combined with min-cut splitting (which is
more permissive than single-branching recursion)
and syntax-driven splitting (which adds an extra
restriction).
3 Discussion
We present some empirical observations about the
behaviour of the above-mentioned heuristics. Our
observations are based on a grammar extracted
from the GMB. A formal evaluation of our system
in the context of a full semantic parsing system is
left for future work.
3.1 Implementation
Currently, our system implements single-
branching recursive splitting along with alignment
consistency. We extracted the word-to-meaning
alignments from the CCG derivations annotated
in the GMB, but kept only alignment edges
to predicate nodes. Sentence grammars were
extracted by generating an initial item for each
sentence and feeding it to the SPLIT procedure.
In addition to alignment consistency and single-
branching recursion, we enforce three simple cri-
teria to rule out highly implausible items: The
count of arrows in an extracted meaning represen-
tation?s type is limited to eight, the number of split
nodes is limited to three, and the number of free
variables in extracted components is also limited
to three.
A major limitation of our implementation is that
it currently only considers the application combi-
nator during splitting. We take this as a main rea-
son for the limited granularity we observe in our
output. Generalisation of the splitting implemen-
tation to other combinators such as composition is
therefore necessary before performing any serious
evaluation.
14
3.2 Manual inspection
Manual inspection of the generated grammars
leads to two general observations.
Firstly, many single-word items present in the
CCG annotations of the GMB are recovered.
While this behaviour is not required, it is en-
couraging, as these items exhibit a relatively sim-
ple structure and would be expected to generalise
well.
At the same time, many multi-word phrases
remain in the data that cannot be split further,
and are therefore unlikely to generalise well. We
have identified two likely causes for this phe-
nomenon: The missing implementation of a com-
position combinator, and coarse alignments.
Composition splits would enable the splitting of
items which do not decompose well (i. e., do not
pass the search heuristics in use) under the appli-
cation combinator. Since composition occurs fre-
quently in GMB derivations, it is to be expected
that its lack noticeably impoverishes the quality of
the extracted grammar.
The extraction of alignments currently in use in
our implementation works by retracing the CCG
derivations annotated in the GMB, and thus es-
tablishing a link between a word and the set of
meaning representation elements introduced by it.
However, our current implementation only han-
dles the most common derivation nodes and oth-
erwise cuts this retracing process short, making
alignments to the entire phrase governed by an in-
termediate node. This may cause the correspond-
ing part of the search to be pruned due to a search
space explosion. We plan to investigate using a
statistical alignment tool instead, possibly using
supplementary heuristics for determining aligned
words and nodes. As an additional advantage, this
would remove the need for annotated CCG deriva-
tions in the data.
3.3 Statistical observations
From the total 47 230 sentences present in the
GMB, our software was able to extract a sentence
grammar for 43 046 sentences. Failures occurred
either because processing took longer than 20 min-
utes, because the count of items extracted for a
single sentence surpassed 10 000, or due to pro-
cessing errors.
On average, 825 items were extracted per sen-
tence with a median of 268. After removing dupli-
cate items, the combined grammar for the whole
GMB consisted of about 32 million items. While
the running time of splitting is still exponential
and gets out of hand on some examples, most sen-
tences are processed within seconds.
Single-word items were extracted for 46% of
word occurrences. Ideally, we would like to ob-
tain single-word items for as many words as pos-
sible, as those items have the highest potential to
generalise to unseen data. For those occurrences
where no single-word item was extracted, the me-
dian length of the smallest extracted item was 12,
with a maximum of 49.
4 Conclusion
We have presented a method for bringing the in-
duction of semantic CCGs to a larger scale than
has been feasible so far. Using the heuristics of
alignment consistency and single-branching recur-
sive splitting, we are able to extract a grammar
from the full GMB. Our observations suggest a
mixed outcome: We obtain desirable single-word
items for only about half of all word occurrences.
However, due to the incompleteness of the im-
plementation and the lack of a formal evaluation,
these observations do not yet permit any conclu-
sions. In future work, we will address both of
these shortcomings.
5 Final remarks
The software implementing the presented func-
tionality is available for download
1
.
This work has been supported by the Ger-
man Research Foundation (DFG) as part of the
CINACS international graduate research group.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC?12, Is-
tanbul, Turkey.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL 2013, Sofia, Bul-
garia.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts, USA.
1
http://nats-www.informatik.
uni-hamburg.de/User/SebastianBeschke
15
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP 2010,
Cambridge, Massachusetts, USA.
Peng Li, Yang Liu, and Maosong Sun. 2013. An ex-
tended GHKM algorithm for inducing ?-scfg. In
Proceedings of AAAI 2013, Bellevue, Washington,
USA.
Mark Steedman. 2001. The Syntactic Process. MIT
Press, January.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of AAAI-96, Portland,
Oregon, USA.
16
